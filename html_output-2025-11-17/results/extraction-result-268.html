<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-268 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-268</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-268</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-271098101</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.08400v2.pdf" target="_blank">Self-training Language Models for Arithmetic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Recent language models achieve impressive results in tasks involving complex multistep reasoning, but scaling these capabilities further traditionally requires expensive collection of more annotated data. In this work, we explore the potential of improving models’ reasoning capabilities without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning ( self-training ). In systematic experimentation across six different arithmetic reasoning datasets, we find that models can substantially improve in both single-round (offline) and online self-training, reaching a correct result in +13.9% and +25.9% more cases, respectively, underlining the importance of actuality of self-training feedback. We further find that in the single-round, offline self-training, traditional supervised training can deliver gains comparable to preference optimization, but in online self-training, preference optimization methods largely outperform supervised training thanks to their superior stability and robustness on unseen types of problems.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e268.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e268.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALCFORMER-FLAN-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALCFORMER (FLAN-XL variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3-billion-parameter calculator-assisted language model (FLAN-finetuned) used as the base model in this paper for arithmetic word-problem solving; it generates chain-of-thought solutions and can interact with a symbolic/calculator tool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CALCFORMER-FLAN-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (chain-of-thought), single-step and two-step arithmetic; includes addition, subtraction, multiplication, division embedded in word problems</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>not explicitly numeric-ranged; tested on datasets of elementary-grade word problems (GSM8K mean ≈3.25 reasoning steps) and simpler 1–2 step datasets (MAWPS, SVAMP, ASDiv-A); exact numeric ranges not specified</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>chain-of-thought generation with calculator-assisted tool interaction; baseline evaluation; used as seed model for offline and online self-training (supervised fine-tuning and preference optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Base (pre-self-training) accuracies (online eval reported): GSM8K 43.2% ±2.7, AQuA-RAT 37.8% ±6.1, Ape210K 26.3% ±2.1, MAWPS 61.9% ±4.2, SVAMP 51.8% ±3.2, ASDiv-A 78.7% ±2.3 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>No low-level mechanistic attribution (e.g., attention-head specialization) is provided; the paper reports that calculator-assisted interaction and chain-of-thought outputs are central to how these models solve arithmetic problems and that exposing models to their own outputs (self-training) improves robustness; LoRA regularization reduces overfitting in fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Authors note CALCFORMERS (3B) perform competitively versus much larger models on some simple tasks (comparison noted to Llama-2 70B and Toolformer 6.7B as self-reported baselines), but no systematic scaling law with model size is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Arithmetic errors (wrong intermediate arithmetic or result), incorrect fraction/rounding handling, repeated or corrupted output tokens, omission or degradation of chain-of-thought steps after some fine-tuning variants, and forgetting of required response formats (notably for multiple-choice AQuA-RAT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to itself before/after self-training, and compared to supervised fine-tuning (SFT) and multiple preference-optimization (PO) methods (DPO, KTO, IPO); also includes self-reported baselines TOOLFORMER (6.7B) and LLAMA-2 (70B).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A calculator-assisted 3B CALCFORMER achieves moderate accuracy on multi-step arithmetic word problems and can be substantially improved via self-training (no new data), with the largest gains coming from online self-training using preference-optimization methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-training Language Models for Arithmetic Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e268.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e268.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Offline self-training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-iteration (offline) self-training using model-generated solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline self-training protocol where the base model generates multiple chain-of-thought solutions to problems in one collection; correct vs incorrect solutions are labeled automatically and used to fine-tune the model either with supervised next-token loss (SFT) or preference-optimization (PO) objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CALCFORMER-FLAN-XL (as the generator and fine-tuning target)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (chain-of-thought solutions), single-step and two-step problems across Ape210K, GSM8K, AQuA-RAT, MAWPS, SVAMP, ASDiv-A</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>datasets include elementary-grade multi-step problems (GSM8K avg 3.25 steps) and simpler 1–2 step datasets; exact numeric ranges not specified</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Collect 16 sampled chain-of-thought predictions per prompt; label solutions correct if final numeric result matches annotation; construct training pairs (SFT) or triples (PO) and fine-tune in a single offline pass; LoRA low-rank adaptation (rank 32) used for some runs; hyperparameter tuning (β/τ) for PO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Authors report that offline self-training yields substantial improvements over base: best-performing offline methods produce an average improvement of +13.9% (absolute across evaluated datasets, reported as the best offline improvement). With LoRA, SFT was often comparable to PO on offline tasks; specific per-dataset offline table entries are in the paper (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Offline improvements arise from exposing the model to correct chain-of-thought solutions; LoRA reduces overfitting (beneficial for both SFT and PO); PO methods tend to converge faster and are more prone to overfitting without LoRA (DPO indicated inclination to overfit). No deeper neural mechanistic explanation (e.g., learned algorithmic operations or attention patterns) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>PO methods converge much faster (≈2,400 steps) than SFT (≈16,600 steps) in the offline setting; LoRA regularization improves generalization and narrows differences between SFT and PO performance in offline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>SFT WITH NEGATIVES performed poorly (suggesting naively training on negatives is nontrivial); PO methods without proper regularization can overfit; offline-only feedback (stale/generated once) is less effective than up-to-date feedback (online).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared the base model vs SFT variants (PLAIN, BALANCED, WITH_NEGATIVES) vs PO methods (DPO, KTO, IPO) in an offline one-shot data-generation setup.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Offline self-training (single-iteration) using model-generated solutions and automated outcome labeling can substantially improve arithmetic reasoning accuracy without new data; with proper regularization (LoRA) supervised fine-tuning can match PO gains offline, but PO converges faster and risks overfitting without regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-training Language Models for Arithmetic Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e268.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e268.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Online self-training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous online self-training with on-the-fly generation and feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online self-training protocol where the model continuously generates multiple solutions for sampled problems, labels them by correctness, and immediately trains on recent correct/incorrect pairs; evaluated with both SFT and PO objectives and with a buffer of generated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CALCFORMER-FLAN-XL (online self-training target and generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems (GSM8K, Ape210K) and simpler 1–2 step datasets (MAWPS, SVAMP, ASDiv-A); chain-of-thought solutions used as training signal</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>not explicitly specified; same datasets as offline experiments (elementary-grade multi-step and 1–2 step problems)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Generate 16 sampled solutions per problem with top-k=50 sampling; label solutions correct if final numeric result matches dataset annotation; maintain a buffer (8192 slots) of training instances; train continuously using SFT or PO (DPO, KTO, IPO) with sampling rules to balance use of solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Online PO methods substantially outperformed SFT and base model. Representative online results (Table 2): Base model GSM8K 43.2% ±2.7; SFT GSM8K 27.4% ±2.5 (degraded); DPO GSM8K 49.1% ±2.7; KTO GSM8K 52.7% ±2.7 (best); IPO GSM8K 49.1% ±2.8. Across datasets KTO improved on average by 12.9 percentage points absolute (25.9% relative to base); KTO improved Ape210K by +11.3% absolute and MAWPS by +16.9% absolute (paper cites these improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Preference-optimization methods (especially KTO) preserve response formats and faithfulness better than SFT in online, preventing format-forgetting (e.g., multiple-choice formats) and improving robustness; exposing the model to its own current outputs (online feedback) yields larger gains than stale offline feedback, indicating actuality of feedback matters; LoRA and KL-weighting (β, τ) are important hyperparameters to control model drift/overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Online training yields larger gains than offline (paper reports best offline +13.9% vs best online +25.9% average improvements); PO methods specifically benefit in online setting, while SFT can degrade on out-of-distribution datasets during online self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>SFT in online training can cause catastrophic forgetting of response formats and omission/degeneration of rationales (notably causing AQuA-RAT degradation); SFT sometimes learns to omit or distort chain-of-thought while still producing correct final answers in-distribution; PO methods can overfit without appropriate KL/LoRA regularization but are overall more stable in online self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared base model vs online SFT vs online PO (DPO, KTO, IPO); also compared online vs offline self-training results; KTO yielded the best online improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Online self-training with up-to-date automated outcome feedback substantially improves arithmetic reasoning, and preference-optimization (especially KTO) outperforms supervised fine-tuning in the online setting by being more stable, preserving rationales and formats, and producing the largest accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-training Language Models for Arithmetic Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e268.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e268.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rationale and failure analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual analysis of chain-of-thought rationales and dominant failure types</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qualitative analysis of model rationales comparing base, SFT-online, and KTO-online checkpoints on GSM8K correct predictions, identifying dominant flaw categories and differences in faithfulness/usability of rationales across training methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CALCFORMER-FLAN-XL (base, SFT-online checkpoint, KTO-online checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multi-step arithmetic word problems analyzed via chain-of-thought outputs</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>GSM8K problems (elementary multi-step; avg ~3.25 steps); analysis sample: 20 correct predictions per checkpoint</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Manual inspection of 20 correct-chain-of-thought outputs per checkpoint; classification of dominant flaw types; per-sample report of faithfulness of rationales (Table 4 referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>The paper finds that SFT-trained checkpoints often learn to omit or partially omit rationales (sacrificing faithfulness/interpretability), while PO-trained checkpoints (KTO) better preserve chain-of-thought faithfulness; exposure to own-generated outputs (self-training) increases robustness but the training objective crucially determines whether rationales remain usable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Five dominant flaw types occur even when final answers are correct (examples include incorrect intermediate arithmetic steps, rounding/fraction mistakes, token repetition/corruption, omitted steps or rationales, and format forgetting). SFT tends to produce less faithful/omitted rationales; PO checkpoints retain more usable rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared rationales from base checkpoint, SFT-online checkpoint, and KTO-online checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Preference-optimization during self-training preserves usable, faithful chain-of-thought rationales better than supervised fine-tuning, which can obtain accuracy gains at the cost of rationale faithfulness and usability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-training Language Models for Arithmetic Reasoning', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Solving math word problems with process-and outcome-based feedback <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct <em>(Rating: 1)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>Kto: Model alignment as prospect theoretic optimization <em>(Rating: 2)</em></li>
                <li>Talm: Tool augmented language models <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-268",
    "paper_id": "paper-271098101",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "CALCFORMER-FLAN-XL",
            "name_full": "CALCFORMER (FLAN-XL variant)",
            "brief_description": "A 3-billion-parameter calculator-assisted language model (FLAN-finetuned) used as the base model in this paper for arithmetic word-problem solving; it generates chain-of-thought solutions and can interact with a symbolic/calculator tool.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CALCFORMER-FLAN-XL",
            "model_size": "3B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (chain-of-thought), single-step and two-step arithmetic; includes addition, subtraction, multiplication, division embedded in word problems",
            "number_range_or_complexity": "not explicitly numeric-ranged; tested on datasets of elementary-grade word problems (GSM8K mean ≈3.25 reasoning steps) and simpler 1–2 step datasets (MAWPS, SVAMP, ASDiv-A); exact numeric ranges not specified",
            "method_or_intervention": "chain-of-thought generation with calculator-assisted tool interaction; baseline evaluation; used as seed model for offline and online self-training (supervised fine-tuning and preference optimization).",
            "performance_result": "Base (pre-self-training) accuracies (online eval reported): GSM8K 43.2% ±2.7, AQuA-RAT 37.8% ±6.1, Ape210K 26.3% ±2.1, MAWPS 61.9% ±4.2, SVAMP 51.8% ±3.2, ASDiv-A 78.7% ±2.3 (Table 2).",
            "mechanistic_insight": "No low-level mechanistic attribution (e.g., attention-head specialization) is provided; the paper reports that calculator-assisted interaction and chain-of-thought outputs are central to how these models solve arithmetic problems and that exposing models to their own outputs (self-training) improves robustness; LoRA regularization reduces overfitting in fine-tuning.",
            "performance_scaling": "Authors note CALCFORMERS (3B) perform competitively versus much larger models on some simple tasks (comparison noted to Llama-2 70B and Toolformer 6.7B as self-reported baselines), but no systematic scaling law with model size is provided in this paper.",
            "failure_modes": "Arithmetic errors (wrong intermediate arithmetic or result), incorrect fraction/rounding handling, repeated or corrupted output tokens, omission or degradation of chain-of-thought steps after some fine-tuning variants, and forgetting of required response formats (notably for multiple-choice AQuA-RAT).",
            "comparison_baseline": "Compared to itself before/after self-training, and compared to supervised fine-tuning (SFT) and multiple preference-optimization (PO) methods (DPO, KTO, IPO); also includes self-reported baselines TOOLFORMER (6.7B) and LLAMA-2 (70B).",
            "key_finding": "A calculator-assisted 3B CALCFORMER achieves moderate accuracy on multi-step arithmetic word problems and can be substantially improved via self-training (no new data), with the largest gains coming from online self-training using preference-optimization methods.",
            "uuid": "e268.0",
            "source_info": {
                "paper_title": "Self-training Language Models for Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Offline self-training",
            "name_full": "Single-iteration (offline) self-training using model-generated solutions",
            "brief_description": "An offline self-training protocol where the base model generates multiple chain-of-thought solutions to problems in one collection; correct vs incorrect solutions are labeled automatically and used to fine-tune the model either with supervised next-token loss (SFT) or preference-optimization (PO) objectives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CALCFORMER-FLAN-XL (as the generator and fine-tuning target)",
            "model_size": "3B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (chain-of-thought solutions), single-step and two-step problems across Ape210K, GSM8K, AQuA-RAT, MAWPS, SVAMP, ASDiv-A",
            "number_range_or_complexity": "datasets include elementary-grade multi-step problems (GSM8K avg 3.25 steps) and simpler 1–2 step datasets; exact numeric ranges not specified",
            "method_or_intervention": "Collect 16 sampled chain-of-thought predictions per prompt; label solutions correct if final numeric result matches annotation; construct training pairs (SFT) or triples (PO) and fine-tune in a single offline pass; LoRA low-rank adaptation (rank 32) used for some runs; hyperparameter tuning (β/τ) for PO.",
            "performance_result": "Authors report that offline self-training yields substantial improvements over base: best-performing offline methods produce an average improvement of +13.9% (absolute across evaluated datasets, reported as the best offline improvement). With LoRA, SFT was often comparable to PO on offline tasks; specific per-dataset offline table entries are in the paper (Table 1).",
            "mechanistic_insight": "Offline improvements arise from exposing the model to correct chain-of-thought solutions; LoRA reduces overfitting (beneficial for both SFT and PO); PO methods tend to converge faster and are more prone to overfitting without LoRA (DPO indicated inclination to overfit). No deeper neural mechanistic explanation (e.g., learned algorithmic operations or attention patterns) is provided.",
            "performance_scaling": "PO methods converge much faster (≈2,400 steps) than SFT (≈16,600 steps) in the offline setting; LoRA regularization improves generalization and narrows differences between SFT and PO performance in offline experiments.",
            "failure_modes": "SFT WITH NEGATIVES performed poorly (suggesting naively training on negatives is nontrivial); PO methods without proper regularization can overfit; offline-only feedback (stale/generated once) is less effective than up-to-date feedback (online).",
            "comparison_baseline": "Compared the base model vs SFT variants (PLAIN, BALANCED, WITH_NEGATIVES) vs PO methods (DPO, KTO, IPO) in an offline one-shot data-generation setup.",
            "key_finding": "Offline self-training (single-iteration) using model-generated solutions and automated outcome labeling can substantially improve arithmetic reasoning accuracy without new data; with proper regularization (LoRA) supervised fine-tuning can match PO gains offline, but PO converges faster and risks overfitting without regularization.",
            "uuid": "e268.1",
            "source_info": {
                "paper_title": "Self-training Language Models for Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Online self-training",
            "name_full": "Continuous online self-training with on-the-fly generation and feedback",
            "brief_description": "An online self-training protocol where the model continuously generates multiple solutions for sampled problems, labels them by correctness, and immediately trains on recent correct/incorrect pairs; evaluated with both SFT and PO objectives and with a buffer of generated examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CALCFORMER-FLAN-XL (online self-training target and generator)",
            "model_size": "3B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems (GSM8K, Ape210K) and simpler 1–2 step datasets (MAWPS, SVAMP, ASDiv-A); chain-of-thought solutions used as training signal",
            "number_range_or_complexity": "not explicitly specified; same datasets as offline experiments (elementary-grade multi-step and 1–2 step problems)",
            "method_or_intervention": "Generate 16 sampled solutions per problem with top-k=50 sampling; label solutions correct if final numeric result matches dataset annotation; maintain a buffer (8192 slots) of training instances; train continuously using SFT or PO (DPO, KTO, IPO) with sampling rules to balance use of solutions.",
            "performance_result": "Online PO methods substantially outperformed SFT and base model. Representative online results (Table 2): Base model GSM8K 43.2% ±2.7; SFT GSM8K 27.4% ±2.5 (degraded); DPO GSM8K 49.1% ±2.7; KTO GSM8K 52.7% ±2.7 (best); IPO GSM8K 49.1% ±2.8. Across datasets KTO improved on average by 12.9 percentage points absolute (25.9% relative to base); KTO improved Ape210K by +11.3% absolute and MAWPS by +16.9% absolute (paper cites these improvements).",
            "mechanistic_insight": "Preference-optimization methods (especially KTO) preserve response formats and faithfulness better than SFT in online, preventing format-forgetting (e.g., multiple-choice formats) and improving robustness; exposing the model to its own current outputs (online feedback) yields larger gains than stale offline feedback, indicating actuality of feedback matters; LoRA and KL-weighting (β, τ) are important hyperparameters to control model drift/overfitting.",
            "performance_scaling": "Online training yields larger gains than offline (paper reports best offline +13.9% vs best online +25.9% average improvements); PO methods specifically benefit in online setting, while SFT can degrade on out-of-distribution datasets during online self-training.",
            "failure_modes": "SFT in online training can cause catastrophic forgetting of response formats and omission/degeneration of rationales (notably causing AQuA-RAT degradation); SFT sometimes learns to omit or distort chain-of-thought while still producing correct final answers in-distribution; PO methods can overfit without appropriate KL/LoRA regularization but are overall more stable in online self-training.",
            "comparison_baseline": "Compared base model vs online SFT vs online PO (DPO, KTO, IPO); also compared online vs offline self-training results; KTO yielded the best online improvements.",
            "key_finding": "Online self-training with up-to-date automated outcome feedback substantially improves arithmetic reasoning, and preference-optimization (especially KTO) outperforms supervised fine-tuning in the online setting by being more stable, preserving rationales and formats, and producing the largest accuracy gains.",
            "uuid": "e268.2",
            "source_info": {
                "paper_title": "Self-training Language Models for Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Rationale and failure analysis",
            "name_full": "Manual analysis of chain-of-thought rationales and dominant failure types",
            "brief_description": "Qualitative analysis of model rationales comparing base, SFT-online, and KTO-online checkpoints on GSM8K correct predictions, identifying dominant flaw categories and differences in faithfulness/usability of rationales across training methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CALCFORMER-FLAN-XL (base, SFT-online checkpoint, KTO-online checkpoint)",
            "model_size": "3B",
            "model_architecture": null,
            "arithmetic_operation_type": "multi-step arithmetic word problems analyzed via chain-of-thought outputs",
            "number_range_or_complexity": "GSM8K problems (elementary multi-step; avg ~3.25 steps); analysis sample: 20 correct predictions per checkpoint",
            "method_or_intervention": "Manual inspection of 20 correct-chain-of-thought outputs per checkpoint; classification of dominant flaw types; per-sample report of faithfulness of rationales (Table 4 referenced).",
            "performance_result": null,
            "mechanistic_insight": "The paper finds that SFT-trained checkpoints often learn to omit or partially omit rationales (sacrificing faithfulness/interpretability), while PO-trained checkpoints (KTO) better preserve chain-of-thought faithfulness; exposure to own-generated outputs (self-training) increases robustness but the training objective crucially determines whether rationales remain usable.",
            "performance_scaling": null,
            "failure_modes": "Five dominant flaw types occur even when final answers are correct (examples include incorrect intermediate arithmetic steps, rounding/fraction mistakes, token repetition/corruption, omitted steps or rationales, and format forgetting). SFT tends to produce less faithful/omitted rationales; PO checkpoints retain more usable rationales.",
            "comparison_baseline": "Compared rationales from base checkpoint, SFT-online checkpoint, and KTO-online checkpoint.",
            "key_finding": "Preference-optimization during self-training preserves usable, faithful chain-of-thought rationales better than supervised fine-tuning, which can obtain accuracy gains at the cost of rationale faithfulness and usability.",
            "uuid": "e268.3",
            "source_info": {
                "paper_title": "Self-training Language Models for Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Solving math word problems with process-and outcome-based feedback",
            "rating": 2,
            "sanitized_title": "solving_math_word_problems_with_processand_outcomebased_feedback"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
            "rating": 1,
            "sanitized_title": "wizardmath_empowering_mathematical_reasoning_for_large_language_models_via_reinforced_evolinstruct"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Kto: Model alignment as prospect theoretic optimization",
            "rating": 2,
            "sanitized_title": "kto_model_alignment_as_prospect_theoretic_optimization"
        },
        {
            "paper_title": "Talm: Tool augmented language models",
            "rating": 1,
            "sanitized_title": "talm_tool_augmented_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.014485499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-training Language Models for Arithmetic Reasoning</p>
<p>Marek Kadlčík kadlcik@mail.muni.cz 
Faculty of Informatics
Masaryk University
Czech Republic</p>
<p>Michal Štefánik stefanik.m@mail.muni.cz 
Faculty of Informatics
Masaryk University
Czech Republic</p>
<p>Self-training Language Models for Arithmetic Reasoning
9E9D710F003E89361ABD7DFD335607DD
Recent language models achieve impressive results in tasks involving complex multistep reasoning, but scaling these capabilities further traditionally requires expensive collection of more annotated data.In this work, we explore the potential of improving models' reasoning capabilities without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning (self-training).In systematic experimentation across six different arithmetic reasoning datasets, we find that models can substantially improve in both single-round (offline) and online self-training, reaching a correct result in +13.9% and +25.9% more cases, respectively, underlining the importance of actuality of self-training feedback.We further find that in the single-round, offline self-training, traditional supervised training can deliver gains comparable to preference optimization, but in online self-training, preference optimization methods largely outperform supervised training thanks to their superior stability and robustness on unseen types of problems.</p>
<p>Introduction</p>
<p>Despite recent improvements in the practical usability of language models (LMs) attributed to preference alignment methods (Wang et al., 2023), these models often struggle with tasks requiring reasoning, i.e., a process of inferring a conclusion or decision logically and systematically (Huang and Chang, 2023).Previous work improves the reasoning capabilities of language models by scaling training data to more diverse (Kadlčík et al., 2023) or complex (Hendrycks et al., 2021) collections, but reaching further improvements in this direction becomes exceedingly expensive.In the offline variant, the model generates all predictions in a single round.In the online variant, the training data is continuously generated.</p>
<p>In this work, we evaluate the potential of improving models' capabilities by training from implicit, automated feedback to models' responses.Arithmetic reasoning tasks present a challenge that reflects heavily on the model's reasoning capabilities, while the quality of the model's responses can be automatically assessed against the annotated correct results rather than expensive and possibly subjective judgments of model outputs (Hu et al., 2023).Thus, we choose the arithmetic reasoning to address our two main research questions:</p>
<p>RQ1: Can we improve the reasoning abilities of language models with self-training without any new data data?RQ2: Can the preference optimization bring further improvements to models' capabilities over traditional supervised fine-tuning?arXiv:2407.08400v3[cs.CL] 23 Oct 2024</p>
<p>We address these questions by implementing two variants of self-training: (1) an offline variant, where the training feedback to the model responses is constructed in a single iteration ( §3.1), and (2) an online variant, where the model obtains and trains on the feedback to its current predictions ( §3.2).</p>
<p>Our experiments reveal that both self-training variants present an efficient method for improving LMs' capabilities with implicit training signal; both variants allow to significantly improve the initial model without any new data.In the offline variant, similar improvements can be achieved by both supervised and preference optimization methods.However, the online variant reveals crucial issues in scaling the supervised training to autonomous settings.On the contrary, preference optimization methods can robustly persist the original capabilities even in autonomous self-training while reaching further improvements.</p>
<p>Finally, the difference in average improvement between our best-performing offline (+13.9%) and online method (+25.9%)indicates that the actuality of self-training feedback is a crucial factor of self-training effectivity.Our results motivate future research towards exploring new sources of implicit feedback able to provide language models with immediate feedback to their current predictions.</p>
<p>Related Work</p>
<p>We build upon a line of previous work that experiments with providing feedback to language models in arithmetical reasoning.Notably, Luo et al. (2023) train models with PPO (Schulman et al., 2017) against feedback on individual steps given by ChatGPT 3.5.Uesato et al. (2022) apply variants of self-training on GMS8K and compare the effectiveness of giving outcome-based (per solution) or process-based (per each step in solution) feedback, concluding that the two approaches result in comparable accuracy, but outcome-based feedback delivers a higher error rate in the rationales.Lightman et al. (2023) also focus on a comparison of process-based and outcome-based feedback on a larger scale and conclude that process-based feedback outperforms outcome-based at end-result accuracy.</p>
<p>Our work is closest to Parisi et al. (2022) and Zelikman et al. (2022).Parisi et al. (2022) apply self-training with a traditional supervised objective: they train the model on a small set of seed data and continuously use the trained model to generate solutions for a larger set, from which correct solutions are used in another training epoch.They show that three such subsequent epochs can improve the accuracy with diminishing returns.Zelikman et al. (2022) experiment with self-training with supervised fine-tuning on commonsense and math reasoning.They report positive results of self-training on the model's reasoning capabilities under specific conditions: (1) the initial model must be capable enough to be able to achieve improvements, and (2) training tasks must hold a negligible chance of random success (unlike, e.g., binary classification).</p>
<p>Our work builds upon these findings but differs from previous work in our objectives and data setting; We provide a systematic comparison of different training objectives in both online and offline settings, including the most recent preference optimization methods and show that training objective indeed plays a crucial role, especially in the online setting.Our data setting is more ambitious than of previous work: we show that self-training can deliver substantial improvements also by using only problems already seen in previous training.Finally, contrary to previous self-training work, we make our code1 and models2 freely available to accelerate future work in self-training.</p>
<p>Experiments</p>
<p>Our experiments build upon the 3-billion-parameter FLAN models fine-tuned specifically for arithmetic reasoning in previous work of Kadlčík et al. (2023).These relatively compact calculator-assisted models called CALCFORMERS were shown to perform noticeably well on multi-step reasoning, while even on single-step and two-step problems perform compared to 70B Llama-2 (Touvron et al., 2023).Another desiderata of these models is the transparency of their training data.In our experiments, this allows us to opt for a more challenging yet realistic self-training setting where we do not train the models on any new data, but only on the problems that CALCFORMERS have already seen in the training, merely with a complementary training signal.Specifically, we self-train these models with the prompts from Ape210K (Zhao et al., 2020) (Patel et al., 2021).</p>
<p>In both self-training variants, we use the trained model to generate training data (see Fig. 1).The generated data consists of the original input prompt (x i ) and associated model predictions (y i ) in the form of a chain-of-thought sequence containing the model's final result at the end.For each prompt, we generate 16 predictions using sampled generation.Annotations of correct results then allow us to automatically annotate each prediction for either being correct (y OK i ), or incorrect (y NOK i ), assigning a set of both correct and incorrect predictions to each input prompt.</p>
<p>For the supervised fine-tuning (SFT) objective, we construct the training dataset from pairs of (x i , y OK i ).SFT uses a standard next-token prediction with cross-entropy loss and teacher forcing (Bahdanau et al., 2015).All preference optimization (PO) objectives then train on triples (x i , y OK i , y NOK i ), with the y OK i marked as being preferred over y NOK i .</p>
<p>We experiment with three recent preference optimization methods: Direct Preference Optimization; DPO (Rafailov et al., 2023), Kahneman-Tversky Optimization; KTO (Ethayarajh et al., 2024) and Identity Preference Optimization; IPO (Azar et al., 2023).These methods differ in a variety of aspects in the formulation of training loss.For brevity, we direct the reader to the referenced work for further details on preference optimisation methods.Further details of our general training setup can be found in Appendix A.</p>
<p>Offline Self-training</p>
<p>In the offline variant, we perform a single iteration of collecting predictions with prompts from Ape210K, resulting in over 24,000 prompts with at least one positive and one negative prediction.</p>
<p>All PO methods rely on a crucial parameter β or τ that weights the KL regularization of the trained model according to the original "reference" model.We perform a hyperparameter tuning of this parameter with β ∈ (0.01, 0.1, 0.3, 0.6, 0.9, 0.99) according to in-domain validation accuracy separately for each method and report the results for the best two configurations.</p>
<p>For SFT, we experiment with 3 variants.SFT PLAIN is trained on pairs (x i , y OK i ).In SFT BAL-ANCED and SFT WITH NEGATIVES, we aim to compensate for the potential data disadvantage of SFT PLAIN compared to PO methods exhibiting the trained model to two solutions (y OK i , y NOK i ) per problem: (i) In SFT BALANCED, we use two different correct predictions y OK i for one x i .(ii) In SFT WITH NEGATIVES, we use both positive y OK i and negative y NOK i as targets for each x i .In the training data constructed from y NOK i , we prefix x i with a phrase "Write incorrect solution for the following problem".This exposes the model to both correct and incorrect solutions, conceivably helping it to differentiate between the two within SFT training.</p>
<p>Finally, we re-train the best-performing run of each method with a low-rank adaptation (LoRA) (Hu et al., 2021), a commonly used fine-tuning regularization technique that restricts the fine-tuning update of each weight to have a specific low rank.We apply LoRA with a rank of 32 on all linear projections in the model.</p>
<p>Results</p>
<p>Table 1 compares the accuracy achieved in offline self-training with each method.A comparison of supervised and more complex preference optimization methods reveals a relatively small difference between the best-performing configurations of both categories.Especially thanks to LoRA regularization, SFT shows the ability to reach results comparable in most datasets.Similar to SFT, LoRA regularization also has a positive effect on DPO, evidencing DPO's inclination to overfitting, as also evidenced by previous work (Azar et al., 2023).Among all supervised methods, the SFT WITH NEGATIVES performs the worst, showing that using negative feedback in supervised training analogically to preference optimization is nontrivial.</p>
<p>On the practical side, we note that PO methods converge much faster than SFT methods, achieving the best validation scores on average after around 2,400 training steps compared to 16,600 steps in supervised setups.A detailed comparison of training steps and time can be found in Table 3.</p>
<p>Online Self-training</p>
<p>In the online self-training, we generate the training data on the fly.Therefore, throughout the whole training, both the positive and negative predictions used for conditioning the updates can realistically be generated by the trained model.Previous work showed that exposing the model to its own outputs might itself improve its robustness (Štefánik et al., 2023).In our online self-training experiments, we additionally evaluate the LM's capability to auto-nomously improve its reasoning capability based on the up-to-date feedback to its own predictions.</p>
<p>A methodology of constructing training samples from the model's predictions for both SFT and PO methods remains identical to the offline variant.Details of data processing can be found in Appendix A.1.As the generation process in online training substantially slows down updates, we restrain the scale of experiments to the bestperforming configurations from the offline variant.</p>
<p>Results</p>
<p>Table 2 shows the accuracy of training methods in online self-training.This setting reveals much larger differences between methods.Supervised fine-tuning (SFT) improves accuracy on simple one-step and two-step datasets (MAWPS, SVAMP, and ASDiv-A) but substantially degrades performance on out-of-distribution GSM8K and AQuA-RAT.Manual inspection (Appendix B) reveals that the degradation on AQuA-RAT is caused by the model's forgetting of the response format of multiple-choice questions, well-preserved by all PO methods.Contrary to the SFT, PO methods deliver significant improvements compared to both the base model and their offline variants (Table 1).Noticeable is the improvement of DPO on GSM8K (by 11.9% of absolute accuracy, i.e. by 22.0% relative to base model), among other cases, suggesting that self-training can mitigate overfitting of PO methods.The best-performing KTO method also substantially improved compared to the offline variant; by 11.3% of accuracy on in-domain Ape210K, or by 16.9% on simpler, out-of-domain MAWPS.Among all other online methods, KTO performs best on every dataset except for AQuA-RAT, on average improving by 12.9% of absolute accuracy, i.e. by 25.9% relative to the base model.</p>
<p>Appendix B provides a per-sample analysis of differences between outputs of SFT and PO models, with a report from a manual assessment of faithfulness of models' rationales in Table 4. Noticeably, we find that while the SFT also achieves large indistribution improvements, this comes for the price of faithfulness and usability of its rationales, as the SFT model learns to completely or partially omit most of the rationales.2023), and are limited to single-step reasoning datasets due to inherent limitations of their tool-using mechanism.</p>
<p>tion of problems that it always solves correctly and, more importantly, robustly reduces the proportion of problems that it can not solve.</p>
<p>Conclusions</p>
<p>This work explores the potential of autonomously improving language models for arithmetic reasoning: a task allowing automated, immediate, and objective feedback based on the correct results.</p>
<p>We experiment with two settings: (i) offline selftraining, collecting the feedback in a single iteration, and (ii) online self-training, where the model trains continuously from feedback to its up-todate predictions.In both settings, we apply and compare recent preference optimization methods (DPO, KTO, IPO) with standard supervised training (SFT).</p>
<p>We find that self-training provides an opportunity to improve models' capabilities without any new data, using exclusively models' own predictions and automated feedback.In addition to the offline variant, online self-training provides further opportunities for data-free improvements thanks to the enhanced robustness of preference optimization methods.</p>
<p>Our work motivates future work towards seeking other sources of implicit training feedback beyond arithmetic reasoning, exemplified in previous work in a reasoning coherence (Akyürek et al., 2024) or consistency (Štefánik et al., 2024).Presenting language models with novel sources of implicit feedback via self-training can fill the gap of the traditional, largely simplified training objectives and empower models to capture more complex structural dependencies necessary in many real-world applications.</p>
<p>Limitations</p>
<p>Despite the fact that our proposed self-training methods do not require any new human annotation, we acknowledge their limitations in the extensive computational requirements given by generating the data.While the data generation for the offline variant can be parallelized, this is more difficult for the online variant, where the model is trained with its own most recent predictions.As a result, our self-training experiments took between 15 and 30 days to converge on a single Nvidia A100 GPU.</p>
<p>The time-demanding character of online selftraining experiments is a direct cause of another limitation: a constrained diversity of models and datasets that we experiment with.As such, the experiments and conclusions of our work should inspire experiments with self-training in other applications but may not be generalized to claims on the general effectiveness of self-training.</p>
<p>A Training Details</p>
<p>In every configuration of both preference and supervised training, the model is trained with Adafactor (Shazeer and Stern, 2018) optimizer with an effective batch size of 32, a learning rate of 2 • 10 −5 with 1,000 warmup steps, and a linear decay to 0 in 1 million steps.The models were trained in bfloat16 (Wang and Kanwar, 2023) precision with mixed precision training (Micikevicius et al., 2017).</p>
<p>The training terminates after convergence on the in-domain dataset (Ape210K), and then the best checkpoint from the training is selected according to in-domain validations.</p>
<p>Each of our experiments can be reproduced with a single Nvidia A100/A40 graphic card and 32GB of RAM.Note that especially the online self training experiments can take up to 31 days to converge.</p>
<p>A.1 Online self-training</p>
<p>To create new data in online self-training, we sample a random problem from Ape210K and generate predictions with the current model.Next, we label each solution as correct if its result matches the one in the data.The online self-training process is illustrated in Figure 1.</p>
<p>In this experiment, we again compare supervised training and preference optimization.In all variants, we generate 16 solutions per problem with top-k=50 sampling using the latest model, but the subsequent data processing is method-specific.Preference Optimization: After the solutions are generated, we create all possible pairs of solutions where one solution has a correct result and the other one does not.We then sample with repetition from the pairs, such that:</p>
<ol>
<li>every correct solution is used at most 4 times, Almost the same number of times means a maximal difference of one.</li>
</ol>
<p>In both supervised and preference training, the training instances are put into a buffer with 8192 slots, from which they are sampled randomly for training.When a batch of data gets sampled, it is removed from the buffer, and new data are generated with the correct model to fill the empty slots.</p>
<p>During training, we track the proportion of problems that the models consistently solve correctly or fail to solve across 16 trials.Figure 2 shows the progression of the best-performing online training run elaborating the preference optimisation with KTO.</p>
<p>B Output analyses</p>
<p>Aiming to better understand the difference between self-training with preference optimization methods and supervised training, we manually analyze a set of randomly chosen rationales generated for prompts of the GSM8K test set.We collect the rationales from (i) the original checkpoint, (ii) the checkpoint trained in online self-training and supervised method (denoted SFT), and (iii) the checkpoint trained on online self-training with the bestperforming method (KTO).Due to the time complexity of evaluating long chain-of-thought output sequences, we analyze 20 predictions marked as correct for each checkpoint.</p>
<p>Within the analysis, we encounter 5 types of dominant flaws that models' outcomes exhibit, even when being correct:</p>
<p>B.1 Examples of predictions</p>
<p>For an illustration of differences between the supervised and preference optimization checkpoints, below we include a set of predictions of SFT and KTO checkpoints for randomly selected prompts of GSM8K collection.These examples can also be found by associated IDs in the original dataset repository 3 .For better readability, we removed the navigation tokens used by the model for interaction with the calculator tool during the generation.</p>
<p>id: gsm8k__aBNdImSK9ddCwuYb</p>
<p>Question: Tobias bought a big pizza with 60 pieces.He ate 2/5 of the pieces on the first day, 10 pieces on the second day, and 7/13 of the remaining pieces on the third day.How many pizza pieces has he eaten so far?</p>
<p>Correct answer: 48</p>
<p>Prediction with SFT: 60 * 2/5 = 28 ✗ Prediction with KTO:</p>
<p>On the first day, Tobias ate 60 * 2/5 = 60<em>2/5 24 24 pieces.The number of pieces that remained after the second day is 60 -24 = 60-24 36 36 pieces.On the third day, he ate 36 * 7/13 = 36</em>7/13 252/13 = around 19.384615 25 pieces.The total number of pieces he has eaten so far is 10 + 252/13 = 10+252/13 382/13 = around 29.384615 38 pieces.Tobias ate a total of 24 + 10 + (382/13) = 24+10+352/13 794/13 = around 61.076923 69 pieces.794/13 = around 69.076923 ✗ 3 https://huggingface.co/datasets/MU-NLPC/Calc-gsm8kid: gsm8k__qIygQI2WNfRPWoYj Question: Jennie is helping at her mom's office.She has a pile of 60 letters needing stamps, and a pile of letters already stamped.She puts stamps on one-third of the letters needing stamps.If there are now 30 letters in the pile of already-stamped letters, how many were in that pile when Jennie began?</p>
<p>Figure 1 :
1
Figure 1: Schema of self-training that we apply to provide the model with training feedback to its predictions.In the offline variant, the model generates all predictions in a single round.In the online variant, the training data is continuously generated.</p>
<p>Figure 2
2
Figure 2 visualizes the dynamics of online selftraining in solving known problems during training.We can see that self-training increases the propor-</p>
<p>Supervised training: After generating the solutions, we discard the incorrect ones.The correct solutions are oversampled to generate 32 training examples.Each solution is sampled at most 4 times each, and all solutions are used almost the same number of times (maximal difference of one).</p>
<ol>
<li>the number of preference pairs per problem is 32 if possible without violating the condition 1, 3. all correct solutions are used almost the same number of times, 4. all incorrect solutions are used almost the same number of times.</li>
</ol>
<p>Figure 2 :
2
Figure 2: Training dynamics of online training: The fraction of training problems for which the model predicted all and none of 16 trials correctly during training of the online KTO with β = 0.1.The fraction is computed from a sliding window of the last 1000 problems and the chart is smoothed for visual clarity.</p>
<p>Jennie stamps 60 / 3 = 60/3 20 20 letters.Then the number of letters still needing stamps is 60 -20 = 60-20 40 40.The number of letters already stamped is 30 -20 = 30-20 10 10.There were 40 + 10 = 40+10 50 50 letters in the pile when Jennie began.50 ✗id: gsm8k__0oOjz5Ub66DF4inZQuestion: There are 6 trees in Chris's yard.Ferdinand has half the number of trees that Chris has.Harry has 5 more than twice the number of trees that Ferdinand has.How many more trees are in Harry's yard than Ferdinand's yard?Half the number of trees that Chris has is 6 trees / 2 = 6/2 3 3 trees.Twice the number of trees that Ferdinand has is 3 trees * 2 = 3*2 6 6 trees.Harry has 5 trees + 6 trees = 6+5 11 11 trees.There are 11 trees -6 trees = 11-6 5 5 more trees in Harry's yard than in Ferdinand's yard.5 5 5 more trees in Harry's yard than in Ferdinand's yard.5 5 5 more trees in Harry's yard than in Ferdinand's yard.11 11 5 more trees than in Ferdinand's yard.11-6 5 5 more trees in Harry's yard.5 ✗</p>
<p>Table 1 :
1, to our
(Cobbe et al., 2021)t results obtained in offline self-training of BASE MODEL (CALCFORMER-FLAN-XL) on Ape210K problems.For each preference optimization method, we report results for its two best-performing configurations.Bold entries denote the best results among supervised and preference optimization methods per dataset.Confidence intervals are bootstrapped (500 samples, 1,000 repeats).tiontoApe210K'stestset,weevaluateour models on five other math datasets, assessing the robustness of models' capabilities in new types of math problems; GSM8K(Cobbe et al., 2021)containing multistep elementary-grade problems requiring on average 3.25 steps to achieve correct result, AQuA-RAT(Ling et al., 2017)with more complex, multiple-choice tasks, and three simpler, one to two-steps datasets: MAWPS(Koncel-Kedziorski  et al., 2016), ASDiv-A (Miao et al., 2020), and SVAMP</p>
<p>Table 2 :
2
Schick et al. (2023)t results obtained by online self-training of BASE MODEL (CALCFORMER-FLAN-XL) on Ape210K problems.Bold denotes the best self-trained result per dataset.Confidence intervals are obtained from bootstrapping (500 samples, 1,000 repeats).Evaluations of the previous tool-using arithmetic reasoning models (TOOLFORMER and LLAMA 2) are self-reported results fromSchick et al. (2023)andTouvron et al. (
GSM8K AQuA-RAT Ape210K MAWPS SVAMP ASDiv-ATOOLFORMER (6.7B)44.029.440.4LLAMA 2 (70B)82.469.267.1BASE MODEL (3B)43.2±2.737.8±6.126.3±2.1 61.9±4.2 51.8±3.2 78.7±2.3SFT27.4±2.57.9±3.341.2±2.3 63.8±4.2 59.8±3.1 83.3±2.1DPO (β = 0.9)49.1±2.739.8±5.937.9±2.3 79.6±3.4 57.3±3.1 85.6±2.0KTO (β = 0.1)52.7±2.736.6±6.149.6±2.4 85.2±3.0 62.6±3.1 90.6±1.6IPO (τ = 0.99)49.1±2.835.8±5.942.2±2.3 81.5±3.4 56.8±3.0 86.6±1.9</p>
<p>Table 4 :
4
Output analysis: ratio of model predictions containing one of our identified flaws, evaluated on a sample of models' correct predictions.</p>
<p>https://github.com/prompteus/calc-x
see our HuggingFace Hub
AcknowledgementsWe acknowledge the Centre for Biomedical Image Analysis at Masaryk University supported by MEYS CR (LM2023050 and CZ.02.1.01/0.0/0.0/18_046/0016045Czech-BioImaging) for providing computational resources for training models and collecting evaluations presented in this paper.
Deductive closure training of language models for coherence, accuracy, and updatability. Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, Jacob Andreas, ; Mohammad, Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos, 10.18653/v1/2024.findings-acl.584arXiv:2310.12036Neural Machine Translation by Jointly Learning to Align and Translate. Bangkok, Thailand; San Diego, USA2024. 2023. 2015PreprintFindings of the Association for Computational Linguistics ACL 2024. 3rd International Conference on Learning Representations, ICLR 2015</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Kto: Model alignment as prospect theoretic optimization. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela, arXiv:2402.013062024Preprint</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, CoRR, abs/2103.038742021</p>
<p>. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen, </p>
<p>Low-rank adaptation of large language models. CoRR, abs/2106.09685Lora</p>
<p>ACL. Marek Kadlčík, Michal Štefánik, Ondřej Sotolář, and Vlastimil Martinek. 2023. Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Fei Liu, 10.18653/v1/N16-1136Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Acl , Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore; Toronto, Canada; Singapore; San Diego, CaliforniaAssociation for Computational Linguistics2023. 2023. 2016Proceedings of the 2016 Conference of the North American Chapter</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, Ilya Edwards ; John Schulman, Karl Sutskever, Cobbe, arXiv:2305.20050CoRR, abs/1705.04146Wang Ling, Dani Yogatama, Chris Dyer, and Phil BlunsomJan Leike,. 2023. 2017Bowen Baker, Teddy LeearXiv preprintLet's verify step by step</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang ; Shen-Yun Miao, Chao-Chun Liang, Keh-Yih Su, 10.18653/v1/2020.acl-main.92arXiv:2308.09583Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2023. 2020PreprintA diverse corpus for evaluating and developing English math word problem solvers</p>
<p>. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu, CoRR, abs/1710.037402017</p>
<p>Talm: Tool augmented language models. Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.122552022Preprint</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.182902023Preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023Preprint</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347CoRR, abs/1804.042352017. 2018PreprintProximal policy optimization algorithms</p>
<p>Concept-aware data construction improves in-context learning of language models. Michal Štefánik, Marek Kadlčík, Petr Sojka, 10.18653/v1/2024.findings-acl.733Findings of the Association for Computational Linguistics ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>Soft Alignment Objectives for Robust Adaptation of Language Generation. Michal Štefánik, Marek Kadlčík, Petr Sojka, 10.18653/v1/2023.acl-long.492Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada. ACL20231</p>
<p>. Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cantón Cristian, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony S Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel M Khabsa, A V Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, R Smith, Xia Subramanian, Binh Tan, Ross Tang, Adina Taylor, Jian Williams, Puxin Xiang Kuan, Zhengxu Xu, Iliyan Yan, Yuchen Zarov, Zhang, ArXiv, abs/2307.09288Aurelien RodriguezAngela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.14275Shibo Wang and Pankaj Kanwar. 2022. 2023PreprintBfloat16: The secret to high performance on cloud tpus</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, arXiv:2307.129662023arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Ape210k: A large-scale and template-rich dataset of math word problems. Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu, 2020. 2009.11506</p>            </div>
        </div>

    </div>
</body>
</html>
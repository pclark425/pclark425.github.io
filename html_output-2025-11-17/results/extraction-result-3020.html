<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3020 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3020</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3020</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-265149884</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07226v1.pdf" target="_blank">Large Language Models for Robotics: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3020.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3020.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-extended agent architecture that stores natural-language records of agents' experiences and uses synthesis and retrieval of those memories to drive realistic social behavior and planning in a simulated environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (GA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An architecture built on large language models that augments the LLM with a natural-language memory store of complete experience records; agents synthesize and reflect over accumulated memories and dynamically retrieve relevant memories to plan and guide behavior in a social sandbox.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>interactive social simulation / interactive novel / text-adventure style simulated world (sandbox)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce human-like persona-consistent behaviors and social interactions (e.g., planning and executing invitations, conversations, attending events) in an open-ended simulated social environment; example scenario: organizing/attending a Valentine's Day party.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>natural-language episodic / autobiographical memory (long-term memory store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Stores chronological 'complete records' of agents' experiences as natural-language memory entries; performs synthesis/reflection over accumulated memories and dynamic retrieval of relevant memory items to inform planning and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Enables retention and recall of past experiences, higher-level reflection and planning over past events, and more coherent, persona-consistent social behaviors and interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Paper does not report quantitative failures for GA here; the survey notes general scaling and indexing challenges (memory growth, retrieval efficiency) that would apply to GA-style stores.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use explicit natural-language memory traces plus periodic synthesis/reflection and dynamic retrieval to produce more coherent, long-horizon, persona-consistent behavior; be aware of memory indexing and growth challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Robotics: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3020.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3020.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-text-games</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs for interactive novels and text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A broad class of applications where LLMs are used to create or drive interactive novels and text-adventure style games, and to plan/decompose action sequences for simulated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-driven interactive-novel / text-adventure agent (generic mention)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generic description: large language models used to generate narrative, decide agent actions, and decompose tasks in interactive/textual game environments; referenced as a common training/interaction modality for embodied agents and for research in interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>interactive novels / text-adventure games (unnamed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate interactive narrative, plan and decompose sequences of textual actions, and control in-text characters within text-adventure or narrative simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Not specified in detail in this survey; general implication is that memory mechanisms (when used) can support longer-term coherence in narrative and agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Not reported for the generic mention in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Survey only notes that games and simulators are used for training and that LLMs have been applied to interactive novels/text adventures; no detailed recommendations specific to memory in text games are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Robotics: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3020.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3020.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-memory-survey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory mechanisms in LLM-based embodied agents (survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The survey's discussion of memory for embodied LLM agents: memory is analogous to human memory (experiences, strategies), is necessary for long-horizon and re-use of prior strategies, and poses challenges for storage, indexing, and catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-based agents (memory discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Survey-level summary: LLM 'brains' for agents require memory to store experiences and enable strategy re-use; memory mechanisms are described conceptually rather than as a single implemented system.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>applies to games, simulators and embodied tasks discussed throughout the survey (no single benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>General requirement: use past experiences to inform planning and decision-making for complex/long-horizon tasks across embodied or simulated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>conceptual: episodic/long-term memory, indexed memory stores; also mentions challenges analogous to catastrophic forgetting</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>High-level issues only: storing long memory sequences, efficient storage and indexing as memories grow, retrieval mechanisms required; no concrete implementation (e.g., vector DBs, attention over memory buffer) specified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Survey states memory helps agents review past strategies and formulate better plans for new complex tasks, improving long-term task performance and persistence in dialogue/interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Survey highlights challenges: growth of memory burden, efficient storage/indexing, context length limits, catastrophic forgetting, and dialogue/context persistence limits that impair long-term tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Design memory systems that can index and retrieve relevant past experiences efficiently; manage memory growth to avoid catastrophic forgetting; incorporate mechanisms for dialogue persistence and periodic synthesis/reflection to support long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Robotics: A Survey', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Cognitive architectures for language agents <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 1)</em></li>
                <li>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3020",
    "paper_id": "paper-265149884",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "GA",
            "name_full": "Generative Agents",
            "brief_description": "An LLM-extended agent architecture that stores natural-language records of agents' experiences and uses synthesis and retrieval of those memories to drive realistic social behavior and planning in a simulated environment.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (GA)",
            "agent_description": "An architecture built on large language models that augments the LLM with a natural-language memory store of complete experience records; agents synthesize and reflect over accumulated memories and dynamically retrieve relevant memories to plan and guide behavior in a social sandbox.",
            "game_or_benchmark_name": "interactive social simulation / interactive novel / text-adventure style simulated world (sandbox)",
            "task_description": "Produce human-like persona-consistent behaviors and social interactions (e.g., planning and executing invitations, conversations, attending events) in an open-ended simulated social environment; example scenario: organizing/attending a Valentine's Day party.",
            "uses_memory": true,
            "memory_type": "natural-language episodic / autobiographical memory (long-term memory store)",
            "memory_implementation_details": "Stores chronological 'complete records' of agents' experiences as natural-language memory entries; performs synthesis/reflection over accumulated memories and dynamic retrieval of relevant memory items to inform planning and action selection.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Enables retention and recall of past experiences, higher-level reflection and planning over past events, and more coherent, persona-consistent social behaviors and interactions.",
            "memory_limitations_or_failures": "Paper does not report quantitative failures for GA here; the survey notes general scaling and indexing challenges (memory growth, retrieval efficiency) that would apply to GA-style stores.",
            "best_practices_or_recommendations": "Use explicit natural-language memory traces plus periodic synthesis/reflection and dynamic retrieval to produce more coherent, long-horizon, persona-consistent behavior; be aware of memory indexing and growth challenges.",
            "uuid": "e3020.0",
            "source_info": {
                "paper_title": "Large Language Models for Robotics: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-text-games",
            "name_full": "LLMs for interactive novels and text-adventure games",
            "brief_description": "A broad class of applications where LLMs are used to create or drive interactive novels and text-adventure style games, and to plan/decompose action sequences for simulated tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "LLM-driven interactive-novel / text-adventure agent (generic mention)",
            "agent_description": "Generic description: large language models used to generate narrative, decide agent actions, and decompose tasks in interactive/textual game environments; referenced as a common training/interaction modality for embodied agents and for research in interaction.",
            "game_or_benchmark_name": "interactive novels / text-adventure games (unnamed in survey)",
            "task_description": "Generate interactive narrative, plan and decompose sequences of textual actions, and control in-text characters within text-adventure or narrative simulation tasks.",
            "uses_memory": null,
            "memory_type": null,
            "memory_implementation_details": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "memory_benefits": "Not specified in detail in this survey; general implication is that memory mechanisms (when used) can support longer-term coherence in narrative and agent behavior.",
            "memory_limitations_or_failures": "Not reported for the generic mention in this survey.",
            "best_practices_or_recommendations": "Survey only notes that games and simulators are used for training and that LLMs have been applied to interactive novels/text adventures; no detailed recommendations specific to memory in text games are given here.",
            "uuid": "e3020.1",
            "source_info": {
                "paper_title": "Large Language Models for Robotics: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLM-memory-survey",
            "name_full": "Memory mechanisms in LLM-based embodied agents (survey summary)",
            "brief_description": "The survey's discussion of memory for embodied LLM agents: memory is analogous to human memory (experiences, strategies), is necessary for long-horizon and re-use of prior strategies, and poses challenges for storage, indexing, and catastrophic forgetting.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "LLM-based agents (memory discussion)",
            "agent_description": "Survey-level summary: LLM 'brains' for agents require memory to store experiences and enable strategy re-use; memory mechanisms are described conceptually rather than as a single implemented system.",
            "game_or_benchmark_name": "applies to games, simulators and embodied tasks discussed throughout the survey (no single benchmark)",
            "task_description": "General requirement: use past experiences to inform planning and decision-making for complex/long-horizon tasks across embodied or simulated environments.",
            "uses_memory": true,
            "memory_type": "conceptual: episodic/long-term memory, indexed memory stores; also mentions challenges analogous to catastrophic forgetting",
            "memory_implementation_details": "High-level issues only: storing long memory sequences, efficient storage and indexing as memories grow, retrieval mechanisms required; no concrete implementation (e.g., vector DBs, attention over memory buffer) specified in the survey.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Survey states memory helps agents review past strategies and formulate better plans for new complex tasks, improving long-term task performance and persistence in dialogue/interaction.",
            "memory_limitations_or_failures": "Survey highlights challenges: growth of memory burden, efficient storage/indexing, context length limits, catastrophic forgetting, and dialogue/context persistence limits that impair long-term tasks.",
            "best_practices_or_recommendations": "Design memory systems that can index and retrieve relevant past experiences efficiently; manage memory growth to avoid catastrophic forgetting; incorporate mechanisms for dialogue persistence and periodic synthesis/reflection to support long-horizon tasks.",
            "uuid": "e3020.2",
            "source_info": {
                "paper_title": "Large Language Models for Robotics: A Survey",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Cognitive architectures for language agents",
            "rating": 2,
            "sanitized_title": "cognitive_architectures_for_language_agents"
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 1,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "rating": 1,
            "sanitized_title": "lmnav_robotic_navigation_with_large_pretrained_models_of_language_vision_and_action"
        }
    ],
    "cost": 0.01315475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Robotics: A Survey
13 Nov 2023</p>
<p>Fanlong Zeng flzeng1@gmail.com 
School of Intelligent Systems Science and Engineering
Jinan University
519070ZhuhaiChina</p>
<p>Wensheng Gan 
School of Intelligent Systems Science and Engineering
Jinan University
519070ZhuhaiChina</p>
<p>Yongheng Wang yonghengwwang@gmail.com 
School of Intelligent Systems Science and Engineering
Jinan University
519070ZhuhaiChina</p>
<p>Ning Liu tliuning@jnu.edu.cn 
School of Intelligent Systems Science and Engineering
Jinan University
519070ZhuhaiChina</p>
<p>Philip S Yu psyu@uic.edu 
Department of Computer Science
University of Illinois Chicago
ChicagoUSA</p>
<p>Robotics LLM Embodied Intelligence / Agents</p>
<p>Large Language Models for Robotics: A Survey
13 Nov 2023CA5C52356644339950D616D205FAA66BarXiv:2311.07226v1[cs.RO]Preprint submitted to Elsevierlarge language models robotics control and interaction decision-making embodied intelligence
The human ability to learn, generalize, and control complex manipulation tasks through multimodality feedback suggests a unique capability, which we refer to as dexterity intelligence.Understanding and assessing this intelligence is a complex task.Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention.LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots.Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy.Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs.We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction.Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future.Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.</p>
<p>Introduction</p>
<p>Humans possess exceptional proficiency in executing intricate and dexterous manipulation skills by integrating tactile, visual, and other sensory inputs.Research in the field of robotics aspires to imbue robots with comparable manipulation intelligence.Although recent advancements in robotics and machine learning have yielded promising results in visual mitigation and exploration learning for robot manipulation, there remains much to be accomplished in this area.Large language models (LLMs), such as BERT [31], Roberta [79], GPT-3 [27], GPT-4 [110], have emerged as significant research achievements in the field of artificial intelligence (AI) in recent years.Through deep learning techniques [76], LLMs can be trained on massive text corpora, enabling them to generate high-quality natural language text.This development has sparked new thinking in natural language processing and dialogue systems.At the same time, the rapid advancement of robotics technology [66,32] has created a demand for more intelligent and natural human-machine interaction.Combining LLMs with robots can provide robots with stronger natural language understanding and generation capabilities, enabling more intelligent and human-like conversations and interactions.</p>
<p>Applying LLMs to the field of robotics has important research significance and practical value.Firstly, LLMs can significantly enhance a robot's natural language understanding and generation capabilities.Traditional robot dialogue systems often require manual rules and template writing, making it difficult to handle complex natural language inputs.LLMs, on the other hand, can better understand and generate natural language by learning from massive text corpora, enabling robots to have more intelligent and natural conversation abilities.Secondly, LLMs can provide more diverse conversation content and personalized interaction experiences.Through interaction with LLMs, robots can generate varied responses and personalize interactions based on user preferences and needs.This helps improve user satisfaction and interactions.In addition, the combination of LLMs and robots contributes to the advancement of artificial intelligence and robotics technology, laying the foundation for future intelligent robots (or called smart robots).</p>
<p>Currently, many research teams and companies have begun exploring the application of LLMs in the field of robotics.Some research focuses on using LLMs for natural language understanding in robots.By using pre-trained language models [152], robots can better understand user intentions and needs [34,117].Other research focuses on using LLMs for natural language generation in robots.Robots can generate fluent and coherent natural language responses through interaction with language models.Furthermore, some research explores how to combine LLMs with other technologies, such as knowledge graphs and sentiment analysis, to further enhance robot dialogue capabilities and user experiences.From multiple perspectives, LLMsbased robotics is one of the most promising paths to achieve embodied intelligence in the future.</p>
<p>Although the combination of LLMs and robots has many potential advantages, it also faces challenges and issues [50,86].Firstly, training and deploying LLMs require substantial computing resources and data, which can be challenging for resource-limited robot platforms [7].Secondly, LLMs may generate inaccurate, unreasonable, or even harmful content when generating natural language text.Effective filtering and control mechanisms are necessary to ensure that the content generated by robots complies with ethical and legal requirements [86].Additionally, robot dialogue systems need to address challenges such as multi-turn dialogues, context understanding, and dialogue consistency to provide more coherent and human-like interactions.Furthermore, the shape of robots has not been standardized across the industry.The question remains whether robots should adopt a humanoid form or take on a different shape [57].In other words, what form of robot is best suited for our needs?</p>
<p>The impact of embodied intelligence on our society cannot be overstated.Will robots eventually replace human labor?How should we respond to this seismic shift in the future?Moreover, if robots were to gain consciousness, should we still view them as tools?How should humans define a conscious robot?</p>
<p>In conclusion, the applications of large language models in robotics hold tremendous potential.They provide new paradigms and methods for robot control, path planning, and intelligence.Through more intuitive and natural human-machine interaction, language-based path planning, and intelligent semantic understanding, large language models not only enhance the performance and efficiency of robots but also improve the experience and interaction modes of human-robot interaction.</p>
<p>Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning.To summarize, there are four key contributions in this paper, as follows:</p>
<p>• We discussed the latest advancements in LLMs and their significant impact on the field of robotics.We highlighted the benefits of LLMs for robots, as well as the emergence of new robot models equipped with LLMs in recent years.</p>
<p>• We discussed the current state of robot technology, focusing on advancements in perception, decisionmaking, control, and interaction combined with LLMs.Specifically, we highlighted the critical role of LLMs in decision-making modules, which have enabled robots to make more informed and effective decisions in various applications.</p>
<p>Language Model Overview</p>
<p>Language Model Basics</p>
<p>We first provide an overview of LLM, starting with an introduction to some fundamental concepts.We then delve into the history of LLM's development, followed by a brief discussion of its growing popularity in recent years.A language model is a computational model that utilizes statistical methods to analyze and predict the probability of word sequences in a given language.It is designed to capture the patterns, grammar, and semantic meaning of natural language [92].</p>
<p>• N-gram models are a simple form of language models that calculate the probability of a word based on the preceding (n-1) words.They are widely used due to their simplicity and efficiency.The accuracy of the N-gram model is directly related to the length of the context used, with larger 'n' values leading to higher accuracy [13].</p>
<p>• Unigram models [147] are often employed for various language processing tasks including information retrieval.It evaluates each word or term independently.It is calculated without considering any conditional context, only the probability of the current word itself appearing.</p>
<p>• Bidirectional models differ from unidirectional models, it analyzes text in both directions: backward and forward.This dual approach is commonly employed in various machine learning models and speech generation applications.Bidirectional models harness the power of contextual information from both directions, providing a deeper understanding of the text [6].</p>
<p>• Exponential models [28] employ an equation that combines feature functions and n-grams to evaluate text.Unlike n-grams, this type of model allows for more flexibility in analyzing parameters and does not mandate the specification of individual gram sizes.Essentially, exponential models define features and parameters based on the desired outcomes, providing a more open-ended approach to text analysis.</p>
<p>• Neural language models, including recurrent neural networks (RNNs) [150] and transformers [131], have gained popularity in recent years.These models use deep learning techniques to capture complex language patterns and dependencies.</p>
<p>• Transformer architecture's development revolutionized language modeling.Transformers use self-attention mechanisms to capture relationships between words in a sentence.This is currently the most popular architecture [131].</p>
<p>Development of LLMs</p>
<p>Some well-known developments in LLMs are described below in detail.</p>
<p>• Eliza.The concept of language generation models originated in the 1960s with the development of Eliza, the world's first chatbot, by MIT researcher Joseph Weizenbaum.Eliza's creation laid the groundwork for natural language processing (NLP) research, paving the way for subsequent advancements in this field [125].</p>
<p>• LSTM.The year 1997 witnessed the emergence of Long Short-Term Memory (LSTM) networks, introducing a significant advancement in neural network architecture.The introduction of LSTM networks enabled the development of deeper and more intricate neural networks capable of effectively processing vast amounts of data.</p>
<p>• Stanford coreNLP.In 2010, Stanford's CoreNLP suite brought about a significant milestone in the field by offering developers a versatile toolkit.This suite empowers developers to conduct various natural language processing tasks.</p>
<p>• Google brain.In 2011, a scaled-down version of Google Brain surfaced, introducing groundbreaking features such as word embeddings.These advanced capabilities revolutionized natural language processing (NLP) systems by enhancing their ability to comprehend context with greater clarity.</p>
<p>• Transformer models.Transformer models [131], introduced in 2017, brought significant advancements to language modeling.They employ self-attention mechanisms to capture global dependencies and have achieved state-of-the-art performance in various natural language processing tasks.</p>
<p>• Large language model.OpenAI unveiled GPT-4 [92], a language model boasting an astounding scale of approximately one trillion parameters.This represents a five-fold increase compared to its predecessor, GPT-3 [14], and a staggering 3,000-fold increase compared to the initial release of BERT [31].The introduction of GPT-4 sets a new benchmark in the field of language models, showcasing the remarkable progress in model size and capacity.</p>
<p>Popular LLMs</p>
<p>Until now, there are many foundation models or LLMs have been developed.We present some selected models below, including GPT-3.5, GPT-4, BERT, T5, and LLaMA.</p>
<p>• GPT-3 (Generative pre-trained transformer 3) [14].</p>
<p>Developed by OpenAI, GPT-3 is one of the most prominent language models.With 175 billion parameters, it can generate coherent and contextually relevant text across a wide range of domains.</p>
<p>• GPT-4 (Generative pre-trained transformer 4) [92].Unveiled on March 14, 2023, GPT-4 represents a significant advancement in language models, prioritizing factual accuracy and enhancing reliability compared to its predecessors, GPT-3 and GPT-3.5.Notably, GPT-4 introduces multimodal capabilities, enabling it to process images as input and generate comprehensive descriptions, classifications, and analyses across different modalities.This multimodal functionality expands the model's versatility and enhances its ability to understand and generate content across various media formats.</p>
<p>• BERT (Bidirectional encoder representations from transformers) [31].Developed by Google, BERT introduced the concept of pre-training and fine-tuning for language understanding tasks.It has achieved remarkable results in tasks such as question answering and text classification.</p>
<p>• T5 (Text-to-Text transfer transformer) [101].Developed by Google, T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, including summarization, translation, and text generation.</p>
<p>• LLaMA [128].Developed by Google, LLaMA is a language model pre-trained and fine-tuned generative text model with parameter counts ranging from 7 to 70 billion.LLaMA removes the absolute position embedding and instead adds rotational position embedding at each layer of the network.</p>
<p>Benefits of LLM for Robotics</p>
<p>The advent of LLM-based robots has brought about a plethora of innovative changes to the field.Here, we explore the various benefits that LLM will bring to robots.The necessity and significance of LLMs for robotics can be summarized in the following ten points:</p>
<p>• Natural language interaction.LLMs provide robots with the ability to engage in natural language interactions, allowing users to communicate with robots in an intuitive and convenient manner.This interaction method aligns better with human habits and needs, enhancing the usability and acceptance of robots.</p>
<p>• Task execution.LLMs assist robots in performing various tasks by understanding and generating natural language instructions.Robots can navigate, manipulate objects, and execute specific actions based on user language commands [126].This opens up broader possibilities for robot applications in everyday life.</p>
<p>• Knowledge acquisition and reasoning.LLMs possess powerful information retrieval and reasoning capabilities, which can help robots acquire and process rich knowledge.Robots can interact with language models to obtain real-time and accurate information, thereby improving their decision-making ability and intelligence.</p>
<p>• Flexibility and adaptability.The flexibility of LLMs enables robots to adapt to different tasks and environments.Through interaction with language models, robots can make flexible adjustments and selfadaptation based on specific circumstances, better meeting user needs [52].</p>
<p>• Learning and improvement.LLMs enable continuous learning and improvement through interaction with users.By analyzing and understanding user feedback, robots can enhance their performance and proficiency.This learning and improvement capability allows robots to gradually adapt to user personalities and preferences, providing more personalized services.</p>
<p>• Multimodal interaction.LLMs also support multimodal interaction, enabling robots to process different forms of inputs such as speech, images, and text simultaneously.This multimodal capability [141] allows robots to comprehensively understand user needs and provide richer interaction experiences.</p>
<p>• Education and entertainment.LLMs offer potential applications for education and entertainment purposes in robotics.Robots can provide educational content, answer questions, or engage in games and entertainment activities through interaction with language models.This has significant implications for children's education, language learning, and the entertainment industry.</p>
<p>• Emotional interaction.The application of LLMs enhances the emotional interaction capabilities of robots.By generating emotionally responsive outputs, robots can establish closer and more meaningful relationships with users.This emotional interaction is valuable in fields such as care robots, emotional support, and psychotherapy.</p>
<p>• Collaboration and cooperation.LLMs enable robots to collaborate and cooperate better with humans.Robots can jointly solve problems, formulate plans, and execute tasks through interaction with language models [126].This collaboration and cooperation ability is significant for industrial automation, team collaboration, and human-robot coexistence.</p>
<p>• Innovation and exploration.The application of LLMs stimulates innovation and exploration in the field of robotics.Through interaction with language models, robots can possess higher-level intelligence and comprehension abilities, opening up new avenues for research and development in robotics.</p>
<p>Robotics Based on LLMs</p>
<p>In this subsection, we introduce the smart robotics based on LLMs in recent years.LLMs are used as brains in the part of robotics.First, we summarize the models in recent years in Table 2.</p>
<p>PaLM-SayCan</p>
<p>With the increasing popularity of LLMs, people have begun to wonder whether these models can be used to assist robots in performing various daily tasks.However, there are challenges in enabling robots to extract knowledge from LLMs and interact with the physical world.LLMs contain valuable semantic information about the real world, aiding robots in understanding natural language.Nonetheless, giving LLMs a physical form capable of interacting and making real-world decisions is challenging due to their lack of experience with physical objects and environments.PaLM-SayCan [1] can function as the physical embodiment of LLM, utilizing LLM's semantic capabilities to process natural language instructions.PaLM-SayCan enables robots to execute tasks assigned by humans through the value function.PaLM-SayCan features pre-trained meta-actions controlled by visual motors, while BC-Z [58] and MT-Opt [64] are employed to learn language-conditioned BC and RL policies, respectively.LLM can decompose received natural language instructions into smaller, manageable tasks.Based on the current status, capabilities, and surrounding environment of the robot, actions can be flexibly executed.To determine the feasibility of an action, PaLM-SayCan relies on a logarithmic estimation of the value function and PaLM-SayCan can function as the physical embodiment of LLM, utilizing LLM's semantic capabilities to process natural language instructions.Enabling robots to execute tasks assigned by humans through the value function.</p>
<p>2023</p>
<p>PaLM-E [34] PaLM-E boasts an LLM capable of integrating continuous sensory information from the real world, effectively bridging the gap between language and perception.</p>
<p>2023 LM-Nav [117] LM-Nav was developed, exploiting the advantages of language to facilitate effective communication between users and robots.The LM-Nav system comprises three components: a vision-navigation model (VNM); a vision language model (VLM); and a large language model (LLM).affordance function.It will perform the most likely action to succeed in the current environment and state.For instance, upon receiving the instruction, "Can you help me get an apple?".LLM may decompose it into several tasks: "walking to the kitchen, opening the refrigerator, obtaining the apple, and delivering it to the requester.",just like in Figure 2(a).</p>
<p>PaLM-E</p>
<p>While LLMs have demonstrated remarkable capabilities in handling complex tasks, integrating them as an interface into robots remains a significant challenge.A major limitation of LLMs is their reliance on text input, which is insufficient for robots that require physical interaction.PaLM-E [34] boasts an LLM capable of integrating continuous sensory information from the real world, effectively bridging the gap between language and perception.Its multi-modal input encompasses vision, text, and state estimation, like in Figure 2(b), as exemplified by the question "What is it in <img_1>?"The model's processing is end-to-end, whose performance is state-of-the-art in OK-VQA [84].PaLM-E is a visual-language generalist.PaLM-E treats images and text as multi-modal inputs represented by latent vectors.PaLM-E is a decoder-only model that generates text completions autonomously when provided with a prefix or hint.The output of PaLM-E is separated into two parts: when tackling text generation tasks (such as embedded question answering or scene description), the model directly produces the final output (i.e., output text or speech).In contrast, when utilized for specific planning and control tasks, PaLM-E generates low-level instruction text (e.g., instructions for controlling robot meta-actions).</p>
<p>LM-Nav</p>
<p>Goal-based robot navigation can leverage large, unlabeled datasets for training, resulting in strong generalization capabilities in real-world scenarios.However, in visionbased settings, specifying targets often requires images.Current supervised learning methods are not only expensive but also demand linguistically described and labeled trajectory data, making them impractical for widespread use.How can users communicate with robots more conveniently?To address the challenge, LM-Nav [117] was developed, exploiting the advantages of language to facilitate effective communication between users and robots.The LM-Nav system comprises three components: a vision-navigation model (VNM); a visual-language model (VLM); and a large-scale language model (LLM).Notably, LM-Nav operates without the requirement of labeled data or fine-tuning.By leveraging the VLM and VNM, LM-Nav can extract landmark names from commands and navigate to specified locations.LM-Nav leverages three pre-trained models to achieve successful navigation in pre-explored environments.First, it employs ViNG [114] as a VNM creates a topological map using observations from a prior exploration of the environment.Subsequently, GPT-3 [27] serves as the LLM [14] processes free-form text instructions to determine the target landmark.Finally, CLIP [99] serves as the VLM to locate the corresponding position in the topology map based on the identified landmark.By combining these models, LM-Nav can effectively follow natural language instructions to complete navigation tasks.</p>
<p>Expedition A1</p>
<p>Expedition A12 , developed by AGIBot, embodies the company's commitment to seamlessly integrating advanced AI into robotics and fostering harmonious collaboration between humans and machines.Envisioning a future where robots serve as indispensable assistants to humans, AGI-Bot's mission is to create intelligent and versatile robots Robotics Transformer X (RT-X) is categorized into two branches: RT-1-X and RT-2-X.RT-1-X employs the RT-1 architecture and utilizes the X-embodiment repository for training, while RT-2-X leverages the strategy architecture of RT-2 and is trained on the same dataset.Experiments demonstrate that both RT-1-X and RT-2-X have exhibited enhanced capabilities.</p>
<p>capable of unlocking limitless productivity.The company's founding ethos is centered around the belief that "intelligent robots can create unlimited productivity" when designed to parallel human flexibility and intelligence.Expedition A1 is a humanoid robot equipped with reflex knee joints, designed to resemble a human form.This design choice stems from the fact that most work environments are currently tailored for human functionality.Humanoid robots are allowed to seamlessly integrate and function without requiring significant environmental modifications.A key advantage of humanoid robots is their strong generalization capabilities, enabling them to adapt to diverse situations.While the Expedition A1 can also swap out components, such as replacing legs with tires, mimicking human movement and perception remains a significant challenge for robots.Expedition A1 integrates cutting-edge perception, control, and decision-making technologies, incorporating both a state-of-the-art language model and an independently developed visual model.Designed with industrial manufacturing in mind, it boasts 49 degrees of freedom, surpassing the limitations of traditional robots with only 20 degrees of freedom.Its high degree of freedom enables it to meet various industrial manufacturing requirements.The Expedition A1 is also modular, allowing for autonomous component replacement.For instance, PowerFlow is a joint motor for enhanced flexibility, while SkillHand features vision-based fingertip sensors for precision manufacturing scene design.</p>
<p>In addition to its robust hardware, the Expedition A1 utilizes LLM as its brain, complemented by EI-Brain's embodied intelligence framework.This framework divides the robot's system into different levels of management, including Expedition A1's super brain in the cloud, local brain, cerebellum, and brainstem, each corresponding to diverse task levels.</p>
<p>New Transformer Architecture for Robotics</p>
<p>In this part, we introduce the Transformer designed for robotics.We summarize the Transformer for robotics in recent years in Table 3.</p>
<p>Control Transformer</p>
<p>Reinforcement learning [94] methods struggle to effectively tackle long-horizon tasks like navigation, but from a different angle, sample-based path planning techniques can discover collision-free paths without the need for learning in a known environment.Control Transformer (CT) [75] utilizes a sample-based probabilistic road map (PRM) [67] planner to generate conditional sequences from lowlevel policy, enabling it to complete navigation tasks solely through local information.CT has been shown to be effective in complex terrain and unknown environments through relevant experiments.By leveraging local observations, CT can solve long-horizon and robot navigation tasks.Following training, CT can obtain a policy and complete navigation from partially observed or unknown environments.CT is a Transformer [131] framework designed to model conditional sequences generated by robot actions.It utilizes a learnable value function to assess the initial cost of reaching the target position and guides the sequence modeling and generation process of the Transformer.To facilitate learning from data collections guided by sampling, the CT problem is treated as a sequence modeling problem with a goal-oriented approach.In essence, CT processing involves auto-regressively predicting actions within a sequence.</p>
<p>Q-Transformer</p>
<p>Many proposed high-capability machine learning models rely on supervised learning, but their performance is limited by the quality of human demonstrations.Neither the full potential of the hardware nor the required experience can be obtained automatically (given the availability of unlabeled datasets).Reinforcement learning [94] can address these limitations, but training Transformer-based models using reinforcement learning has proven challenging at large dataset sizes.To integrate reinforcement learning and Transformer [131], Q-Transfomer [18] is proposed.It combines the Transformer structure with offline reinforcement learning, enabling the exploitation of Q-values for each dimension.This is achieved by utilizing a Transformer-based architecture that leverages offline reinforcement learning to extend the representation of the Q-Function [74] through offline temporal differential backup [139].The approach involves discretizing each action dimension and representing each action dimension as separate tokens using Q-values.This allows for the utilization of large and diverse robot datasets, enhancing the efficiency and effectiveness of the reinforcement learning process.</p>
<p>Robotics Transformer</p>
<p>Robotics transformer 1.By migrating large and diverse datasets, machine learning has now been targeted at downstream tasks and significantly improved performance in many areas (such as computer vision, natural language processing, or speech recognition) by fine-tuning with zeroshot or few-shot.However, the field of robotics has yet to show similar generalization capabilities.Training a general robotics model through open-ended task-agnostic training and incorporating high-performance architectures that can absorb large and diverse datasets may be a promising approach.If a model could act like a sponge, absorbing ubiquitous patterns of language and perception, it may be able to perform better on specific downstream tasks.The question remains whether it is possible to train a model in the field of robotics that can absorb knowledge from other fields.Could the model demonstrate zero-shot generalization capabilities for new tasks?Robotics Transformer 1 (RT-1) [10] was proposed to address the aforementioned question.RT-1 is capable of encoding high-dimensional input and output data, including images and instructions, into compact tokens that can be efficiently processed by Transformer [131].It exhibits real-time operation characteristics, making it suitable for applications that require rapid processing and response times.In experimental evaluations, RT-1 demonstrated strong generalization.The structure of RT-1 is composed of FiLM [96], conditioned EfficientNet [124], a TokenLearner [107], and Transformer [131].However, RT-1 is not an end-to-end model.</p>
<p>Robotics transformer 2. Can we pre-train a visionlanguage model (VLM) [22,34] that can be seamlessly integrated into low-level robot control?Hereby enhancing VLM generalization capabilities?We can achieve this by training the robot's trajectory to be represented as a sequence of tokens, effectively mapping natural language instructions into a series of robot actions.To create an end-to-end model that can directly map robot observations into actions, DeepMind employs a collaborative fine-tuning approach.Combining state-of-the-art VLMs with network-scale visual-language tasks on robot trajectory data, Robot Transformer 2 (RT-2) [9] is a model that leverages fine-tuning of a VLM.RT-2 is trained on a web-scale dataset to achieve direct possession of generalization ability and semantic awareness for new tasks.Through fine-tuning a VLM, it is adapted to generate actions based on text encoding.Specifically, the model is trained on a dataset that incorporates action-related text tokens.This type of model can be called a visual-language-action model (VLA) [9].RT-2 builds upon the policy trained by Robotic Transformer 1 (RT-1) [10], leveraging the same dataset and an expanded VLA to significantly enhance the model's generalization capabilities for new tasks.</p>
<p>Robotics transformer X.In robot learning, it is common to train a separate large model for each application or environment.However, this approach can be limiting, as it may not allow for adaptability across different robots or environments.Can we develop a robot policy that is versatile and can be applied across various robots and environments?</p>
<p>With the advancements in large models, it is within the realm of possibility to train a versatile model that exhibits strong generalization capabilities for a specific task.Inspired by these large models, X-embodiment training 3 is proposed, which involves using robot data from diverse platforms for training.This approach enables the model to better adapt to changes in both the robot and the environment, leading to improved performance and versatility.Robotics Transformer X (RT-X) [29] is categorized into two branches: RT-1-X and RT-2-X.RT-1-X employs the RT-1 architecture and utilizes the X-embodiment repository 4 for training, while RT-2-X leverages the strategy architecture of RT-2 and is trained on the same dataset.Experiments demonstrate that both RT-1-X and RT-2-X have exhibited enhanced capabilities.Similarly, robots may benefit from acquiring knowledge across various domains, much like humans.</p>
<p>Robotics Perception</p>
<p>Cognition</p>
<p>VLM</p>
<p>CLIP [99], OSRT [109], etc VLA Fine-tuning VLM into VLA in RT-2 [9], etc.</p>
<p>Navigation VLN VLN in real-environments [89], VLN in simulation [157], etc.</p>
<p>VNM ViNG [114], RECON [115], ViKiNG [116], etc.</p>
<p>Decision-making</p>
<p>Reasoning</p>
<p>CoT [136,69,138], etc.</p>
<p>Planning Process natural language instruction [117], task decomposition [145,103], etc.</p>
<p>Control</p>
<p>Language-conitioned behavior BC-Z [58], MT-Opt [64], etc.</p>
<p>Action after paring instruction HRL [55], Behavior Transformer [113], etc</p>
<p>Interaction</p>
<p>Visual scenarios Game Decision Transformer [20], etc Simulator Simulated Language-Table tasks [82], etc.</p>
<p>Physical world</p>
<p>PaLM-E [34], LM-Nav [117], etc.</p>
<p>Related Technologies</p>
<p>In this section, we introduce the related technology used in robotics.Noticing that agents, embodied AI, and robotics based on LLMs all have the same meaning in this paper.</p>
<p>Here we divided the model of robotics into four parts, which are perception, decision-making, control, and interaction.We certainly provide a more detailed introduction to the decision-making component, as it serves as the core of robotics based on LLMs.Decision-making serves as a connecting link between perception and control.We summarize the related technologies introduced below in Figure 3.</p>
<p>Perception</p>
<p>Perception is a fundamental capability of robots, akin to their input.Currently, multi-modality is a popular approach for robot perception.The models discussed below employ different treatments of perception.</p>
<p>Vision-navigation model</p>
<p>Berkeley Autonomous Driving Ground Robot (BADGR) [62] is a mobile robot navigation system that leverages endto-end learning and self-supervised non-policy data collected in real-world environments to train its algorithms without any simulation or human supervision.This innovative approach enables BADGR to navigate complex environments with ease and efficiency, paving the way for future advancements in autonomous driving technology.ViNG [114] is a goal-condition model that draws inspiration from GoalConditionedRL [37].It is capable of predicting the temporal distance between image pairs and the corresponding actions to be performed.By integrating learned policies with topological maps constructed from previously observed data, ViNG's system can effectively determine how to achieve visually indicated goals, even in the presence of variable appearance and lighting conditions.RECON [115] is a system for robot learning designed for exploring autonomously and navigating in complex and unpredictable real-world surroundings.The core of RECON leverages a latent variable model of learning distance and action, along with non-parametric topology memory, to enable efficient and effective exploration.ViKiNG [116], built upon RECON mapping, incorporates geographical hints to propose an integrated learning and planning method that utilizes auxiliary information.This method combines a local traversability model.The model evaluates the robot's present camera observation and utilizes a potential sub-goal to infer the difficulty of achieving it.With a heuristic model that examines hints in the cost graph and evaluates the suitability of these sub-goals in achieving the overall goals, the general navigation model (GNM) [118] aims to train a general goalcondition model for vision-based navigation that can broadly generalize across diverse environments and embodiments, leveraging data from multiple structurally similar robots.By developing pre-trained navigation models with such capabilities, GNM represents a significant step toward realizing this vision that envisions applications for new types of robots.</p>
<p>Vision-language model</p>
<p>In recent years, large language models and visual models have had great success in their field.However, each of them can only process input in their own corresponding fields (for example, the language model only accepts text as input, and the visual model only accepts images as input), which is relatively simple.People began to focus on the processing of multi-modal input, combining large language models and visual models.Therefore, the multi-modal model that can take both vision and natural language as input was created -the visual-language model (VLM).VLM can process images and text at the same time.In actual use, we also need to distinguish between recognizing 2D scenes (such as some Visual Transformers (ViTs) [23,33,107]) or 3D scenes (such as OSRT [109]) when processing vision.VLMs come in various types [40].There are many VLM models emerging.Contrastive Language-Image Pre-training (CLIP) [99] is a neural network that has been trained on diverse pairs of images and text.It has the capability to understand natural language instructions and predict the most pertinent text excerpts associated with a given image, all without directly optimizing for this specific task.CLIP is similar to the zeroshot function of GPT-2 and 3. CLIP is also used in LM-Nav [117] as a VLM to predict the text based on natural language.The landmarks are extracted and built into the topological map.VLM has the versatility to be employed in various downstream tasks including visual question answering (VQA) [151,155], optical character recognition (OCR) [78], and image captioning [53].Such as PaLM-E [34] treats text and images as latent vectors of multi-modal input.Frozen [129] is also processed similarly to PaLM-E.</p>
<p>Vision-and-language navigation model</p>
<p>One of the primary objectives of AI research is to develop an embodied intelligence that can effectively communicate with humans and interact with the environment.This embodied intelligence is capable of understanding human language and navigating its surroundings with ease, which has the potential to greatly benefit human society.However, achieving this goal is not without its challenges, including insufficient dataset, navigation processing strategies, processing of multi-modal inputs, and model migration from familiar environments to unfamiliar environments.Despite these obstacles, the development of embodied intelligence remains a crucial area of research in the field of AI [47].Visual-and-language navigation (VLN) is a model that leverages visual observations to directly learn navigation implications and seamlessly links images and actions across time.As an extension of visual navigation in both real environments [89] and simulated [157], VLN boasts the capability to navigate complex 3D environments.There are many datasets in VLN that can be exploited.</p>
<p>Vision-language-action model</p>
<p>Can we pre-train a model that integrates multimodal inputs and low-level robot protocols to enhance the robot's generalization and semantic reasoning abilities?DeepMind aimed to develop a straightforward end-to-end model that could seamlessly map the robot's observations into action, thereby creating Vision-Language-Action Models (VLA) [9].Prior approaches involved incorporating VLMs into robot policies or designing novel robot visual-languageaction architectures.VLA instantiated by fine-tuning is first introduced and implemented in RT-2 [9], leveraging a large VLM.DeepMind fine-tunes the large-scale VLM [22,34] and pre-trains it on a vast network-scale dataset, transforming VLM into VLA.To unify robot actions and natural language responses, DeepMind integrates actions as text tokens directly into the pre-trained dataset, forming multimodal sentences [34].Multimodal statements can respond to the command set generated by the robot through observation, outputting corresponding actions.This processing is analogous to LLM processing natural language data, where action-related tokens are decoded and converted into robot actions during interface processing.VLA can significantly enhance the generalization capabilities of robots.</p>
<p>Decision-making</p>
<p>Decision-making is a fundamental capability of robots, enabling them to make informed decisions and plan tasks based on their current state and environment.As the core of a robot, decision-making plays a crucial role in connecting the preceding and the following, analyzing input from the perception module to generate appropriate actions.</p>
<p>What brings intelligence to robotics?</p>
<p>LLM has the potential to significantly aid intelligent agents, with numerous studies successfully utilizing LLM as the brain to implement intelligent agents [10,34,117] and achieve promising results [93,100].Our ideal embodied intelligence should be an intelligent entity that can perceive the surrounding environment and produce corresponding output after interacting with humans or the environment.LLM plays a vital role in this process, serving as a central hub for analyzing multi-modal input and converting it into appropriate action output.The development of intelligent agents has progressed through various stages [142]: from symbolic agents relying on symbolic logic [43,91]; Reactive agents prioritizing environmental interaction and instantaneously responding [12,11]; Reinforcement learning-based agents trained to handle complex tasks [105] but lacking generalization [41]; Agents with transfer learning [15,158] and meta-learning [48,102] based on meta-learning and transfer learning to improve the generalization of the agent to the task.To the current LLM-based agents, where LLM is used as the brain of the agents [95,122].LLM can interpret inputs, plan output actions, and demonstrate reasoning even with the abilities of decision-making.</p>
<p>The emergence of ChatGPT [27] has sparked a surge of interest in LLMs within the scientific research community and industry in recent years.LLMs possess exceptional capabilities, often serving as the brains of agents, and have zero-shot and few-shot generalization abilities that enable them to adapt to various tasks without parameter updates.Their natural language understanding and generation capabilities are unparalleled, allowing them to gain reasoning and planning abilities [138].Additionally, LLMs can parse high-level abstract instructions to perform complex tasks without requiring step-by-step guidance 5 , and their humanlike text-generation capabilities make them highly effective communicators [46].Furthermore, LLMs can sense their environment [44], and technologies that expand their action space allow them to interact with the physical environment and complete tasks [149,156].They also possess reasoning and planning capabilities, such as logical and mathematical reasoning [134,138], task decomposition [154], and planning [143] for specific tasks.LLM-based agents have been used in various real-world scenarios [77,97] and have shown potential for multi-agent interactions and social capabilities.</p>
<p>Overall, LLMs have revolutionized the field of artificial intelligence and hold great promise for future advancements.</p>
<p>Capacity of LLM in robotics</p>
<p>LLM serves as the brain of the robot, functioning as the central component that integrates knowledge, memory, and reasoning capabilities to enable the robot to plan and execute tasks intelligently.</p>
<p>Knowledge.The knowledge of LLM for robotics can be categorized into two types: the knowledge that needs to be acquired through learning (which is the pre-trained dataset) and the knowledge that has been learned and stored in memory [142].</p>
<p>• Pre-trained data.There are various types of pretrained datasets available, and the more extensive and richer the knowledge learned, the stronger the LLM's generalization and natural language understanding capabilities will be [106].Theoretically, the more a language model learns, the more parameters it has, enabling it to learn complex knowledge in natural language and gain powerful capabilities [65].</p>
<p>Research has shown that a richer dataset for language model learning can result in correct answers to diverse questions [106].Datasets can be categorized into different types, such as basic semantic knowledge, which provides an understanding of language meaning [133]; Common sense, including everyday facts like people eating when hungry or the sun rising in the east [108]; Professional field knowledge, which can aid humans in completing tasks like programming [146] and mathematics [24].</p>
<p>• Memory.Just like human memory, embodied intelligence should be able to formulate strategies and make decisions for new tasks based on experiences (i.e., observed actions, thoughts, etc.).When faced with complex tasks, the memory mechanism can aid in reviewing past strategies to obtain more effective solutions [56,121].However, memory poses some challenges, such as the length of memory sequences and how to efficiently store and index them as the number of memories grows.As the robot's memory burden increases over time, it must be able to effectively manage and retrieve memories to avoid catastrophic forgetfulness [68].</p>
<p>Reasoning.Reasoning serves as a foundational element in human cognition, playing a crucial role in problemsolving, decision-making, and the analytical examination of information [135,136].Reasoning plays a crucial role in enabling LLMs to solve complex tasks.Reasoning capabilities allow LLMs to break down problems into smaller, manageable steps and solve them starting from the current status and known conditions.There is ongoing debate about how LLMs acquire their reasoning abilities, with some arguing that it is a result of pre-training or fine-tuning [54], while others believe that it emerges only at a certain scale [137].</p>
<p>Research has shown that Chain-of-Thought (CoT) [136] can help LLMs reveal their reasoning capabilities, and some studies suggest that inference abilities may stem from the local static structure of the training data.</p>
<p>Planning.Humans plan when faced with complex challenges.Planning can help people organize their thoughts, set goals, and decide what they should do in the current situation [45,130].In this case, they can gradually approach their goals.The core of planning is reasoning.The agent can use reasoning capabilities to deconstruct the received highlevel abstract instructions into executable subtasks and make reasonable plans for each subtask [26,112].For example, LM-Nav uses ChatGpt to process received natural language instructions [117].PaLM-E directly implements end-to-end processing, converting the received multi-modal input into multi-modal sentences for LLM processing [34].Agents may also be able to reasonably update task planning based on the current situation through multiple rounds of dialogue and self-questioning and answering in the future.Many studies have proposed methods of dividing the execution tasks into many executable small tasks during the planning process.For example, directly break down the execution task into many small tasks and execute them sequentially [103,145].CoT only processes one sub-task at a time and can adaptively complete the task, which has a certain degree of flexibility [69,138].There are also some vertical planning methods that divide tasks into tree diagrams [49,148].</p>
<p>Control</p>
<p>Here, we argue that the control module is the key component responsible for regulating robotic actions.This module plays a crucial role in ensuring that the robot's actions are executed accurately and successfully, with a focus on the hardware aspects of action execution.</p>
<p>How to learn language-conditioned behavior</p>
<p>Much of the previous work has focused on enabling robots and other agents to comprehend and execute natural language instructions [19,35,81].There are various approaches to learning linguistically conditioned behaviors, such as image-based behavioral cloning that follows the BC-Z [58] method or the MT-Opt [64] reinforcement learning method.Imitation learning techniques train protocols on demonstration datasets [58,153], while offline reinforcement learning has also been studied extensively [59,71,88].However, some works suggest that imitation learning on demonstration data performs better than offline reinforcement learning [83], and other studies demonstrate the feasibility of offline reinforcement learning in theory and practice [72,73].Many works combine RL and Transformer structures [20,60], and some works integrate imitation learning with reward conditions, such as Decision Transformer (DT) [20], namely combines imitation learning with reinforcement learning elements.However, DT does not enable the model to learn from the demonstration dataset to have better performance.Deep Skill Graphs (DSG) [5] present a novel approach to skill learning utilizing the option framework.This method leverages graphs to represent discrete aspects of the environment, enabling the model to acquire structured knowledge and learn complex skills within the given domain.CT employs goal-conditioned RL to transform the local skill-learning problem into a goal-conditioned Markov decision process (MDP) [61].</p>
<p>In the context of navigation robots, early approaches to enhancing navigation strategies with the natural language employed static machine translation [80] to discover patterns.The process involves utilizing discovery patterns to translate free-form instructions into formal languages that adhere to specific grammatical rules [19,85,127].However, these methods were limited to structured state spaces.Recent works have also developed the VLN task as a sequence prediction problem [3,87,119].Additionally, there are methods that leverage nearly 1M labeled simulation trajectory demonstration data for training [47], but applying these models in unstructured environments remains a significant challenge.Data-driven approaches for vision-based mobile robot navigation often depend on the utilization of realistic simulation techniques [70,111,144] or gathering supervised data to directly learn policies for achieving goals based on observations [38].Alternatively, self-supervised learning methods can utilize unlabeled datasets or trajectories generated automatically by onboard sensors and hindsight relabeling learning [51,63,114].</p>
<p>How to execute action after parsing nature language</p>
<p>To determine whether a skill can be executed in the current state after parsing a natural language command, a temporal-difference-based (TD) reinforcement learning approach can be employed.This method learns a value function to evaluate whether the skill is executable or not [1].The value function is derived from the corresponding affordance function of reinforcement learning [42].Additionally, LM-Nav [117] utilizes a self-supervised learning method to enhance the parsing of free-form language instructions leveraging pre-trained VLM in a large number of previous environments.To address the challenges of long-term tasks, hierarchical reinforcement learning (HRL) [55] can be employed, where higher-level policies play a role in setting objectives for lower-level protocols to execute [90,132].The process of mapping natural language and observations into robot actions can also be viewed as a sequence modeling problem [9,10,29].Transformer-based robot control, such as the Behavior Transformer [113], focuses on learning demonstrations that correspond to each task.Gato [104] suggests training a model on large datasets including robotic and non-robotic.</p>
<p>Interaction</p>
<p>Interaction serves as a fundamental module that enables robots to engage and interact with both the environment and humans.To enhance robots' ability to interact in the physical world, they are often trained extensively.While some researchers utilize artificial intelligence to interact in virtual environments, such as games or simulations, ultimately, these models must be transferred to the real world.</p>
<p>However, the accuracy of these models tends to be lower in real-world settings compared to simulated environments.</p>
<p>Game</p>
<p>Traditional game developers manually write over a dozen character behaviors (including class methods and attributes) for the implementation of a game in the Valentine's Day party's specific game environment.Almost all of these behaviors are fixed sets, making the process very cumbersome with poor scalability.In games, LLMs have been used to create interactive novels and text adventure games [17].LLMs are increasingly utilized for planning robotic tasks due to their capacity to generate and decompose sequences of actions.In GA [95], they created a computer program that can mimic the behavior of human beings, called the Generative Agents.It extends the LLM by using natural language to store complete records of the intelligentsia's experiences.Synthesizing accumulated memories and reflecting upon them at higher levels over time, the system can dynamically retrieve these memories to plan and guide its behavior.Agent characters engage in comprehensive verbal exchanges utilizing authentic human language.They possess knowledge of other intelligent entities within their vicinity, and the generative agent framework dictates whether they proceed to interact or initiate a dialogue.These intelligent agents' characters can exhibit quite realistic personal behavior and social interactions.For example, when someone tells one of the agents that they have a desire to organize and host a festive gathering to celebrate Valentine's Day, these agents will spontaneously invite others to attend, meet each other, date, and be on time for the party together.This innovative architecture empowers generative agents with the ability to retain, recall, contemplate, engage with fellow agents, and strategize amidst ever-changing circumstances.</p>
<p>Language-based human-robot interaction</p>
<p>There are GUI (Graphical User Interface) and LUI (Language User Interface) for human-robot interaction.GUI refers to a computer-operated user interface that is graphically displayed and uses an interactive device to manage the interaction with the system.Unlike GUI, LUI can directly use natural human language for human-robot interaction, and the most representative LUI product is ChatGPT.Traditionally, the task of simulating human-robot interaction using natural language has proven to be difficult due to the constraints imposed on users by rigid instructions, or the need for intricate algorithms to manage numerous probability distributions related to actions and target objects [4].However, it is not easy to translate instructions into commands that robots can understand in the real world, and traditionally, fixed collections of desired actions and directives have been used to enable robots to understand human language.However, this can significantly limit the robot's flexibility and has limited generalizability across different hardware platforms.The LAnguage Trajectory TransformEr [16] introduces a versatile language-driven framework that empowers users to customize and adapt the overall trajectories of robots.</p>
<p>The approach leverages pre-trained language models (e.g., BERT [31] and CLIP [99]) to encode the user's intention and target objects directly from unrestricted text inputs and scene images.It combines geometric features produced by a network of transformer encoders and generates the trajectory using a transformer decoder, eliminating the requirement for prior task-related or robot-specific information.</p>
<p>Considering the vagueness and ambiguity of natural language, from the point of view of human-robot interaction, robots should enhance the initiative of interaction in the future, that is to say, let the robot actively ask the user questions through the large language model.If the robot feels that the user's words are problematic and is not sure what they mean, it should ask you back what you mean or whether you mean what you say.</p>
<p>Applications of LLMs in Robotics</p>
<p>Applications of large models and robotics across various domains.Here are ten specific applications of the combination of large models and robotics, along with their explanations:</p>
<p>• Autonomous navigation and path planning.Large models provide powerful semantic understanding and reasoning capabilities for robots, assisting them in autonomous navigation and path planning in unknown environments.By combining large models with sensor data, robots can comprehend semantic information in the environment, recognize obstacles, target locations, and navigation objectives, and generate suitable pathplanning solutions [25].</p>
<p>• Speech interaction and NLP.LLMs excel in speech recognition, semantic understanding, and natural language generation.Robots can leverage large models for speech interaction, understanding and answering user queries, executing specific tasks, and providing personalized service experiences.</p>
<p>• Visual perception and object recognition.Large models possess strong capabilities in image and video analysis, aiding robots in object recognition, target detection, and scene understanding.By integrating deep learning and large models, robots can achieve efficient and accurate visual perception, which can be applied in autonomous driving, robot vision-based navigation, and industrial automation.</p>
<p>• Human-robot collaboration and social robots.Large models with natural language processing and emotion analysis help robots understand human feelings and intentions better, making interactions between humans and robots more natural and smart.Social robots can engage in conversations, comprehend emotions, and provide companionship and support, which are applied in fields like healthcare, education, and entertainment.</p>
<p>• Humanoid robots and emotional expression.Large models can help humanoid robots better understand and express emotions.Through natural language generation and emotion recognition technologies, robots can engage in emotional communication and expression with humans, providing emotional support and companionship.</p>
<p>• Industrial automation and robot control.Large models can be combined with sensor data for industrial process monitoring, anomaly detection, and predictive maintenance.By learning and analyzing largescale data, robots can achieve intelligent industrial automation and adaptive control.</p>
<p>• Healthcare and rehabilitation robots.Large models can be applied in medical and rehabilitation robots to assist in diagnosis, treatment, and patient care.</p>
<p>Robots can analyze medical images, patient data, and clinical records, aiding in disease detection, surgical planning, and personalized therapy.They can also provide physical assistance and rehabilitation exercises for mobility-impaired patients.</p>
<p>• Environmental monitoring and exploration.Large models can be combined with robot platforms for monitoring and exploration in various environments, such as oceans, forests, and disaster sites.These robots can analyze sensor data, satellite imagery, and other environmental data to monitor pollution levels, detect natural disasters, and explore uncharted territories.</p>
<p>• Agriculture and farm mechanization.Large models and robots can be applied in agriculture and farm mechanization, optimizing crop management, monitoring plant health, and automating labor-intensive tasks.Robots equipped with sensors and cameras can collect data from farmlands, and analyze soil conditions, climate changes, and crop requirements, providing farmers with decision support to enhance agricultural productivity and sustainability.</p>
<p>• Education and learning assistance.Large models and robots can provide personalized tutoring and learning support in the field of education.Robots can interact with students, and then offer personalized learning materials and guidance based on their abilities and needs [2].Leveraging the semantic understanding and knowledge reasoning capabilities of large models, robots can answer questions, explain concepts, and help students deepen their understanding of knowledge.</p>
<p>In summary, the combination of large models and robotics holds tremendous potential across various domains, including autonomous navigation, speech interaction, visual perception, human-robot collaboration, industrial automation, healthcare, environmental monitoring, agriculture, and education.It can bring convenience and innovation to human life and work.</p>
<p>Challenges</p>
<p>Datasets</p>
<p>In the realm of Web 3.0 [39], big data [123], AI-Generated Content (AIGC) [140], and machine learning, collecting datasets has always been a challenge.Currently, training LLMs require vast amounts of data to support their capabilities, particularly high-quality datasets that consume considerable resources.In the field of robotics, collecting datasets is even more difficult.While LLM like ChatGPT relies on text data for pre-training [14], VLM uses a combination of text and image data [99].Robotics, however, requires a combination of both, with the addition of multimodal data, such as text, images, and touch, to serve as the robot's sensory input.These diverse datasets need to be processed in a unified format [34], allowing the robot's brain to plan and divide tasks effectively.Unfortunately, there is a lack of ready-made, multi-modal datasets, and collecting them requires a significant time investment.Moreover, policy control is necessary, which includes the interaction between the robot and its environment, necessitating 3D data [7].The data required for robotics are diverse and scarce, with poor general applicability.For instance, a dataset used to train robot dogs cannot be applied to humanoid robots, and a dataset used for screwing in an assembly line may not be suitable for robots that assemble items.However, with the emergence of platforms similar to X-embodiment 6 , the challenges of dataset collection in robotics may be alleviated in the future.</p>
<p>Training Scemes</p>
<p>As embodied intelligence necessitates interaction with the physical environment, the model's training requires specific scenarios, e.g., distributed training [152].Current research involves training robot-related models in various environments, such as games [95], simulations [30], and realworld scenarios [8].Training in-game scenarios is straightforward, with simple operations like button-pressing.However, the knowledge gained from games may not translate well to real-world scenarios, as the information in complex scenes varies greatly, and language models cannot provide a universal solution.Simulation environments aim to closely replicate reality, with low energy consumption and cost.However, modeling real scenes in simulators can be necessary.While game and simulation environments can train models, they share a common issue: poor transferability to real scenes.For instance, a model with 90% accuracy in a game or simulation may only have 10% accuracy in a real scene.Real-scene training faces significant challenges, such as cost.In simulations, objects can be generated through code [21], but in reality, purchasing them can be expensive.Transferring models between different training scenarios is a significant challenge.</p>
<p>Shape</p>
<p>Currently, most work environments in human society are well-suited for humanoid robots.However, the question arises whether robots must be human-shaped [57].There are numerous types of robots currently in existence, each with its unique capabilities and applications, like in Figure 4(a).From an energy consumption perspective, wheels are more energy-efficient than legs.Therefore, if a humanoid robot is built, it may be inappropriate to use legs to move objects instead of a conveyor belt.Similarly, a chef robot may not need to hold a shovel and cook like a human.In many cases, designing a pipeline tailored to the specific task at hand can lead to more efficient automation than humanoid robots.While humanoid robots are often depicted in animation scenes, such as in animation like Mobile Suit Gundam or games like Armored Core, their design may not always be practical for applications.For instance, a robot designed solely for washing dishes may not need the ability to sing.Modular concepts like Expedition A1 7 , can offer optimal results for different scenarios by replacing certain components.The shape of the robot remains a topic of debate, and the decision should ultimately focus on suitability for the task at hand.</p>
<p>LLM Deployment</p>
<p>Given embodied intelligence, the question arises regarding the deployment of its brain.Current technical limitations prevent the LLM from being deployed locally on the robot.The prevailing industry practice involves employing two brains: a cloud-based super brain and a local brain, like in Figure 4(b).However, a unified consensus on this deviceside plus cloud testing deployment method has yet to be established.A feasible solution could be to create a dynamic, compact model on the local client side, capable of handling basic scenario interactions.The cloud-based super brain, on the other hand, would tackle complex and challenging problems.The LLM deployment architecture remains a pressing issue that must be addressed in the future development of agents.This deployment structure also introduces latency issues, as information exchange between the robot and the super brain requires signal transmission.In certain environments, such as those with signal loss, the robot may be left with only its local brain, potentially leading to control loss or unpredictable behavior.</p>
<p>Security</p>
<p>LLM like ChatGPT may harbor biases or misconceptions stemming from their pre-training data.These biases can manifest in problematic guidance for users, and robots that rely on LLM as their brains may also exhibit biases [142].Since robots' outputs are typically physical actions, biased or misunderstood guidance can lead to harmful consequences for users [36,98], such as a chef robot burning down a house while cooking.Beyond physical safety risks, robots also raise concerns about data security [86].For instance, a robot butler who resides in a home may become intimately familiar with the household's environment and occasionally require cloud interaction for certain tasks.During user interaction, there is a risk of private data leakage, which could be mitigated by an offline environment, but this may compromise the robot's performance.</p>
<p>Dialogue Consistency</p>
<p>Humans often don't complete tasks in a single, static step.Instead, they iteratively adjust strategies and goals based on feedback received after taking action.The same is true for embodied intelligence.When faced with highlevel, abstract, or ambiguous commands, robots may not be able to decompose them into executable small tasks at first.They need to obtain further feedback from the environment and humans through continuous dialogue to update their goals.Without this ability to engage in continuous dialogue, which enables robots to perform tasks dynamically, their performance will be significantly impaired [120].Moreover, the maximum length limit of a robot's context is another issue worth considering.Typically, embodied intelligence may play a housekeeper role, handling daily tasks like washing dishes or drying clothes.However, for long-term tasks like scientific research, robots require more context-understanding capabilities.Currently, there's a limit to the length of context that robots can handle, and this limitation can lead to catastrophic forgetting [68].Dialogue persistence is a crucial challenge for long-term tasks.</p>
<p>Social Influence</p>
<p>The rapid advancement of LLMs is bringing the era of embodied intelligence, as depicted in science fiction movies and games, closer to reality.This technological breakthrough will undoubtedly revolutionize human society and unleash unprecedented productivity.With robots capable of performing repetitive tasks, the need for human labor in various industries will diminish.However, this shift may also have farreaching consequences, potentially disrupting social structures and stability [50].As robots replace low-end manual labor, it raises questions about the fate of those who previously held these jobs.The double-edged sword of embodied intelligence presents both liberation and disruption.While automation may usher in unprecedented efficiency, it also poses challenges for societal adaptation.Some works of science fiction, such as Detroit Become Human, depict a future where robots gain consciousness and conflict with humans, leading to a war between the two.Alternatively, technology may fall into the wrong hands, becoming a tool for exploitation and solidifying class divisions.However, in a worst-case scenario, robots may become a replacement for humans.As we embrace the development of embodied intelligence, we must also confront the ethical and societal implications it entails.</p>
<p>Ethic</p>
<p>Embodied intelligence has long been regarded as a mere tool, but it may hold more significance in the eyes of some users.For instance, companion robots can bring solace to lonely individuals, much like a loyal companion.In fact, some people even develop emotional attachments to their first car or a vehicle that has been with them for a long time.If we were to create robots that resemble humans or exhibit human-like intelligence, would they evoke different emotions?In science fiction movies, robots that gain selfawareness and break free from their programming often develop emotions and even marry humans.Interestingly, robots powered by LLMs have already demonstrated a degree of intelligence.Will they eventually become conscious?If embodied intelligence evolves to possess consciousness, should we still consider them tools?This raises questions about the definition of conscious robots and whether they can be considered human.Although this challenge is still far off in the future of smart robot development, it is an intriguing topic to ponder.</p>
<p>Promising Directions for Future Work</p>
<p>Security of Task Executing</p>
<p>Security has always been a pressing concern in various models, particularly with regard to user privacy.However, we argue that the safety of agents during task execution is of paramount importance.In this article, we explore the question of whether an agent's actions during task execution could cause harm [98,36].For instance, consider a scenario where a robot is asked to make lunch, but in the process, it sets the kitchen on fire.In other scenes, imagine a robot tasked with killing fish, but it mistakenly identifies humans as fish and proceeds to chase and harm them.These scenarios highlight the need to limit the actions an agent can perform to prevent potential harm.Current robot systems focus on enabling the robot to determine which actions can be performed based on the current state and environment, without fully considering the consequences of executing those actions.Therefore, we propose that ensuring the safety of task execution must be a top priority, by guaranteeing that the robot's actions do not harm human rights and interests.</p>
<p>Training Scenario Transfer</p>
<p>Due to technical or economic constraints, it is common to train robot action policies in simulated [30] or gaming environments [95].However, the ultimate goal of agent training is to apply it in real-world scenarios.Unfortunately, training in diverse scenarios can lead to not being acclimatized, which may compromise the agent's performance when deployed in real-world situations.The fundamental source of this problem can be attributed to the disparity of feedback mechanisms between simulated and real-world environments.In games or simulations, feedback is often more straightforward, with the robot receiving clear and concise information about the outcome of its actions.In contrast, real-world feedback is more complex and nuanced, making it challenging to assess the feasibility of a task in a limited scenario.Therefore, a valuable research direction is to explore methods for transferring model training across different scenarios while maintaining their accuracy in the original training environments.</p>
<p>Unify Format of Modal</p>
<p>Currently, many models are utilizing LLM as the robot's brain, and text-type data is typically the input that LLM accepts.However, for agents reliant on multi-modal perception, efficiently handling diverse input formats poses a significant challenge.To address this issue, a VLA model has been proposed [9], which uniformly converts visual and natural language multi-modal inputs into multi-modal sentences for processing, and outputs actions in the same format.In other words, multi-modal statements are employed to harmonize input and output.Nevertheless, there is currently no unified processing for other modalities such as touch and smell.It is anticipated that unified multi-modal models like VLA will gain popularity in the future.</p>
<p>Modular Components</p>
<p>As previously discussed, the field of robotics currently lacks a unified approach to robot design, with varying opinions on the matter.We believe that there should be a modular design method, wherein each part of the robot can be swapped out like a machine, just like in Figure 4(c), allowing for greater versatility and adaptability 8 .To achieve this, we must first establish unified specifications for the various modules of the robot.For instance, a robot can be composed of a head, torso, upper limbs, and lower limbs, with the upper limbs and lower limbs being interchangeable based on the task at hand.Among them, the upper limbs and lower limbs can be replaced according to specific tasks.When we need to cook, we can use our upper limbs as a shovel, and when we need to deal with weeds in the yard, we can use our lower limbs as a weeder.</p>
<p>Autonomous Perception</p>
<p>Our current research focuses on developing robots that can interact with humans using natural language instructions.In many cases, we study how humans issue instructions and how robots can decompose abstract tasks into specific sub-tasks for execution [1].However, we also hope that robots can perceive and respond autonomously to handle our current needs.For instance, if our cup falls to the ground and breaks, an agent should be able to perceive the situation through hearing and vision, and then autonomously handle the glass fragments for us.Autonomous perception requires the robot to have common sense, which is a capability that can be integrated into robots based on LLM as the brain.Research on robots' autonomous perception capabilities is crucial for improving our quality of life in the future.</p>
<p>Conclusions</p>
<p>In this survey, we summarized the methods and technologies currently used for large models in robots.First, we reviewed some basic concepts of large language models and common large models.We explain what improvements will be brought to robots by using large models as brains.We also introduce the representative LLM-based robot models proposed in recent years, such as LM-Nav [117], PaLM-SayCan [1], PaLM-E [34], etc. Next, we divide the robot into four modules: perception, decision-making, control, and interaction.For each module, we discuss the relevant technologies and their functions, including the perception module's ability to process the robot's input from the surroundings; the decision-making module's capacity to understand human instructions and plan; the control module's role in processing output actions; the interaction module's ability to interact with the environment.We also explore the potential application scenarios of current robots based on LLMs and discuss the challenges, such as training, safety, shape, deployment, and long-term task performance.Finally, we consider the social and ethical implications of post-intelligent robots and their potential impact on human society.</p>
<p>As LLMs continue to evolve, robots may become increasingly intelligent and capable of processing instructions and tasks more efficiently.With advancements in hardware, robots may eventually become reliable assistants for humans, as depicted in science fiction movies.However, we must also be mindful of their potential impact on society and address any concerns proactively.Embodied intelligence is a new paradigm for the development of intelligent science and is of great significance in leading the development of the future.LLM-based robotics represent a potential path to embodied intelligence.We hope this survey can provide some inspiration to the community and facilitate research in related fields.</p>
<p>Figure 1 :
1
Figure 1: Robotics based on LLM.</p>
<p>2023 Expedition A1 1
1
Expedition A1, developed by AGIBot, embodies the company's commitment to seamlessly integrating advanced AI into robotics and fostering harmonious collaboration between humans and machines.the requester Can you help me get an apple?(a) Task decomposition.EmbodyAI / Agents How many object in ?There three objects in (b) Multi-modal sentences in PaLM-E.</p>
<p>Figure 2 :
2
Figure 2: Task decomposition and multi-modal sentences in PaLM-E.</p>
<p>Figure 3 :
3
Figure 3: Components of robotics in this survey.</p>
<p>Figure 4 :
4
Figure 4: Challenge in embodied intelligence.</p>
<p>Table 2
2
LLMs for robot in recent years
YearLLM-based roboticsDescription2022PaLM-SayCan [1]</p>
<p>Table 3
3
Transformer architecture in robotics CT) utilizes a sample-based probabilistic road map (PRM) planner to generate conditional sequences from low-level policy, enabling it to complete navigation tasks solely through local information.RT-1 is capable of encoding high-dimensional input and output data, including images and instructions, into compact tokens that can be efficiently processed by Transformer.It exhibits real-time operation characteristics, making it suitable for applications that require rapid processing and response times.
YearTransformer architectureDescription2022 Control Transformer (2022 Control Transformer [75] Robotics Transformer 1 [10]2023Q-Transformer [18]Q-Transfomer is proposed to combine the Transformer structure with offline reinforcement learning, enabling the exploitation of Q-values for each dimension.2023Robotics Transformer 2 [9]Robot Transformer 2 (RT-2) is a model that leverages fine-tuning of a VLM. RT-2 training on a web-scale dataset to achieve direct possession of generalization ability and semantic awareness for new tasks.2023Robotics Transformer X [29]
https://www.agibot.com
X-embodiment training means training robot policy with Open Xembodiment repository's datasets
4 Open X-embodiment repository, a dataset consisting of different platforms. https://robotics-transformer-x.github.io
BabyAGI https://github.com/yoheinakajima/babyagi
Open X-embodiment repository, a dataset consisting of different platforms. https://robotics-transformer-x.github.io/
https://www.agibot.com
https://www.agibot.com
AcknowledgmentThis research was supported in part by the National Natural Science Foundation of China (Nos.62002136 and 62272196), the Natural Science Foundation of Guangdong Province (No. 2022A1515011861), Engineering Research Center of Trustworthy AI, Ministry of Education (Jinan University), and Guangdong Key Laboratory of Data Security and Privacy Preserving.
Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, The Conference on Robot Learning. 2022</p>
<p>Social robots in education for long-term humanrobot interaction: socially supportive behaviour of robotic tutor for creating robo-tangible learning environment in a guided discovery learning interaction. A Alam, ECS Transactions. 107123892022</p>
<p>Visionand-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, Van Den, A Hengel, IEEE Conference on Computer Vision and Pattern Recognition. 2018</p>
<p>Multimodal estimation and communication of latent semantic knowledge for robust execution of robot instructions. J Arkin, D Park, S Roy, M R Walter, N Roy, T M Howard, R Paul, The International Journal of Robotics Research. 392020</p>
<p>Skill discovery for exploration and planning using deep skill graphs. A Bagaria, J K Senthil, G Konidaris, International Conference on Machine Learning, PMLR. 2021</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, the International Conference on Learning Representations. 2015</p>
<p>Overview of embodied artificial intelligence. L Bermudez, </p>
<p>RoboAgent:: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. H Bharadhwaj, J Vakil, M Sharma, A Gupta, S Tulsiani, V Kumar, arXiv:2309.019182023arXiv preprint</p>
<p>RT-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023aarXiv preprint</p>
<p>RT-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, Robotics: Science and Systems XIX. 2023b</p>
<p>A robust layered control system for a mobile robot. R Brooks, IEEE Journal on Robotics and Automation. 21986</p>
<p>Intelligence without representation. R A Brooks, Artificial Intelligence. 471991</p>
<p>Class-based n-gram models of natural language. P F Brown, V J Della Pietra, P V Desouza, J C Lai, R L Mercer, Computational linguistics. 181992</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 332020</p>
<p>Policy transfer using reward shaping. T Brys, A Harutyunyan, M E Taylor, A Nowé, The International Conference on Autonomous Agents and Multiagent Systems. 2015</p>
<p>LATTE: Language trajectory transformer. A Bucker, L Figueredo, S Haddadin, A Kapoor, S Ma, S Vemprala, R Bonatti, 2023IEEE</p>
<p>Alice and kev: The story of being homeless in the sims 3. R Burkinshaw, 2009. February 19. 2010</p>
<p>Y Chebotar, Q Vuong, A Irpan, K Hausman, F Xia, Y Lu, A Kumar, T Yu, A Herzog, K Pertsch, arXiv:2309.10150Q-Transformer: Scalable offline reinforcement learning via autoregressive q-functions. 2023arXiv preprint</p>
<p>Learning to interpret natural language navigation instructions from observations. D Chen, R Mooney, The AAAI Conference on Artificial Intelligence. 2011</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Advances in Neural Information Processing Systems. 342021</p>
<p>InterAct: Exploring the potentials of chatgpt as a cooperative agent. P L Chen, C S Chang, arXiv:2308.015522023arXiv preprint</p>
<p>X Chen, J Djolonga, P Padlewski, B Mustafa, S Changpinyo, J Wu, C R Ruiz, S Goodman, X Wang, Y Tay, arXiv:2305.18565PaLI-X: On scaling up a multilingual vision and language model. 2023aarXiv preprint</p>
<p>PaLI: A jointly-scaled multilingual language-image model. X Chen, X Wang, S Changpinyo, A Piergiovanni, P Padlewski, D Salz, S Goodman, A Grycner, B Mustafa, L Beyer, 2023bInternational Conference on Learning Representations</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Semantic information for robot navigation: A survey. J Crespo, J C Castillo, O M Mozos, R Barber, Applied Sciences. 104972020</p>
<p>Automated agent decomposition for classical planning. M Crosby, M Rovatsos, R Petrick, The International Conference on Automated Planning and Scheduling. 2013</p>
<p>GPT-3: What's it good for?. R Dale, Natural Language Engineering. 272021</p>
<p>A C Davison, N Reid, arXiv:2106.10496The tangent exponential model. 2021arXiv preprint</p>
<p>Deepmind, Open X-Embodiment: Robotic learning datasets and RT-X models. </p>
<p>Learning modular neural network policies for multi-task and multirobot transfer. C Devin, A Gupta, T Darrell, P Abbeel, S Levine, 2017IEEE</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, The Conference of the North American Chapter. the Association for Computational Linguistics2018</p>
<p>Swarm robotics: Past, present, and future. M Dorigo, G Theraulaz, V Trianni, Proceedings of the IEEE. the IEEE2021109point of view</p>
<p>A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations. 2021</p>
<p>PaLM-E: An embodied multimodal language model. D Driess, F Xia, M S E Sajjadi, International Conference on Machine Learning. 2023</p>
<p>Inferring maps and behaviors from natural language instructions. F Duvallet, M R Walter, T Howard, S Hemachandra, J Oh, S Teller, N Roy, A Stentz, The 14th International Symposium on Experimental Robotics. Springer2016</p>
<p>A mathematical framework for transformer circuits. N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, Transformer Circuits Thread. 12021</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. 201932</p>
<p>Long-range indoor navigation with PRM-RL. A Francis, A Faust, H T L Chiang, J Hsu, J C Kew, M Fiser, T W E Lee, IEEE Transactions on Robotics. 362020</p>
<p>Web 3.0: The future of internet. W Gan, Z Ye, S Wan, P S Yu, Companion Proceedings of the ACM Web Conference. 2023</p>
<p>Visionlanguage pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision. Z Gan, L Li, C Li, L Wang, Z Liu, J Gao, 202214</p>
<p>Why generalization in RL is difficult: Epistemic pomdps and implicit partial observability. D Ghosh, J Rahme, A Kumar, A Zhang, R P Adams, S Levine, Advances in Neural Information Processing Systems. 342021</p>
<p>The theory of affordances. J J Gibson, 19771Hilldale, USA</p>
<p>Essentials of artificial intelligence. M Ginsberg, 2012Newnes</p>
<p>Formalizing properties of agents. R Goodwin, Journal of Logic and Computation. 51995</p>
<p>Planning and the brain, in: The cognitive psychology of planning. J Grafman, L Spector, M J Rattermann, 2004Psychology Press</p>
<p>Auto-GPT: An Autonomous GPT-4 Experiment. S Gravitas, 2023</p>
<p>Visionand-language navigation: A survey of tasks, methods, and future directions. J Gu, E Stefani, Q Wu, J Thomason, X E Wang, Annual Meeting of the Association for Computational Linguistics. 2022</p>
<p>Meta-reinforcement learning of structured exploration strategies. A Gupta, R Mendonca, Y Liu, P Abbeel, S Levine, Advances in neural information processing systems. 201831</p>
<p>S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Chatgpt and the AI Act. N Helberger, N Diakopoulos, Internet Policy Review. 122023</p>
<p>Deep visual mpc-policy learning for navigation. N Hirose, F Xia, R Martín-Martín, A Sadeghian, S Savarese, IEEE Robotics and Automation Letters. 42019</p>
<p>Towards adaptive multi-robot systems: Self-organization and self-adaptation. C E Hrabia, M Lützenberger, S Albayrak, The Knowledge Engineering Review. 33e162018</p>
<p>Scaling up vision-language pre-training for image captioning. X Hu, Z Gan, J Wang, Z Yang, Z Liu, Y Lu, L Wang, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</p>
<p>Towards reasoning in large language models: A survey. J Huang, K C C Chang, 2023The Association for Computational Linguistics</p>
<p>Hierarchical reinforcement learning: A survey and open research challenges. M Hutsebaut-Buysse, K Mets, S Latré, Machine Learning and Knowledge Extraction. 42022</p>
<p>A theory of universal artificial intelligence based on algorithmic complexity. M Hutter, cs/00040012000arXiv preprint</p>
<p>The effects of overall robot shape on the emotions invoked in users and the perceived personalities of robot. J Hwang, T Park, W Hwang, Applied Ergonomics. 442013</p>
<p>BC-Z: Zero-shot Task Generalization with Robotic Imitation Learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, The Conference on Robot Learning, PMLR. 2022</p>
<p>GPT-critic: Offline reinforcement learning for end-to-end task-oriented dialogue systems. Y Jang, J Lee, K E Kim, International Conference on Learning Representations. 2021</p>
<p>Offline reinforcement learning as one big sequence modeling problem. M Janner, Q Li, S Levine, Advances in Neural Information Processing Systems. 202134</p>
<p>Learning to achieve goals. L P Kaelbling, The International Joint Conference on Artificial Intelligence, Citeseer. 1993</p>
<p>BADGR: An autonomous self-supervised learning-based navigation system. G Kahn, P Abbeel, S Levine, IEEE Robotics and Automation Letters. 62021</p>
<p>Selfsupervised deep reinforcement learning with generalized computation graphs for robot navigation. G Kahn, A Villaflor, B Ding, P Abbeel, S Levine, 2018IEEE</p>
<p>D Kalashnikov, J Varley, Y Chebotar, B Swanson, R Jonschkowski, C Finn, S Levine, K Hausman, arXiv:2104.08212Mt-Opt: Continuous multi-task robotic reinforcement learning at scale. 2021arXiv preprint</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Deep learning in robotics: Survey on model structures and training strategies. A I Károly, P Galambos, J Kuti, I J Rudas, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 512020</p>
<p>Probabilistic roadmaps for path planning in high-dimensional configuration spaces. L E Kavraki, P Svestka, J C Latombe, M H Overmars, IEEE Transactions on Robotics and Automation. 121996</p>
<p>Measuring catastrophic forgetting in neural networks. R Kemker, M Mcclure, A Abitino, T Hayes, C Kanan, The AAAI Conference on Artificial Intelligence. 2018</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual AI. 2017arXiv preprint</p>
<p>Offline reinforcement learning with implicit Q-learning. I Kostrikov, A Nair, S Levine, International Conference on Learning Representations. 2021</p>
<p>A Kumar, J Hong, A Singh, S Levine, arXiv:2204.05618When should we prefer offline reinforcement learning over behavioral cloning? arXiv preprint. 2022a</p>
<p>A Kumar, A Singh, F Ebert, Y Yang, C Finn, S Levine, arXiv:2210.05178Pre-training for robots: Offline RL enables learning new tasks from a handful of trials. 2022barXiv preprint</p>
<p>Conservative Q-learning for offline reinforcement learning. A Kumar, A Zhou, G Tucker, S Levine, Advances in Neural Information Processing Systems. 332020</p>
<p>Control Transformer: Robot navigation in unknown environments through prm-guided returnconditioned sequence modeling. D Lawson, A H Qureshi, arXiv:2211.064072022arXiv preprint</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5212015</p>
<p>CAMEL: Communicative agents for" mind" exploration of large language model society. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, Thirty-seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>TrOCR: Transformer-based optical character recognition with pre-trained models. M Li, T Lv, J Chen, L Cui, Y Lu, D Florencio, C Zhang, Z Li, F Wei, The AAAI Conference on Artificial Intelligence. 2023b</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692RoBERTa: A robustly optimized BERT pretraining approach. 2019arXiv preprint</p>
<p>Statistical machine translation. A Lopez, ACM Computing Surveys. 402008</p>
<p>A survey of reinforcement learning informed by natural language. J Luketina, N Nardelli, G Farquhar, J Foerster, J Andreas, E Grefenstette, S Whiteson, T Rocktäschel, The International Joint Conference on Artificial Intelligence. 2019</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, P Sermanet, Robotics: Science and Systems XVII, Virtual Event. 2021</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. A Mandlekar, D Xu, J Wong, S Nasiriany, C Wang, R Kulkarni, L Fei-Fei, S Savarese, Y Zhu, R Martín-Martín, Proceedings of Machine Learning Research. Machine Learning Research2021</p>
<p>OK-VQA: A visual question answering benchmark requiring external knowledge. K Marino, M Rastegari, A Farhadi, R Mottaghi, The IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019</p>
<p>Learning to parse natural language commands to a robot control system. C Matuszek, E Herbst, L Zettlemoyer, D Fox, International Symposium on Experimental Robotics. Springer2013</p>
<p>Chatgpt banned in italy over privacy concerns. S Mccallum, 2023BBC News</p>
<p>Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. H Mei, M Bansal, M Walter, AAAI Conference on Artificial Intelligence. 2016</p>
<p>Offline pre-trained multi-agent decision transformer: One big sequence model tackles all SMAC tasks. L Meng, M Wen, Y Yang, C Le, X Li, W Zhang, Y Wen, H Zhang, J Wang, B Xu, 2023Machine Intelligence Research</p>
<p>Learning to navigate in cities without a map. P Mirowski, M Grimes, M Malinowski, K M Hermann, K Anderson, D Teplyashin, K Simonyan, A Zisserman, R Hadsell, Advances in Neural Information Processing Systems. 312018</p>
<p>Data-efficient hierarchical reinforcement learning. O Nachum, S S Gu, H Lee, S Levine, Advances in Neural Information Processing Systems. 201831</p>
<p>Computer science as empirical inquiry: Symbols and search. A Newell, H A Simon, 2007ACM Turing Award Lectures1975</p>
<p>OpenAI, 2023. GPT-4 technical report. CoRR</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Stabilizing Transformers for reinforcement learning. E Parisotto, F Song, J Rae, R Pascanu, C Gulcehre, S Jayakumar, M Jaderberg, R L Kaufman, A Clark, S Noury, The International Conference on Machine Learning, PMLR. 2020</p>
<p>J S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>FiLM: Visual reasoning with a general conditioning layer. E Perez, F Strub, H De Vries, V Dumoulin, A Courville, AAAI Conference on Artificial Intelligence. 2018</p>
<p>C Qian, X Cong, C Yang, W Chen, Y Su, J Xu, Z Liu, M Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, arXiv:2304.08354Tool learning with foundation models. 2023arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, The International Conference on Machine Learning, PMLR. 2021</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 212020</p>
<p>Efficient off-policy meta-reinforcement learning via probabilistic context variables. K Rakelly, A Zhou, C Finn, S Levine, D Quillen, The International Conference on Machine Learning, PMLR. 2019</p>
<p>Planning with large language models via corrective reprompting. S S Raman, V Cohen, E Rosen, I Idrees, D Paulius, S Tellex, arXiv:2211.099352022arXiv preprint</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, Transactions on Machine Learning Research. 2022</p>
<p>Reinforcement learning agents. C Ribeiro, Artificial Intelligence Review. 172002</p>
<p>How much knowledge can you pack into the parameters of a language model?. A Roberts, C Raffel, N Shazeer, The Conference on Empirical Methods in Natural Language Processing. 2020</p>
<p>TokenLearner: Adaptive space-time tokenization for videos. M Ryoo, A Piergiovanni, A Arnab, M Dehghani, A Angelova, Advances in Neural Information Processing Systems. 342021</p>
<p>Relational world knowledge representation in contextual language models: A review. T Safavi, D Koutra, 2021Association for Computational Linguistics</p>
<p>Object scene representation transformer. M S Sajjadi, D Duckworth, A Mahendran, S Van Steenkiste, F Pavetic, M Lucic, L J Guibas, K Greff, T Kipf, Advances in Neural Information Processing Systems. 352022</p>
<p>GPT-4 is here: what scientists think. K Sanderson, Nature. 6157732023</p>
<p>Habitat: A platform for embodied AI research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, IEEE/CVF International Conference on Computer Vision. 2019</p>
<p>Decomposition of planning problems. L Sebastia, E Onaindia, E Marzal, AI Communications. 192006</p>
<p>Behavior transformers: Cloning 𝑘 modes with one stone. N M Shafiullah, Z Cui, A A Altanzaya, L Pinto, Advances in Neural Information Processing Systems. 352022</p>
<p>ViNG: Learning open-world navigation with visual goals. D Shah, B Eysenbach, G Kahn, N Rhinehart, S Levine, The IEEE International Conference on Robotics and Automation. IEEE2021a</p>
<p>Rapid exploration for open-world navigation with latent goal models. D Shah, B Eysenbach, N Rhinehart, S Levine, The Conference on Robot Learning. 2021b</p>
<p>ViKiNG: Vision-based kilometer-scale navigation with geographic hints. D Shah, S Levine, Robotics: Science and Systems XVIII. 2022</p>
<p>LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osiński, S Levine, The Conference on Robot Learning, PMLR. 2023a</p>
<p>GNM: A general navigation model to drive any robot. D Shah, A Sridhar, A Bhorkar, N Hirose, S Levine, 2023bIEEE</p>
<p>Learning to follow navigational route instructions. N Shimizu, A Haas, International Joint Conference on Artificial Intelligence. 2009</p>
<p>Generating persona consistent dialogues by exploiting natural language inference. H Song, W N Zhang, J Hu, T Liu, The AAAI Conference on Artificial Intelligence. 2020</p>
<p>Mechanisms of memory. L R Squire, Science. 2321986</p>
<p>T Sumers, S Yao, K Narasimhan, T L Griffiths, arXiv:2309.02427Cognitive architectures for language agents. 2023arXiv preprint</p>
<p>Big data meets metaverse: A survey. J Sun, W Gan, Z Chen, J Li, P S Yu, arXiv:2210.162822022arXiv preprint</p>
<p>EfficientNet: Rethinking model scaling for convolutional neural networks. M Tan, Q Le, International Conference on Machine Learning, PMLR. 2019</p>
<p>T Team, The history, timeline, and future of LLMs. </p>
<p>Robots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Annual Review of Control, Robotics, and Autonomous Systems. 32020</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, AAAI Conference on Artificial Intelligence. 2011</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Multimodal few-shot learning with frozen language models. M Tsimpoukelli, J L Menick, S Cabi, S Eslami, O Vinyals, F Hill, Advances in Neural Information Processing Systems. 342021</p>
<p>Planning and problem solving: from neuropsychology to functional neuroimaging. J M Unterrainer, A M Owen, Journal of Physiology-Paris. 992006</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Feudal networks for hierarchical reinforcement learning. A S Vezhnevets, S Osindero, T Schaul, N Heess, M Jaderberg, D Silver, K Kavukcuoglu, International Conference on Machine Learning, PMLR. 2017</p>
<p>Probing pretrained language models for lexical semantics. I Vulić, E M Ponti, R Litschko, G Glavaš, A Korhonen, The Conference on Empirical Methods in Natural Language Processing. 2020</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Reasoning about a rule. P C Wason, Quarterly journal of experimental psychology. 201968</p>
<p>Psychology of reasoning: Structure and content. P C Wason, P N Johnson-Laird, 1972Harvard University Press86</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 352022</p>
<p>Monte carlo augmented actor-critic for sparse reward deep reinforcement learning from suboptimal demonstrations. A Wilcox, A Balakrishna, J Dedieu, W Benslimane, D Brown, K Goldberg, Advances in Neural Information Processing Systems. 352022</p>
<p>Ai-generated content (AIGC): A survey. J Wu, W Gan, Z Chen, S Wan, H Lin, arXiv:2304.066322023aarXiv preprint</p>
<p>Multimodal large language models: A survey. J Wu, W Gan, Z Chen, S Wan, P S Yu, 2023bIEEE</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023aarXiv preprint</p>
<p>Z Xi, S Jin, Y Zhou, R Zheng, S Gao, T Gui, Q Zhang, X Huang, arXiv:2305.14497Self-Polish: Enhance reasoning in large language models via problem refinement. 2023barXiv preprint</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, IEEE Conference on Computer Vision and Pattern Recognition. 2018</p>
<p>ReWOO: Decoupling reasoning from observations for efficient augmented language models. B Xu, Z Peng, B Lei, S Mukherjee, Y Liu, D Xu, arXiv:2305.183232023arXiv preprint</p>
<p>A systematic evaluation of large language models of code. F F Xu, U Alon, G Neubig, V J Hellendoorn, The ACM SIGPLAN International Symposium on Machine Programming. 2022</p>
<p>A survey on context learning. G Xun, X Jia, V Gopalakrishnan, A Zhang, IEEE Transactions on Knowledge and Data Engineering. 292016</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>W Zaremba, I Sutskever, O Vinyals, arXiv:1409.2329Recurrent neural network regularization. 2014arXiv preprint</p>
<p>MERLOT: Multimodal neural script knowledge models. R Zellers, X Lu, J Hessel, Y Yu, J S Park, J Cao, A Farhadi, Y Choi, Advances in Neural Information Processing Systems. 342021</p>
<p>Distributed training of large language models. F Zeng, W Gan, Y Wang, P S Yu, The 29th IEEE International Conference on Parallel and Distributed Systems. IEEE2023</p>
<p>Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. T Zhang, Z Mccarthy, O Jow, D Lee, X Chen, K Goldberg, P Abbeel, 2018</p>
<p>Leastto-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, The International Conference on Learning Representations. 2023</p>
<p>Unified vision-language pre-training for image captioning and vqa. L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao, AAAI Conference on Artificial Intelligence. 2020</p>
<p>MiniGPT-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023aarXiv preprint</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, 2017The IEEE</p>
<p>Transfer learning in deep reinforcement learning: A survey. Z Zhu, K Lin, A K Jain, J Zhou, IEEE Transactions on Pattern Analysis and Machine Intelligence. 452023b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5431 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5431</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5431</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-c85c90ef9e9a71efe031c3f7d6e34561f91168fe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c85c90ef9e9a71efe031c3f7d6e34561f91168fe" target="_blank">Deliberate then Generate: Enhanced Prompting Framework for Text Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors, and shows that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable success across a wide range of natural language generation tasks, where proper prompt designs make great impacts. While existing prompting methods are normally restricted to providing correct information, in this paper, we encourage the model to deliberate by proposing a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors. DTG is a simple yet effective technique that can be applied to various text generation tasks with minimal modifications. We conduct extensive experiments on 20+ datasets across 7 text generation tasks, including summarization, translation, dialogue, and more. We show that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks. We also provide in-depth analyses to reveal the underlying mechanisms of DTG, which may inspire future research on prompting for LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5431.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5431.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DTG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deliberate then Generate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that asks an LLM to detect errors on a supplied candidate output (often an empty or irrelevant string) and then produce a refined generation; used as a single-step generate-then-reflect prompt to induce deliberation and error avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5) and GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box large language model APIs: text-davinci-003 (GPT-3.5, OpenAI text completion engine) and GPT-4 (commercial OpenAI LLM). Paper does not provide exact parameter counts; used in 1-shot and few-shot settings with temperature=0, top_p=1.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Deliberate then Generate (DTG)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Prompt template supplies (1) task instruction, (2) a synthesized candidate output (commonly an empty string or irrelevant text) under label [SYS], and (3) an explicit instruction to 'Please detect the error type firstly, and refine the ... then'. The model is asked to (a) detect potential error types in the candidate given the source, then (b) generate a refined final output in the same single model call (single-pass generate-then-reflect). Candidate can be empty, irrelevant, or baseline system output; empty string is used by default in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple text-generation tasks (Machine translation (WMT sets), Abstractive summarization (CNN/DailyMail, GigaWord), Dialogue summarization (SamSum, DialogSum), Text simplification (ASSET, WikiAuto), Style transfer (GYAFC, Amazon, Yelp), Paraphrase (QQP), Commonsense generation (CommonGen))</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A broad set of generation tasks: translation across many language pairs, abstractive and dialogue summarization, simplification, style transfer, paraphrase generation, and constrained commonsense generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Examples reported in paper (DTG vs baseline GPT prompting): Machine translation ZH->EN (GPT 1-shot) BLEU 23.7 -> +DTG 25.3; ZH->EN (GPT 5-shot) BLEU 23.6 -> +DTG 25.2. Summarization CNN/DailyMail (GPT 1-shot) ROUGE-1 38.87 -> +DTG 40.17; GigaWord (GPT 10-shot) ROUGE-1 47.37 -> +DTG 50.48. Text simplification (ASSET, GPT 1-shot) BLEU 67.6 -> +DTG 72.9, SARI 46.12 -> +DTG 47.23. Style transfer (GYAFC EM, GPT 1-shot) BLEU 52.9 -> +DTG 66.8, BLEURT 73.42 -> +DTG 75.20. CommonGen (GPT 5-shot) BLEU-3/4 39.7/30.0 -> +DTG 43.2/33.5; ROUGE-2/L 25.28/46.55 -> +DTG 27.02/48.47. COMET improvement when refining MS-Translator: 80.4 -> 81.65 (example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline GPT prompting numbers from same tables: Machine translation ZH->EN GPT 1-shot BLEU 23.7 (no DTG); GPT 5-shot 23.6. Summarization CNN/DailyMail ROUGE-1 38.87 (no DTG). Text simplification ASSET BLEU 67.6, SARI 46.12 (no DTG). Style transfer (GYAFC EM) BLEU 52.9, BLEURT 73.42 (no DTG). CommonGen GPT 5-shot BLEU-3/4 39.7/30.0 (no DTG).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: consistent gains across many datasets and metrics (BLEU, COMET, ChrF, ROUGE, SARI, BLEURT); concrete examples given above. Ablations: removing the explicit error-detection instruction reduces BLEU (see ablation). Error-statistics: DTG reduced under-translation and entity-translation error rates (Figure 6). Refiner experiments: DTG can improve an external system's COMET score (MS-Translator 80.4 -> 81.65). Human-aligned GPT-based evaluation (500 samples) shows DTG beating the best system in GPT evaluation on several generation tasks. Qualitative case studies show more faithful, coherent outputs under DTG.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations reported: (1) DTG can be less helpful or marginal for some language directions (paper reports marginal or negative changes for some EN->X pairs). (2) If the candidate is a correct output produced by the model itself (a 'correct candidate'), DTG can fail to improve and may even hurt performance (ablation: 'correct candidate' BLEU 23.0 vs DTG 25.2). (3) DTG's effectiveness depends on model capacity — smaller models (e.g., LLaMA-7B) may not follow instructions and thus see weaker gains. (4) DTG still trails specialized fine-tuned SOTA models in some metrics (e.g., BART/UniLMv2 in ROUGE on summarization). (5) Experiments with GPT-4 are limited in scale due to API constraints; most results are with GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>DTG is implemented as a single-step prompt (no multi-round loop). The choice of candidate (empty/irrelevant vs correct baseline) strongly affects whether the prompt triggers deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deliberate then Generate: Enhanced Prompting Framework for Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5431.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5431.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Refine (baseline-candidate refine)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Refine using baseline candidate outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A refinement-style prompt variant where the candidate supplied to the model is a baseline system output (e.g., 5-shot GPT or another system) and the model is asked to detect errors and refine that candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (case studies) and GPT-3.5 (in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same commercial LLM APIs; in the paper GPT-4 is used for qualitative case comparisons of Refine vs DTG, while GPT-3.5 is used broadly in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Refine (generate candidate from baseline, then detect-and-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Provide an actual baseline system output as [SYS] in the DTG prompt (rather than empty/irrelevant). The model is instructed to detect error types in that candidate and to output a refined version. Implemented as a single-step generate-then-reflect prompt using the baseline output as the candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Primarily machine translation (example Chinese->English shown in case study Table 7) and other generation tasks when used for ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refinement of an initially generated candidate (e.g., 5-shot baseline translation) into an improved final output.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative case examples in Table 7 show 'Refine' yields more paraphrastic and sometimes more elaborate renditions (e.g., 'The flavors are amazing, the meat is excellent...'). Quantitative ablation: using a 'correct candidate' (i.e., good baseline candidate) in WMT ZH->EN experiment resulted in BLEU 23.0 and COMET 81.17 (worse than DTG default). Using a 'fixed incorrect candidate' or irrelevant-language candidate produced comparable COMET/BLEU to default DTG (Table 9: +DTG COMET 81.70; fixed incorrect candidate COMET 81.72; irrelevant languages COMET 81.81).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline GPT (no refine prompt) for same settings: GPT 5-shot BLEU 23.6, COMET 81.12 (WMT ZH-EN).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>When the candidate is an external high-quality system, DTG/refine can further improve COMET (example: MS-Translator improved from 80.4 to 81.65 via DTG using that system's output as candidate). However, when the candidate is the model's own correct output, refinement does not help and can degrade metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Using a correct/baseline candidate generated by the same LLM may cause the model to avoid reproducing correct phrasings (leads to unnecessary rewriting), sometimes reducing fluency or accuracy; conversely, incorrect/irrelevant candidates are better triggers for deliberation. Thus, picking candidate source matters. Also implemented as a single-step refinement, not multi-iteration, so it does not perform iterative self-criticize loops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deliberate then Generate: Enhanced Prompting Framework for Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5431.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5431.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DTG ablation: w/o error detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deliberate-then-Generate prompt ablation without explicit error-detection instruction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation removing the explicit 'Please detect the error type firstly...' instruction from DTG, used to test whether explicit error-detection phrasing is required to trigger deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-3.5 API as above, evaluated in 5-shot WMT ZH->EN experiments for the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>DTG without explicit error-detection phrase</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>DTG prompt variant where the phrase instructing the model to 'detect the error type firstly, and refine ... then' is removed; candidate remains empty string. This tests whether the explicit error-detection instruction is the trigger for deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (WMT ZH->EN ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>1- or 5-shot translation evaluation comparing standard few-shot prompting, full DTG, and DTG without error-detection instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Full DTG (GPT 5-shot) BLEU 25.2, COMET 81.70 (from Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>DTG w/o error detection BLEU 23.3, COMET 81.05. Baseline GPT 5-shot BLEU 23.6, COMET 81.12.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Removing the explicit error-detection instruction causes a marked BLEU drop relative to full DTG (25.2 -> 23.3) and yields performance similar to or worse than standard prompting, indicating that the explicit 'detect error' instruction is a crucial trigger for the deliberative behavior and metric improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Shows that DTG's benefits depend on the prompt phrasing: without explicit error-detection instruction, the method does not induce the same deliberation and performance gains. This suggests fragility to prompt wording and dependence on the model understanding that meta-instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deliberate then Generate: Enhanced Prompting Framework for Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progressive-hint prompting improves reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>GPTEval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5431",
    "paper_id": "paper-c85c90ef9e9a71efe031c3f7d6e34561f91168fe",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "DTG",
            "name_full": "Deliberate then Generate",
            "brief_description": "A prompting framework that asks an LLM to detect errors on a supplied candidate output (often an empty or irrelevant string) and then produce a refined generation; used as a single-step generate-then-reflect prompt to induce deliberation and error avoidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5) and GPT-4",
            "model_description": "Black-box large language model APIs: text-davinci-003 (GPT-3.5, OpenAI text completion engine) and GPT-4 (commercial OpenAI LLM). Paper does not provide exact parameter counts; used in 1-shot and few-shot settings with temperature=0, top_p=1.",
            "reflection_method_name": "Deliberate then Generate (DTG)",
            "reflection_method_description": "Prompt template supplies (1) task instruction, (2) a synthesized candidate output (commonly an empty string or irrelevant text) under label [SYS], and (3) an explicit instruction to 'Please detect the error type firstly, and refine the ... then'. The model is asked to (a) detect potential error types in the candidate given the source, then (b) generate a refined final output in the same single model call (single-pass generate-then-reflect). Candidate can be empty, irrelevant, or baseline system output; empty string is used by default in experiments.",
            "num_iterations": 1,
            "task_name": "Multiple text-generation tasks (Machine translation (WMT sets), Abstractive summarization (CNN/DailyMail, GigaWord), Dialogue summarization (SamSum, DialogSum), Text simplification (ASSET, WikiAuto), Style transfer (GYAFC, Amazon, Yelp), Paraphrase (QQP), Commonsense generation (CommonGen))",
            "task_description": "A broad set of generation tasks: translation across many language pairs, abstractive and dialogue summarization, simplification, style transfer, paraphrase generation, and constrained commonsense generation.",
            "performance_with_reflection": "Examples reported in paper (DTG vs baseline GPT prompting): Machine translation ZH-&gt;EN (GPT 1-shot) BLEU 23.7 -&gt; +DTG 25.3; ZH-&gt;EN (GPT 5-shot) BLEU 23.6 -&gt; +DTG 25.2. Summarization CNN/DailyMail (GPT 1-shot) ROUGE-1 38.87 -&gt; +DTG 40.17; GigaWord (GPT 10-shot) ROUGE-1 47.37 -&gt; +DTG 50.48. Text simplification (ASSET, GPT 1-shot) BLEU 67.6 -&gt; +DTG 72.9, SARI 46.12 -&gt; +DTG 47.23. Style transfer (GYAFC EM, GPT 1-shot) BLEU 52.9 -&gt; +DTG 66.8, BLEURT 73.42 -&gt; +DTG 75.20. CommonGen (GPT 5-shot) BLEU-3/4 39.7/30.0 -&gt; +DTG 43.2/33.5; ROUGE-2/L 25.28/46.55 -&gt; +DTG 27.02/48.47. COMET improvement when refining MS-Translator: 80.4 -&gt; 81.65 (example).",
            "performance_without_reflection": "Baseline GPT prompting numbers from same tables: Machine translation ZH-&gt;EN GPT 1-shot BLEU 23.7 (no DTG); GPT 5-shot 23.6. Summarization CNN/DailyMail ROUGE-1 38.87 (no DTG). Text simplification ASSET BLEU 67.6, SARI 46.12 (no DTG). Style transfer (GYAFC EM) BLEU 52.9, BLEURT 73.42 (no DTG). CommonGen GPT 5-shot BLEU-3/4 39.7/30.0 (no DTG).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: consistent gains across many datasets and metrics (BLEU, COMET, ChrF, ROUGE, SARI, BLEURT); concrete examples given above. Ablations: removing the explicit error-detection instruction reduces BLEU (see ablation). Error-statistics: DTG reduced under-translation and entity-translation error rates (Figure 6). Refiner experiments: DTG can improve an external system's COMET score (MS-Translator 80.4 -&gt; 81.65). Human-aligned GPT-based evaluation (500 samples) shows DTG beating the best system in GPT evaluation on several generation tasks. Qualitative case studies show more faithful, coherent outputs under DTG.",
            "limitations_or_failure_cases": "Limitations reported: (1) DTG can be less helpful or marginal for some language directions (paper reports marginal or negative changes for some EN-&gt;X pairs). (2) If the candidate is a correct output produced by the model itself (a 'correct candidate'), DTG can fail to improve and may even hurt performance (ablation: 'correct candidate' BLEU 23.0 vs DTG 25.2). (3) DTG's effectiveness depends on model capacity — smaller models (e.g., LLaMA-7B) may not follow instructions and thus see weaker gains. (4) DTG still trails specialized fine-tuned SOTA models in some metrics (e.g., BART/UniLMv2 in ROUGE on summarization). (5) Experiments with GPT-4 are limited in scale due to API constraints; most results are with GPT-3.5.",
            "additional_notes": "DTG is implemented as a single-step prompt (no multi-round loop). The choice of candidate (empty/irrelevant vs correct baseline) strongly affects whether the prompt triggers deliberation.",
            "uuid": "e5431.0",
            "source_info": {
                "paper_title": "Deliberate then Generate: Enhanced Prompting Framework for Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Refine (baseline-candidate refine)",
            "name_full": "Refine using baseline candidate outputs",
            "brief_description": "A refinement-style prompt variant where the candidate supplied to the model is a baseline system output (e.g., 5-shot GPT or another system) and the model is asked to detect errors and refine that candidate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (case studies) and GPT-3.5 (in experiments)",
            "model_description": "Same commercial LLM APIs; in the paper GPT-4 is used for qualitative case comparisons of Refine vs DTG, while GPT-3.5 is used broadly in experiments.",
            "reflection_method_name": "Refine (generate candidate from baseline, then detect-and-refine)",
            "reflection_method_description": "Provide an actual baseline system output as [SYS] in the DTG prompt (rather than empty/irrelevant). The model is instructed to detect error types in that candidate and to output a refined version. Implemented as a single-step generate-then-reflect prompt using the baseline output as the candidate.",
            "num_iterations": 1,
            "task_name": "Primarily machine translation (example Chinese-&gt;English shown in case study Table 7) and other generation tasks when used for ablation.",
            "task_description": "Refinement of an initially generated candidate (e.g., 5-shot baseline translation) into an improved final output.",
            "performance_with_reflection": "Qualitative case examples in Table 7 show 'Refine' yields more paraphrastic and sometimes more elaborate renditions (e.g., 'The flavors are amazing, the meat is excellent...'). Quantitative ablation: using a 'correct candidate' (i.e., good baseline candidate) in WMT ZH-&gt;EN experiment resulted in BLEU 23.0 and COMET 81.17 (worse than DTG default). Using a 'fixed incorrect candidate' or irrelevant-language candidate produced comparable COMET/BLEU to default DTG (Table 9: +DTG COMET 81.70; fixed incorrect candidate COMET 81.72; irrelevant languages COMET 81.81).",
            "performance_without_reflection": "Baseline GPT (no refine prompt) for same settings: GPT 5-shot BLEU 23.6, COMET 81.12 (WMT ZH-EN).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "When the candidate is an external high-quality system, DTG/refine can further improve COMET (example: MS-Translator improved from 80.4 to 81.65 via DTG using that system's output as candidate). However, when the candidate is the model's own correct output, refinement does not help and can degrade metrics.",
            "limitations_or_failure_cases": "Using a correct/baseline candidate generated by the same LLM may cause the model to avoid reproducing correct phrasings (leads to unnecessary rewriting), sometimes reducing fluency or accuracy; conversely, incorrect/irrelevant candidates are better triggers for deliberation. Thus, picking candidate source matters. Also implemented as a single-step refinement, not multi-iteration, so it does not perform iterative self-criticize loops.",
            "uuid": "e5431.1",
            "source_info": {
                "paper_title": "Deliberate then Generate: Enhanced Prompting Framework for Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "DTG ablation: w/o error detection",
            "name_full": "Deliberate-then-Generate prompt ablation without explicit error-detection instruction",
            "brief_description": "An ablation removing the explicit 'Please detect the error type firstly...' instruction from DTG, used to test whether explicit error-detection phrasing is required to trigger deliberation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "Same GPT-3.5 API as above, evaluated in 5-shot WMT ZH-&gt;EN experiments for the ablation.",
            "reflection_method_name": "DTG without explicit error-detection phrase",
            "reflection_method_description": "DTG prompt variant where the phrase instructing the model to 'detect the error type firstly, and refine ... then' is removed; candidate remains empty string. This tests whether the explicit error-detection instruction is the trigger for deliberation.",
            "num_iterations": 1,
            "task_name": "Machine translation (WMT ZH-&gt;EN ablation)",
            "task_description": "1- or 5-shot translation evaluation comparing standard few-shot prompting, full DTG, and DTG without error-detection instruction.",
            "performance_with_reflection": "Full DTG (GPT 5-shot) BLEU 25.2, COMET 81.70 (from Table 6).",
            "performance_without_reflection": "DTG w/o error detection BLEU 23.3, COMET 81.05. Baseline GPT 5-shot BLEU 23.6, COMET 81.12.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Removing the explicit error-detection instruction causes a marked BLEU drop relative to full DTG (25.2 -&gt; 23.3) and yields performance similar to or worse than standard prompting, indicating that the explicit 'detect error' instruction is a crucial trigger for the deliberative behavior and metric improvements.",
            "limitations_or_failure_cases": "Shows that DTG's benefits depend on the prompt phrasing: without explicit error-detection instruction, the method does not induce the same deliberation and performance gains. This suggests fragility to prompt wording and dependence on the model understanding that meta-instruction.",
            "uuid": "e5431.2",
            "source_info": {
                "paper_title": "Deliberate then Generate: Enhanced Prompting Framework for Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progressive-hint prompting improves reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1
        },
        {
            "paper_title": "GPTEval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 1
        }
    ],
    "cost": 0.0167015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deliberate then Generate: Enhanced Prompting Framework for Text Generation</h1>
<p>Bei Li ${ }^{1}$; Rui Wang ${ }^{2}$; Junliang Guo ${ }^{2}$; Kaitao Song ${ }^{2}$, Xu Tan ${ }^{2}$; Hany Hassan ${ }^{3}$ Arul Menezes ${ }^{3}$, Tong Xiao ${ }^{1,4}$; Jiang Bian ${ }^{2}$ and JingBo Zhu ${ }^{1,4}$<br>${ }^{1}$ School of Computer Science and Engineering, Northeastern University, Shenyang, China<br>${ }^{2}$ Microsoft Research Asia, ${ }^{3}$ Microsoft Azure Translation, ${ }^{4}$ NiuTrans Research<br>libei_neu@outlook.com, {xiaotong, zhujingbo}@mail.neu.edu.cn<br>{ruiwa, junliangguo, kaitaosong, xuta, hanyh, arulm, jiabia}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown remarkable success across a wide range of natural language generation tasks, where proper prompt designs make great impacts. While existing prompting methods are normally restricted to providing correct information, in this paper, we encourage the model to deliberate by proposing a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors. DTG is a simple yet effective technique that can be applied to various text generation tasks with minimal modifications. We conduct extensive experiments on 20+ datasets across 7 text generation tasks, including summarization, translation, dialogue, and more. We show that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks. We also provide in-depth analyses to reveal the underlying mechanisms of DTG, which may inspire future research on prompting for LLMs.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) [4, 28, 41] are revolutionizing the area of natural language generation, which have demonstrated exceptional abilities in generating coherent and fluent text as well as exhibited a remarkable aptitude in performing a diverse range of text generation tasks with high accuracy [13, 26]. When adapting to downstream tasks, traditional fine-tuning methods require access to the parameters of LLMs, which hinder their application on powerful black-box LLMs (e.g., ChatGPT) that only provide APIs to interact with. Therefore, prompting methods that guide the generation results by providing several task-specific instructions and demonstrations have attracted lots of attention in recent works [37, 36], which show that the prompt can significantly influence the resulting outcomes and thus require careful design.
While prompting is itself a general approach, the current use of this approach is a bit rigid, say, an LLM only operates on the basis of what is correct [4, 13, 46]. This is not the case for language acquisition where a human can learn from both positive and negative feedback and improve the ability of language use through corrections. In this work, we examine whether and how the deliberation ability emerges by asking the LLMs to rethink and learn to detect potential errors in their output. To do this, we develop a new prompting template termed Deliberate then Generate (DTG) that contains instructions and candidate outputs to enable an error detection process before generation, i.e., adding "Please detect the error type firstly, and provide the refined results then" in the prompt.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of standard GPT prompting and our DTG prompt desgin for summarization task. Note that prompt in blue denotes the demonstration, and that in red denotes the test input. [SRC] and [Input] means the source input, TGT means the target reference and [INCORRECT SYS] means the irrelevant system output (e.g., such as a randomly sampled text or even an empty string).</p>
<p>A key design aspect of DTG is how to determine the candidate. One straightforward choice is utilizing the results from an extra baseline system, which typically exhibits high quality and requires only minor adjustments. Accordingly, it cannot well facilitate the deliberation ability. In this work, we propose to utilize the text that is irrelevant from the reference (e.g., such as a randomly sampled text or even an empty string) as the candidate. In this way, the method successfully triggers the deliberation ability of LLMs, without having to resort to other text generation systems to create correction examples, which enables DTG to be easily applied to a wide range of text generation tasks only with minimal modifications in prompts. This work is in part motivated from a psychological perspective by considering negative evidence in developing language abilities, which is a canonical case for language learning [24].
We conduct extensive experiments on 7 text generation tasks and more than 20 datasets on GPT3.5 (text-davinci-003) and GPT4, where the proposed DTG prompting consistently improves model performance compared to conventional prompts. GPT with DTG prompting achieves state-of-the-art performance on multiple datasets across different text generation tasks, including machine translation, simplification and commonsense generation. Extensive ablation studies and error statistical analysis illustrate that the proposed DTG prompting does enable deliberation ability and error avoidance before generation.</p>
<p>The main contributions of this work are summarized as follows:</p>
<ul>
<li>We propose a novel prompting framework named Deliberate then Generate (DTG) for LLMs. Extensive ablation studies and analyses show that by prompting the model to detect errors and refine, LLMs indeed deliberate and avoid possible errors in generation.</li>
<li>We conduct experiments on 20+ datasets across 7 text generation tasks, where DTG prompting brings consistent improvements and achieves SoTA performance on several benchmarks.</li>
<li>To the best of our knowledge, we are the first to evaluate the performance of GPT3.5 and GPT4 on multiple benchmark text generation tasks including text summarization, dialogue summarization, simplification, style transfer, paraphrase and commonsense generation. We hope the experimental results help deepen our understanding of SoTA LLMs.</li>
</ul>
<h1>2 Related Work</h1>
<p>Large Language Models. With the scaling of model and corpus sizes, Large Language Models (LLMs) [7, 32, 18] have achieved remarkable success in various areas of natural language processing. Considering the large scale of the LLMs, exploring cost-effective fine-tuning methods is one appealing line of work when adapting to downstream tasks [14, 20]. The fine-tuning approach poses a challenge when applied to powerful black-box LLMs that only offer APIs for interaction, as it requires access to the underlying parameters. With the help of instruction tuning [44] and reinforcement learning from human feedback [29], recent LLMs can achieve gradient-free adaptation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of DTG demonstration design for machine translation, style transfer and text simplification tasks. Due to the limited page, please refer to the Appendix for the remained 3 generation tasks, including dialogue summarization, paraphrase and commonsense generation.
to various downstream tasks by prompting with natural language instructions, and some powerful capacities such as in-context learning [4] have also emerged.</p>
<p>Prompting Methods. Prompting is a general method for humans to interact with LLMs, which is usually designed as an instruction for a task that guides LLMs toward intended outputs [37, 36]. To make the most of LLMs on downstream tasks, the prompts need to be carefully designed, either manually [13] or automatically [8, 51]. Prompting also provides a way to interact with LLMs in natural language, such as letting them utilize external tools [38], resources [9] and models [47, 40], or conducting Chain-of-Thought (CoT) reasoning in generation [45, 16]. A concurrent work incorporates answers in previous rounds into prompts in an iterative process to improve the accuracy of LLMs on reasoning tasks [50]. Besides multi-step reasoning, basic prompts are still widely utilized in general text generation tasks such as machine translation and summarization, where previous advanced methods such as CoT have been shown ineffective [30]. In this paper, we propose Deliberate then Generate (DTG), a simple and general prompting method that consistently improves model performance across various text generation tasks, without task-specific designs.</p>
<h1>3 Deliberate then Generate</h1>
<p>Language acquisition by a human is normally based on both positive and negative feedback and improves the ability of language use through corrections. Inspired by this, unlike the conventional prompts only with correct information, we introduce a more deliberate approach termed Deliberate then Generate (DTG) prompting by facilitating LLMs to detect errors on a synthesized text that may contain errors. Specifically, the proposed DTG method unfolds in the following manner: 1) It begins by a concise and explicit instruction of the desired task, providing guidance on generating an intended text based on a given input text; 2) A synthesized text is then provided as a candidate output; (3) Finally, DTG encourages the model to detect potential errors, and subsequently generate an improved output after thorough deliberation.</p>
<p>Figure 1 illustrates a comparison between standard prompting and our proposed DTG prompting for the summarization task in the oneshot scenario. A distinctive feature of DTG is its emphasis on error detection other than immediate response. Instead of generating the outcome directly from the given input text, DTG steers the model to make deliberate decisions by detecting the error type firstly based on both the input text, denoted as "[SRC]", and a pre-defined candidate, denoted as "[SYS]", before the final decisions. This deliberative process forms the bedrock of the DTG approach and will be further elaborated upon in the analysis section (i.e., Section 6). Besides, a few demonstrations can be provided, imbuing LLMs with an awareness of the expected output (highlighted in blue), and the test input (marked in red). DTG is a general prompting method that
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: BLEU scores against the similarity (Edit Distance) on ZH-EN task. could be easily applied to any text generation task with minimal modifications to the prompt. Fig-</p>
<p>ure 2 illustrates the particular prompts used for 3 generation tasks we considered, indicating that minimal customization is required across different tasks as highlighted in yellow.</p>
<p>The determination of the synthesized text is another key part of DTG. Straightforwardly, using the output of a baseline system, which can either be LLMs themselves or any other models, is a natural choice. However, such baseline text just requires minor modifications, and thus cannot well trigger the deliberation ability of LLMs. Moreover, we find that the lower the similarity between the candidate and the reference, the better the quality of the generated text. As shown in Figure 3, we select sentences that have various similarities with the reference (using edit distance) as the synthesized sentence, and the performance decreases monotonically in general when the similarity increases. Therefore, we seek to choose a sentence that does not contain any correct information as the synthesized text. Potential candidates include a randomly sampled sentence or more extremely, an empty string, i.e., setting "[SYS]" as " ". Both choices successfully facilitate deliberation and consistently improve the outcomes across multiple text generation tasks. We use an empty string in our experiments as it is more general and elegant.</p>
<p>DTG has the following exceptional properties to steer LLMs on various text generation tasks:</p>
<ul>
<li>Simple: The final results can be obtained through a single-step inference of the LLM, without any additional resources or costs.</li>
<li>General: It can be effortlessly applied to a broad range of text generation tasks only with minimal adjustments in the prompt.</li>
</ul>
<h1>4 Datasets and Evaluation</h1>
<p>In experiments, we are devoted to evaluating the generation ability of LLMs and the proposed DTG prompting. We select 7 representative generation tasks, including machine translation, abstractive summarization, dialogue summarization, text simplification, style transfer, paraphrase and commonsense generation.</p>
<p>Machine Translation For the machine translation task, we aligned with Hendy et al. [13]'s work and experimented on both high-resource and low-resource scenarios. For the high-resource setting, we include German, Czech, Chinese, Japanese, Russian, and Ukrainian paired with English. In the low-resource context, we examine Icelandic and Hausa. The performance is evaluated in terms of SacreBLEU ${ }^{3}$ [31], ChrF, TER (translation error rate) and COMET-22 [35].</p>
<p>Abstractive Summarization We also evaluate LLM's ability to process long sequence on CNNDailyMail and Gigaword, two widely used abstractive summarization datasets. The evaluation metric is F1-Rouge [22], consisting of Rouge-1, Rouge-2 and Rouge-L.</p>
<p>Dialog Summarization Dialogue summarization presents greater challenges than traditional text summarization due to the intricate conversation contexts that models need to comprehend, though their contexts are relatively shorter. This attribute enables us to test few-shot abilities due to the restricted input length. To investigate this, we select SamSum ${ }^{4}$ [10] and DialogSum ${ }^{5}$ [5], two benchmark datasets for dialogue summarization. The evaluation metric is the same as abstractive summarization.</p>
<p>Text Simplification The purpose of text simplification is to revise complex text into sequences with simplified grammar and word choice. In this work, we mainly report the performance on two benchmarks, namely Asset [2] and Wiki-auto [15]. Asset is a multi-reference dataset for the evaluation of sentence simplification in English. The dataset uses the same 2,359 sentences from TurkCorpus [48] and each sentence is associated with 10 crowdsourced simplifications. Similarly, each test set in Wiki-auto owns 8 references. We use SacreBLEU and BLEURT as the metric.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Evaluation results of GPT and GPT4 on six high-resource and two-low resource machine translation tasks from WMT Testsets. The best scores across different systems are marked in bold.</p>
<table>
<thead>
<tr>
<th>System</th>
<th>COMET-22 $\uparrow$</th>
<th>TER $\downarrow$</th>
<th>ChrF $\uparrow$</th>
<th>BLEU $\uparrow$</th>
<th>COMET-22 $\uparrow$</th>
<th>TER $\downarrow$</th>
<th>ChrF $\uparrow$</th>
<th>BLEU $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>DE-EN</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>ZH-EN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WMT-Best $\dagger$</td>
<td>85.0</td>
<td>51.5</td>
<td>58.5</td>
<td>33.4</td>
<td>81.0</td>
<td>54.7</td>
<td>61.1</td>
<td>33.5</td>
</tr>
<tr>
<td>MS-Translator $\dagger$</td>
<td>84.7</td>
<td>51.2</td>
<td>58.5</td>
<td>33.5</td>
<td>80.4</td>
<td>60.0</td>
<td>57.7</td>
<td>27.9</td>
</tr>
<tr>
<td>GPT 1-shot</td>
<td>84.7</td>
<td>53.7</td>
<td>56.2</td>
<td>30.4</td>
<td>81.0</td>
<td>64.4</td>
<td>54.9</td>
<td>23.7</td>
</tr>
<tr>
<td>+ DTG</td>
<td>85.0</td>
<td>52.4</td>
<td>57.7</td>
<td>32.3</td>
<td>81.4</td>
<td>63.6</td>
<td>56.2</td>
<td>25.3</td>
</tr>
<tr>
<td>GPT 5-shot</td>
<td>85.3</td>
<td>52.3</td>
<td>57.6</td>
<td>32.3</td>
<td>81.1</td>
<td>63.7</td>
<td>54.6</td>
<td>23.6</td>
</tr>
<tr>
<td>+ DTG</td>
<td>85.4</td>
<td>51.9</td>
<td>58.2</td>
<td>33.2</td>
<td>81.7</td>
<td>62.4</td>
<td>55.9</td>
<td>25.2</td>
</tr>
<tr>
<td>GPT4 1-shot</td>
<td>85.6</td>
<td>51.7</td>
<td>58.9</td>
<td>33.5</td>
<td>82.4</td>
<td>62.8</td>
<td>57.3</td>
<td>26.0</td>
</tr>
<tr>
<td>+ DTG</td>
<td>85.8</td>
<td>51.8</td>
<td>58.8</td>
<td>33.4</td>
<td>82.9</td>
<td>62.0</td>
<td>57.3</td>
<td>25.7</td>
</tr>
<tr>
<td>CS-EN</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>RU-EN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WMT-Best $\dagger$</td>
<td>89.0</td>
<td>26.8</td>
<td>79.3</td>
<td>64.2</td>
<td>86.0</td>
<td>43.8</td>
<td>68.9</td>
<td>45.1</td>
</tr>
<tr>
<td>MS-Translator $\dagger$</td>
<td>87.4</td>
<td>34.5</td>
<td>74.0</td>
<td>54.9</td>
<td>85.2</td>
<td>45.1</td>
<td>68.3</td>
<td>43.9</td>
</tr>
<tr>
<td>GPT 1-shot</td>
<td>86.2</td>
<td>43.7</td>
<td>67.5</td>
<td>44.8</td>
<td>84.8</td>
<td>48.2</td>
<td>65.3</td>
<td>39.7</td>
</tr>
<tr>
<td>+ DTG</td>
<td>86.7</td>
<td>42.6</td>
<td>68.8</td>
<td>45.6</td>
<td>85.0</td>
<td>48.3</td>
<td>66.1</td>
<td>40.0</td>
</tr>
<tr>
<td>GPT 5-shot</td>
<td>86.9</td>
<td>40.7</td>
<td>69.2</td>
<td>47.2</td>
<td>84.9</td>
<td>48.0</td>
<td>65.2</td>
<td>39.9</td>
</tr>
<tr>
<td>+ DTG</td>
<td>87.0</td>
<td>40.9</td>
<td>69.6</td>
<td>47.4</td>
<td>85.1</td>
<td>47.7</td>
<td>66.2</td>
<td>40.3</td>
</tr>
<tr>
<td>GPT4 1-shot</td>
<td>87.3</td>
<td>40.3</td>
<td>70.9</td>
<td>48.1</td>
<td>86.1</td>
<td>45.2</td>
<td>68.5</td>
<td>43.1</td>
</tr>
<tr>
<td>+ DTG</td>
<td>87.3</td>
<td>39.8</td>
<td>70.9</td>
<td>48.9</td>
<td>86.3</td>
<td>45.1</td>
<td>68.5</td>
<td>43.1</td>
</tr>
<tr>
<td>JA-EN</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>UK-EN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WMT-Best $\dagger$</td>
<td>81.6</td>
<td>69.4</td>
<td>49.8</td>
<td>24.8</td>
<td>86.0</td>
<td>42.7</td>
<td>67.3</td>
<td>44.6</td>
</tr>
<tr>
<td>MS-Translator $\dagger$</td>
<td>81.5</td>
<td>69.0</td>
<td>49.6</td>
<td>24.5</td>
<td>83.5</td>
<td>45.7</td>
<td>65.3</td>
<td>42.4</td>
</tr>
<tr>
<td>GPT 1-shot</td>
<td>81.3</td>
<td>74.4</td>
<td>47.9</td>
<td>21.5</td>
<td>83.5</td>
<td>50.5</td>
<td>61.1</td>
<td>36.8</td>
</tr>
<tr>
<td>+ DTG</td>
<td>81.7</td>
<td>74.6</td>
<td>47.9</td>
<td>21.4</td>
<td>84.0</td>
<td>49.9</td>
<td>61.7</td>
<td>37.1</td>
</tr>
<tr>
<td>GPT 5-shot</td>
<td>81.2</td>
<td>74.2</td>
<td>47.0</td>
<td>20.5</td>
<td>84.0</td>
<td>49.2</td>
<td>61.9</td>
<td>38.0</td>
</tr>
<tr>
<td>+ DTG</td>
<td>82.2</td>
<td>72.6</td>
<td>48.2</td>
<td>22.4</td>
<td>84.2</td>
<td>48.4</td>
<td>62.6</td>
<td>39.0</td>
</tr>
<tr>
<td>GPT4 1-shot</td>
<td>83.4</td>
<td>69.6</td>
<td>51.1</td>
<td>24.7</td>
<td>85.7</td>
<td>46.9</td>
<td>65.2</td>
<td>39.9</td>
</tr>
<tr>
<td>+ DTG</td>
<td>83.6</td>
<td>69.5</td>
<td>51.1</td>
<td>24.8</td>
<td>85.7</td>
<td>47.1</td>
<td>65.2</td>
<td>39.9</td>
</tr>
<tr>
<td>IS-EN</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>HA-EN</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WMT-Best $\dagger$</td>
<td>87.0</td>
<td>44.8</td>
<td>62.3</td>
<td>41.7</td>
<td>80.0</td>
<td>69.0</td>
<td>48.7</td>
<td>21.0</td>
</tr>
<tr>
<td>MS-Translator $\dagger$</td>
<td>85.9</td>
<td>45.2</td>
<td>62.8</td>
<td>40.5</td>
<td>73.3</td>
<td>73.4</td>
<td>43.4</td>
<td>16.2</td>
</tr>
<tr>
<td>GPT 1-shot</td>
<td>83.5</td>
<td>52.7</td>
<td>57.0</td>
<td>33.6</td>
<td>78.0</td>
<td>72.8</td>
<td>47.3</td>
<td>18.6</td>
</tr>
<tr>
<td>+ DTG</td>
<td>84.0</td>
<td>51.7</td>
<td>58.3</td>
<td>35.2</td>
<td>78.3</td>
<td>74.8</td>
<td>48.0</td>
<td>18.6</td>
</tr>
<tr>
<td>GPT 5-shot</td>
<td>84.1</td>
<td>50.6</td>
<td>58.0</td>
<td>35.0</td>
<td>78.3</td>
<td>72.2</td>
<td>47.6</td>
<td>18.8</td>
</tr>
<tr>
<td>+ DTG</td>
<td>84.6</td>
<td>50.2</td>
<td>58.8</td>
<td>36.0</td>
<td>78.6</td>
<td>71.9</td>
<td>48.0</td>
<td>19.2</td>
</tr>
<tr>
<td>GPT4 1-shot</td>
<td>86.9</td>
<td>47.0</td>
<td>63.8</td>
<td>39.9</td>
<td>77.5</td>
<td>75.7</td>
<td>47.8</td>
<td>18.3</td>
</tr>
<tr>
<td>+ DTG</td>
<td>87.0</td>
<td>46.7</td>
<td>63.9</td>
<td>40.3</td>
<td>77.9</td>
<td>75.1</td>
<td>47.9</td>
<td>18.7</td>
</tr>
</tbody>
</table>
<p>Style Transfer We used three widely-used English transfer learning datasets, namely Grammalry's Yahoo Answers Formality Corpus (GYAFC), Amazon and Yelp reviews. The GYAFC dataset [34] was originally a question-and-answer dataset on an online forum, consisting of informal and formal sentences from the two categories: Entertainment \&amp; Music (EM) and Family \&amp; Relationships (FR). Both FR and EM provide 4 references to evaluate the fidelity. The Amazon dataset is a product review dataset, labeled as either a positive or negative sentiment. Similarly, the Yelp dataset is a restaurant and business review dataset with positive and negative sentiments. Both Amazon and Yelp are single-reference. The evaluation metrics contain BLEU and BLEURT [39].</p>
<p>Paraphrase We endeavor to evaluate the paraphrase ability of LLMs upon the well-known Quora Question Pairs (QQP) dataset, which requires generating an alternative surface form in the same language expressing the same semantic content. We utilize the preprocessed data from [11].</p>
<p>Common Sense Generation We choose CommonGen [21], a novel constrained generation task that requires models to generate a coherent sentence with the providing key concepts. We summarize the details of each dataset for each task, including the test sets, the selection of demonstrations (mostly from validation sets) and the corresponding prompts we have used. For more details please refer to the attached Appendix.</p>
<p>Table 2: Experimental results on four summarization tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">CNN/DailyMail</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GigaWord</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SamSum</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DialogSum</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">R2</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">R2</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">R2</td>
<td style="text-align: center;">RL</td>
<td style="text-align: center;">R1</td>
<td style="text-align: center;">R2</td>
<td style="text-align: center;">RL</td>
</tr>
<tr>
<td style="text-align: center;">Transformer [42]</td>
<td style="text-align: center;">40.47</td>
<td style="text-align: center;">17.73</td>
<td style="text-align: center;">37.29</td>
<td style="text-align: center;">37.57</td>
<td style="text-align: center;">18.90</td>
<td style="text-align: center;">34.69</td>
<td style="text-align: center;">37.20</td>
<td style="text-align: center;">10.86</td>
<td style="text-align: center;">34.69</td>
<td style="text-align: center;">35.91</td>
<td style="text-align: center;">8.74</td>
<td style="text-align: center;">33.50</td>
</tr>
<tr>
<td style="text-align: center;">BART [19]</td>
<td style="text-align: center;">44.16</td>
<td style="text-align: center;">21.28</td>
<td style="text-align: center;">40.90</td>
<td style="text-align: center;">39.29</td>
<td style="text-align: center;">20.09</td>
<td style="text-align: center;">35.65</td>
<td style="text-align: center;">53.12</td>
<td style="text-align: center;">27.95</td>
<td style="text-align: center;">49.15</td>
<td style="text-align: center;">47.28</td>
<td style="text-align: center;">21.18</td>
<td style="text-align: center;">44.83</td>
</tr>
<tr>
<td style="text-align: center;">UniLMv2 [3]</td>
<td style="text-align: center;">43.16</td>
<td style="text-align: center;">20.42</td>
<td style="text-align: center;">40.14</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.53</td>
<td style="text-align: center;">26.62</td>
<td style="text-align: center;">48.81</td>
<td style="text-align: center;">47.04</td>
<td style="text-align: center;">21.13</td>
<td style="text-align: center;">45.04</td>
</tr>
<tr>
<td style="text-align: center;">GPT 1-shot</td>
<td style="text-align: center;">38.87</td>
<td style="text-align: center;">15.36</td>
<td style="text-align: center;">35.11</td>
<td style="text-align: center;">31.24</td>
<td style="text-align: center;">11.61</td>
<td style="text-align: center;">27.99</td>
<td style="text-align: center;">44.52</td>
<td style="text-align: center;">19.92</td>
<td style="text-align: center;">39.60</td>
<td style="text-align: center;">36.84</td>
<td style="text-align: center;">14.23</td>
<td style="text-align: center;">32.20</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">40.17</td>
<td style="text-align: center;">15.60</td>
<td style="text-align: center;">36.04</td>
<td style="text-align: center;">31.50</td>
<td style="text-align: center;">12.00</td>
<td style="text-align: center;">28.50</td>
<td style="text-align: center;">45.50</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">40.13</td>
<td style="text-align: center;">39.01</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">34.13</td>
</tr>
<tr>
<td style="text-align: center;">GPT 5-shot</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.04</td>
<td style="text-align: center;">12.78</td>
<td style="text-align: center;">29.86</td>
<td style="text-align: center;">46.44</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">41.10</td>
<td style="text-align: center;">40.86</td>
<td style="text-align: center;">17.10</td>
<td style="text-align: center;">35.78</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.54</td>
<td style="text-align: center;">13.63</td>
<td style="text-align: center;">30.36</td>
<td style="text-align: center;">48.72</td>
<td style="text-align: center;">23.16</td>
<td style="text-align: center;">43.23</td>
<td style="text-align: center;">42.64</td>
<td style="text-align: center;">18.12</td>
<td style="text-align: center;">37.57</td>
</tr>
<tr>
<td style="text-align: center;">GPT 10-shot</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.24</td>
<td style="text-align: center;">13.26</td>
<td style="text-align: center;">30.46</td>
<td style="text-align: center;">47.37</td>
<td style="text-align: center;">22.08</td>
<td style="text-align: center;">42.20</td>
<td style="text-align: center;">41.28</td>
<td style="text-align: center;">17.48</td>
<td style="text-align: center;">36.69</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.02</td>
<td style="text-align: center;">14.21</td>
<td style="text-align: center;">31.04</td>
<td style="text-align: center;">50.48</td>
<td style="text-align: center;">24.88</td>
<td style="text-align: center;">45.31</td>
<td style="text-align: center;">45.11</td>
<td style="text-align: center;">19.50</td>
<td style="text-align: center;">39.71</td>
</tr>
</tbody>
</table>
<h1>5 Experiments</h1>
<p>In this section, we assess the efficacy of the text-davinci-003 (also known as GPT3.5, which is denoted as GPT in the following for simplicity) across 7 sequence generation tasks, including machine translation, abstractive summarization, dialogue summarization, text simplification, style transfer, commonsense generation and paraphrase. The chosen baseline comparisons consist of 1-shot, and few-shot (mostly 5-shot) learning scenarios. It is worth mentioning that while the performance of GPT models on machine translation has been extensively investigated in previous research, other generation tasks (e.g., text simplification and style transfer) have received comparatively limited attention. To demonstrate the versatility of DTG method and address the primary limitation of GPT3.5, we conduct further experiments with GPT4, a cutting-edge LLM API. Due to the considerable computational cost and API request constraints associated with the GPT4, it is challenging to perform extensive experiments. In the current manuscript, we only report the results on machine translation and text simplification. We aim to highlight the significant potential of GPT models to excel in downstream tasks without the necessity for fine-tuning.</p>
<h3>5.1 Results on Machine Translation</h3>
<p>We compare the performance of GPT standard prompting and our deliberate then generate method (DTG) with that of a commercial system (Microsoft Translator) in addition to WMT SoTA systems. Table 1 presents the results in both 1-shot and 5-shot scenarios. Without meticulous parameter tuning, we set the temperature to 0 and top_p to 1 when calling the API. The findings here indicate that our re-implementation aligns with the trends observed in previous study [13], that 5 -shot beats 1shot in most language pairs. Benefiting from the deliberation, DTG effectively pushes the boundaries and leads to enhanced results across all to-English language pairs in both 1-shot and 5-shot settings based on GPT3.5 model. For instance, DTG method exhibits substantial BLEU score increases in DE-EN, ZH-EN, and UK-EN language pairs in 5 shot scenarios. More concretely, DTG even beats WMT-Best system in terms of COMET-22, which is a more recognized metric recently in the machine translation literature. Moreover, the consistent improvements on IS-EN and HA-EN demonstrate the effectiveness of DTG in low-resource settings.</p>
<p>We only conduct experiments on 1-shot scenario due to the limited access, and leave the remained 5shot explorations as future work. We observe GPT4 1-shot can beat GPT3.5 5-shot by a large margin in most language pairs. Meanwhile, DTG is still effective on GPT4. This finding demonstrates much stronger LLMs can still benefit from deliberation.</p>
<h3>5.2 Results on Summarization</h3>
<p>For abstractive summarization, we mainly evaluate GPT models on CNN/DailyMail and GigaWord, two of the most widely-used summarization tasks. Due to the limit of max length for GPT models (4097) and the long input length of CNN/DailyMail, we only evaluate the performance in 1-shot scenario. As shown in Table 2, GPT models show comparative performance with Transformer which is specially tuned on the downstream training set. Our DTG can also shown further improvement in terms of three Rouge metrics, which demonstrate the effectiveness of DTG on long-term modeling task. However, DTG still falls lag behind of large-scale pretrained models, such as BART [19] and UniLMv2 [3] in automatic evaluations. We will add more human alignment judgment in Section 6.</p>
<p>Table 4: Comparisons of 1-shot and 5-shot on four style transfer tasks, including Entertainment Music, Family Relationships, Amazon and Yelp. †denotes results borrowed from [17].</p>
<table>
<thead>
<tr>
<th>System</th>
<th>GYAFC \&amp; EM</th>
<th></th>
<th>GYAFC \&amp; FR</th>
<th></th>
<th>Amazon</th>
<th></th>
<th>Yelp</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BLEU</td>
<td>BLEURT</td>
<td>BLEU</td>
<td>BLEURT</td>
<td>BLEU</td>
<td>BLEURT</td>
<td>BLEU</td>
<td>BLEURT</td>
</tr>
<tr>
<td>Transformer†[42]</td>
<td>40.3</td>
<td>-</td>
<td>47.7</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>BART $\dagger[19]$</td>
<td>76.9</td>
<td>75.38</td>
<td>79.3</td>
<td>75.11</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT 1-shot</td>
<td>52.9</td>
<td>73.42</td>
<td>44.6</td>
<td>70.73</td>
<td>36.1</td>
<td>64.56</td>
<td>30.9</td>
<td>64.03</td>
</tr>
<tr>
<td>+ DTG</td>
<td>66.8</td>
<td>75.20</td>
<td>65.9</td>
<td>74.60</td>
<td>35.4</td>
<td>63.60</td>
<td>31.3</td>
<td>64.19</td>
</tr>
<tr>
<td>GPT 5-shot</td>
<td>61.3</td>
<td>75.40</td>
<td>63.9</td>
<td>74.35</td>
<td>39.3</td>
<td>64.76</td>
<td>31.4</td>
<td>64.16</td>
</tr>
<tr>
<td>+ DTG</td>
<td>69.9</td>
<td>76.36</td>
<td>74.1</td>
<td>75.43</td>
<td>40.9</td>
<td>65.42</td>
<td>32.2</td>
<td>64.87</td>
</tr>
</tbody>
</table>
<p>Dialogue generation represents a critical aspect of language tasks. In this context, we further corroborate the efficacy of Large Language Models (LLMs) in dialogue summarization, a composite task encapsulating elements of both dialogue and summarization. It is important to note that the results for DialogSum are averaged over three individual scores, each calculated using unique references spanning a range of topics. As observed, GPT 1-shot achieves commendable results compared to constrained systems, e.g., Transformer. Furthermore, DTG substantially incites GPT models to generate more precise summaries derived from extensive multi-turn dialogues. An upward trend in performance is observed with the introduction of additional demonstrations, further underscoring the effectiveness of the DTG method. Nonetheless, in the absence of specialized fine-tuning, the GPT3.5 model falls short of surpassing the performance of BART. Despite this, the model's performance remains notably impressive, highlighting the potential of LLMs in complex language generation tasks.</p>
<h1>5.3 Results on Style Transfer</h1>
<p>Table 4 displays performance across style transfer tasks from the GYAFC dataset: Entertainment Music (EM) and Family Relationships (FR), both involving informal to formal transformations. Evidently, the Deliberate then Generate (DTG) method prompts the GPT model to correct inaccuracies and generate more precise informal sentences. Specifically, DTG achieves an 8-point and 10.04point increase in BLEU score for EM and FR tasks, respectively, compared to standard prompting. Although DTG trails BART [19] in BLEU scores, it surpasses BART in BLEURT scores, registering gains of 0.98 and 0.32 for EM and FR tasks, respectively. These results highlight the potential of LLMs and our DTG method in style transfer tasks.</p>
<h3>5.4 Results on Text Simplification</h3>
<p>Experiments were conducted on two text simplification benchmarks, Asset and Wiki-Auto, where the primary goal of which is to create a simplified rendition of the given text input. The main evaluation metric is the SARI score. Our findings illustrate that GPT models demonstrate robust performance across both simplification benchmarks, even surpassing the existing state-of-the-art models (MUSS) built based on BART. Furthermore, the incorporation of DTG method significantly enhances</p>
<p>Table 3: Comparisons of 1-shot, 5-shot with and without our DTG method on two text simplification tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">Asset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Wiki-auto</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">SARI</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">SARI</td>
</tr>
<tr>
<td style="text-align: center;">MUSS [25]</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">44.15</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.59</td>
</tr>
<tr>
<td style="text-align: center;">Control Prefix [6]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.58</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">TST-Final [27]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.46</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT 1-shot</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">46.12</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">44.97</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">47.23</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">47.15</td>
</tr>
<tr>
<td style="text-align: center;">GPT 5-shot</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">45.95</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">45.12</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">47.05</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">47.54</td>
</tr>
<tr>
<td style="text-align: center;">GPT4 5-shot</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">47.10</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">45.96</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">47.67</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">47.03</td>
</tr>
</tbody>
</table>
<p>GPT model performance, leading to improvements in both BLEU and SARI scores. Specifically, DTG establishes a new benchmark for state-of-the-art results on these two simplification tasks.
We also observe similar competitiveness of DTG on the two other style transfer benchmarks. It outperforms the standard prompting method with identical configurations in terms of both BLEU and BLEURT scores, further attesting to its efficacy. Again, GPT4 is superior to GPT3.5 and DTG also works at this time, though the obtained improvement is slightly marginal than that of GPT3.5.</p>
<p>Results on Commonsense Generation Table 5 summarizes the comparison between GPT models with and without DTG method on an open Commonsense generation benchmark. This task is more flexible than the aforementioned, meanwhile raising the evaluation difficulty. We see that GPT models with standard prompting even surpasses large-scale pretrained generation models, such as BART [18] and T5 [33]. Our DTG achieves further improvements in terms of BLEU-3/BLEU-4 and Rouge-2/Rouge-L, resulting in an average of 3.50 BLEU scores and almost 2.00 Rouge score improvements. This also establishes a new SoTA on this benchmark.</p>
<p>Results on Paraphrase Figure 4 delineates the BLEU and Rouge-L scores for GPT and DTG in relation to various few-shot scenarios. In our preliminary experiments, we find that only 5 -shot demonstrations cannot enable LLMs to clearly capture the underline mapping rule between the source and the target. To this end, we test LLMs on 20-shot and 50-shot and observe intriguing phenomenon.</p>
<p>Across all scenarios, DTG outperforms GPT models in terms of both BLEU and Rouge-L metrics. However, when the number of demonstrations is restricted, e.g., 1-shot and 5-shot, LLMs noticeably trail behind state-of-the-art systems. Interestingly, a significant enhancement in DTG performance is observed with the increase in the number of demonstrations. This improvement can be attributed to the model's enhanced ability to comprehend the underlying mapping rules between the source and target, a capability that intensifies with an expanded demonstration set.</p>
<p>Table 5: Results on the CommonGen benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU-3/4</th>
<th style="text-align: center;">Rouge-2/L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART [19]</td>
<td style="text-align: center;">$36.3 / 26.4$</td>
<td style="text-align: center;">$22.23 / 41.98$</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large [33]</td>
<td style="text-align: center;">$39.0 / 28.6$</td>
<td style="text-align: center;">$22.01 / 42.97$</td>
</tr>
<tr>
<td style="text-align: left;">GPT 5-shot</td>
<td style="text-align: center;">$39.7 / 30.0$</td>
<td style="text-align: center;">$25.28 / 46.55$</td>
</tr>
<tr>
<td style="text-align: left;">+ DTG</td>
<td style="text-align: center;">$\mathbf{4 3 . 2 / 3 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 7 . 0 2 / 4 8 . 4 7}$</td>
</tr>
</tbody>
</table>
<p>Figure 4: BLEU and Rouge-L scores against the number of demonstrations.</p>
<h1>6 Analysis</h1>
<p>In this section, we delve into a series of intriguing questions to elucidate the circumstances and reasons underpinning the robust performance of DTG. Unless specified otherwise, the base engine utilized throughout this investigation is text-davinci-003.</p>
<p>Ablation Study Prior research [49, 43, 1] underscores the significant impact of both the quality and quantity of demonstrations on the performance of LLMs. Thus, it becomes essential to discern whether the improvements observed are attributable to modifications in the template or the deliberate capability inherent to the LLMs. To this end, we conduct experiments on WMT ZH-EN and show the comparisons in Table 6. Firstly, eliminating the phrase "Please detect the error type firstly, and refine the translation then", denoted</p>
<p>Table 6: Ablations on DTG prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">COMET</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT 5-shot</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">81.12</td>
</tr>
<tr>
<td style="text-align: left;">+ DTG</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">81.70</td>
</tr>
<tr>
<td style="text-align: left;">+ w/o error detection</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">81.05</td>
</tr>
<tr>
<td style="text-align: left;">+ wrong error type</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">81.74</td>
</tr>
<tr>
<td style="text-align: left;">+ fixed error type</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">81.35</td>
</tr>
<tr>
<td style="text-align: left;">+ correct candidate</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">81.17</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>as "w/o error detection" in Table 6, DTG experiences a significant decrement in BLEU score, suggesting that the excised segment may contain crucial triggers stimulating the deliberate capability of the LLM. Along this line, we make two explorations: 1) replacing "incorrect translation" by "good/correct translation" in the demonstration only, resulting in no BLEU degradation, which is denoted as "wrong error type" in Table 6. This reveals that LLMs can rethink by themselves and make "correct" decisions though the demonstration is incorrect. 2) using fixed error type, e.g., under translation in the LLM response. This leads to a 1.1 BLEU drop, indicating that restricting the thought of LLMs would hinder the performance. Moreover, we observe that adopting the correct candidate generated by itself cannot bring further improvements than standard prompting. A plausible explanation for this observation could be that GPT3.5 might lack the necessary ability to accurately identify and concisely correct the parts of text requiring modification.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: GPT3.5 and GPT4 evaluation on 4 generation tasks. Note that we random select 500 samples due to the limitation of GPT4 access.</p>
<p>Table 7: Case study on refining from the previous candidate (Refine) and the proposed DTG method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">味道赞、肉类好、服务热情</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">Nice taste, great meat, enthusiastic service.</td>
</tr>
<tr>
<td style="text-align: left;">GPT 1-shot</td>
<td style="text-align: left;">The taste is great, the meat is good, and the service is enthusiastic.</td>
</tr>
<tr>
<td style="text-align: left;">+ Refine</td>
<td style="text-align: left;">The flavors are amazing, the meat is excellent, and the service is warm and welcoming.</td>
</tr>
<tr>
<td style="text-align: left;">+ DTG</td>
<td style="text-align: left;">Great taste, good meat, enthusiastic service.</td>
</tr>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: left;">目前已经购买了这个系列3款机器！</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">I have bought three laptops of this series!</td>
</tr>
<tr>
<td style="text-align: left;">GPT 1-shot</td>
<td style="text-align: left;">So far, 3 machines from this series have been purchased!</td>
</tr>
<tr>
<td style="text-align: left;">+ Refine</td>
<td style="text-align: left;">Up until now, 3 machines from this series have been purchased!</td>
</tr>
<tr>
<td style="text-align: left;">+ DTG</td>
<td style="text-align: left;">I have already purchased 3 models from this series!</td>
</tr>
</tbody>
</table>
<p>Evaluation by GPT Models As previously discussed, despite DTG's impressive performance, it falls short of BART in some scenarios-most notably, it exhibits a significant gap in terms of Rouge scores in summarization tasks. However, Liu et al. [23] suggested that Rouge may not accurately represent the true performance of summarization tasks, given its poor alignment with human evaluations. In contrast, GPT models achieve optimal alignment with human justification and substantially outperform all previous state-of-the-art evaluators on the SummEval benchmark. This observation prompts an investigation into whether the generation output by DTG can surpass that of BART. Following their suggestion, we conduct reference-based evaluation and design a prompt as shown in Figure 5. We extract 500 test sets and compared DTG with the best result using GPT3.5 and GPT4 to select a better candidate. We see that DTG significantly beats the best system within GPT evaluation, except for the style transfer dataset.</p>
<p>Error Statistical Analysis To evaluate whether the proposed DTG prompting can facilitate error avoidance in GPT, we conduct error statistics on machine translation, where two frequently occurring error types are considered (i.e., under translation and incorrect entity translation) [12]. Figure 6 provides a comparison of the error rates between GPT models with and without the application of the DTG method. It is obvious to see that DTG reduces both error rates compared with the direct generation manner.</p>
<p>Case Study We provide a case study based on GPT4 model in Table 7, where "Refine" indicates utilizing the 5-shot baseline results as the synthesized sentences, i.e., "[INCORRECT SYS]" in Figure 1, and DTG is our method that uses an empty string instead. The conclusions are two-fold. 1) Using the baseline results will cause the model to avoid generating the same segmentations in it although they may be correct already, e.g., "taste" to "flavors", "so far" to "up until now", as well as others in red. As a result, the fluency and accuracy of the final results may be affected. 2) Equipped with DTG, fluency, coherence and grammatical correctness of generated results are all promoted. In the first case, the DTG result is</p>
<p>more faithful not only in semantics but also in structure than the baseline. In the second case, DTG is able to complete the subject "I" which does not appear in the source sentence.</p>
<p>DTG Can Serve as A Good Refiner To investigate the correlation between the performance of DTG and the provided candidate, we consider the translation task on WMT ZH-EN and create candidates with varying quality by randomly removing certain words from the translations generated by MS-Translator. Figure 7 displays the performance measured by COMET versus the word drop rate. The blue line represents the performance of MS-Translator, and the red line represents DTG with various candidates. Upon deliberation, GPT can improve the translation of MS-Translator from 80.4 to 81.65 in COMET. As aforementioned that DTG suffers from performance degradation when the candidate is a correct one generated by itself (See the last line in Table 6). However, upon deeper investigation, we discern that selecting candidates from systems other than GPT itself is a superior choice. This underscores the effectiveness of our DTG framework, demonstrating its capability to work even with high-quality candidates generated by other systems. Moreover, the performance declines when more words are dropped from the MS-Translator candidate, but interestingly, it increases when the candidate almost resembles an empty string. Though with additional high-quality systems, DTG also successfully improves the performance, using an empty string as a candidate can always lead to a better outcome without any additional resources and cost, as well as specific demonstration construction.</p>
<h1>7 Conclusions</h1>
<p>In this paper, we propose DTG prompting, which encourages LLMs to deliberate before generating the final results by letting the model detect the error type on a synthetic text that may contain errors. Using an empty string as the synthetic text successfully gets rid of an extra baseline system and improves the quality of the generated text. The DTG prompting can be easily applied to various text generation tasks with minimal adjustments in the prompt. Extensive experiments conducted on over 20 datasets across 7 text generation tasks demonstrate the effectiveness and broad applicability of the DTG prompting framework. One potential avenue for further enhancing the efficacy of DTG prompting involves leveraging task-specific domain knowledge. (e.g., explicitly listing the potential error types in the prompts to provide guidance for deliberation), which is worth future investigation.</p>
<h2>References</h2>
<p>[1] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022.
[2] Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoît Sagot, and Lucia Specia. ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4668-4679, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.424. URL https://aclanthology.org/2020.acl-main. 424.
[3] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. Unilmv2: Pseudo-masked language models for unified language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 642-652. PMLR, 2020. URL http://proceedings.mlr.press/v119/bao20a.html.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-</p>
<p>els are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020 .
[5] Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. DialogSum: A real-life scenario dialogue summarization dataset. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5062-5074, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.449. URL https://aclanthology.org/2021.findings-acl. 449.
[6] Jordan Clive, Kris Cao, and Marek Rei. Control prefixes for parameter-efficient text generation. In Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 363-382, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.gem-1.31.
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[8] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better fewshot learners. arXiv preprint arXiv:2012.15723, 2020.
[9] Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. Dictionary-based phrase-level prompting of large language models for machine translation. arXiv preprint arXiv:2302.07856, 2023.
[10] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70-79, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/019-5409.
[11] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022.
[12] Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving human parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567, 2018.
[13] Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210, 2023.
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[15] Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. Neural CRF model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943-7960, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.709. URL https://aclanthology.org/2020.acl-main. 709.
[16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
[17] Huiyuan Lai, Antonio Toral, and Malvina Nissim. Thank you BART! rewarding pre-trained models improves formality style transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 484-494, Online, August</p>
<ol>
<li>Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.62. URL https://aclanthology.org/2021.acl-short.62.
[18] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.
[19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78717880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-main.703. URL https://aclanthology.org/2020.acl-main.703.
[20] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.
[21] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020.findings-emnlp.165.
[22] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.
[23] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.
[24] Gary F Marcus. Negative evidence in language acquisition. Cognition, 46(1):53-85, 1993.
[25] Louis Martin, Angela Fan, Éric de la Clergerie, Antoine Bordes, and Benoît Sagot. MUSS: Multilingual unsupervised sentence simplification by mining paraphrases. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 16511664, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.176.
[26] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
[27] Kostiantyn Omelianchuk, Vipul Raheja, and Oleksandr Skurzhanskyi. Text Simplification by Tagging. In Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 11-25, Online, April 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.bea-1.2.
[28] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[30] Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. Towards making the most of chatgpt for machine translation. arXiv preprint arXiv:2303.13780, 2023.
[31] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W18-6319.</li>
</ol>
<p>[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html.
[34] Sudha Rao and Joel Tetreault. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 129-140, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1012. URL https://aclanthology.org/N18-1012.
[35] Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.52.
[36] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.
[37] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020.
[38] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.
[39] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation. In Proceedings of ACL, 2020.
[40] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.
[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, pages 5998-6008, 2017.
[43] David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102, 2022.
[44] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Confed.</p>
<p>[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.
[48] Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris CallisonBurch. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401-415, 2016. URL https://cocoxu.github.io/publications/tacl2016-smt-simplification.pdf.
[49] Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine translation: A case study. arXiv preprint arXiv:2301.07069, 2023.
[50] Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.
[51] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Illustration of DTG demonstration design for dialogue summarization, paraphrase and commonsense generation tasks within minimal modifications.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Illustration of the standard GPT prompting involving both demonstration and test input on six generation tasks, including machine translation, dialogue summarization, text simplification, style transfer, paraphrase and commonsense generation.</p>
<h1>A Limitation</h1>
<p>Due to restricted access to GPT4, we have evaluated our Deliberate then Generate (DTG) method on just two generation tasks: machine translation (across 8 language pairs) and simplification. There exists a necessity for more expansive experimentation across other tasks. Additionally, the effectiveness of DTG is contingent on model capacity. Models such as LLaMa-7B might not fully comprehend the instructions provided, resulting in weaker performance on downstream tasks. In our future work, we aim to ascertain the required scale of a language model to successfully facilitate deliberative generation.</p>
<p>Our work inherits the biases from pre-trained language models. For example, we only conduct experiments on English generation that GPT models are most powerful at. We provide results and analysis on English-to-Others translation in Appendix C. Future works could investigate the performance of DTG on multilingual pre-trained models.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: Illustration of the prompting design of GPT evaluation for Figure 5. We adhere to the recommendation proposed in [23]'s work, implementing a zero-shot GPT evaluation approach to identifying superior candidate translations through the adjudication of LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(a) Prompt template of "w/o error detection"</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Given the [src] sentence: [SRC] <br> Demonstration <br> the [tgt] translation is: [SYS] <br> The refined [tgt] translation is: [TGT] <br> Given the [src] sentence: [SRC] <br> Test <br> the [tgt] translation is: [SYS] <br> The refined [tgt] translation is:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(b) Prompt template of "wrong error type"</td>
</tr>
<tr>
<td style="text-align: center;">Given the [src] sentence: [SRC] <br> Demonstration <br> the [tgt] translation is: [SYS] <br> Please detect the error type firstly, and refine the translation then. <br> Error type: good/correct translation, the refined [tgt] translation is: [TGT] <br> Given the [src] sentence: [SRC] <br> Test <br> the [tgt] translation is: [SYS] <br> Please detect the error type firstly, and refine the translation then. <br> Error type:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(c) Prompt template of "fixed error type"</td>
</tr>
<tr>
<td style="text-align: center;">Given the [src] sentence: [SRC] <br> Demonstration <br> the [tgt] translation is: [SYS] <br> Please detect the error type firstly, and refine the translation then. <br> Error type: under translation, the refined [tgt] translation is: [TGT] <br> Given the [src] sentence: [SRC] <br> Test <br> the [tgt] translation is: [SYS] <br> Please detect the error type firstly, and refine the translation then. <br> Error type: under translation, the refined [tgt] translation is:</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 11: Illustration of the prompting design of the ablation study in Table 6. Note that all [SYS] here is empty string. The purpose here is to evaluate the deliberation ability of LLMs.</p>
<p>Table 8: Evaluation results of GPT on six high-resource and two-low resource machine translation tasks from WMT Testsets in from English directions. The best scores are marked in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">COMET-22 $\uparrow$</th>
<th style="text-align: center;">TER $\downarrow$</th>
<th style="text-align: center;">ChrF $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">COMET-22 $\uparrow$</th>
<th style="text-align: center;">TER $\downarrow$</th>
<th style="text-align: center;">ChrF $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-DE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-ZH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WMT-Best $\dagger$</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">102.3</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">44.8</td>
</tr>
<tr>
<td style="text-align: center;">MS-Translator $\dagger$</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">48.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT 5-shot</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-CS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-RU</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WMT-Best $\dagger$</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: center;">MS-Translator $\dagger$</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT 5-shot</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">28.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-JA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-UK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WMT-Best $\dagger$</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">105.9</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">32.5</td>
</tr>
<tr>
<td style="text-align: center;">MS-Translator $\dagger$</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">106.0</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT 5-shot</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">111.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">111.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-IS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-HA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WMT-Best $\dagger$</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: center;">MS-Translator $\dagger$</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">10.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT 5-shot</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">9.9</td>
</tr>
<tr>
<td style="text-align: center;">+ DTG</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">10.1</td>
</tr>
</tbody>
</table>
<h1>B Design of Prompts</h1>
<p>Figure 8 presents the DTG demonstration design across the other three text generation tasks. It can be observed that DTG does not necessitate task-specific designs; instead, a clear instruction outlining the main task for each work suffices. For the ease of replication of our results, we also furnish all baseline prompts, as depicted in Figure 9. Also, we provide the prompting design for GPT evaluation in Figure 10, which follows a zero-shot fashion.</p>
<p>To facilitate a more comprehensive understanding of the prompt ablations conducted in Section 6, we provide the corresponding design of prompts in Figure 11. Please note that prompts in blue represent the pre-designed demonstration, while those in red represent the test input. As observed, firstly, removing the error detection leads to the prompting in 11 (a). Additionally, the term "wrong error type" implies that we fed an empty string into LLMs, presenting it as a good translation. However, LLMs can autonomously detect the correct error type as an "incorrect translation" and subsequently generate an accurate response following careful deliberation (Figure 11 (b)). Conversely, if we constrain the error type detection process and solely allow LLMs to generate the translation, a considerable performance gap emerges (See Figure 11 (c)).</p>
<h2>C More Analyses</h2>
<p>Results on Machine Translation from English Table 8 summarizes the results of standard prompting and our DTG method in 5-shot scenarios, alongside results from WMT-Best and MSTranslator. When compared to results from to-English directional language pairs, such as DEEN, the improvements provided by DTG over the standard prompting strategy appear somewhat marginal. Furthermore, DTG may yield results inferior to standard prompting in EN-ZH and ENUK scenarios. This can likely be ascribed to the disparities in the balance of training sets across different languages.</p>
<p>Ablations on Candidates In Section 3, we demonstrated that the empty string serves as a universal and effective choice to stimulate LLMs to engage in the Deliberate then Generate process, and that a candidate more distinct from the reference can yield superior results. In this section, we aim to explore if other candidates may also prove effective in this context. Here, we take the WMT ZH-EN translation as</p>
<p>Table 9: Ablations on DTG prompting in terms of different candidates.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># Model</th>
<th style="text-align: left;">BLEU COMET</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1 GPT 5-shot</td>
<td style="text-align: left;">23.6</td>
</tr>
<tr>
<td style="text-align: left;">2 + DTG</td>
<td style="text-align: left;">25.2</td>
</tr>
<tr>
<td style="text-align: left;">3 + fixed incorrect candidate</td>
<td style="text-align: left;">25.0</td>
</tr>
<tr>
<td style="text-align: left;">4 + irrelevant languages</td>
<td style="text-align: left;">25.1</td>
</tr>
<tr>
<td style="text-align: left;">5 + correct candidate</td>
<td style="text-align: left;">23.0</td>
</tr>
</tbody>
</table>
<p>81.12
81.70
81.72
81.81
81.17</p>
<p>Table 10: Statistics of the dataset we used on over 20 benchmarks. Note that "Num." represents the number of test sets for each benchmark. "Total Words" and "Ave. Words" denote the total word count and average lengths, respectively. These statistics are based on tokenization sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Num.</th>
<th style="text-align: center;">Total Words</th>
<th style="text-align: center;">Ave. Words</th>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Num.</th>
<th style="text-align: center;">Total Words</th>
<th style="text-align: center;">Ave. Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WMT DE-EN</td>
<td style="text-align: center;">1984</td>
<td style="text-align: center;">33540</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: left;">CNN/DailyMail</td>
<td style="text-align: center;">11490</td>
<td style="text-align: center;">9017116</td>
<td style="text-align: center;">784.8</td>
</tr>
<tr>
<td style="text-align: left;">WMT CS-EN</td>
<td style="text-align: center;">1448</td>
<td style="text-align: center;">26050</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: left;">GigaWord</td>
<td style="text-align: center;">1951</td>
<td style="text-align: center;">72171</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: left;">WMT JA-EN</td>
<td style="text-align: center;">2008</td>
<td style="text-align: center;">36731</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: left;">SamSum</td>
<td style="text-align: center;">819</td>
<td style="text-align: center;">104492</td>
<td style="text-align: center;">127.6</td>
</tr>
<tr>
<td style="text-align: left;">WMT ZH-EN</td>
<td style="text-align: center;">1875</td>
<td style="text-align: center;">14353</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: left;">DialogSum</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">96385</td>
<td style="text-align: center;">192.7</td>
</tr>
<tr>
<td style="text-align: left;">WMT RU-EN</td>
<td style="text-align: center;">2016</td>
<td style="text-align: center;">32992</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: left;">EM</td>
<td style="text-align: center;">1416</td>
<td style="text-align: center;">17279</td>
<td style="text-align: center;">12.2</td>
</tr>
<tr>
<td style="text-align: left;">WMT UK-EN</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">29273</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: left;">FR</td>
<td style="text-align: center;">1332</td>
<td style="text-align: center;">16799</td>
<td style="text-align: center;">12.6</td>
</tr>
<tr>
<td style="text-align: left;">WMT IS-EN</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">19930</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: left;">Amazon</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">6055</td>
<td style="text-align: center;">12.1</td>
</tr>
<tr>
<td style="text-align: left;">WMT HA-EN</td>
<td style="text-align: center;">997</td>
<td style="text-align: center;">30955</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: left;">Yelp</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">5432</td>
<td style="text-align: center;">10.9</td>
</tr>
<tr>
<td style="text-align: left;">CommonGen</td>
<td style="text-align: center;">993</td>
<td style="text-align: center;">6465</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: left;">Asset</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">8115</td>
<td style="text-align: center;">22.6</td>
</tr>
<tr>
<td style="text-align: left;">QQP</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">27543</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: left;">Wiki-auto</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">43860</td>
<td style="text-align: center;">21.9</td>
</tr>
</tbody>
</table>
<p>an instance. Table 9 shows the comparison of various candidate inputs. Specifically, the term "fixed incorrect candidate" (#3) refers to the use of a fixed yet incorrect (irrelevant) English translation as the candidate. ${ }^{6}$ Likewise, system #4 indicates that the candidates neither belong to the target language nor conform to the correct structure or grammar. ${ }^{7}$ Interestingly, both 2 systems deliver comparable performance with our default setting, with system #4 even achieving a higher COMET score. However, when shifting to a correct candidate, LLMs seem to underperform. This observation suggests that LLMs can effectively deliberate when the candidate is incorrect - whether it is an empty string or other incorrect translations - and subsequently generate a substantially improved translation.</p>
<h1>D Details of Datasets</h1>
<p>In this section, we offer more detailed statistics concerning the test sets utilized in this study, encompassing 8 machine translation, 4 summarization, 4 style transfer, 2 simplification, 1 commonsense generation, and 1 paraphrase benchmarks. Table 10 provides a summary of the number of test sets, total words, and the average length. We will release the test sets and the corresponding demonstrations in the future. Note that the statistic is conducted based on tokenization sequences, which would be further segmented by BPE before feeding into LLMs. Consequently, the average length of summarization inputs would appear significantly larger, leading to an elevated risk in the context of few-shot requests.</p>
<h2>E Details of Error Statistical</h2>
<p>In Figure 6, two types of error are considered (i.e., under translation and entity translation error). In this section, we provide the details of the method to conduct the error statistics.</p>
<p>Under Translation We first use awesome-align ${ }^{8}$ to get the alignment between the source and target sentences. Then, a word in the source sentence is regarded as under translation, when it is aligned to a word in the reference target sentence but failed to be aligned in the generated target sentence.</p>
<p>Entity Translation We first use $s p a C y^{9}$ to recognize the named entities in the reference target sentence, where person names, organizations and locations are considered. Then, an entity in the reference is considered an error if it cannot be found in the generated target sentence.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We random sample an English sentence: [SYS]: EBA Education Team together with Accace Ukraine invite you to join the EBA Education Update: Performance Audit.
${ }^{7}$ Similarly, we random sample an Ukraine sentence: [SYS]: З впевненістю можете довіряти нам і будь ласка, звертайтеся до нас, якщо у вас є які-небудь питання чи коментарі.
${ }^{8} \mathrm{https}: / /$ github.com/neulab/awesome-align
${ }^{9} \mathrm{https}: / /$ github.com/explosion/spaCy&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
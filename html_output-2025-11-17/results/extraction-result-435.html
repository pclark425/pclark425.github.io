<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-435 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-435</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-435</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-270045052</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15164v2.pdf" target="_blank">From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks</a></p>
                <p><strong>Paper Abstract:</strong> Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e435.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e435.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuro-symbolic hybrids</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-symbolic hybrid models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of hybrid AI systems that combine explicit symbolic/declarative components (rules, programs, knowledge) with neural/imperative components (deep networks for perception or learned heuristics); cited in the paper as an approach that explicitly implements symbolic compositional mechanisms within neural systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-symbolic hybrid models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular hybrid architectures that pair an explicit symbolic (declarative) reasoning module with neural (imperative) components; typically the symbolic module implements rule-based or program-like compositional operations while the neural components handle high-dimensional inputs (images, raw text, sensory signals) or provide learned heuristics/guidance for search and perception.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit symbolic processes (rule systems, program synthesizers, symbolic libraries or knowledge representations); paper describes these generally as explicit symbolic mechanisms responsible for compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (deep models used for perception, representation learning, or to guide search) — the paper describes the neural parts as handling high-dimensional inputs like images and providing learned processing for perceptual or heuristic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular combination: symbolic engine integrated with neural front-end or neural-guided search; the neural modules process raw inputs and supply structured inputs (or probabilities/priors) to the symbolic module which applies explicit rules/programs to produce compositional reasoning. (Described in the paper at a high level; not presented as end-to-end differentiable integration.)</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Compositional generalization and systematicity are explicitly provided by the symbolic component; improved sample efficiency for compositional tasks when symbolic knowledge is present; interpretability/explainability afforded by explicit symbolic structures (rules, programs, libraries). According to the paper, these hybrids by design yield the behavioral signatures of compositionality because the symbolic component encodes them.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General compositional reasoning / tasks that require explicit symbolic manipulation (as discussed broadly in the paper: compositional generalization benchmarks like SCAN, program synthesis, symbolic reasoning tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Hybrid systems are described as able to exhibit strong compositional/o.o.d. generalization on tasks where compositional structure is built into the symbolic component; the paper emphasizes that this compositional generalization is due to the explicit symbolic module rather than arising from purely neural learning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>The declarative symbolic component provides readily interpretable representations (rules, programs, symbolic proofs) that can be inspected to explain decisions; the paper highlights this as a contrast to opaque purely neural solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>By design such hybrids may be 'mere implementations' of classical symbolic architectures (i.e., they do not provide novel explanatory mechanisms beyond reproducing symbolic operations); they can be criticized as not being necessary if purely neural metalearning or large-scale pretraining can produce similar behavioral signatures. The paper notes uncertainty whether hybrids are ultimately required.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division-of-labor / complementarity: declarative modules supply compositional structure and interpretability, neural modules supply perception and heuristic search; invoked in the paper as the rationale for neuro-symbolic approaches and as the form that would satisfy Fodor & Pylyshyn's demand for symbolic mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e435.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e435.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamCoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited example of a neuro-symbolic system that bootstraps symbolic program libraries while leveraging learned components; included in the paper as an instance of hybrid architectures that combine symbolic program induction with learned machinery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DreamCoder (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A system (cited) that performs inductive program synthesis by building and reusing symbolic program fragments (a learned library) while employing learning procedures to guide search and learning; presented in the paper as an instance of an explicit-symbolic / neural hybrid approach.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic program synthesizer and learned symbolic library of reusable program fragments (explicit program representations used for compositional construction).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Learned components that guide search and library construction (the paper cites DreamCoder as a hybrid example but does not detail the neural architectures in this review article).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Wake-sleep style bootstrapping where symbolic program induction and learned recognition/guidance components are trained in alternation to improve program synthesis performance (cited at high level in the references).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Bootstrapped reusable symbolic primitives / libraries that improve sample efficiency and enable compositional program synthesis beyond raw search; structured, inspectable solutions due to symbolic program outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Inductive program synthesis / program induction tasks (as in the DreamCoder work cited).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Enables compositional reuse of learned symbolic primitives leading to improved generalization on program-synthesis style tasks (paper cites DreamCoder as an example of explicit-symbolic approach delivering compositional behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: learned library and synthesized programs are explicit symbolic artifacts that provide human-readable explanations for solutions (noted as a property of such hybrids in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Classified in the review as an example of systems that, by design, 'merely implement' classical symbolic architectures — hence they may not offer novel mechanistic explanations beyond implementing symbolic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Explicit-symbolic implementation with learned guidance: symbolic program synthesis provides compositional mechanisms while learned components bootstrap and accelerate search (the paper treats DreamCoder as representative of this design philosophy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e435.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e435.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge-Infused Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited conceptual/system-level approach advocating incorporation of symbolic knowledge (knowledge graphs, constraints) into neural learners to produce hybrid systems with improved reasoning and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-infused neuro-symbolic systems (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approaches that inject structured symbolic knowledge (for example, knowledge graphs, ontologies, constraint sets) into neural architectures to bias and inform learning and inference; referenced in the paper as representative of neuro-symbolic proposals to secure compositional behavior by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Structured symbolic knowledge representations (knowledge graphs, ontologies, rule sets or constraints) that encode domain facts and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (deep models) that consume raw inputs and incorporate or condition on the injected symbolic knowledge during learning or inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Knowledge injection / conditioning: symbolic knowledge is added as supervision, constraints, or auxiliary inputs to neural training (modular or tightly coupled depending on implementation); the review cites this class of methods as neuro-symbolic hybrids but does not detail a single integration recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved reasoning on tasks requiring domain knowledge or structured relational inference, increased sample efficiency, and enhanced interpretability due to the explicit knowledge artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Broadly: tasks requiring factual/structural knowledge and relational reasoning (knowledge-intensive NLP, certain VQA/QA and reasoning benchmarks as discussed generally in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claimed to boost generalization in domains where injected knowledge matches required inductive biases; according to the review this is the rationale proponents offer for neuro-symbolic hybrids, though the paper notes that purely neural metalearning/LLMs sometimes achieve similar behavior without explicit knowledge injection.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic knowledge artifacts provide interpretable structure and can support explanations; the review highlights this as an advantage of knowledge-infused hybrids.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May be unnecessary if large-scale neural pretraining or metalearning can produce comparable compositional behaviors; reliance on curated knowledge raises issues of scalability and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Complementary strengths: symbolic knowledge supplies strong inductive biases and explanations, neural models supply robustness to raw inputs and learning from data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e435.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e435.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Marcus & Davis hybrids</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-symbolic hybrids advocated by Marcus and colleagues</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Advocated neuro-symbolic hybrid proposals (Marcus 2020; Marcus & Davis 2020) that argue explicit symbolic mechanisms must be added to neural networks to achieve robust compositional/general reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rebooting AI: Building Artificial Intelligence We Can Trust.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Marcus-style neuro-symbolic hybrids (advocacy)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Position and design family promoted by Gary Marcus and collaborators that favor combining neural learners with explicit symbolic reasoning modules (rules, logic engines, program induction) to attain systematic compositional generalization and reliable reasoning; cited in this review as the principal advocates for hybrid approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit symbolic mechanisms (rule-based systems, logical inference engines, symbolic program representations) proposed to underlie compositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks for perception and handling unstructured high-dimensional data; used alongside symbolic engines rather than as sole solution.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular composition: symbolic modules handle structured reasoning while neural modules handle perception and feature extraction; proponents often advocate clear interfaces between symbolic and neural parts (paper describes this at a conceptual level only).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Robust, human-interpretable compositional reasoning and systematic generalization attributed to the symbolic modules, combined with neural robustness on raw inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Posited to improve performance on compositional generalization tasks and reasoning benchmarks that pure neural models struggle with (discussed at a conceptual level in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Argued by proponents to achieve superior out-of-distribution and compositional generalization via explicit symbolic structure; the review contrasts this claim with emerging evidence that purely neural metalearning/LLMs can also produce similar behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability from the declarative component; advocates stress explainability and verifiability as advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>The review notes that current empirical results (metalearning, LLMs) complicate the claim that explicit symbolic mechanisms are strictly necessary, and frames hybrid advocacy as an empirical hypothesis that must be tested rather than assumed.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Strict symbol-plus-perception division: symbolic systems implement compositional rules and reasoning, neural systems implement perception and statistical learning; cited as a principled remedy to compositionality failures in pure neural nets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e435.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e435.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural-symbolic survey (Yu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A survey on neural-symbolic learning systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey work cited in the paper that reviews neural-symbolic learning systems and taxonomy of approaches combining symbolic and neural methods; cited here as a representative reference for the hybrid literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A survey on neural-symbolic learning systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural-symbolic systems (surveyed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Collection and taxonomy of approaches that combine symbolic/declarative reasoning with neural/imperative learning, covering varied integrations (modular, end-to-end, differentiable logic, knowledge injection); cited in the review as part of the neuro-symbolic literature.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Varies across surveyed systems; includes logic programming, symbolic rules, knowledge graphs, program representations, and other explicit symbolic artifacts (survey reference listed in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Varies across surveyed systems; typically includes deep neural networks (CNNs, transformers, RNNs) and learned search/recognition modules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Surveyed methods include modular architectures, knowledge injection, differentiable approximations to logic, hybrid training schedules, and symbolic solvers coupled to neural perception; the review references the survey as covering this space but does not detail individual methods.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Surveyed benefits reported across the literature: improved compositional/generalization behavior when symbolic structure matches task demands, better interpretability, and sometimes improved sample efficiency on structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Wide-ranging across surveyed works: program synthesis, visual reasoning, logical QA, compositional generalization benchmarks (e.g., SCAN-like tasks), and other tasks requiring structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Survey notes that neuro-symbolic methods often improve out-of-distribution and compositional generalization when symbolic priors align with tasks, though the review cautions that purely neural metalearning and LLMs sometimes match or approach these behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Survey emphasizes that declarative components provide more explicit explanations and inspectable reasoning traces compared to opaque neural-only systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Surveyed challenges include reliance on curated symbolic knowledge (scalability), difficulty of end-to-end differentiable integration, and potential brittleness when symbolic assumptions mismatch real data; the review echoes these limitations and frames them as empirical questions.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Taxonomic and pragmatic: different integration patterns yield complementary strengths and tradeoffs; the paper cites the survey to situate neuro-symbolic approaches within a broader taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning. <em>(Rating: 2)</em></li>
                <li>Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI. <em>(Rating: 2)</em></li>
                <li>Rebooting AI: Building Artificial Intelligence We Can Trust. <em>(Rating: 2)</em></li>
                <li>A survey on neural-symbolic learning systems <em>(Rating: 2)</em></li>
                <li>Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-435",
    "paper_id": "paper-270045052",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "Neuro-symbolic hybrids",
            "name_full": "Neuro-symbolic hybrid models",
            "brief_description": "Class of hybrid AI systems that combine explicit symbolic/declarative components (rules, programs, knowledge) with neural/imperative components (deep networks for perception or learned heuristics); cited in the paper as an approach that explicitly implements symbolic compositional mechanisms within neural systems.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Neuro-symbolic hybrid models (general)",
            "system_description": "Modular hybrid architectures that pair an explicit symbolic (declarative) reasoning module with neural (imperative) components; typically the symbolic module implements rule-based or program-like compositional operations while the neural components handle high-dimensional inputs (images, raw text, sensory signals) or provide learned heuristics/guidance for search and perception.",
            "declarative_component": "Explicit symbolic processes (rule systems, program synthesizers, symbolic libraries or knowledge representations); paper describes these generally as explicit symbolic mechanisms responsible for compositionality.",
            "imperative_component": "Neural networks (deep models used for perception, representation learning, or to guide search) — the paper describes the neural parts as handling high-dimensional inputs like images and providing learned processing for perceptual or heuristic tasks.",
            "integration_method": "Modular combination: symbolic engine integrated with neural front-end or neural-guided search; the neural modules process raw inputs and supply structured inputs (or probabilities/priors) to the symbolic module which applies explicit rules/programs to produce compositional reasoning. (Described in the paper at a high level; not presented as end-to-end differentiable integration.)",
            "emergent_properties": "Compositional generalization and systematicity are explicitly provided by the symbolic component; improved sample efficiency for compositional tasks when symbolic knowledge is present; interpretability/explainability afforded by explicit symbolic structures (rules, programs, libraries). According to the paper, these hybrids by design yield the behavioral signatures of compositionality because the symbolic component encodes them.",
            "task_or_benchmark": "General compositional reasoning / tasks that require explicit symbolic manipulation (as discussed broadly in the paper: compositional generalization benchmarks like SCAN, program synthesis, symbolic reasoning tasks).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Hybrid systems are described as able to exhibit strong compositional/o.o.d. generalization on tasks where compositional structure is built into the symbolic component; the paper emphasizes that this compositional generalization is due to the explicit symbolic module rather than arising from purely neural learning.",
            "interpretability_properties": "The declarative symbolic component provides readily interpretable representations (rules, programs, symbolic proofs) that can be inspected to explain decisions; the paper highlights this as a contrast to opaque purely neural solutions.",
            "limitations_or_failures": "By design such hybrids may be 'mere implementations' of classical symbolic architectures (i.e., they do not provide novel explanatory mechanisms beyond reproducing symbolic operations); they can be criticized as not being necessary if purely neural metalearning or large-scale pretraining can produce similar behavioral signatures. The paper notes uncertainty whether hybrids are ultimately required.",
            "theoretical_framework": "Division-of-labor / complementarity: declarative modules supply compositional structure and interpretability, neural modules supply perception and heuristic search; invoked in the paper as the rationale for neuro-symbolic approaches and as the form that would satisfy Fodor & Pylyshyn's demand for symbolic mechanisms.",
            "uuid": "e435.0",
            "source_info": {
                "paper_title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DreamCoder",
            "name_full": "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
            "brief_description": "Cited example of a neuro-symbolic system that bootstraps symbolic program libraries while leveraging learned components; included in the paper as an instance of hybrid architectures that combine symbolic program induction with learned machinery.",
            "citation_title": "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning.",
            "mention_or_use": "mention",
            "system_name": "DreamCoder (as cited)",
            "system_description": "A system (cited) that performs inductive program synthesis by building and reusing symbolic program fragments (a learned library) while employing learning procedures to guide search and learning; presented in the paper as an instance of an explicit-symbolic / neural hybrid approach.",
            "declarative_component": "Symbolic program synthesizer and learned symbolic library of reusable program fragments (explicit program representations used for compositional construction).",
            "imperative_component": "Learned components that guide search and library construction (the paper cites DreamCoder as a hybrid example but does not detail the neural architectures in this review article).",
            "integration_method": "Wake-sleep style bootstrapping where symbolic program induction and learned recognition/guidance components are trained in alternation to improve program synthesis performance (cited at high level in the references).",
            "emergent_properties": "Bootstrapped reusable symbolic primitives / libraries that improve sample efficiency and enable compositional program synthesis beyond raw search; structured, inspectable solutions due to symbolic program outputs.",
            "task_or_benchmark": "Inductive program synthesis / program induction tasks (as in the DreamCoder work cited).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Enables compositional reuse of learned symbolic primitives leading to improved generalization on program-synthesis style tasks (paper cites DreamCoder as an example of explicit-symbolic approach delivering compositional behavior).",
            "interpretability_properties": "High: learned library and synthesized programs are explicit symbolic artifacts that provide human-readable explanations for solutions (noted as a property of such hybrids in the review).",
            "limitations_or_failures": "Classified in the review as an example of systems that, by design, 'merely implement' classical symbolic architectures — hence they may not offer novel mechanistic explanations beyond implementing symbolic procedures.",
            "theoretical_framework": "Explicit-symbolic implementation with learned guidance: symbolic program synthesis provides compositional mechanisms while learned components bootstrap and accelerate search (the paper treats DreamCoder as representative of this design philosophy).",
            "uuid": "e435.1",
            "source_info": {
                "paper_title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Knowledge-Infused Learning",
            "name_full": "Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI",
            "brief_description": "Cited conceptual/system-level approach advocating incorporation of symbolic knowledge (knowledge graphs, constraints) into neural learners to produce hybrid systems with improved reasoning and interpretability.",
            "citation_title": "Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI.",
            "mention_or_use": "mention",
            "system_name": "Knowledge-infused neuro-symbolic systems (as cited)",
            "system_description": "Approaches that inject structured symbolic knowledge (for example, knowledge graphs, ontologies, constraint sets) into neural architectures to bias and inform learning and inference; referenced in the paper as representative of neuro-symbolic proposals to secure compositional behavior by construction.",
            "declarative_component": "Structured symbolic knowledge representations (knowledge graphs, ontologies, rule sets or constraints) that encode domain facts and relations.",
            "imperative_component": "Neural networks (deep models) that consume raw inputs and incorporate or condition on the injected symbolic knowledge during learning or inference.",
            "integration_method": "Knowledge injection / conditioning: symbolic knowledge is added as supervision, constraints, or auxiliary inputs to neural training (modular or tightly coupled depending on implementation); the review cites this class of methods as neuro-symbolic hybrids but does not detail a single integration recipe.",
            "emergent_properties": "Improved reasoning on tasks requiring domain knowledge or structured relational inference, increased sample efficiency, and enhanced interpretability due to the explicit knowledge artifacts.",
            "task_or_benchmark": "Broadly: tasks requiring factual/structural knowledge and relational reasoning (knowledge-intensive NLP, certain VQA/QA and reasoning benchmarks as discussed generally in the review).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claimed to boost generalization in domains where injected knowledge matches required inductive biases; according to the review this is the rationale proponents offer for neuro-symbolic hybrids, though the paper notes that purely neural metalearning/LLMs sometimes achieve similar behavior without explicit knowledge injection.",
            "interpretability_properties": "Symbolic knowledge artifacts provide interpretable structure and can support explanations; the review highlights this as an advantage of knowledge-infused hybrids.",
            "limitations_or_failures": "May be unnecessary if large-scale neural pretraining or metalearning can produce comparable compositional behaviors; reliance on curated knowledge raises issues of scalability and coverage.",
            "theoretical_framework": "Complementary strengths: symbolic knowledge supplies strong inductive biases and explanations, neural models supply robustness to raw inputs and learning from data.",
            "uuid": "e435.2",
            "source_info": {
                "paper_title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Marcus & Davis hybrids",
            "name_full": "Neuro-symbolic hybrids advocated by Marcus and colleagues",
            "brief_description": "Advocated neuro-symbolic hybrid proposals (Marcus 2020; Marcus & Davis 2020) that argue explicit symbolic mechanisms must be added to neural networks to achieve robust compositional/general reasoning.",
            "citation_title": "Rebooting AI: Building Artificial Intelligence We Can Trust.",
            "mention_or_use": "mention",
            "system_name": "Marcus-style neuro-symbolic hybrids (advocacy)",
            "system_description": "Position and design family promoted by Gary Marcus and collaborators that favor combining neural learners with explicit symbolic reasoning modules (rules, logic engines, program induction) to attain systematic compositional generalization and reliable reasoning; cited in this review as the principal advocates for hybrid approaches.",
            "declarative_component": "Explicit symbolic mechanisms (rule-based systems, logical inference engines, symbolic program representations) proposed to underlie compositional reasoning.",
            "imperative_component": "Neural networks for perception and handling unstructured high-dimensional data; used alongside symbolic engines rather than as sole solution.",
            "integration_method": "Modular composition: symbolic modules handle structured reasoning while neural modules handle perception and feature extraction; proponents often advocate clear interfaces between symbolic and neural parts (paper describes this at a conceptual level only).",
            "emergent_properties": "Robust, human-interpretable compositional reasoning and systematic generalization attributed to the symbolic modules, combined with neural robustness on raw inputs.",
            "task_or_benchmark": "Posited to improve performance on compositional generalization tasks and reasoning benchmarks that pure neural models struggle with (discussed at a conceptual level in the review).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Argued by proponents to achieve superior out-of-distribution and compositional generalization via explicit symbolic structure; the review contrasts this claim with emerging evidence that purely neural metalearning/LLMs can also produce similar behaviors.",
            "interpretability_properties": "High interpretability from the declarative component; advocates stress explainability and verifiability as advantages.",
            "limitations_or_failures": "The review notes that current empirical results (metalearning, LLMs) complicate the claim that explicit symbolic mechanisms are strictly necessary, and frames hybrid advocacy as an empirical hypothesis that must be tested rather than assumed.",
            "theoretical_framework": "Strict symbol-plus-perception division: symbolic systems implement compositional rules and reasoning, neural systems implement perception and statistical learning; cited as a principled remedy to compositionality failures in pure neural nets.",
            "uuid": "e435.3",
            "source_info": {
                "paper_title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Neural-symbolic survey (Yu et al.)",
            "name_full": "A survey on neural-symbolic learning systems",
            "brief_description": "Survey work cited in the paper that reviews neural-symbolic learning systems and taxonomy of approaches combining symbolic and neural methods; cited here as a representative reference for the hybrid literature.",
            "citation_title": "A survey on neural-symbolic learning systems.",
            "mention_or_use": "mention",
            "system_name": "Neural-symbolic systems (surveyed)",
            "system_description": "Collection and taxonomy of approaches that combine symbolic/declarative reasoning with neural/imperative learning, covering varied integrations (modular, end-to-end, differentiable logic, knowledge injection); cited in the review as part of the neuro-symbolic literature.",
            "declarative_component": "Varies across surveyed systems; includes logic programming, symbolic rules, knowledge graphs, program representations, and other explicit symbolic artifacts (survey reference listed in this review).",
            "imperative_component": "Varies across surveyed systems; typically includes deep neural networks (CNNs, transformers, RNNs) and learned search/recognition modules.",
            "integration_method": "Surveyed methods include modular architectures, knowledge injection, differentiable approximations to logic, hybrid training schedules, and symbolic solvers coupled to neural perception; the review references the survey as covering this space but does not detail individual methods.",
            "emergent_properties": "Surveyed benefits reported across the literature: improved compositional/generalization behavior when symbolic structure matches task demands, better interpretability, and sometimes improved sample efficiency on structured tasks.",
            "task_or_benchmark": "Wide-ranging across surveyed works: program synthesis, visual reasoning, logical QA, compositional generalization benchmarks (e.g., SCAN-like tasks), and other tasks requiring structured reasoning.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Survey notes that neuro-symbolic methods often improve out-of-distribution and compositional generalization when symbolic priors align with tasks, though the review cautions that purely neural metalearning and LLMs sometimes match or approach these behaviors.",
            "interpretability_properties": "Survey emphasizes that declarative components provide more explicit explanations and inspectable reasoning traces compared to opaque neural-only systems.",
            "limitations_or_failures": "Surveyed challenges include reliance on curated symbolic knowledge (scalability), difficulty of end-to-end differentiable integration, and potential brittleness when symbolic assumptions mismatch real data; the review echoes these limitations and frames them as empirical questions.",
            "theoretical_framework": "Taxonomic and pragmatic: different integration patterns yield complementary strengths and tradeoffs; the paper cites the survey to situate neuro-symbolic approaches within a broader taxonomy.",
            "uuid": "e435.4",
            "source_info": {
                "paper_title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning.",
            "rating": 2,
            "sanitized_title": "dreamcoder_bootstrapping_inductive_program_synthesis_with_wakesleep_library_learning"
        },
        {
            "paper_title": "Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI.",
            "rating": 2,
            "sanitized_title": "knowledgeinfused_learning_a_sweet_spot_in_neurosymbolic_ai"
        },
        {
            "paper_title": "Rebooting AI: Building Artificial Intelligence We Can Trust.",
            "rating": 2,
            "sanitized_title": "rebooting_ai_building_artificial_intelligence_we_can_trust"
        },
        {
            "paper_title": "A survey on neural-symbolic learning systems",
            "rating": 2,
            "sanitized_title": "a_survey_on_neuralsymbolic_learning_systems"
        },
        {
            "paper_title": "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems.",
            "rating": 1,
            "sanitized_title": "neurocompositional_computing_from_the_central_paradox_of_cognition_to_a_new_generation_of_ai_systems"
        }
    ],
    "cost": 0.0190315,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FROM FREGE TO CHATGPT: COMPOSITIONALITY IN LANGUAGE, COGNITION, AND DEEP NEURAL NETWORKS
4 Sep 2025</p>
<p>Jacob Russin jake_russin@brown.edu 
Equal primary contribution</p>
<p>Sam Whitman Mcgrath smcgrath20@fordham.edu 
Equal primary contribution</p>
<p>Danielle J Williams </p>
<p>Department of Computer Science
Brown University</p>
<p>Department of Philosophy
Fordham University</p>
<p>Department of Philosophy Psychological and Brain Sciences
Washington University in St. Louis</p>
<p>FROM FREGE TO CHATGPT: COMPOSITIONALITY IN LANGUAGE, COGNITION, AND DEEP NEURAL NETWORKS
4 Sep 202538526147552CD61210EFD62A522FB7C4arXiv:2405.15164v2[cs.NE]
Compositionality can be traced even further back in the history of philosophy, to Aristotle and the Sanskrit grammarians (Pagin and Westerståhl, 2010), but our focus here will be on the 20th century analytic tradition, in which this phenomenon was first named and analyzed(Putnam, 1995;Katz and Fodor, 1963).2 For discussion, see Pagin and Westerståhl (2010) and Partee (2004).3 These two readings might be tethered to a distinction between two different senses of "determination."Consider an analogy.Objectively speaking, the solution to the equation "a x b = ?" is determined by the values of "a" and "b" (as well as the nature of multiplication).Psychologically speaking, however, there are numerous ways that someone might determine this solution-by searching their memory, by guessing, or by actually multiplying the component values.Our question is which of these senses of determination, the objective or the subjective sense (so to speak), is involved in the compositionality principle.</p>
<p>Introduction</p>
<p>Deep neural networks (DNNs) have made breakthrough after breakthrough in artificial intelligence (AI) over the last decade, reproducing sophisticated cognitive behaviors from advanced gameplay in board games like Go (Silver et al., 2016), Chess (Silver et al., 2018), and Diplomacy (FAIR et al., 2022), to catalyzing achievements in mathematics (Romera-Paredes et al., 2024), science (Jumper et al., 2021), engineering (Merchant et al., 2023), and medicine (Omiye et al., 2023;Singhal et al., 2023).The large language models (LLMs) powering AI products like chatGPT (Brown et al., 2020;Bubeck et al., 2023;OpenAI et al., 2024) have shown remarkable proficiency in cognitive domains previously regarded as uniquely human, such as natural language syntax (Linzen and Baroni, 2021), critical reasoning and argumentation (Herbold et al., 2023), and computer programming (Chen et al., 2021;Bubeck et al., 2023).</p>
<p>In humans, these creative cognitive behaviors have traditionally been explained by postulating a special cognitive property: compositionality.The precise definition of compositionality varies by discipline, but intuitively it can be understood as the ability to compose familiar constituents (words, concepts, percepts) into novel complex combinations.In particular, cognitive scientists have often modeled human generalization behaviors with classical architectures that intrinsically guarantee that familiar constituents can be redeployed in novel constructions (Fodor and Pylyshyn, 1988).Theorists have argued that this explanatory framework is 'non-negotiable,' that compositionality is the "only game in town" for explaining the power and flexibility of human intelligence across many domains, including natural language syntax and semantics (Fodor, 1975).</p>
<p>Unlike classical symbolic AI systems, however, the neural networks powering recent advances are not designed to embody a strong assumption of compositionality, relying instead on acquiring capabilities by learning statistical regularities from data (Fodor and Pylyshyn, 1988;McClelland et al., 1986b;Rumelhart et al., 1986b;Smolensky et al., 2022).Whereas compositionality is integral to certain fundamental design features of classical computational architectures (e.g., predicate-argument structure, variable binding, constituent structure), these features are not native to standard DNNs.Traditionally, this has been lodged as an objection to the viability of neural networks as cognitive models (Fodor and Pylyshyn, 1988).However, modern DNNs' impressive performance in domains traditionally thought to require compositionality (e.g., language) calls this into question.Have today's models overcome previously hypothesized limitations?</p>
<p>Or are they merely imitating compositional behaviors through useful but imperfect heuristics?Do DNNs instantiate a novel form of non-classical compositionality?Or does their success in reproducing sophisticated cognitive behaviors show that there is a new game in town and that compositionality is negotiable after all?</p>
<p>In this chapter, we'll review and contextualize recent studies of compositionality in DNNs, with a particular focus on language.We begin in section 2.1 with a historical overview of the explanatory role of compositionality in philosophy, linguistics, and psychology, outlining its evolution from a property of interest primarily to logicians and philosophers to an empirically motivated psychological construct central to contemporary cognitive science.We then briefly describe the rise of connectionism in the 1980s and 90s (section 2.2) and the challenge that compositionality poses for neural networks (section 2.3).With this crucial context in place, we move to today's deep neural networks, reviewing key recent breakthroughs in replicating compositional behaviors (section 3).In particular, we argue that metalearning (section 3.2), or learning to learn, offers a novel perspective on how neural networks like LLMs (section 3.3) can come to reproduce the behavioral signatures of compositionality.To close, we discuss potential implications that this work may have for our understanding of human compositionality, and suggest avenues for future empirical and philosophical work (section 4).</p>
<p>Compositionality in Context</p>
<p>2.1 From Frege to Fodor Though the compositionality of human language and thought is a central commitment of contemporary cognitive science, its current explanatory role was not always evident.Appreciation of the significance of compositionality first emerged at a time when philosophy, psychology, logic, and linguistics were still intertwined, and it took nearly a century of philosophical investigation to cleanly excavate the question(s) that the compositionality principle purports to answer. 1   Gottlob Frege made indispensable contributions to the disentangling of these domains and was arguably the first philosopher to fully appreciate the explanatory potential of the compositionality principle (Dummett, 1993).In contemporary terms, the compositionality principle states that the meaning of a complex expression is determined by the meaning of its parts and the way they are combined.There are a number of different ways of precisifying this principle, 2 and two importantly different ways of interpreting its significance-as a claim about the metaphysics of meaning or as a claim about the psychological processes with which humans generate and grasp meanings.On the first interpretation, the compositionality principle asserts that the meaning of a complex expression (e.g. a sentence) is determined by or supervenes on the meanings of its parts (e.g.words), which are explanatorily or metaphysically fundamental.On the second, it expresses a psychological hypothesis about how language users themselves determine this meaning-namely, by assembling the meanings of the parts in accordance with tacitly known rules (Dummett, 1996). 3  Though Frege clearly appreciated the significance of compositionality and anticipated key contemporary arguments in its favor (Frege, 1956(Frege, , 1980)), it is not clear which of these two construals of the compositionality principle he intended.In fact, though it has been dubbed "Frege's Principle" (Cresswell, 1973), Frege never explicitly formulated the compositionality principle himself.The first philosopher to do so was Frege's one-time student, Rudolf Carnap, who named it after his former teacher (Carnap, 1988).</p>
<p>Carnap was the leading proponent of logical empiricism and, among other contributions, paved the way for what was arguably the central research program in 20th century formal semantics (Carnap, 1988;Montague, 1974).Carnap employed "Frege's Principle" as a crucial theoretical assumption.However, like Frege before him, Carnap was deeply skeptical that compositional formal languages could be treated as models of natural languages, like German, English, or Mandarin (Carnap, 1937).His theoretical ambition was not to explain everyday language use or uncover underlying psychological mechanisms, as a contemporary theorist might expect, but to develop artificial languages ideally suited to mathematical and scientific purposes (Harris, 2017).Compositionality was an indispensable feature of these artificial languages, but Carnap was well aware of the idealizations that they involved-abstracting away from central features of natural language, like context-sensitivity and ambiguity-and he consciously avoided putting them forward as models of "vernacular" human language.This was true for philosophers in the 'ideal language tradition' more broadly.They appreciated the power of mathematical logic and the importance of the compositionality principle, but did not advance their formal theories as psychological models or treat compositionality as an empirical hypothesis about human linguistic processing.</p>
<p>Ordinary Language Philosophy, the other major offshoot of the analytic tradition's 'linguistic turn' (Rorty, 1992), criticized the ideal language philosophers for this abstraction and focused their analyses on the messy realities of natural language use (Ryle, 1949).Leaving aside the movement's grander ambitions (Hacker, 1975;Horwich, 2013), philosophers in the ordinary language tradition made clear-cut contributions to linguistic theorizing, from J.L. Austin's work on speech acts (Austin, 1975) to H.P. Grice's pioneering contributions to the field of pragmatics (Grice, 1991).However, these philosophers were either skeptical or silent about the compositionality of natural language (see, for example, Gilbert Ryle's attack on the 'building block' theory of meaning in Ryle, 1957).</p>
<p>The tension between these approaches persisted into the early 1960s, with ideal language philosophers developing compositional models, but abstracting away from natural language use, and ordinary language philosophers advancing analyses of natural language, but rejecting or simply ignoring the principle of compositionality.This contrast formed the philosophical backdrop for Noam Chomsky's revolution in the field of syntax (Chomsky, 1957(Chomsky, , 1965)), which would help inaugurate the 'cognitive turn' and fundamentally alter the role of compositionality in linguistic theorizing.</p>
<p>Chomsky rejected the key assumption that both groups of philosophers shared-that formal mathematical languages could not be treated as psychological models of ordinary linguistic knowledge.This allowed him to synthesize their approaches, using the "methodological toolkit" developed by philosophers interested in formal logic, from Frege onwards (Harris, 2017), but taking them seriously as models of the syntax of natural languages (Chomsky, 1957;Katz, 1971).Chomsky did not focus on questions of meaning, but the synthesis that his work enacted was a crucial step in the emergence of the compositionality principle as an empirical hypothesis about human linguistic processing.Indeed, Chomsky's synthesis was promptly extended to formal semantics by Richard Montague and other key contributors (Katz and Fodor, 1963;Davidson, 1965).Montague explicitly argued that there is "no important theoretical difference between natural languages and the artificial languages of logicians," allowing him to treat rule-based semantic systems of the kind pioneered by Carnap as psychological models of human linguistic competence (Montague, 1974, p. 15). 4   It would be hard to overstate the impact that this shift had on linguistic theorizing.In effect, it transformed formal syntactic and semantic theories from abstract objects of study for logicians, mathematicians, and philosophers into psychological constructs that underlie and explain the linguistic capacities of even the youngest children capable of speech.With this shift, the compositionality principle took on a newfound significance and key role in explaining human linguistic behavior.</p>
<p>Chomsky drew attention to the creativity of human language use and its open-ended, potentially infinite expressive capacity, which must be achieved through finite means (Chomsky, 1966(Chomsky, , 2020;;Frege, 1980).To use a classic example discussed by Katz and Fodor (1963), in even the most routine linguistic interactions we regularly encounter novel sentences, yet fluidly and effortlessly comprehend their meaning and generate appropriate responses.This open-ended creative capacity seems to cry out for explanation. 5How are humans capable of understanding sentences that they have never before encountered?</p>
<p>The answer, in one word, is compositionality.If the meaning of an unfamiliar sentence is determined by the meaning of its parts, their mode of combination, and nothing else, then knowing the meanings of component words and the grammar of the language will suffice for grasping this novel meaning.Appealing to tacit knowledge (Fodor, 1968;Evans, 1985) thus provides an explanation of the human capacity to understand novel sentences-we derive the meaning of novel constructions by recombining known components according to known syntactic rules.</p>
<p>In fact, it can seem that tacit knowledge of a compositional grammar is the only adequate explanation of this capability.Chomsky argued that humans' creative capacity is essentially unbounded (Chomsky, 1957).To take a classic example, if we idealize away from constraints of memory and attention (as well as the finitude of a human lifetime!), a compositional subject could go on adding new conjuncts to the end of a well-understood sentence ad infinitum.Provided that they understand the meaning of the new conjunct (and conjunction itself), they will also understand these newly formed sentences.It is this open-endedness that makes an internalized compositional system seem like "the only game in town."Alternative explanatory approaches of the time, like behaviorism, struggled to account for this form of unbounded extension.For Chomsky, Montague, Fodor, and others, the solution was clear: posit internal compositional mechanisms that support these behavioral capacities.This abductive inference to the best explanation forges a close link between creativity, productivity, and compositionality.Particular observations about productive or systematic behaviors in humans motivate the ascription of internal compositional mechanisms to ordinary speakers.Such arguments commit theorists to interpreting the compositionality principle as a psychological, rather than a metaphysical, thesis. 6This point deserves emphasis.To serve as a robust explanation of human language comprehension and observable linguistic behavior, these internal compositional mechanisms must be 'psychologically real' and actually employed as humans determine the meanings of novel expressions. 7For the proposed explanation to work, it is not enough that the language itself admits of a compositional analysis-after all, one and the same language may admit both compositional and non-compositional models.</p>
<p>To see the point, consider a simplified 'toy' language (Evans, 1985) with only 10 names (a, b, c...) and 10 predicates (F, G, H...), generating 100 possible sentences (Fa, Fb, Ga...).This language can be provided with a compositional model, along familiar lines.However, every admissible sentence in this language could also be generated by a very different, though 'extensionally equivalent,' grammar that has 100 different axioms specifying the meaning of each.How can we tell which of these models a speaker of this minimal language possesses?Many philosophers, from W. V. O. Quine (1970) to Crispin Wright (1981), were skeptical that such questions could be made empirically or behaviorally tractable.Evans (1985), however, argues that there are clear empirical considerations that justify the ascription of the compositional system rather than the non-compositional one.In principle, we could scrutinize the speaker's internal states and attempt to detect compositional structure.In practice, (and in keeping with the methodological assumptions of researchers in the Chomskyan tradition) we can instead rely on a wider class of behavior-compositional generalization behavior involving the introduction of novel names or predicate.If you introduce a new sentence, Fz, will the speaker be able to understand Gz and Hz?If so, this is evidence that they understand the language compositionally.Evans' proposal about how to operationalize compositionality is very similar to those used in contemporary machine learning research (see section 3.1), and it demonstrates the central role that linguistic behavior plays in motivating cognitive explanation in terms of the compositionality principle.</p>
<p>These theoretical developments close the gap between early philosophical theorizing about compositionality and the ongoing debates in contemporary cognitive science.Compositionality is understood as an explanatory psychological property, operationalized in terms of the specific kinds of productive and systematic generalization behaviors that humans exhibit (Evans, 1985).In each case, the characteristic 'behavioral signatures' of compositionality are taken as evidence diagnostic of internalized compositional mechanisms, which serve to explain the behaviors in question.As Chomsky often emphasizes, this means that even the most abstract reaches of formal linguistics are properly understood as a branch of cognitive psychology (and ultimately biology, though developing an account of how symbolic systems are implemented in the brain is sometimes treated as a subsidiary enterprise; see Chomsky, 1968).</p>
<p>Two further consequences of the Chomskyan revolution deserve mention before concluding this historical overview.First, the rise of this new explanatory paradigm brought with it a notable resurgence of nativism, as Chomsky and others argued that the linguistic data to which children are exposed is insufficient for recovering the kind of fine-grained syntactic structures necessary for explaining human linguistic competences (Laurence and Margolis, 2001).As we will briefly discuss in 4.2, the rise of DNNs and LLMs may make novel contributions to this long-standing philosophical issue (Buckner, 2018(Buckner, , 2019(Buckner, , 2023b)).And second, though we have framed the discussion here in terms of the meaning of sentences, this period saw a distinct shift in focus from language to thought.This 'cognitive turn' transposed key debates about language into questions about the structure of cognition itself (Burge, 2007).Compositionality was no exception.Jerry Fodor influentially argued that the compositional structure of natural languages is a reflection of the compositional structure of thought, reintroducing the language of thought hypothesis to the philosophical mainstream (Fodor, 1975;Pelletier and Roques, 2017).On this view, human cognition is compositional from the ground up-it is a feature of the mind's basic operating system, not just an artifact of one specific domain (language).Across this historical evolution, compositionality arrives in the position familiar to cognitive scientists today: a core property of human cognition, motivated by its non-negotiable role in explaining the creativity, systematicity and productivity of language and thought.From this perspective, the inability to adequately account for compositionality is a fatal flaw, which Fodor wielded like a sledgehammer to smash rival theories of meaning and defend his language of thought hypothesis.Over the years, he would take this sledgehammer to the use theory of meaning (Fodor and Lepore, 2001;Fodor, 2003), conceptual role semantics (Fodor and LePore, 1993), prototype theory (Fodor and Lepore, 1996), and most importantly for our present purposes, neural network models of cognition (Fodor and Pylyshyn, 1988).</p>
<p>The Rise of Connectionism</p>
<p>In the 80s and 90s, a series of key advances in neural networks, such as the invention of Hopfield networks (Hopfield, 1982) and the backpropagation algorithm (Rumelhart et al., 1986a), led many to abandon the classical, Fodorian picture and turn to an alternative view of the mind (McClelland et al., 1986b;Rumelhart et al., 1986b).The connectionist movement was initially motivated by considerations other than compositionality, such as the basic computational structure of biological brains.Proponents sought to demonstrate that sophisticated cognitive behaviors could emerge from a network of simple interconnected units (or "neurons").Though these units abstract away from a great deal of what is known about biological neurons, they implement a similar kind of computation: each unit performs a weighted sum of its inputs and applies a threshold or nonlinearity, much as biological neurons integrate dendritic inputs and spike at a certain threshold (Hodgkin and Huxley, 1952;McCulloch and Pitts, 1943;O'Reilly et al., 2012).Early work demonstrated how these networks could learn by incrementally updating their weights (or "synapses") to improve overall performance on a given task (Rumelhart et al., 1986b).</p>
<p>Connectionists sought to construct models that explained behavioral phenomena without building in classical representational elements (e.g., rules, variables, symbols, propositions, constituent structure), instead emphasizing how useful representations could emerge in a neural networks' distributed patterns of activity over the course of learning (Rumelhart and McClelland, 1986).This approach stood in tension with existing explanations based on the principle of compositionality, since standard distributed representations have no natural notion of "parts" or "constituents" (although key later work by Smolensky, 1990, would explore this possibility).The network's objective is to discover and deploy whatever representational elements are most useful for solving a given problem.These emergent representations may not align with the kinds of features a linguistic or cognitive theorist might put forward, and may not even afford succinct interpretations in psychological terms at all (Smolensky, 1986).In fact, as we have discussed elsewhere (McGrath et al., 2023a), many early connectionists were eliminativists about classical symbolic constructs (Rumelhart and McClelland, 1986;Ramsey et al., 1990;McClelland and Patterson, 2002), arguing that the rise of neural networks repudiated traditional approaches to cognitive science and constituted a paradigm shift for the field (McClelland et al., 1986a).</p>
<p>Neural networks thus seemed to offer a radically new approach to the study of cognition, providing novel explanations for many different kinds of behavioral phenomena (Rumelhart et al., 1986b), including perceptual processes (McClelland and Rumelhart, 1981), memory processes (Hopfield, 1982;McClelland et al., 1995), executive and decision-making processes (Cohen et al., 1990;Miller and Cohen, 2001), and language processes (Dell, 1986;Rumelhart and McClelland, 1986).The emergence of the connectionist movement generated a great deal of excitement and controversy across the cognitive sciences, including within philosophy (Ramsey et al., 1991), where Fodor was waiting (with his sledgehammer).</p>
<p>The Compositionality Challenge</p>
<p>In response to the threat posed by connectionism, Fodor penned a long critique of the movement with the psychologist, Zenon Pylyshyn (Fodor and Pylyshyn, 1988).Their highly influential article, "Connectionism and Cognitive Architecture: A Critical Analysis," develops a dilemma for proponents of neural network models of cognition. 8The first horn of this dilemma is the claim that core properties of neural networks make them incapable of replicating the behavioral signatures of compositionality, rendering them empirically inadequate as models of human cognition.This claim has real predictive upshot-if it is correct, neural networks should fail to match human performance on the specific tasks that demand compositional generalization (Marcus, 1998).The second horn, which we will call the 'mere implementation' objection, is subtler and involves more overtly philosophical considerations.It alleges that, even if DNNs were to replicate the behavioral signatures of compositionality, this would only mean that they had merely implemented a classical symbolic architecture (McGrath and Russin, 2024), rather than providing a genuine alternative explanation.Taken together, these two horns comprise the compositionality challenge for artificial neural networks.This first horn of Fodor and Pylyshyn's argument draws on two key cognitive properties that are taken to require compositionality: productivity and systematicity.As we have seen, classical symbolic accounts provide a natural mechanism for productively generating an unbounded set of novel thoughts through the recombination of a finite pool of primitive elements.Connectionist architectures, on the other hand, do not seem to build in the kind of combinatorial constituent structure capable of supporting this form of unbounded productivity.They also struggle to capture the systematicity of cognition-the apparently intrinsic connection between the ability to comprehend and generate specific sets of thoughts/sentences.To use the classic example, it seems that anyone capable of understanding the sentence "John loves Mary" should be capable of understanding the sentence "Mary loves John."The classical account again provides a natural explanation-the meaning of the second sentence is a function of the very same components and mode of combination as the first.Therefore, any compositional learner capable of understanding the first will be able to redeploy this knowledge in understanding the second.Fodor and Pylyshyn argue that in a connectionist network without constituent structure, on the other hand, each of these individual acts of understanding would be an independent capacity.There would therefore be no inherent reason why they should hang together-a network trained on one could easily misunderstand the other.For both productivity and systematicity, Fodor and Pylyshyn's conclusions are the same-without underlying compositional mechanisms, connectionist networks will fail to replicate these core features of human language and cognition.</p>
<p>The second horn of Fodor and Pylyshyn's dilemma draws on the possibility of implementing a classical symbolic system in an artificial neural network.Even if a connectionist network did manage to exhibit systematic and productive behaviors, they argue, this would have to be because it had merely implemented a classical symbolic system, undermining any claim to revolutionary implications or novel insight into the nature of the mind (for further discussion, see section 4.3).Thus, the connectionist is left with no way out: either their models are empirically inadequate, or they are mere implementations, which may prove informative for theorists interested in how symbolic systems are realized in the human brain (McLaughlin, 1993), but will not contribute to higher-level cognitive theory.As should be clear, this argument is a species of the genus described in section 2.1-compositionality is treated as an empirically motivated psychological construct, necessary for the explanation of certain key human behaviors.</p>
<p>The compositionality challenge ignited a heated debate and drew a wide array of rebuttals and clarifications (Chalmers, 1993;Hadley, 1997;Horgan and Tienson, 1991;Matthews, 1994;Ramsey et al., 1990).Some connectionist researchers-most notably Paul Smolensky-tried to meet the 'mere implementation' objection head on by building neural networks with internal compositional structure that did not merely recapitulate classical architectures (Smolensky, 1990(Smolensky, , 1991)).This led to further debate about just what kind of explanation compositionality offered and what internal compositional structure was rightly considered 'non-negotiable' (MacDonald and MacDonald, 1991;van Gelder, 1990).Our primary goal in this paper-and the sole focus of the review in section 3-is to survey recent findings suggesting that contemporary DNNs have plausibly surmounted the first horn of this dilemma.In 4.3, we also briefly evaluate whether this means that they fall on the second horn (for further discussion, see McGrath et al., 2023a;McGrath and Russin, 2024).</p>
<p>Over time, the debates over connectionism waned, moving from the center to the periphery of cognitive science.Neural networks, however, continued to progress, and exploded into new prominence with the inception of the deep learning revolution (Krizhevsky et al., 2012;LeCun et al., 2015;Goodfellow et al., 2016), instilling the debates over connectionism and compositionality with renewed importance.</p>
<p>In-Context Compositionality</p>
<p>Many advances in neural networks have accompanied the deep learning revolution, including the introduction of the transformer architecture (Vaswani et al., 2017).However, the single most important factor that differentiates modern neural networks from their connectionist predecessors is their scale: modern networks are deeper (i.e. more layers), have many orders of magnitude more learnable parameters (weights), and are trained on many orders of magnitude more data.This is possible due to a combination of advances in hardware (e.g., GPUs) and the availability of large amounts of data from the internet (e.g., Wikipedia).These are merely quantitative differences, but they have led to qualitative increases in these systems' capabilities (Brown et al., 2020;Caballero et al., 2023;Kaplan et al., 2020;Wei et al., 2022b).</p>
<p>Although these developments have led to remarkable gains in performance across many domains, it is unclear a priori whether they should have a significant impact on DNNs' underlying compositionality.The features of neural networks that led theorists like Fodor and Pylyshyn to conclude that they were not compositional seem prima facie to be largely unchanged in modern deep networks: DNNs are still extremely unbiased learners with no explicit rule-like representations or built-in sensitivity to constituent structure.9As we have seen, accounting for compositionality is a pressing theoretical challenge-if neural networks are to provide illuminating accounts of human cognitive capacities, they must be able to model the creative, productive, and systematic generalization behaviors exhibited by humans.</p>
<p>Our central question in this review will be an empirical one: can contemporary neural networks replicate the behavioral signatures of compositionality?We will argue that although many current neural networks (including ones leveraging modern architectures such as the transformer) still seem incapable of capturing these behavioral signatures, novel approaches have shown that it is possible to endow neural networks with the properties that are required to produce them.In particular, recent work has shown that metalearning neural networks, which learn how to learn new tasks given in context, can reproduce key compositional generalization behaviors.These results cast doubt on the empirical predictions made by classical theorists that neural networks would never be capable of generalizing systematically or productively, and suggest that DNNs may meet the challenge posed in the first horn of Fodor and Pylyshyn's dilemma.</p>
<p>Operationalizing Compositionality</p>
<p>An empirical investigation of compositionality in neural networks requires that we operationalize it by specifying exactly what kinds of behaviors it is supposed to afford-we need a set of behavioral measures that allow us to determine whether a given learner is compositional (Garson, 1994).Broadly speaking, compositionality has been evaluated in DNNs by measuring the degree to which they generalize under specific conditions.This reflects the explanatory role played by compositionality for classical theorists (Katz and Fodor, 1963;Evans, 1985): compositionality is a property of the underlying cognitive system whose existence we can infer precisely because it explains certain kinds of generalizations (e.g., systematic and productive ones) that we observe in the system's behavior.</p>
<p>The behavioral tasks used to evaluate the compositionality of DNNs typically operationalize it as the ability to generalize from a given set of training samples to a given set of test samples.These train and test sets are crafted such that a compositional learner ought to be able to infer the underlying grammatical structures or rules that govern the training examples, and then leverage that knowledge to systematically generalize to the test samples. 10For example, the model might be trained on the meanings of instructions using novel verbs, like "dax," "flug," and "flug twice," and then tested on an unseen instruction "dax twice."If the learner is compositional, it should be able to extract the meanings of "dax," "flug," and "twice" from the training samples, as well as knowledge of how adverbs modify verbs in the underlying grammar.11If the model infers this underlying structure correctly, it should be able to generalize to the test sample ("dax twice") by composing the familiar meaning of the verb "dax" with the familiar meaning of the adverb "twice" according to the known grammatical rules (Lake and Baroni, 2018).On the other hand, a non-compositional learner-for example, a stimulus-response learner that simply stored each training sample in a lookup table (see Figure 1)-would have no basis upon which to generalize to the novel expression "dax twice."</p>
<p>In a seminal study on compositionality in modern DNNs, Lake and Baroni (2018) leveraged these principles to develop the SCAN task, a sequence-to-sequence task where the model takes instructions as inputs (e.g., "walk left"), and must return the corresponding sequence of actions (e.g., TURN_LEFT WALK).To test Figure 1: Are neural networks stimulus-response learners, or do they learn to solve problems with compositional algorithms?Two extreme possibilities are depicted for natural language (top) and simple arithmetic (bottom).In either case, a stimulus-response learner (left) would simply memorize a lookup table where entries corresponded to entire inputs/outputs, ignoring any compositional structure.A compositional learner (right) would solve the same problems by composing familiar elements according to known rules, allowing generalization.compositional generalization, the researchers created a systematic difference between the train and test sets: one of the four primitive verbs ("jump") was left out of the training set except in its simplest form ("jump" → JUMP).This created a scenario like the one discussed above, where a model could in principle infer the underlying grammar from all of the complicated constructions containing the other primitive verbs (e.g., "walk twice," "look twice," etc.), and combine this with knowledge of the action associated with "jump" (JUMP) to generalize to the analogous constructions containing "jump" (e.g., "jump twice").</p>
<p>The results showed that even though standard recurrent neural networks (RNNs) performed nearly perfectly when the train and test sets were randomly split, these same networks performed quite poorly on this compositional generalization test (about 1.2% accuracy reported in the original study).This was true for all of the models they tested, and follow-up work showed that other standard architectures such as convolutional neural networks (Dessì and Baroni, 2019) and transformers (Keysers et al., 2020) exhibit similar reductions in performance on the compositional test (though less extreme in the case of the convolutional neural network).Analogous experiments in humans using a modified version of the same task have shown that humans perform quite well (Lake et al., 2019;Lake and Baroni, 2023).A number of subsequent machine learning studies followed up on this work, creating similar tests of compositional generalization and showing that these standard DNNs fail to generalize compositionally in domains such as semantic parsing (Kim and Linzen, 2020), string manipulation with a probabilistic context-free grammar (Hupkes et al., 2020), mathematics (Saxton et al., 2019), natural-language translation (Dankers et al., 2022), and grounded language understanding (Ruis et al., 2020), among others.</p>
<p>Taken together, the studies that have evaluated the compositionality of standard DNN architectures show that while they excel at generalizing when train and test data are split randomly, they perform remarkably worse when there is a systematic difference between train and test.One way of understanding these results is in terms of classical cognitive theory-as vindicating theorists who held that compositionality should be a necessary consequence of cognitive architecture and consequently predicted that neural networks will inevitably fail to replicate the systematicity and productivity apparent in human performance (Fodor and Pylyshyn, 1988;Marcus, 1998).</p>
<p>Another (perhaps compatible) way of understanding these results is to take a statistical learning theory perspective.Neural networks are universal function approximators (Cybenkot, 1989;Hornik, 1991) that assume that the train data and test data are sampled from the same distribution, i.e., that the train and test data will be independent and identically distributed (i.i.d.).Viewed through this lens, the generalization behaviors that are taken to be diagnostic of compositionality (Evans, 1985;Fodor and Pylyshyn, 1988) are out-of-domain (o.o.d.) generalizations, and require extrapolation, rather than interpolation (Marcus, 1998(Marcus, , 2018)).For an unbiased learner like a standard neural network, this kind of generalization should be unlikely or impossible.</p>
<p>The extent to which a learner is expected to generalize beyond its training distribution is the extent to which it must make additional assumptions about the world.These assumptions, called 'inductive biases' in statistical learning parlance, can take many forms in neural networks, including architectural inductive biases or extra regularization terms in the loss function (e.g., weight decay, a regularization term that encourages a network's weights to be small).Standard neural networks do not make very many of these assumptions and thus tend to struggle when even the most superficial distributional shifts occur between train and test (Lake et al., 2017).For example, standard neural networks used in deep reinforcement learning agents trained on Atari games fail to generalize when superficial features of the game are changed (Kansky et al., 2017).It is not surprising, then, that they also fail in compositional generalization settings where there can be extreme distributional shifts from train to test.</p>
<p>However, recent work has investigated methods that endow neural networks with inductive biases that would facilitate their adaptation to these shifts and allow them to replicate the behavioral signatures of compositionality.In the following, we'll review recent work suggesting that neural networks can acquire such inductive biases via metalearning, or learning to learn.In the metalearning setting (Lake and Baroni, 2023), an inductive bias is imparted to a neural network by training it on a specific distribution of tasks (e.g., compositional generalization tasks), endowing it with an ability to learn new tasks in specific ways.Similarly, when neural networks such as LLMs are pretrained on a large amount of data, they can acquire an emergent ability to learn new tasks given in context (Brown et al., 2020;Bubeck et al., 2023).</p>
<p>In the following, we will review key findings from recent studies exploring compositionality in each of these settings, highlighting how they challenge the traditional view of compositionality in neural networks.Our review will largely focus on language, as this has been the domain in which compositionality has traditionally played a central explanatory role (see section 2.1).However, there has also been a recent surge of interest in compositionality in vision (Lepori et al., 2023;Zhou et al., 2024), multi-modal models (Castro et al., 2024;Hsieh et al., 2023;Lewis et al., 2023;Ma et al., 2023b,a;Mitra et al., 2023;Ossowski et al., 2024;Shukor et al., 2023), and in reinforcement learning (Bakirtzis et al., 2022;Jothimurugan et al., 2021;Liu and Frank, 2022;Žikelić et al., 2023), where similar issues have been explored.Furthermore, while we have chosen to focus our review on metalearning and LLMs, it should be noted that much recent work has pursued other approaches.For example, researchers have investigated whether certain architectural inductive biases can facilitate compositional generalization (Altabaa et al., 2023;Csordás et al., 2022;Kriete et al., 2013;Palangi et al., 2017;Rougier et al., 2005;Russin et al., 2020;Soulos et al., 2023;Schlag et al., 2019;Webb et al., 2024Webb et al., , 2023)), explored techniques involving data augmentation (Andreas, 2020;Cazzaro et al., 2024;Chai et al., 2023;Jiang et al., 2022) and the injection of structural knowledge via additional supervision or auxiliary tasks (Chakravarthy et al., 2022;Jiang and Bansal, 2021), and investigated the limits of standard transformers on compositional tasks (Dziri et al., 2023;Zhou et al., 2023b).</p>
<p>After the review, we discuss how these new approaches may inform our understanding of human compositionality, including their implications for its neural mechanisms and development.We conclude with a discussion of the 'mere implementation' objection, the second horn of Fodor and Pylyshyn's dilemma.</p>
<p>Metalearning</p>
<p>In metalearning (Binz et al., 2023;Griffiths et al., 2019;Sandbrink and Summerfield, 2024;Wang et al., 2018;Wang, 2020), a model is trained not on a single task but on a distribution of tasks, giving it the opportunity to learn how to learn new tasks more rapidly and to leverage regularities among tasks to generalize in specific ways.Metalearning can impart specific inductive biases to a model.For example, a metalearning agent trained on a distribution of navigation tasks may learn exploration strategies that are specifically effective for that distribution, thus allowing it to learn more efficiently how to navigate in similar environments.</p>
<p>Importantly, these inductive biases are imparted to the model without specifying any particular algorithmic features in advance (McCoy et al., 2020).Instead, the researcher can simply define the set of learning problems at which the model should excel and rely on the optimization process during metalearning to discover whatever inductive biases are useful for accomplishing these behaviors.</p>
<p>A number of metalearning methods have been developed for use with neural networks (Bengio et al., 1991(Bengio et al., , 1995;;Duan et al., 2016;Finn et al., 2017;Hospedales et al., 2020;Schmidhuber, 1987).In many of these methods, the model learns to implement a separate "inner-loop" or "in-context" learning (ICL) algorithm in its forward activation dynamics, distinct from the "outer-loop" or "in-weight" learning (IWL) algorithm used to train the network in the first place.In this case, the network's in-context learning algorithm "learns" within the flow of information from its inputs to its outputs (Duan et al., 2016;Lake, 2019;Lake and Baroni, 2023;Santoro et al., 2016;Wang et al., 2018).The network's weights are trained such that the forward pass of the model can take some study examples as inputs in context and learn from them to produce the correct generalization behavior on a test query.For example, a sequence-processing network such as an RNN or a transformer can be trained on inputs that consist of (1) a set of study examples along with their labels, and (2) a single test query whose label is not given, but must be inferred.In this case, the network is trained to produce a prediction about the label corresponding to the test query, conditioned on the set of training samples and their labels.The model is then evaluated by freezing its weights and testing on new study examples and a new test query drawn from another task.</p>
<p>In this setting, we can distinguish the IWL algorithm that determines the updates to the network's weights during metalearning, and the ICL algorithm that is implicitly implemented in the network's activation dynamics.The ICL algorithm does not require updates to the weights of the network to learn (i.e., it can still learn a new task when the weights are frozen).Crucially, these two distinct learning algorithms can have different properties.For example, the ICL algorithm may have a completely different updating procedure or effective learning rate, or may embody inductive biases that are not present in the original IWL algorithm.</p>
<p>This suggests a novel perspective on how a neural network might instantiate compositionality without incorporating explicit classical symbols: by (meta)learning an ICL algorithm with an inductive bias for compositional generalization.Even if the usual IWL algorithm used by neural networks lacks compositionality, as theorists such as Fodor and Pylyshyn (1988) and Marcus (1998) suggest, it is still possible that the ICL algorithm that a network comes to implement in its activation dynamics will exhibit compositionality.Lake and Baroni (2023), following up on earlier work (Lake, 2019), applied this approach to compositional generalization problems such as the SCAN task (Lake and Baroni, 2018), and showed that a metalearning transformer was capable of near-perfect accuracy on the task.The transformer was trained on a distribution of tasks, each a compositional generalization problem in its own right.In particular, each task had a few samples from the grammar that were in principle sufficient to infer the rules and produce the correct answer on the test sample.The model was trained on some generalization tasks, and tested on ones that it had not seen.The results showed that the transformer was capable of metalearning to implement a compositional learning algorithm in its activation dynamics, achieving near-perfect accuracy on held-out compositional generalization tasks that were not seen during metalearning.</p>
<p>These results demonstrate a novel strategy for endowing neural network models with the property of compositionality, and for overcoming the problem of o.o.d.generalization.After metalearning was complete, the model was given an o.o.d.problem in context, and its ICL (inner-loop) algorithm exhibited an ability to generalize.However, from the perspective of the IWL (outer-loop) algorithm, the problem was familiar (i.i.d.), because it was trained on similar o.o.d.generalization tasks during metalearning.Thus, metalearning creates a scenario where two kinds of generalization can coexist simultaneously: the IWL algorithm is simply generalizing i.i.d., while the ICL algorithm is capable of generalizing o.o.d.This o.o.d.generalization is possible because of the inductive biases imparted to the ICL algorithm throughout metalearning.Lake and Baroni (2023) conclude that although their metalearner could generalize on o.o.d.problems that were similar to those seen during metalearning, it could not generalize to completely new kinds of problems (i.e., ones that were o.o.d. with respect to the metalearning dataset).One way of interpreting this finding is to maintain that the kind of compositionality that humans exhibit is truly o.o.d., so the model's failure shows that it is inadequate as a model of human compositionality.However, another interpretation invites a reexamination of human compositionality and suggests that it too could be a property of an ICL algorithm, in this case implemented by the human brain (Russin et al., 2024a).This would challenge the claim that humans are capable of generalizing compositionally in completely novel domains, instead establishing the weaker claim that humans generalize o.o.d., but only in specific domains determined either by evolutionary history or by prior learning experiences.</p>
<p>Large Language Models</p>
<p>The original tasks used to evaluate compositionality (e.g. the SCAN dataset; Lake and Baroni, 2018) were developed at a time when most models were trained from scratch to perform a specific task, such as machine translation (Bahdanau et al., 2014;Vaswani et al., 2017).However, researchers soon began to utilize self-supervised methods that did not require human labels (e.g., annotations, translations, etc.), thus allowing models to be trained on the huge amounts of unlabeled text available on the internet.The dominant self-supervised task has been language modeling, where a neural network parameterizes a distribution over sequences of natural language text.For example, in causal language modeling this distribution is expressed in terms of the probability of the next token given the entire preceding context.Researchers found that as transformer models were scaled up (in terms of number of parameters and amount of data) language modeling performance continued to improve.This meant that better performance could be achieved if LLMs were pretrained on unlabeled text and then fine-tuned on the specific tasks of interest, rather than training from scratch on the specific task (Devlin et al., 2019;Peters et al., 2018;Radford et al., 2019).</p>
<p>Researchers then discovered that at even larger scales (e.g., at the scale of GPT-3, 175 billion parameters), you could in some cases forgo the fine-tuning step and simply ask these LLMs to do a new task in context by providing a few examples of the task to be performed (Brown et al., 2020).For example, rather than fine-tuning a pretrained model to predict whether a human-labeled restaurant review was positive or negative, you could prompt the model with a few study examples consisting of reviews and their corresponding labels, and evaluate it on a test query-an unlabeled review.These emergent ICL abilities allow LLMs to flexibly solve new tasks that are explicitly instructed or demonstrated with a few examples, and is further improved when models are fine-tuned on human-generated instruction-following datasets (Wei et al., 2022a) and with reinforcement learning by human feedback (RLHF; Ouyang et al., 2022).This created yet another paradigm shift in natural language processing: very large models are trained on very large datasets and then tested in context on specific tasks.</p>
<p>From a theoretical perspective, this "pretrain-test" paradigm (Hupkes et al., 2023) has important similarities with the metalearning setting described above.In metalearning, models acquire inductive biases by training on a distribution of tasks.The pretraining step in LLMs, where they are trained to predict the next word on a very large dataset, can be seen as training on a distribution of tasks (see Figure 2; Brown et al., 2020;Chan et al., 2022;Radford et al., 2019).Sometimes this prediction will look more like guessing based on frequencies (e.g., memorizing a common idiom), but other times the task can involve more sophisticated behaviors such as understanding recursion in computer programming.There are doubtless many instances in these large datasets where the best way to predict the next word is to learn something in context, and perhaps even to compose novel concepts in context (see "in-context composition" in Figure 2).Thus, although it is easy to dismiss next-word prediction as a simplistic or purely statistical training objective, upon further reflection one can see how it might put pressure on the model to develop an ICL algorithm in its forward activation dynamics, and perhaps even to endow this ICL algorithm with the property of compositionality.</p>
<p>LLMs at this scale perform quite well on many kinds of tasks that seem to require compositionality.As anyone who has interacted with chatGPT or similar products knows, these models are capable of writing whole paragraphs of coherent text about almost any topic, including novel concepts or made-up words that are included in the prompt.They can construct sentences with sophisticated grammatical structure (Linzen and Baroni, 2021), and can put together well-reasoned arguments (Herbold et al., 2023).They even recapitulate some of the syntactic phenomena thought to require innate grammatical constraints such as syntactic island effects (Wilcox et al., 2023, although see Lan et al. 2024).They are also adept at computer programming, and are capable of writing novel functions or scripts (e.g., involving compositions of known functions) to accomplish a goal given in natural language instructions.These models have achieved state-of-the-art performance on natural language inference tasks such as recognizing logical entailment (Brown et al., 2020), and have even demonstrated human-like tendencies in analogical reasoning tasks such as Raven's Progressive Matrices (Webb et al., 2023), which arguably require compositionality as well.</p>
<p>Researchers have also evaluated these models on the tasks designed specifically to test compositional generalization capabilities (Lake and Baroni, 2018).As in the metalearning setting, the evaluation works slightly differently than in the original SCAN dataset, where models were trained from scratch on tens of thousands of examples without "jump" and tested on instructions containing "jump."In the pretrain-test setting, the pretrained LLMs are given just a few study examples from the SCAN task in context and asked to generalize to new test samples that are also given in the same input.The compositional generalization test can be replicated by excluding certain kinds of examples from the context (e.g., longer examples, or more complicated constructions involving "jump").When GPT-3 (code-davinci-002) was evaluated with a standard prompt on a compositional split of the SCAN task in this way, it only achieved 16.7% accuracy (Zhou et al., 2023a).However, the inclusion of novel prompting methods improved performance to a near-perfect accuracy of 99.7%.These prompting methods, called "chain-of-thought" prompting (Wei et al., 2023) and "least-to-most" prompting (Zhou et al., 2023a), have been shown to improve performance on many reasoning tasks.In chain-of-thought prompting (Kojima et al., 2023;Wei et al., 2023), the model is asked to "think step-by-step," and/or given examples that show intermediate steps of reasoning.In least-to-most prompting (Drozdov et al., 2022;Zhou et al., 2023a), these in-context examples are simply given in order of least difficult to most difficult.Many other prompting strategies have been developed to further improve performance on similar tasks (Huang and Chang, 2023;Nye et al., 2021;Press et al., 2023;Yao et al., 2023).</p>
<p>The fact that the prompting strategy makes such a difference in the resulting measures of performance may be taken to indicate that the compositional generalization exhibited with certain prompts is fragile and cannot reflect a genuine competence (Bender and Koller, 2020;Bender et al., 2021;Lanham et al., 2023;Turpin et al., 2023;Webson and Pavlick, 2022).However, it is well understood that human performance depends in large part on the instructions they are given (Evans et al., 1994), and the specific content or domain on which their abilities are tested (Klauer et al., 2000;Wason, 1968).LLMs exhibit similar content effects (Dasgupta et al., 2023), highlighting the need for fair comparisons between humans and machines (Firestone, 2020;Buckner, 2023a).</p>
<p>Aside from these language-specific compositional generalization settings, much work has focused on evaluating whether large vision models or vision-language models exhibit similar compositional abilities (Ma et al., 2023b;Conwell and Ullman, 2022;Zheng et al., 2024).The results here have been mixed, suggesting a dissociation between the performance measured on compositional generalization problems developed in language-specific settings and those developed in other domains.</p>
<p>Figure 2: Predicting the next word on a large corpus of text can be seen as a kind of metalearning (Brown et al., 2020;Radford et al., 2019), in which the LLM trains on tasks requiring diverse capabilities such as memorization of factual knowledge, idioms, inflectional morphology (past tense), programming, and identifying the indirect object of a sentence (Wang et al., 2022).In some cases, successful prediction of the next word may require in-context composition of novel elements.As in metalearning, a distinction can be made between the IWL algorithm training the weights of the network, and an implicit ICL algorithm that operates on information given in context to make inferences relevant to predicting the next word(s).</p>
<p>Taken together, the impressive performance of LLMs shows that at least in some cases, the behavioral signatures of compositionality can emerge from the ICL algorithm implemented in the forward activation dynamics of a large model trained to predict the next word on a large corpus of text.Similar to the studies using metalearning models (Lake, 2019;Lake and Baroni, 2023), these LLMs demonstrate that even though the IWL algorithm may not itself be capable of compositional (o.o.d.) generalization, it can endow an emergent ICL algorithm with this capability.This again shows that the o.o.d.generalization problem solved by the ICL algorithm may be an i.i.d.generalization problem from the perspective of the original IWL algorithm, which has probably had many opportunities to learn how to learn similar problems in context from its training set (see Figure 2).</p>
<p>Can these findings about LLMs inform us about human compositionality?It is of course difficult to make any inferences from LLMs to humans because there are so many differences between them.For example, transformers are not biologically plausible in many ways, and the scale of LLM training datasets is orders of magnitude larger than a human lifetime's worth of language data (Frank, 2023;Linzen and Baroni, 2021;Pavlick, 2023;Warstadt and Bowman, 2024).However, these results do show that in principle it is possible to metalearn an ICL algorithm capable of compositional generalization by simple self-supervised learning on natural language.Furthermore, in contrast with explicit metalearning (Lake and Baroni, 2023), the impressive compositional behaviors of LLMs show that the distribution of tasks on which a metalearner is trained does not need to be deliberately contrived to encourage compositionality-strong compositional generalization abilities emerge in these models even when they are trained on unstructured text.This suggests that humans, who also learn from relatively unstructured experiences, could plausibly metalearn throughout development to implement a compositional ICL algorithm as well (Russin et al., 2024a).</p>
<p>Discussion</p>
<p>Classical cognitive theorists like Chomsky (1965) and Fodor (1975) developed compelling arguments that compositionality is a central property of human cognition, crucial for explaining the creativity, productivity, and systematicity of language and thought (see section 2.1).Neural network modeling was dismissed as an empirically inadequate explanatory paradigm because these models seem to lack the kind of combinatorial constituent structure intrinsic to classical architectures based on atomic symbols and governed by syntactic rules (see section 2.3; Fodor and Pylyshyn, 1988).</p>
<p>Even as deep neural networks blew past competing frameworks in virtually every area of AI, initial investigations into their compositionality seemed to confirm traditional intuitions that they were not capable of replicating the kinds of compositional generalization that humans exhibit (see section 3.1; Lake and Baroni, 2018;Kim and Linzen, 2020).One way of understanding these initial findings is from the statistical learning perspective: compositional generalization requires out-of-distribution (o.o.d.) generalization or extrapolation, but neural networks assume that train and test data will be independent and identically distributed (i.i.d.).Standard neural networks fail on these problems, then, because they lack the strong inductive biases that would allow them to generalize outside of their training distributions.Some have taken these results to demonstrate that neural networks will never achieve compositionality on their own and concluded that they must therefore be augmented with classical symbolic processes ("neuro-symbolic hybrids;" Marcus and Davis, 2020;Marcus, 2020).</p>
<p>More recent advances complicate this picture, suggesting alternative ways in which strictly neural models might explain the same generalization behaviors (Baroni, 2020).In particular, recent results suggest that metalearning (section 3.2) and large-scale pretraining in LLMs (section 3.3) can endow neural network models with sophisticated in-context learning abilities, and that these abilities can in some cases capture compositional generalization behaviors.</p>
<p>It can be argued that none of these neural networks are actually generalizing outside of their training distributions, and that they have therefore failed to explain human generalization capabilities.However, another perspective on these results is that they show that the generalization behaviors taken as diagnostic of compositionality in humans are possible without strongly out-of-domain generalization.</p>
<p>More generally, these findings provide a novel perspective on human compositionality, suggesting that metalearning and in-context learning may be important principles for explaining key aspects of these generalization behaviors in humans.In the following, we briefly discuss how these principles can provide novel empirical predictions about the neural mechanisms and development of human compositionality.</p>
<p>Metalearning and Neural Mechanisms</p>
<p>The metalearning approach hypothesizes that human compositionality is a property of an in-context learning (ICL) algorithm that has been metalearned via the usual in-weight learning (IWL) (Binz et al., 2023;Dubey et al., 2020;Lake and Baroni, 2023;Russin et al., 2024a;Wang et al., 2018). 12This hypothesis predicts that when the neural mechanisms supporting ICL, rather than IWL, are driving behavior, this should facilitate compositional generalization.Russin et al. (2024b) found that this interplay between ICL and IWL in neural network models explained compositional generalization performance on a task recently studied in humans (Dekker et al., 2022).Both LLMs and metalearning neural networks achieved much better compositional generalization performance on the task when ICL rather than IWL was responsible for behavior.Furthermore, Russin et al. (2024b) found that the dynamic interaction between ICL and IWL (Chan et al., 2022), which has similarities to a tradeoff between working memory and reinforcement learning observed in humans (Collins and Frank, 2018;Rac-Lubashevsky et al., 2023), also explained the curriculum effects observed in the human study (Dekker et al., 2022).</p>
<p>There is further support from cognitive and computational neuroscience for the idea that ICL and compositional generalization share a set of underlying neural mechanisms in humans.The prefrontal cortex is important for basic capacities known to be involved in human ICL, such as working memory (O'Reilly and Frank, 2006), top-down attention (Cohen et al., 1990), and cognitive control (Miller and Cohen, 2001), and, as previously mentioned, is thought to be important for compositional generalization processes such as rule learning (Calderon et al., 2022;Collins and Frank, 2013;Frank and Badre, 2012;Kriete et al., 2013), reasoning (Crescentini et al., 2011;Goel, 2007), and processing complex syntax (Thompson-Schill, 2005).Furthermore, it has been hypothesized that metalearning (specifically, meta-reinforcement learning) is an important aspect of the functioning of the prefrontal cortex (Hattori et al., 2023;Wang et al., 2018).Thus, while it may be difficult to directly establish that human compositional generalization behaviors are emergent properties of an ICL algorithm, there is indirect empirical support for the idea that compositional generalization and ICL share an underlying set of neural mechanisms in humans.</p>
<p>Metalearning and Human Development</p>
<p>The metalearning approach emphasizes how inductive biases can themselves be learned from data.As has been articulated clearly in other work (Binz et al., 2023;Lake and Baroni, 2023;McCoy et al., 2020;Wang, 2020), the metalearning approach is agnostic about whether this occurs on an evolutionary timescale or within an individual human lifetime.Many have shied away from interpreting metalearning neural networks as models of human development (although see Russin et al., 2024a;Wang, 2020;Wang et al., 2018), instead choosing to emphasize its utility as a methodological tool (Binz et al., 2023;McCoy et al., 2020).</p>
<p>When taken seriously as models of human development, however, metalearning models generate testable empirical predictions.If human compositionality is the result of metalearning, we should expect older children to show signs of improved compositional generalization abilities.Existing evidence supports this prediction, suggesting that children learn how to learn more efficiently over time (Bergelson, 2020), and that the specific ability to compose rules improves over the course of development (Piantadosi and Aslin, 2016;Piantadosi et al., 2018).</p>
<p>A more specific prediction of the metalearning hypothesis is that ICL, as opposed to IWL, should improve with experience.This is also consistent with existing evidence, which suggests that older children are much more adept at learning and reasoning in context using working memory and other executive functions (Munakata et al., 2012).Indeed, executive functions, which are intimately related to our ability to learn in context (Cole et al., 2011;Duncan et al., 2017;Hampshire et al., 2011;Miller and Cohen, 2001;Rougier et al., 2005), emerge especially late in development and continue to mature throughout adolescence and early adulthood (Ferguson et al., 2021).</p>
<p>Finally, the metalearning hypothesis emphasizes that compositional generalization abilities are facilitated by exposure to similar generalization problems.This predicts that humans will be more likely to exhibit compositional generalization behaviors with concepts or stimuli that they have previously encountered.The extensive literature on category learning has demonstrated that humans generalize better when categories are congruent (or verbalizable) with familiar features (Ashby and Maddox, 2011;Flesch et al., 2018Flesch et al., , 2022;;Love et al., 2004).It has also been shown that in domains such as motor learning, humans reliably fail to generalize outside of their training distributions on certain tasks (Zhou et al., 2017).</p>
<p>These results are consistent with the idea that humans are capable of generalizing out-of-distribution in certain domains where they have been trained to do so (e.g., learning certain kinds of new words or categories), but fail to generalize in completely novel scenarios.The question of whether a given task is truly outof-distribution with respect to a lifetime of human experience is difficult to establish empirically.Better naturalistic dataset collection, including egocentric video and audio recordings from children (Sullivan et al., 2021;Vong et al., 2024), and efforts to curate more ecologically/psychologically plausible language datasets (Warstadt and Bowman, 2024;Warstadt et al., 2023a,b), should facilitate this line of inquiry.</p>
<p>It is important to note that humans learn from experiences that are highly unstructured compared to the training datasets used in most studies on metalearning (e.g., Lake, 2019;Lake and Baroni, 2023;Russin et al., 2024b), which are contrived by researchers to engender very specific inductive biases on very specific tasks.While humans are trained on structured curricula in formal educational settings, there is evidence that competences related to compositionality such as sensitivity to hierarchical structure in syntax (Chomsky, 1957) and to geometric features in visual tasks (Dehaene et al., 2022) do not depend on such experiences.However, LLMs have shown that simply predicting the next word on a large dataset of unstructured text can engender an inductive bias for compositionality (Brown et al., 2020;Bubeck et al., 2023;Zhou et al., 2023a).These models develop ICL abilities (von Oswald et al., 2023;Xie et al., 2022) that allow good generalization performance on compositional tasks (Webb et al., 2023;Wei et al., 2023;Zhou et al., 2023a), even though their training sets were not specifically designed to achieve this.These results are impressive, and provide a proof of concept that an inductive bias for compositionality can be learned from unstructured language data.However, current LLMs are trained on orders of magnitude more data than humans experience in an entire lifetime (Frank, 2023;Linzen and Baroni, 2021;Warstadt and Bowman, 2024), making it unclear whether similar capabilities could emerge in models trained on a more realistic scale.The emergence of ICL abilities has been shown to depend on the distributional properties of the training data and on network architecture (Chan et al., 2022).Furthermore, architectural inductive biases such as those related to the "relational bottleneck" (Webb et al., 2024) have been shown to improve sample efficiency on certain tasks (Webb et al., 2021).Thus, perhaps human compositionality is best characterized as an emergent property of an in-context learning algorithm, but the brain embodies important architectural inductive biases that encourage this algorithm to emerge.</p>
<p>Mere Implementations?</p>
<p>The empirical evidence reviewed up to this point suggests that neural network models may be able to get past the first horn of Fodor and Pylyshyn's critical dilemma (see section 2.3).We have focused on this initial empirical challenge because we believe that recent advances represent a crucial development on this front, eliminating a major obstacle that has hindered the acceptance of neural networks as viable models of human cognition.But what about the 'mere implementation' objection, the second horn of the compositionality challenge?Is the success of contemporary DNNs on compositional generalization problems a reflection of the fact that they have implemented classical architectures?And if so, does this mean that they are uninformative for cognitive theory?Though this is a nuanced and challenging issue, certain cases are unambiguous.Neuro-symbolic hybrid models (Ellis et al., 2021;Gaur et al., 2022;Marcus, 2020;Marcus and Davis, 2020;Yu et al., 2023;Zhou et al., 2024), which combine explicit symbolic processes with neural networks, are 'mere implementations' by design-proponents of this approach treat the behavioral signatures of compositionality in humans as diagnostic of underlying symbolic mechanisms and explicitly build such mechanisms into neural networks.Though the neural components of these networks are utilized to deal with high-dimensional inputs like images, their compositionality is entirely due to their built-in symbolic components.This captures something of the spirit of Fodor and Pylyshyn's original objection (in fact, they allude to this possibility themselves), and transforms it into an empirical hypothesis with clear predictive upshot-explicit implementation of symbolic mechanisms will be a necessary prerequisite for neural networks to succeed on compositional problems.Current results do not seem to favor this hypothesis, as the findings on metalearning (section 3.2), and LLMs (section 3.3) suggest that such explicit symbolic mechanisms may not be necessary.However, it is still unclear whether neuro-symbolic hybrids will win out in the end.If so, it would mean that DNNs yield few novel insights into compositionality, though they may still offer alternative perspectives on other aspects of cognition (Chalmers, 1993).</p>
<p>However, the claim that neural networks may implement classical symbolic operations is invoked to stave off challenges from DNNs, even in the absence of any tangible predictions of this kind.Without clear mechanistic commitments, it becomes harder to assess.The results reviewed here indicate that even strictly neural models with no built-in symbolic components may be adequate for replicating the behavioral signatures of compositionality.What should we make of this outcome?One possibility that was widely discussed in the original connectionist debate is eliminativism (Ramsey et al., 1990).An eliminative connectionist would argue that the success of strictly neural models demonstrates that Chomsky and Montague's shift to treating formal syntactic and semantic theories as models of human psychological processing (see section 2) was a grave mistake (Piantadosi, 2023), and that the ideal language philosophers' skepticism about this approach was well-founded.If DNNs can achieve compositional generalization without internally representing any rule-based, combinatorial systems, this undermines the motivation for positing them to explain human behavior.Chomsky was right that this capacity cries out for explanation, but the eliminativist holds that the neural network vocabulary of nodes, weights, layers, and so forth, will furnish sufficient explanatory resources for doing so.</p>
<p>The possibility that these networks are implementing classical architectures serves as an obstacle to such eliminativist conclusions (McGrath et al., 2023a;McGrath and Russin, 2024).Physical systems can exhibit functional organization at different levels of description.The fact that no explicit symbols or syntactic operations were built into a DNN that has metalearned to generalize compositionally doesn't mean that it hasn't learned to implicitly implement these classical constructs through training.For all we know-runs this version of the objection-the DNNs that succeed on these compositional tasks are not alternative cognitive models at all, but mere implementations of the very same theoretical constructs that classical theorists have emphasized all along.On this view, they may help us to understand how classical architectures are implemented in the human brain (McLaughlin, 1993), but will not be informative at the cognitive level.</p>
<p>Though this is an important empirical possibility, there is a crucial gap between the claim that a pretrained LLM could, for all we know, be merely implementing a classical symbolic architecture and any claim that it must be.The problem with Fodor and Pylyshyn's dilemma is that it treats mere implementation as a virtual guarantee-if neural networks ever overcome the first horn and exhibit the behavioral signatures of compositionality, it must be because they have merely implemented classical architectures (thereby falling on the second horn).There may be arguments that narrow this gap,13 but Fodor and Pylyshyn's blithe confidence is unwarranted.</p>
<p>The only way to guarantee that the success of DNNs reflects mere implementation would be to treat the behavioral signatures of compositionality as constitutive (rather than diagnostic) of an underlying classical architecture (McGrath et al., 2023b).What it is to be a classical architecture, on this constitutive view, just is to be the kind of system that generates this behavioral profile.Adopting this position would ensure that any successful DNN would count as a mere implementation of a classical architecture, but it would do so at the cost of sapping classical theory of the kind of explanatory value that it was originally intended to contribute.The rise of cognitivism and its attendant rejection of behaviorism hinge on the claim that psychological theory must do more than merely redescribe behavior-we need an understanding of the independently identifiable underlying mechanisms that produce the given behavior (Cao and Yamins, 2021a).If giving rise to these generalization behaviors is constitutive of being a classical architecture, however, then we preclude even the logical possibility that a DNN could replicate the behavioral signatures of compositionality without implementing a classical system.This means, in effect, that we have failed to specify any theoretical content beyond this characterization of behavior and have lapsed back into mere redescription.</p>
<p>Rather than simply assuming that contemporary DNNs implement classical architectures or searching for some a priori guarantee, what we need to do is try to find out whether or not they are doing so.The burgeoning field of "mechanistic interpretability" is making this increasingly feasible (Elhage et al., 2021;Olsson et al., 2022;Merullo et al., 2024;Wu et al., 2023).Though interpretability work is often raised in the context of AI safety considerations (Rudner and Toner, 2021), it can also play a crucial role in informing cognitive theory, allowing us to extract high-level, functionally relevant descriptions of these models' internal representations that we can put into contact with existing cognitive theory (McGrath et al., 2023b;Millière, 2024;Millière and Buckner, 2024b).In principle, this process could point us towards eliminativism (if successful models' representations bear no resemblance to classical constructs), or mere implementationism (if they exactly recapitulate the structure of a fully specified symbolic theory).In the cases where this work has been pursued, however, it seems likely that these models land somewhere in between, recapitulating certain aspects of classical theory, but diverging in interesting ways (Baroni, 2022;Chen et al., 2024;Manning et al., 2020).In such cases, we have suggested that it may be worth preserving key terms to encompass these behaviors (McGrath et al., 2023a;McGrath and Russin, 2024), as long as it does not obscure the fact that they contain new insights.In a slogan, not all systems that we may count as implementations are 'mere'-informative implementations can point us to new insights into previously postulated structures (Smolensky, 1988;Smolensky and Legendre, 2011).Though interpretive work evaluating the compositional models reviewed here is still in its initial stages, it is a promising line of future investigation (Andreas, 2019;Lepori et al., 2023;Lewis et al., 2023;McCoy et al., 2019;Russin et al., 2021;Soulos et al., 2020;Todd et al., 2024).From a philosophical perspective, further progress on deeper issues about the nature of explanation and implementation will put us in a position to use these findings to resolve the mere implementation objection once and for all.</p>
<p>Conclusion</p>
<p>In this article, we have traced the notion of compositionality from its emergence in early analytic philosophy to its present status as a key explanatory construct in contemporary cognitive science.We have emphasized the role that compositionality plays in debates over the explanatory adequacy of neural network models of human cognition and reviewed a range of recent results suggesting that current deep neural networks replicate key behavioral signatures that have motivated the ascription of internal compositional systems to human beings.In particular, these recent results suggest that metalearning (or large-scale pretraining) is a viable approach to reproducing these behavioral signatures within strictly neural architectures.When taken seriously as an approach to modeling human compositionality, the metalearning framework generates concrete predictions about its underlying neural mechanisms and development.DNNs' recent success also reopens key philosophical questions about the theoretical significance and proper interpretation of neural networks-whether they should be understood as merely implementing classical compositional mechanisms, or as sources of novel insight that may motivate revisions to classical theories.These pressing questions take us beyond what neural networks can do, impelling us to ask what they mean for our understanding of the human mind.We expect that resolving them will require an interdisciplinary approach and increased philosophical engagement.</p>
<p>Acknowledgments</p>
<p>We would like to thank Lotem Elber-Dorozko, Richard Kimberley Heck, Christopher Hill, Elizabeth Miller, Joshua Schechter, Roman Feiman, Michael J. Frank, Ellie Pavlick, Randall O'Reilly, Edouard Machery, Suraj Anand, Steven Frankland, Taylor Webb, Stavros Orfeas Zormpalas, Felipe De Brigard, Walter Sinnott-Armstrong, and</p>
<p>Figure 1: Are neural networks stimulus-response learners, or do they learn to solve problems with compositional algorithms?Two extreme possibilities are depicted for natural language (top) and simple arithmetic (bottom).In either case, a stimulus-response learner (left) would simply memorize a lookup table where entries corresponded to entire inputs/outputs, ignoring any compositional structure.A compositional learner (right) would solve the same problems by composing familiar elements according to known rules, allowing generalization.Figure reproduced from Russin et al. (2021).</p>
<p>the participants in the Brown University Dissertation Workshop, the Philosophy and Cognitive Science of Deep Learning reading group, and the 2024 SSNAP Workshop for helpful comments and discussions.JR was supported by NIH NIGMS COBRE grant GR5271961.</p>
<p>Montague signals his alignment with Chomsky on this key point: "I consider it possible to comprehend the syntax and semantics of both kinds of languages within a single natural and mathematically precise theory. On this point I differ from a number of philosophers, but agree, I believe, with Chomsky and his associates"(Montague, 1974, p.15).
Chomsky suggests that some 17th century philosophers, like Descartes and Wilhelm von Humboldt, recognized this explanatory challenge and anticipated his own ideas(Chomsky, 1966). It is also prefigured by Frege, who wrote in 1914 that, "the possibility of our understanding sentences which we have never heard before rests evidently on this, that we can construct the sense of a sentence out of parts that correspond to words. . . Without this, language in the proper sense would be impossible. . . we would always be restricted to a very narrow area and could not form a completely new proposition"(Frege, 1980).
To return to our analogy, if we aim to explain the behavior of someone solving a multiplication problem, what matters is how they determined the solution, in the subjective sense, rather than the metaphysical or objective grounds of this solution.
This observation may seem obvious, but it is in fact easy to overlook. Even Davidson, one of the earliest and most influential proponents of compositional natural language semantics, fails to fully appreciate the point. For a detailed discussion of this tension in Davidson's work, seeHeck (2013).
Framing Fodor and Pylyshyn's argument as a dilemma is common in the literature(Millière and Buckner, 2024a) and is consonant with the earliest connectionist replies to their article(Smolensky, 1991), but one could argue that the two horns of this dilemma really comprise a single objection-that neural networks cannot explain compositionality. Our goal here is not to engage in 'Fodorology.' Dividing what we will call the 'compositionality challenge' into two distinct horns is, in our view, a useful way of distinguishing two importantly different objections to DNNs, which often get misleadingly run together.
While most agree that the transformer is a very unbiased architecture (e.g., it is in some ways even more unbiased than the convolutional neural network; Dosovitskiy et al., 2021), some have argued that the attention mechanism might play a special role in the compositional behaviors observed in transformers(Smolensky et al., 2022).
An alternative way to operationalize compositionality is to test sample efficiency, or the number of training samples it takes to achieve a certain level of test performance(Bahdanau et al., 2018). This makes a similar assumption that a compositional learner should be able to infer the rules governing the data from relatively few samples.
It would probably require more than the three examples given above to truly infer their underlying structure, but we have chosen to keep it brief for illustrative purposes.
Some have assumed that metalearning in neural networks is a better model of learning on an evolutionary timescale than within an individual's lifetime(Binz et al., 2023). Here, we assume that metalearning can take place in the brain over the course of an individual's lifetime, and that ICL and IWL can simultaneously exist within a single system. This idea is directly investigated byRussin et al. (2024b).
One way to bridge the gap between these two claims would be to argue that the "possibility space"(Cao and Yamins, 2021b) is narrow, so that DNNs are likely to be hitting on the very same solutions postulated by classical theorists. One issue with this approach is how to justify the claim that the space of possible solutions is narrow-reliance on intuition here may reflect a failure of contemporary imagination, rather than any genuine limitation. Moreover, even if the solution set is limited, it need not be limited to a single possibility, leaving room for interesting and informative variation between the solutions devised by researchers and those identified by the models themselves.</p>
<p>A Altabaa, T Webb, J Cohen, J Lafferty, Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. 2023</p>
<p>Measuring Compositionality in Representation Learning. J Andreas, 2019</p>
<p>Good-Enough Compositional Data Augmentation. J Andreas, 2020</p>
<p>Human category learning 2.0. F G Ashby, W T Maddox, Annals of the New York Academy of Sciences. 122412011</p>
<p>J L Austin, How to Do Things with Words: Second Edition. Cambridge, MassHarvard University Press19752nd edition edition</p>
<p>D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473Neural Machine Translation by Jointly Learning to Align and Translate. 2014cs, stat</p>
<p>Systematic Generalization: What Is Required and Can It Be Learned?. D Bahdanau, S Murty, M Noukhovitch, T H Nguyen, H De Vries, A Courville, arXiv:1811.128892018</p>
<p>Categorical semantics of compositional reinforcement learning. G Bakirtzis, M Savvas, U Topcu, 2022</p>
<p>Linguistic generalization and compositionality in modern artificial neural networks. M Baroni, Philosophical Transactions of the Royal Society B: Biological Sciences. 375201903072020. 1791</p>
<p>On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. M Baroni, 2022</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>On the search for new learning rules for ANNs. S Bengio, Y Bengio, J Cloutier, Neural Processing Letters. 241995</p>
<p>Learning a synaptic learning rule. Y Bengio, S Bengio, J Cloutier, IJCNN-91-Seattle International Joint Conference on Neural Networks. ii1991969</p>
<p>The Comprehension Boost in Early Word Learning: Older Infants Are Better Learners. E Bergelson, Child development perspectives. 1432020</p>
<p>Meta-Learned Models of Cognition. M Binz, I Dasgupta, A K Jagadish, M Botvinick, J X Wang, E Schulz, Behavioral and Brain Sciences. 2023</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Language Models are Few-Shot Learners. 2020</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>Empiricism without magic: Transformational abstraction in deep convolutional neural networks. C Buckner, Synthese. 195122018</p>
<p>Deep learning: A philosophical introduction. C Buckner, Philosophy Compass. 1410e126252019</p>
<p>Black Boxes or Unflattering Mirrors? Comparative Bias in the Science of Machine Behaviour. C Buckner, British Journal for the Philosophy of Science. 7432023a</p>
<p>From Deep Learning to Rational Machines: What the History of Philosophy Can Teach Us about the Future of Artificial Intelligence. C J Buckner, 2023bOxford University PressNew York, NY</p>
<p>Philosophy of Mind: 1950-2000. T Burge, The Edinburgh Companion to Twentieth-Century Philosophies. Edinburgh University Press2007</p>
<p>. E Caballero, K Gupta, I Rish, D Krueger, 2023Broken Neural Scaling Laws</p>
<p>Thunderstruck: The ACDC model of flexible sequences and rhythms in recurrent neural circuits. C B Calderon, T Verguts, M J Frank, PLOS Computational Biology. 182e10098542022</p>
<p>Explanatory models in neuroscience: Part 1 -taking mechanistic abstraction seriously. R Cao, D Yamins, 2021a</p>
<p>Explanatory models in neuroscience: Part 2 -constraint-based intelligibility. R Cao, D Yamins, 2021b</p>
<p>The Logical Syntax of Language. International Library of Psychology. R Carnap, Philosophy and Scientific Method. 1937Brace and Company</p>
<p>Meaning and Necessity: A Study in Semantics and Modal Logic. R Carnap, 1988University of Chicago PressChicago, IL2nd edition</p>
<p>CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models. S Castro, A Ziai, A Saluja, Z Yuan, R Mihalcea, 2024</p>
<p>Align and Augment: Generative Data Augmentation for Compositional Generalization. F Cazzaro, D Locatelli, A Quattoni, Conference of the European Chapter. the Association for Computational Linguistics2024</p>
<p>Y Chai, Z Li, J Liu, L Chen, F Li, D Ji, C Teng, Compositional Generalization for Multi-label Text Classification: A Data-Augmentation Approach. 2023</p>
<p>Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention. A K Chakravarthy, J Russin, O' Reilly, R Ippolito, D Li, L H Pacheco, M L Chen, D Xue, N , Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research WorkshopHybrid: Seattle, Washington + OnlineAssociation for Computational Linguistics2022</p>
<p>Why Fodor and Pylyshyn Were Wrong: The Simplest Refutation. D J Chalmers, Philosophical Psychology. 1993</p>
<p>Data Distributional Properties Drive Emergent In-Context Learning in Transformers. S C Y Chan, A Santoro, A K Lampinen, J X Wang, A Singh, P H Richemond, J Mcclelland, F Hill, 2022</p>
<p>A Chen, R Shwartz-Ziv, K Cho, M L Leavitt, N Saphra, Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. 2024</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, 2021Evaluating Large Language Models Trained on Code</p>
<p>Syntactic Structures. N Chomsky, 1957De Gruyter Mouton</p>
<p>Aspects of the Theory of Syntax. N Chomsky, 1965. 1966Cartesian Linguistics. Harper &amp; RowChomsky, N.</p>
<p>Language and Mind. N Chomsky, The UCLA Lectures. Chomsky, N.Cambridge University Press1968. 2020</p>
<p>On the Control of Automatic Processes: A Parallel Distributed Processing Model of the Stroop Effect. J D Cohen, K Dunbar, J L Mcclelland, Psychological Review. 9731990</p>
<p>Rapid Transfer of Abstract Rules to Novel Contexts in Human Lateral Prefrontal Cortex. M W Cole, J A Etzel, J M Zacks, W Schneider, T S Braver, Frontiers in Human Neuroscience. 52011</p>
<p>Cognitive control over learning: Creating, clustering, and generalizing task-set structure. A G E Collins, M J Frank, Psychological Review. 12012013</p>
<p>Within-and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory. A G E Collins, M J Frank, Proceedings of the National Academy of Sciences. 115102018</p>
<p>Testing Relational Understanding in Text-Guided Image Generation. C Conwell, T Ullman, 2022</p>
<p>Mechanisms of Rule Acquisition and Rule Following in Inductive Reasoning. C Crescentini, S Seyed-Allaei, N D Pisapia, J Jovicich, D Amati, T Shallice, Journal of Neuroscience. 31212011</p>
<p>Logics and Languages. M Cresswell, U.S.A. by Harper &amp; Row1973Methuen; London</p>
<p>The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. R Csordás, K Irie, J Schmidhuber, 2022</p>
<p>Approximation by superpositions of a sigmoidal function. G Cybenkot, Mathematics of Control, Signals and Systems. 21989</p>
<p>The paradox of the compositionality of natural language: A neural machine translation case study. V Dankers, E Bruni, D Hupkes, 2022</p>
<p>Language models show human-like content effects on reasoning tasks. I Dasgupta, A K Lampinen, S C Y Chan, H R Sheahan, A Creswell, D Kumaran, J L Mcclelland, F Hill, 2023</p>
<p>Theories of Meaning and Learnable Languages. D Davidson, Proceedings of the 1964 International Congress for Logic, Methodology, and Philosophy of Science. Y Bar-Hillel, the 1964 International Congress for Logic, Methodology, and Philosophy of ScienceNorth-Holland1965</p>
<p>Symbols and mental programs: A hypothesis about human singularity. S Dehaene, F Al Roumi, Y Lakretz, S Planton, M Sablé-Meyer, Trends in Cognitive Sciences. 2692022</p>
<p>Curriculum learning for human compositional generalization. R B Dekker, F Otto, C Summerfield, Proceedings of the National Academy of Sciences. 11941e22055821192022</p>
<p>A spreading-activation theory of retrieval in sentence production. G S Dell, Psychological Review. 9331986</p>
<p>R Dessì, M Baroni, arXiv:1905.08527CNNs found to jump around more skillfully than RNNs: Compositional generalization in seq2seq convolutional networks. 2019</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, 2019. 2021An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</p>
<p>A Drozdov, N Schärli, E Akyürek, N Scales, X Song, X Chen, O Bousquet, D Zhou, Compositional Semantic Parsing with Large Language Models. 2022</p>
<p>Y Duan, J Schulman, X Chen, P L Bartlett, I Sutskever, P Abbeel, RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning. 2016</p>
<p>Connecting Context-specific Adaptation in Humans to Meta-learning. R Dubey, E Grant, M Luo, K Narasimhan, T Griffiths, 2020</p>
<p>What Is a Theory of Meaning? (II). M Dummett, The Seas of Language, page 0. Cambridge, MassOxford University Press1993. 1996Frege: Philosophy of Language</p>
<p>Complexity and compositionality in fluid intelligence. J Duncan, D Chylinski, D J Mitchell, A Bhandari, 2017Proceedings of the National Academy of Sciences</p>
<p>. N Dziri, X Lu, M Sclar, X L Li, L Jiang, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, S Sanyal, S Welleck, X Ren, A Ettinger, Z Harchaoui, Y Choi, 2023Faith and Fate: Limits of Transformers on Compositionality</p>
<p>. N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, N Dassarma, D Drain, D Ganguli, Z Hatfield-Dodds, D Hernandez, A Jones, J Kernion, L Lovitt, K Ndousse, D Amodei, T Brown, J Clark, J Kaplan, S Mccandlish, C Olah, 2021A mathematical framework for transformer circuits. Transformer Circuits Thread</p>
<p>DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning. K Ellis, C Wong, M Nye, M Sablé-Meyer, L Morales, L Hewitt, L Cary, A Solar-Lezama, J B Tenenbaum, Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021. the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Semantic Theory and Tacit Knowledge. G Evans, Collected Papers. Oxford University Press1985</p>
<p>Debiasing by instruction: The case of belief bias. J S Evans, S E Newstead, J Allen, P Pollard, European Journal of Cognitive Psychology. 631994</p>
<p>Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Bakhtin Fair, A Brown, N Dinan, E Farina, G Flaherty, C Fried, D Goff, A Gray, J Hu, H Jacob, A P Komeili, M Konath, K Kwon, M Lerer, A Lewis, M Miller, A H Mitts, S Renduchintala, A Roller, S Rowe, D Shi, W Spisak, J Wei, A Wu, D Zhang, H Zijlstra, M , Science. 00e90972022</p>
<p>The developmental trajectories of executive function from adolescence to old age. H J Ferguson, V E A Brunsdon, E E F Bradford, Scientific Reports. 11113822021</p>
<p>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. C Finn, P Abbeel, S Levine, 2017</p>
<p>Performance vs. competence in human-machine comparisons. C Firestone, Proceedings of the National Academy of Sciences. 117432020</p>
<p>Comparing continual task learning in minds and machines. T Flesch, J Balaguer, R Dekker, H Nili, C Summerfield, Proceedings of the National Academy of Sciences. 115442018</p>
<p>T Flesch, K Juechems, T Dumbalska, A Saxe, C Summerfield, Orthogonal representations for robust context-dependent task performance in brains and neural networks. 202222</p>
<p>The red herring and the pet fish: Why concepts still can't be prototypes. J Fodor, E Lepore, Cognition. 5821996</p>
<p>J Fodor, E Lepore, Why Compositionality Won't Go Away: Reflections on Horwich's 'Deflationary' Theory. Ratio. 200114</p>
<p>The Appeal to Tacit Knowledge in Psychological Explanation. J A Fodor, The Journal of Philosophy. 65201968</p>
<p>The Language of Thought. J A Fodor, 1975Harvard University Press</p>
<p>Hume Variations. J A Fodor, 2003Oxford University PressUK, Oxford, GB</p>
<p>Why Meaning (Probably) Isn't Conceptual Role. J A Fodor, E Lepore, Philosophical Issues. 31993</p>
<p>Connectionism and cognitive architecture: A critical analysis. J A Fodor, Z W Pylyshyn, Cognition. 2811988</p>
<p>Bridging the data gap between children and large language models. M C Frank, Trends in Cognitive Sciences. 27112023</p>
<p>Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis. M J Frank, D Badre, Cerebral Cortex. 2232012. 1991</p>
<p>The Thought: A Logical Inquiry. G Frege, Mind. 652591956</p>
<p>Letter to Jourdain. G Frege, G Gabriel, H Hermes, F Kambartel, C Thiel, Veraart, Philosophical and Mathematical Correspondence. Basil Blackwell1980</p>
<p>Cognition without classical architecture. J W Garson, Synthese. 10021994</p>
<p>Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI. M Gaur, K Gunaratna, S Bhatt, A Sheth, IEEE Internet Computing. 2642022</p>
<p>Anatomy of deductive reasoning. V Goel, Trends in Cognitive Sciences. 11102007</p>
<p>I Goodfellow, Y Bengio, A Courville, Deep Learning. Adaptive Computation and Machine Learning. Cambridge, MassachusettsThe MIT Press2016</p>
<p>P Grice, Studies in the Way of Words. Harvard University Press1991</p>
<p>Doing more with less: Meta-reasoning and meta-learning in humans and machines. T L Griffiths, F Callaway, M B Chang, E Grant, P M Krueger, F Lieder, Current Opinion in Behavioral Sciences. 292019</p>
<p>Insight and Illusion: Wittgenstein on Philosophy and the Metaphysics of Experience. P M S Hacker, R F Hadley, Cognition, Systematicity, and Nomic Necessity. Mind and Language. 1221975. 1997Oxford University Press</p>
<p>Lateral prefrontal cortex subregions make dissociable contributions during fluid reasoning. A Hampshire, R Thompson, J Duncan, A M Owen, Cerebral Cortex. 2112011. 1991</p>
<p>The History and Prehistory of Natural-Language Semantics. D W Harris, Innovations in the History of Analytical Philosophy. S Lapointe, C Pincock, LondonPalgrave Macmillan UK2017</p>
<p>Meta-reinforcement learning via orbitofrontal cortex. R Hattori, N G Hedrick, A Jain, S Chen, H You, M Hattori, J.-H Choi, B K Lim, R Yasuda, T Komiyama, Nature Neuroscience. 26122023</p>
<p>What is Compositionality?. R K Heck, 2013Unpublished Manuscript</p>
<p>A large-scale comparison of human-written versus ChatGPT-generated essays. S Herbold, A Hautli-Janisz, U Heuer, Z Kikteva, A Trautsch, Scientific Reports. 131186172023</p>
<p>A quantitative description of membrane current and its application to conduction and excitation in nerve. A L Hodgkin, A F Huxley, The Journal of Physiology. 11741952</p>
<p>Neural networks and physical systems with emergent collective computational abilities. J J Hopfield, Proceedings of the National Academy of Sciences. 7981982</p>
<p>Structured Representations in Connectionist Systems. T E Horgan, J L Tienson, Connectionism: Theorye and Practice. S Davis, Oxford University Press1991</p>
<p>Approximation Capabilities of Muitilayer Feedforward Networks. K Hornik, Neural Networks. 41991</p>
<p>Wittgenstein's Metaphilosophy. P Horwich, 2013Oxford University PressOxford, New York</p>
<p>T Hospedales, A Antoniou, P Micaelli, A Storkey, Meta-Learning in Neural Networks: A Survey. 2020</p>
<p>C.-Y Hsieh, J Zhang, Z Ma, A Kembhavi, R Krishna, SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. 2023</p>
<p>J Huang, K C Chang, -C , Towards Reasoning in Large Language Models: A Survey. 2023</p>
<p>Compositionality decomposed: How do neural networks generalise?. D Hupkes, V Dankers, M Mul, E Bruni, arXiv:1908.083512020cs, stat</p>
<p>A taxonomy and review of generalization research in NLP. D Hupkes, M Giulianelli, V Dankers, M Artetxe, Y Elazar, T Pimentel, C Christodoulopoulos, K Lasri, N Saphra, A Sinclair, D Ulmer, F Schottmann, K Batsuren, K Sun, K Sinha, L Khalatbari, M Ryskina, R Frieske, R Cotterell, Jin , Z , Nature Machine Intelligence. 5102023</p>
<p>Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks. Y Jiang, M Bansal, 2021</p>
<p>Y Jiang, X Zhou, M Bansal, Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality. 2022</p>
<p>Compositional Reinforcement Learning from Logical Specifications. K Jothimurugan, S Bansal, O Bastani, R Alur, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, Nature. 59678732021</p>
<p>Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. K Kansky, T Silver, D A Mély, M Eldawy, M Lázaro-Gredilla, X Lou, N Dorfman, S Sidor, S Phoenix, D George, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR2017</p>
<p>Scaling Laws for Neural Language Models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, 2020</p>
<p>The Underlying Reality of Language and Its Philosophical Import. J J Katz, 1971Harper &amp; RowNew York</p>
<p>The Structure of a Semantic Theory. J J Katz, J A Fodor, Language. 3921963</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. D Keysers, N Schärli, N Scales, H Buisman, D Furrer, S Kashubin, N Momchev, D Sinopalnikov, L Stafiniak, T Tihon, D Tsarkov, X Wang, M Van Zee, O Bousquet, 2020</p>
<p>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. N Kim, T Linzen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>On belief bias in syllogistic reasoning. K C Klauer, J Musch, B Naumer, Psychological Review. 10742000</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 2023</p>
<p>Indirection and symbol-like processing in the prefrontal cortex and basal ganglia. T Kriete, D C Noelle, J D Cohen, O' Reilly, R C , 2013110Proceedings of the National Academy of Sciences U.S.A.</p>
<p>ImageNet Classification with Deep Convolutional Neural Networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. Curran Associates, Inc201225</p>
<p>Compositional generalization through meta sequence-to-sequence learning. B M Lake, 2019</p>
<p>Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. B M Lake, M Baroni, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. J G Dy, A Krause, the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLR2018. July 10-15. 201880of Proceedings of Machine Learning Research</p>
<p>Human-like systematic generalization through a meta-learning neural network. B M Lake, M Baroni, Nature. 2023</p>
<p>Human few-shot learning of compositional instructions. B M Lake, T Linzen, M Baroni, Proceedings of the 41th Annual Meeting of the Cognitive Science Society, CogSci 2019: Creativity + Cognition + Computation. A K Goel, C M Seifert, C Freksa, the 41th Annual Meeting of the Cognitive Science Society, CogSci 2019: Creativity + Cognition + ComputationMontreal, Canada2019. July 24-27, 2019</p>
<p>Building machines that learn and think like people. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, The Behavioral and Brain Sciences. 40e2532017</p>
<p>Large Language Models and the Argument From the Poverty of the Stimulus. N Lan, E Chemla, R Katzir, 2024</p>
<p>. T Lanham, A Chen, A Radhakrishnan, B Steiner, C Denison, D Hernandez, D Li, E Durmus, E Hubinger, J Kernion, K Lukošiūtė, K Nguyen, N Cheng, N Joseph, N Schiefer, O Rausch, R Larson, S Mccandlish, S Kundu, S Kadavath, S Yang, T Henighan, T Maxwell, T Telleen-Lawton, T Hume, Z Hatfield-Dodds, J Kaplan, J Brauner, S R Bowman, E Perez, 2023Measuring Faithfulness in Chain-of-Thought Reasoning</p>
<p>The Poverty of the Stimulus Argument. S Laurence, E Margolis, The British Journal for the Philosophy of Science. 5222001</p>
<p>Y Lecun, Y Bengio, G Hinton, Deep learning. 2015521</p>
<p>M A Lepori, T Serre, E Pavlick, Break It Down: Evidence for Structural Compositionality in Neural Networks. 2023</p>
<p>M Lewis, N V Nayak, P Yu, Q Yu, J Merullo, S H Bach, E Pavlick, Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. 2023</p>
<p>Syntactic Structure from Deep Learning. T Linzen, M Baroni, Annual Review of Linguistics. 712021</p>
<p>Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning. R G Liu, M J Frank, Artificial Intelligence. 3121037702022</p>
<p>SUSTAIN: A network model of category learning. B C Love, D L Medin, T M Gureckis, Psychological Review. 11122004</p>
<p>An Examination of the Compositionality of Large Generative Vision-Language Models. T Ma, R Li, J Liang, 2023a</p>
<p>CREPE: Can Vision-Language Foundation Models Reason Compositionally. Z Ma, J Hong, M O Gul, M Gandhi, I Gao, R Krishna, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Vancouver, BC, CanadaIEEE2023b</p>
<p>Connectionism: Debates on Psychological Explanation. C Macdonald, G Macdonald, 1991Wiley-Blackwell2Oxford1st edition edition</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. C D Manning, K Clark, J Hewitt, U Khandelwal, O Levy, 2020Proceedings of the National Academy of Sciences201907367</p>
<p>Deep learning: A critical appraisal. G Marcus, 2018</p>
<p>G Marcus, The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence. 2020</p>
<p>G Marcus, E Davis, Rebooting AI: Building Artificial Intelligence We Can Trust. New YorkVintage2020</p>
<p>Rethinking Eliminative Connectionism. G F Marcus, Cognitive Psychology. 3731998</p>
<p>Three-Concept Monte: Explanation, Implementation and Systematicity. R J Matthews, Synthese. 10131994</p>
<p>Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory. J L Mcclelland, B L Mcnaughton, O' Reilly, R C , Psychological Review. 10231995</p>
<p>Rules or connections in past-tense inflections: What does the evidence rule out?. J L Mcclelland, K Patterson, Trends in Cognitive Sciences. 6112002</p>
<p>The Appeal of Parallel Distributed Processing. J L Mcclelland, D Rumelhart, G Hinton, Psychological and Biological Models. J L Mcclelland, D E Rumelhart, P R Group, Cambridge, MAMIT Press1986a2Parallel Distributed Processing</p>
<p>An interactive activation model of context effects in letter perception: I. An account of basic findings. J L Mcclelland, D E Rumelhart, Psychological Review. 8851981</p>
<p>J L Mcclelland, D E Rumelhart, Research Group, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press1986b2</p>
<p>Universal linguistic inductive biases via meta-learning. R T Mccoy, E Grant, P Smolensky, T L Griffiths, T Linzen, 2020</p>
<p>R T Mccoy, T Linzen, E Dunbar, P Smolensky, RNNs Implicitly Implement Tensor Product Representations. 2019</p>
<p>A logical calculus of the ideas immanent in nervous activity. W S Mcculloch, W Pitts, The bulletin of mathematical biophysics. 541943</p>
<p>How Can Deep Neural Networks Inform Theory in Psychological Science?. S Mcgrath, J Russin, E Pavlick, R Feiman, S W Mcgrath, J Russin, Proceedings of the 46th Annual Conference of the Cognitive Science Society (forthcoming). the 46th Annual Conference of the Cognitive Science Society (forthcoming)2023a. 2024Multiple Realizability and the Rise of Deep Learning</p>
<p>Properties of Lots: The Footprints or the Bear Itself?. S W Mcgrath, J Russin, E Pavlick, R Feiman, Behavioral and Brain Sciences. 46e2842023b</p>
<p>The connectionism/classicism battle to win souls. B P Mclaughlin, Philosophical Studies. 7121993</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, Nature. 62479902023</p>
<p>Circuit Component Reuse Across Tasks in Transformer Language Models. J Merullo, C Eickhoff, E Pavlick, 2024</p>
<p>An integrative theory of prefrontal cortex function. E K Miller, J D Cohen, Annual Review of Neuroscience. 242001</p>
<p>Philosophy of Cognitive Science in the Age of Deep Learning. R Millière, 2024</p>
<p>A Philosophical Introduction to Language Models -Part I: Continuity With Classic Debates. R Millière, C Buckner, R Millière, C Buckner, A Philosophical Introduction to Language Models -Part II: The Way Forward. 2024a. 2024b</p>
<p>Compositional Chain-of-Thought Prompting for Large Multimodal Models. C Mitra, B Huang, T Darrell, R Herzig, 2023</p>
<p>Formal Philosophy; Selected Papers of Richard Montague. R Montague, 1974Yale University PressNew Haven</p>
<p>Developing Cognitive Control: Three Key Transitions. Y Munakata, H R Snyder, C H Chatham, Current directions in psychological science. 2122012</p>
<p>M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, Show Your Work: Scratchpads for Intermediate Computation with Language Models. 2021</p>
<p>. C Olsson, N Elhage, N Nanda, N Joseph, N Dassarma, T Henighan, B Mann, A Askell, Y Bai, A Chen, T Conerly, D Drain, D Ganguli, Z Hatfield-Dodds, D Hernandez, S Johnston, A Jones, J Kernion, L Lovitt, K Ndousse, D Amodei, T Brown, J Clark, J Kaplan, S Mccandlish, C Olah, 2022In-context Learning and Induction Heads</p>
<p>Large language models in medicine: The potentials and pitfalls. J A Omiye, H Gui, S J Rezaei, J Zou, R Daneshjou, 2023</p>
<p>. Achiam Openai, J Adler, S Agarwal, S Ahmad, L Akkaya, I Aleman, F L Almeida, D Altenschmidt, J Altman, S Anadkat, S Avila, R Babuschkin, I Balaji, S Balcom, V Baltescu, P Bao, H Bavarian, M Belgum, J Bello, I Berdine, J Bernadett-Shapiro, G Berner, C Bogdonoff, L Boiko, O Boyd, M Brakman, A.-L Brockman, G Brooks, T Brundage, M Button, K Cai, T Campbell, R Cann, A Carey, B Carlson, C Carmichael, R Chan, B Chang, C Chantzis, F Chen, D Chen, S Chen, R Chen, J Chen, M Chess, B Cho, C Chu, C Chung, H W Cummings, D Currier, J Dai, Y Decareaux, C Degry, T Deutsch, N Deville, D Dhar, A Dohan, D Dowling, S Dunning, S Ecoffet, A Eleti, A Eloundou, T Farhi, D Fedus, L Felix, N Fishman, S P Forte, J Fulford, I Gao, L Georges, E Gibson, C Goel, V Gogineni, T Goh, G Gontijo-Lopes, R Gordon, J Grafstein, M Gray, S Greene, R Gross, J Gu, S S Guo, Y Hallacy, C Han, J Harris, J He, Y Heaton, M Heidecke, J Hesse, C Hickey, A Hickey, W Hoeschele, P Houghton, B Hsu, K Hu, S Hu, X Huizinga, J Jain, S Jain, S Jang, J Jiang, A Jiang, R Jin, H Jin, D Jomoto, S Jonn, B Jun, H Kaftan, T Kaiser, Ł Kamali, A Kanitscheider, I Keskar, N S Khan, T Kilpatrick, L Kim, J W Kim, C Kim, Y Kirchner, J H Kiros, J Knight, M Kokotajlo, D Kondraciuk, Ł Kondrich, A Konstantinidis, A Kosic, K Krueger, G Kuo, V Lampe, M Lan, I Lee, T Leike, J Leung, J Levy, D Li, C M Lim, R Lin, M Lin, S Litwin, M Lopez, T Lowe, R Lue, P Makanju, A Malfacini, K Manning, S Markov, T Markovski, Y Martin, B Mayer, K Mayne, A Mcgrew, B Mckinney, S M Mcleavey, C Mcmillan, P Mcneil, J Medina, D Mehta, A Menick, J Metz, L Mishchenko, A Mishkin, P Monaco, V Morikawa, E Mossing, D Mu, T Murati, M Murk, O Mély, D Nair, A Nakano, R Nayak, R Neelakantan, A Ngo, R Noh, H Ouyang, L O'keefe, C Pachocki, J Paino, A Palermo, J Pantuliano, A Parascandolo, G Parish, J Parparita, E Passos, A Pavlov, M Peng, A Perelman, A Peres, F D A B Petrov, M Pinto, H P D O Michael, Pokorny, M Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N Tezak, M B Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C J Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, B Zoph, 2024GPT-4 Technical Report</p>
<p>Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia. R C O'reilly, M J Frank, Neural Computation. 1822006</p>
<p>R C O'reilly, Y Munakata, M J Frank, T E Hazy, Contributors , Computational Cognitive Neuroscience. Wiki Book20121st Edition</p>
<p>Prompting Large Vision-Language Models for Compositional Reasoning. T Ossowski, M Jiang, J Hu, 2024</p>
<p>. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022Training language models to follow instructions with human feedback</p>
<p>Compositionality I: Definitions and Variants. P Pagin, D Westerståhl, Philosophy Compass. 532010</p>
<p>H Palangi, P Smolensky, X He, L Deng, arXiv:1705.08432Question-Answering with Grammatically-Interpretable Representations. 2017</p>
<p>Compositionality. B Partee, Compositionality in Formal Semantics. Blackwell Publishing2004</p>
<p>Symbols and grounding in large language models. E Pavlick, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 381202200412023. 2251</p>
<p>The Language of Thought in Late Medieval Philosophy: Essays in Honor of Claude Panaccio. J Pelletier, M Roques, 2017Springer International Publishing5Chamof Historical-Analytical Studies on Nature, Mind and Action</p>
<p>Modern language models refute Chomsky's approach to language. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Deep contextualized word representations. S Piantadosi, 2018. 2023</p>
<p>Compositional Reasoning in Early Childhood. S Piantadosi, R Aslin, PLOS ONE. 119e01477342016</p>
<p>S T Piantadosi, H Palmeri, R Aslin, Limits on composition of conceptual operations in 9-month-olds. Infancy : the official journal of the International Society on Infant Studies. 201823</p>
<p>Measuring and Narrowing the Compositionality Gap in Language Models. O Press, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, 2023</p>
<p>Do True Assertions Correspond to Reality?. H Putnam, Mind, Language and Reality: Philosophical Papers. Cambridge University Press19952</p>
<p>. W V Quine, Methodological Reflections on Current Linguistic Theory. Synthese. 213/41970</p>
<p>Neural Index of Reinforcement Learning Predicts Improved Stimulus-Response Retention under High Working Memory Load. R Rac-Lubashevsky, A Cremer, A G E Collins, M J Frank, L Schwabe, Journal of Neuroscience. 43172023</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Ramsey, W., Rumelhart, D. E., and Stitch, S. P.2019. 1991RoutledgePhilosophy and Connectionist Theory</p>
<p>Connectionism, Eliminativism and The Future of Folk Psychology. W Ramsey, S Stich, J Garon, Philosophical Perspectives. 41990</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J R Ruiz, J S Ellenberg, P Wang, O Fawzi, P Kohli, A Fawzi, Nature. 62579952024</p>
<p>The Linguistic Turn: Essays in Philosophical Method. R M Rorty, 1992University of Chicago PressChicago, IL</p>
<p>Prefrontal cortex and flexible cognitive control: Rules without symbols. N P Rougier, D C Noelle, T S Braver, J D Cohen, O' Reilly, R C , Proceedings of the National Academy of Sciences. 102202005</p>
<p>T Rudner, H Toner, Key Concepts in AI Safety: Interpretability in Machine Learning. 2021</p>
<p>A Benchmark for Systematic Generalization in Grounded Language Understanding. L Ruis, J Andreas, M Baroni, D Bouchacourt, B M Lake, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Learning representations by back-propagating errors. D E Rumelhart, G E Hinton, R J Williams, Nature. 32360881986a</p>
<p>On learning the past tenses of english verbs. D E Rumelhart, J L Mcclelland, Psychological and Biological Models. J L Mcclelland, D E Rumelhart, P R Group, Cambridge, MAMIT Press19862Parallel Distributed Processing</p>
<p>D E Rumelhart, J L Mcclelland, Group Research, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Cambridge, MA, USAPsychological and Biological Models. MIT Press1986b2</p>
<p>Compositional Processing Emerges in Neural Networks Solving Math Problems. J Russin, R Fernandez, H Palangi, E Rosen, N Jojic, P Smolensky, J Gao, Proceedings for the 43rd Annual Meeting of the Cognitive Science Society. for the 43rd Annual Meeting of the Cognitive Science Society2021</p>
<p>Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics. J Russin, J Jo, R C O'reilly, Y Bengio, Proceedings for the 42nd Annual Meeting of the Cognitive Science Society. for the 42nd Annual Meeting of the Cognitive Science Society20207</p>
<p>Is human compositionality meta-learned?. J Russin, S W Mcgrath, E Pavlick, M J Frank, Behavioral and Brain Sciences. 2024aforthcoming</p>
<p>Human Curriculum Effects Emerge with In-Context Learning in Neural Networks. J Russin, E Pavlick, M J Frank, Philosophy. 24882024b. 1949Discussion: Meaning and Necessity</p>
<p>The Theory of Meaning. G Ryle, British Philosophy in the Mid-Century. J H Muirhead, George Allen and Unwin1957</p>
<p>Modelling cognitive flexibility with deep neural networks. K Sandbrink, C Summerfield, Current Opinion in Behavioral Sciences. 571013612024</p>
<p>Meta-Learning with Memory-Augmented Neural Networks. A Santoro, S Bartunov, M Botvinick, D Wierstra, T Lillicrap, International Conference on Machine Learning. 2016</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, 7th Intern. Conf. on Lear. Repr., new orleans. LA, USA. OpenReview.net. 2019</p>
<p>Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving. I Schlag, P Smolensky, R Fernandez, N Jojic, J Schmidhuber, J Gao, arXiv:1910.066112019cs, stat</p>
<p>Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook. Master's thesis. J Schmidhuber, 1987GermanyTechnische Universitat Munchen</p>
<p>Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with. M Shukor, A Rame, C Dancette, M Cord, 2023Context Learning</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 52975872016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, T Lillicrap, K Simonyan, D Hassabis, Science. 36264192018</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, P Payne, M Seneviratne, P Gamble, C Kelly, A Babiker, N Schärli, A Chowdhery, P Mansfield, D Demner-Fushman, B Agüera Y Arcas, D Webster, G S Corrado, Y Matias, K Chou, J Gottweis, N Tomasev, Y Liu, A Rajkomar, J Barral, C Semturs, A Karthikesalingam, V Natarajan, Nature. 62079722023</p>
<p>Neural and Conceptual Interpretation of PDP Models. P Smolensky, Psychological and Biological Models. J L Mcclelland, D E Rumelhart, P R Group, Cambridge, MAMIT Press19862Parallel Distributed Processing</p>
<p>On the proper treatment of connectionism. P Smolensky, Behavioral and Brain Sciences. 1111988</p>
<p>Tensor product variable binding and the representation of symbolic structures in connectionist systems. P Smolensky, Artificial Intelligence. 461-21990</p>
<p>Connectionism, Constituency and the Language of Thought. P Smolensky, Meaning in Mind: Fodor and His Critics. B M Loewer, Blackwell1991</p>
<p>The Harmonic Mind. P Smolensky, G Legendre, From Neural Computation to Optimality-Theoretic Grammar. 12011Cognitive Architecture. MIT Pressreprint edition edition</p>
<p>Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems. P Smolensky, R T Mccoy, R Fernandez, M Goldrick, J Gao, AI Magazine. 4332022</p>
<p>Differentiable Tree Operations Promote Compositional Generalization. P Soulos, E J Hu, K Mccurdy, Y Chen, R Fernandez, P Smolensky, J Gao, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023</p>
<p>Discovering the Compositional Structure of Vector Representations with Role Learning Networks. P Soulos, T Mccoy, T Linzen, P Smolensky, Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP2020</p>
<p>SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant's Perspective. J Sullivan, M Mei, A Perfors, E Wojcik, M C Frank, Open Mind: Discoveries in Cognitive Science. 52021</p>
<p>Dissecting the Language Organ: A New Look at the Role of Broca's Area in Language Processing. S L Thompson-Schill, Twenty-First Century Psycholinguistics. Routledge2005</p>
<p>E Todd, M L Li, A S Sharma, A Mueller, B C Wallace, D Bau, Function Vectors in Large Language Models. 2024</p>
<p>Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. M Turpin, J Michael, E Perez, S Bowman, Advances in Neural Information Processing Systems. 202336</p>
<p>Compositionality: A Connectionist Variation on a Classical Theme. T Van Gelder, Cognitive Science. 1431990</p>
<p>Attention is All you Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, I Guyon, U Von Luxburg, S Bengio, H M Wallach, R Fergus, S V N Vishwanathan, R Garnett, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA2017. 2017, 4-9 December 2017</p>
<p>. J Von Oswald, E Niklasson, M Schlegel, S Kobayashi, N Zucchet, N Scherrer, N Miller, M Sandler, B A Arcas, M Vladymyrov, R Pascanu, J Sacramento, 2023Uncovering mesa-optimization algorithms in Transformers</p>
<p>Grounded language acquisition through the eyes and ears of a single child. W K Vong, W Wang, A E Orhan, B M Lake, Science. 38366822024</p>
<p>J X Wang, Meta-learning in natural and artificial intelligence. 2020</p>
<p>Prefrontal cortex as a meta-reinforcement learning system. J X Wang, Z Kurth-Nelson, D Kumaran, D Tirumala, H Soyer, J Z Leibo, D Hassabis, M Botvinick, Nature Neuroscience. 2162018</p>
<p>Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small. K Wang, A Variengien, A Conmy, B Shlegeris, J Steinhardt, 2022</p>
<p>What Artificial Neural Networks Can Tell Us About Human Language Acquisition. A Warstadt, S R Bowman, 2024</p>
<p>A Warstadt, L Choshen, A Mueller, A Williams, E Wilcox, C Zhuang, Call for Papers -The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. 2023a</p>
<p>Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora. A Warstadt, A Mueller, L Choshen, E Wilcox, C Zhuang, J Ciro, R Mosquera, B Paranjabe, A Williams, T Linzen, R Cotterell, A Warstadt, A Mueller, L Choshen, E Wilcox, C Zhuang, J Ciro, R Mosquera, B Paranjabe, A Williams, T Linzen, R Cotterell, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language LearningSingaporeAssociation for Computational Linguistics2023b</p>
<p>Reasoning about a Rule. P C Wason, Quarterly Journal of Experimental Psychology. 2031968</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 792023</p>
<p>T W Webb, S M Frankland, A Altabaa, K Krishnamurthy, D Campbell, J Russin, R O'reilly, J Lafferty, J D Cohen, The Relational Bottleneck as an Inductive Bias for Efficient Abstraction. 2024</p>
<p>T W Webb, I Sinha, J D Cohen, arXiv:2012.14601Emergent Symbols through Binding in External Memory. 2021</p>
<p>Do Prompt-Based Models Really Understand the Meaning of Their Prompts. A Webson, E Pavlick, Proceedings of the 2022 Conference of the North American Chapter. M Carpuat, M.-C De Marneffe, I V Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, Finetuned Language Models Are Zero-Shot Learners. 2022a</p>
<p>. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 2022bEmergent Abilities of Large Language Models</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Using Computational Models to Test Syntactic Learnability. E G Wilcox, R Futrell, R Levy, Linguistic Inquiry. 2023</p>
<p>Rule-Following, Objectivity and the Theory of Meaning. C Wright, Wittgenstein: To Follow A Rule. S H Holtzman, C M Leich, Routledge1981</p>
<p>Z Wu, A Geiger, T Icard, C Potts, N D Goodman, Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. 2023</p>
<p>. S M Xie, A Raghunathan, P Liang, T Ma, 2022An Explanation of In-context Learning as Implicit Bayesian Inference</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 2023</p>
<p>A survey on neural-symbolic learning systems. D Yu, B Yang, D Liu, H Wang, S Pan, Neural Networks. 1662023</p>
<p>Iterated Learning Improves Compositionality in Large Vision-Language Models. C Zheng, J Zhang, A Kembhavi, R Krishna, 2024</p>
<p>D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, Chi , E , Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. 2023a</p>
<p>. H Zhou, A Bradley, E Littwin, N Razin, O Saremi, J Susskind, S Bengio, P Nakkiran, 2023bWhat Algorithms can Transformers Learn? A Study in Length Generalization</p>
<p>The temporal stability of visuomotor adaptation generalization. W Zhou, J Fitzgerald, K Colucci-Chang, K G Murthy, W M Joiner, Journal of Neurophysiology. 11842017</p>
<p>Compositional diversity in visual concept learning. Y Zhou, R Feinman, B M Lake, Cognition. 2441057112024</p>
<p>Ð Žikelić, M Lechner, A Verma, K Chatterjee, T A Henzinger, Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1905 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1905</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1905</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-277510376</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.02069v1.pdf" target="_blank">RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics</a></p>
                <p><strong>Paper Abstract:</strong> Visual Language Models (VLMs) have emerged as pivotal tools for robotic systems, enabling cross-task generalization, dynamic environmental interaction, and long-horizon planning through multimodal perception and semantic reasoning. However, existing open-source VLMs predominantly trained for generic vision-language alignment tasks fail to model temporally correlated action semantics that are crucial for robotic manipulation effectively. While current image-based fine-tuning methods partially adapt VLMs to robotic applications, they fundamentally disregard temporal evolution patterns in video sequences and suffer from visual feature entanglement between robotic agents, manipulated objects, and environmental contexts, thereby limiting semantic decoupling capability for atomic actions and compromising model generalizability.To overcome these challenges, this work presents RoboAct-CLIP with dual technical contributions: 1) A dataset reconstruction framework that performs semantic-constrained action unit segmentation and re-annotation on open-source robotic videos, constructing purified training sets containing singular atomic actions (e.g.,"grasp"); 2) A temporal-decoupling fine-tuning strategy based on Contrastive Language-Image Pretraining (CLIP) architecture, which disentangles temporal action features across video frames from object-centric characteristics to achieve hierarchical representation learning of robotic atomic actions.Experimental results in simulated environments demonstrate that the RoboAct-CLIP pretrained model achieves a 12% higher success rate than baseline VLMs, along with superior generalization in multi-object manipulation tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1905.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1905.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboAct-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CLIP-derived video pre-training model that extracts temporally disentangled, subject/action/object features from robotic videos using frame-differencing + a Temporal Diff-Transformer and a feature-disentanglement + recombination training objective to improve atomic action understanding for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboAct-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extends CLIP by (1) using a frozen CLIP visual encoder to embed uniformly sampled frames, (2) computing consecutive frame-differences fed into a Transformer (Temporal Diff-Transformer) to capture temporal dynamics, and (3) applying a feature disentanglement module that projects the fused visual representation into subject/action/object branches with orthogonality constraints and a feature-bank recombination CLIP loss.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal video-text contrastive pretraining (CLIP-style) on robot manipulation videos with temporal modeling</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Derived from RH20T robotic video dataset after semantic-constrained atomic action segmentation and re-annotation: 199,797 videos, 143 tasks, 52 atomic actions, ~63,922,209 frames. Annotations include subject/action/object textual descriptions (atomic verbs like 'grasp', 'open', objects, and environments).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (atomic action execution and multi-step manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Robotic manipulation in Franka Kitchen simulated environment (tasks: opening drawer, pushing plate, placing cream cheese in bowl, turning on stove) and a real-world teleoperated sequence (open drawer, pick scotch tape, place tape, close drawer). Policies trained with the encoder frozen; success rate measured over episodes up to 200 timesteps. Action space not exhaustively specified but implied continuous robot arm control for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Yes — high: pretraining data was re-annotated into atomic subject/action/object triplets so training objective directly aligns visual temporal-action patterns with textual action descriptions; RH20T contains language descriptions for sequences enabling close overlap of object and verb semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Average success rate: reported as 12.0% higher than best-performing baseline (MPI (Base)) across evaluated manipulation tasks; absolute success rates not reported in the paper (only relative improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baselines (frozen encoders) include CLIP (no fine-tuning), R3M, MPI variants; absolute baseline rates are not reported in-text — only that RoboAct-CLIP outperforms all baselines and improves average success rate by 12.0% vs MPI (Base).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported quantitatively. The paper states encoders are frozen during policy learning and emphasizes better generalization, but provides no numeric comparison of samples/episodes-to-convergence between language-pretrained and non-language baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Partial: architecture uses self-attention (MultiHeadAttention) inside the disentanglement module, but the paper does not provide attention visualizations or quantitative analysis of attention focusing on semantically relevant regions/affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes at a design level: enforces orthogonality (cosine-sim minimization) between subject/action/object branches and maintains feature banks per semantic class; performs recombination CLIP loss to verify that disentangled features recombine to match text. No explicit clustering/PCA visualizations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes. Multiple mechanisms provide evidence: (1) temporal differencing + Transformer emphasizes motion dynamics over static context (grounding dynamics to action semantics); (2) disentanglement branches trained with orthogonality and auxiliary classification losses to represent subject/action/object separately; (3) feature-bank recombination and recombination CLIP loss show that recombined subject/action/object visual features can be matched to textual instructions (synthetic recombination example 'Robot opens the drawer, action is open'). Empirically, improved manipulation success and ablations support grounding claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Partly: model explicitly separates higher-level semantic branches (subject/object/action) and uses temporal-difference features to capture mid/high-level dynamics; the paper does not provide systematic analysis across low-level (edges/textures) vs high-level (objects/actions) beyond architectural design and ablation results.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Reported transfer is effective from pretraining on curated RH20T atomic videos to downstream policies in simulation and to a physical robot teleoperation dataset; success appears sensitive to temporally-segmented, semantically-pure pretraining data and disentanglement (removal of disentanglement loses robustness). Domain similarity (robotic manipulation videos to robotic tasks) and atomic-action alignment are emphasized as important.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No quantitative breakdown. The paper claims improved generalization in multi-object manipulation but does not report explicit performance split between objects seen during pretraining vs novel objects.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not demonstrated quantitatively. The paper emphasizes zero-shot/generalization capabilities conceptually and use of frozen encoder for downstream policy training, but does not report zero-shot task success rates or few-shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Ablations: encoders are frozen during policy training; ablation of the Temporal Diff-Transformer and Feature Disentanglement modules performed — removal of Feature Disentanglement caused a 6.5 percentage-point decrease in overall performance. No fine-grained per-layer probing provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit evidence reported that language pretraining hurts performance. The paper argues that prior VLMs lacking temporal modeling suffer in temporal tasks; no negative transfer quantified for RoboAct-CLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared against vision-focused baseline R3M (ResNet50 pretrained on visual data) and other baselines; RoboAct-CLIP outperformed all baselines, with an average improvement of 12% vs best baseline (MPI (Base)). Exact numeric comparisons vs R3M/CLIP not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Yes in method and ablations: uses frame differencing and a Transformer to capture temporal action dynamics; ablations indicate temporal module materially contributes to performance (text references performance drops when temporal modeling is ablated, although exact numbers are not all reported). The paper does not provide time-series learning curves across training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality metrics (PCA/intrinsic dimension) reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1905.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language contrastive model trained on image-text pairs to align visual and textual embedding spaces for zero-shot transfer across vision tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive vision-language model mapping images and text to a shared embedding space using dual encoders; used in this paper both as a frozen visual encoder for frame embeddings and as the backbone text encoder design inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs (contrastive)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale web image-caption pairs (generic objects, scenes, and captions). The paper emphasizes CLIP's training on static image-text pairs and its lack of temporal action modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as encoder for robotic manipulation feature extraction and used as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same Franka Kitchen manipulation tasks when used as baseline or as frozen visual encoder in RoboAct-CLIP; in the latter case CLIP processes individual frames (16 sampled) which are then temporally processed by RoboAct-CLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Partially discussed: CLIP's static image-text pretraining lacks temporal action verbs and thus has limited alignment with continuous robotic action dynamics; the paper argues a gap exists between CLIP pretraining data and temporally-correlated manipulation semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As baseline, CLIP (without fine-tuning) was used and outperformed by RoboAct-CLIP; exact numeric baseline performance for CLIP is not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable (CLIP is a language-vision pretrained model); comparisons to vision-only baselines (e.g., R3M) are discussed but numeric values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper for CLIP (no attention visualizations provided).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analyzed in detail here beyond noting CLIP's image-text alignment limitations for temporal actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper argues CLIP lacks temporal action grounding due to static-image training; no direct evidence for or against grounding provided for CLIP within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper claims CLIP transfers to robotic tasks only partially and misses temporal dynamics; thus domain mismatch (static images vs video actions) limits transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIP is used as zero-shot baseline conceptually, but no zero-shot task numbers are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported beyond qualitative discussion that CLIP's lack of temporal modeling reduces performance on action tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP is compared conceptually to vision-only models; exact numeric head-to-head results are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>CLIP lacks explicit temporal modeling; this limitation motivates RoboAct-CLIP's Temporal Diff-Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1905.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robotic-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic-CLIP: Fine-tuning CLIP on action data for robotic applications</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that fine-tunes CLIP with action initiation and completion frames via contrastive learning to improve action outcome understanding for robotics, but using sparse dual-frame comparisons rather than full temporal modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robotic-clip: Fine-tuning clip on action data for robotic applications.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Robotic-CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tunes CLIP on robot action data using contrastive objectives comparing frames that correspond to action start and completion to improve alignment of visual features with action outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language fine-tuning on robot action frames (dual-frame contrastive)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Robotic action videos with identified start and end frames; contains action outcome semantics but does not model intermediate states per this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic action understanding / manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Improves recognition of action outcomes (e.g., did an action achieve its goal) via frame pairs; not described as modeling continuous intermediate dynamics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Partially aligned: uses robotic data and action semantics but only aligns discrete start/end frames rather than full temporal sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper (mentioned as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper for Robotic-CLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Robotic-CLIP attempts to ground outcomes via start/end frame contrastive training, but the present paper criticizes it for failing to decode intermediate state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper suggests limitations (temporal sparsity) but no direct negative transfer numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not compared quantitatively in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Robotic-CLIP models only start/end frames (sparse temporal modeling) and thus lacks intermediate temporal modeling per the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1905.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2: Vision-Language-Action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end Vision-Language-Action model that discretizes actions into text tokens and learns joint mappings from visual inputs and language to action outputs, aimed at transferring web-scale knowledge to robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>End-to-end VLA architecture mapping visual and textual inputs to action tokens; joint training uses a hybrid of static images and robotic data but (per this paper) lacks explicit temporal modeling across video sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal (vision+language) with hybrid static image and robotic data; end-to-end action token training (as described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining mixes web/static image-language data and additional robotic data for action token learning; paper critiques it for lacking explicit temporal sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic control/action generation from vision and language</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>End-to-end generation of action tokens/commands for robot control; specifics not provided in this paper except that it discretizes actions into text tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Partially aligned via hybrid training data, but paper asserts RT-2 does not explicitly model temporal features in videos which limits alignment for continuous-action tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-2 performs end-to-end mapping to actions, but this paper criticizes the joint training for not explicitly modeling temporal features (no detailed grounding evidence provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>RT-2 is cited as transferring web knowledge to control, but no zero/few-shot metrics are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Paper highlights RT-2's limitation in temporal modeling; RT-2 does not explicitly capture temporal evolution in video sequences per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1905.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-language foundation models as effective robot imitators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLM-based framework enabling zero-shot cross-scenario generalization with minimal demonstration data for robot imitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vision-language foundation models as effective robot imitators.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboFlamingo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLM-based framework that adapts vision-language foundation models for robot imitation, aiming for zero-shot generalization with few demonstrations (paper-level description in the current article).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language foundation model pretraining (open-source VLMs) with adaptation for imitation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in the present paper; described generally as VLMs trained on image-text data and adapted with minimal demonstrations for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robot imitation and cross-scenario generalization</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Imitation learning for robotic tasks with minimal demonstrations; specific action space/details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not quantified in this paper; described as enabling cross-scenario generalization via VLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here (though described as 'minimal demonstration data' conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Claimed in prior work to support imitation via VLMs, but current paper does not provide empirical grounding details for RoboFlamingo.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Described conceptually as enabling zero-shot cross-scenario generalization; no numeric results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1905.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLM: Endowing vision-language models with spatial reasoning capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM variant that augments spatial reasoning to support 3D environment comprehension and dense reward annotation for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language model enhanced for spatial reasoning and 3D environment understanding to support tasks such as dense reward annotation and spatially-aware decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language with emphasis on spatial/3D reasoning (paper cites SpatialVLM as enhancing spatial capabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper; described conceptually as including spatial relationships and 3D environment information to improve spatial comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>3D environment comprehension / embodied tasks requiring spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Designed for tasks that require spatial reasoning and dense reward annotation in 3D environments; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Characterized as improving alignment for spatial concepts, but no quantitative overlap metrics provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Referenced as enhancing spatial reasoning, which can support grounding of spatial aspects of instructions; no empirical grounding evidence included here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1905.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OmniManip</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OmniManip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An object-centric approach that transforms VLM high-level reasoning into low-level high-precision actions using object-centric interaction primitives and a dual-loop design combining VLM planning and robotic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OmniManip</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines object-centric 3D interaction primitives with VLM planning to constrain and guide precise robotic execution; designed to mitigate hallucination and enable accurate low-level actions from VLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (object-centric interaction primitives combined with VLM planning; likely multimodal/3D-aware training in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>General robotic manipulation with object-centric constraints</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Transforms high-level VLM outputs into constrained low-level manipulation primitives for high-precision actions; specific environments not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Argued to improve alignment by imposing spatial/object-centric constraints on VLM plans, but no quantitative data provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Conceptually proposes grounding via object-centric primitives and dual-loop execution, but no empirical details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1905.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R3M: A universal visual representation for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ResNet50-based visual representation model pretrained on visual data to provide general-purpose visual features for robot manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>R3m: A universal visual representation for robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>R3M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ResNet50 backbone pretrained on visual datasets (video/robotic data per R3M original work) to produce visual embeddings for downstream robot manipulation tasks; used as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-only pretraining on visual (robotic and/or general) datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Described in this paper as 'pre-trained on visual data' to obtain general visual representations for robotics; specific dataset details are not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (baseline encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated within the same policy learning framework on Franka Kitchen tasks; encoder frozen during policy training when used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Generally aligned for visual features but lacks explicit language alignment (vision-only), so less alignment to textual action descriptions than VLMs per the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable (vision-only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Reported only qualitatively as a baseline that RoboAct-CLIP outperforms; exact baseline numeric success rates not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>R3M provides visual features useful for policies but lacks explicit grounding to language-conditioned actions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>R3M is itself a vision-only baseline; RoboAct-CLIP outperforms it per the authors' summary (exact numbers not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>R3M does not explicitly include the Temporal Diff-Transformer temporal-difference processing used by RoboAct-CLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1905.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1905.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MPI (multimodal predictive interaction model - referenced as Learning manipulation by predicting interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal transformer encoder-decoder architecture designed to predict image-goal interaction states and detect interaction objects, evaluated in two ViT sizes in this paper as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning manipulation by predicting interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MPI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal transformer encoder + transformer decoder model to predict interaction states and detect interaction objects; evaluated in ViT-small and ViT-Base variants as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not explicitly detailed in this paper; described as a multimodal transformer architecture for interaction prediction (pretraining specifics not given here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (baseline encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a frozen encoder within the same policy learning framework on Franka Kitchen tasks; evaluated as ViT-small and ViT-Base variants.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Treated as a competitive baseline with multimodal capabilities; exact overlap with target task semantics not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>MPI (Base) reported as the best-performing baseline; RoboAct-CLIP achieved 12.0% higher average success rate than MPI (Base). Absolute MPI success rates are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported beyond baseline behavior; training details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not detailed here; used as a baseline to compare end-task success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>MPI is a multimodal baseline; RoboAct-CLIP outperforms it by an average of 12% according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>MPI's temporal modeling specifics are not detailed in this paper; treated as a strong baseline but inferior to RoboAct-CLIP when temporal disentanglement is required.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Vision-language foundation models as effective robot imitators. <em>(Rating: 2)</em></li>
                <li>Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities. <em>(Rating: 2)</em></li>
                <li>Robotic-clip: Fine-tuning clip on action data for robotic applications. <em>(Rating: 2)</em></li>
                <li>R3m: A universal visual representation for robot manipulation. <em>(Rating: 2)</em></li>
                <li>Learning manipulation by predicting interaction. <em>(Rating: 2)</em></li>
                <li>Openflamingo: An open-source framework for training large autoregressive visionlanguage models. <em>(Rating: 1)</em></li>
                <li>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1905",
    "paper_id": "paper-277510376",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "RoboAct-CLIP",
            "name_full": "RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics",
            "brief_description": "A CLIP-derived video pre-training model that extracts temporally disentangled, subject/action/object features from robotic videos using frame-differencing + a Temporal Diff-Transformer and a feature-disentanglement + recombination training objective to improve atomic action understanding for robotic manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoboAct-CLIP",
            "model_description": "Extends CLIP by (1) using a frozen CLIP visual encoder to embed uniformly sampled frames, (2) computing consecutive frame-differences fed into a Transformer (Temporal Diff-Transformer) to capture temporal dynamics, and (3) applying a feature disentanglement module that projects the fused visual representation into subject/action/object branches with orthogonality constraints and a feature-bank recombination CLIP loss.",
            "pretraining_type": "multimodal video-text contrastive pretraining (CLIP-style) on robot manipulation videos with temporal modeling",
            "pretraining_data_description": "Derived from RH20T robotic video dataset after semantic-constrained atomic action segmentation and re-annotation: 199,797 videos, 143 tasks, 52 atomic actions, ~63,922,209 frames. Annotations include subject/action/object textual descriptions (atomic verbs like 'grasp', 'open', objects, and environments).",
            "target_task_name": "Robotic manipulation (atomic action execution and multi-step manipulation)",
            "target_task_description": "Robotic manipulation in Franka Kitchen simulated environment (tasks: opening drawer, pushing plate, placing cream cheese in bowl, turning on stove) and a real-world teleoperated sequence (open drawer, pick scotch tape, place tape, close drawer). Policies trained with the encoder frozen; success rate measured over episodes up to 200 timesteps. Action space not exhaustively specified but implied continuous robot arm control for manipulation.",
            "semantic_alignment": "Yes — high: pretraining data was re-annotated into atomic subject/action/object triplets so training objective directly aligns visual temporal-action patterns with textual action descriptions; RH20T contains language descriptions for sequences enabling close overlap of object and verb semantics.",
            "performance_with_language_pretraining": "Average success rate: reported as 12.0% higher than best-performing baseline (MPI (Base)) across evaluated manipulation tasks; absolute success rates not reported in the paper (only relative improvement).",
            "performance_without_language_pretraining": "Baselines (frozen encoders) include CLIP (no fine-tuning), R3M, MPI variants; absolute baseline rates are not reported in-text — only that RoboAct-CLIP outperforms all baselines and improves average success rate by 12.0% vs MPI (Base).",
            "sample_efficiency_comparison": "Not reported quantitatively. The paper states encoders are frozen during policy learning and emphasizes better generalization, but provides no numeric comparison of samples/episodes-to-convergence between language-pretrained and non-language baselines.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Partial: architecture uses self-attention (MultiHeadAttention) inside the disentanglement module, but the paper does not provide attention visualizations or quantitative analysis of attention focusing on semantically relevant regions/affordances.",
            "embedding_space_analysis": "Yes at a design level: enforces orthogonality (cosine-sim minimization) between subject/action/object branches and maintains feature banks per semantic class; performs recombination CLIP loss to verify that disentangled features recombine to match text. No explicit clustering/PCA visualizations reported.",
            "action_grounding_evidence": "Yes. Multiple mechanisms provide evidence: (1) temporal differencing + Transformer emphasizes motion dynamics over static context (grounding dynamics to action semantics); (2) disentanglement branches trained with orthogonality and auxiliary classification losses to represent subject/action/object separately; (3) feature-bank recombination and recombination CLIP loss show that recombined subject/action/object visual features can be matched to textual instructions (synthetic recombination example 'Robot opens the drawer, action is open'). Empirically, improved manipulation success and ablations support grounding claims.",
            "hierarchical_features_evidence": "Partly: model explicitly separates higher-level semantic branches (subject/object/action) and uses temporal-difference features to capture mid/high-level dynamics; the paper does not provide systematic analysis across low-level (edges/textures) vs high-level (objects/actions) beyond architectural design and ablation results.",
            "transfer_conditions": "Reported transfer is effective from pretraining on curated RH20T atomic videos to downstream policies in simulation and to a physical robot teleoperation dataset; success appears sensitive to temporally-segmented, semantically-pure pretraining data and disentanglement (removal of disentanglement loses robustness). Domain similarity (robotic manipulation videos to robotic tasks) and atomic-action alignment are emphasized as important.",
            "novel_vs_familiar_objects": "No quantitative breakdown. The paper claims improved generalization in multi-object manipulation but does not report explicit performance split between objects seen during pretraining vs novel objects.",
            "zero_shot_or_few_shot": "Not demonstrated quantitatively. The paper emphasizes zero-shot/generalization capabilities conceptually and use of frozen encoder for downstream policy training, but does not report zero-shot task success rates or few-shot counts.",
            "layer_analysis": "Ablations: encoders are frozen during policy training; ablation of the Temporal Diff-Transformer and Feature Disentanglement modules performed — removal of Feature Disentanglement caused a 6.5 percentage-point decrease in overall performance. No fine-grained per-layer probing provided.",
            "negative_transfer_evidence": "No explicit evidence reported that language pretraining hurts performance. The paper argues that prior VLMs lacking temporal modeling suffer in temporal tasks; no negative transfer quantified for RoboAct-CLIP.",
            "comparison_to_vision_only": "Compared against vision-focused baseline R3M (ResNet50 pretrained on visual data) and other baselines; RoboAct-CLIP outperformed all baselines, with an average improvement of 12% vs best baseline (MPI (Base)). Exact numeric comparisons vs R3M/CLIP not provided in-text.",
            "temporal_dynamics": "Yes in method and ablations: uses frame differencing and a Transformer to capture temporal action dynamics; ablations indicate temporal module materially contributes to performance (text references performance drops when temporal modeling is ablated, although exact numbers are not all reported). The paper does not provide time-series learning curves across training steps.",
            "dimensionality_analysis": "No explicit dimensionality metrics (PCA/intrinsic dimension) reported.",
            "uuid": "e1905.0"
        },
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pre-training)",
            "brief_description": "A vision-language contrastive model trained on image-text pairs to align visual and textual embedding spaces for zero-shot transfer across vision tasks.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_description": "Contrastive vision-language model mapping images and text to a shared embedding space using dual encoders; used in this paper both as a frozen visual encoder for frame embeddings and as the backbone text encoder design inspiration.",
            "pretraining_type": "vision-language on image-text pairs (contrastive)",
            "pretraining_data_description": "Large-scale web image-caption pairs (generic objects, scenes, and captions). The paper emphasizes CLIP's training on static image-text pairs and its lack of temporal action modeling.",
            "target_task_name": "Used as encoder for robotic manipulation feature extraction and used as baseline",
            "target_task_description": "Same Franka Kitchen manipulation tasks when used as baseline or as frozen visual encoder in RoboAct-CLIP; in the latter case CLIP processes individual frames (16 sampled) which are then temporally processed by RoboAct-CLIP.",
            "semantic_alignment": "Partially discussed: CLIP's static image-text pretraining lacks temporal action verbs and thus has limited alignment with continuous robotic action dynamics; the paper argues a gap exists between CLIP pretraining data and temporally-correlated manipulation semantics.",
            "performance_with_language_pretraining": "As baseline, CLIP (without fine-tuning) was used and outperformed by RoboAct-CLIP; exact numeric baseline performance for CLIP is not reported in the paper.",
            "performance_without_language_pretraining": "Not applicable (CLIP is a language-vision pretrained model); comparisons to vision-only baselines (e.g., R3M) are discussed but numeric values not provided.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper for CLIP (no attention visualizations provided).",
            "embedding_space_analysis": "Not analyzed in detail here beyond noting CLIP's image-text alignment limitations for temporal actions.",
            "action_grounding_evidence": "Paper argues CLIP lacks temporal action grounding due to static-image training; no direct evidence for or against grounding provided for CLIP within this work.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Paper claims CLIP transfers to robotic tasks only partially and misses temporal dynamics; thus domain mismatch (static images vs video actions) limits transfer.",
            "novel_vs_familiar_objects": "Not analyzed here.",
            "zero_shot_or_few_shot": "CLIP is used as zero-shot baseline conceptually, but no zero-shot task numbers are reported in this paper.",
            "layer_analysis": "Not performed here.",
            "negative_transfer_evidence": "Not reported beyond qualitative discussion that CLIP's lack of temporal modeling reduces performance on action tasks.",
            "comparison_to_vision_only": "CLIP is compared conceptually to vision-only models; exact numeric head-to-head results are not provided.",
            "temporal_dynamics": "CLIP lacks explicit temporal modeling; this limitation motivates RoboAct-CLIP's Temporal Diff-Transformer.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.1"
        },
        {
            "name_short": "Robotic-CLIP",
            "name_full": "Robotic-CLIP: Fine-tuning CLIP on action data for robotic applications",
            "brief_description": "A prior work that fine-tunes CLIP with action initiation and completion frames via contrastive learning to improve action outcome understanding for robotics, but using sparse dual-frame comparisons rather than full temporal modeling.",
            "citation_title": "Robotic-clip: Fine-tuning clip on action data for robotic applications.",
            "mention_or_use": "mention",
            "model_name": "Robotic-CLIP",
            "model_description": "Fine-tunes CLIP on robot action data using contrastive objectives comparing frames that correspond to action start and completion to improve alignment of visual features with action outcomes.",
            "pretraining_type": "vision-language fine-tuning on robot action frames (dual-frame contrastive)",
            "pretraining_data_description": "Robotic action videos with identified start and end frames; contains action outcome semantics but does not model intermediate states per this paper's description.",
            "target_task_name": "Robotic action understanding / manipulation",
            "target_task_description": "Improves recognition of action outcomes (e.g., did an action achieve its goal) via frame pairs; not described as modeling continuous intermediate dynamics in this paper.",
            "semantic_alignment": "Partially aligned: uses robotic data and action semantics but only aligns discrete start/end frames rather than full temporal sequences.",
            "performance_with_language_pretraining": "Not reported in this paper (mentioned as prior work).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper for Robotic-CLIP.",
            "embedding_space_analysis": "Not discussed here.",
            "action_grounding_evidence": "Robotic-CLIP attempts to ground outcomes via start/end frame contrastive training, but the present paper criticizes it for failing to decode intermediate state transitions.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not detailed in this paper.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Paper suggests limitations (temporal sparsity) but no direct negative transfer numbers given.",
            "comparison_to_vision_only": "Not compared quantitatively in this paper.",
            "temporal_dynamics": "Robotic-CLIP models only start/end frames (sparse temporal modeling) and thus lacks intermediate temporal modeling per the present paper.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.2"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2: Vision-Language-Action models transfer web knowledge to robotic control",
            "brief_description": "An end-to-end Vision-Language-Action model that discretizes actions into text tokens and learns joint mappings from visual inputs and language to action outputs, aimed at transferring web-scale knowledge to robot control.",
            "citation_title": "Rt-2: Visionlanguage-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "End-to-end VLA architecture mapping visual and textual inputs to action tokens; joint training uses a hybrid of static images and robotic data but (per this paper) lacks explicit temporal modeling across video sequences.",
            "pretraining_type": "Multimodal (vision+language) with hybrid static image and robotic data; end-to-end action token training (as described in this paper).",
            "pretraining_data_description": "Pretraining mixes web/static image-language data and additional robotic data for action token learning; paper critiques it for lacking explicit temporal sequence modeling.",
            "target_task_name": "Robotic control/action generation from vision and language",
            "target_task_description": "End-to-end generation of action tokens/commands for robot control; specifics not provided in this paper except that it discretizes actions into text tokens.",
            "semantic_alignment": "Partially aligned via hybrid training data, but paper asserts RT-2 does not explicitly model temporal features in videos which limits alignment for continuous-action tasks.",
            "performance_with_language_pretraining": "Not reported in this paper (cited as related work).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "RT-2 performs end-to-end mapping to actions, but this paper criticizes the joint training for not explicitly modeling temporal features (no detailed grounding evidence provided here).",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not detailed in this paper.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "RT-2 is cited as transferring web knowledge to control, but no zero/few-shot metrics are provided here.",
            "layer_analysis": "Not discussed here.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "Not directly compared in this paper.",
            "temporal_dynamics": "Paper highlights RT-2's limitation in temporal modeling; RT-2 does not explicitly capture temporal evolution in video sequences per the authors.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.3"
        },
        {
            "name_short": "RoboFlamingo",
            "name_full": "Vision-language foundation models as effective robot imitators",
            "brief_description": "An open-source VLM-based framework enabling zero-shot cross-scenario generalization with minimal demonstration data for robot imitation.",
            "citation_title": "Vision-language foundation models as effective robot imitators.",
            "mention_or_use": "mention",
            "model_name": "RoboFlamingo",
            "model_description": "A VLM-based framework that adapts vision-language foundation models for robot imitation, aiming for zero-shot generalization with few demonstrations (paper-level description in the current article).",
            "pretraining_type": "Vision-language foundation model pretraining (open-source VLMs) with adaptation for imitation tasks.",
            "pretraining_data_description": "Not detailed in the present paper; described generally as VLMs trained on image-text data and adapted with minimal demonstrations for robotics.",
            "target_task_name": "Robot imitation and cross-scenario generalization",
            "target_task_description": "Imitation learning for robotic tasks with minimal demonstrations; specific action space/details not provided here.",
            "semantic_alignment": "Not quantified in this paper; described as enabling cross-scenario generalization via VLM alignment.",
            "performance_with_language_pretraining": "Not reported here (cited as related work).",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported here (though described as 'minimal demonstration data' conceptually).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Claimed in prior work to support imitation via VLMs, but current paper does not provide empirical grounding details for RoboFlamingo.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not provided here.",
            "novel_vs_familiar_objects": "Not provided here.",
            "zero_shot_or_few_shot": "Described conceptually as enabling zero-shot cross-scenario generalization; no numeric results provided in this paper.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "Not provided here.",
            "temporal_dynamics": "Not discussed here.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.4"
        },
        {
            "name_short": "SpatialVLM",
            "name_full": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities",
            "brief_description": "A VLM variant that augments spatial reasoning to support 3D environment comprehension and dense reward annotation for embodied tasks.",
            "citation_title": "Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities.",
            "mention_or_use": "mention",
            "model_name": "SpatialVLM",
            "model_description": "Vision-language model enhanced for spatial reasoning and 3D environment understanding to support tasks such as dense reward annotation and spatially-aware decision-making.",
            "pretraining_type": "Vision-language with emphasis on spatial/3D reasoning (paper cites SpatialVLM as enhancing spatial capabilities).",
            "pretraining_data_description": "Not detailed in this paper; described conceptually as including spatial relationships and 3D environment information to improve spatial comprehension.",
            "target_task_name": "3D environment comprehension / embodied tasks requiring spatial reasoning",
            "target_task_description": "Designed for tasks that require spatial reasoning and dense reward annotation in 3D environments; specifics not provided in this paper.",
            "semantic_alignment": "Characterized as improving alignment for spatial concepts, but no quantitative overlap metrics provided here.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper.",
            "embedding_space_analysis": "Not provided in this paper.",
            "action_grounding_evidence": "Referenced as enhancing spatial reasoning, which can support grounding of spatial aspects of instructions; no empirical grounding evidence included here.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not provided here.",
            "novel_vs_familiar_objects": "Not provided here.",
            "zero_shot_or_few_shot": "Not provided here.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "Not provided here.",
            "temporal_dynamics": "Not discussed here.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.5"
        },
        {
            "name_short": "OmniManip",
            "name_full": "OmniManip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints",
            "brief_description": "An object-centric approach that transforms VLM high-level reasoning into low-level high-precision actions using object-centric interaction primitives and a dual-loop design combining VLM planning and robotic execution.",
            "citation_title": "Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints.",
            "mention_or_use": "mention",
            "model_name": "OmniManip",
            "model_description": "Combines object-centric 3D interaction primitives with VLM planning to constrain and guide precise robotic execution; designed to mitigate hallucination and enable accurate low-level actions from VLM outputs.",
            "pretraining_type": "Not specified in this paper (object-centric interaction primitives combined with VLM planning; likely multimodal/3D-aware training in original work).",
            "pretraining_data_description": "Not described in this paper.",
            "target_task_name": "General robotic manipulation with object-centric constraints",
            "target_task_description": "Transforms high-level VLM outputs into constrained low-level manipulation primitives for high-precision actions; specific environments not detailed here.",
            "semantic_alignment": "Argued to improve alignment by imposing spatial/object-centric constraints on VLM plans, but no quantitative data provided here.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Conceptually proposes grounding via object-centric primitives and dual-loop execution, but no empirical details are provided in this paper.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not provided here.",
            "novel_vs_familiar_objects": "Not provided here.",
            "zero_shot_or_few_shot": "Not provided here.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "Not provided here.",
            "temporal_dynamics": "Not discussed here.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.6"
        },
        {
            "name_short": "R3M",
            "name_full": "R3M: A universal visual representation for robot manipulation",
            "brief_description": "A ResNet50-based visual representation model pretrained on visual data to provide general-purpose visual features for robot manipulation tasks.",
            "citation_title": "R3m: A universal visual representation for robot manipulation.",
            "mention_or_use": "use",
            "model_name": "R3M",
            "model_description": "ResNet50 backbone pretrained on visual datasets (video/robotic data per R3M original work) to produce visual embeddings for downstream robot manipulation tasks; used as a baseline in this paper.",
            "pretraining_type": "Vision-only pretraining on visual (robotic and/or general) datasets",
            "pretraining_data_description": "Described in this paper as 'pre-trained on visual data' to obtain general visual representations for robotics; specific dataset details are not given in this paper.",
            "target_task_name": "Robotic manipulation (baseline encoder)",
            "target_task_description": "Evaluated within the same policy learning framework on Franka Kitchen tasks; encoder frozen during policy training when used as baseline.",
            "semantic_alignment": "Generally aligned for visual features but lacks explicit language alignment (vision-only), so less alignment to textual action descriptions than VLMs per the paper's discussion.",
            "performance_with_language_pretraining": "Not applicable (vision-only).",
            "performance_without_language_pretraining": "Reported only qualitatively as a baseline that RoboAct-CLIP outperforms; exact baseline numeric success rates not provided.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "R3M provides visual features useful for policies but lacks explicit grounding to language-conditioned actions in this paper.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not discussed in detail in this paper.",
            "novel_vs_familiar_objects": "Not provided here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "R3M is itself a vision-only baseline; RoboAct-CLIP outperforms it per the authors' summary (exact numbers not provided).",
            "temporal_dynamics": "R3M does not explicitly include the Temporal Diff-Transformer temporal-difference processing used by RoboAct-CLIP.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.7"
        },
        {
            "name_short": "MPI",
            "name_full": "MPI (multimodal predictive interaction model - referenced as Learning manipulation by predicting interaction)",
            "brief_description": "A multimodal transformer encoder-decoder architecture designed to predict image-goal interaction states and detect interaction objects, evaluated in two ViT sizes in this paper as a strong baseline.",
            "citation_title": "Learning manipulation by predicting interaction.",
            "mention_or_use": "use",
            "model_name": "MPI",
            "model_description": "Multimodal transformer encoder + transformer decoder model to predict interaction states and detect interaction objects; evaluated in ViT-small and ViT-Base variants as baselines.",
            "pretraining_type": "Not explicitly detailed in this paper; described as a multimodal transformer architecture for interaction prediction (pretraining specifics not given here).",
            "pretraining_data_description": "Not described in this paper.",
            "target_task_name": "Robotic manipulation (baseline encoder)",
            "target_task_description": "Used as a frozen encoder within the same policy learning framework on Franka Kitchen tasks; evaluated as ViT-small and ViT-Base variants.",
            "semantic_alignment": "Treated as a competitive baseline with multimodal capabilities; exact overlap with target task semantics not quantified here.",
            "performance_with_language_pretraining": "MPI (Base) reported as the best-performing baseline; RoboAct-CLIP achieved 12.0% higher average success rate than MPI (Base). Absolute MPI success rates are not reported.",
            "performance_without_language_pretraining": "Not reported beyond baseline behavior; training details not provided here.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not detailed here; used as a baseline to compare end-task success rates.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Not discussed here.",
            "novel_vs_familiar_objects": "Not provided here.",
            "zero_shot_or_few_shot": "Not provided here.",
            "layer_analysis": "Not provided here.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "MPI is a multimodal baseline; RoboAct-CLIP outperforms it by an average of 12% according to the paper.",
            "temporal_dynamics": "MPI's temporal modeling specifics are not detailed in this paper; treated as a strong baseline but inferior to RoboAct-CLIP when temporal disentanglement is required.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1905.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Visionlanguage-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Vision-language foundation models as effective robot imitators.",
            "rating": 2
        },
        {
            "paper_title": "Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities.",
            "rating": 2
        },
        {
            "paper_title": "Robotic-clip: Fine-tuning clip on action data for robotic applications.",
            "rating": 2
        },
        {
            "paper_title": "R3m: A universal visual representation for robot manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Learning manipulation by predicting interaction.",
            "rating": 2
        },
        {
            "paper_title": "Openflamingo: An open-source framework for training large autoregressive visionlanguage models.",
            "rating": 1
        },
        {
            "paper_title": "Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints.",
            "rating": 2
        }
    ],
    "cost": 0.01944825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics
2 Apr 2025</p>
<p>Zhiyuan Zhang 
ROAS Thrust, System Hub
Hong Kong University of Science and Technology (Guangzhou)
GuangdongChina</p>
<p>Yuxin He 
ROAS Thrust, System Hub
Hong Kong University of Science and Technology (Guangzhou)
GuangdongChina</p>
<p>Yong Sun 
ROAS Thrust, System Hub
Hong Kong University of Science and Technology (Guangzhou)
GuangdongChina</p>
<p>Junyu Shi 
ROAS Thrust, System Hub
Hong Kong University of Science and Technology (Guangzhou)
GuangdongChina</p>
<p>Lijiang Liu 
ROAS Thrust, System Hub
Hong Kong University of Science and Technology (Guangzhou)
GuangdongChina</p>
<p>Qiang Nie qiangnie@hkust-gz.edu.cn 
ROAS Thrust, System Hub
Hong Kong University of Science and Technology (Guangzhou)
GuangdongChina</p>
<p>RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics
2 Apr 202528BE664C428529BB3FC31DDB708AABF5arXiv:2504.02069v1[cs.RO]
Visual Language Models (VLMs) have emerged as pivotal tools for robotic systems, enabling cross-task generalization, dynamic environmental interaction, and long-horizon planning through multimodal perception and semantic reasoning.However, existing open-source VLMs predominantly trained for generic vision-language alignment tasks fail to model temporally correlated action semantics that are crucial for robotic manipulation effectively.While current imagebased fine-tuning methods partially adapt VLMs to robotic applications, they fundamentally disregard temporal evolution patterns in video sequences and suffer from visual feature entanglement between robotic agents, manipulated objects, and environmental contexts, thereby limiting semantic decoupling capability for atomic actions and compromising model generalizability.To overcome these challenges, this work presents RoboAct-CLIP with dual technical contributions: 1) A dataset reconstruction framework that performs semantic-constrained action unit segmentation and re-annotation on open-source robotic videos, constructing purified training sets containing singular atomic actions (e.g., "grasp"); 2) A temporaldecoupling fine-tuning strategy based on Contrastive Language-Image Pretraining (CLIP) architecture, which disentangles temporal action features across video frames from objectcentric characteristics to achieve hierarchical representation learning of robotic atomic actions.Experimental results in simulated environments demonstrate that the RoboAct-CLIP pretrained model achieves a 12% higher success rate than baseline VLMs, along with superior generalization in multiobject manipulation tasks.Cross-platform validation on physical robotic arms confirms the method's capability for stable atomic action execution.The proposed framework advances robotic action semantics understanding by effectively resolving temporal feature entanglement while providing a systematic paradigm for adapting VLMs to temporally sensitive robotic applications.</p>
<p>I. INTRODUCTION</p>
<p>Recently, the application of Vision-Language Models (VLMs) in robotics has garnered significant attention, with their multimodal integration capabilities driving revolutionary advancements in robotic perception, decision-making, and control paradigms [1], [2], [3].By establishing deep alignment between visual representations and linguistic semantics, VLMs demonstrate exceptional proficiency in interpreting complex environmental scenarios while achieving task generalization through natural language instructions.Notable breakthroughs include RT-2 [4], which pioneered the extension of VLMs into Vision-Language-Action (VLA) models through an end-to-end architecture for direct robotic action generation; RoboFlamingo [5], an open-source VLM-based framework enabling zero-shot cross-scenario generalization with minimal demonstration data; and SpatialVLM [6], which enhances spatial reasoning capabilities to support 3D environment comprehension and dense reward annotation.The integration of closed-loop feedback systems [7] and atomic action decomposition methodologies [8] has further enhanced the practical utility of VLMs in dynamic task execution.These collective advancements signify that VLMbased robotic systems are progressively transcending the limitations of conventional approaches in semantic understanding and generalization capabilities, thereby establishing crucial groundwork for the development of universal intelligent agents.</p>
<p>Although VLMs have demonstrated potential in robotic applications, their training paradigms exhibit notable limitations.Current mainstream VLMs (e.g., CLIP [9], OpenFlamingo [10]) predominantly rely on static image-text pairs with minimal incorporation of robotic domain data, resulting in inadequate capture of temporal dynamics in continuous actions.While some studies attempt domainspecific fine-tuning using robotic data [11], [12] -exemplified by Robotic-CLIP [12]'s contrastive learning approach that incorporates action initiation and completion frames to enhance action outcome understanding -their implicit modeling of action processes through dual-frame comparisons fails to decode intermediate state transitions in continuous operations, such as trajectory optimization during grasping or fluid dynamic adjustments in pouring tasks.Similarly, RT-2 [4] achieves end-to-end control through discretizing actions into text tokens, yet its joint training paradigm employing hybrid static image and robotic data inputs lacks explicit modeling of temporal features in video sequences.The neglect of temporal information may lead to two critical issues: first, models struggle to distinguish between action intentions and transient states (e.g., sequential "moving cup" versus "tilting cup" actions); second, cumulative errors in long-horizon tasks can cause policy trajectories to deviate significantly from expected paths [13], [14], [15].These deficiencies reveal that current fine-tuning paradigms relying on static or sparsely sampled temporal data remain insufficient to meet the spatiotemporal modeling precision required for robotic atomic action understanding.</p>
<p>Furthermore, the visual features of the robot itself, manipulated objects, and background environment are highly entangled in video data, which exacerbates the difficulty of semantic decoupling for atomic actions.In complex operational scenarios (such as kitchen-based tableware organization), the motion trajectories of robotic arms, morphological changes in target objects (such as fluid dynamics during pouring), and background clutter (such as scattered items on tabletops) often create interfering visual features, making it challenging for models to distinguish between action subjects and contextual noise.This visual feature uncertainty tends to amplify hallucination phenomena in VLMs, thereby affecting the alignment between visual inputs and textual instructions, and consequently impacting overall model performance [16], [17].To address this issue, approaches such as DP3 [18] construct more efficient and complex visual representation modules to obtain more accurate visual features, while ACP [19] dynamically adjusts prediction intervals based on real-time data and integrates long-term memory.This methodology enables robots to recognize their limitations and seek human assistance when necessary.The integration of long-term memory within the ACP framework allows robots to learn from past experiences and continuously refine their decisionmaking processes.OmniManip [20], based on object-centered 3D interaction primitives, transforms the high-level reasoning capabilities of VLMs into low-level, high-precision robotic actions through a dual-loop system design incorporating VLM planning and robotic execution to mitigate related hallucination issues.However, existing methods still rely on manually defined complex network architectures and lack the ability to autonomously discover semantic boundaries of atomic actions from video sequences, which limits the adaptability of VLM models in open-world scenarios.</p>
<p>To address the aforementioned challenges, we present RoboAct-CLIP, a video pre-training model specifically designed for robotic atomic action understanding.Unlike existing approaches, our core innovation lies in the design of a Temporal Diff-Transformer module that enhances the model's ability to extract and comprehend temporal features of atomic actions in videos, coupled with a feature disentanglement architecture that explicitly separates robotic embodiment features, action semantic features, and object manipulation features.Through three recombination modules utilizing Compositional CLIP Loss to align recombined features with reconstructed textual instructions, we further enhance the disentanglement effect.As illustrated in Figure 1, our model captures fine-grained temporal action information through frame differencing and temporal Transformer mechanisms, while the disentanglement module maintains low similarity between feature branches, thereby achieving purer action representations.This disentangled design enables the model to focus on the action itself, unaffected by environmental changes or object appearance variations.To support model training, we developed a semantically-guided data filtering and re-annotation methodology, screening video datasets into atomic action units and re-annotating them to ensure each training sample contains only a single, well-defined action semantic.Experimental results demonstrate that our approach significantly outperforms baseline models in robotic policy learning tasks and exhibits superior performance in multitask generalization tests.</p>
<p>Our main contributions are as follows:</p>
<p>• We propose a novel framework, RoboAct-CLIP, that enables robots to understand the essence of atomic actions through temporal-aware feature decoupling.To support this framework, we develop a comprehensive atomic action video filtering and re-annotation paradigm that provides high-quality training data.• We design a temporal modeling method based on frame differencing and Transformer architecture, effectively capturing temporal action features in action videos • We introduce a novel action feature disentanglement architecture, achieving pure representation learning of robotic actions through cosine similarity minimization and L2 regularization • We validate the effectiveness of our model through simulation and physical robot experiments, demonstrating its potential in practical applications II.METHODOLOGY In this section, we present the methodological framework for training RoboAct-CLIP.Our approach initiates with the establishment of a comprehensive curation and annotation pipeline based on open-source robotic video datasets.Subsequently, we architect RoboAct-CLIP through the systematic integration of two novel components: a Temporal Difference Transformer module for temporal reasoning and a Feature Disentanglement module for representation decoupling.The implementation paradigm for applying this architecture to robotic policy learning is then formally elaborated, demonstrating its operationalization in sequential decision-making tasks.</p>
<p>A. Dataset Preparation</p>
<p>Here, we have selected the RH20T [21] dataset, an opensource collection.The RH20T dataset encompasses over 110,000 contact-rich robot manipulation sequences, spanning a variety of actions, environments, robots, and camera viewpoints.Additionally, the dataset provides corresponding human demonstration videos and language descriptions for each robotic sequence.Algorithm 1 illustrates our data preparation process:</p>
<p>As shown in Table I, after processing, the dataset consists of 199,797 videos categorized into 143 unique tasks.These videos encompass 52 different atomic actions and contain a total of 63,922,209 frames.</p>
<p>B. RoboAct-CLIP</p>
<p>Our model extends the CLIP architecture by incorporating a temporal difference Transformer and feature disentanglement modules to achieve fine-grained understanding of robot manipulation actions.The core architecture is illustrated in Figure 1.Unzip dataset containing video files and their corresponding textual annotations.Read the textual annotation T associated with V .</p>
<p>5:</p>
<p>Query the DeepSeek R1 [22] API with T using the prompt: "Please identify how many actions are described in the following text, along with the relevant verbs and objects."Extract subject (S), action (A), and object (O) information from the API response.end for 13: end procedure 1) CLIP Text Encoder: The text encoder processes natural language instructions I text and aligns them with visual representations through our multi-branch architecture:
F text = CLIP text (tokenize(I text ))(1)
We decompose the text representation into three semantic components:
F text-subject = MLP text-subject (F text ), F text-action = MLP text-action (F text ),(2)F text-object = MLP text-object (F text )
2) CLIP Visual Encoder: Specifically, each video in our processed dataset consists of a sequence of frames [F rame start , ..., F rame i , ..., F rame end ], where F rame i ∈ R H×W ×4 .We first uniformly sample n frames from each video where n is 16 in our case, then process each frame individually through a frozen CLIP visual encoder to obtain corresponding visual representations:
F i = CLIP VisualEncoder (F rame i )(3)
This results in a sequence of frame features [F 1 , ..., F i , ..., F n ].</p>
<p>3) Temporal Diff-Transformer: After obtaining the framelevel features {F i } n i=1 and F i ∈ R d (d is the embedding dimension), we employ a sophisticated temporal modeling approach to capture the dynamic information in the video sequence.We first compute the consecutive frame differences to capture the temporal dynamics:
∆F i = F i − F i−1 , i ∈ {2, . . . , n}(4)
The differential operation between adjacent frame embeddings functions as an implicit attention mechanism that suppresses static environmental context and extraneous visual elements, thereby accentuating the temporal action dynamics.This feature difference approach effectively isolates the salient motion patterns by computing the gradient of visual representations across the temporal dimension, resulting in a more discriminative signal for action recognition tasks.The sequence of enhanced difference features {∆F i } n i=2 is then processed through a Transformer [23] encoder with relative positional encoding to model the higher-order temporal relationships:
{T em i } n i=2 = Transformer({∆F i } n i=2 )(5)
We extract the last output from the Transformer sequence, which encodes the cumulative temporal information:
T em = ∆T em n(6)
In parallel, we compute the difference between the last and first frames to capture the overall effect of the action in the video:
∆F = F n − F 1(7)
Finally, we concatenate the Transformer output T em and the start-end difference feature ∆F , the start and end frame F 1 , F n , followed by a multi-layer perceptron(MLP) projection to obtain the final visual representation:
F v = MLP(Concat[T emp; ∆F ; F 1 ; T n ])(8)
4) Feature Disentanglement: After obtaining the fused visual representation F v , we employ a novel feature disentanglement module to decompose it into 3 semantically meaningful components corresponding to different aspects of the robot manipulation task.To enhance the feature's contextual awareness, we apply a self-attention mechanism:
attn = MultiHeadAttention(Q = F v , K = F v , V = F v )(9)
Then the attention-enhanced representation is projected into three separate semantic spaces:
F subject = MLP subject (F attn ) F object = MLP object (F attn )(10)F action = MLP action (F attn )
To ensure effective separation of the 3 components, we apply an orthogonality constraint that minimizes the cosine similarity between the branch features:
L sim = − 1 N N i=1 1 3 CosSim(F subject , F action )+ CosSim(F subject , F object )+(11)
CosSim(F action , F object )</p>
<p>where CosSim(a, b) = a•b ||a||•||b|| is the cosine similarity, and N is the batch size.</p>
<p>Additionally, we apply an L2 regularization to prevent feature magnitude explosion:
L L2 = 0.01 • (||F subject || 2 + ||F action || 2 + ||F object || 2 ) (12)
To further enhance the performance of our disentanglement module, we implement a feature bank mechanism that stores representative features for each category of subject, action, and object:
B subject = {F 1 subject , F 2 subject , . . . , F Ks subject }(13)B action = {F 1 action , F 2 action , . . . , F Ka action }(14)B object = {F 1 object , F 2 object , . . . , F Ko object }(15)
where K s , K a , and K o represent the number of unique subjects, actions, and objects in our dataset.The feature banks are updated at fixed step intervals during training.</p>
<p>Every N steps, we compute the average feature for each class from the current batch and replace the corresponding entry in the feature bank:</p>
<p>if step mod Setting Step = 0 :</p>
<p>B subject [c] = F i subject where i has class (16) where S c is the set of samples with subject class c in the current batch.This direct replacement strategy is applied similarly for action and object feature banks.When multiple instances of the same class appear in a batch, we use the last instance's feature for the update.</p>
<p>During the recombination phase, we leverage these stored features to compute an additional CLIP loss between recombined visual features and corresponding text instructions.For instance, the stored features for "robot" (subject), "open" (action), and "drawer" (object) can be recombined to create a synthetic visual representation that is then compared against the text encoding of "Robot opens the drawer, action is open."This approach allows us to evaluate whether the disentangled features can be effectively recombined to match novel combinations of subjects, actions, and objects described in text instructions:
F i,j,k recomb = Combiner(B subject [i], B action [j], B object [k])(17)
T i,j,k = RoboAct-CLIP-TextEncoder(" i j the k, action is j")
L recomb = − 1 N N i=1 log exp(sim(F a recomb , T a )/τ ) b∈M exp(sim(F a recomb , T b )/τ ) ((18))19
where M is a set of valid (subject, action, object) triplets sampled from the feature banks, and for each sample a ∈ M in the batch of size N , we randomly select one triplet from M. This recombination loss encourages the disentangled features to capture the essential characteristics of each semantic component, as they must be effectively recombined to match the corresponding text descriptions.The final disentanglement loss is thus enhanced: (20) where λ ortho and λ recomb are hyperparameters controlling the contribution of the losses.
L disent-enhanced = λ ortho • (L sim + L L2 ) + λ recomb L recomb
To guide the learning process and enhance the model's performance, we incorporate auxiliary classification tasks for each disentangled feature.These auxiliary tasks provide additional supervision signals that help the model learn more discriminative and semantically meaningful representations [24], [25]:
P subject = Softmax(MLP classif y−subject (F attn )) P object = Softmax(MLP classif y−object (F attn )) (21) P action = Softmax(MLP classif y−action (F attn ))
with the combined auxiliary classification loss:
L aux = − 1 N N i=1 α s CE(P subject , y subject ) − 1 N N i=1
α a CE(P action , y action )
− 1 N N i=1 α o CE(P object , y object ) (22)
where CE is the cross-entropy, α r , α a , α o are task-specific weights and y is the label.By incorporating these auxiliary tasks, we encourage each branch to focus on its specific semantic aspect, thereby improving the overall representation quality and downstream task performance.This feature disentanglement approach enables our model to learn specialized representations for different aspects of manipulation videos, facilitating more effective downstream task performance and interpretability.</p>
<p>5) Loss: To ensure cross-modal alignment between visual and textual representations, we compute the final CLIP contrastive loss:
F i video = Concat(F i subject , F i object , F i action ) F i text = Concat(F i text subject , F i text object , F i text action ) L CLIP = − 1 N N i=1 log exp sim(F i video , F i text )/τ N j=1 exp sim(F i video , F j text )/τ(23)
where sim(•, •) is the cosine similarity function, and τ is the temperature parameter.(24) where λ disent and λ aux are hyperparameters controlling the contribution of the losses.
L Total = L CLIP + λ disent * L disent-enhanced + λ aux * L aux</p>
<p>III. EXPERIMENTS</p>
<p>In this section, we conducted robotic tasks in simulated environments to demonstrate the effectiveness of our proposed RoboAct-CLIP.Specifically, we employed the pre-trained RoboAct-CLIP as an information encoder for downstream tasks, freezing the weights of this network component during the training process.Additionally, comprehensive ablation studies were performed to validate the efficacy of our designed Temporal Diff-Transformer and Feature Disentanglement modules.Finally, experiments with physical robotic manipulators in real-world settings further confirmed the effectiveness of our approach.</p>
<p>A. Simulation Experiment</p>
<p>To evaluate our RoboAct-CLIP model, we conducted experiments in the Franka Kitchen [26] simulation environment, a benchmark for robotic manipulation in household settings.This environment features interactive kitchen objects requiring precise manipulation and semantic understanding of instructions.</p>
<p>We employed a robotic arm to perform four representative tasks:</p>
<p>• Task 1: Opening the middle drawer of the cabinet • Task 2: Pushing a plate to the front of the stove • Task 3: Placing cream cheese in a bowl • Task 4: Turning on the stove As for the ecaluation metrics, we reported the success rate, defined as the percentage of episodes where the robot successfully completed the instructed task within a maximum of 200 timesteps.</p>
<p>We compared our RoboAct-CLIP against several state-ofthe-art baselines:</p>
<p>• R3M [27]: A ResNet50-based model pre-trained on visual data to obtain general visual representations for robotic tasks.</p>
<p>• MPI [28]: A model featuring a multimodal transformer encoder and a transformer decoder, designed to predict image-goal interaction states and detect interaction objects.We evaluated both ViT-small and ViT-Base versions.</p>
<p>• CLIP [9]: The original CLIP model without any finetuning, used as a feature encoder.• RoboAct-CLIP (Ours): Our proposed model with temporal modeling and feature disentanglement.All methods were integrated into the same policy learning framework for fair comparison, with their respective encoders frozen during policy training.</p>
<p>Table II presents the performance comparison between our RoboAct-CLIP and the baseline methods across the manipulation tasks.Our RoboAct-CLIP model significantly outperformed all baseline methods across all tasks, achieving an average success rate improvement of 12.0% compared to the bestperforming baseline, MPI (Base)..Further analysis indicates that the superior performance of our model can be attributed to two key factors: (1) The Temporal Diff-Transformer effectively captures the dynamic patterns of manipulation actions, enabling more precise action execution; and (2) The Feature Disentanglement module successfully separates robot embodiment features from action and object features, allowing the policy to focus on task-relevant information while maintaining robustness to environmental variations.</p>
<p>Figure 2 illustrates the execution process of our RoboAct-CLIP model on the four manipulation tasks.These visualizations demonstrate how our model effectively handles different types of interactions, from precise grasping and manipulation (opening drawers, turning knobs) to controlled pushing and placing of objects.These visualizations further substantiate our findings, validating the exceptional performance of the RoboAct-CLIP model as a feature encoder.The qualitative results demonstrate the model's capacity to generate precise and contextually appropriate representations that facilitate successful task completion across diverse manipulation scenarios.2 and 4.This confirms the critical role of temporal modeling in capturing action dynamics.Similarly, the absence of the Feature Disentanglement module (Ablation 2) resulted in a 6.5 percentage point decrease in overall performance, highlighting its effectiveness in separating task-relevant features from embodiment-specific information.These findings validate our architectural design choices and underscore the complementary nature of the proposed components.</p>
<p>B. Ablation Study</p>
<p>C. Robotic Experiment</p>
<p>To further validate the efficacy of RoboAct-CLIP, we conducted experiments in real-world environments.We maintained the same network architecture as in the simulation experiments while training the policy on a dataset collected through teleoperation.The sequential manipulation task consisted of the following steps:</p>
<p>1) Open the middle drawer 2) Pick up the scotch tape 3) Place the scotch tape on the table 4) Close the drawer Figure 3 illustrates the execution sequence of these actions.The robot successfully completed this complex manipulation sequence, demonstrating the transferability of our approach from simulation to real-world scenarios.The temporal modeling capabilities of our Temporal Diff-Transformer proved particularly valuable in handling the variations in lighting, object appearance, and robot dynamics present in real-world settings.</p>
<p>IV. CONCLUSIONS</p>
<p>This paper presented RoboAct-CLIP, a novel approach that enhances Vision-Language Models for atomic action understanding for robotics through temporal-aware feature decoupling.Our Temporal Diff-Transformer effectively captures action dynamics while the Feature Disentanglement module separates subject, action, and object representations, addressing key limitations in existing VLMs for robotic applications.Experimental results demonstrate RoboAct-CLIP's superior performance, achieving a 12% higher success rate than baseline methods across various manipulation tasks in both simulated and real-world environments.Ablation studies confirm the critical contribution of each proposed component.By resolving temporal feature entanglement, our approach advances robotic action understanding and provides a systematic framework for adapting VLMs to manipulation tasks guided by natural language instructions.</p>
<p>Fig. 1 .
1
Fig. 1.Overall framework of RoboAct-CLIP. .</p>
<p>3 :
3
for each video file V in the dataset do 4:</p>
<p>6 :
6
if the response indicates multiple actions then 7:Eliminate V from further consideration.</p>
<p>10 :
10
Generate description as "Robot (or Human)[A] [O], Action is A, Object is O."</p>
<p>Fig. 2 .
2
Fig. 2. Visualization of RoboAct-CLIP performing four manipulation tasks in the Franka Kitchen environment.Each row shows a different task.Our model demonstrates precise control throughout the action sequences, successfully completing diverse manipulation tasks requiring different interaction patterns.</p>
<p>Fig. 3 .
3
Fig. 3. Execution sequence of the real-world manipulation task using RoboAct-CLIP.</p>
<p>Large language models for multi-robot systems: A survey. P Li, Z An, S Abrar, L Zhou, 2025</p>
<p>A survey of robot intelligence with large language models. H Jeong, H Lee, C Kim, S Shin, Applied Sciences. 141988682024</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024arXiv preprint</p>
<p>Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Vision-language foundation models as effective robot imitators. X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, H Liu, H Li, T Kong, arXiv:2311.013782023arXiv preprint</p>
<p>Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Driess, P Florence, D Sadigh, L Guibas, F Xia, 2024</p>
<p>Closed-loop open-vocabulary mobile manipulation with gpt-4v. P Zhi, Z Zhang, M Han, Z Zhang, Z Li, Z Jiao, B Jia, S Huang, 2024</p>
<p>Dart-llm: Dependency-aware multirobot task decomposition and execution using large language models. Y Wang, R Xiao, J Y L Kasahara, R Yajima, K Nagatani, A Yamashita, H Asama, arXiv:2411.090222024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PmLR2021</p>
<p>Openflamingo: An open-source framework for training large autoregressive visionlanguage models. A Awadalla, I Gao, J Gardner, J Hessel, Y Hanafy, W Zhu, K Marathe, Y Bitton, S Gadre, S Sagawa, arXiv:2308.013902023arXiv preprint</p>
<p>Language-driven representation learning for robotics. S Karamcheti, S Nair, A S Chen, T Kollar, C Finn, D Sadigh, P Liang, arXiv:2302.127662023arXiv preprint</p>
<p>Robotic-clip: Fine-tuning clip on action data for robotic applications. N Nguyen, M N Vu, T D Ta, B Huang, T Vo, N Le, A Nguyen, arXiv:2409.177272024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. M Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, arXiv:2406.092462024arXiv preprint</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. M Pan, J Zhang, T Wu, Y Zhao, W Gao, H Dong, 2025</p>
<p>Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, IEEE Robotics and Automation Letters. 732022</p>
<p>A survey on hallucination in large vision-language models. H Liu, W Xue, Y Chen, D Chen, X Zhao, K Wang, L Hou, R Li, W Peng, arXiv:2402.002532024arXiv preprint</p>
<p>Hallucination detection in foundation models for decision-making: A flexible definition and review of the state of the art. N Chakraborty, M Ornik, K Driggs-Campbell, ACM Computing Surveys. 2025</p>
<p>Y Ze, G Zhang, K Zhang, C Hu, M Wang, H Xu, 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. 2024</p>
<p>Robots that adaptively learn when to ask for help: Hallucination reduction in robotic task planning using large language models. A Misic, 2024NTNUMaster's thesis</p>
<p>Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. M Pan, J Zhang, T Wu, Y Zhao, W Gao, H Dong, arXiv:2501.038412025arXiv preprint</p>
<p>Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot. H.-S Fang, H Fang, Z Tang, J Liu, C Wang, J Wang, H Zhu, C Lu, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>User response modeling in reinforcement learning for ads allocation. Z Zhang, Q Zhang, X Wu, X Shi, G Liao, Y Wang, X Wang, D Zhao, Companion Proceedings of the ACM Web Conference 2024, WWW '24. New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Learning list-wise representation in reinforcement learning for ads allocation with multiple auxiliary tasks. Z Wang, G Liao, X Shi, X Wu, C Zhang, Y Wang, X Wang, D Wang, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management, CIKM '22. the 31st ACM International Conference on Information &amp; Knowledge Management, CIKM '22New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, arXiv:1910.119562019arXiv preprint</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>Learning manipulation by predicting interaction. J Zeng, Q Bu, B Wang, W Xia, L Chen, H Dong, H Song, D Wang, D Hu, P Luo, arXiv:2406.004392024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
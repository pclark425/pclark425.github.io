<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1194 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1194</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1194</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-1387794</p>
                <p><strong>Paper Title:</strong> Optimal Learning Paths in Information Networks</p>
                <p><strong>Paper Abstract:</strong> Each sphere of knowledge and information could be depicted as a complex mesh of correlated items. By properly exploiting these connections, innovative and more efficient navigation strategies could be defined, possibly leading to a faster learning process and an enduring retention of information. In this work we investigate how the topological structure embedding the items to be learned can affect the efficiency of the learning dynamics. To this end we introduce a general class of algorithms that simulate the exploration of knowledge/information networks standing on well-established findings on educational scheduling, namely the spacing and lag effects. While constructing their learning schedules, individuals move along connections, periodically revisiting some concepts, and sometimes jumping on very distant ones. In order to investigate the effect of networked information structures on the proposed learning dynamics we focused both on synthetic and real-world graphs such as subsections of Wikipedia and word-association graphs. We highlight the existence of optimal topological structures for the simulated learning dynamics whose efficiency is affected by the balance between hubs and the least connected items. Interestingly, the real-world graphs we considered lead naturally to almost optimal learning performances.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1194.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1194.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Brain Cloud word-association network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A real-world weighted undirected word-association graph derived from free association data (Human Brain Cloud); used here as a proxy of an information/knowledge space whose nodes are words and edges association strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Human Brain Cloud (HBC) word-association network</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Semantic / lexical information network built from free word-association data (words as nodes, association strengths as weighted edges). Domain: semantic memory / language.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Weighted, undirected, empirically small-world and scale-free-like; contains hubs (high-strength nodes) and many low-degree nodes (leaves); connectivity varied by perturbation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random Learning (RL), Preferential Acquisition (PA), Random Surfing (RS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RL: choose new node uniformly among unseen; PA: choose new nodes with probability proportional to degree/strength; RS: with prob p pick an unexplored neighbor of the last-introduced node (prob. proportional to degree/strength), otherwise jump (PageRank-like random surfing). Agents operate the scheduling/navigation policy over the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Coverage time t_N (time to present every node once and empty forgetting queue) and introduction rate n(t) (distinct nodes presented as function of time).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Local-structure exploiting policies (Random Surfing / Preferential Acquisition) outperform Random Learning; exploration that leverages neighborhood relations is optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Performance is non-monotonic with connectivity: minimal coverage time occurs at intermediate connectivity; graph-level balance between hubs and poorly connected nodes is critical—too many hubs or too many low-degree nodes degrade learning efficiency; the original (unperturbed) HBC graph is close to this optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Perturbations that increase or decrease connectivity show that coverage time is minimized for intermediate connectivities; adding edges randomly or between second neighbors shifts efficiency differently, but optimality aligns with the original HBC structure (indicating near-optimal hub/leaf balance).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that exploit local links (RS) gain efficiency by leveraging passive/active reinforcement through neighbors; random exploration (RL) fails to exploit topological information. The structure (presence of hubs and leaves) determines how much a locality-based policy can accelerate introductions (n(t)).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal Learning Paths in Information Networks', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1194.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1194.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WikiPhysics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikipedia Physics subsection subgraph (original, 2-core, 3-core)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subsection of Wikipedia (Physics) extracted as a graph of articles linked together; used as a knowledge-space environment to test learning-schedule navigation across real semantic links and inner cores (2-core, 3-core).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Wikipedia Physics subsection (original; 2-core; 3-core)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Article-link graph from the Physics category of Wikipedia (pages as nodes, links between pages as edges); used to represent a topical knowledge network for navigation/learning scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Directed/undirected representation treated as (filtered) subgraph; connectivity varied by edge-addition/removal perturbations; contains peripheral leaves and inner core structure (2-core, 3-core).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random Learning (RL), Preferential Acquisition (PA), Random Surfing (RS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same three agenda-generation/navigation policies as defined in the study: RL (uniform unseen selection), PA (degree/strength-proportional selection), RS (local neighbor-first with jumps otherwise).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Coverage time t_N and introduction rate n(t).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Local exploration (Random Surfing / preferential strategies) over core subgraphs; exploration that uses topology performs better than random introduction.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Leaves/poorly connected nodes negatively affect learning efficiency; when leaves are removed (considering 2-core and 3-core), the Physics subgraph becomes closer to optimality for intermediate increases in connectivity. Random learning shows monotonic decrease of coverage time with connectivity (inefficient use of topology), while topology-aware policies show non-monotonic minimal coverage at intermediate connectivities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Comparisons between original subgraph and perturbed graphs (edge deletions and two types of edge additions) show that optimal coverage occurs at intermediate average degree; removing leaves (examining inner cores) improves efficiency and shifts the optimal connectivity regime.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Order of exploration matters: topology-aware policies that tend to locally explore (RS) obtain significant gains; policies ignoring links (RL) do not leverage topology. The presence of leaves increases the difficulty of accessing and reinforcing those nodes, requiring exploration strategies that prioritize neighborhood information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal Learning Paths in Information Networks', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1194.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1194.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SyntheticGraphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic network ensembles: Erdős–Rényi (ER), Barabási–Albert (BA), Uncorrelated Configuration Model (UCM, γ=2,3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sets of synthetic graphs used to study how different topologies (random, scale-free with different exponents) affect learning-path navigation and efficiency under the proposed scheduling algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic graph ensembles (ER, BA, UCM γ=2, UCM γ=3)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Abstract graph environments produced by canonical network models: ER (random graph), BA (preferential-attachment scale-free), and UCM (uncorrelated scale-free with tunable exponent); used as controlled knowledge-space topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Connectivity varied systematically via average degree ⟨deg⟩; BA and UCM produce scale-free degree distributions (power-law) while ER is homogeneous; UCM variants differ in fraction of hubs (γ=2 has more hubs than γ=3).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Simulations used N = 10^4 nodes (reported for synthetic experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random Learning (RL), Preferential Acquisition (PA), Random Surfing (RS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RL: uniform unseen selections; PA: selection probability ∝ node degree/strength; RS: with probability p select an unexplored neighbor of last node (local surfing), else perform a jump—page-rank-inspired hybrid local/global policy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Coverage time t_N (time to cover all nodes and empty forgetting queue) and introduction rate n(t) (new distinct nodes vs time); Heaps'-law-like fits (n(t) ∝ t^β) used for baseline/unconnected case.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Baseline uncorrelated items fit: n(t) ∝ t^β with β ≈ 0.85 (sub-linear Heaps' law). Synthetic graphs show a crossover: early sub-linear then a super-linear tail n(t) = c* t^γ with γ > 1 (no numeric γ given).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Topology-aware, local-exploration policies (Random Surfing; PA in some regimes) outperform pure random introduction. RS particularly advantageous in scale-free graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Scale-free, small-world-like topologies with an intermediate average degree and a balanced mix of hubs and low-degree nodes minimize coverage time; ER graphs show monotonically decreasing coverage time with increasing connectivity, while BA/UCM (scale-free) show non-monotonic behavior with a clear optimum at intermediate connectivity; fraction of hubs influences performance (UCM with γ=2 — more hubs — yields stronger improvement than γ=3 for same max degree).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Direct comparison: ER vs BA vs UCM across varying ⟨deg⟩. ER: coverage time decreases monotonically with ⟨deg⟩. BA/UCM: minimal coverage time at intermediate ⟨deg⟩; UCM(γ=2) (more hubs) gives greater improvement than UCM(γ=3) with same max degree. Random learning is ineffective at exploiting topology in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Optimal policies depend on topology: in scale-free networks, local surfing (RS) that exploits neighbor relations is superior; random strategies do not leverage structural advantages. The implementation of active/passive knowledge reinforcement in the agent policy (memory of past visits and neighbor-driven passive reinforcement) is crucial: the policy effectively needs to remember prior visits (knowledge strengths) and exploit neighborhood passives to schedule introductions, i.e., memory-based scheduling outperforms memoryless random introduction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal Learning Paths in Information Networks', 'publication_date_yy_mm': '2015-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Complex structures and semantics in free word association <em>(Rating: 2)</em></li>
                <li>Navigating word association norms to extract semantic information <em>(Rating: 2)</em></li>
                <li>Google and the mind predicting fluency with pagerank <em>(Rating: 2)</em></li>
                <li>Education of a model student <em>(Rating: 1)</em></li>
                <li>Semantic networks: structure and dynamics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1194",
    "paper_id": "paper-1387794",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "HBC",
            "name_full": "Human Brain Cloud word-association network",
            "brief_description": "A real-world weighted undirected word-association graph derived from free association data (Human Brain Cloud); used here as a proxy of an information/knowledge space whose nodes are words and edges association strengths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Human Brain Cloud (HBC) word-association network",
            "environment_description": "Semantic / lexical information network built from free word-association data (words as nodes, association strengths as weighted edges). Domain: semantic memory / language.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Weighted, undirected, empirically small-world and scale-free-like; contains hubs (high-strength nodes) and many low-degree nodes (leaves); connectivity varied by perturbation in experiments.",
            "environment_size": null,
            "agent_name": "Random Learning (RL), Preferential Acquisition (PA), Random Surfing (RS)",
            "agent_description": "RL: choose new node uniformly among unseen; PA: choose new nodes with probability proportional to degree/strength; RS: with prob p pick an unexplored neighbor of the last-introduced node (prob. proportional to degree/strength), otherwise jump (PageRank-like random surfing). Agents operate the scheduling/navigation policy over the graph.",
            "exploration_efficiency_metric": "Coverage time t_N (time to present every node once and empty forgetting queue) and introduction rate n(t) (distinct nodes presented as function of time).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Local-structure exploiting policies (Random Surfing / Preferential Acquisition) outperform Random Learning; exploration that leverages neighborhood relations is optimal.",
            "topology_performance_relationship": "Performance is non-monotonic with connectivity: minimal coverage time occurs at intermediate connectivity; graph-level balance between hubs and poorly connected nodes is critical—too many hubs or too many low-degree nodes degrade learning efficiency; the original (unperturbed) HBC graph is close to this optimum.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Perturbations that increase or decrease connectivity show that coverage time is minimized for intermediate connectivities; adding edges randomly or between second neighbors shifts efficiency differently, but optimality aligns with the original HBC structure (indicating near-optimal hub/leaf balance).",
            "policy_structure_findings": "Policies that exploit local links (RS) gain efficiency by leveraging passive/active reinforcement through neighbors; random exploration (RL) fails to exploit topological information. The structure (presence of hubs and leaves) determines how much a locality-based policy can accelerate introductions (n(t)).",
            "uuid": "e1194.0",
            "source_info": {
                "paper_title": "Optimal Learning Paths in Information Networks",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "WikiPhysics",
            "name_full": "Wikipedia Physics subsection subgraph (original, 2-core, 3-core)",
            "brief_description": "A subsection of Wikipedia (Physics) extracted as a graph of articles linked together; used as a knowledge-space environment to test learning-schedule navigation across real semantic links and inner cores (2-core, 3-core).",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Wikipedia Physics subsection (original; 2-core; 3-core)",
            "environment_description": "Article-link graph from the Physics category of Wikipedia (pages as nodes, links between pages as edges); used to represent a topical knowledge network for navigation/learning scheduling.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Directed/undirected representation treated as (filtered) subgraph; connectivity varied by edge-addition/removal perturbations; contains peripheral leaves and inner core structure (2-core, 3-core).",
            "environment_size": null,
            "agent_name": "Random Learning (RL), Preferential Acquisition (PA), Random Surfing (RS)",
            "agent_description": "Same three agenda-generation/navigation policies as defined in the study: RL (uniform unseen selection), PA (degree/strength-proportional selection), RS (local neighbor-first with jumps otherwise).",
            "exploration_efficiency_metric": "Coverage time t_N and introduction rate n(t).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Local exploration (Random Surfing / preferential strategies) over core subgraphs; exploration that uses topology performs better than random introduction.",
            "topology_performance_relationship": "Leaves/poorly connected nodes negatively affect learning efficiency; when leaves are removed (considering 2-core and 3-core), the Physics subgraph becomes closer to optimality for intermediate increases in connectivity. Random learning shows monotonic decrease of coverage time with connectivity (inefficient use of topology), while topology-aware policies show non-monotonic minimal coverage at intermediate connectivities.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Comparisons between original subgraph and perturbed graphs (edge deletions and two types of edge additions) show that optimal coverage occurs at intermediate average degree; removing leaves (examining inner cores) improves efficiency and shifts the optimal connectivity regime.",
            "policy_structure_findings": "Order of exploration matters: topology-aware policies that tend to locally explore (RS) obtain significant gains; policies ignoring links (RL) do not leverage topology. The presence of leaves increases the difficulty of accessing and reinforcing those nodes, requiring exploration strategies that prioritize neighborhood information.",
            "uuid": "e1194.1",
            "source_info": {
                "paper_title": "Optimal Learning Paths in Information Networks",
                "publication_date_yy_mm": "2015-06"
            }
        },
        {
            "name_short": "SyntheticGraphs",
            "name_full": "Synthetic network ensembles: Erdős–Rényi (ER), Barabási–Albert (BA), Uncorrelated Configuration Model (UCM, γ=2,3)",
            "brief_description": "Sets of synthetic graphs used to study how different topologies (random, scale-free with different exponents) affect learning-path navigation and efficiency under the proposed scheduling algorithms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Synthetic graph ensembles (ER, BA, UCM γ=2, UCM γ=3)",
            "environment_description": "Abstract graph environments produced by canonical network models: ER (random graph), BA (preferential-attachment scale-free), and UCM (uncorrelated scale-free with tunable exponent); used as controlled knowledge-space topologies.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Connectivity varied systematically via average degree ⟨deg⟩; BA and UCM produce scale-free degree distributions (power-law) while ER is homogeneous; UCM variants differ in fraction of hubs (γ=2 has more hubs than γ=3).",
            "environment_size": "Simulations used N = 10^4 nodes (reported for synthetic experiments).",
            "agent_name": "Random Learning (RL), Preferential Acquisition (PA), Random Surfing (RS)",
            "agent_description": "RL: uniform unseen selections; PA: selection probability ∝ node degree/strength; RS: with probability p select an unexplored neighbor of last node (local surfing), else perform a jump—page-rank-inspired hybrid local/global policy.",
            "exploration_efficiency_metric": "Coverage time t_N (time to cover all nodes and empty forgetting queue) and introduction rate n(t) (new distinct nodes vs time); Heaps'-law-like fits (n(t) ∝ t^β) used for baseline/unconnected case.",
            "exploration_efficiency_value": "Baseline uncorrelated items fit: n(t) ∝ t^β with β ≈ 0.85 (sub-linear Heaps' law). Synthetic graphs show a crossover: early sub-linear then a super-linear tail n(t) = c* t^γ with γ &gt; 1 (no numeric γ given).",
            "success_rate": null,
            "optimal_policy_type": "Topology-aware, local-exploration policies (Random Surfing; PA in some regimes) outperform pure random introduction. RS particularly advantageous in scale-free graphs.",
            "topology_performance_relationship": "Scale-free, small-world-like topologies with an intermediate average degree and a balanced mix of hubs and low-degree nodes minimize coverage time; ER graphs show monotonically decreasing coverage time with increasing connectivity, while BA/UCM (scale-free) show non-monotonic behavior with a clear optimum at intermediate connectivity; fraction of hubs influences performance (UCM with γ=2 — more hubs — yields stronger improvement than γ=3 for same max degree).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Direct comparison: ER vs BA vs UCM across varying ⟨deg⟩. ER: coverage time decreases monotonically with ⟨deg⟩. BA/UCM: minimal coverage time at intermediate ⟨deg⟩; UCM(γ=2) (more hubs) gives greater improvement than UCM(γ=3) with same max degree. Random learning is ineffective at exploiting topology in all cases.",
            "policy_structure_findings": "Optimal policies depend on topology: in scale-free networks, local surfing (RS) that exploits neighbor relations is superior; random strategies do not leverage structural advantages. The implementation of active/passive knowledge reinforcement in the agent policy (memory of past visits and neighbor-driven passive reinforcement) is crucial: the policy effectively needs to remember prior visits (knowledge strengths) and exploit neighborhood passives to schedule introductions, i.e., memory-based scheduling outperforms memoryless random introduction.",
            "uuid": "e1194.2",
            "source_info": {
                "paper_title": "Optimal Learning Paths in Information Networks",
                "publication_date_yy_mm": "2015-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Complex structures and semantics in free word association",
            "rating": 2,
            "sanitized_title": "complex_structures_and_semantics_in_free_word_association"
        },
        {
            "paper_title": "Navigating word association norms to extract semantic information",
            "rating": 2,
            "sanitized_title": "navigating_word_association_norms_to_extract_semantic_information"
        },
        {
            "paper_title": "Google and the mind predicting fluency with pagerank",
            "rating": 2,
            "sanitized_title": "google_and_the_mind_predicting_fluency_with_pagerank"
        },
        {
            "paper_title": "Education of a model student",
            "rating": 1,
            "sanitized_title": "education_of_a_model_student"
        },
        {
            "paper_title": "Semantic networks: structure and dynamics",
            "rating": 1,
            "sanitized_title": "semantic_networks_structure_and_dynamics"
        }
    ],
    "cost": 0.0129545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Optimal Learning Paths in Information Networks OPEN
Published: 01 June 2015</p>
<p>G C Rodi 
Dept. of Mathematical Sciences
Polytechnic University of Turin
Corso Duca degli Abruzzi
2410129TurinItaly</p>
<p>Institute for Scientific Interchange (ISI)
Via Alassio 11C10126TurinItaly</p>
<p>V Loreto 
Institute for Scientific Interchange (ISI)
Via Alassio 11C10126TurinItaly</p>
<p>Physics Dept
Sapienza University of Rome
Piazzale Aldo Moro 200185RomeItaly</p>
<p>V D P Servedio 
Physics Dept
Sapienza University of Rome
Piazzale Aldo Moro 200185RomeItaly</p>
<p>&amp; F Tria 
Institute for Scientific Interchange (ISI)
Via Alassio 11C10126TurinItaly</p>
<p>Optimal Learning Paths in Information Networks OPEN</p>
<p>4 SONY-Computer Science Lab (CSL)
Paris575005Published: 01 June 201510.1038/srep10286Received: 11 November 2014 Accepted: 08 April 20151 France. 5 Institute for Complex Systems (ISC-CNR), Via dei Taurini 19, 00185 Rome, Italy. Correspondence and requests for materials should be addressed to G.C.R. (email: giovannachiara.rodi@polito.it)
Each sphere of knowledge and information could be depicted as a complex mesh of correlated items. By properly exploiting these connections, innovative and more efficient navigation strategies could be defined, possibly leading to a faster learning process and an enduring retention of information. In this work we investigate how the topological structure embedding the items to be learned can affect the efficiency of the learning dynamics. To this end we introduce a general class of algorithms that simulate the exploration of knowledge/information networks standing on well-established findings on educational scheduling, namely the spacing and lag effects. While constructing their learning schedules, individuals move along connections, periodically revisiting some concepts, and sometimes jumping on very distant ones. In order to investigate the effect of networked information structures on the proposed learning dynamics we focused both on synthetic and real-world graphs such as subsections of Wikipedia and word-association graphs. We highlight the existence of optimal topological structures for the simulated learning dynamics whose efficiency is affected by the balance between hubs and the least connected items. Interestingly, the real-world graphs we considered lead naturally to almost optimal learning performances.Modern global positioning systems allow human beings to locate themselves and find their way in the physical space with an unprecedented accuracy. GPS technologies beautifully complemented space perception humans naturally possess. The idea of moving on a space is actually far more general and we, as humans, constantly wander in what could be defined an information or a knowledge space, i.e., a complex structure linking, through semantic and logic relations, pieces of our knowledge and culture. Nowadays, the notion of knowledge or information space is not only an abstraction and information networks are widespread, from the whole World Wide Web to the paramount example of Wikipedia 1-3 , from word-association graphs 4,5 to ontologies and taxonomies. Whenever we work, study, play, we naturally and constantly navigate information networks and our activities could be intuitively thought as a path on a network of points encoding information and knowledge. But how we stand and how we shape our way in this space as well as the structure of this space itself are often largely unknown. In this sense, nowadays, the Socratic "know thyself " is far from being a concrete reality. Still a better knowledge of our trajectories in knowledge spaces would be key to better design learning, professional or leisure activities.The explosion since 2012 of Massive Open Online Courses (MOOCs) witnesses the exponential growth in the demand for access to education. The recent success of web platforms and applications designed for learning, e.g., Anki 6 or Duolingo 7 , reveals an increasing interest in educational software, which could provide self-learners with tailored, efficient and innovative tools for learning. Within this framework, the research of optimal educational algorithms has to deal with the problem of finding the best scheduling of the study practices, i.e., the best timing for introducing new material and reviewing the older, in order to make the retention enduring and to minimize the forgetting. In a recent pioneering work 8 , Novikoff et al. beautifully formalized mathematically this problem and developed some models</p>
<p>for the generation of learning schedules that would yield to lifelong learning or cramming, without any forgetting during the time considered.</p>
<p>In the scheme proposed in 8 , the knowledge to be acquired is pictured as a set of independent units. To make a step towards a more realistic scenario, correlations among bits of information must be also considered. Our work moves along this direction, by using a complex network representation of the units to be acquired and their interconnections.</p>
<p>The complex systems perspective in dealing with cognitive and linguistic systems is not novel in the literature. Indeed, the first efforts to depict the semantic memory through graphs of concepts go back to the sixties, with the Quillian's model of the semantic memory and its successive generalizations 9,10 . More recently, the complex network approach has become widely used, for instance to gain a deeper understanding and characterization of the properties of semantic networks 5,11 and possibly even to model the mechanisms underlying their growth 12 . Furthermore, also the dynamics of cognitive processes have been addressed and investigated within the framework of the network theory as diffusion processes on the networks. Successful examples of such approach regard the extraction of semantic similarities relations on graphs of free-associated words 13 or the predictive analysis of fluency tasks by means of algorithms like the PageRank 14 . The reviews by Borge-Holthoefer and Arenas 15 and by Baronchelli et al. 16 provide a comprehensive overview of the complex network contributions to the investigation of language and cognitive science.</p>
<p>Here the cognitive task we model is a learning process of a-priori defined collection of items embedded in a complex network. In particular, we focus on the role that the structure of this network, i.e., the set of logic or semantic links among the different bits of knowledge, can have in enhancing or hindering the acquisition and retention of information, thus determining the learning efficiency. In this work we consider both synthetic graph structures and real-world ones, namely some subsections of the Wikipedia graph and the Human Brain Cloud network of free word associations 4 , both of them taken as proxies of information and knowledge spaces.</p>
<p>In order to investigate how the topology and the statistical properties of the posited complex network structures can affect the efficiency of the learning process, we introduce a general class of algorithms for the generation of a learning schedule, i.e., an ordered sequence of item presentations. In other words our algorithms generate paths in the network structure, where a path is defined as an ordered set of visits to the nodes of the network. The whole process implies a subtle balance between the introduction of new units and the repetition of old ones, also taking into account the possibility of failures of the learning procedure, i.e., forgetting episodes, and the corresponding retrieval processes.</p>
<p>As in the work cited 8 , our starting point are some results of the century-long cognitive science research on cognition and memory. In particular, we focus on how the allocation over time of the study practices for each item can affect the learning performance. In his 1885 milestone work 17 , Ebbinghaus introduced the spacing effect. This finding refers to the notion that spreading the study sessions of any item over time makes its retention more durable than massing them in a short period, where the inter-study session intervals can be empty or filled with practices of other items. Many references can be found in literature on both the theoretical discussion of the psychological mechanisms involved 18,19 and on some experimental evidences of the validity of this effect 20 . Furthermore, among all the possible inter-study intervals, it has been reported 21-23 that the benefits gained by spacing are enhanced if, for each item, the intervals between its study practices expand with the reviews rather than remaining fixed. This phenomenon is usually referred to with lag effect or expanded retrieval.</p>
<p>Standing on the shoulders of this copious literature, our class of learning algorithms incorporate the above mentioned effects, namely the spacing and lag effects, in the generation of learning paths. We also make assumptions on the role that connections between items may play in the schedule planning. We suppose that while learning, semantically related concepts could be primed or reinforced in memory, thus adapting to a learning process some of the suggestions of the seminal spreading-activation theory for information retrieval 9,10 . Further fundamental references are some results of previous research on the early words learning in toddlers 24,25 or in second language learners, for which cognitive rather than linguistic associations seem to enhance the acquisition process 26 .</p>
<p>Our main result is that the acquisition process is strongly affected by the topology of the underlying knowledge network. We observe that a notion of optimality in the learning process can be introduced and that optimal performances can be obtained if the underlying graph features small-world and scale-free properties with a balance between the number of hubs and of the least connected items. Surprisingly the real-world networks we analyzed here turn out to be close to optimality. That is the case of the networks based on collaborative tasks or spontaneous activity of users, like some subsections of Wikipedia and the Human Brain Cloud dataset of free-associated words, both considered in our work. This finding represents a very interesting hint towards a subtle link between the way in which humans construct knowledge spaces and the way in which they possibly explore them, retrieve the information and learn.</p>
<p>Results</p>
<p>The model. We represent the set of items to be learned as nodes in a graph and we model learning as a dynamical process through which we construct a learning schedule, defined as a sequence of successive visits an hypothetical student would make to the nodes of the graph. At each extraction either a new node (never visited before) can enter in the sequence, or an already considered one can be repeated (subfigure (A) of Fig. 1). In particular, at each time step, the item i to be presented to the student, i.e., appended to the learning sequence, is stochastically chosen according to three factors: (i) the time, t i , elapsed for each item i, since its last presentation; (ii) the time, t new , elapsed since the last introduction of a brand new item; (iii) the knowledge strength S i (t) of item i at time t. The algorithm takes into account both the number of times the item i has already been repeated, and the repetitions of items connected to it in the graph, that is the knowledge of the context of i. In particular, the knowledge strength is the sum of three distinct contributions (S t k t k t k i i i pass i 0 ( ) = ( ) + ( ) + ), corresponding to three different mechanisms that are supposed to lead to the acquisition and reinforcement of any item knowledge: (a) k i (t) is the number of time the item i is repeated since its first introduction or since its reintroduction from the forgetting queue (see next paragraph for its definition); (b) k t i pass ( ): every time an item is repeated, one among its neighbors already introduced (and not forgotten), say i, is randomly selected (uniformly or with probability proportional to the weight of the connecting link, respectively in In a learning schedule the interval required between any two successive presentations of the same item i expands with the number of reviews. To this end the probability of a repetition is computed for every node already introduced as illustrated. If the k-th presentation of a node i with knowledge strength S i (see the main text) occurred at time t k , the (k + 1)-th happens at time t with probability proportional to F t t
S t k i ( − ) ( ) . This function is non null in the temporal interval a b S t S t i i    ,    ( ) ( )
, whose bounds are increasing functions of the total knowledge strength of item i. After b S t i ( ) steps without being repeated, the item i is forgotten and has to be reintroduced. In (B) the supposed mechanisms of knowledge reinforcement are illustrated. When item i is introduced, it gains a starting knowledge value k i 0 depending on how much its neighborhood is known. This mechanism is referred to as active effect. Afterwards, at every successive repetition, its knowledge is reinforced by 1 and one among its introduced neighbors, say j, is randomly selected to receive a passive reinforcement, i.e., k j pass is incremented by a quantity α. In all our simulations α = 0.1.</p>
<p>Scientific RepoRts | 5:10286 | DOi: 10.1038/srep10286 unweighted or weighted graph) and k i pass increases by a value α &lt; 1. We name this process passive effect; (c) when an item i enters in the sequence for the first time or from the forgetting queue, its starting knowledge k i 0 is a weighted average of the knowledge acquired so far on its neighbors. We name this the active effect and we refer the reader to the methods section for its complete definition.</p>
<p>Constraints on the time window useful for reviewing an item are provided, implementing the spacing and lag effects. As in a previous work 8 , two successive occurrences of the same item i should occur inside a given interval a b
S t S t i i    ,    ( ) ( )
, whose bounds are monotonic non-decreasing function of the knowledge strength S i , in order to prevent the forgetting of the item. Our agenda generation rule is thus the following. At each discrete time t, for each item i among the n(t) already introduced in the schedule, the temporal distance since its last occurrence is evaluated:
Δ i t = (t − t i ), where t i is the last time at which the item i entered in the sequence. If t b i S i ∆ &gt; , the item is forgotten, put into a forgetting queue and its knowledge strength S i is reset to zero. If t b i S i ∆ ≤ , a monotonic non-decreasing function of Δ i t, F t S i i (∆ )
, determines the probability for node i to be repeated at time t (refer to the methods section for its definition). The probability of introducing in the sequence a new item instead of repeating an already introduced one depends linearly on the time elapsed since the last introduction of a novel item (we refer again the reader to the method section for a complete definition of the probabilities). In the case of a new introduction event, the oldest item stored in the forgetting queue is reintroduced, without updating t new . If the forgetting queue is empty, a brand new node is introduced to the learning schedule and t new is updated.</p>
<p>We investigate different criteria determining the particular brand new node to be introduced in the learning schedule in order to investigate the effect of the semantic structure underlying the items to be learned (other possible criteria are presented in the Supplementary Information): (i) random learning (RL): each new entry is randomly and uniformly selected among the ones not already presented; (ii) preferential acquisition (PA): the new entries are chosen with probability proportional to their degrees (or strength, in case of weighted graph). In doing so, we reproduce the preferential acquisition model for the early words learning in toddlers discussed by Hills et al. 25 ; (iii) random surfing (RS): every time a new item has to be chosen, with probability p a nearest neighbor not already introduced of the last item introduced in the sequence is selected, if any, with probability proportional to its degree. Otherwise, with probability (1 − p) or in case all neighbors were already introduced, a jump is made in the network and a random node is selected with a PA step. In case of weighted graph, strengths are considered instead of degrees. This criterion is reminiscent of the PageRank algorithm 27 . Outcomes. On the generated sequences, two main quantities are studied to evaluate the efficiency of the corresponding learning processes. The first one is the introduction rate n(t), namely the number of distinct nodes presented throughout the sequence as a function of time and not forgotten. The second variable is the graph coverage time, that is t N such that n(t N ) = N, i.e., the time needed to present every node at least once and to empty the forgetting queue. For these quantities two different behaviors can be expected in the limit cases of totally disconnected and connected graphs. Because of the generation rule previously explained, and in particular the active knowledge reinforcement term, interconnections between nodes lead to a faster rate of introductions and therefore to a shorter coverage time. However, for intermediate connectivity values, the learning efficiency does depend on both the topology of the graph explored and, for a given topology, on the criterion according to which novel nodes are to be introduced. For this, we carried out simulations on different types of synthetic graphs and on networks generated from real data. In the first case, for each graph type, we have compared sequences obtained from graphs with increasing average degree. For the real networks, methods of perturbation have been developed to increase and decrease the connectivity while only slightly modifying the other statistical properties, such as the degree or strength distributions. They are described in the Methods section.</p>
<p>A global insight into the role of the graph topology and its connectivity properties can be gained by comparing the coverage times. In Fig. 2 we report the main coverage time obtained for four different synthetic network types, both random graphs 28 (subfigure (A)), and different scale-free graphs: BA 29 (subfigures (B)) and graphs generated with the Uncorrelated Configuration Model (UCM) 30 (subfigures (C)-(D)), and for the three entry selection criteria earlier defined. Scale-free graphs together with no random criteria of exploration lead to optimal learning performances for intermediate average connectivities. The improvement in the coverage time is even more meaningful in graphs with the same maximum degree but a larger fraction of hubs, as it emerges by comparing the UCM networks with two different exponents of the degree probability distribution, reported in subfigures (C) and (D). With regard to the selection criteria, an efficiency gain is achieved in the scale-free graphs when they are locally explored, namely when the random surfing criterion is used. A greater insight into the dynamics of the learning schedule construction process is given by looking at the introduction rate n(t). In Fig. 3 (subfigure (A)) results on random 28 and BA 29 graphs with similar average degree are compared. In subfigure (B) the data refer to graphs generated with the UCM model with low, intermediate and high values of average connectivity, and exponent γ = 2 in the power law degree distribution P(deg) ∝ deg −γ . In both figures, we contrast the data with the results obtained on an equivalent set of completely disconnected nodes (and a linear trend is also reported for comparison). For uncorrelated items, the introduction rate turns out to be a sub-linear function of time (n(t) ≃ t β , with β &lt; 1), in accordance with Heaps' law 31  embedded in a graph, two different behaviors can be identified. As long as the graph is largely unexplored, the introduction rate has the same trend as in the case of disconnected items, namely sub-linear. Later on along the learning dynamics, new items are introduced with higher frequency, featuring a super-linear tail for the introduction rate, i.e., n(t) = c * t γ , with γ &gt; 1 and c * &lt; &lt; 1. Note that, for a short time interval, such a super-linear rate is still compatible with the schedule constraint that at most one brand new unit can be introduced at each discrete time. The origin of this super-linear behaviour is related to the active effect contributing to the knowledge strength of each item (see the model subsection and the methods section). Indeed, when a significant fraction of items have already been introduced, new items typically enter the schedule with higher and higher knowledge strengths, thus requesting longer intervals before they need to be reviewed, allowing in this way the introduction of further new items.</p>
<p>The coverage times resulting from simulations on real-world graphs and their perturbed versions are shown in Fig. 4. Data in subfigure (A) refer to weighted, undirected graph generated from the Human Brain Cloud 4 word association dataset. In particular, a filtered version of the data was provided by Gravino et al. 5 . For the data reported in the other subfigures, we considered the subgraph in Wikipedia 32 corresponding to the Physics subsection. The procedure implemented to extract it is reported in the Methods section, while some statistical properties of the graphs considered are analyzed in the SI.</p>
<p>As for the synthetic graphs, the random learning algorithm for choosing the new entries does not lead to meaningful performances, the coverage time monotonically decreasing as the connectivity enlarges. On the contrary, when the information stored in the topology is used to more shrewdly select the novel nodes, the minimal coverage time is achieved for intermediate connectivities. Moreover, the structures leading to the optimal performance coincide with the original HBC graph (subfigure (A)) and with the original Physics graph, when the least connected nodes are removed, i.e., when the inner cores are considered and treated as unperturbed new graphs. In particular, the results obtained on the Physics original graphs (subfigure (B)) and its more external inner cores (the 2-core in subfigure (C) and the 3-core in (D)), suggest a meaningful role of the poorest connected nodes in affecting the learning efficiency. Indeed, as soon as the leaves are removed, the topology of the Wiki subgraph becomes closer to the optimal one, with respect to a further increase of the number of connections. This finding can be used in future to suggest a topological reorganization of Wikipedia subgraphs resulting in an optimization of thematic learning paths. By looking at the data acquired when the two positive perturbation procedures are implemented, it can be concluded that it is not the average connectivity that triggers the most efficient learning performance, rather the relative presence of poorly connected nodes with respect to the hubs.</p>
<p>Discussion and Conclusions</p>
<p>In this paper we investigated the role of the topology of complex information and knowledge networks when generating efficient learning schedules for the items they embed. We proposed a general class of stochastic algorithms to sequence the introductions of the different items and their reviews over time, while satisfying some constraints on the best timing, as they can be derived from previous results of cognitive science research. Furthermore, we studied how the topological structure representing the complex semantic and logic relationships among the items to be learned can affect the learning procedure. We investigated, in particular, how different statistical properties and topologies of the graphs in which the items are embedded affect the process, as well as the ways such graphs should be explored while introducing new material in order to achieve efficient learning paths.</p>
<p>Our results show that some topologies lead to optimal learning schedules, i.e., schedules that minimize the learning time while preventing forgetting episodes. They are small-world, scale-free structures, in which the relative number of hubs and low-connected nodes are balanced. In fact, structures with either too many hubs or poorly connected nodes hinder the learning process. In the first case, the context for items is indeed too large to take advantage of it. In the latter case, the more specific and low connected the nodes, the more difficult it is to access them or to achieve a gain in the knowledge reinforcement throughout the learning process. Furthermore, we find that the order through which the networks are explored as new items are introduced in the agenda is essential for taking full advantage of the topology features, a random exploration turning out to be ineffective in eliciting the information stored in the graph. Finally, a very interesting outcome of our study is that the real-world graphs we considered here, the Human Brain Cloud word-association network and the Wikipedia graph, turned out to be almost optimal with respect to the criterion described above. This points to a subtle link between the way in which humans organise their knowledge, i.e., the structure of the knowledge space, and the way in which the information could be retrieved, for instance through a learning path. From a technological perspective this is very interesting since it suggests the existence of a feedback loop between the dynamical evolution of information networks, i.e., the way in which users shape them by contributing content and semantics, and the way in which users navigate the sea of information and knowledge. This can lead to an improvement of both editing and navigation strategies and suggests: (i) both a brand new role for users and editors in information networks, e.g., not only content provider but more and more crucially path designers, and (ii) a leading direction towards search engines for learning paths.  In summary, the outcomes presented here suggest a key role of the conceptual structures embedding the items to be learned in making learning processes faster and the retention longer. From this perspective, empirical research on how different patterns of associations could drive the acquisition of new concepts would be key to progress and proceed towards more informed algorithms to generate learning paths. This understanding can help in designing novel educational software and more in general to improve both the teaching ability of mentors and the learning experiences of both students and individual self-learners. We believe this approach can have a far-reaching impact since a better understanding of the complexity of our knowledge spaces, as well as the way in which we navigate them, may have the potential to trigger the development of new tools to orient human beings in complex information networks to better shape education, professional growth and leisure activities.</p>
<p>Methods</p>
<p>Definition of the active effect k i 0 . The starting knowledge k i 0 of an item reflects the knowledge of its context, i.e., of its neighbors. It is defined by:
k k nn max 1 int 1 1 1 i nn intro i 0 i =       ,      ⋅       −                   ( )
where nn intro i is the number of neighbors of i already introduced and not forgotten and k nn i is the average no-passive knowledge strength over the set of neighbors  i of item i. It is defined as:
 k s k k w 1 2 nn i j j j ij 0 i i ( ) ∑ = + ( ) ∈
where w ij is the weight of the link connecting node i to node j (w ij = 1 in an unweighted graph) and s i is the strength of node i: s w i j ij = ∑ . Probabilities of selecting a node i. An item i is chosen to be repeated or a new one is introduced according respectively to the following normalized probabilities:  Fig. 1, subfig. (A). In so doing, we suppose that the temporal window useful for a review to occur expands exponentially with the number of reviews 33 . Moreover, in order to take full advantage of the lag effect, we choose the repetition probability function so that a review is more likely to happen the closer the time is to the upper bound b S t i ( ) . For each item i, we define F t
P t F t F t F t P t F tS i i (∆ ) as F t F t F F b F 0 0 4 S i S i S S S S i i i i i i ( ) (∆ ) = (∆ ) − ( ) − ( ) , ( ) ⁎ ⁎ ⁎ ⁎ where F t LR b t b 1 2 tanh 2 1 5 S i S i S i i i (∆ ) = ⋅                   ∆ −            +        . ( ) ⁎
In this definition, we introduce LR as the only free parameter, which stands for learning rigidity and fixes the function slope. In the following, we consider LR = 2 3 , while tests are reported in the Supplementary Information on how its value affects the learning efficiency.</p>
<p>Perturbation of real-world graphs. Starting from a real-data based graph, a predefined percentage of links were created or deleted according to the following criteria. When it was required to remove some connections, they were randomly selected and deleted, regardless of their weights or of the degrees of the connected nodes. As a main consequence, some disconnected components might emerge. In adding links, two different strategies were implemented. In a first case, two reciprocally disconnected nodes were randomly selected and a connection was created between them, no matter their distance on the graph.</p>
<p>Figure 1 .
1Model illustration. (A)</p>
<p>Figure 2 .
2Coverage times on synthetic graphs. In the figures, the mean coverage times scaled to the network order N = 10 4 as a function of the network average degrees are shown for different synthetic graphs, generated according to the (A) Erdös-Rényi model 28 , (B) Barabási-Albert model 29 , (C) uncorrelated configuration model30 where P(deg) ∝ deg −γ and γ = 2, (D) same as in (C) with γ = 3. The data are averaged over 10 different graph realizations and 5 learning agendas for each of them. Standard errors are also reported but they are covered by symbols. Different colors refer to the three criteria used to select the entries: random learning (RL, magenta), preferential acquisition (PA, blue) and random surfing (RS, green). Note the logarithmic scale of the horizontal axis for (A) and (B).</p>
<p>Figure 3 .
3Introduction rate on synthetic graphs. Fraction of distinct nodes introduced as a function of the (rescaled) average time needed to cover them. In figure (A) we report data obtained on Erdös-Rényi model28 and Barabási-Albert model 29 graphs with average connectivity 〈 deg〉 ∼ 100. In (B) the graphs considered are generated according to the uncorrelated configuration model30 with P(deg) ∝ deg −γ and γ = 2 and different average connectivities. In all the cases, the graph size is N = 10 4 and the algorithm used to select the entries is the random surfing RS. In both (A) and (B), with black dots we report the introduction rate for an equivalent set of uncorrelated items. It is fitted with a sub-linear curve y ∝ x β , with β = 0.85. The fitting curve is shown with red line in the main graph, while in the insets we report an eye-guide power-law with same exponent. An eye-guide linear (β = 1, grey line) curve is also reported in the main figures. The insets axes are in log-log scale. The data are averaged over 50 agenda simulations. Standard errors are reported, though not visible at the plot scale.Scientific RepoRts | 5:10286 | DOi: 10.1038/srep10286</p>
<p>Figure 4 .
4Coverage times on real-world graphs. It is reported the coverage times obtained by simulating the learning agendas on a real-world graph (red circled points) and on three perturbed versions of it. The real-world graphs are: (A) the weighted network generated from the Human Brain Cloud 4 dataset, (B) the Physics section of Wikipedia and its (C) 2-core and (D) 3-core subgraphs. In each figure, the squares refer to data resulting on graphs with reduced connectivity, obtained by randomly selecting and deleting different amounts of links in the original graphs. With circles and triangles data are reported when two different procedures for increasing the connectivity are considered. In the first case (circles, solid line), links are created by randomly selecting pairs of unconnected nodes. In the latter (triangles, dashed line), new links are added only between second-neighbor nodes. In all the cases, the fraction of links deleted/created are equal to 0.01, 0.05, 0.1 and 0.5. Different colors refer to the three criteria used to select the entries: random learning (RL, magenta), preferential acquisition (PA, blue) and random surfing (RS, green). The data referring to the unperturbed graphs are averaged over 50 agendas. In all the other cases, for each type of perturbation procedure and percentage of edges added or removed, 10 different perturbed versions of the graphs are generated and 5 agendas are simulated on each of them. Then, the averages are done over the 50 aggregated agendas realizations. Standard errors are reported, though not visible at the plot scale.Scientific RepoRts | 5:10286 | DOi: 10.1038/srep10286</p>
<p>F new (t) = 1/2 (t − t new ), being t new the time of the last introduction of a brand new item in the learning sequence.</p>
<p>Scientific RepoRts | 5:10286 | DOi: 10.1038/srep10286</p>
<p>. Instead, for items Scientific RepoRts | 5:10286 | DOi: 10.1038/srep10286
AcknowledgmentsWe are grateful to Steven H. Strogatz for inspiring conversations on the subject and to Pietro Gravino for providing us with the filtered version of the Human Brain Cloud dataset. SONY-CSL provided support for author VL, but did not have any additional role in the study design, data collection and analysis,According to a second procedure, a node was randomly selected and a new connection was created with one among its second-neighbors. In both cases, the new link weight was possibly assigned by sampling the original weight distribution. In particular, an edge in the original network was randomly selected, and its same weight was assigned to the new link. How the perturbation procedures affect the graphs strength or degree distribution is reported in the Supplementary Information. HBC data filtering procedure. The data set here considered is a modified, undirected filtered version of the original one, provided by Gravino et al.5. Details on the filtering procedure are reported in the Appendix A of their work.Wikipedia subsection extraction. The MediaWiki API were used34. First, the list of thematic Wikipedia article titles was fetched by enquiring the API for the corresponding scientific area, i.e., by restricting to the corresponding category, e.g., Category: Physics articles by importance. Then, each page referring to the titles collected was scanned for the included links to other pages. Pages containing talks, templates and categories were not taken into account as well as connections toward pages not belonging to the subsection.Author ContributionsG.C.R., V.L., V.D.P.S and F.T. designed the experiments, analyzed the data and wrote the paper.Additional Information
Preferential attachment in the growth of social networks: The internet encyclopedia Wikipedia. A Capocci, Phys. Rev. E. 7436116Capocci, A. et al. Preferential attachment in the growth of social networks: The internet encyclopedia Wikipedia. Phys. Rev. E 74, 036116 (2006).</p>
<p>Wikipedias: Collaborative web-based encyclopedias as complex networks. V Zlatić, M Božičević, H Štefančić, M Domazet, Phys. Rev. E. 7416115Zlatić, V., Božičević, M., S Štefančić, H. &amp; Domazet, M. Wikipedias: Collaborative web-based encyclopedias as complex networks. Phys. Rev. E 74, 016115 (2006).</p>
<p>Analyzing and visualizing the semantic coverage of wikipedia and its authors: Research articles. T Holloway, M Bozicevic, K Börner, Complex. 12Holloway, T., Bozicevic, M. &amp; Börner, K. Analyzing and visualizing the semantic coverage of wikipedia and its authors: Research articles. Complex. 12, 30-40 (2007).</p>
<p>Human Brain Cloud homepage. K Gabler, Gabler, K. Human Brain Cloud homepage. http://www.humanbraincloud.com/. Date of access: 09/12/2014.</p>
<p>Complex structures and semantics in free word association. P Gravino, V D P Servedio, A Barrat, V Loreto, Adv. Complex Syst. 15250054Gravino, P., Servedio, V. D. P., Barrat, A. &amp; Loreto, V. Complex structures and semantics in free word association. Adv. Complex Syst. 15, 250054 (2012).</p>
<p>. D Elmes, Anki Website, Elmes, D. Anki website. http://ankisrs.net/. Date of access: 09/12/2014.</p>
<p>Education of a model student. T P Novikoff, J M Kleinberg, S H Strogatz, PNAS. 109Novikoff, T. P., Kleinberg, J. M. &amp; Strogatz, S. H. Education of a model student. PNAS 109, 1868-1873 (2012).</p>
<p>Facilitating retrieval from semantic memory: The effect of repeating part of an inference. A M Collins, M R Quillian, Acta Psychol. 33Collins, A. M. &amp; Quillian, M. R. Facilitating retrieval from semantic memory: The effect of repeating part of an inference. Acta Psychol. 33, 304-314 (1970).</p>
<p>A spreading-activation theory of semantic processing. A M Collins, E F Loftus, Psychol. Rev. 82407Collins, A. M. &amp; Loftus, E. F. A spreading-activation theory of semantic processing. Psychol. Rev. 82, 407 (1975).</p>
<p>The small world of human language. R Ferrer-I-Cancho, R V Solé, Proc. R. Soc. B. 268Ferrer-i-Cancho, R. &amp; Solé, R. V. The small world of human language. Proc. R. Soc. B 268, 2261-2265 (2001).</p>
<p>The large-scale structure of semantic networks: statistical analyses and a model of semantic growth. M Steyvers, J B Tenenbaum, Cognitive sci. 29Steyvers, M. &amp; Tenenbaum, J. B. The large-scale structure of semantic networks: statistical analyses and a model of semantic growth. Cognitive sci. 29, 41-78 (2005).</p>
<p>Navigating word association norms to extract semantic information. J Borge-Holthoefer, A Arenas, Proceedings of the 31st Annual Conference of the Cognitive Science Society. the 31st Annual Conference of the Cognitive Science SocietyAmsterdam, The NetherlandsBorge-Holthoefer, J. &amp; Arenas, A. Navigating word association norms to extract semantic information .In Proceedings of the 31st Annual Conference of the Cognitive Science Society, Amsterdam, The Netherlands(2009).</p>
<p>Google and the mind predicting fluency with pagerank. T L Griffiths, M Steyvers, A Firl, Psychol. Sci. 18Griffiths, T. L., Steyvers, M. &amp; Firl, A. Google and the mind predicting fluency with pagerank. Psychol. Sci. 18, 1069-1076 (2007).</p>
<p>Semantic networks: structure and dynamics. J Borge-Holthoefer, A Arenas, Entropy. 12Borge-Holthoefer, J. &amp; Arenas, A. Semantic networks: structure and dynamics. Entropy 12, 1264-1302 (2010).</p>
<p>Networks in cognitive science. A Baronchelli, R Ferrer-I Cancho, R Pastor-Satorras, N Chater, M H Christiansen, Trends Cogn. Sci. 17Baronchelli, A., Ferrer-i Cancho, R., Pastor-Satorras, R., Chater, N. &amp; Christiansen, M. H. Networks in cognitive science. Trends Cogn. Sci. 17, 348-360 (2013).</p>
<p>Memory: a contribution to experimental psychology (1885). H Ebbinghaus, Trans. H. A. Ruger and C. E. BusseniusTeachers College at Columbia UniversityEbbinghaus, H. Memory: a contribution to experimental psychology (1885). Trans. H. A. Ruger and C. E. Bussenius, Teachers College at Columbia University, 1913.</p>
<p>Principles of learning and memory. R G Crowder, Lawrence Erlbaum AssociatesCrowder, R. G. Principles of learning and memory (Lawrence Erlbaum Associates, 1976).</p>
<p>Theoretical implications of the spacing effect. D L Hintzman, Theories in cognitive psychology: The Loyola Symposium. (Lawrence Erlbaum. Solso, R. L.OxfordHintzman, D. L. Theoretical implications of the spacing effect. In Solso, R. L. (ed.) Theories in cognitive psychology: The Loyola Symposium. (Lawrence Erlbaum, Oxford, 1974).</p>
<p>Spacing Effects and Their Implications for Theory and Practice. F N Dempster, Educ. Psychol. Rev. 1Dempster, F. N. Spacing Effects and Their Implications for Theory and Practice. Educ. Psychol. Rev. 1, 309-330 (1989).</p>
<p>The importance of retrieval failures to long-term retention: A metacognitive explanation of the spacing effect. H Bahrick, L Hall, J. Mem. Lang. 52Bahrick, H. &amp; Hall, L. The importance of retrieval failures to long-term retention: A metacognitive explanation of the spacing effect. J. Mem. Lang. 52, 566-577 (2005).</p>
<p>Distributed practice in verbal recall tasks: A review and quantitative synthesis. N J Cepeda, H Pashler, E Vul, J T Wixted, D Rohrer, Psychol. Bull. 132Cepeda, N. J., Pashler, H., Vul, E., Wixted, J. T. &amp; Rohrer, D. Distributed practice in verbal recall tasks: A review and quantitative synthesis. Psychol. Bull. 132, 354-380 (2006).</p>
<p>Is Expanded Retrieval Practice a Superior Form of Spaced Retrieval? A Critical Review of the Extant Literature. D A Balota, J M Duchek, J M Logan, The foundations of remembering: Essays in honor of Henry L. Roediger, III. New YorkPsychology PressBalota, D. A., Duchek, J. M. &amp; Logan, J. M. Is Expanded Retrieval Practice a Superior Form of Spaced Retrieval? A Critical Review of the Extant Literature. In The foundations of remembering: Essays in honor of Henry L. Roediger, III, 83-105 (Psychology Press, New York, 2007).</p>
<p>Longitudinal analysis of early semantic networks: preferential attachment or preferential acquisition?. T T Hills, M Maouene, J Maouene, A Sheya, L Smith, Psychol. Sci. 20Hills, T. T., Maouene, M., Maouene, J., Sheya, A. &amp; Smith, L. Longitudinal analysis of early semantic networks: preferential attachment or preferential acquisition? Psychol. Sci. 20, 729-739 (2009).</p>
<p>The associative structure of language: contextual diversity in early word learning. T T Hills, J Maouene, B Riordan, L B Smith, J. Mem. Lang. 63Hills, T. T., Maouene, J., Riordan, B. &amp; Smith, L. B. The associative structure of language: contextual diversity in early word learning. J. Mem. Lang. 63, 259-273 (2010).</p>
<p>The effects of semantic and thematic clustering on the learning of second language vocabulary. T Tinkham, Second Lang. Res. 13Tinkham, T. The effects of semantic and thematic clustering on the learning of second language vocabulary. Second Lang. Res. 13, 138-163 (1997).</p>
<p>The anatomy of a large-scale hypertextual Web search engine. S Brin, L Page, Comput. Networks ISDN. 30Brin, S. &amp; Page, L. The anatomy of a large-scale hypertextual Web search engine. Comput. Networks ISDN. 30, 107-117 (1998).</p>
<p>On the evolution of random graphs. P Erdös, A Rényi, Publ. Math. Inst. Hung. Acad. Sci. 5Erdös, P. &amp; Rényi, A. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci. 5, 17-61 (1960).</p>
<p>Emergence of scaling in random networks. A.-L Barabási, R Albert, Science. 286Barabási, A.-L. &amp; Albert, R. Emergence of scaling in random networks. Science 286, 509-512 (1999).</p>
<p>Generation of uncorrelated random scale-free networks. M Catanzaro, R Pastor-Satorras, Phys. Rev. E. 714Catanzaro, M. &amp; Pastor-Satorras, R. Generation of uncorrelated random scale-free networks. Phys. Rev. E 71, 4 (2005).</p>
<p>. H S Heaps, Information Retrieval: Computational and Theoretical Aspects. Academic Press, IncHeaps, H. S. Information Retrieval: Computational and Theoretical Aspects (Academic Press, Inc., Orlando, FL, USA, 1978).</p>
<p>The Free Encyclopedia. Wikipedia, Wikipedia, The Free Encyclopedia. http://en.wikipedia.org/. Date of access: 22/11/2013.</p>
<p>A memory schedule. P Pimsleur, Mod. Lang. J. 51Pimsleur, P. A memory schedule. Mod. Lang. J. 51, 73-75 (1967).</p>
<p>. Api Mediawiki, MediaWiki API. http://en.wikipedia.org/w/api.php. Date of access: 22/11/2013.</p>
<p>1038/srep10286 decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the "author contributions statement". VDPS acknowledges the EU FP7 Grant 611272 (project GROWTHCOM) and CNR PNR Project "CRISIS Lab" for financial support. | Doi, Scientific RepoRts |. 510The authors acknowledge support from the KREYON project funded by the Templeton Foundation under contract n. 51663Scientific RepoRts | 5:10286 | DOi: 10.1038/srep10286 decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the "author contributions statement". VDPS acknowledges the EU FP7 Grant 611272 (project GROWTHCOM) and CNR PNR Project "CRISIS Lab" for financial support. The authors acknowledge support from the KREYON project funded by the Templeton Foundation under contract n. 51663.</p>            </div>
        </div>

    </div>
</body>
</html>
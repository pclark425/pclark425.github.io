<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-981 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-981</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-981</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-bad47c294e972d4997d6c121a14e719a5b4a9e99</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bad47c294e972d4997d6c121a14e719a5b4a9e99" target="_blank">Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper addresses the largely unexplored problem of designing experiments that simultaneously intervene on multiple variables, and develops efficient algorithms for optimizing different objective functions quantifying the informativeness of experiments.</p>
                <p><strong>Paper Abstract:</strong> Causal structure learning is a key problem in many domains. Causal structures can be learnt by performing experiments on the system of interest. We address the largely unexplored problem of designing experiments that simultaneously intervene on multiple variables. While potentially more informative than the commonly considered single-variable interventions, selecting such interventions is algorithmically much more challenging, due to the doubly-exponential combinatorial search space over sets of composite interventions. In this paper, we develop efficient algorithms for optimizing different objective functions quantifying the informativeness of experiments. By establishing novel submodularity properties of these objectives, we provide approximation guarantees for our algorithms. Our algorithms empirically perform superior to both random interventions and algorithms that only select single-variable interventions.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e981.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e981.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DGC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Double Greedy Continuous</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Stochastic continuous-greedy based algorithm for selecting multi-perturbation interventions to maximize the edge-orientation objective (F_EO); uses a continuous relaxation, stochastic gradient estimates, and rounding to produce near-greedy interventions from an enormous ground set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Double Greedy Continuous (DGC)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Optimizes the edge-orientation objective F_EO for multi-node interventions by (1) framing the marginal selection problem as maximization of a (non-monotone) submodular function over individual nodes, (2) using the Non-monotone Stochastic Continuous Greedy (NMSCG) algorithm to optimize the multilinear extension via stochastic gradient estimates (sampling DAGs and interventions), and (3) rounding the resulting fractional solution (e.g., pipage rounding) to produce an intervention set. Iteratively greedily adds approximately-greedy interventions to build a batch of size m and applies Meek rules to propagate orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated DAGs and gene regulatory network (GRN) models (DREAM3 subnetworks) under infinite- and finite-sample experimental simulations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Synthetic DAGs (Erdős–Rényi, trees, fully connected small graphs) and simulated GRN subnetworks from DREAM3 are used; environments are interactive/experimental in the sense that the algorithm actively chooses interventions (batches) and receives interventional samples (either infinite-sample idealization or finite-sample linear SEM draws).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Assumes no unobserved confounding; addresses spurious apparent associations indirectly via active interventions and posterior updating rather than explicit distractor models (confounding, measurement noise handled only via sampling assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Uses targeted interventions (multi-node interventions) to remove incoming edges to intervened nodes (hard interventions) and the Meek rules to propagate orientations; by performing interventions and observing outcomes (infinite or finite samples) the algorithm reduces the interventional Markov equivalence class, thereby refuting spurious edges implied by observational data alone.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Greedy batch construction: iteratively add an intervention that approximately maximizes marginal gain in F_EO using stochastic continuous greedy over a multilinear extension (sample DAGs to approximate gradients), then round to an intervention of size ≤ q; repeat until m interventions selected.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Empirically outperforms random and single-node greedy baselines on proportion of edges oriented in infinite-sample experiments; comparable to SSG-B in many cases and more robust on adversarial graph structures (e.g., fully connected small graphs, disconnected-star MECs). Performance measured as proportion of edges identified and downstream F1 in finite-sample experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline comparisons: substantially better than RAND (random q-node interventions) and single-node greedy; in finite-sample MI objective optimization DGC-∞ (optimizing F_EO as proxy) can perform worse than methods that directly optimize the finite-sample MI approximation (SSG-B). No numeric SHD/F1 numbers are provided in paper—results reported qualitatively and via plots (proportion of edges oriented, F1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DGC provides a principled, provably near-optimal approach (constant-factor guarantee) for selecting batches of multi-node interventions to maximize edge orientations; stochastic gradient sampling of DAGs and pipage rounding make searching the exponentially large ground set tractable; DGC is robust across graph families and particularly strong on adversarial structures where separating-system constructions may fail to include the best multi-node interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e981.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Separating System Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A greedy algorithm that restricts the huge intervention ground set to a constructed q-sparse separating system (set of small-size interventions guaranteed to separate all undirected edges) and then greedily selects m interventions from that system to approximately maximize the mutual-information infinite-sample objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SEPARATING SYSTEM GREEDY (SSG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructs a q-sparse separating system S (using graph-agnostic or graph-sensitive constructions) that guarantees for every undirected edge there exists an intervention in S that separates its endpoints; then greedily picks up to m interventions from S to maximize an approximation of the infinite-sample mutual information objective F_infty (approximated via a sampled multiset of DAGs). Submodularity of the approximated objective over S yields provable lower bounds on objective value; two variants (SSG-A: graph-agnostic, SSG-B: graph-sensitive) differ in how S is constructed.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated DAGs and GRN subnetworks (same experimental simulation environments as DGC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive experimental simulation where algorithm actively selects interventions; both infinite-sample idealized experiments and finite-sample linear SEM experiments are considered. Separating system construction may be graph-agnostic (guaranteed for any graph) or graph-sensitive (leverages structure).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Assumes no unobserved confounding; addresses spurious observational associations indirectly by guaranteeing via separating system that the selected interventions can distinguish endpoints of undirected edges (hence will refute spurious orientations arising only from observational equivalence).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation occurs via designing interventions (from the separating system) that distinguish endpoints of undirected edges; performing those interventions (with many samples) collapses the interventional MEC and rules out spurious edge orientations. The objective uses sampled DAGs to estimate expected reduction in essential-graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Construct a small covering/separating set S of interventions (size O((p/q) log p) for generic constructions); greedily pick the m interventions from S that maximize the sampled-approximation objective (tilde F_infty). For practical use, run for all q' ≤ q and pick best.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>SSG-B (graph-sensitive separating system) performs comparably to DGC and often better than SSG-A; both SSG variants significantly outperform random and single-node greedy selection. In finite-sample MI optimization SSG-B (directly optimizing F_MI via approximate computations) outperforms DGC-∞ which optimizes F_EO as a proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Greedy single-node baselines and random q-node interventions perform substantially worse (fewer edges oriented) than SSG approaches per plots in the paper. Exact numeric metrics not provided in text; comparisons presented as proportions and F1 curves.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reducing the candidate intervention set with a separating system makes the intractable search over multi-node interventions tractable while retaining theoretical guarantees (lower-bounds) and strong empirical performance; graph-sensitive separating systems (SSG-B) are especially effective and can match or beat DGC on many graph families, though DGC is more robust on some adversarial structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e981.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>F_MI / F_infty</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mutual Information Objective (finite-sample MI and infinite-sample F_infty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic objective: choose interventions to maximally reduce posterior entropy over DAGs (mutual information between DAGs and interventional data); in the infinite-sample limit this reduces to minimizing expected log size of the interventional essential graph (F_infty).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ABCD-strategy: Budgeted experimental design for targeted causal structure discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Mutual Information objective (F_MI) and infinite-sample approximation F_infty</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>F_MI: expected mutual information between interventional samples and posterior over graphs given current parameter estimates and prior over DAGs; operationalized by sampling DAGs and computing posterior updates with finite samples. F_infty: infinite-sample limiting form where each intervention yields enough samples to fully identify edges implied by that intervention, giving F_infty(ξ) = - (1/|G|) Σ_G log2 |Ess^{ξ∪ξ'}(G)|, which measures expected log essential-graph size after interventions and is used as a tractable optimization target in the infinite-sample regime.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated experimental settings (observational + interventional data), both infinite-sample idealized labs and finite-sample linear SEM simulations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Active experimental setup where batches of interventions are chosen under constraints (m interventions per batch, at most q nodes per intervention); objective used to select interventions that most reduce posterior uncertainty about the DAG.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Targets spurious graph structures that are members of the observational MEC (i.e., edges indistinguishable from observational data). Not designed to handle unobserved confounding (assumed absent).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Direct experimental refutation: interventions produce interventional distributions that rule out DAGs incompatible with observed interventional outcomes; in finite-sample implementations DAG posterior weights are re-weighted by likelihoods of combined observational and interventional data, downweighting spurious DAGs that poorly explain interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Optimize expected information gain (mutual information) over candidate batches; in practice approximated via sampling a multiset of DAGs (tilde G) and computing approximate F_infty or F_MI and then greedy or approximate maximization over interventions (SSG or DGC variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Directly optimizing finite-sample F_MI (using bootstrapped initial DAG distributions and likelihood reweighting) gave slightly better finite-sample performance (F_MI objective value and downstream F1) than optimizing the infinite-sample approximation F_infty; SSG-B (optimizing approximate F_MI) outperformed DGC-∞ in finite-sample experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Using infinite-sample proxies or optimizing F_EO as a proxy can lead to degraded finite-sample performance (DGC-∞ worse than SSG-B on finite-sample F_MI). Random and single-node greedy approaches obtain lower MI gains and downstream F1.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mutual-information-driven experimental design is a principled target for selecting experiments to reduce uncertainty over DAGs; approximating this objective via sampled DAGs and using separating-system reductions or stochastic optimization yields practical algorithms; accounting for finite samples (direct optimization of F_MI approximation) improves downstream edge-prediction performance versus relying solely on infinite-sample approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e981.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>F_EO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-Orientation Objective (EO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Objective that seeks to maximize the expected number of edges oriented by a batch of interventions, computed by simulating orientations due to interventions and applying Meek rules, averaged over DAGs in the current essential graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Budgeted experiment design for causal structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Edge-Orientation objective (F_EO)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>F_EO(ξ) = (1/|G|) Σ_G |R(ξ, G)| where R(ξ,G) is the set of edges oriented when performing interventions ξ on DAG G plus propagation via Meek rules; can be generalized with weights a(G) and w(e). Submodular and monotone over intervention sets, enabling greedy and approximate greedy algorithms with provable guarantees (used as target for DGC).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated DAGs and GRN models with active interventions (infinite- and finite-sample simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive experimental setting where batches of interventions are selected to maximize oriented edges; used both in idealized infinite-sample experiments and as a proxy objective for finite-sample design.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Indirectly targets spurious observational equivalences by selecting interventions that directly orient ambiguous edges; does not model unobserved confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventions cause incoming edges to intervened nodes to be removed; by observing resulting orientations (and applying Meek rules) edges inconsistent with interventional observations are ruled out, refuting spurious causal hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Greedy (or approximately greedy via DGC) maximization of expected number of oriented edges, with stochastic sampling over DAGs to approximate the objective and Meek-rule propagation to compute R(ξ,G).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Optimizing F_EO with DGC yields provable approximation guarantees and strong empirical performance in infinite-sample experiments; it's computationally tractable via stochastic gradient estimates. However, F_EO optimization can be misaligned with finite-sample MI objective leading to lower finite-sample F_MI performance compared to direct optimization of F_MI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Single-node greedy baselines perform worse in orienting edges than multi-perturbation methods optimizing F_EO; random interventions are substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>F_EO is submodular and monotone, enabling approximate-greedy strategies with theoretical guarantees; optimizing F_EO directly (via DGC) is effective in infinite-sample settings but may be a less faithful proxy under realistic finite-sample noise compared to direct MI-based objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e981.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NMSCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-monotone Stochastic Continuous Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic gradient-based continuous relaxation algorithm for approximately maximizing (non-monotone) submodular functions, used here to find approximately-greedy interventions over large combinatorial ground sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stochastic conditional gradient methods: From convex minimization to submodular maximization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Non-monotone Stochastic Continuous Greedy (NMSCG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Optimizes the multilinear extension of a set function by taking stochastic gradient steps: sample a DAG G and an intervention I from the distribution induced by the current fractional vector x to construct an unbiased gradient estimator, update x under constraints (Σ x_i ≤ q) and then round (e.g., pipage rounding) to get a discrete intervention; provides 1/e-type approximation guarantees for finding an approximately-greedy element for adding to a batch.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as a subroutine within the intervention-selection process over sampled DAGs and the essential graph (simulated experimental environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous-relaxation optimization executed within the active experimental design loop; interacts with sampled DAGs from the current essential graph and evaluations of the R function (Meek-rule propagation) to produce stochastic gradient estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Used to select an approximately-greedy intervention that will be added to the current batch by optimizing the multilinear extension under sparsity constraints and then rounding to a feasible intervention set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Theoretical guarantee: NMSCG with pipage rounding finds an intervention I satisfying E[F_EO^ξ(I)] ≥ (1/e) F_EO^ξ(I*) − ε with O(p^{5/2}/ε^3) evaluations of R; practical stabilized variants (Hessian approximation) reduce runtime order. Empirically enables DGC to obtain constant-factor approximation guarantees for batch selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stochastic continuous greedy methods make optimization over the exponentially large multi-node intervention space tractable by operating on a continuous relaxation and using sampling over DAGs to estimate gradients; combining NMSCG with rounding yields provable approximation bounds when used inside the greedy batch-construction loop.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e981.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Separating Systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>q-sparse G-separating system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combinatorial construction of a small set of q-node interventions such that for every undirected edge in the essential graph there exists an intervention in the set that includes exactly one of the edge endpoints, guaranteeing identifiability when this system is executed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On separating systems whose elements are sets of at most k elements</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>q-sparse separating system (construction from Wegener / Shanmugam et al. / Lindgren et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Algorithmic constructions (graph-agnostic and graph-sensitive variants) produce a set S of interventions with |S| = O((p/q) log p) (graph-agnostic upper bound) such that every undirected edge is separated by some S_i (exactly one endpoint in S_i). Executing S (or selecting from S greedily) suffices to fully orient the true DAG in the infinite-sample setting; using S reduces the search ground set dramatically for greedy optimization of submodular objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Experimental causal discovery over essential graphs derived from observational data (simulated DAGs/GRNs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Used as a pre-computed candidate groundset of feasible interventions (each of size ≤ q) from which greedy selection is performed; applicable in active experimental setups that allow executing these designed interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Targets ambiguity due to observational Markov equivalence (edges indistinguishable from observational data); not designed for unobserved confounding or irrelevant variables outside the causal model assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By ensuring an intervention in S separates each undirected edge, executing S with enough samples will identify edge directions and thereby refute spurious orientations consistent with observational equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Construct separating system S (either graph-agnostic or graph-sensitive) and then greedily choose up to m interventions from S to maximize the objective (tilde F_infty or F_EO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Using separating systems to limit candidate interventions yields strong empirical performance and theoretical bounds: SSG with a separating system produces objective guarantees (tilde F_infty(ξ) ≥ (1 - m/|S|) tilde F_infty(∅)) and in practice SSG-B (graph-sensitive) often matches or outperforms DGC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Separating-system constructions provide a principled reduction of the multi-perturbation combinatorial search space while preserving the ability to fully identify the DAG (in infinite-sample limit); graph-sensitive constructions (Lindgren et al.) are more effective in practice than graph-agnostic ones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e981.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MeekRules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meek Rules for orientation propagation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of sound and order-independent graph rules that orient additional edges in a partially directed graph after some edges become directed (due to background knowledge or interventions); used to propagate orientations after interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Casual Inference and Causal Explanation with Background Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Meek rules</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Four graphical implication rules (R1–R4) that repeatedly orient edges in a partially directed acyclic graph when certain patterns are present (e.g., if a→b and b−c and a and c are adjacent, orient a→c). In the context of interventions, after edges directly identified by intervention (R0) are directed, Meek rules are applied exhaustively to infer further orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Graph-based causal discovery over essential graphs (both simulated and applied to GRN subnetworks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Deterministic, graph-theoretic propagation step applied after each set of interventions (and their direct orientation effects) to infer additional edge directions; used both in objective computation (R function) and in evaluation of oriented edges.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Not targeted at statistical distractors per se; used to deterministically derive logical consequences of oriented edges, thereby reducing ambiguity from observational equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Orientation propagation via Meek rules can rule out spurious candidate directions by logical implication once some edges are oriented by interventions; thus helps refute spurious causal hypotheses that are incompatible with observed interventional orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Meek rules are computationally polynomial-time and order-independent; they are a core part of accurately computing R(ξ,G) used in objectives and are essential for the theoretical guarantees of orientation-based objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying Meek rules exhaustively after interventions is necessary to capture the full set of edges oriented by experiments; they are a deterministic tool for propagating orientation information and thereby help convert direct interventional signals into broader refutations of spurious orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e981.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrapping+Reweighting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap-based initial DAG distribution and posterior reweighting (Yang et al. method used here)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical finite-sample pipeline: estimate an initial distribution over DAGs by bootstrapping observational data and inferring DAGs, weight DAGs by observational likelihoods, then update weights after interventional data by computing interventional likelihoods to approximate the posterior over DAGs for MI-based objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Bootstrap DAG sampling + likelihood reweighting for finite-sample MI approximation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Obtain an initial set of candidate DAGs by bootstrapping observational data and running DAG inference (Yang et al.); weight each DAG proportionally to the likelihood of the observational data under that DAG and its MLE parameters; after interventions, re-weight DAGs by the likelihood of the combined observational and interventional data to approximate the posterior and compute F_MI (or its approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite-sample linear SEM simulations (observational samples + 3 samples per intervention in experiments presented)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Finite-sample experimental setting where realistic noise and small sample sizes matter; used as the practical approach to approximate the MI objective for active intervention selection and posterior-based edge prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Bootstrap sampling and posterior likelihood reweighting downweight DAGs that poorly explain combined data, which indirectly downweights spurious edges/hypotheses not supported by interventional evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Finite-sample noise and spurious DAG hypotheses arising from limited observational data; not designed to handle unobserved confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Spurious candidates are detected as low-likelihood models under observed data (observational and interventional); bootstrap spread reveals instability in learned structures.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Likelihood-based reweighting of sampled DAGs after receiving interventional data (posterior updating) reduces weight of DAGs that fail to explain interventional observations, thereby downweighting spurious causal hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventional data is used to reweight and eventually eliminate candidate DAGs inconsistent with interventions; thus interventions actively refute spurious correlations present in the observational bootstrap ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Used within finite-sample MI optimization: sample candidate DAGs via bootstrap, simulate/score candidate interventions by updating DAG weights with simulated interventional likelihoods, and choose interventions that maximize expected MI under the approximated posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Directly optimizing finite-sample F_MI using this pipeline (SSG-B optimizing F_MI) gave the best finite-sample performance among tested methods (higher F_MI and downstream F1) compared to optimizing infinite-sample proxies; demonstrates practical robustness improvements when accounting for finite-sample uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Approaches that ignored finite-sample effects (optimizing infinite-sample proxies) performed worse on finite-sample MI objective and downstream edge-prediction metrics in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bootstrapping to create an initial DAG ensemble plus likelihood reweighting after interventions is a practical technique for finite-sample active experimental design; it effectively downweights spurious candidate DAGs and improves downstream edge-prediction (F1) when the MI objective is directly optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e981.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e981.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InvariantCausalPrediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Invariant Causal Prediction (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related method that selects experiments via invariant causal prediction principles (stability across contexts) to identify direct causes of a target variable and weed out spurious correlates caused by non-causal factors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Invariant Causal Prediction: Experiment Selection through Stability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Active Invariant Causal Prediction (Gamella & Heinze-Deml)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Approach based on invariant causal prediction: identify variables whose conditional distribution of the target remains stable across environments/contexts; active experiment selection is used to create contexts to reveal invariances, thereby distinguishing causal predictors from spurious correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned as related work (not used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not applied in this paper; typically operates in multi-environment/interactive settings where one can create contexts and test invariance to prune spurious associations.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Stability/invariance testing across contexts to detect and discard variables whose association with the target is not stable (i.e., spurious correlates); experimental selection used to create informative contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlates due to context-specific associations, selection bias across environments, non-causal associations that fail invariance tests.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Tests for invariance of conditional distributions of the target given predictors across different experimentally induced contexts/environments.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Variables failing invariance tests are excluded or downweighted as likely non-causal predictors; the method focuses selection on experiments that best evaluate invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Active experiments that break spurious associations reveal non-invariant predictors which can be refuted as direct causes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Select interventions that create diverse contexts to test stability/invariance of predictor-target relationships; prioritize experiments that most reduce uncertainty about invariance status.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Not evaluated in this paper (mentioned as related work); method is known in literature to improve robustness to spurious correlations when invariance assumptions hold.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a complementary experimental-design approach focused on learning direct causes of one variable via invariance; relevant because it explicitly targets spurious correlates using stability tests, unlike the main algorithms in this paper which focus on global DAG identifiability under the no-unobserved-confounding assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ABCD-strategy: Budgeted experimental design for targeted causal structure discovery <em>(Rating: 2)</em></li>
                <li>Budgeted experiment design for causal structure learning <em>(Rating: 2)</em></li>
                <li>Learning causal graphs with small interventions <em>(Rating: 2)</em></li>
                <li>Experimental design for cost-aware learning of causal graphs <em>(Rating: 2)</em></li>
                <li>Active Invariant Causal Prediction: Experiment Selection through Stability <em>(Rating: 2)</em></li>
                <li>Stochastic conditional gradient methods: From convex minimization to submodular maximization <em>(Rating: 1)</em></li>
                <li>On separating systems whose elements are sets of at most k elements <em>(Rating: 1)</em></li>
                <li>Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-981",
    "paper_id": "paper-bad47c294e972d4997d6c121a14e719a5b4a9e99",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "DGC",
            "name_full": "Double Greedy Continuous",
            "brief_description": "Stochastic continuous-greedy based algorithm for selecting multi-perturbation interventions to maximize the edge-orientation objective (F_EO); uses a continuous relaxation, stochastic gradient estimates, and rounding to produce near-greedy interventions from an enormous ground set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Double Greedy Continuous (DGC)",
            "method_description": "Optimizes the edge-orientation objective F_EO for multi-node interventions by (1) framing the marginal selection problem as maximization of a (non-monotone) submodular function over individual nodes, (2) using the Non-monotone Stochastic Continuous Greedy (NMSCG) algorithm to optimize the multilinear extension via stochastic gradient estimates (sampling DAGs and interventions), and (3) rounding the resulting fractional solution (e.g., pipage rounding) to produce an intervention set. Iteratively greedily adds approximately-greedy interventions to build a batch of size m and applies Meek rules to propagate orientations.",
            "environment_name": "Simulated DAGs and gene regulatory network (GRN) models (DREAM3 subnetworks) under infinite- and finite-sample experimental simulations",
            "environment_description": "Synthetic DAGs (Erdős–Rényi, trees, fully connected small graphs) and simulated GRN subnetworks from DREAM3 are used; environments are interactive/experimental in the sense that the algorithm actively chooses interventions (batches) and receives interventional samples (either infinite-sample idealization or finite-sample linear SEM draws).",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Assumes no unobserved confounding; addresses spurious apparent associations indirectly via active interventions and posterior updating rather than explicit distractor models (confounding, measurement noise handled only via sampling assumptions).",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "Uses targeted interventions (multi-node interventions) to remove incoming edges to intervened nodes (hard interventions) and the Meek rules to propagate orientations; by performing interventions and observing outcomes (infinite or finite samples) the algorithm reduces the interventional Markov equivalence class, thereby refuting spurious edges implied by observational data alone.",
            "uses_active_learning": true,
            "inquiry_strategy": "Greedy batch construction: iteratively add an intervention that approximately maximizes marginal gain in F_EO using stochastic continuous greedy over a multilinear extension (sample DAGs to approximate gradients), then round to an intervention of size ≤ q; repeat until m interventions selected.",
            "performance_with_robustness": "Empirically outperforms random and single-node greedy baselines on proportion of edges oriented in infinite-sample experiments; comparable to SSG-B in many cases and more robust on adversarial graph structures (e.g., fully connected small graphs, disconnected-star MECs). Performance measured as proportion of edges identified and downstream F1 in finite-sample experiments.",
            "performance_without_robustness": "Baseline comparisons: substantially better than RAND (random q-node interventions) and single-node greedy; in finite-sample MI objective optimization DGC-∞ (optimizing F_EO as proxy) can perform worse than methods that directly optimize the finite-sample MI approximation (SSG-B). No numeric SHD/F1 numbers are provided in paper—results reported qualitatively and via plots (proportion of edges oriented, F1).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "DGC provides a principled, provably near-optimal approach (constant-factor guarantee) for selecting batches of multi-node interventions to maximize edge orientations; stochastic gradient sampling of DAGs and pipage rounding make searching the exponentially large ground set tractable; DGC is robust across graph families and particularly strong on adversarial structures where separating-system constructions may fail to include the best multi-node interventions.",
            "uuid": "e981.0",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "SSG",
            "name_full": "Separating System Greedy",
            "brief_description": "A greedy algorithm that restricts the huge intervention ground set to a constructed q-sparse separating system (set of small-size interventions guaranteed to separate all undirected edges) and then greedily selects m interventions from that system to approximately maximize the mutual-information infinite-sample objective.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SEPARATING SYSTEM GREEDY (SSG)",
            "method_description": "Constructs a q-sparse separating system S (using graph-agnostic or graph-sensitive constructions) that guarantees for every undirected edge there exists an intervention in S that separates its endpoints; then greedily picks up to m interventions from S to maximize an approximation of the infinite-sample mutual information objective F_infty (approximated via a sampled multiset of DAGs). Submodularity of the approximated objective over S yields provable lower bounds on objective value; two variants (SSG-A: graph-agnostic, SSG-B: graph-sensitive) differ in how S is constructed.",
            "environment_name": "Simulated DAGs and GRN subnetworks (same experimental simulation environments as DGC)",
            "environment_description": "Interactive experimental simulation where algorithm actively selects interventions; both infinite-sample idealized experiments and finite-sample linear SEM experiments are considered. Separating system construction may be graph-agnostic (guaranteed for any graph) or graph-sensitive (leverages structure).",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Assumes no unobserved confounding; addresses spurious observational associations indirectly by guaranteeing via separating system that the selected interventions can distinguish endpoints of undirected edges (hence will refute spurious orientations arising only from observational equivalence).",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "Refutation occurs via designing interventions (from the separating system) that distinguish endpoints of undirected edges; performing those interventions (with many samples) collapses the interventional MEC and rules out spurious edge orientations. The objective uses sampled DAGs to estimate expected reduction in essential-graph size.",
            "uses_active_learning": true,
            "inquiry_strategy": "Construct a small covering/separating set S of interventions (size O((p/q) log p) for generic constructions); greedily pick the m interventions from S that maximize the sampled-approximation objective (tilde F_infty). For practical use, run for all q' ≤ q and pick best.",
            "performance_with_robustness": "SSG-B (graph-sensitive separating system) performs comparably to DGC and often better than SSG-A; both SSG variants significantly outperform random and single-node greedy selection. In finite-sample MI optimization SSG-B (directly optimizing F_MI via approximate computations) outperforms DGC-∞ which optimizes F_EO as a proxy.",
            "performance_without_robustness": "Greedy single-node baselines and random q-node interventions perform substantially worse (fewer edges oriented) than SSG approaches per plots in the paper. Exact numeric metrics not provided in text; comparisons presented as proportions and F1 curves.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Reducing the candidate intervention set with a separating system makes the intractable search over multi-node interventions tractable while retaining theoretical guarantees (lower-bounds) and strong empirical performance; graph-sensitive separating systems (SSG-B) are especially effective and can match or beat DGC on many graph families, though DGC is more robust on some adversarial structures.",
            "uuid": "e981.1",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "F_MI / F_infty",
            "name_full": "Mutual Information Objective (finite-sample MI and infinite-sample F_infty)",
            "brief_description": "An information-theoretic objective: choose interventions to maximally reduce posterior entropy over DAGs (mutual information between DAGs and interventional data); in the infinite-sample limit this reduces to minimizing expected log size of the interventional essential graph (F_infty).",
            "citation_title": "ABCD-strategy: Budgeted experimental design for targeted causal structure discovery",
            "mention_or_use": "use",
            "method_name": "Mutual Information objective (F_MI) and infinite-sample approximation F_infty",
            "method_description": "F_MI: expected mutual information between interventional samples and posterior over graphs given current parameter estimates and prior over DAGs; operationalized by sampling DAGs and computing posterior updates with finite samples. F_infty: infinite-sample limiting form where each intervention yields enough samples to fully identify edges implied by that intervention, giving F_infty(ξ) = - (1/|G|) Σ_G log2 |Ess^{ξ∪ξ'}(G)|, which measures expected log essential-graph size after interventions and is used as a tractable optimization target in the infinite-sample regime.",
            "environment_name": "Simulated experimental settings (observational + interventional data), both infinite-sample idealized labs and finite-sample linear SEM simulations",
            "environment_description": "Active experimental setup where batches of interventions are chosen under constraints (m interventions per batch, at most q nodes per intervention); objective used to select interventions that most reduce posterior uncertainty about the DAG.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Targets spurious graph structures that are members of the observational MEC (i.e., edges indistinguishable from observational data). Not designed to handle unobserved confounding (assumed absent).",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "Direct experimental refutation: interventions produce interventional distributions that rule out DAGs incompatible with observed interventional outcomes; in finite-sample implementations DAG posterior weights are re-weighted by likelihoods of combined observational and interventional data, downweighting spurious DAGs that poorly explain interventions.",
            "uses_active_learning": true,
            "inquiry_strategy": "Optimize expected information gain (mutual information) over candidate batches; in practice approximated via sampling a multiset of DAGs (tilde G) and computing approximate F_infty or F_MI and then greedy or approximate maximization over interventions (SSG or DGC variants).",
            "performance_with_robustness": "Directly optimizing finite-sample F_MI (using bootstrapped initial DAG distributions and likelihood reweighting) gave slightly better finite-sample performance (F_MI objective value and downstream F1) than optimizing the infinite-sample approximation F_infty; SSG-B (optimizing approximate F_MI) outperformed DGC-∞ in finite-sample experiments.",
            "performance_without_robustness": "Using infinite-sample proxies or optimizing F_EO as a proxy can lead to degraded finite-sample performance (DGC-∞ worse than SSG-B on finite-sample F_MI). Random and single-node greedy approaches obtain lower MI gains and downstream F1.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Mutual-information-driven experimental design is a principled target for selecting experiments to reduce uncertainty over DAGs; approximating this objective via sampled DAGs and using separating-system reductions or stochastic optimization yields practical algorithms; accounting for finite samples (direct optimization of F_MI approximation) improves downstream edge-prediction performance versus relying solely on infinite-sample approximations.",
            "uuid": "e981.2",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "F_EO",
            "name_full": "Edge-Orientation Objective (EO)",
            "brief_description": "Objective that seeks to maximize the expected number of edges oriented by a batch of interventions, computed by simulating orientations due to interventions and applying Meek rules, averaged over DAGs in the current essential graph.",
            "citation_title": "Budgeted experiment design for causal structure learning",
            "mention_or_use": "use",
            "method_name": "Edge-Orientation objective (F_EO)",
            "method_description": "F_EO(ξ) = (1/|G|) Σ_G |R(ξ, G)| where R(ξ,G) is the set of edges oriented when performing interventions ξ on DAG G plus propagation via Meek rules; can be generalized with weights a(G) and w(e). Submodular and monotone over intervention sets, enabling greedy and approximate greedy algorithms with provable guarantees (used as target for DGC).",
            "environment_name": "Simulated DAGs and GRN models with active interventions (infinite- and finite-sample simulations)",
            "environment_description": "Interactive experimental setting where batches of interventions are selected to maximize oriented edges; used both in idealized infinite-sample experiments and as a proxy objective for finite-sample design.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Indirectly targets spurious observational equivalences by selecting interventions that directly orient ambiguous edges; does not model unobserved confounding.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "Interventions cause incoming edges to intervened nodes to be removed; by observing resulting orientations (and applying Meek rules) edges inconsistent with interventional observations are ruled out, refuting spurious causal hypotheses.",
            "uses_active_learning": true,
            "inquiry_strategy": "Greedy (or approximately greedy via DGC) maximization of expected number of oriented edges, with stochastic sampling over DAGs to approximate the objective and Meek-rule propagation to compute R(ξ,G).",
            "performance_with_robustness": "Optimizing F_EO with DGC yields provable approximation guarantees and strong empirical performance in infinite-sample experiments; it's computationally tractable via stochastic gradient estimates. However, F_EO optimization can be misaligned with finite-sample MI objective leading to lower finite-sample F_MI performance compared to direct optimization of F_MI.",
            "performance_without_robustness": "Single-node greedy baselines perform worse in orienting edges than multi-perturbation methods optimizing F_EO; random interventions are substantially worse.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "F_EO is submodular and monotone, enabling approximate-greedy strategies with theoretical guarantees; optimizing F_EO directly (via DGC) is effective in infinite-sample settings but may be a less faithful proxy under realistic finite-sample noise compared to direct MI-based objectives.",
            "uuid": "e981.3",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "NMSCG",
            "name_full": "Non-monotone Stochastic Continuous Greedy",
            "brief_description": "A stochastic gradient-based continuous relaxation algorithm for approximately maximizing (non-monotone) submodular functions, used here to find approximately-greedy interventions over large combinatorial ground sets.",
            "citation_title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
            "mention_or_use": "use",
            "method_name": "Non-monotone Stochastic Continuous Greedy (NMSCG)",
            "method_description": "Optimizes the multilinear extension of a set function by taking stochastic gradient steps: sample a DAG G and an intervention I from the distribution induced by the current fractional vector x to construct an unbiased gradient estimator, update x under constraints (Σ x_i ≤ q) and then round (e.g., pipage rounding) to get a discrete intervention; provides 1/e-type approximation guarantees for finding an approximately-greedy element for adding to a batch.",
            "environment_name": "Used as a subroutine within the intervention-selection process over sampled DAGs and the essential graph (simulated experimental environments)",
            "environment_description": "Continuous-relaxation optimization executed within the active experimental design loop; interacts with sampled DAGs from the current essential graph and evaluations of the R function (Meek-rule propagation) to produce stochastic gradient estimates.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Used to select an approximately-greedy intervention that will be added to the current batch by optimizing the multilinear extension under sparsity constraints and then rounding to a feasible intervention set.",
            "performance_with_robustness": "Theoretical guarantee: NMSCG with pipage rounding finds an intervention I satisfying E[F_EO^ξ(I)] ≥ (1/e) F_EO^ξ(I*) − ε with O(p^{5/2}/ε^3) evaluations of R; practical stabilized variants (Hessian approximation) reduce runtime order. Empirically enables DGC to obtain constant-factor approximation guarantees for batch selection.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Stochastic continuous greedy methods make optimization over the exponentially large multi-node intervention space tractable by operating on a continuous relaxation and using sampling over DAGs to estimate gradients; combining NMSCG with rounding yields provable approximation bounds when used inside the greedy batch-construction loop.",
            "uuid": "e981.4",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Separating Systems",
            "name_full": "q-sparse G-separating system",
            "brief_description": "Combinatorial construction of a small set of q-node interventions such that for every undirected edge in the essential graph there exists an intervention in the set that includes exactly one of the edge endpoints, guaranteeing identifiability when this system is executed.",
            "citation_title": "On separating systems whose elements are sets of at most k elements",
            "mention_or_use": "use",
            "method_name": "q-sparse separating system (construction from Wegener / Shanmugam et al. / Lindgren et al.)",
            "method_description": "Algorithmic constructions (graph-agnostic and graph-sensitive variants) produce a set S of interventions with |S| = O((p/q) log p) (graph-agnostic upper bound) such that every undirected edge is separated by some S_i (exactly one endpoint in S_i). Executing S (or selecting from S greedily) suffices to fully orient the true DAG in the infinite-sample setting; using S reduces the search ground set dramatically for greedy optimization of submodular objectives.",
            "environment_name": "Experimental causal discovery over essential graphs derived from observational data (simulated DAGs/GRNs)",
            "environment_description": "Used as a pre-computed candidate groundset of feasible interventions (each of size ≤ q) from which greedy selection is performed; applicable in active experimental setups that allow executing these designed interventions.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Targets ambiguity due to observational Markov equivalence (edges indistinguishable from observational data); not designed for unobserved confounding or irrelevant variables outside the causal model assumptions.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "By ensuring an intervention in S separates each undirected edge, executing S with enough samples will identify edge directions and thereby refute spurious orientations consistent with observational equivalence.",
            "uses_active_learning": true,
            "inquiry_strategy": "Construct separating system S (either graph-agnostic or graph-sensitive) and then greedily choose up to m interventions from S to maximize the objective (tilde F_infty or F_EO).",
            "performance_with_robustness": "Using separating systems to limit candidate interventions yields strong empirical performance and theoretical bounds: SSG with a separating system produces objective guarantees (tilde F_infty(ξ) ≥ (1 - m/|S|) tilde F_infty(∅)) and in practice SSG-B (graph-sensitive) often matches or outperforms DGC.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Separating-system constructions provide a principled reduction of the multi-perturbation combinatorial search space while preserving the ability to fully identify the DAG (in infinite-sample limit); graph-sensitive constructions (Lindgren et al.) are more effective in practice than graph-agnostic ones.",
            "uuid": "e981.5",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "MeekRules",
            "name_full": "Meek Rules for orientation propagation",
            "brief_description": "Set of sound and order-independent graph rules that orient additional edges in a partially directed graph after some edges become directed (due to background knowledge or interventions); used to propagate orientations after interventions.",
            "citation_title": "Casual Inference and Causal Explanation with Background Knowledge",
            "mention_or_use": "use",
            "method_name": "Meek rules",
            "method_description": "Four graphical implication rules (R1–R4) that repeatedly orient edges in a partially directed acyclic graph when certain patterns are present (e.g., if a→b and b−c and a and c are adjacent, orient a→c). In the context of interventions, after edges directly identified by intervention (R0) are directed, Meek rules are applied exhaustively to infer further orientations.",
            "environment_name": "Graph-based causal discovery over essential graphs (both simulated and applied to GRN subnetworks)",
            "environment_description": "Deterministic, graph-theoretic propagation step applied after each set of interventions (and their direct orientation effects) to infer additional edge directions; used both in objective computation (R function) and in evaluation of oriented edges.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Not targeted at statistical distractors per se; used to deterministically derive logical consequences of oriented edges, thereby reducing ambiguity from observational equivalence.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": "Orientation propagation via Meek rules can rule out spurious candidate directions by logical implication once some edges are oriented by interventions; thus helps refute spurious causal hypotheses that are incompatible with observed interventional orientations.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Meek rules are computationally polynomial-time and order-independent; they are a core part of accurately computing R(ξ,G) used in objectives and are essential for the theoretical guarantees of orientation-based objectives.",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Applying Meek rules exhaustively after interventions is necessary to capture the full set of edges oriented by experiments; they are a deterministic tool for propagating orientation information and thereby help convert direct interventional signals into broader refutations of spurious orientations.",
            "uuid": "e981.6",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Bootstrapping+Reweighting",
            "name_full": "Bootstrap-based initial DAG distribution and posterior reweighting (Yang et al. method used here)",
            "brief_description": "Practical finite-sample pipeline: estimate an initial distribution over DAGs by bootstrapping observational data and inferring DAGs, weight DAGs by observational likelihoods, then update weights after interventional data by computing interventional likelihoods to approximate the posterior over DAGs for MI-based objectives.",
            "citation_title": "Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions",
            "mention_or_use": "use",
            "method_name": "Bootstrap DAG sampling + likelihood reweighting for finite-sample MI approximation",
            "method_description": "Obtain an initial set of candidate DAGs by bootstrapping observational data and running DAG inference (Yang et al.); weight each DAG proportionally to the likelihood of the observational data under that DAG and its MLE parameters; after interventions, re-weight DAGs by the likelihood of the combined observational and interventional data to approximate the posterior and compute F_MI (or its approximations).",
            "environment_name": "Finite-sample linear SEM simulations (observational samples + 3 samples per intervention in experiments presented)",
            "environment_description": "Finite-sample experimental setting where realistic noise and small sample sizes matter; used as the practical approach to approximate the MI objective for active intervention selection and posterior-based edge prediction.",
            "handles_distractors": null,
            "distractor_handling_technique": "Bootstrap sampling and posterior likelihood reweighting downweight DAGs that poorly explain combined data, which indirectly downweights spurious edges/hypotheses not supported by interventional evidence.",
            "spurious_signal_types": "Finite-sample noise and spurious DAG hypotheses arising from limited observational data; not designed to handle unobserved confounding.",
            "detection_method": "Spurious candidates are detected as low-likelihood models under observed data (observational and interventional); bootstrap spread reveals instability in learned structures.",
            "downweighting_method": "Likelihood-based reweighting of sampled DAGs after receiving interventional data (posterior updating) reduces weight of DAGs that fail to explain interventional observations, thereby downweighting spurious causal hypotheses.",
            "refutation_method": "Interventional data is used to reweight and eventually eliminate candidate DAGs inconsistent with interventions; thus interventions actively refute spurious correlations present in the observational bootstrap ensemble.",
            "uses_active_learning": true,
            "inquiry_strategy": "Used within finite-sample MI optimization: sample candidate DAGs via bootstrap, simulate/score candidate interventions by updating DAG weights with simulated interventional likelihoods, and choose interventions that maximize expected MI under the approximated posterior.",
            "performance_with_robustness": "Directly optimizing finite-sample F_MI using this pipeline (SSG-B optimizing F_MI) gave the best finite-sample performance among tested methods (higher F_MI and downstream F1) compared to optimizing infinite-sample proxies; demonstrates practical robustness improvements when accounting for finite-sample uncertainty.",
            "performance_without_robustness": "Approaches that ignored finite-sample effects (optimizing infinite-sample proxies) performed worse on finite-sample MI objective and downstream edge-prediction metrics in the experiments.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Bootstrapping to create an initial DAG ensemble plus likelihood reweighting after interventions is a practical technique for finite-sample active experimental design; it effectively downweights spurious candidate DAGs and improves downstream edge-prediction (F1) when the MI objective is directly optimized.",
            "uuid": "e981.7",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "InvariantCausalPrediction",
            "name_full": "Active Invariant Causal Prediction (related work)",
            "brief_description": "Related method that selects experiments via invariant causal prediction principles (stability across contexts) to identify direct causes of a target variable and weed out spurious correlates caused by non-causal factors.",
            "citation_title": "Active Invariant Causal Prediction: Experiment Selection through Stability",
            "mention_or_use": "mention",
            "method_name": "Active Invariant Causal Prediction (Gamella & Heinze-Deml)",
            "method_description": "Approach based on invariant causal prediction: identify variables whose conditional distribution of the target remains stable across environments/contexts; active experiment selection is used to create contexts to reveal invariances, thereby distinguishing causal predictors from spurious correlates.",
            "environment_name": "Mentioned as related work (not used in experiments)",
            "environment_description": "Not applied in this paper; typically operates in multi-environment/interactive settings where one can create contexts and test invariance to prune spurious associations.",
            "handles_distractors": true,
            "distractor_handling_technique": "Stability/invariance testing across contexts to detect and discard variables whose association with the target is not stable (i.e., spurious correlates); experimental selection used to create informative contexts.",
            "spurious_signal_types": "Spurious correlates due to context-specific associations, selection bias across environments, non-causal associations that fail invariance tests.",
            "detection_method": "Tests for invariance of conditional distributions of the target given predictors across different experimentally induced contexts/environments.",
            "downweighting_method": "Variables failing invariance tests are excluded or downweighted as likely non-causal predictors; the method focuses selection on experiments that best evaluate invariance.",
            "refutation_method": "Active experiments that break spurious associations reveal non-invariant predictors which can be refuted as direct causes.",
            "uses_active_learning": true,
            "inquiry_strategy": "Select interventions that create diverse contexts to test stability/invariance of predictor-target relationships; prioritize experiments that most reduce uncertainty about invariance status.",
            "performance_with_robustness": "Not evaluated in this paper (mentioned as related work); method is known in literature to improve robustness to spurious correlations when invariance assumptions hold.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as a complementary experimental-design approach focused on learning direct causes of one variable via invariance; relevant because it explicitly targets spurious correlates using stability tests, unlike the main algorithms in this paper which focus on global DAG identifiability under the no-unobserved-confounding assumption.",
            "uuid": "e981.8",
            "source_info": {
                "paper_title": "Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ABCD-strategy: Budgeted experimental design for targeted causal structure discovery",
            "rating": 2
        },
        {
            "paper_title": "Budgeted experiment design for causal structure learning",
            "rating": 2
        },
        {
            "paper_title": "Learning causal graphs with small interventions",
            "rating": 2
        },
        {
            "paper_title": "Experimental design for cost-aware learning of causal graphs",
            "rating": 2
        },
        {
            "paper_title": "Active Invariant Causal Prediction: Experiment Selection through Stability",
            "rating": 2
        },
        {
            "paper_title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization",
            "rating": 1
        },
        {
            "paper_title": "On separating systems whose elements are sets of at most k elements",
            "rating": 1
        },
        {
            "paper_title": "Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions",
            "rating": 2
        }
    ],
    "cost": 0.023731,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning</h1>
<p>Scott Sussex<br>Department of Computer Science<br>ETH Zürich<br>Zürich, Switzerland<br>scott.sussex@inf.ethz.ch</p>
<p>Andreas Krause<br>Department of Computer Science<br>ETH Zürich<br>Zürich, Switzerland</p>
<p>Caroline Uhler<br>Laboratory for Information \&amp; Decision Systems<br>Massachusetts Institute of Technology<br>Cambridge, MA</p>
<h4>Abstract</h4>
<p>Causal structure learning is a key problem in many domains. Causal structures can be learnt by performing experiments on the system of interest. We address the largely unexplored problem of designing a batch of experiments that each simultaneously intervene on multiple variables. While potentially more informative than the commonly considered single-variable interventions, selecting such interventions is algorithmically much more challenging, due to the doubly-exponential combinatorial search space over sets of composite interventions. In this paper, we develop efficient algorithms for optimizing different objective functions quantifying the informativeness of a budget-constrained batch of experiments. By establishing novel submodularity properties of these objectives, we provide approximation guarantees for our algorithms. Our algorithms empirically perform superior to both random interventions and algorithms that only select single-variable interventions.</p>
<h2>1 Introduction</h2>
<p>The problem of finding the causal relationships between a set of variables is ubiquitous throughout the sciences. For example, scientists are interested in reconstructing gene regulatory networks (GRNs) of biological cells [11]. Directed Acyclic Graphs (DAGs) are a natural way to represent causal structures, with a directed edge from variable $X$ to $Y$ representing $X$ being a direct cause of $Y$ [33].
Learning the causal structure of a set of variables is fundamentally difficult. With only observational data, in general we can only identify the true DAG up to a set of DAGs called its Markov Equivalence Class (MEC) [35]. Empirically, for sparse DAGs the size of the MEC grows exponentially in the number of nodes [17]. Identifiability can be improved by intervening on variables, meaning one perturbs a subset of the variables and then observes more samples from the system [8, 16, 40]. There exist various inference algorithms for learning causal structures from a combination of observational and interventional data $[16,36,40,28,34]$. Here we focus on the identification of DAGs that have no unobserved confounding variables.
Performing experiments is often expensive, however. Thus, we are interested in learning as much about the causal structure as possible given some constraints on the interventions. In this work, we focus on the batched setting, where several interventions are performed in parallel. This is a natural setting in scientific domains like reconstructing GRNs. Existing works propose meaningful objective</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: a) We illustrate the MEC of a tree graph on 5 nodes. b) Two single-node interventions are required to fully identify the true DAG. c) Only one two-node intervention is required to fully identify the true DAG. d) This MEC contains 5 DAGs, each corresponding to a different root node (marked white). This is a property particular to tree MECs.</p>
<p>functions for this batched causal structure learning problem and then give algorithms that have provable guarantees [3, 13]. However, these works focus on the setting where only a <em>single</em> random variable is perturbed per intervention. It is an open question as to whether there exist efficient algorithms for the <em>multiple-perturbation</em> setting, where more than one variable is perturbed in each intervention.</p>
<p>For the task of reconstructing GRNs, it is now possible for experimenters to perturb multiple genes in a single cell [2, 7]. Figures 1 b) and c) illustrate a specific example where a two-node intervention completely identifies a DAG in half as many interventions as single-node interventions. In general, it is possible for a set of q-node interventions to orient <em>up to q-times more</em> edges in a DAG than single-node interventions (see the supplementary material for a more general example). While multi-perturbation interventions can be more informative, designing them is algorithmically challenging because it leads to an <em>exponentially</em> larger search space: any algorithm must now select a set of sets.</p>
<p>Our main contribution is to provide efficient algorithms for different objective functions with accompanying performance bounds. We demonstrate empirically on both synthetic and GRN graphs that our algorithms result in greater identifiability than existing approaches that do not make use of multiple perturbations [13, 40], as well as a random strategy.</p>
<p>We begin by introducing the notation and the objective functions considered in this work in Section 2, before reviewing related work in Section 3. In Section 4 we present our algorithms along with proofs of their performance guarantees. Finally, in Section 5 we demonstrate the superior empirical performance of our method over existing baselines on both synthetic networks and on data generated from models of real GRNs.</p>
<h2>2 Background and Problem Statement</h2>
<h3>Causal DAGs</h3>
<p>Consider a causal DAG $G = ([p], E)$ where $[p]: = {1, \ldots, p}$ is a set of nodes and $E$ is a set of directed edges. Let $(i,j) \in E$ iff there is an edge from node $i$ to node $j$. Each node $i$ is associated with a random variable $X_i$. In the GRN example, $X_i$ would be the measurement of the gene expression level for gene $i$. An edge from $i \to j$ would represent gene $i$ having a causal effect on the expression of gene $j$. The functional dependence of a random variable on its parents can be described by a <em>structural equation model</em> (SEM).</p>
<p>The probability distribution over $X = (X_1, \ldots, X_p)$ is related to $G$ by the Markov property, meaning each variable $X_i$ is conditionally independent of its non-descendants given its parents [33]. From conditional independence tests one can determine the MEC of $G$, a set of DAGs with the same conditional independancies between variables. All members of the MEC share the same undirected skeleton and colliders [35]. A collider is a pair of edges $(i,k), (j,k) \in E$ such that $(i,j), (j,i) \notin E$. The <em>essential graph</em> of $G$, Ess($G$), is a partially directed graph, with directed edges where all members of the MEC share the same edge direction, and with undirected edges otherwise [5]. Ess($G$) uniquely represents the MEC of $G$. These MECs can be large, so we seek to perform interventions on the nodes to reduce the MEC to a smaller set of possible DAGs.</p>
<h3>Interventions</h3>
<p>We use the term <em>intervention</em> to refer to a set $I \subset [p]$ of perturbation targets (variables). We assume all interventions are <em>hard interventions</em>, meaning intervening on a set $I$ removes the incoming edges to the random variables $X_I := (X_i)_{i \in I}$ and sets their joint distribution</p>
<p>to some interventional distribution $\mathcal{P}^{I}$ [9]. In the GRN reconstruction example this corresponds to, for example, running an experiment where we knockout all genes in set $I$. Some of our results extend easily to the alternative model of soft interventions [25], as we discuss in the supplementary material.</p>
<p>We use $\mathcal{I}=2^{|p|}$ to refer to the set of all possible interventions. Our goal will be to select a batch $\xi$ of interventions, where $\xi$ is a multiset of some $I \in \mathcal{I}$. For practical reasons, we typically have constraints on the number of interventions, i.e., $|\xi| \leq m$ and on the number of variables involved in each intervention $|I| \leq q, \forall I \in \xi$. Namely, there are at most $m$ interventions per batch and each intervention contains at most $q$ nodes. A constraint on the number of perturbations per intervention is natural in reconstructing GRNs, since perturbing too many genes in one cell will leave it unlikely to survive. We refer to the set of $\xi \mathrm{s}$ satisfying these constraints as $C_{m, q}$. The observational distribution (no intervention) is given by $\xi=\emptyset$.
For any set of interventions $\xi$ and DAG $G$, there is a set of $\xi$-Markov equivalent DAGs. These are the set of DAGs that have the same set of conditional independencies under all $I \in \xi$ and under the observational distribution. This set of DAGs is no larger than the MEC of $G$ and can similarly be characterized by an essential graph $\operatorname{Ess}^{\xi}(G)$ [16].
We will always assume that there exist no unobserved common causes of any pair of nodes in $G$. We also assume that the distribution of the random variables satisfies faithfulness with respect to $G$ [33].
Choosing optimal interventions We seek to maximize an objective function $F$ that quantifies our certainty in the true DAG. In general our goal is to determine</p>
<p>$$
\underset{\xi \in C_{m, q}}{\arg \max } F(\xi)
$$</p>
<p>A natural choice for $F$ is given by Agrawal et al. [3]. They assume that there exist parameters $\theta$ that determine the functional relationships between random variables. For example, this could be the coefficients in a linear model. Given existing data $D$, we try to choose $\xi$ that maximizes</p>
<h1>Objective 1 (Mutual Information (MI)).</h1>
<p>$$
F_{\mathrm{MI}}(\xi)=\mathbb{E}<em G_="G," _hat_theta="\hat{\theta" _mid="\mid" y="y">{G \mid D} \mathbb{E}</em>(y, \xi ; D)]
$$}, \xi}[\hat{U}_{M . I</p>
<p>where $y$ is the set of samples from the interventions, $\hat{\theta}$ is the current estimate of the parameters given $D$ and $G$, and $\hat{U}<em m_="m," q="q">{M . I}(y, \xi ; D)$ is the mutual information between the posterior over $G$ and the samples $y$. Each intervention produces one sample in $y$. The use of mutual information means the objective aims to, in expectation over all observed samples and true DAGs, minimize the entropy of the posterior distribution over DAGs. There already exist a number of algorithms for determining the posterior over DAGs from observational or experimental data [40, 36, 16].
Infinite sample objectives Finding algorithms that optimize the MI objective is difficult because we have to account for noisy observations and limited samples. To remove this complexity, we study the limiting case of infinitely many samples per unique intervention. The constraints given by $C</em>$ to be the set of interventions contained in our dataset before our current batch. In this setting, maximizing Objective 1 reduces to maximizing}$ still stipulate that there can be only $m$ unique interventions, but each intervention can be performed with an infinite number of samples. We also assume that an essential graph is already known (i.e., we have infinite observational samples and infinite samples for any experiments performed so far). For objectives with infinite samples per intervention, we treat $\xi$ as a set of interventions, not a multiset, since there is no change in objective value for choosing an intervention twice. Consider $\xi^{\prime</p>
<h2>Objective 2 (Mutual info. inf. samples (MI- $\infty$ )).</h2>
<p>$$
F_{\infty}(\xi)=-\frac{1}{|\mathcal{G}|} \sum_{G \in \mathcal{G}} \log _{2}\left|\operatorname{Ess}^{\xi \cup \xi^{\prime}}(G)\right|
$$</p>
<p>where $\operatorname{Ess}^{\xi \cup \xi^{\prime}}(G)$ refers to the updated essential graph after performing interventions in $\xi$. The objective aims to, on average across possible true DAGs, minimize the log of the essential graph size after performing the interventions. The derivation of this objective is given in the supplementary material.
Ghassami et al. [13] study a different objective in the infinite-sample setting. The objective seeks to, on average across possible $G$ given the current essential graph, orient as many edges as possible. Let</p>
<p>$R\left(\xi, G, \xi^{\prime}\right)$ be the set of edges oriented by $\xi$ if the true DAG is $G$, and the essential graph is given by the $\xi^{\prime}$-MEC.</p>
<p>Objective 3 (Edge-orientation (EO)).</p>
<p>$$
F_{\mathrm{EO}}(\xi)=\frac{1}{|\mathcal{G}|} \sum_{G \in \mathcal{G}}\left|R\left(\xi, G, \xi^{\prime}\right)\right|
$$</p>
<p>The function $R$ is computed as follows. Firstly, $\forall I \in \xi$, orient undirected edge $i-j$ in $\operatorname{Ess}^{\xi^{\prime}}(G)$ if $i \in I$ but $j \notin I$ or vice-versa. Secondly, execute the Meek Rules [26], which allow inferring additional edge orientations (discussed in the supplementary material) on the resulting partially directed graph. Finally, output the set of all edges oriented. Agrawal et al. [3] show that this objective is not consistent; however, this is because they fix $\mathcal{G}$ to be the MEC instead of using the most up-to-date essential graph for each batch. In the supplementary material, we show that the version of the objective we work with is indeed consistent. We will drop the dependence of $R$ on $\xi^{\prime}$ for readability.
Below, we provide algorithms with near-optimality guarantees for Objectives 2 and 3, while motivating a practical algorithm for Objective 1.</p>
<h1>3 Related Work</h1>
<p>Causality has been widely studied in machine learning [30, 31]. Here we focus on prior research that is most relevant to our work.
Agrawal et al. [3] and Ghassami et al. [13] give near-optimal greedy algorithms for Objectives 1 and 3 respectively. Ahmaditeshnizi, Salehkaleybar, and Kiyavash [4] present a dynamic programming algorithm for an adversarial version of Objective 3, optimizing for the worst case ground truth DAG in the MEC. However, all these algorithms only apply to single-perturbation interventions. Both of these works use the submodularity of the two objectives. In this paper we address the exponentially large search space that arises when designing multi-perturbation interventions, a strictly harder problem.
Much existing work in experimental design for causal DAGs is focused on identifying the graph uniquely, while minimizing some cost associated with doing experiments [9, 18, 32, 21, 23]. When the MEC is large and the number of experiments is small, identifying the entire graph will be infeasible. Instead, one must select interventions that optimize a measure of the information gained about the causal graph.
Lindgren et al. [23] show NP-hardness for selecting an intervention set of at most $m$ interventions, with minimum number of perturbations, that completely identifies the true DAG. This, however, does not directly imply a hardness result for our problem.
Gamella and Heinze-Deml [10] propose an approach to experimental design for causal structure learning based on invariant causal prediction. While our approach has guarantees for objectives relating to either the whole graph or functions of the oriented edges, their work is specific to the problem of learning the direct causes of one variable.
Acharya et al. [1] consider testing between two candidate causal models. However, the setting differs from ours: they assume the underlying DAG is known but allow for unobserved confounding variables.
Designing multi-perturbation interventions has been previously studied in linear cyclic networks, with a focus on parameter identification [14]. Here we focus on causal graph identification in DAGs.</p>
<h2>4 Greedy Algorithms for Experiment Design</h2>
<p>All of our algorithms follow the same general strategy. Like in previous works on single-perturbation experimental design [3, 13], we greedily add interventions to our intervention set. We add $I$ maximizing $F(\xi \cup{I})$ where $\xi$ is the currently proposed set of interventions. This greedy selection is justified because our objectives are submodular, a property we define formally later. For singleperturbation experimental design, this is algorithmically simple since there are only $p$ possible interventions. However, for multi-perturbation interventions even selecting greedily is intractable at scale since we have $\binom{p}{2}$ possible interventions. Therefore we provide ways to find an intervention</p>
<p>that is approximately greedy, i.e, an intervention with marginal improvement in objective that is close to that of the greedy intervention.</p>
<p>A further challenge with the greedy approach is that it involves evaluating the objective, which for our objectives is a potentially exponential sum over members of an essential graph. Each of the two algorithms we give has a different strategy for overcoming this.</p>
<p>In Section 4.1, we present Double Greedy Continuous (DGC) for optimizing Objective 3, the edge-orientation objective. For greedily selecting interventions to maximize an exponential sum, we employ the stochastic continuous optimization technique of Hassani et al. [15].</p>
<p>In Section 4.2, we present SEPARATING SYSTEM GREEDY (SSG) for optimizing Objective 2, MI- $\infty$. To greedily select interventions, we use the construction of separating systems (SS) [37, 32, 23], to create a smaller set of interventions to search over. Collectively, the interventions in the SS fully orient the graph. To handle tractably evaluating the objective, we use the idea of Ghassami et al. [13] and Agrawal et al. [3] to optimize an approximation of the objective constructed using a limited sample of DAGs.</p>
<p>To give near-optimality guarantees for these algorithms, we will use two properties of the objectives: monotonicity and submodularity.
Definition 1. A set function $F: 2^{V} \rightarrow \mathbb{R}$ is monotonically increasing if for all sets $I_{1} \subseteq I_{2} \subseteq V$ we have $F\left(I_{1}\right) \leq F\left(I_{2}\right)$.
Definition 2. A set function $F: 2^{V} \rightarrow \mathbb{R}$ is submodular if for all sets $I_{1} \subseteq I_{2} \subseteq V$ and all $v \in V \backslash I_{2}$ we have $F\left(I_{1} \cup{v}\right)-F\left(I_{1}\right) \geq F\left(I_{2} \cup{v}\right)-F\left(I_{2}\right)$.</p>
<p>Submodularity is a natural diminishing returns property, and many strategies have been studied for optimizing submodular objectives [22]. In both the above definitions, $V$ is called the groundset, the set that we can choose elements from. In the single-perturbation problem, the groundset is just $[p]$, whereas in our case it is all subsets of up to $q$ nodes.</p>
<p>We show that DGC achieves an objective value within a constant factor of the optimal intervention set on Objective 3. SSG does not achieve a constant-factor guarantee, but for both infinite sample objectives we obtain a lower bound on its performance.</p>
<p>All of our algorithms run in polynomial time; however, they assume access to a uniform sampler across all DAGs in the essential graph. This exists for sampling from the MEC [39] but not for essential graphs given existing interventions. In practice, we find that an efficient non-uniform sampler [13] can be used to achieve strong empirical performance.</p>
<h1>4.1 Optimizing the Edge-orientation Objective</h1>
<p>In the following, we develop an algorithm for maximizing Objective 3.
In fact, the algorithm we provide has a near-optimality guarantee for a more general form of $F_{\mathrm{EO}}$, namely</p>
<p>$$
F_{\mathrm{EO}}(\xi)=\sum_{G \in \mathcal{G}} a(G) \sum_{e \in G} w(e) \mathbb{1}(e \in R(\xi, G))
$$</p>
<p>where $\forall e, w(e) \geq 0$ and $\forall G, a(G) \geq 0$. The weights $a(G)$ can be thought of as corresponding to having a non-uniform prior over the DAGs in the essential graph, whilst the weights $w(e)$ can be thought of as assigning priority to the orienting of certain edges. The inner sum above is a weighted coverage function [22] over the set of edges.
We will first show that $F_{\mathrm{EO}}$ is monotone submodular over groundset $\mathcal{I}$. This generalizes a result by Ghassami et al. [13] who showed the same result for groundset $[p]$ (single perturbation interventions).
Lemma 1. $F_{E O}$ is monotone submodular over the groundset $\mathcal{I}$.
Proof. All proofs are presented in the supplementary material unless otherwise stated.
As mentioned, we cannot use a greedy search directly since the groundset $\mathcal{I}$ is too large. Instead, we develop an algorithm for selecting an intervention with near-maximal utility compared to the greedy choice. In particular, our strategy is to prove a submodularity result over the function $F$ with modified domain. Consider the set function $F_{\mathrm{EO}}^{\xi}(I)=F_{\mathrm{EO}}(\xi \cup{I})$ for fixed $\xi$.</p>
<p>Lemma 2. $F_{E O}^{\xi}$ is non-monotone submodular over the groundset $[p]$.
The Non-monotone Stochastic Continuous Greedy (NMSCG) algorithm of Mokhtari, Hassani, and Karbasi [27] can therefore be used as a subroutine to select, in expectation, an approximately greedy intervention to add to an existing intervention set. The algorithm uses a stochastic gradient-based method to optimize a continuous relaxation of our objective, and then rounds the solution to obtain a set of interventions. The continuous relaxation of $F_{\mathrm{EO}}^{\xi}$ is the multilinear extension</p>
<p>$$
f_{\mathrm{EO}}^{\xi}(x)=\sum_{I \in \mathcal{I}} F_{\mathrm{EO}}^{\xi}(I) \prod_{i \in I} x_{i} \prod_{i \notin I}\left(1-x_{i}\right)
$$</p>
<p>with constraints $\sum_{i} x_{i} \leq q, 0 \leq x_{i} \leq 1$ for all nodes $i$. The multilinear extension can be thought of as computing the expectation of $F_{\mathrm{EO}}^{\xi}(I)$, when input $x$ is a vector of independent probabilities such that $x_{i}$ is the probability of including node $i$ in the intervention. The sum over $\mathcal{I}$ in $f_{\mathrm{EO}}^{\xi}$ and the sum over DAGs in $F_{\mathrm{EO}}^{\xi}$ make computing the gradient of this objective intractable. Therefore, we compute an unbiased stochastic approximation of the gradient $\nabla f_{\mathrm{EO}}^{\xi}(x)$ by uniformly sampling a DAG $G$ from $\mathcal{G}$ and intervention $I$ from the distribution specified by $x$. Define</p>
<p>$$
\hat{f}_{\mathrm{EO}}^{\xi}(I, G)=|R(\xi \cup{I}, G)|
$$</p>
<p>Mokhtari, Hassani, and Karbasi [27] show that an unbiased estimate of the gradient of $f(x)$ can be computed by sampling $G$ and $I$ to approximate</p>
<p>$$
\frac{\partial}{\partial x_{i}} f_{\mathrm{EO}}^{\xi}(x)=\underset{G, I \mid x}{\mathbb{E}}\left[\hat{f}<em i="i">{\mathrm{EO}}^{\xi}\left(I, G ; I</em>} \leftarrow 1\right)-\hat{f<em i="i">{\mathrm{EO}}^{\xi}\left(I, G ; I</em> \leftarrow 0\right)\right]
$$</p>
<p>where $I_{i} \leftarrow 0$ means that if $i \in I$, remove it. The use of a stochastic gradient means that $F_{\mathrm{EO}}$ can be efficiently optimized despite it being a possibly exponential sum over $\mathcal{G}$.
After several gradient updates we obtain a vector of probabilities $x$ that approximately maximizes $f_{\mathrm{EO}}$. To obtain an intervention $I$ one uses a Round function, for example pipage rounding which, on submodular functions, has the guarantee that $\mathbb{E}\left[F_{\mathrm{EO}}^{\xi}(\operatorname{Round}(x))\right]=f_{\mathrm{EO}}^{\xi}(x)$ [6].
Theorem 1 (Mokhtari, Hassani, and Karbasi [27]). Let $I^{*}$ be the maximizer of $F_{E O}^{\xi}$. NMSCG with pipage rounding, after $\mathcal{O}\left(p^{5 / 2} / \epsilon^{3}\right)$ evaluations of $R$, achieves a solution $I$ such that</p>
<p>$$
\mathbb{E}\left[F_{E O}^{\xi}(I)\right] \geq \frac{1}{e} F_{E O}^{\xi}\left(I^{*}\right)-\epsilon
$$</p>
<p>The original result measures runtime in terms of the number of times we approximate the gradient in Equation 5 with a single sample (in our case a single $G, I$ tuple). From Equations 4 and 5 we can see that the number of gradient approximations is a constant factor of the number of evaluations of $R$. Hence, we measure runtime in terms of number of evaluations of $R$. The bottleneck for evaluating $R$ is applying the Meek Rules, which can be computed in time polynomial in $p$ [26]. Note that the NMSCG subroutine can be modified to stabilize gradient updates by the approximation of a Hessian, in which case the same guarantee can be achieved in $\mathcal{O}\left(p^{3 / 2} / \epsilon^{2}\right)$ [15]. We use this version of NMSCG for the experiments.
Our main result now follows from the fact that selecting interventions approximately greedily will lead to an approximation guarantee due to lemma 1.
Theorem 2. Let $\xi^{*} \in C_{N, b}$ be the maximizer of Objective 3. DGC will, after $\mathcal{O}\left(m^{4} p^{5 / 2} / \epsilon^{3}\right)$ evaluations of $R$, achieve a solution $\xi$ such that</p>
<p>$$
\mathbb{E}\left[F_{E O}(\xi)\right] \geq\left(1-\frac{1}{e^{1 / e}}\right) F_{E O}\left(\xi^{*}\right)-\epsilon
$$</p>
<p>We highlight this is a constant-factor guarantee with respect to the optimal batch of interventions. Our bound requires compute that is low order in $p$ with no dependence on $q$. Without even accounting for computing the possibly exponential sum in the objective, merely enumerating all possible interventions for fixed $q$ is $\mathcal{O}\left(p^{q}\right)$.
Ghassami et al. [13] give a similar constant-factor guarantee for batches of single-perturbation interventions $(q=1)$. Their bound is within $1-\frac{1}{e}$ of the optimal single-perturbation batch. In the</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Double</span><span class="w"> </span><span class="n">Greedy</span><span class="w"> </span><span class="n">Continu</span><span class="o">-</span>
<span class="n">OUS</span><span class="p">(</span><span class="n">DGC</span><span class="p">)</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">essential</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">constraints</span><span class="w"> </span>\<span class="p">(</span><span class="n">C_</span><span class="p">{</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">}</span>\<span class="p">),</span>
<span class="w">    </span><span class="n">objective</span><span class="w"> </span>\<span class="p">(</span><span class="n">F_</span><span class="p">{</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">EO</span><span class="p">}}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Init</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">xi</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">emptyset</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span>\<span class="p">(</span><span class="o">|</span>\<span class="n">xi</span><span class="o">|</span><span class="w"> </span>\<span class="n">leq</span><span class="w"> </span><span class="n">m</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">I</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">ROUND</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">NMSCG</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">F_</span><span class="p">{</span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">EO</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span>\<span class="n">xi</span><span class="p">},</span><span class="w"> </span><span class="n">q</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">xi</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">xi</span><span class="w"> </span>\<span class="n">cup</span>\<span class="p">{</span><span class="n">I</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">xi</span>\<span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">interventions</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">SEPARATING</span><span class="w"> </span><span class="n">SYSTEM</span>
<span class="n">GREEDY</span><span class="p">(</span><span class="n">SSG</span><span class="p">)</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">essential</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">constraints</span><span class="w"> </span>\<span class="p">(</span><span class="n">C_</span><span class="p">{</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">}</span>\<span class="p">),</span>
<span class="w">    </span><span class="n">objective</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">tilde</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">infty</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Init</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">I</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">emptyset</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">SEPARATE</span><span class="p">}(</span><span class="n">q</span><span class="p">,</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">})</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span>\<span class="p">(</span><span class="o">|</span>\<span class="n">xi</span><span class="o">|</span><span class="w"> </span>\<span class="n">leq</span><span class="w"> </span><span class="n">m</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">I</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">arg</span><span class="w"> </span>\<span class="nb">max</span><span class="w"> </span><span class="n">I_</span><span class="p">{</span>\<span class="n">xi</span><span class="p">}</span><span class="w"> </span>\<span class="n">tilde</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">infty</span><span class="p">}(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">I</span><span class="p">}</span><span class="w"> </span>\<span class="n">cup</span>\<span class="p">{</span><span class="n">I</span>\<span class="p">})</span>\<span class="p">)</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">xi</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">xi</span><span class="w"> </span>\<span class="n">cup</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">I</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">xi</span>\<span class="p">)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">interventions</span>
</code></pre></div>

<p>supplementary material, we show that the optimal multi-perturbation intervention can orient up to $q$ times more edges than the optimal single perturbation intervention. In Section 5 we experimentally verify the value of multi-perturbation interventions by comparing DGC to the algorithm presented in Ghassami et al. [13]. If we allow for soft interventions, the $1-\frac{1}{\epsilon}$ guarantee can also be obtained for multi-perturbation interventions. See the supplementary material for details.</p>
<h1>4.2 Optimizing the Mutual Information Objective</h1>
<p>We now consider an algorithm for maximizing Objective 2. First, we note that computing the sum over $\mathcal{G}$ and the size of $\xi \cup \xi^{\prime}$-essential graphs is computationally intractable. The computation of $\left|\operatorname{Ess}^{\xi \cup \xi^{\prime}}(G)\right|$ makes $F_{\infty}$ a nested sum over DAGs. Like Agrawal et al. [3], we optimize a computationally tractable approximation to $F_{\infty}$. First, we uniformly sample a multiset of DAGs $\tilde{\mathcal{G}}$ from $\mathcal{G}$ to construct our approximate objective</p>
<p>$$
\tilde{F}<em G="G" _in="\in" _tilde_mathcal_G="\tilde{\mathcal{G">{\infty}(\xi)=-\frac{1}{|\tilde{\mathcal{G}}|} \sum</em>(G)\right|
$$}}} \log _{2}\left|\widetilde{\operatorname{Ess}}^{\xi \cup \xi^{\prime}</p>
<p>where $\widetilde{\operatorname{Ess}}^{\xi \cup \xi^{\prime}}$ is the submultiset of $\tilde{\mathcal{G}}$ consisting of elements in $\operatorname{Ess}^{\xi \cup \xi^{\prime}}$. A submodularity result similar to lemma 1 can also be proven for $\tilde{F}<em _infty="\infty">{\infty}$.
Lemma 3. $\tilde{F}</em>$ is monotone submodular.
We first show that an approach similar to that used by DGC does not so easily give a near-optimal guarantee. Similarly to above, define $\tilde{F}<em _infty="\infty">{\infty}^{\xi}(I)=\tilde{F}</em>(\xi \cup{I})$ for fixed $\xi$.
Proposition 1. There exists $\mathcal{G}, \tilde{\mathcal{G}}$ such that $\tilde{F}<em _infty="\infty">{\infty}^{\xi}$ is not submodular.
Hence we cannot use existing algorithms for submodular optimization to construct near-greedy interventions. We instead take a different approach. Suppose we can reduce $\mathcal{I}$ to some set of interventions $\mathcal{S}$ much smaller than $\mathcal{I}$, such that $\tilde{F}</em>$ comes from separating system constructions.
Definition 3. A $q$-sparse $\mathcal{G}$-separating system of size $N$ is a set of interventions $\mathcal{S}=\left{S_{1}, S_{2} \ldots, S_{N}\right}$ such that $\left|S_{i}\right| \leq q$ and for every undirected edge $(i, j) \in \mathcal{G}$ there is an element $S \in \mathcal{S}$ such that exactly one of $i, j$ is in $S$ [32].}(\mathcal{S})$ has the maximum possible objective value. Due to lemma 3, we can obtain a guarantee by greedily selecting $m$ interventions from $\mathcal{S}$. A method for constructing the set $\mathcal{S</p>
<p>A separating system of $\mathcal{G}$ completely identifies the true DAG, and hence obtains the maximum possible objective value 0 without necessarily satisfying the constraints $C_{m, q}$. As an example, in Figure 1(b,c) we see 1 and 2-sparse separating systems respectively.
We will make use of an algorithm $\operatorname{SEPARATE}(q, \mathcal{G})$ which efficiently constructs a $q$-sparse separating system of $\mathcal{G}$. Wegener [37] and Shanmugam et al. [32] give construction methods that are agnostic to the structure of $\mathcal{G}$ (it will identify any DAG with $p$ nodes). Lindgren et al. [23] give a construction method that depends on the structure of $\mathcal{G}$.
Since the separating system obtains the maximum objective value, we can greedily select $m$ interventions from this set and obtain a lower bound on the objective value due to submodularity (lemma 3).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: a-c and f give infinite sample experiments and d-e give finite samples. a) Our algorithms ( $q=3$ ) orient more edges than random interventions ( $q=3$ ) and greedily chosen $q=1$ interventions for $p=40$, ER( 0.1$)$ graphs. b) On a fully connected graph ( $\mathrm{p}=5, \mathrm{~m}=2$ ), SSG-B does not improve as $q$ increases. c) On a $p=20$ forest of 3 disconnected star graphs, SSG-A cannot orient the full graph as quickly as our alternative approaches $(q=3)$. d) For finite-samples, $p=40, \operatorname{ER}(0.1)$ graphs, the proposed methods $(q=5)$ achieve greater objective value than greedy $q=1$ interventions. Optimizing the finite sample objective directly yields slightly greater objective value than the infinite sample approximations. e) The F1 scores for predicting the presence of each edge correspond well with the objective in d). f) For a $p=50$ yeast subnetwork from the DREAM3 challenge, our algorithms $(q=5)$ orient more edges in the ground truth DAG than random or $q=1$ greedy algorithms with the same batch size.</p>
<p>Theorem 3. For $q \leq\lfloor p / 2\rfloor, m \leq|\mathcal{S}|$, SSG outputs $\xi \in C_{m, q}$ with objective value</p>
<p>$$
\tilde{F}<em _infty="\infty">{\infty}(\xi) \geq\left(1-\frac{m}{\lceil p / q\rceil\lceil\log p\rceil}\right) \tilde{F}</em>(\emptyset)
$$</p>
<p>in $\mathcal{O}\left(m|\hat{\mathcal{G}}| \frac{p}{q} \log p\right)$ evaluations of $R$, when using SEPARATE as in Shanmugam et al. [32].
Increasing $q$ does not necessarily increase $\tilde{F}<em _infty="\infty">{\infty}(\xi)$. However, the bound we give becomes more favourable as $q$ increases because the upper bound on $|\mathcal{S}|$ decreases. In practice, we run $\operatorname{SSG} \forall q^{\prime} \leq q$ and pick the intervention set with the highest objective value on $\tilde{F}</em>$.
Note that SSG can also be used with a similar guarantee for an analogous approximation of $F_{\mathrm{EO}}$.</p>
<h1>5 Experiments</h1>
<p>To evaluate our algorithms we consider three settings. Firstly, randomly generated DAGs, using infinite samples per intervention. Secondly, randomly generated DAGs with linear SEMs, using finite samples per intervention. Finally, subnetworks of GRN models, using infinite samples per intervention. Full details on all of our experiments can be found in the supplementary material. For code to reproduce the experiments, see https://github.com/ssethz/multi-perturbation-ed.</p>
<p>Infinite samples We evaluate our algorithms using Objective 3. We consider selecting a batch of experiments where only the MEC is currently known. We vary the type of random graph and the constraint set $C_{m, q}$. The following methods are compared:</p>
<ul>
<li>
<p>RAND: a baseline that for $m$ interventions, independently and uniformly at random selects $q$ nodes from those adjacent to at least one undirected edge;</p>
</li>
<li>
<p>Greedy: greedily selects a single-perturbation intervention as in Ghassami et al. [13];</p>
</li>
<li>DGC: our stochastic optimization approach;</li>
<li>SSG-A: our greedy approach using the graph agnostic separating systems of Shanmugam et al. [32];</li>
<li>SSG-B: as above, using the graph-dependent separating system constructor of Lindgren et al. [23].</li>
</ul>
<p>Since there are infinite samples per intervention, the exact SEM used to generate data is not relevant. We plot the mean proportion of edges identified and error bars of 1 standard deviation over 100 repeats. Noise between repeats is due to randomness in the graph structure and in the algorithms themselves.</p>
<p>In Figure 2 a) we display the results for Erdös-Renyí random graphs with edge density 0.1 (ER(0.1)) and 40 nodes. To prevent trivial graphs and large runtimes, we only consider graphs with MEC sizes in the range $[20,200]$. The observations given here were also found for denser Erdös-Renyí graphs and tree graphs, in addition to graphs with less nodes.</p>
<p>For all constraint values, all the algorithms improve greatly over Rand and Greedy. SSG-B outperforms SSG-A, likely because the graph-sensitive separating system construction tends to return a groundset of more effective interventions.</p>
<p>SSG-B achieves similar objective value to DGC. However, DGC behaves most robustly when the graph structure is chosen adversarially. For example, consider Figure 2 b). Here we plot the proportion of identified edges on a $p=5$ fully connected graph. On this graph, the separating system construction of Lindgren et al. [23] will always return the set of all single-node interventions. Therefore, its performance does not improve with $q$, whilst DGC's does. An adversarial example for SSG-A is constructed in Figure 2 c): an MEC that consists of 3 disconnected star graphs with 7, 7 and 6 nodes. In this case, DGC and SSG-B can orient most of the graph in a single intervention, whereas SSG-A will likely not contain such an intervention in the separating system it constructs.</p>
<p>Finite samples We use linear SEMs, with weights generated uniformly in $[-1,-0.25] \cup[0.25,1]$. Measurement noise is given by the standard normal distribution. The underlying DAGs are generated in the same way as the infinite sample experiments. Before experiment selection, we obtain 800 observational samples of the system. 3 samples are obtained for each intervention selected by our algorithms. Each perturbation fixes the value of a node to 5 . We approximate Objective 1 using the methods of Agrawal et al. [3]. In particular, an initial distribution over DAGs is estimated by bootstrapping the observational data and using the techniques of Yang, Katcoff, and Uhler [40] to infer DAGs. Each DAG in the distribution is weighted proportionally to the likelihood of the observational data given the DAG and the maximum likelihood estimate of the linear model weights. The posterior over DAGs after interventions is computed by re-weighting the existing set of DAGs based on the likelihood of the combined observational and interventional data. To ensure the distribution has support near the true DAG, we include all members of the true DAG's MEC in the initial distribution.</p>
<p>With finite samples, our methods do not have guarantees but can be adapted into practical algorithms:</p>
<ul>
<li>Greedy: greedily optimize Objective 1 with single perturbation interventions (Agrawal et al. [3]);</li>
<li>DGC- $\infty$ : optimizes Objective 3, with the summation over DAGs being a weighted sum over the DAGs in the initial distribution;</li>
<li>SSG-B: optimizes $F_{\mathrm{MI}}$, greedily selecting from the separating system of Lindgren et al. [23];</li>
<li>SSG-B- $\infty$ : approximates the objective using Objective 2 and optimizes with SSG.</li>
</ul>
<p>For each algorithm, we record $F_{\mathrm{MI}}$ of the selected interventions, over 200 repeats. In Figure 2 d) the objective values obtained by each algorithm are shown for varying batch size and $q=5$. DGC- $\infty$ performs worse than SSG-B and SSG-B- $\infty$, perhaps because it is optimizing $F_{\mathrm{EO}}$ which is not totally aligned with $F_{\mathrm{MI}}$. Between SSG-B and SSG-B- $\infty$, there was a small benefit to directly optimizing $F_{\mathrm{MI}}$ as opposed to its infinite sample approximation $F_{\infty}$. Accounting for finite samples may lead to greater improvements when there is heteroscedastic noise or a wider range of weights.</p>
<p>Performance on $F_{\mathrm{MI}}$ corresponds closely with performance on a downstream task as shown in Figure 2 e). For each algorithm, we compute the posterior over DAGs given the selected interventions. Then, we independently predict the presence of each edge in the true DAG. Figure 2 e) plots the average F1 score for each algorithm. In finite samples, our approaches outperform both RAND and GREEDY.</p>
<p>DREAM 3 networks We evaluate our algorithms under infinite samples using subgraphs of GRN models. From the "DREAM 3 In Silico Network" challenge [24], we use the 5 subgraphs with $p=50$ nodes. Here, we present the results for "Yeast1", which is the graph requiring the most interventions for our methods to orient. Results for the other graphs are in the supplementary material.
We compare the same algorithms as considered in the other infinite sample experiments. In Figure 2 f) we record the proportion of unknown edges that were oriented by each algorithm. Our methods and RAND all have intervention sizes of 5. For each method we perform 5 repeats. On Yeast1, our methods all perform similarly and outperform RAND and Greedy. This is found for the other DREAM 3 graphs too, except on one subgraph where Greedy performs similarly to our methods.</p>
<h1>6 Conclusions</h1>
<p>We presented near-optimal algorithms for causal structure learning through multi-perturbation interventions. Our results make novel use of submodularity properties and separating systems to search over a doubly exponential domain. Empirically, we demonstrated that these algorithms yield significant improvements over random interventions and state-of-the-art single-perturbation algorithms. These methods are particularly relevant in genomics applications, where causal graphs are large but multiple genes can be intervened upon simultaneously.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>This research was supported in part by the Swiss National Science Foundation, under NCCR Automation, grant agreement 51NF40 180545. Caroline Uhler was partially supported by NSF (DMS1651995), ONR (N00014-17-1-2147 and N00014-18-1-2765), IBM, and a Simons Investigator Award.
Thank you to Raj Agrawal for a helpful discussion regarding experiments.
Experiments were performed on the Leonhard cluster managed by the HPC team at ETH Zürich</p>
<h2>References</h2>
<p>[1] Jayadev Acharya et al. "Learning and Testing Causal Models with Interventions". In: Advances in Neural Information Processing Systems. Ed. by S. Bengio et al. Vol. 31. Curran Associates, Inc., 2018.
[2] Britt Adamson et al. "A multiplexed single-cell CRISPR screening platform enables systematic dissection of the unfolded protein response". In: Cell 167.7 (2016), pp. 1867-1882.
[3] Raj Agrawal et al. "ABCD-strategy: Budgeted experimental design for targeted causal structure discovery". In: The 22nd International Conference on Artificial Intelligence and Statistics. 2019, pp. 3400-3409.
[4] Ali Ahmaditeshnizi, Saber Salehkaleybar, and Negar Kiyavash. "LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments". In: International Conference on Machine Learning. 2020.
[5] Steen A Andersson, David Madigan, Michael D Perlman, et al. "A characterization of Markov equivalence classes for acyclic digraphs". In: The Annals of Statistics 25.2 (1997), pp. 505-541.
[6] Gruia Calinescu et al. "Maximizing a monotone submodular function subject to a matroid constraint". In: SIAM Journal on Computing 40.6 (2011), pp. 1740-1766.
[7] Atray Dixit et al. "Perturb-Seq: dissecting molecular circuits with scalable single-cell RNA profiling of pooled genetic screens". In: Cell 167.7 (2016), pp. 1853-1866.
[8] F. Eberhardt and R. Scheines. "Interventions and causal inference". In: Philosophy of Science 74.5 (2007), pp. 981-995.
[9] Frederick Eberhardt, Clark Glymour, and Richard Scheines. "On the number of experiments sufficient and in the worst case necessary to identify all causal relations among N variables". In: Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence. 2005, pp. 178-184.
[10] Juan L Gamella and Christina Heinze-Deml. "Active Invariant Causal Prediction: Experiment Selection through Stability". In: arXiv preprint arXiv:2006.05690 (2020).</p>
<p>[11] Timothy S Gardner et al. "Inferring genetic networks and identifying compound mode of action via expression profiling". In: Science 301.5629 (2003), pp. 102-105.
[12] AmirEmad Ghassami, Saber Salehkaleybar, and Negar Kiyavash. "Interventional Experiment Design for Causal Structure Learning". In: arXiv preprint arXiv:1910.05651 (2019).
[13] AmirEmad Ghassami et al. "Budgeted experiment design for causal structure learning". In: International Conference on Machine Learning. 2018, pp. 1724-1733.
[14] Torsten Gross and Nils Blüthgen. "Identifiability and experimental design in perturbation studies". In: Bioinformatics 36.Supplement 1 (July 2020), pp. i482-i489. ISSN: 1367-4803. DOI: 10.1093/bioinformatics/btaa404.
[15] Hamed Hassani et al. "Stochastic conditional gradient++". In: arXiv preprint arXiv:1902.06992 (2019).
[16] Alain Hauser and Peter Bühlmann. "Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs". In: The Journal of Machine Learning Research 13.1 (2012), pp. 2409-2464.
[17] Yangbo He, Jinzhu Jia, and Bin Yu. "Counting and exploring sizes of Markov equivalence classes of directed acyclic graphs". In: The Journal of Machine Learning Research 16.1 (2015), pp. 2589-2609.
[18] Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. "Experiment selection for causal discovery". In: The Journal of Machine Learning Research 14.1 (2013), pp. 3041-3071.
[19] Seiya Imoto et al. "Analysis of gene networks for drug target discovery and validation". In: Target Discovery and Validation Reviews and Protocols. Springer, 2007, pp. 33-56.
[20] Richard M Karp. "Reducibility among combinatorial problems". In: Complexity of computer computations. Springer, 1972, pp. 85-103.
[21] Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. "Cost-Optimal Learning of Causal Graphs". In: International Conference on Machine Learning. 2017, pp. 1875-1884.
[22] Andreas Krause and Daniel Golovin. "Submodular Function Maximization". In: Tractability: Practical Approaches to Hard Problems. Cambridge University Press, 2014, pp. 71-104.
[23] Erik Lindgren et al. "Experimental design for cost-aware learning of causal graphs". In: Advances in Neural Information Processing Systems. 2018, pp. 5279-5289.
[24] Daniel Marbach et al. "Generating realistic in silico gene networks for performance assessment of reverse engineering methods". In: Journal of computational biology 16.2 (2009), pp. 229239.
[25] Florian Markowetz, Steffen Grossmann, and Rainer Spang. "Probabilistic soft interventions in conditional Gaussian networks". In: Tenth International Workshop on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics. 2005, pp. 214-221.
[26] Christopher Meek. "Casual Inference and Causal Explanation with Background Knowledge". In: Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann. 1995, pp. 403-410.
[27] Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. "Stochastic conditional gradient methods: From convex minimization to submodular maximization". In: Journal of Machine Learning Research 21.105 (2020), pp. 1-49.
[28] Joris M. Mooij, Sara Magliacane, and Tom Claassen. "Joint causal inference from multiple contexts". In: Journal of Machine Learning Research 21 (2020), pp. 1-108.
[29] Christos H Papadimitriou and Mihalis Yannakakis. "Optimization, approximation, and complexity classes". In: Journal of computer and system sciences 43.3 (1991), pp. 425-440.
[30] Judea Pearl. Causality. Cambridge university press, 2009.
[31] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017.
[32] Karthikeyan Shanmugam et al. "Learning causal graphs with small interventions". In: Advances in Neural Information Processing Systems 28 (2015), pp. 3195-3203.
[33] Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, prediction, and search. Lecture Notes in Statistics. New York: Springer, 1993. DOI: 10.1007/978-1-4612-2748-9.
[34] Chandler Squires, Yuhao Wang, and Caroline Uhler. "Permutation-based causal structure learning with unknown intervention targets". In: The Thirty-Sixth Conference on Uncertainty in Artificial Intelligence. 2020.</p>
<p>[35] Thomas Verma and Judea Pearl. "Equivalence and synthesis of causal models". In: Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence. Elsevierence Inc. 1990, pp. 255-270.
[36] Yuhao Wang et al. "Permutation-based causal inference algorithms with interventions". In: Advances in Neural Information Processing Systems. 2017.
[37] Ingo Wegener. "On separating systems whose elements are sets of at most k elements". In: Discrete Mathematics 28.2 (1979), pp. 219-222.
[38] Dominic JA Welsh and Martin B Powell. "An upper bound for the chromatic number of a graph and its application to timetabling problems". In: The Computer Journal 10.1 (1967), pp. 85-86.
[39] Marcel Wienöbst, Max Bannach, and Maciej Liśkiewicz. Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs. 2020. arXiv: 2012.09679 [cs.LG].
[40] Karren Yang, Abigail Katcoff, and Caroline Uhler. "Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions". In: International Conference on Machine Learning. 2018.</p>
<h1>Appendix</h1>
<h2>A. 1 Potential Negative Societal Impacts</h2>
<p>We propose algorithms for experimental design when learning causal structures. The most obvious application is in designing scientific experiments to learn gene regulatory networks. It is possible that the ability to gather increasingly detailed information about gene networks could be used by malicious actors. However, to the authors' knowledge there are no examples of such use with existing related methods. On the other hand, there is longstanding scientific interest in learning such networks and potentially beneficial applications in drug design [19].</p>
<h2>A. 2 The Power of Multi-Perturbation Interventions</h2>
<p>We can construct a more general example than the one given in Figure 1, to demonstrate that the optimal $q$-node intervention can learn up to $q$-times more edges than the optimal single node intervention. Consider a graph with MEC that is a forest of undirected star graphs, each with an equal number of nodes. The optimal single-node intervention can intervene on the center node in one of these stars, and entirely orient that star. The optimal $q$-node intervention can achieve this for $q$ of the stars.</p>
<p>This example illustrates the upper bound on the number of additional edges that can be oriented by the optimal $q$-node intervention compared to the optimal single node intervention. It can be seen that this is a tight upper bound from the submodularity of $F_{\mathrm{EO}}^{\xi}$ in lemma 2.</p>
<h2>A. 3 Deriving $F_{\infty}$</h2>
<p>Agrawal et al. [3] derive this infinite sample objective for the case of having gathered only infinite observational data before doing experiments. For the case of having infinite samples from observational data and some interventions, we follow a similar argument.
For evaluating $U_{M, I}(y, \xi, D)=H(G \mid D)-H(G \mid D, y, \xi)$ where $H$ is the entropy, we only consider the second term. The first term does not depend on the interventions we do so is irrelevant for optimizing the objective.
Given $D$ consisting of existing interventions $\xi^{\prime}$ and observational data, the true DAG is already recovered up to it's $\xi^{\prime}$-MEC. After obtaining infinite samples from each intervention in intervention set $\xi$, we recover the true DAG up to its $\xi^{\prime} \cup \xi$-MEC. Therefore $H(G \mid D, y, \xi)=\log _{2}\left(\left|E s s^{\xi \cup \xi^{\prime}}(G)\right|\right)$ when the true DAG is $G . E s s^{\xi \cup \xi^{\prime}}(G)$ is the essential graph obtained after interventions $\xi$. Our prior distribution over DAGs is uniform over the $\xi^{\prime}$-MEC of the true DAG. Averaging over these possibilities, the final objective is given by Objective 2.</p>
<h2>A. 4 Background on the Meek Rules</h2>
<p>After performing interventions, the Meek rules can be used to orient additional edges. The rules are given in Figure 3. The Meek rules are continually applied until none of the left side patterns appear in the partially directed graph.</p>
<h2>A. $5 F_{\text {EO }}$ is consistent</h2>
<p>We use the same definition of budgeted batch consistency introduced in Agrawal et al. [3].
Definition 4 (Agrawal et al. [3]). Assume our goal is to identify the true DAG $G^{*}$. Let us have constraints for the $b$ th batch $(0 \leq b \leq B)$ of experiments $C_{m, q}^{b}(q \geq 1)$. Objective $F$ is budgeted batch consistent if maximizing it in every batch implies</p>
<p>$$
\mathbb{P}\left(G \mid D_{B}\right) \rightarrow \mathbb{1}\left(G=G^{*}\right)
$$</p>
<p>asymptotically as $m, B \rightarrow \infty$, where $D_{B}$ is the combined data obtained from all batches of experiments and the original dataset.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The 4 Meek rules. When a pattern on the left of the implication occurs, edges are oriented according to the pattern on the right of the implication. A dashed line means the direction of the edge may or may not be already identified. The name of each Meek rule is given above the implication sign. R0 refers to the step of orienting edges before Meek rules are applied.</p>
<p>We show that our definition of $F_{\mathrm{EO}}$ satisfies budgeted batch consistency. Then we explain how the slight difference in the definition of $F_{\mathrm{EO}}$ given in Agrawal et al. [3] leads to the authors concluding that it is not budgeted batch consistent.
Proposition 2. $F_{\text {EO }}$ is budgeted batch consistent.</p>
<p>Proof. Since each intervention offers an infinite number of samples, we can reason directly about the subsequent orienting of edges due to the obtained samples. Let the set of interventions after $b$ batches be $\xi_{b}$. We simply need to prove that as $k \rightarrow \infty, \xi_{b}$ will identify the orientation of every edge in the true DAG. Equivalently, we show that if $\xi_{b}$ has not oriented every edge (the current essential graph has size greater than 1), $\xi_{b+1}$ will orient additional edges in the true DAG. There are only a finite number of edges to be oriented, so this means $\xi_{\infty}$ will fully identify the true DAG.
If $\xi_{b}$ does not orient every edge in the true DAG, Objective 3 will have a maximum in the next batch of greater than 0 . Moreover, to obtain objective greater than 0 , the interventions selected in batch $b+1$ must orient edges not oriented by $\xi_{b}$. This is because after each batch, the objective is updated to be with reference to the essential graph of $G$ under interventions in $\xi_{b}$. To orient unidentified edges in any of the possible DAGs (those in the current $\xi_{b}-\mathrm{MEC}$ ), we must orient at least one edge in the true DAG after obtaining samples. This can be seen from the first step of computing function $R$. One can also see that if an edge can be oriented, it can always be oriented by selecting a single unique intervention of size 1 and thus the constraints $C_{m, q}$ can always be satisfied.</p>
<p>The key difference between this argument and the one given by Agrawal et al. [3] is that they maintain a static objective function between batches, so the same set of interventions is selected every round.</p>
<h1>A. 6 Proof of Lemma 1</h1>
<p>We'll work with the notation of $\xi_{1} \subset \xi_{2}$ are sets of interventions, and $I$ is an intervention. We'll write $R(\xi, G)=M(A(\xi, G), G)$. Here, $A$ carries out the first step of orienting edges based on one of the nodes in that edge being intervened on (we refer to this as R0). $M$ carries out the Meek rules given the orientations of edges in $A$. Note that $R, M, A$ implicitly depend on the interventions carried out in previous batches $\xi^{\prime}$, since this determines what edges might already be oriented in $G$ (whether we are orienting the MEC or some essential graph).
We'll first show that function $R$ has a monotonicity-like property: adding an intervention only adds to the set of oriented edges.
Proposition 3. Monotonicity-like property of $R: R\left(\xi_{1}, G\right) \subseteq R\left(\xi_{2}, G\right)$ for all $G, \xi_{1} \subseteq \xi_{2}$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The pattern when an edge $v_{a} \rightarrow v_{b}$ is identified by R4 and $v_{a}-v_{c}, v_{b}-v_{c}$</p>
<p>Proof. The same argument is given in Ghassami et al. [13]. By the definition of $A, A\left(\xi_{1}, G\right) \subseteq$ $A\left(\xi_{2}, G\right)$. The Meek rules are sound and order-independent [26], and therefore $M\left(A\left(\xi_{1}, G\right)\right) \subseteq$ $M\left(A\left(\xi_{2}, G\right)\right)$</p>
<p>From this we can see that $F_{\mathrm{EO}}$ is also monotonic.
To prove lemma 1, we swill first prove some propositions regarding the marginal change in $R$ when adding a new intervention.
Proposition 4. Consider vertices $v_{a}, v_{b}, v_{c}$. Consider $\bar{G}$, the partially directed graph obtained after doing intervention set $\xi$ and then applying the Meek rules exhaustively. If $v_{a} \rightarrow v_{b} \in \bar{G}$, and $v_{b}-v_{c} \in \bar{G}$, then $v_{a} \rightarrow v_{c} \in \bar{G}$.</p>
<p>Proof. If $v_{a} \rightarrow v_{b}$, and $v_{b}-v_{c}$, we must have that $v_{a}$ and $v_{c}$ are adjacent, else $v_{b} \rightarrow v_{c}$ by R1. We cannot have $v_{c} \rightarrow v_{a}$ since this would identify $v_{b}-v_{c}$ by R2. Hence we have either that $v_{a}-v_{c}$ or $v_{a} \rightarrow v_{c}$.
Suppose for contradiction that, after applying all Meek rules, for some nodes $v_{a}, v_{b}, v_{c}$ we have $v_{a} \rightarrow v_{b}, v_{b}-v_{c}$ and $v_{a}-v_{c}$. We will gain a contradiction by an infinite descent. Any DAG can be associated with some permutation of its nodes that specifies a topological ordering, with the lowest ranked node being the root. Suppose that $B$ is the lowest ranked node in the topological ordering given by the true DAG (closest to the root) such that the supposed pattern holds. For all ways in which $v_{a} \rightarrow v_{b}$ could have been identified, we will show that either in fact $v_{a} \rightarrow v_{c}$ or find a vertex lower than $v_{b}$ in the topological ordering that fits into an identical pattern.
Some cases are covered in Meek [26] when proving a similar result (lemma 1).
Suppose $v_{a} \rightarrow v_{b}$ is known by being a collider (identified before any interventions take place). This is handled in Meek [26].
Suppose $v_{a} \rightarrow v_{b}$ is known by R1, R2, R3. These cases are all handled by Meek [26].
Suppose $v_{a} \rightarrow v_{b}$ is learnt directly by an intervention (rule R0). If it is identfied in this way, there must exist an intervention $I$ such that exactly one of $v_{a}, v_{b} \in I$. However, in either case regardless of whether $v_{c} \in I$ or not, we identify one of $v_{a}-v_{c}$ or $v_{b}-v_{c}$ by R0. Hence the pattern cannot occur if $v_{a} \rightarrow v_{b}$ is identified by R0.
Suppose $v_{a} \rightarrow v_{b}$ is learnt directly by R4 as in Figure 4. Consider the pattern given in Figure 4 where $v_{e}$ and $v_{d}$ are the other edges in the R4 pattern. Now if $v_{a}-v_{d}$ is undirected, then $A E D$ is the same pattern as $v_{a}, v_{b}, v_{c}$ but with $v_{d}&lt;v_{b}$ in the topological ordering (a contradiction). If $v_{a} \rightarrow v_{d}$, then $v_{a}, v_{d}, v_{b}$ gives the same setup as if we discovered $v_{a} \rightarrow v_{b}$ through R2. Similarly, if $v_{d} \rightarrow v_{a}$, then $v_{e} \rightarrow v_{a}$ by R2 and then we have the same setup as if we discovered $v_{a} \rightarrow v_{b}$ by R1. An alternative R4 pattern can also orient $v_{a} \rightarrow v_{b}$, however it involves node $v_{c}$ being part of the pattern. In this case, we must have oriented $v_{c} \rightarrow v_{b}$, a contradiction.</p>
<p>Proposition 4 allows us to prove two propositions more directly related to our final result.
Proposition 5. $R\left(\xi_{2} \cup{I}\right) \backslash R\left(\xi_{2}\right) \subseteq R\left(\xi_{1} \cup{I}\right) \backslash R\left(\xi_{1}\right)$</p>
<p>Proof. Here we'll take $R$ to also include all edges oriented before the interventions, since this doesn't change the outcome of the set difference operation above. We also drop $G$ from the notation since we work with a fixed true graph. This is just done out of convenience for the proof.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The Meek rules with labels on edges and nodes. a) R1, b) R2, c) R3, d) R4.</p>
<p>Take edge $e \in R\left(\xi_{2} \cup{I}\right)$ and $e \notin R\left(\xi_{2}\right)$. By the monotonicity-like property, $R\left(\xi_{1}\right) \subseteq R\left(\xi_{2}\right)$, so $e \notin R\left(\xi_{1}\right)$. Thus we just need to prove that for all such $e$, we have $e \in R\left(\xi_{1} \cup{I}\right)$.</p>
<p>Assume for contradiction there is some nonempty set $E^{\dagger}$ of edges such that $\forall e \in E^{\dagger}, e \in R\left(\xi_{2} \cup{I}\right)$ and $e \notin R\left(\xi_{2}\right)$, but $e \notin R\left(\xi_{1} \cup{I}\right)$. We can specify an ordering over these edges. Order the edges (written $v_{1} \rightarrow v_{2}$ ) such that: the edges are in increasing order upon the position of $v_{2}$ in the topological ordering of graph $G$ (lower is closer to the root). Settle all ties by decreasing order on the position of $v_{1}$ in the topological ordering. We will now show that if there exists some $e$ that is the lowest ordered element in $E^{\dagger}$, we can always either create a contradiction or find some alternative edge in $E^{\dagger}$ with lower position in the ordering (itself a contradiction).
If $e=v_{1} \rightarrow v_{2}$ is discovered in $R\left(\xi_{2} \cup{I}\right)$ by R 0 , we know $I$ must intervene on one of $v_{1}$, or $v_{2}$ but not the other. Hence $e \in R\left(\xi_{1} \cup{I}\right)$.
The diagram in Figure A. 6 is given for following the other cases. Notation refers to the names of nodes and edges given on the diagrams.
If $e=v_{1} \rightarrow v_{2}$ is discovered in $R\left(\xi_{2} \cup{I}\right)$ by R1, we must have $e_{1} \in R\left(\xi_{2} \cup{I}\right)$. We must also have that $e_{1} \notin R\left(\xi_{2}\right)$ else $e \in R\left(\xi_{2}\right)$ by R1. The same must be true of $e^{\prime}$ not being in $R\left(\xi_{1} \cup{I}\right)$, so $e_{1} \in E^{\dagger}$. But $e_{1}$ will have lower ordering since $v_{1}$ is below $v_{2}$ in the topological ordering.
If $e=v_{1} \rightarrow v_{2}$ is discovered in $R\left(\xi_{2} \cup{I}\right)$ by R2, we must have some pair $e_{1}, e_{2} \in R\left(\xi_{2} \cup{I}\right)$ such that $e_{1}=v_{3} \rightarrow v_{2}$ and $e_{2}=v_{1} \rightarrow v_{3}$. One of these edges cannot be in $R\left(\xi_{2}\right)$. In fact, neither can be in $R\left(\xi_{2}\right)$ due to proposition 4 . This is because assuming only one is identified, based on proposition 4 , another edge is either identified which leads to orienting $e$ or another edge is incorrectly oriented in the true DAG. The same holds for $R\left(\xi_{1} \cup{I}\right)$, but then we've found an edge $e_{2} \in E^{\dagger}$ that is lower in the ordering than $e$.
If $e=v_{1} \rightarrow v_{2}$ is discovered in $R\left(\xi_{2} \cup{I}\right)$ by R3, we must have $e_{1}, e_{2} \in R\left(\xi_{2} \cup{I}\right)$ and $e_{3}, e_{4} \in G$ but not necessarily in $R\left(\xi_{2} \cup{I}\right)$. $e_{1}$ and $e_{2}$ form an unshielded collider and are identified before intervening, so $e \in R\left(\xi_{1} \cup{I}\right)$ by R3.
If $e=v_{1} \rightarrow v_{2}$ is discovered in $R\left(\xi_{2} \cup{I}\right)$ by R4, we must have $e_{1}, e_{2} \in R\left(\xi_{2} \cup{I}\right)$ and $e_{3}, e_{4} \in G$ but not necessarily in $R\left(\xi_{2} \cup{I}\right)$. At least one of $e_{1}, e_{2} \notin R\left(\xi_{2}\right)$. Suppose only $e_{1}$ is in, then $e_{2}$ is in by $R 1$. Therefore $e_{1}$ is not in $R\left(\xi_{2}\right)$ or $R\left(\xi_{1} \cup{I}\right)$, but this is lower in the ordering than $e$.</p>
<p>Proposition 6. $R\left(\xi_{1}\right) \backslash R\left(\xi_{1} \cup{I}\right) \subseteq R\left(\xi_{2}\right) \backslash R\left(\xi_{2} \cup{I}\right)$
Proof. Follows by monotonicity of $R$, both sides are the empty set.</p>
<p>We can rewrite $F_{E O}=\sum_{G \in \mathcal{G}} g(R(\xi, G))$ where $g$ is the weighted coverage function. $g$ is a monotonic function of the set of edges in $R$.</p>
<p>$$
\begin{aligned}
g\left(R\left(\xi_{1} \cup{I}\right)\right)-g\left(R\left(\xi_{1}\right)\right) &amp; \stackrel{(i)}{=} g\left(R\left(\xi_{1} \cup{I}\right) \backslash R\left(\xi_{1}\right)\right) \
&amp; -g\left(R\left(\xi_{1}\right) \backslash R\left(\xi_{1} \cup{I}\right)\right) \
&amp; \stackrel{(i)}{=} g\left(R\left(\xi_{2} \cup{I}\right) \backslash R\left(\xi_{2}\right)\right) \
&amp; -g\left(R\left(\xi_{2}\right) \backslash R\left(\xi_{2} \cup{I}\right)\right) \
&amp; =g\left(R\left(\xi_{2} \cup{I}\right)\right)-g\left(R\left(\xi_{2}\right)\right)
\end{aligned}
$$</p>
<p>Step (i) is a property of the weighted coverage function. Step (ii) comes from propositions 5 and 6 and the monotonicity of $g$. This shows that definition 2 holds for $g(R(\xi, G))$ as a function of $\xi$ for all $G$. Since the sum of submodular functions is submodular, this implies lemma 1.</p>
<h1>A. 7 Proof of Lemma 2</h1>
<p>We can see that a monotonicity property like proposition 3 does not hold in this case. The intervention $[p]$, for example, orients no edges. Nevertheless, we follow the same approach to prove submodularity of $F_{\mathrm{EO}}^{\xi}$.
For this we consider $I_{1} \subset I_{2}$ and consider adding node $v \notin I_{2}$ to these interventions.
For notational simplicity, $R(I, G)$ will now denote all edges oriented after intervention set $\xi$ (fixed) and intervention $I$ on true graph $G$. We will drop $G$ from the notation in cases where the graph is fixed.
Proposition 7. $R\left(I_{2} \cup{v}\right) \backslash R\left(I_{2}\right) \subseteq R\left(I_{1} \cup{v}\right) \backslash R\left(I_{1}\right)$
Proof. We want to show two things. First we want to show that if $e \in R\left(I_{2} \cup{v}\right)$ and $e \notin R\left(I_{2}\right)$, then $e \in R\left(I_{1} \cup{v}\right)$. This is shown with an identical technique to the one in proposition 5 . This is because the Meek rules are the same in both cases. The only difference is the case when $e=v_{1} \rightarrow v_{2}$ is discovered by R0. In this case, $v$ must be either $v_{1}$ or $v_{2}$ and neither of $v_{1}$ or $v_{2}$ can be in $I_{2}$. However therefore neither are in $I_{1}$ and hence $e \in R\left(I_{1} \cup{v}\right)$.
The second thing we need to show is that if $e \in R\left(I_{2} \cup{v}\right)$ and $e \notin R\left(I_{2}\right)$, then $e \notin R\left(I_{1}\right)$. Suppose for contradiction that there exists some such $e \in R\left(I_{1}\right)$. We'll proceed in two steps. First we'll show that in order to avoid a contradiction, we must have that $I_{2}$ intervenes on both vertices in $e$. Second we'll show that if we intervene on both vertices in $e$ for $I_{2}$, we cannot have that $e \in R\left(I_{2} \cup{v}\right)$ and $e \notin R\left(I_{2}\right)$.
We can represent the identification of edge $e$ in $R\left(I_{1}\right)$ by a directed tree diagram. The root in the diagram is $e$, and the children of each node in the diagram are the directed edges involved in the Meek rule that identifies the parent edge. Each node can have either one or two children (since each Meek Rule depends on up to two specific edges being directed). Leaf nodes must have been identified by R0. Since $e \in R\left(I_{1}\right)$ and $e \notin R\left(I_{2}\right)$, there must be leaf nodes in the diagram that are not identified by R0 using intervention $I_{2}$. Since $I_{1} \subset I_{2}$, this means that for these leaf edges, $I_{2}$ must contain both nodes. However we now show that, for the structure of all Meek rules, in our tree diagram if a child edge has both vertices intervened on then the parent is identified anyway, unless both vertices of the parent are intervened on. This is shown pictorially in Figure 6. R4 case 1 requires some extra explanation. $e_{1}$ is identified and must be oriented towards the node in $e$ to prevent identifying $e$ by R2. Hence $e_{2}$ must direct into $e$ to prevent a cycle. Then $e$ is learnt by R1. Given all this, we can see that to prevent identification of $e$ by $I_{2}$, there must be a path from a leaf edge to the root $e$ where all the edges in the path have both vertices in the edge intervened on in $I_{2}$. Hence $e$ has both vertices contained in $I_{2}$.
Now consider a different tree diagram for how $e$ is oriented in $R\left(I_{2} \cup{v}\right)$. Clearly $v$ must be included as a vertex in at least one of the leaf edges, else the same pattern could allow us to orient $e$ using $I_{2}$. We show that if both vertices of an edge are intervened on, to prevent the identification of both child edges and hence the edge itself by the Meek rules, it must be that both children have both of their vertices intervened on in $I_{2}$. This is shown in Figure 7.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The pattern of each Meek rule as in the first part of proposition 7. An $X$ denotes that an intervention occured at that node. In our tree representation, if a child edge has both vertices intervened on then the parent is identified anyway, unless both vertices of the parent are intervened on.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The pattern of each Meek rule as in the second part of the proof of proposition 7. An $X$ denotes that an intervention occured at that node. In our tree diagrams, if both vertices of an edge are intervened on, to prevent the identification of both child edges (and hence the edge itself by the corresponding Meek rule), it must be that both children have both of their vertices intervened on.</p>
<p>In Figure 7, R1, R2 and R3 are clear. For R4 case 1, $e_{2}$ must orient right to left (or get $e$ by R1). Also, $e_{1}$ must go towards the top right or we get $e$ by R2. Thus $e_{3}$ must point upwards by R2 which is a contradiction, since we know in the R4 pattern $e_{3}$ points downwards. Intervening on more edges to avoid this leads us to cases 2 and 3. For R4 case 2, we learn $e_{3}$ and $e_{4}$ and hence orient $e$. For R4 case 3, $e_{2}$ must point left else we get $e$ by R1. We know $e_{3}$ points down and it is oriented by R0. Then, $e_{4}$ goes left to right by R1. $E_{1}$ then points towards the bottom left by $R_{2}$ and hence $e$ is oriented by R2.</p>
<p>Hence, there must be some path from the root $e$ to a leaf edge such that all members of the path have both of their vertices intervened on in $I_{2}$ and hence $I_{2} \cup{v}$. If $v \in e$, we have a contradiction since $v \notin I_{2}$. If $v \notin e$, then the leaf edge in our tree representation containing $v$ is not identified using R0 with $I_{2}+v$ either. Hence, this tree cannot possibly represent the sequence of Meek rules that lead to orienting $e$. Thus if $e \in R\left(I_{2} \cup{v}\right)$ and $e \notin R\left(I_{2}\right)$, then $e \notin R\left(I_{1}\right)$.</p>
<p>Proposition 8. $R\left(I_{1}\right) \backslash R\left(I_{1} \cup{v}\right) \subseteq R\left(I_{2}\right) \backslash R\left(I_{2} \cup{v}\right)$</p>
<p>Proof. As in the proof of lemma 1, we can write $R(I)=M(A(I))$, where $A$ returns edges oriented directly by the intervention and $M$ returns these in addition to any oriented due to Meek rules.
By symmetry in the definition of $A$, we can see that $A\left(I^{C}\right)=A(I)$ and hence $R\left(I^{C}\right)=R(I)$. Take some edge $e \in R\left(I_{1}\right) \backslash R\left(I_{1} \cup{v}\right)$. Then by symmetry we have $e \in R\left(I_{1}^{C}\right) \backslash R\left(I_{1}^{C} \backslash{v}\right)$. Then since $I_{1}^{C} \backslash{v} \supseteq I_{2}-{v}$, by proposition 7 we must have $e \in R\left(I_{2}^{C}\right) \backslash R\left(I_{2}^{C} \backslash{v}\right)$. Again by symmetry we then have $e \in R\left(I_{2}\right) \backslash R\left(I_{2} \cup{v}\right)$.</p>
<p>We can conclude that $F_{\mathrm{EO}}^{2}$ is submodular in an identical way to how we did in proving lemma 1: by combining propositions 7 and 8 .</p>
<h1>A. 8 Proof of Theorem 2</h1>
<p>At each iteration of selecting an intervention, Theorem 1 lower bounds how close the marginal gain compared to the greedy intervention is. Let $\xi$ be the set of interventions DGC selects. Let $\xi^{<em>}$ be the optimal batch of interventions, and $I^{</em>}{ }<em i="i">{i}$ be the ith intervention in this set. Due to lemma 1 (monotonicity), $\xi^{*}$ contains exactly $m$ interventions. $\left{\xi</em>\right}<em _mathrm_EO="\mathrm{EO">{i \geq 0}$ is the entire intervention set after each greedy selection. Define marginal improvement $\Delta(I \mid \xi)=F</em>(\xi)$. The following holds for all $i$ :}}(I \cup \xi)-F_{\mathrm{EO}</p>
<p>$$
\begin{aligned}
F_{\mathrm{EO}}\left(\xi^{<em>}\right) &amp; \leq F_{\mathrm{EO}}\left(\xi^{</em>} \cup \xi_{i}\right) \
&amp; \leq F_{\mathrm{EO}}\left(\xi_{i}\right)+\sum_{j=1}^{k} \Delta\left(I^{<em>}{ }<em i="i">{j} \mid \xi</em> \cup\left{I^{</em>}{ }<em i="i">{1}, \ldots, I^{<em>}{ }<em _mathrm_EO="\mathrm{EO">{j-1}\right}\right) \
&amp; \leq F</em>\right)+\sum_{I \in \xi^{}}\left(\xi_{i</em>}} \Delta\left(I \mid \xi</em>\right) \
&amp; \leq F_{\mathrm{EO}}\left(\xi_{i}\right)+\sum_{I \in \xi^{*}} e \mathbb{E}\left[F_{\mathrm{EO}}\left(\xi_{i+1}\right)-F_{\mathrm{EO}}\left(\xi_{i}\right)\right]+e \epsilon \
&amp; \leq F_{\mathrm{EO}}\left(\xi_{i}\right)+e m \mathbb{E}\left[F_{\mathrm{EO}}\left(\xi_{i+1}\right)-F_{\mathrm{EO}}\left(\xi_{i}\right)\right]+e m \epsilon
\end{aligned}
$$</p>
<p>The first line is due to monotonicity. The second is a telescoping sum. The third is due to submodularity (lemma 1). The fourth is a result of Theorem 1, since what we write is lower bounded by the greedy choice, which by definition has greater marginal improvement than any other intervention. The expectation here is over noise in selecting the $(i+1)$ th intervention. The final line just notes that there are $m$ elements in the sum.
Now define $\delta_{i}=F_{\mathrm{EO}}\left(\xi^{*}\right)-F_{\mathrm{EO}}\left(\xi_{i}\right)$. We rearrange the above to get</p>
<p>$$
\delta_{i} \leq e m\left(\delta_{i}-\mathbb{E}\left[\delta_{i+1}\right]+\epsilon\right)
$$</p>
<p>where again the expectation is over selection of the $(i+1)$ th intervention.
Now we telescope this inequality, subbing in $i+1=m$ to obtain our final result.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The 3 DAGS in $\tilde{\mathcal{G}}$ for the counterexample in proving Proposition 1.</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\delta_{m}\right] &amp; \leq\left(1-\frac{1}{e m}\right) \delta_{m-1}+\epsilon \
&amp; \leq\left(1-\frac{1}{e m}\right)^{m} \delta_{0}+\sum_{j=0}^{m}\left(1-\frac{1}{e m}\right)^{j} \epsilon \
&amp; \leq e^{-1 / \epsilon} \delta_{0}+\sum_{j=0}^{m} e^{-\frac{j}{e m}} \epsilon
\end{aligned}
$$</p>
<p>The third line uses $1-x \leq e^{-x}$. The final term on the right hand side of the last line reduces to $\sigma \epsilon$, where</p>
<p>$$
\sigma=\frac{e^{-1 / e}-e^{1 /(e m)}}{1-e^{1 /(e m)}}
$$</p>
<p>By noting that $\delta_{0}=F_{\mathrm{EO}}\left(\xi^{*}\right)$, we get</p>
<p>$$
\mathbb{E}\left[F_{\mathrm{EO}}(\xi)\right] \geq\left(1-e^{-\frac{1}{\epsilon}}\right) F_{\mathrm{EO}}\left(\xi^{*}\right)-\epsilon \sigma
$$</p>
<p>Given Theorem 1 and that we select $m$ greedy interventions, this requires $\mathcal{O}\left(m p^{5 / 2} / \epsilon^{3}\right)$ calls to $R$. We remove the dependence on $m$ in $\sigma$ into the runtime by noting that $\sigma=\mathcal{O}(m)$ and hence that if we do $\mathcal{O}\left(m^{4} p^{5 / 2} / \epsilon^{3}\right)$ calls to the Meek rules, we get</p>
<p>$$
\mathbb{E}\left[F_{\mathrm{EO}}(\xi)\right] \geq\left(1-e^{-\frac{1}{\epsilon}}\right) F_{\mathrm{EO}}\left(\xi^{*}\right)-\epsilon
$$</p>
<h1>A. 9 Proof of Lemma 3</h1>
<p>Agrawal et al. [3] prove monotone submodularity of $F_{\mathrm{MI}}$. $\tilde{F}_{\infty}$ is the special case of this objective in the limit of infinite samples per intervention.</p>
<h2>A. 10 Proof of Proposition 1</h2>
<p>To show that $\tilde{F}<em 2="2">{\infty}^{\xi}$ is in general not submodular, we need to give a specific example of a constraint set, and set of DAGs $\tilde{\mathcal{G}}$. We let $m=1, k=4$. The set $\tilde{\mathcal{G}}$ is given in Figure 8. We consider the set of interventions before carrying out experiments, $\xi^{\prime}$ to be the empty set. To break the definition of submodularity as given in definition 2 we need to define an existing intervention to add, $I</em>=[1,2]$. We also need to choose a perturbation to add to each intervention, and we choose node 0 . The computation of the objective is carried out in proposition.py in the accompanying code.}$, and a subset of this, $I_{1}$. For this example we choose $I_{2}=[1,2,3]$ with nodes numbered as in Figure 8. $I_{1</p>
<p>Note that for the special case $\tilde{\mathcal{G}}=\mathcal{G}$, we have not constructed a counterexample. In fact, in this case we suspect that $\tilde{F}_{\infty}^{k}$ is submodular, but don't have a proof. If true, this may suggest that an algorithm similar to DGC could be used to maximize Objective 2 directly without approximating the MEC with a bag of DAGs. An additional difficulty for this objective, however, is the second potentially exponential sum required to compute essential graph sizes embedded within the logarithm.</p>
<h1>A. 11 Proof of Theorem 3</h1>
<p>We know that $\tilde{F}<em _infty="\infty">{\infty}(\mathcal{S})=0$, because the graph is then fully identified, meaning $\mid \operatorname{Ess}^{\xi \cup \xi^{\prime}}(G) \mid$ is 1 for all $G$. We also know that $\min \left(\tilde{F}</em>}\right)=\tilde{F<em _infty="\infty">{\infty}(\emptyset)$. The submodularity of $\tilde{F}</em>$ (lemma 3) along with this boundedness of the function is used to get the final bound.}$ over groundset $\mathcal{S} \subset \mathcal{I</p>
<p>Say we greedily select $m$ members of $\mathcal{S}$ to construct $\xi_{m}$. We prove by induction that $Q(m)=\left(\tilde{F}<em m="m">{\infty}\left(\xi</em>(\emptyset)\right)$ is true for all $m$ where $0 \leq m \leq|S|$.}\right) \geq\left(1-\frac{m}{|\mathcal{S}|}\right) \tilde{F}_{\infty</p>
<p>The base case $Q(0)$ is trivial. Now assume that $Q(m)$ is true. Since $\tilde{F}<em m="m">{\infty}$ is submodular over groundset $\mathcal{S}$, it satisfies the diminishing returns property of definition 2 . Therefore it must be the case that $\exists I \in \mathcal{S} \backslash \xi</em>}$ such that $\tilde{F<em m="m">{\infty}\left({I} \cup \xi</em>}\right) \geq \frac{1}{|\mathcal{S}|-m} \tilde{F<em m="m">{\infty}\left(\xi</em>)&lt;0$ which would be a contradiction. Therefore}\right)$. Note that if this was not the case, because of submodularity, adding all of the remaining interventions in $S$ in sequence would give $\tilde{F}_{\infty}(\mathcal{S</p>
<p>$$
\begin{aligned}
\tilde{F}<em m_1="m+1">{\infty}\left(\xi</em>}\right) &amp; \geq \frac{1}{|\mathcal{S}|-m} \tilde{F<em m="m">{\infty}\left(\xi</em>\right) \
&amp; \geq \frac{1}{|\mathcal{S}|-m}\left(1-\frac{m}{|\mathcal{S}|}\right) \tilde{F}<em _infty="\infty">{\infty}(\emptyset) \
&amp; =\left(1-\frac{m+1}{|\mathcal{S}|}\right) \tilde{F}</em>(\emptyset)
\end{aligned}
$$</p>
<p>which completes the induction. The final result follows by applying the lower-bound on $|\mathcal{S}|$ given in Shanmugam et al. [32]. A bound for the graph-sensitive separating system in Lindgren et al. [23] can also be obtained by plugging in their lower-bound on $|\mathcal{S}|$.
The runtime can be seen by observing that for each intervention, we compare $|\mathcal{S}|=\mathcal{O}\left(\frac{p}{2} \log p\right)$ possible interventions. For each we evaluate $R$ a total of $\mathcal{O}(|\tilde{\mathcal{G}}|)$ times to compute $\tilde{F}<em _infty="\infty">{\infty}$. Computing $R$ for each $G \in \mathcal{G}$ is sufficient to compute $\tilde{F}</em> \log p)$ evaluations of $R$. The construction of the separating system itself is efficient compared to the computation of the Meek rules required to evaluate $R$ [32].}$ because $R$ outputs all of the oriented edges in a graph given an intervention and hence can determine if a graph is in a certain interventional MEC. Thus, the overall runtime is $\mathcal{O}(m|\tilde{\mathcal{G}}| \frac{p}{2</p>
<h2>A. 12 Extension to Soft Interventions</h2>
<p>Our results can also be used to develop algorithms for the soft intervention setting. A hard intervention makes the value of a variable independent of its parents. However, a soft intervention modifies a variable's value whilst maintaining the dependence on its parents. A simple example of a soft intervention is adding a constant value to the intervened node. In a GRN, a soft intervention might correspond to a gene knockdown, where a gene's expression is reduced but not set to 0 .
Definition 5 (Soft intervention). A soft intervention $I$ on nodes $X_{T}$ for all $i \in I$, adds the intervention variables $W_{i}$ as an extra direct cause of $X_{i}$.</p>
<p>The soft intervention setting is greatly simplified by the following result in Ghassami, Salehkaleybar, and Kiyavash [12].
Lemma 4 (Ghassami, Salehkaleybar, and Kiyavash [12]). In the infinite sample setting, a set of $k$ soft interventions of size 1 is equivalent to targeting the same $k$ nodes in a single soft intervention. By equivalent, we mean it identifies the same information regarding the true DAG.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5257 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5257</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5257</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267626823</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07630v3.pdf" target="_blank">G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\footnote{Our codes and datasets are available at: \url{https://github.com/XiaoxinHe/G-Retriever}}</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5257.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5257.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textualized Graph (flattening)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Textualized Graph via Flattening Nodes and Edges into Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A straightforward graph-to-text conversion that flattens node and edge textual attributes into a natural-language / CSV-like sequence (node_id,node_attr and src,edge_attr,dst) for input to LLMs; used as the textual prompt/context for generation and prompt-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Textualized Graph (flattening / CSV-like serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph to a list of textual lines representing nodes and edges. Node lines: "node_id, node_attr" (attributes include textual descriptions and object metadata); edge lines: "src, edge_attr, dst". In generation the textualized subgraph textualize(S*) is concatenated with the query and fed to the text embedder (the LLM's first layer). This paper implements textualization as part of the generation stage and provides concrete CSV-like examples in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General textual graphs (scene graphs, commonsense explanation graphs, knowledge graphs / Freebase subset)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Faithful to original textual attributes and interpretable (human-readable CSV-like form); simple and general-purpose; straightforward to embed with existing text embedders. Major downside: token-inefficient — whole-graph textualizations can exceed LLM context windows and require truncation, causing information loss; interacts naturally with retrieval but scales poorly without subgraph selection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>GraphQA benchmark tasks (ExplaGraphs: generative commonsense reasoning; SceneGraphs: scene-graph question answering; WebQSP: multi-hop knowledge graph QA). Used in inference-only, prompt-tuning, and fine-tuning configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported downstream metrics in this paper include accuracy (ExplaGraphs, SceneGraphs) and Hit@1 (WebQSP). With the full G-Retriever pipeline (which includes textualized subgraphs): ExplaGraphs 0.8516 ± 0.0092 (accuracy), SceneGraphs 0.8131 ± 0.0162 (accuracy), WebQSP Hit@1 70.49 ± 1.21. Ablation removing the textualized graph (w/o Textualized Graph) yields WebQSP Hit@1 = 56.96 ± 1.83 (a 19.19% relative drop vs full).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly to GraphToken (graph prompt tuning) and triple-based prepending (KAPING) baselines: the G-Retriever pipeline that uses textualized subgraphs outperforms GraphToken in SceneGraphs and WebQSP (e.g., WebQSP: GraphToken ~57.05 vs G-Retriever 70.49 Hit@1) and yields large overall gains vs simple prompt tuning. The paper reports average improvements of ~30.8% over GraphToken and ~40.6% over traditional prompt tuning when using the complete G-Retriever approach (which relies on textualized retrieval outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Primary challenge is scalability: fully textualizing large graphs creates extremely long token sequences that exceed LLM context windows; naive truncation loses crucial information. Requires complementary subgraph-selection/retrieval to be practical. The paper notes the textualization method adopted is intentionally simple (not claimed optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5257.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5257.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCST+Textualization (G-Retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prize-Collecting Steiner Tree (PCST) based subgraph retrieval followed by textualization (as used in G-Retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step conversion strategy for LLM input: (1) retrieve a compact, connected subgraph relevant to the query by solving a PCST optimization over node/edge relevance scores; (2) textualize that retrieved subgraph (flatten nodes/edges) and feed it to the LLM. This enables tractable graph-to-text conversion for very large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PCST-based Subgraph Selection + Textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Assign prizes to top-k nodes/edges based on cosine similarity between node/edge SentenceBert embeddings and the query embedding; set prizes descending from k..1. Solve a Prize-Collecting Steiner Tree (PCST) optimization to find a connected subgraph S* that maximizes total prize minus cost(S) where cost(S)=|E_S| * C_e. To accommodate edge prizes, edges with prize > cost are represented via a virtual node with adjusted prizes/costs. The selected connected S* is then flattened (textualize(S*)) into node and edge lines and concatenated with the query for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General textual graphs (scene graphs, knowledge graphs, explanation graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compactness: returns a small, connected, semantically-relevant subgraph so textualization stays within LLM context limits; explainability: returns an explicit subgraph (supports human inspection); faithfulness: retrieval from the graph reduces hallucination because answers are grounded in retrieved graph tokens. Tunable via k and edge cost C_e to balance recall vs compactness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>GraphQA benchmark (ExplaGraphs, SceneGraphs, WebQSP) and retrieval-quality measurements (retrieval success rate defined as whether the label is contained in the retrieved subgraph). Also used in hallucination evaluation (models asked to cite supporting nodes/edges).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Retrieval / downstream metrics: PCST-based retrieval used in G-Retriever achieves retrieval accuracy (WebQSP) 70.49 (Hit@1 downstream). Compared to top-k triple retrieval (KAPING) retrieval accuracy 60.81 (Table 11). Efficiency: after applying graph RAG (i.e., PCST selection) tokens reduced dramatically (SceneGraphs tokens ↓83%, nodes ↓74%, training time ↓29%; WebQSP tokens ↓99%, nodes ↓99%, training time ↓67%). Hallucination mitigation: G-Retriever (PCST retrieval + textualization) yields Valid Nodes 77%, Valid Edges 76%, Fully Valid Graphs 62% vs baseline LLM+Graph Prompt Tuning Valid Nodes 31%, Valid Edges 12%, Fully Valid 8% (Table 5). Sensitivity to k: Hit@1 on WebQSP for k ∈ {3,5,10,20} = {0.6977, 0.7063, 0.7248, 0.7039} (peaks around k=10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms isolated triple retrieval (KAPING) which chooses top-k triples in isolation (60.81 retrieval accuracy vs 70.49). Compared to GraphToken (graph-as-soft-prompt), PCST+textualization+soft graph-token (G-Retriever) yields higher downstream QA metrics and dramatically lower hallucination. Ablation shows retrieval is important: removing retrieval (w/o Retrieval) causes Hit@1 drop of 9.43% (to 63.84 on WebQSP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires setting hyperparameters (k, edge cost C_e); small k may omit needed knowledge while large k adds noise. PCST is adapted for edge prizes via virtual nodes (added complexity). Current retrieval component is static (not end-to-end trainable) which the authors note as a limitation/future direction. Computational complexity is modest and retrieval can be done offline, but tuning is needed per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5257.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5257.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphToken (graph prompt tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphToken / Graph Prompt Tuning (soft-prompt via GNN-produced tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-LLM approach that encodes graph information into a trainable soft prompt (vector tokens) produced by a Graph Neural Network instead of converting the graph into literal text; the soft prompt is concatenated to the LLM input and tuned (prompt tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Prompt Tuning (GraphToken)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A GNN encodes the graph and produces a fixed-size vector token (or set of virtual tokens) that serve as a learned soft prompt for a frozen LLM; the actual node/edge textual attributes are not flattened into the LLM token stream. Training adjusts the soft prompt (and optionally GNN) but leaves the LLM parameters frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General textual graphs (used as baseline on scene graphs / knowledge graphs / explanation graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact (avoids huge token counts), parameter-efficient (PEFT); can represent graph structure in continuous embeddings. However, this approach summarizes whole-graph information into embeddings, which can lose fine-grained verifiable details and is prone to hallucination because the LLM cannot retrieve explicit node/edge text from the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>GraphQA benchmark tasks in this paper (ExplaGraphs, SceneGraphs, WebQSP) used as a baseline in the Frozen LLM w/ Prompt Tuning setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported performance (Frozen LLM w/ PT - GraphToken): ExplaGraphs ~0.8508 ± 0.0551 (accuracy), SceneGraphs 0.4903 ± 0.0105 (accuracy), WebQSP 57.05 ± 0.74 (Hit@1). Hallucination metrics for the baseline LLM+Graph Prompt Tuning: Valid Nodes 31%, Valid Edges 12%, Fully Valid Graphs 8% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to textualization-with-retrieval (G-Retriever), GraphToken is substantially weaker on SceneGraphs and WebQSP in this paper and produces much higher hallucination rates. G-Retriever shows average improvement of ~30.8% over GraphToken across tasks (text reports this averaged improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prone to hallucination because embeddings are lossy and not directly verifiable; limited recall of large graphs when compressed into fixed-size prompt; performance sensitive to how much structure the GNN encodes and how the prompt is interfaced with the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5257.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5257.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KAPING / top-k triple prepending</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering (KAPING / top-k triple retrieval and prepending)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A triple-level graph-to-text method that retrieves top-k triples relevant to the query and prepends them (as text) to the LLM input; commonly used for knowledge-graph QA baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Top-k Triple Prepending (KAPING)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Retrieve top-k triples from the knowledge graph by similarity to the query, format them as textual triples (e.g., subject - predicate - object) and prepend them to the input prompt for the LLM. This operates at the triple level and treats each triple independently.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase subset / WebQSP)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact and straightforward to prepare; keeps token counts manageable. However, triples are retrieved in isolation and ignore neighborhood/context, so they can miss necessary bridging nodes/relations for multi-hop QA and are less explainable as a connected subgraph.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebQSP retrieval / QA and inference-only prompting experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported retrieval accuracy on WebQSP: KAPING (top-k triple retrieval) 60.81 vs G-Retriever 70.49 (Table 11). In inference-only QA experiments KAPING variants are included in performance comparisons (see Table 3 / Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to PCST-based connected subgraph retrieval + textualization, KAPING's isolated triple retrieval is less accurate on retrieval and downstream QA. The paper argues PCST's connected subgraph better captures neighborhood/bridging nodes, improving retrieval accuracy and reducing hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ignores graph connectivity and neighborhood context when selecting triples; may fail on multi-hop queries that require bridging facts; retrieval quality sensitive to k; performance lower than PCST-based connected retrieval in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5257.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5257.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Build-a-Graph (CoT-BAG) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Build-a-Graph Prompting (BAG) / Chain-of-Thought based 'construct a graph' prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering method that instructs the LLM to explicitly construct/represent a graph from the textual graph description within its generation process (e.g., 'Let's construct a graph with the nodes and edges first.') to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Build-a-Graph Prompting (CoT-BAG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Append an instruction to the LLM input that requests the model to build an explicit graph (nodes & edges) as an intermediate step (a form of chain-of-thought guidance), then proceed to answer the query using that constructed graph. This is a prompting-time conversion/representation mechanism rather than a data preprocessing serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General textual/scene/knowledge graphs (used as an inference-only prompting variant)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Does not require explicit textualization into long sequences; leverages LLM internal reasoning to form intermediate structure. Can help multi-step reasoning but remains constrained by LLM's token window and internal memory; quality depends heavily on LLM reasoning capabilities and can still hallucinate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Inference-only GraphQA experiments (zero-shot and chain-of-thought style prompting) across GraphQA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included among zero-shot / inference-only baselines in Table 3 (CoT-BAG results reported alongside Zero-shot / Zero-CoT methods). Exact numeric entries vary per dataset; CoT-BAG provided modest improvements over naive zero-shot in some cases but still underperforms retrieval-augmented approaches in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to explicit textualization + retrieval, CoT-BAG (pure prompting) is less reliable on large graphs and multi-hop queries; it avoids token explosion but suffers from hallucination and inability to ground answers in actual graph text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on LLM internal chain-of-thought generation quality; vulnerable to hallucination and cannot guarantee grounding in the original graph (no explicit retrieval of supporting nodes/edges).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let your graph do the talking: Encoding structured data for llms <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering <em>(Rating: 2)</em></li>
                <li>GraphToken <em>(Rating: 1)</em></li>
                <li>Graph reasoning in text space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5257",
    "paper_id": "paper-267626823",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Textualized Graph (flattening)",
            "name_full": "Textualized Graph via Flattening Nodes and Edges into Natural Language",
            "brief_description": "A straightforward graph-to-text conversion that flattens node and edge textual attributes into a natural-language / CSV-like sequence (node_id,node_attr and src,edge_attr,dst) for input to LLMs; used as the textual prompt/context for generation and prompt-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Textualized Graph (flattening / CSV-like serialization)",
            "representation_description": "Convert graph to a list of textual lines representing nodes and edges. Node lines: \"node_id, node_attr\" (attributes include textual descriptions and object metadata); edge lines: \"src, edge_attr, dst\". In generation the textualized subgraph textualize(S*) is concatenated with the query and fed to the text embedder (the LLM's first layer). This paper implements textualization as part of the generation stage and provides concrete CSV-like examples in the benchmark.",
            "graph_type": "General textual graphs (scene graphs, commonsense explanation graphs, knowledge graphs / Freebase subset)",
            "representation_properties": "Faithful to original textual attributes and interpretable (human-readable CSV-like form); simple and general-purpose; straightforward to embed with existing text embedders. Major downside: token-inefficient — whole-graph textualizations can exceed LLM context windows and require truncation, causing information loss; interacts naturally with retrieval but scales poorly without subgraph selection.",
            "evaluation_task": "GraphQA benchmark tasks (ExplaGraphs: generative commonsense reasoning; SceneGraphs: scene-graph question answering; WebQSP: multi-hop knowledge graph QA). Used in inference-only, prompt-tuning, and fine-tuning configurations.",
            "performance_metrics": "Reported downstream metrics in this paper include accuracy (ExplaGraphs, SceneGraphs) and Hit@1 (WebQSP). With the full G-Retriever pipeline (which includes textualized subgraphs): ExplaGraphs 0.8516 ± 0.0092 (accuracy), SceneGraphs 0.8131 ± 0.0162 (accuracy), WebQSP Hit@1 70.49 ± 1.21. Ablation removing the textualized graph (w/o Textualized Graph) yields WebQSP Hit@1 = 56.96 ± 1.83 (a 19.19% relative drop vs full).",
            "comparison_to_other_representations": "Compared directly to GraphToken (graph prompt tuning) and triple-based prepending (KAPING) baselines: the G-Retriever pipeline that uses textualized subgraphs outperforms GraphToken in SceneGraphs and WebQSP (e.g., WebQSP: GraphToken ~57.05 vs G-Retriever 70.49 Hit@1) and yields large overall gains vs simple prompt tuning. The paper reports average improvements of ~30.8% over GraphToken and ~40.6% over traditional prompt tuning when using the complete G-Retriever approach (which relies on textualized retrieval outputs).",
            "limitations_or_challenges": "Primary challenge is scalability: fully textualizing large graphs creates extremely long token sequences that exceed LLM context windows; naive truncation loses crucial information. Requires complementary subgraph-selection/retrieval to be practical. The paper notes the textualization method adopted is intentionally simple (not claimed optimal).",
            "uuid": "e5257.0",
            "source_info": {
                "paper_title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PCST+Textualization (G-Retriever)",
            "name_full": "Prize-Collecting Steiner Tree (PCST) based subgraph retrieval followed by textualization (as used in G-Retriever)",
            "brief_description": "A two-step conversion strategy for LLM input: (1) retrieve a compact, connected subgraph relevant to the query by solving a PCST optimization over node/edge relevance scores; (2) textualize that retrieved subgraph (flatten nodes/edges) and feed it to the LLM. This enables tractable graph-to-text conversion for very large graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "PCST-based Subgraph Selection + Textualization",
            "representation_description": "Assign prizes to top-k nodes/edges based on cosine similarity between node/edge SentenceBert embeddings and the query embedding; set prizes descending from k..1. Solve a Prize-Collecting Steiner Tree (PCST) optimization to find a connected subgraph S* that maximizes total prize minus cost(S) where cost(S)=|E_S| * C_e. To accommodate edge prizes, edges with prize &gt; cost are represented via a virtual node with adjusted prizes/costs. The selected connected S* is then flattened (textualize(S*)) into node and edge lines and concatenated with the query for the LLM.",
            "graph_type": "General textual graphs (scene graphs, knowledge graphs, explanation graphs)",
            "representation_properties": "Compactness: returns a small, connected, semantically-relevant subgraph so textualization stays within LLM context limits; explainability: returns an explicit subgraph (supports human inspection); faithfulness: retrieval from the graph reduces hallucination because answers are grounded in retrieved graph tokens. Tunable via k and edge cost C_e to balance recall vs compactness.",
            "evaluation_task": "GraphQA benchmark (ExplaGraphs, SceneGraphs, WebQSP) and retrieval-quality measurements (retrieval success rate defined as whether the label is contained in the retrieved subgraph). Also used in hallucination evaluation (models asked to cite supporting nodes/edges).",
            "performance_metrics": "Retrieval / downstream metrics: PCST-based retrieval used in G-Retriever achieves retrieval accuracy (WebQSP) 70.49 (Hit@1 downstream). Compared to top-k triple retrieval (KAPING) retrieval accuracy 60.81 (Table 11). Efficiency: after applying graph RAG (i.e., PCST selection) tokens reduced dramatically (SceneGraphs tokens ↓83%, nodes ↓74%, training time ↓29%; WebQSP tokens ↓99%, nodes ↓99%, training time ↓67%). Hallucination mitigation: G-Retriever (PCST retrieval + textualization) yields Valid Nodes 77%, Valid Edges 76%, Fully Valid Graphs 62% vs baseline LLM+Graph Prompt Tuning Valid Nodes 31%, Valid Edges 12%, Fully Valid 8% (Table 5). Sensitivity to k: Hit@1 on WebQSP for k ∈ {3,5,10,20} = {0.6977, 0.7063, 0.7248, 0.7039} (peaks around k=10).",
            "comparison_to_other_representations": "Outperforms isolated triple retrieval (KAPING) which chooses top-k triples in isolation (60.81 retrieval accuracy vs 70.49). Compared to GraphToken (graph-as-soft-prompt), PCST+textualization+soft graph-token (G-Retriever) yields higher downstream QA metrics and dramatically lower hallucination. Ablation shows retrieval is important: removing retrieval (w/o Retrieval) causes Hit@1 drop of 9.43% (to 63.84 on WebQSP).",
            "limitations_or_challenges": "Requires setting hyperparameters (k, edge cost C_e); small k may omit needed knowledge while large k adds noise. PCST is adapted for edge prizes via virtual nodes (added complexity). Current retrieval component is static (not end-to-end trainable) which the authors note as a limitation/future direction. Computational complexity is modest and retrieval can be done offline, but tuning is needed per dataset.",
            "uuid": "e5257.1",
            "source_info": {
                "paper_title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GraphToken (graph prompt tuning)",
            "name_full": "GraphToken / Graph Prompt Tuning (soft-prompt via GNN-produced tokens)",
            "brief_description": "A graph-to-LLM approach that encodes graph information into a trainable soft prompt (vector tokens) produced by a Graph Neural Network instead of converting the graph into literal text; the soft prompt is concatenated to the LLM input and tuned (prompt tuning).",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Graph Prompt Tuning (GraphToken)",
            "representation_description": "A GNN encodes the graph and produces a fixed-size vector token (or set of virtual tokens) that serve as a learned soft prompt for a frozen LLM; the actual node/edge textual attributes are not flattened into the LLM token stream. Training adjusts the soft prompt (and optionally GNN) but leaves the LLM parameters frozen.",
            "graph_type": "General textual graphs (used as baseline on scene graphs / knowledge graphs / explanation graphs)",
            "representation_properties": "Compact (avoids huge token counts), parameter-efficient (PEFT); can represent graph structure in continuous embeddings. However, this approach summarizes whole-graph information into embeddings, which can lose fine-grained verifiable details and is prone to hallucination because the LLM cannot retrieve explicit node/edge text from the prompt.",
            "evaluation_task": "GraphQA benchmark tasks in this paper (ExplaGraphs, SceneGraphs, WebQSP) used as a baseline in the Frozen LLM w/ Prompt Tuning setting.",
            "performance_metrics": "Reported performance (Frozen LLM w/ PT - GraphToken): ExplaGraphs ~0.8508 ± 0.0551 (accuracy), SceneGraphs 0.4903 ± 0.0105 (accuracy), WebQSP 57.05 ± 0.74 (Hit@1). Hallucination metrics for the baseline LLM+Graph Prompt Tuning: Valid Nodes 31%, Valid Edges 12%, Fully Valid Graphs 8% (Table 5).",
            "comparison_to_other_representations": "Compared to textualization-with-retrieval (G-Retriever), GraphToken is substantially weaker on SceneGraphs and WebQSP in this paper and produces much higher hallucination rates. G-Retriever shows average improvement of ~30.8% over GraphToken across tasks (text reports this averaged improvement).",
            "limitations_or_challenges": "Prone to hallucination because embeddings are lossy and not directly verifiable; limited recall of large graphs when compressed into fixed-size prompt; performance sensitive to how much structure the GNN encodes and how the prompt is interfaced with the LLM.",
            "uuid": "e5257.2",
            "source_info": {
                "paper_title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "KAPING / top-k triple prepending",
            "name_full": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering (KAPING / top-k triple retrieval and prepending)",
            "brief_description": "A triple-level graph-to-text method that retrieves top-k triples relevant to the query and prepends them (as text) to the LLM input; commonly used for knowledge-graph QA baselines.",
            "citation_title": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering",
            "mention_or_use": "use",
            "representation_name": "Top-k Triple Prepending (KAPING)",
            "representation_description": "Retrieve top-k triples from the knowledge graph by similarity to the query, format them as textual triples (e.g., subject - predicate - object) and prepend them to the input prompt for the LLM. This operates at the triple level and treats each triple independently.",
            "graph_type": "Knowledge graphs (Freebase subset / WebQSP)",
            "representation_properties": "Compact and straightforward to prepare; keeps token counts manageable. However, triples are retrieved in isolation and ignore neighborhood/context, so they can miss necessary bridging nodes/relations for multi-hop QA and are less explainable as a connected subgraph.",
            "evaluation_task": "WebQSP retrieval / QA and inference-only prompting experiments.",
            "performance_metrics": "Reported retrieval accuracy on WebQSP: KAPING (top-k triple retrieval) 60.81 vs G-Retriever 70.49 (Table 11). In inference-only QA experiments KAPING variants are included in performance comparisons (see Table 3 / Table 12).",
            "comparison_to_other_representations": "Compared to PCST-based connected subgraph retrieval + textualization, KAPING's isolated triple retrieval is less accurate on retrieval and downstream QA. The paper argues PCST's connected subgraph better captures neighborhood/bridging nodes, improving retrieval accuracy and reducing hallucination.",
            "limitations_or_challenges": "Ignores graph connectivity and neighborhood context when selecting triples; may fail on multi-hop queries that require bridging facts; retrieval quality sensitive to k; performance lower than PCST-based connected retrieval in reported experiments.",
            "uuid": "e5257.3",
            "source_info": {
                "paper_title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Build-a-Graph (CoT-BAG) prompting",
            "name_full": "Build-a-Graph Prompting (BAG) / Chain-of-Thought based 'construct a graph' prompting",
            "brief_description": "A prompt-engineering method that instructs the LLM to explicitly construct/represent a graph from the textual graph description within its generation process (e.g., 'Let's construct a graph with the nodes and edges first.') to improve multi-step reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Build-a-Graph Prompting (CoT-BAG)",
            "representation_description": "Append an instruction to the LLM input that requests the model to build an explicit graph (nodes & edges) as an intermediate step (a form of chain-of-thought guidance), then proceed to answer the query using that constructed graph. This is a prompting-time conversion/representation mechanism rather than a data preprocessing serialization.",
            "graph_type": "General textual/scene/knowledge graphs (used as an inference-only prompting variant)",
            "representation_properties": "Does not require explicit textualization into long sequences; leverages LLM internal reasoning to form intermediate structure. Can help multi-step reasoning but remains constrained by LLM's token window and internal memory; quality depends heavily on LLM reasoning capabilities and can still hallucinate.",
            "evaluation_task": "Inference-only GraphQA experiments (zero-shot and chain-of-thought style prompting) across GraphQA datasets.",
            "performance_metrics": "Included among zero-shot / inference-only baselines in Table 3 (CoT-BAG results reported alongside Zero-shot / Zero-CoT methods). Exact numeric entries vary per dataset; CoT-BAG provided modest improvements over naive zero-shot in some cases but still underperforms retrieval-augmented approaches in this paper's experiments.",
            "comparison_to_other_representations": "Compared to explicit textualization + retrieval, CoT-BAG (pure prompting) is less reliable on large graphs and multi-hop queries; it avoids token explosion but suffers from hallucination and inability to ground answers in actual graph text.",
            "limitations_or_challenges": "Depends on LLM internal chain-of-thought generation quality; vulnerable to hallucination and cannot guarantee grounding in the original graph (no explicit retrieval of supporting nodes/edges).",
            "uuid": "e5257.4",
            "source_info": {
                "paper_title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let your graph do the talking: Encoding structured data for llms",
            "rating": 2,
            "sanitized_title": "let_your_graph_do_the_talking_encoding_structured_data_for_llms"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering",
            "rating": 2,
            "sanitized_title": "knowledgeaugmented_language_model_prompting_for_zeroshot_knowledge_graph_question_answering"
        },
        {
            "paper_title": "GraphToken",
            "rating": 1,
            "sanitized_title": "graphtoken"
        },
        {
            "paper_title": "Graph reasoning in text space",
            "rating": 1,
            "sanitized_title": "graph_reasoning_in_text_space"
        }
    ],
    "cost": 0.019713,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering
27 May 2024</p>
<p>Xiaoxin He xiaoxin@comp.nus.edu.sg 
National University of Singapore</p>
<p>Yijun Tian yijun.tian@nd.edu 
University of Notre Dame</p>
<p>Yifei Sun yifeisun@comp.nus.edu.sg 
National University of Singapore</p>
<p>Nitesh V Chawla nchawla@nd.edu 
University of Notre Dame</p>
<p>Thomas Laurent tlaurent@lmu.edu 
Loyola Marymount University</p>
<p>Yann Lecun 
New York University
5 Meta AI</p>
<p>Xavier Bresson xaviercs@comp.nus.edu.sg 
National University of Singapore</p>
<p>Bryan Hooi bhooi@comp.nus.edu.sg 
National University of Singapore</p>
<p>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering
27 May 202412BBBC5EE29D79FCF26000D202E6C531arXiv:2402.07630v3[cs.LG]
Given a graph with textual attributes, we enable users to 'chat with their graph': that is, to ask questions about the graph using a conversational interface.In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph.While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs.In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning.Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks.Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting.To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem.Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination. 1</p>
<p>Introduction</p>
<p>Graphs and Large Language Models (LLMs).The advent of LLMs has significantly shaped the artificial intelligence landscape.As these models are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital.In particular, in our interconnected world, a significant portion of real-world data inherently possesses a graph structure, such as the Web, e-commerce, recommendation systems, knowledge graphs, and many others.Moreover, many of these involve graphs with textual attributes (i.e., textual graphs), making them well-suited for LLM-centric methods.This has spurred interest in combining graph-based technologies, particularly graph neural networks (GNNs), with LLMs to enhance their reasoning on graphs [44,15,24].</p>
<p>The Present Work: Enabling 'Chat With Your Graph'.While existing works integrate LLMs and GNNs in various ways, they mostly focus on conventional graph tasks such as node, edge and graph</p>
<p>New Benchmark</p>
<p>GraphQA New Architecture</p>
<p>G-Retriever</p>
<p>Techniques</p>
<p>Retrieval Augmented Generation</p>
<p>Graph Prompt Tuning Graph Textualization</p>
<p>Sure! Here's an example of a 500-word advertisement for the place based on the scene graph: "Experience the breathtaking beauty of our place, where the rolling hills meet the sparkling water of the swimming pool.Take a leisurely stroll through the lush green elds, and let the gentle breeze caress your face as you gaze out at the picturesque view.The sheep graze peacefully in the distance, their white coats glistening in the sunlight.The sky is a brilliant blue, with u y white clouds drifting lazily across.Come and bask in the serene ambiance of our place, where the natural beauty will leave you in awe."The explanation graph provides a detailed analysis of the reasons why women undergo cosmetic surgery.According to the graph, the main reasons include the desire to change one's appearance, the in uence of societal beauty standards, and the addiction to searching for perfection.These reasons are further supported by the various nodes and edges in the graph, which provide additional insights into the motivations and consequences of cosmetic surgery.Firstly, the node labeled "human" highlights the fundamental human desire to feel attractive and con dent in one's appearance... Write a 500-word argument essay based on the information provided in the explanation graph.</p>
<p>Unified Conversational Interface</p>
<p>Write an 500-word advertisement for this place in the scene graph that would make people want to visit it.</p>
<p>Great! Based on the provided knowledge graph, here's an educational tour that highlights interesting facts and connections related to Benjamin Franklin's inventions and other notable achievements:</p>
<ol>
<li>Franklin's Lightning Rod: Start by exploring Franklin's most famous invention -the lightning rod.Discover how Franklin's invention protected buildings from lightning strikes and how it changed the way people understood electricity... What else did ben franklin invent?Surrounding this question, develop an educational tour through the knowledge graph, highlighting interesting facts and connections in an engaging way.classification [8], or answering simple questions on small or synthetic graphs [44,31].In contrast, we develop a flexible question-answering framework targeting complex and real-world graphs.This framework enables users to 'chat with their graph' via a unified conversational interface, representing a leap towards intuitive interaction with graph data, as demonstrated in Figure 1.</li>
</ol>
<p>The Need for a Comprehensive GraphQA Benchmark.Question answering (QA) is a fundamentally important task in natural language processing, serving as a key benchmark for assessing LLMs and providing a unified interface for various capabilities.Despite extensive research in QA, a comprehensive benchmark specifically tailored for the graph modality is lacking.In contrast to existing benchmarks that focus on basic graph-based reasoning tasks such as node degree, edge existence, and shortest path [6,44], our benchmark addresses complex and real-world graph applications including common sense reasoning, scene understanding, and knowledge graph reasoning (refer to Figure 2).This is vital for measuring progress toward a model capable of answering a wide range of questions about graphs from diverse applications.</p>
<p>New Architecture for GraphQA.To enable effective and efficient graph QA, even on large graphs, we propose G-Retriever, a new framework combining the strengths of GNNs, LLMs, and RAG (Figure 3).Next, we will discuss the motivation, strengths, and details of our model.</p>
<p>Tackling Hallucination in Graph LLMs.LLMs are prone to hallucination, a phenomenon where the generated content is factually inaccurate or nonsensical [12].We validate the presence of this issue in graph settings.In particular, we employ a baseline method that adapts MiniGPT-4 [57] to graphs, where a frozen LLM interacts with a trainable GNN that encodes graph data as a soft prompt, as in GraphToken [31].Our findings, shown in Table 1, indicate that hallucination, an important problem in text-based LLMs, is also prevalent in Graph LLMs.This may be attributed to the baseline's inability to recall the entire graph structure from a single graph embedding, leading to the generation of incorrect nodes or edges during the QA task.In contrast, by employing RAG for direct information retrieval from the actual graph, our G-Retriever mitigates this issue, as substantiated by Table 1.</p>
<p>Enhancing Scalability and Efficiency in Graph LLMs.Recent research endeavors have explored translating graphs into natural language, such as by flattening nodes and edges into a text sequence, enabling their processing by LLMs for graph-based tasks [56,6].However, this method faces critical scalability issues.Converting a graph with thousands of nodes and edges into a text sequence results in an excessive number of tokens, surpassing the input capacity of many LLMs.An alternative of truncating the graph text sequence to fit the LLM's input token limit leads to loss of information and response quality.G-Retriever overcomes these issues with its RAG component, which allows for effective scaling to larger graphs by selectively retrieving only relevant parts of the graph.</p>
<p>Tailoring the RAG Approach to Graphs.Existing RAG methodologies are primarily designed for simpler data types or knowledge graphs, where information is retrieved in a manner isolated from the graph structure [7,1,36,16].Hence, we introduce a new retrieval approach for general textual graphs.Notably, we formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST) What kind of animal is in the bushes?Answer the question and list the nodes or edges from the graph that support your answer.</p>
<p>LLM w/ Graph Prompt Tuning</p>
<p>[Incorrect response with hallucinated nodes and edges] The animal in the bushes is a deer.Nodes: optimization problem, which takes the neighborhood information into account during retrieval.This also allows the return of a subgraph most relevant to a query, thereby improving explainability.</p>
<p>The contributions of this paper are outlined as follows:</p>
<p>• Pioneering the integration of Graph RAG.We present the first retrieval approach for general textual graph tasks, which greatly enhances scalability and efficiency.</p>
<p>• Enabling 'Chat with Your Graph'.We develop a flexible question answering framework to handle complex and real-world textual graphs through a unified conversational interface.</p>
<p>• Introducing A Novel GraphQA Benchmark.We introduce a diverse benchmark targeted at real-world graph question answering, filling a crucial research gap.</p>
<p>• Empirical Findings.We demonstrate the efficiency and effectiveness of G-Retriever in multiple domains and present the significant finding of hallucination in graph LLMs.</p>
<p>Related Work</p>
<p>Graphs and Large Language Models.A significant body of research has emerged at the intersection of graph-based techniques and LLMs [30,24,15,44,54].This exploration spans diverse aspects, ranging from the design of general graph models [47,25,51,19,40,31], and multi-modal architectures [23,49] to practical applications.Noteworthy applications include fundamental graph reasoning [52,3,56], node classification [8,11,39,5,50,4,33], graph classification/regression [32,55], and leveraging LLMs for knowledge graph-related tasks [41,14,29].</p>
<p>Retrieval-Augmented Generation (RAG).The concept of Retrieval-Augmented Generation, initially proposed by Lewis et al. [21], has gained increased attention for its ability to mitigate the issue of hallucination within LLMs and enhance trustworthiness and explainability [7].Despite its success in language-related tasks, the application of retrieval-based approaches to general graph tasks remains largely unexplored.Most existing work focuses primarily on the knowledge graph [38,1,36,16].</p>
<p>Our research is the first to apply a retrieval-based approach to general graph tasks, marking a novel advancement in the field and demonstrating the versatility of RAG beyond language processing.and the LLaMA-adapter [53], have been influential.These advancements in PEFT have laid the foundation for the development of sophisticated multimodal models.Prominent examples in this domain include MiniGPT-4 [57], LLaVA [26], and NExT-Chat [46].There are also emerging efforts in applying PEFT to graph LLMs, such as GraphLLM [3] and GraphToken [31] for basic graph reasoing tasks and GNP [41] for multi-option QA on knowledge graphs.</p>
<p>Formalization</p>
<p>This section establishes the notation and formalizes key concepts related to textual graphs, language models for text encoding, and large language models and prompt tuning.</p>
<p>Textual Graphs.A textual graph is a graph where nodes and edges possess textual attributes.Formally, it can be defined as
G = (V, E, {x n } n∈V , {x e } e∈E )
, where V and E represent the sets of nodes and edges, respectively.Additionally, x n ∈ D Ln and x e ∈ D Le denote sequential text associate with a node n ∈ V or an edge e ∈ E, where D represents the vocabulary, and L n and L e signify the length of the text associated with the respective node or edge.</p>
<p>Language Models for Text Encoding.In the context of textual graphs, language models (LMs) are essential for encoding the text attributes associated with nodes and edges, thereby learning representations that capture their semantic meaning.For a node n with text attributes x n ∈ D Ln , an LM encodes these attributes as:
z n = LM(x n ) ∈ R d ,(1)
where z n is the output of the LM, and d is the dimension of the output vector.</p>
<p>Large Language Models and Prompt Tuning.LLMs have introduced a new paradigm for taskadaptation known as "pre-train, prompt, and predict", replacing the traditional "pre-train, fine-tune" paradigm.In this paradigm, the LLM is first pre-trained on a large corpus of text data to learn general language representations.Then, rather than fine-tuning the model on task-specific labeled data, the model is prompted with a textual prompt that specifies the task and context.Subsequently, the model generates the output directly based on the prompt and the input.</p>
<p>The LLM, parameterized by weights θ, takes a sequence of tokens X, and a prompt P as input, and generates a sequence of tokens Y = {y 1 , y 2 , . . ., y r } as output.Formally, the probability distribution of the output sequence given the concatenated input sequence and prompt, i.e., [P ; X], is defined as
p θ (Y |[P ; X]) = r i=1 p θ (y i |y &lt;i , [P ; X]).(2)
Here, y &lt;i represents the prefix of sequence y up to position i − 1, and p(y i |y &lt;i , [P ; X]) represents the probability of generating token y i given y &lt;i and [P ; X].</p>
<p>Soft prompt tuning eliminates the need for manual prompt design.Given a series of p tokens X = {x 1 , x 2 , . . ., x p }, after being processed by the text embedder, it forms a matrix X e ∈ R p×d l , where d l is the dimension of the embedding space.Soft prompts can be represented as parameters P e ∈ R q×d l , where q is the length of the prompt.The prompt is then concatenated with the embedded input, forming a single matrix [P e ; X e ] ∈ R (q+p)×d l .This combined matrix is processed by the self-attention layers in LLM as usual.Training involves maximizing the likelihood of Y through backpropagation, with gradient updates applied solely to P e , while θ remains fixed.</p>
<p>Proposed GraphQA Benchmark</p>
<p>Our GraphQA represents a comprehensive and diverse benchmark for graph question-answering.It is tailored to assess the capabilities of models in answering a wide range of questions about graphs across diverse domains.</p>
<p>Data Format</p>
<p>Each entry in the GraphQA benchmark consists of a textual graph, a question related to the graph, and one or more corresponding answers, as illustrated in Figure 2.  Textual Graphs.The textual graph is converted into a natural language format, resulting in a list of nodes and edges, akin to a CSV file format.It is important to note that while multiple methods exist for textualizing a graph, our focus is not on identifying the optimal solution.Instead, we prioritize a straightforward yet empirically effective approach for representing graphs in natural language, facilitating the benchmark's use in diverse GraphQA scenarios.</p>
<p>Questions and Answers.Questions are designed to explore specific elements or relationships within the graph.Answers, residing within the attributes of nodes or edges, often require multi-hop reasoning for accurate identification.</p>
<p>Description of Datasets</p>
<p>The GraphQA benchmark integrates three existing datasets: ExplaGraphs, SceneGraphs, and WebQSP.Table 2 presents the summary statistics of these datasets.It is important to note that these datasets were not originally developed for this work.However, a significant contribution of our research is the standardization and processing of these diverse datasets into a uniform data format suitable for the GraphQA benchmark.These datasets, previously utilized in different contexts, are reintroduced with a new focus tailored for GraphQA.For a detailed comparison with the original datasets, see the Appendix C.</p>
<p>ExplaGraphs is a dataset for generative commonsense reasoning, focusing on creating explanation graphs for stance prediction in debates.It offers detailed, unambiguous commonsense-augmented graphs to evaluate arguments supporting or refuting a belief.The primary task is to assess whether arguments are supportive or contradictory, using accuracy as the metric.We have converted the triplet-form provided in Saha et al. [35] into a standard graph format.</p>
<p>SceneGraphs, a visual question answering dataset, includes 100,000 scene graphs.Each graph details objects, attributes, and relations within an image.This dataset challenges users with tasks requiring spatial understanding and multi-step inference.The task is to answer open-ended questions based on a textual description of a scene graph, evaluated on accuracy.We have sampled from the GQA dataset [13] and constructed standard graphs from the provided JSON files.</p>
<p>WebQSP is a large-scale multi-hop knowledge graph QA dataset consisting of 4,737 questions.It was proposed by Yih et al. [48] and, following Luo et al. [28], utilizes a subset of Freebase, encompassing facts within 2-hops of entities mentioned in the questions.The task involves answering questions that</p>
<p>Graph Encoder</p>
<p>Question: What is the name of justin bieber brother?</p>
<p>Step 1: Indexing</p>
<p>Storage</p>
<p>Step The most semantically relevant nodes and edges are retrieved, conditioned on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An answer is generated using a 'graph prompt', a textualized graph, and the query.require multi-hop reasoning.Given the possibility of multiple answers for the same question, the hit@1 metric is used to assess the precision of the top returned answer.</p>
<p>G-Retriever</p>
<p>In this section, we introduce G-Retriever, a new architecture tailored for GraphQA, which integrates the strengths of GNNs, LLMs, and RAG.To allow efficient fine-tuning while preserving the LLM's pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output of the GNN.Our RAG-based design mitigates hallucinations through direct retrieval of the graph, while allowing our approach to scale to graphs exceeding the LLM's context window size.To adapt RAG to graphs, we formulate subgraph retrieval as a PCST optimization problem.This approach also allows us to enhance explainability by returning the retrieved subgraph.</p>
<p>G-Retriever comprises four main steps: indexing, retrieval, subgraph construction and generation, as depicted in Figure 3.The implementation details of each step are elaborated in the following sections.</p>
<p>Indexing</p>
<p>We initiate the RAG approach by generating node and graph embeddings using a pre-trained LM.These embeddings are then stored in a nearest neighbor data structure.</p>
<p>To elaborate, consider x n ∈ D Ln as the text attributes of node n.Utilizing a pre-trained LM, such as SentenceBert [34], we apply the LM to x n , yielding the representation z n :
z n = LM(x n ) ∈ R d ,(3)
where d denotes the dimension of the output vector.Similar preprocessing steps are applied to edges.Refer to Figure 3, Step 1 for an illustrative representation.</p>
<p>Retrieval</p>
<p>For retrieval, we employ the same encoding strategy to the query x q , to ensure consistent treatment of textual information:
z q = LM(x q ) ∈ R d .(4)
Next, to identify the most relevant nodes and edges for the current query, we use a k-nearest neighbors retrieval approach.This method yields a set of 'relevant nodes/edges' based on the similarity between the query and each node or edge.The retrieval operation is defined as:
V k = argtopk n∈V cos(z q , z n ) E k = argtopk e∈E cos(z q , z e ),(5)
where z n and z e are the embeddings of node n and edge e, respectively.We use the cosine similarity function, cos(•, •), to measure the similarity between the query representation and the node/edge embeddings.The argtopk operation retrieves the top-k elements based on this similarity, providing a set of nodes V k and edges E k considered most relevant to the query.See Step 2 of Figure 3.</p>
<p>Subgraph Construction</p>
<p>This step aims to construct a subgraph that encompasses as many relevant nodes and edges as possible, while keeping the graph size manageable.This approach offers two key benefits: Firstly, it helps to filter out nodes and edges that are not pertinent to the query.This is crucial because irrelevant information can overshadow the useful data, potentially diverting the focus of the subsequent LLM from the information of interest.Secondly, it enhances efficiency; by keeping the graph size manageable, it becomes feasible to translate the graph into natural language and then input it into the LLM for processing.The Prize-Collecting Steiner Tree algorithm [2] serves as our primary method for identifying such optimally sized and relevant subgraphs.See Step 3 in Figure 3.</p>
<p>Prize-Collecting Steiner Tree (PCST).The PCST problem aims to find a connected subgraph that maximizes the total prize values of its nodes while minimizing the total costs of its edges.Our approach assigns higher prize values to nodes and edges more relevant to the query, as measured by cosine similarity.Specifically, the top k nodes/edges are assigned descending prize values from k down to 1, with the rest assigned zero.The node prize assignment is as follows:
prize(n) = k − i, if n ∈ V k and n is the top i node, 0, otherwise.(6)
Edge prizes are assigned similarly.The objective is to identify a subgraph, S * = (V * , E * ), that optimizes the total prize of nodes and edges, minus the costs associated with the size of the subgraph:
S * = argmax S⊆G, S is connected n∈V S prize(n) + e∈E S prize(e) − cost(S),(7)
where cost(S)
= |E S | × C e ,(8)
and C e denotes a predefined cost per edge, which is adjustable to control the subgraph size.</p>
<p>The original PCST algorithm is designed for node prizes only.However, given the significance of edge semantics in certain scenarios, we adapt the algorithm to accommodate edge prizes as follows: Consider an edge e with a cost C e and a prize P e .If C e &gt; P e , it can be treated as a reduced edge cost of C e − P e .However, if P e &gt; C e , negative edge costs are not allowed in the original algorithm.Our solution involves replacing edge e with a 'virtual node' v e , connected to both endpoints of e.This virtual node is assigned a prize of P e − C e , and the cost of the two new edges leading to the virtual node is set to zero.This modification effectively mirrors the original problem, as including edge e in the original graph is analogous to including the virtual node in the modified graph.Finally, we optimize the PCST problem using a near-linear time approach [9].</p>
<p>Answer Generation</p>
<p>Graph Encoder.Let S * = (V * , E * ) represent the retrieved subgraph.We use a graph encoder to model the structure of this graph, specifically using a standard Graph Attention Network (GAT) [43].Our approach for encoding the retrieved subgraph is defined as follows:
h g = POOL(GNN ϕ1 (S * )) ∈ R dg ,(9)
Here, POOL denotes the mean pooling operation, and d g is the dimension of the graph encoder.</p>
<p>Projection Layer.We incorporate a multilayer perceptron (MLP) to align the graph token with the vector space of the LLM:
ĥg = MLP ϕ2 (h g ) ∈ R d l ,(10)
where d l is the dimension of the LLM's hidden embedding.</p>
<p>Text Embedder.To leverage the text-reasoning capabilities of LLMs, we transform the retrieved subgraph S * into a textual format.This transformation involves flattening the textual attributes of the nodes and edges, as illustrated in the green box in Figure 2. We refer to this operation as textualize(•).Subsequently, we combine the textualized graph with the query to generate a response.Let x q denote the query; we concatenate it with the textualized graph textualize(S * ).We then map the result to an embedding h t using a text embedder, which is the first layer of a pretrained and frozen LLM:
h t = TextEmbedder([textualize(S * ); x q ]) ∈ R L×d l ,(11)
where [; ] represents the concatenation operation, and L is the number of tokens.</p>
<p>LLM Generation with Graph Prompt Tuning.The final stage involves generating the answer Y given the graph token ĥg , acting as a soft prompt, and the text embedder output h t .These inputs are fed through the self-attention layers of a pretrained frozen LLM, with parameter θ.The generation process is represented as follows:
p θ,ϕ1,ϕ2 (Y |S * , x q ) = r i=1 p θ,ϕ1,ϕ2 (y i |y &lt;i , [ ĥg ; h t ]),(12)
where [ ĥg ; h t ] concatenates the graph token ĥg and the text embedder output h t .While θ is frozen, the graph token ĥg receives gradients, enabling the optimization of the parameters of the graph encoder ϕ 1 and the projection layer ϕ 2 through standard backpropagation.</p>
<p>6 Experiments</p>
<p>Experiment Setup</p>
<p>In the indexing step, we use SentenceBert [34] as the LM to encode all node and edge attributes.In the generation step, we use the open-source Llama2-7b [42] as the LLM and Graph Transformer [37] as the graph encoder.Additional details are provided in Appendix B.1.</p>
<p>Main Results</p>
<p>In our experiments, we consider three model configurations: 1) Inference-only: Using a frozen LLM for direct question answering; 2) Frozen LLM w/ prompt tuning (PT): Keeping the parameters of the LLM frozen and adapting only the prompt; 3) Tuned LLM: Fine-tuning the LLM with LoRA [10].</p>
<p>We provide more details in Appendix B.2. Table 3 demonstrates the effectiveness of our method across three datasets in various configurations.</p>
<p>In the inference-only setting, G-Retriever surpasses all baselines.Notably, LLM can perform even better when no graph knowledge is provided (i.e., question only), which might be attributed to the complexity and potential noise in the knowledge.For frozen LLM with prompt tuning, G-Retriever outperforms traditional prompt tuning and GraphToken [31], a graph prompt tuning-based method, with average performance increases of 40.6% and 30.8% respectively.Furthermore, when tuned with LoRA, G-Retriever achieves the best performance.</p>
<p>Efficiency Evaluation</p>
<p>The efficiency of our approach is highlighted by the data in Table 4. Implementing our graph-based retrieval significantly decreases the number of tokens required to describe the graphs in text, reduces the number of nodes in graphs, and speeds up the training process.Specifically, for the SceneGraphs dataset, tokens decreased by 83%, nodes by 74%, and training time by 29%.For the WebQSP dataset, tokens decreased by 99%, nodes by 99%, and training time by 67%.These substantial reductions demonstrate the method's efficiency and potential in managing large-scale graph data.</p>
<p>Mitigation of Hallucination</p>
<p>To evaluate hallucination, we instructed the models to answer graph-related questions, specifically identifying supporting nodes or edges from the graph.We manually reviewed 100 responses from both our method and the baseline (i.e., LLM with graph prompt tuning), verifying the existence of the nodes and edges referenced in the model's output within the actual graph.Table 5 shows that G-Retriever significantly reduces hallucinations by 54% compared to the baseline, as our graph retrieval ensures that the data is sourced directly from the actual graph, leading to less hallucination.See Appendix F for details.</p>
<p>Ablation Study</p>
<p>In this ablation study, we assess the individual impact of key components within our pipeline.As shown in Table 6, there are performance drops when any of these components are removed, with the graph encoder and textualized graph showing declines of 22.51% and 19.19%, respectively.This demonstrates their complementary effects in representing the graph in both textual and embedded formats.Additionally, the retrieval on graphs is also important to the overall performance.Further details are available in Appendix B.3.We also present additional studies on our framework: it is robust to the choice of graph encoders (see Appendix B.4) and benefits from the increased scale of LLMs (see Appendix B.5).</p>
<p>Additionally, we include a detailed comparison with existing retrieval methods (see Appendix D), a discussion on the complexity (see Appendix E), and demonstrations on how to use G-Retriever to 'chat with your graph' (see Appendix G).</p>
<p>Conclusion</p>
<p>In this work, we introduce a new GraphQA benchmark for real-world graph question answering and present G-Retriever, an architecture adept at complex and creative queries.Experimental results show that G-Retriever surpasses baselines in textual graph tasks across multiple domains, scales effectively with larger graph sizes, and demonstrates resistance to hallucination.</p>
<p>Limitations and Future Work: Currently, G-Retriever employs a static retrieval component.Future developments could investigate more sophisticated RAG where the retrieval is trainable.</p>
<p>A Impact Statements</p>
<p>As LLMs are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital.Our work aims to enhance LLMs' ability to interact with graph-structured data, while resisting hallucination, thus improving model reliability.We also enhance explainability, both by returning the retrieved subgraph, and through the use of conversational interfaces for 'chatting with a graph', which allows for better human-AI interaction and for models to behave in a way that is more well-aligned with human expectations.</p>
<p>B Experiment B.1 Implementation Settings</p>
<p>Experiments are conducted using 2 NVIDIA A100-80G GPUs.Each experiment is replicated four times, utilizing different seeds for each run to ensure robustness and reproducibility.</p>
<p>Graph Encoder.We use Graph Transformer [37] as the GNN backbone.Our configuration employs 4 layers, each with 4 attention heads, and a hidden dimension size of 1024.</p>
<p>LLM.</p>
<p>We use the open-sourced Llama2-7b [42] as the LLM backbone.In fine-tuning the LLM with LoRA [10], the lora_r parameter (dimension for LoRA update matrices) is set to 8, and lora_alpha (scaling factor) is set to 16.The dropout rate is set to 0.05.In prompt tuning, the LLM is configured with 10 virtual tokens.The number of max text length is 512, the number of max new tokens, i.e., the maximum numbers of tokens to generate, is 32.</p>
<p>PCST.For retrieval over graphs via PCST, for the SceneGraphs dataset, we select the top k nodes and edges, setting k to 3. Here, the cost of edges, denoted as C e , is set to 1. Regarding the WebQSP dataset, we set k = 3 for nodes and k = 5 for edges, with the edge cost, C e , adjusted to 0.5.For the ExplaGraphs dataset, which is characterized by a small graph size averaging 5.17 nodes and 4.25 edges (as detailed in Table 2), the entire graph can fit in the LLM's context window size.Consequently, we aim to retrieve the whole graph by setting k to 0, effectively returning the original graph unaltered.</p>
<p>Optimization.We use the AdamW [27] optimizer.We set the initial learning rate at 1e-5, with a weight decay of 0.05.The learning rate decays with a half-cycle cosine decay after the warm-up period.The batch size is 4, and the number of epochs is 10.To prevent overfitting and ensure training efficiency, an early stopping mechanism is implemented with a patience setting of 2 epochs.</p>
<p>B.2 Details of Model Configurations</p>
<p>In our experiments, we consider three model configurations:</p>
<p>1) Inference-only: Using a frozen LLM for direct question answering with textual graph and question, see Figure 4. • Zero-shot.In this approach, the model is given a textual graph description and a task description, and is immediately asked to produce the desired output.No additional examples or demonstrations are provided.</p>
<p>• Zero-CoT.Zero-shot Chain-of-thought (Zero-CoT) prompting [18] is a follow-up to CoT prompting [45], which introduces an incredibly simple zero shot prompt by appending the words "Let's think step by step." to the end of a question.</p>
<p>• CoT-BAG.Build-a-Graph Prompting (BAG) [44] is a prompting technique that adds "Let's construct a graph with the nodes and edges first."after the textual description of the graph is explicitly given.</p>
<p>• KAPING.KAPING [1] is a zero-shot knowledge-augmented prompting method for knowledge graph question answering.It first retrieves triples related to the question from the graph, then prepends them to the input question in the form of a prompt, which is then forwarded to LLMs to generate the answer.</p>
<p>2) Frozen LLM w/ prompt tuning (PT): Keeping the parameters of the LLM frozen and adapting only the prompt.This includes soft prompt tuning (see Figure 5a), GraphToken [31], which is a graph prompt tuning method, and our G-Retriever method (see Figure5b).3) Tuned LLM: Fine-tuning the LLM with LoRA.This includes standard fine-tuning of an LLM for downstream tasks using LoRA (see Figure 6a) and G-Retriever with LoRA (see Figure 6b).</p>
<p>B.3 Details of Ablation Study</p>
<p>This section illustrates the modifications made to the original architecture in the ablation study, as presented in Figure 7.</p>
<p>Without Graph Encoder (w/o GraphEncoder): In this setting, we replaced the graph encoder with trainable soft tokens, setting the number of these virtual tokens to 10.</p>
<p>Without Projection Layer (w/o Projection Layer): Here, we removed the projection layer following the graph encoder.We configured the output dimension of the graph encoder to be 4,096, matching the hidden dimension of Llama2-7b.This allows the output graph token (the yellow token in Figure 7b) to be concatenated directly with the LLM tokens (blue tokens).</p>
<p>Without Textualized Graph (w/o Textualized Graph):</p>
<p>In this configuration, we modified the textual input to the LLM.Rather than using a combination of the question and the textualized graph, we solely used the question.</p>
<p>B.4 The Choice of Graph Encoder</p>
<p>In addition to the Graph Transformer [37], we explore other GNNs as the graph encoder, such as GCN [17] and the GAT [43].The comparative results of these models on the WebQSP and ExplaGraphs datasets are presented in Table 7.</p>
<p>Table 7: Performance of different graph encoders on the WebQSP and ExplaGraphs datasets.</p>
<p>Graph Encoder</p>
<p>WebQSP ExplaGraphs GCN [17] 70.70 0.8394 GAT [43] 70.27 0.8430 Graph Transformer [37] 70.490.8516</p>
<p>The results demonstrate that our proposed method exhibits consistent robustness across different graph encoders.Notably, all three encoders -GCN, GAT, and GraphTransformer -demonstrate competitive and closely aligned performance on the WebQSP dataset, with Hit@1 scores of 70.70, 70.27, and 70.49, respectively.However, the performance differentiation becomes more pronounced on the ExplaGraphs dataset, where GraphTransformer exhibits a superior Hit@1 score of 0.8516, followed by GAT and GCN with scores of 0.8430 and 0.8394, respectively.This variation in performance across the datasets highlights the importance of encoder selection based on the specific characteristics and requirements of the dataset.</p>
<p>B.5 The Choice of LLM</p>
<p>As for the choice of LLM, we considered both Llama2-7b and Llama2-13b.Our experiments demonstrate that stronger LLMs enhance the effectiveness of our method, as shown in Table 8, indicating it the scale of the LLMs.</p>
<p>C GraphQA Benchmark</p>
<p>In this section, we detail how our GraphQA benchmark differs from the original datasets, including the specific processing steps we employed.For concrete examples that illustrate the differences between the raw text in the original dataset and in our GraphQA benchmark, please refer to Table 9.</p>
<p>ExplaGraphs.The original dataset2 [35] represents relationships using triplets.We have standardized this format by converting the triplets into a graph representation.Specifically, each and tail in a triplet is transformed into a node, and the relation is transformed into an edge.Since the test dataset labels are not available, we have utilized only the training and validation (val) datasets from the original collection.We further divided these into training, val, and test subsets, using a 6:2:2 ratio.SceneGraphs "width": 500, "objects": "681267": "name": "banana", "h": 34, "relations": ["object": "681262", "name": "to the left of"], "w": 64, "attributes": ["small", "yellow"], "y": 55, "x": 248, "681265": "name": "spots", "h": 16, "relations": [], "w": 26, "attributes": [], "y": 92, "x": 245, "681264": "name": "bananas", "h": 50, "relations": ["object": "681259", "name": "to the left of"], "w": 49, "attributes": ["small", "yellow"], "y": 32, "x": 268, "681263": "name": "picnic", "h": 374, "relations": [], "w": 499, "attributes": ["delicious"], "y": 0, "x": 0, "681262": "name": "straw", "h": 95, "relations": ["object": "681268", "name": "to the right of", "object": "681267", "name": "to the right of", "object": "681253", "name": "to the right of"], "w": 15, "attributes": ["white", "plastic"], "y": 55, "x": 402, "681261": "name": "meat", "h": 27, "relations": ["object": "681255", "name": "on", "object": "681255", "name": "inside"], "w": 24, "attributes": ["small", "brown", "delicious"], "y": 123, "x": 68, "681260": "name": "rice", "h": 57, "relations": ["object": "681255", "name": "on", "object": "681258", "name": "to the left of"], "w": 93, "attributes": ["piled", "white"], "y": 162, "x": 57, "681269": "name": "onions", "h": 16, "relations": [], "w": 24, "attributes": ["green"], "y": 147, "x": 90, "681268": "name": "tablecloth", "h": 374, "relations": ["object": "681262", "name": "to the left of"], "w": 396, "attributes":</p>
<p>["white"], "y": 0, "x": 0, "681258": "name": "bowl", "h": 99, "relations": ["object": "681255", "name": "next to", "object": "681257", "name": "of", "object": "681255", "name": "near", "object": "681256", "name": "to the right of", "object": "681260", "name": "to the right of", "object": "681255", "name": "to the right of"], "w": 115, "attributes": ["full"], "y": 184, "x": 178, "681259": "name": "plantains", "h": 70, "relations": ["object": "681264", "name": "to the right of"], "w": 45, "attributes":</p>
<p>["red"], "y": 0, "x": 346, "681256": "name": "spoon", "h": 65, "relations": ["object": "681255", "name": "on", "object": "681257", "name": "to the left of", "object": "681255", "name": "in", "object": "681258", "name": "to the left of"], "w": 140, "attributes": ["large", "metal", "silver"], "y": 196, "x": 0, "681257": "name": "dish", "h": 81, "relations": ["object": "681258", "name": "inside", "object": "681256", "name": "to the right of", "object": "681258", "name": "in", "object": "681255", "name": "to the right of"], "w": 108, "attributes": ["cream colored"], "y": 199, "x": 187, "681254": "name": "meal", "h": 111, "relations": [], "w": 130, "attributes": [], "y": 121, "x": 58, "681255": "name": "plate", "h": 138, "relations": ["object": "681257", "name": "to the left of", "object": "681254", "name": "of", "object": "681254", "name": "with", "object": "681258", "name": "near", "object": "681258", "name": "to the left of"], "w": 176, "attributes": ["white", "full"], "y": 111, "x": 30, "681253": "name": "banana", "h": 30, "relations": ["object": "681262", "name": "to the left of"], "w": 73, "attributes": ["small", "yellow"], "y": 87, "x": 237, "height": 375 node_id,node_attr 0,"name: banana; attribute: small, yellow  (x,y,w,h): (0, 196, 140, 65)" 12,"name: dish; attribute: cream colored; (x,y,w,h): (187, 199, 108, 81)" 13,"name: meal; (x,y,w,h): (58, 121, 130, 111)" 14,"name: plate; attribute: white, full; (x,y,w,h): (30,111,176,138)" 15,"name: banana; attribute: small, yellow; (x,y,w,h): (237, 87, 73, 30)" src,edge_attr,dst 0,to the left of,4\n 2,to the left of,10\n 4,to the right of,8\n 4,to the right of,0\n 4,to the right of,15\n 5,on,14\n 5,inside,14\n 6,on,14\n 6,to the left of,9\n 8,to the left of,4\n 9,next to,14\n 9,of,12\n 9,near,14\n 9,to the right of,11\n 9,to the right of,6\n 9,to the right of,14\n 10,to the right of,2\n 11,on,14\n 11,to the left of,12\n 11,in,14\n 11,to the left of,9\n 12,inside,9\n 12,to the right of,11\n 12,in,9\n 12,to the right of,14\n 14,to the left of,12\n 14,of,13\n 14,with,13\n 14,near,9\n 14,to the left of,9\n 15,to the left of,4\n discrimination between capital and lowercase words, we have converted all words to lowercase.We used the same dataset split as in the original dataset.</p>
<p>D Graph Retrieval-Augmented Generation (GraphRAG) D.1 Comparison with Existing GraphRAG Methods</p>
<p>Most existing GraphRAG methods are designed specifically for knowledge graphs and focus on node, edge, or triples-level retrieval [1,36,16,38].Our method is different in two main ways: 1) It focuses on more general textual graphs, not just knowledge graphs.2) It enables the return of a subgraph most closely related to a query, rather than a list of top-k triples.The triples in other methods are chosen in isolation from the graph, failing to capture neighborhood information effectively.In contrast, our method takes the context into account during retrieval.</p>
<p>D.2 The Impact of K for Retrieval</p>
<p>We identify the most relevant nodes and edges and use a k-nearest neighbors retrieval approach (see Equation 6).Small k values may omit crucial knowledge or information relevant to the query, while large k values could introduce excessive information, distracting the model from the essential details.To evaluate the impact of the number of k, we have conducted additional experiments by varying the choice of k to 3, 5, 10, and 20.</p>
<p>Table 10: The impact of k on the webqsp dataset.Hit@1 0.6977 0.7063 0.7248 0.7039</p>
<p>As shown in Table 10, the Hit@1 metric initially rises for small k values, peaks at a certain point, and then declines for large k values.Determining the optimal k value can be achieved through techniques like cross-validation using a validation set.</p>
<p>D.3 The Choice of Similarity Function</p>
<p>The choice of similarity function is also important.In this work, we use cosine similarity, a widely adopted metric for measuring vector similarity in models that process vision and language.For instance, CLIP also employs cosine similarity to assess the similarity between text and image features.Although it might not be the optimal choice, we believe that cosine similarity is a general, representative, and valid choice for facilitating fast retrieval tasks.</p>
<p>D.4 The Quality of Retrieval</p>
<p>We quantify the quality of our retrieval method as follows: We examine the retrieval subgraph, and if the label is contained within it, we consider it a successful retrieval.We calculate the retrieval success rate of our method and the retrieval method proposed in KAPING [1] on the WebQSP dataset.As shown in Table 11, these results validate the effectiveness of our retrieval method.In contrast to the triple-based retrieval in KAPING, which relies on the similarity between triples and the query, our PCST-based subgraph retrieval is more accurate as it takes the graph structure into account.By design, it retrieves connected subgraphs, capturing not just nodes or edges with high relevance scores, but also those that act as "bridges" connecting other highly relevant nodes and edges.</p>
<p>E Discussion on the Complexity E.1 The integration of GNNs, LLMs and GraphRAG G-Retriever is framework integrate the strengths of GNNs, LLMs and GraphRAG.The LLM+X framework, which involves enriching LLMs with multi-modal capabilities by integrating an LLM with an encoder from another modality, is a widely adopted approach.Notable examples include Llava, MiniGPT-4, and Flamingo, among others.They are not complex in terms of understanding or implementation.Regarding the integration of GraphRAG, it does not require training and can be implemented during the preprocessing stage or on the fly.This approach does not significantly increase time complexity or computational complexity.On the contrary, it can substantially reduce the size of the graph (e.g., eliminating 99% of nodes in the WebQSP dataset), which in turn speeds up the overall running time (e.g., reducing it from 18.7 min/epoch to 6.2 min/epoch on the WebQSP dataset).</p>
<p>E.2 Computational Resources</p>
<p>Utilizing two A100 GPUs, each with 80GB of memory, we conducted tests on Llama2-7b and WebQSP datasets.Our experiments had a training batch size of 16 and an evaluation batch size of 32, yielding the following results.</p>
<p>F Hallucination in Graph LLMs</p>
<p>In this section, we present quantitative results regarding hallucinations in the SceneGraphs dataset.</p>
<p>Baseline.For our baseline, we adapted MiniGPT-4 [57] to graph contexts.This approach involves a frozen LLM interacting with a trainable GNN that encodes graph data as a soft prompt, denoted as LLM+Graph Prompt Tuning.We focus on graph prompt tuning as the baseline, instead of converting the graph into text, since the textual representation of the graph is large and consistently exceeds the input token limits of LLMs.</p>
<p>Experiment Design.We instructed the LLM to answer graph-related questions and to list nodes or edges in the explanation graph that support its answers.Since standard answers for these questions do not exist, allowing the LLM to respond flexibly, it becomes challenging to evaluate its responses.</p>
<p>To address this, we manually examined 100 responses generated by our method and the LLM with graph prompt tuning, verifying whether the nodes and edges referenced in the LLM's output actually exist in the graph.</p>
<p>Evaluation Metrics.We assessed the model's faithfulness using three metrics: the fraction of valid nodes (denoted as Valid Nodes), the fraction of valid edges (denoted as Valid Edges), and the fraction of times the entire set of nodes and edges cited was valid (denoted as Fully Valid Graphs).</p>
<p>Results.The results, as depicted in Table 5, illustrate the comparative effectiveness of the G-Retriever over the baseline LLM+Graph Prompt Tuning method in reducing hallucinations.The LLM+Graph Prompt Tuning approach demonstrated a significantly lower accuracy in referencing graph elements, with only 31% of nodes and 12% of edges being valid, and the entire set of nodes and edges being valid only 8% of the time.In contrast, G-Retriever showed substantial improvements: 77% validity in nodes, 76% in edges, and 62% in the overall validity of referenced node-edge sets.These results underscore the significant reduction in hallucinations with G-Retriever, particularly in the challenging task of accurately citing both nodes and edges in graph-based contexts.</p>
<p>G Demonstrations</p>
<p>We demonstrate the interaction capabilities of G-Retriever with creative questions on different datasets: ExplaGraphs, SceneGraphs, and WebQSP.These examples are showcased in Tables 13,  14, and 15, respectively.Additionally, the examples are visualized in Figure 1.User Write a 500-word argument essay based on the information provided in the explanation graph.</p>
<p>Model The explanation graph provides a detailed analysis of the reasons why women undergo cosmetic surgery.According to the graph, the main reasons include the desire to change one's appearance, the influence of societal beauty standards, and the addiction to searching for perfection.These reasons are further supported by the various nodes and edges in the graph, which provide additional insights into the motivations and consequences of cosmetic surgery.Firstly, the node labeled "human" highlights the fundamental human desire to feel attractive and confident in one's appearance.This desire is deeply ingrained in our culture, where beauty is often associated with happiness, success, and social acceptance.As a result, many women feel pressure to conform to societal beauty standards, which can lead to feelings of insecurity and low self-esteem.In response to these feelings, some women may turn to cosmetic surgery as a means of improving their appearance and boosting their self-confidence.Secondly, the node labeled "women" highlights the gendered nature of cosmetic surgery.While men also undergo cosmetic surgery, the majority of cosmetic surgery patients are women.This is likely due to the fact that women are more likely to be exposed to societal beauty standards and are more likely to internalize these standards.As a result, women may be more likely to feel insecure about their appearance and more likely to seek cosmetic surgery as a means of improving their looks.Thirdly, the node labeled "addiction" highlights the potential negative consequences of cosmetic surgery.While some women may initially feel improved confidence and self-esteem after undergoing cosmetic surgery, the graph suggests that this initial boost may not last.In fact, some women may become addicted to the constant pursuit of perfection, leading to a cycle of surgery and dissatisfaction.This can have serious psychological and emotional consequences, including depression, anxiety, and a loss of self-esteem.Fourthly, the node labeled "searching for perfection" highlights the unrealistic expectations that many women have about the results of cosmetic surgery.The graph suggests that women may have unrealistic expectations about the ability of cosmetic surgery to improve their appearance and boost their self-esteem.This can lead to disappointment and dissatisfaction with the results of surgery, as well as a continued pursuit of perfection through additional surgery.Finally, the node labeled "stop" highlights the importance of critical thinking and self-reflection when it comes to cosmetic surgery.The graph suggests that women should be aware of the potential negative consequences of cosmetic surgery and should carefully consider the reasons why they are undergoing surgery.They should also be aware of the unrealistic expectations that they may have about the results of surgery and should be prepared to address any negative emotions or feelings of dissatisfaction that may arise.</p>
<p>In conclusion, the explanation graph provides a detailed analysis of the reasons why women undergo cosmetic surgery.The graph highlights the fundamental human desire to feel attractive and confident in one's appearance, the gendered nature of cosmetic surgery, the potential negative consequences of cosmetic surgery, and the importance of critical thinking and self-reflection when it comes to surgery.By understanding these reasons and the potential consequences of cosmetic surgery, women can make informed decisions about whether or not to undergo surgery and can work to address any negative emotions or feelings of dissatisfaction that may arise.Discover how Franklin's stove revolutionized heating systems and how it helped to reduce fuel costs for households and businesses.Connection: Franklin's stove was an early example of energy efficiency and sustainability, and it paved the way for modern heating systems.4. Prokop Diviš: Learn about Franklin's connection to Prokop Diviš, a Czech inventor who built on Franklin's work and developed the first electric generator.Discover how Diviš's invention helped to power the Industrial Revolution and how it changed the way people lived and worked.Connection: Franklin's work on the lightning rod inspired Diviš to explore the properties of electricity, and their collaborative efforts helped to advance the field of electrical engineering.5. Innovator and Original Ideas: Explore Franklin's reputation as an innovator and his ability to come up with original ideas.Discover how Franklin's creativity and curiosity led to many of his inventions and how he was able to see connections between seemingly unrelated things.Connection: Franklin's innovative spirit and ability to think outside the box inspired many other inventors and scientists, including Thomas Edison and Nikola Tesla.6. Collaboration and Networking: Highlight Franklin's collaborative approach to invention and his ability to network with other inventors and scientists.Discover how Franklin's connections with other innovators helped to advance the field of science and technology.Connection: Franklin's collaborations with other inventors and scientists, such as Thomas Jefferson and Prokop Diviš, helped to accelerate the pace of innovation and pave the way for future breakthroughs.7. Legacy and Impact: Reflect on Franklin's lasting legacy and the impact of his inventions on modern society.Discover how Franklin's inventions continue to inspire new generations of inventors and scientists and how they have helped to shape the world we live in today.Connection: Franklin's inventions and ideas have had a lasting impact on many fields, including science, technology, and society as a whole.His legacy continues to inspire and motivate people around the world.By following this educational tour through the knowledge graph, students will gain a deeper understanding of Benjamin Franklin's inventions and their impact on modern society.They will also learn about Franklin's collaborative approach to invention, his innovative spirit, and his ability to think outside the box.</p>
<p>Figure 1 :
1
Figure 1: We develop a flexible question-answering framework targeting real-world textual graph applications via a unified conversational interface.Presented here are examples showcasing the model's adeptness in handling generative and creative queries in practical graph-related tasks: common sense reasoning, scene understanding, and knowledge graph reasoning, respectively.</p>
<p>Figure 2 :
2
Figure 2: Illustrative examples from the GraphQA benchmark datasets.</p>
<p>Figure 3 :
3
Figure3: Overview of the proposed G-Retriever: 1) Indexing: Graphs are indexed for efficient query processing; 2) Retrieval: The most semantically relevant nodes and edges are retrieved, conditioned on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An answer is generated using a 'graph prompt', a textualized graph, and the query.</p>
<p>Figure 4 :
4
Figure 4: Model configuration 1) Inference-only.</p>
<p>Figure 5 :
5
Figure 5: Model configuration 2) Frozen LLM w/ prompt tuning.</p>
<p>is the name of justin bieber brother?Answer: trainable (b) G-Retriever w/ LoRA</p>
<p>Figure 6 :
6
Figure 6: Model configuration 3) Tuned LLM.</p>
<p>is the name of justin bieber brother?Answer:(c) w/o Textualized Graph</p>
<p>Figure 7 :
7
Figure 7: Ablation study configurations.</p>
<p>, dst 0,to the left of,11\n 0,to the right of,13\n 3,to the left of,6\n 6,to the right of,3\n 6,grazing on,10\n 6,in,9\n 6,in,10\n 7,above,5\n 8,eating,10\n 8,to the right of,11\n 8,to the right of,12\n 11,to the left of,8\n 11,to the right of,0\n 11,to the right of,13\n 11,to the right of,12\n 11,eating,10\n 12,eating,10\n 12,to the left of,8\n 12,to the right of,13\n 12,to the left of,11\n 13,to the left of,0\n 13,to the left of,11\n 13,to the left of,12\n 13,eating,10\nImageUserWrite an 500-word advertisement for this place in the scene graph that would make people want to visit it.Model Sure! Here's an example of a 500-word advertisement for the place based on the scene graph: "Experience the breathtaking beauty of our place, where the rolling hills meet the sparkling water of the swimming pool.Take a leisurely stroll through the lush green fields, and let the gentle breeze caress your face as you gaze out at the picturesque view.The sheep graze peacefully in the distance, their white coats glistening in the sunlight.The sky is a brilliant blue, with fluffy white clouds drifting lazily across.Come and bask in the serene ambiance of our place, where the natural beauty will leave you in awe."</p>
<p>Table 1 :
1
Observation and mitigation of hallucination in graph LLMs.
Userelephants eatinginbushesstanding by elephantgrasson the side ofriver</p>
<p>Table 2 :
2
Summary of datasets used in GraphQA benchmark.
DatasetExplaGraphsSceneGraphsWebQSP#Graphs2,766100,0004,737Avg. #Nodes5.1719.131370.89Avg. #Edges4.2568.444252.37Node AttributeCommonsense conceptsObject attributes (e.g., color, shape)Entities in FreebaseEdge AttributeCommonsense relationsRelations (e.g., actions, spatial relations)Relations in FreebaseTaskCommon sense reasoningScene graph question answeringKnowledge based question answeringEvaluation MatrixAccuracyAccuracyHit@1Explanation GraphScene GraphKnowledge Graphcitizenswomen and men causes is awomen causesdesire capable of help the countryto the right ofperson sittingin front ofsibling_sjustin bieberparentsjeremy children bieberhave same rightsbe in combatwomancomputerm.0gxnnwpsiblingjaxon bieberTextualized GraphTextualized GraphTextualized Graphnode_id, node_attrsrc, edge_attr, dstnode_id, node_attrnode_id, node_attrsrc, edge_attr, dst0, women and men0, is a,12, name: computer; (x,y,w,h): (8, 119, 34, 32)15, justin bieber15, people.person.parents, 3561, citizens1, causes, 23, name: person; attribute: sitting; (x,y,w,h): (169, 75, 49, 40)294, jaxon bieber15, people.person.sibling_s, 5512, have same rights2, causes, 315, name: woman; (x,y,w,h): (255, 18, 235, 292)356, jeremy bieber356, people.person.children, 2943, women3, capable of, 4src, edge_attr, dst551, m.0gxnnwp551, people.sibling_relationship.sibling, 2944, help the country4, desires, 515, to the right of, 35, be in combat2, in front of, 3QuestionQuestionQuestionArguement 1: Women should not be in combat.Question: Is there a woman to the right of the person behind theQuestion: what is the name of justin bieber brotherArguement 2: Women and men have the same rights.computer?Question: Do argument 1 and argument 2 support or counter each other?Answer in one word in the form of 'support' or 'counter'.AnswerAnswerAnswercounteryesjaxon bieber</p>
<p>Table 3 :
3
Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings.Mean scores and standard deviations (mean ± std) are presented.The first best result for each task is highlighted in bold and the second best result is highlighted with an underline.
SettingMethodExplaGraphsSceneGraphsWebQSPZero-shot0.56500.397441.06Inference-onlyZero-CoT [18] CoT-BAG [44]0.5704 0.57940.5260 0.568051.30 39.60KAPING [1]0.62270.437552.64Prompt tuning0.5763 ± 0.0243 0.6341 ± 0.0024 48.34 ± 0.64Frozen LLM w/ PTGraphToken [31] G-Retriever0.8508 ± 0.0551 0.4903 ± 0.0105 57.05 ± 0.74 0.8516 ± 0.0092 0.8131 ± 0.0162 70.49 ± 1.21∆Prompt tuning↑ 47.77%↑ 28.23%↑ 45.81%LoRA0.8538 ± 0.0353 0.7862 ± 0.0031 66.03 ± 0.47Tuned LLMG-Retriever w/ LoRA 0.8705 ± 0.0329 0.8683 ± 0.0072 73.79 ± 0.70∆ LoRA↑ 1.95%↑ 11.74%↑ 10.44%</p>
<p>Table 4 :
4
Retrieval on graphs significantly improves efficiency.
DatasetBefore Retrieval (Avg.)After Retrieval (Avg.)# Tokens # Nodes Min/Epoch# Tokens# NodesMin/EpochSceneGraphs1,39619123.1235 (↓83%)5 (↓74%)86.8 (↓29%)WebQSP100,6271,37118.7610 (↓99%) 18 (↓99%)6.2(↓67%)</p>
<p>Table 5 :
5
Quantitative comparison of hallucination on the SceneGraphs dataset.
Baseline G-RetrieverValid Nodes31%77%Valid Edges12%76%Fully Valid Graphs 8%62%</p>
<p>Table 6 :
6
Ablation study on the WebQSP dataset.
MethodHit@1∆G-Retrieverw/o Graph Encoder54.62 ± 0.78 ↓ 22.51%w/o Projection Layer69.70 ± 0.68 ↓ 1.11%w/o Textualized Graph 56.96 ± 1.83 ↓ 19.19%w/o Retrieval63.84 ± 0.41 ↓ 9.43%</p>
<p>Table 8 :
8
Performance of different LLMs on the WebQSP dataset.
LLMLlama2-7b Llama2-13bHit@170.4975.58</p>
<p>Table 9 :
9
Comparison of text formats in original datasets and our GraphQA benchmark.
DatasetOriginal datasetGraphQA Benmarknode_id,node_attr\n 0,entrapment\n 1,being abused\n 2,police\n 3,harm\n 4,people\nExplaGraphs(entrapment; capable of; being abused) (being abused; created by; police) (police; capable of; harm) (harm; used for; people) (people; part of; citizens)5,citizens\n src,edge_attr,dst\n 0,capable of,1\n 1,created by,2\n 2,capable of,3\n 3,used for,4\n4,part of,5\n</p>
<p>Table 11 :
11
The quality of retrieval methods on the WebQSP dataset.
MethodRetrieval AccuracyKAPING [1] (top-k triple retrieval)60.81G-Retriever70.49</p>
<p>Table 12 :
12
Performance and Efficiency of Various Methods on the WebQSP dataset.
SetttingMethodHit@1 TimeInference-onlyQuestion only Textual graph and question 41.06 61.1631 min 40 minFrozen LLM w/ PTPrompt Tuning G-Retriever48.34 70.4918.7 min/epoch 6.2 min/epochTuned LLMLoRA G-Retriever w/ LoRA66.03 73.7919 min/epoch 6.9 min/epochThese results highlight efficiency improvements via graph RAG, which significantly reduces graphsize (e.g., eliminating 99% of nodes in the WebQSP dataset) and speeds up running time.</p>
<p>Table 13 :
13
Example of creative chat on ExplaGraphs.
Graph node_id,node_attr0,human1,cosmetic surgery2,women3,change appearance4,addiction5,searching for perfection6,stop7,positivesrc,edge_attr,dst0,receives action,12,part of,01,used for,33,capable of,44,created by,54,not capable of,64,is not a,7</p>
<p>Table 14 :
14
Example of creative chat on SceneGraphs.We utilize GPT-4 to generate an image from the graph, assisting readers in visualizing it</p>
<p>Graph node_id,node_attr 0,name: head; attribute: black; (x,y,w,h): (386, 185, 8, 4) 1,name: swimming pool; attribute: small, water; (x,y,w,h): (465, 92, 35, 3) 2,name: shore; (x,y,w,h): (49, 85, 436, 108) 3,name: fence; attribute: wired; (x,y,w,h): (11, 158, 167, 46) 4,name: mountains; attribute: covered; (x,y,w,h): (393, 53, 107, 12) 5,name: water; attribute: blue; (x,y,w,h): (0, 50, 499, 154) 6,name: sheep; attribute: walking, grazing; (x,y,w,h): (243, 191, 20, 13) 7,name: sky; attribute: blue, clear, cloudy; (x,y,w,h): (0, 0, 501, 60) 8,name: sheep; attribute: white, fluffy; (x,y,w,h): (409, 176, 13, 10) 9,name: field; attribute: large, green, grassy; (x,y,w,h): (0, 138, 503, 112) 10,name: grass; (x,y,w,h): (0, 84, 500, 166) 11,name: sheep; attribute: white, fluffy; (x,y,w,h): (398, 178, 13, 11) 12,name: sheep; attribute: white, fluffy; (x,y,w,h): (385, 179, 17, 11) 13,name: sheep; attribute: white, fluffy; (x,y,w,h): (364, 181, 14, 10) 14,name: shore; attribute: sandy; (x,y,w,h): (190, 123, 171, 12) src, edge_attr</p>
<p>Table 15 :
15
Example of creative conversation on the WebQSP dataset..kwebbase.kwtopic.connections_from,227 8, book.book_subject.works,99 525, law.invention.inventor,8 442, law.invention.inventor,8 98, law.invention.inventor,1044 8, law.inventor.inventions,98 63, law.invention.inventor,8 8, base.argumentmaps.innovator.original_ideas,442 98, law.invention.inventor,8 8, law.inventor.inventions,525 442, base.argumentmaps.original_idea.innovator,8 8, law.inventor.inventions,442 8, law.inventor.inventions,63 User Question: what else did ben franklin invent?Surrounding this question, develop an educational tour through the knowledge graph, highlighting interesting facts and connections in an engaging way.Model Great!Based on the provided knowledge graph, here's an educational tour that highlights interesting facts and connections related to Benjamin Franklin's inventions and other notable achievements: 1. Franklin's Lightning Rod: Start by exploring Franklin's most famous invention -the lightning rod.Discover how Franklin's invention protected buildings from lightning strikes and how it changed the way people understood electricity.Connection: Franklin worked with Thomas Jefferson on the lightning rod, and their collaboration led to the development of the concept of positive and negative charges.2. Bifocals: Move on to Franklin's invention of bifocal glasses.Learn how Franklin's vision problems inspired him to create a new type of glasses that allowed people to see both near and far objects clearly.Connection: Franklin's bifocal glasses were a groundbreaking invention that improved the lives of millions of people around the world.3. Franklin Stove: Explore Franklin's design of the Franklin stove, a cast-iron stove that provided more heat with less fuel.
node_id, node_attr8, benjamin franklin63, glass harmonica98, lightning rod99, stealing god's thunder: benjamin franklin's lightning rod and the invention of america227, benjamin franklin worked with thomas jefferson442, franklin stove525, bifocals706, benjamin franklin drawing electricity from the sky1044, prokop divišsrc, edge_attr, dstGraph706, visual_art.artwork.art_subject, 8 8, base
https://explagraphs.github.io/
https://huggingface.co/datasets/rmanluo/RoG-webqsp
AcknowledgmentXB is supported by NUS Grant ID R-252-000-B97-133.
Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. Jinheon Baek, Alham Fikri Aji, Amir Saffari, 10.18653/v1/2023.nlrse-1.7Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE). Greg Bhavana Dalvi Mishra, Peter Durrett, Danilo Neves Jansen, Jason Ribeiro, Wei, the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)Toronto, CanadaAssociation for Computational LinguisticsJune 2023</p>
<p>A note on the prize collecting traveling salesman problem. Mathematical programming. Daniel Bienstock, David Michel X Goemans, David Simchi-Levi, Williamson, 199359</p>
<p>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang, arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, arXiv:2307.033932023arXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.046682023arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, arXiv:2305.195232023arXiv preprint</p>
<p>A nearly-linear time framework for graphstructured sparsity. Chinmay Hegde, Piotr Indyk, Ludwig Schmidt, International Conference on Machine Learning. PMLR2015</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Can llms effectively leverage graph structural information: when and why. Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma, arXiv:2309.165952023arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu, 2023</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.09645Structgpt: A general framework for large language model to reason over structured data. 2023arXiv preprint</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.027832023arXiv preprint</p>
<p>Knowledge graphaugmented language models for knowledge-grounded dialogue generation. Minki Kang, Jin Myung Kwak, Jinheon Baek, Sung Ju Hwang, 2023</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Boosting logical reasoning in large language models through a new framework: The graph of thought. Bin Lei, Chunhua Liao, Caiwen Ding, arXiv:2308.086142023arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Graphadapter: Tuning vision-language models with dual knowledge graph. Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang, arXiv:2309.136252023arXiv preprint</p>
<p>Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu, Yu , arXiv:2311.12399A survey of graph meets large language model: Progress and future directions. 2023arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2310.001492023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, arXiv:2310.01061Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, arXiv:2310.01061Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>Yizhen Shirui Pan, Yixin Zheng, Liu, arXiv:2310.05499Integrating graphs with large language models: Methods and prospects. 2023arXiv preprint</p>
<p>Let your graph do the talking: Encoding structured data for llms. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Can large language models empower molecular property prediction?. Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu, arXiv:2307.074432023arXiv preprint</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Yijian Qin, Xin Wang, Ziwei Zhang, Wenwu Zhu, arXiv:2310.181522023arXiv preprint</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Explagraphs: An explanation graph generation task for structured commonsense reasoning. Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal, arXiv:2104.076442021arXiv preprint</p>
<p>Knowledge graph-augmented language models for complex question answering. Priyanka Sen, Sandeep Mavadia, Amir Saffari, 10.18653/v1/2023.nlrse-1.1Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE). Greg Bhavana Dalvi Mishra, Peter Durrett, Danilo Neves Jansen, Jason Ribeiro, Wei, the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)Toronto, CanadaAssociation for Computational LinguisticsJune 2023</p>
<p>Masked label prediction: Unified message passing model for semi-supervised classification. Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, Yu Sun, arXiv:2009.035092020arXiv preprint</p>
<p>Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, Jian Guo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Large language models as topological structure enhancers for text-attributed graphs. Shengyin Sun, Yuxiang Ren, Chen Ma, Xuecang Zhang, arXiv:2311.143242023arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023arXiv preprint</p>
<p>Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, arXiv:2309.15427Graph neural prompting with large language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, arXiv:2305.100372023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, arXiv:2309.05519Next-gpt: Any-to-any multimodal llm. 2023arXiv preprint</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, arXiv:2308.071342023arXiv preprint</p>
<p>The value of semantic parse labeling for knowledge base question answering. Wen-Tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, Jina Suh, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics20162Short Papers)</p>
<p>Multimodal graph learning for generative tasks. Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov, arXiv:2310.074782023arXiv preprint</p>
<p>Empower text-attributed graphs learning with large language models (llms). Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang, arXiv:2310.098722023arXiv preprint</p>
<p>Thought propagation: An analogical approach to complex reasoning with large language models. Junchi Yu, Ran He, Rex Ying, arXiv:2310.039652023arXiv preprint</p>
<p>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. Jiawei Zhang, arXiv:2304.111162023arXiv preprint</p>
<p>Llama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023arXiv preprint</p>
<p>Graph meets llms: Towards large graph models. Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, Wenwu Zhu, 2023</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, Qi Liu, bioRxiv. 2023</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, Graphtext, arXiv:2310.01089Graph reasoning in text space. 2023arXiv preprint</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>FedEx Cup', 'common.topic.notable_types. ] Webqsp, ['brandt Snedeker, Sports League Award Type. freebase.type_profile.strict_included_types. sports.sports_award.award_winner. Sports League Award Type', 'freebase.type_profile.kind. m.0n1v8cy', 'sports.sports_award.season', '2012 PGA Tour'], ['Sports League Award Type', 'type.type.properties', 'Winners'</p>
<p>The original GQA dataset is designed for real-world visual reasoning and compositional question answering, aiming to address key shortcomings of previous VQA datasets [13]. It comprises 108k images, each associated with a Scene Graph. In our study, we focus differently on graph question answering; hence, we did not utilize the image counterparts, leveraging only the scene graphs from the original dataset. Additionally, the original dataset describes images using JSON files. We simplified the object IDs to suit our research needs. We randomly sampled 100k samples from the original dataset and divided them into training, validation. Scenegraphs, and test subsets, following a 6:2:2 ratio</p>
<p>We follow the preprocessing steps from RoG 3 [28]. The original dataset uses a list of triplets format, which we have transformed into our unified graph format. Furthermore, to avoid. Webqsp, </p>            </div>
        </div>

    </div>
</body>
</html>
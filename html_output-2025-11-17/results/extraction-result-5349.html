<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5349 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5349</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5349</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-9a4e4ab77c3d836bab35e0578de68e8ce79af1e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9a4e4ab77c3d836bab35e0578de68e8ce79af1e8" target="_blank">Graph Neural Prompting with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> Graph Neural Prompting (GNP) is proposed, a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs that demonstrates the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5349.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5349.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play method that encodes retrieved KG subgraphs via a GNN and produces a graph-conditioned soft prompt (embedding) that is projected into the LLM token embedding space and concatenated with text inputs to guide frozen or tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Neural Prompt (graph->soft-prompt embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Retrieves a subgraph for each input (entity-linked, two-hop neighborhood), initializes node embeddings with pretrained entity vectors, encodes the subgraph with a Graph Attention Network (GAT) GNN to produce node embeddings H1; applies a self-attention layer to get H2, performs cross-modality attention between node embeddings and transformed text token embeddings (T') to produce H3, average-pools nodes to a graph-level embedding H4, then maps H4 via a domain projector (FFN layers + GELU) into the LLM dictionary dimension d_t to yield the Graph Neural Prompt Z — a sequence/vector of learnable embeddings concatenated to the LLM input. A self-supervised link-prediction objective (DistMult scoring) is jointly trained with the LLM loss to encourage structural learning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph (ConceptNet for commonsense experiments; UMLS for biomedical experiments); retrieved per-instance subgraphs (two-hop neighbors of linked entities)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Instance-level (per-example) soft prompt embedding; maps graph representation into LLM token embedding space (compatibility); retains structural information via GNN encoding and self-supervised link prediction; designed to reduce noise compared to naively inserting all triples; properties discussed: higher faithfulness for relevant facts, compactness (single graph-level embedding), expressivity via GNN, interpretability via retrieved subgraphs and attention, requires hyperparameter tuning (GNN layers, pooling layers, trade-off λ, link drop rate).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multiple-choice question answering (commonsense: OpenBookQA, ARC, PIQA, RiddleSense; biomedical: PubMedQA, BioASQ) under two settings: LLM Frozen (only prompt adapted) and LLM Tuned (LoRA or full finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy on each dataset; reported totals in Table 1: FLAN-T5 xlarge (3B) LLM Frozen total accuracy = 74.36 (GNP) vs Prompt Tuning 65.95 and LLM-only 64.49; FLAN-T5 xxlarge (11B) LLM Frozen total = 77.84 (GNP) vs Prompt Tuning 68.55 and LLM-only 66.88. LLM Tuned (LoRA+GNP) totals: 3B = 77.43 vs LoRA 76.23; 11B = 80.83 vs LoRA 79.41. Per-dataset accuracies are reported in Table 1 (e.g., for 11B frozen: OBQA 87.20, ARC 78.20, PIQA 63.66, Riddle 70.98, PubMedQA 76.75, BioASQ 90.24).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Direct comparisons in experiments show GNP substantially outperforms KG-flattening baselines and KAPING and yields large gains over Prompt Tuning when LLMs are frozen (e.g., average relative improvement Δ over Prompt Tuning: +13.54% for 11B frozen). When combined with LoRA, GNP improves LoRA slightly and in many cases matches or exceeds full finetuning (10/12 evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires retrieval and GNN encoding for each instance (computational overhead); sensitive to hyperparameters (number of GNN layers, pooling layers, λ, link drop rate); needs pretrained entity embeddings for initialization; still dependent on quality of entity linking and retrieved subgraph (irrelevant nodes can exist and must be filtered by attention); mapping to embedding space (domain projector) is essential—ablation shows large drop without it.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Neural Prompting with Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5349.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5349.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Flattening (REL/BFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG Flattening via node ordering (Relevance ranking or Breadth-First Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that linearizes graph content by ordering nodes/triples and concatenating them into a textual sequence fed directly to the LLM; two orderings tested are relevance-score (REL) ranking and breadth-first search (BFS).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep bidirectional language-knowledge graph pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG Flattening (linearization / serialization into text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Selects nodes/entities from the retrieved subgraph and flattens them into a sequence representation (textual sequence or triple list) according to an ordering strategy: (1) REL — rank nodes/triples by relevance score to the query (as in Yasunaga et al. 2022) and serialize them; (2) BFS — traverse the subgraph in breadth-first order and flatten encountered nodes/triples. The flattened sequence is directly concatenated as textual context (hard prompt) for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph subgraphs (retrieved per-question, two-hop neighborhoods of linked entities)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple, directly interpretable textualization; compactness depends on truncation; high potential to introduce noise and irrelevant context (as many triples may be unrelated); no learned graph encoder (non-parametric except for ranking heuristics), dataset- or instance-level ordering affects content; easy to implement but can degrade LLM performance due to noise and altered semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multiple-choice QA (same benchmarks as GNP): OBQA, ARC, PIQA, RiddleSense, PubMedQA, BioASQ, evaluated under LLM Frozen setting in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy reported in Table 1. FLAN-T5 xlarge (3B) LLM Frozen totals: KG Flattening REL = 60.18, BFS = 60.29 (per-dataset numbers provided in table; e.g., 3B REL OBQA 61.80, ARC 64.12, ...). FLAN-T5 xxlarge (11B) LLM Frozen totals: REL = 64.35, BFS = 64.20 (per-dataset numbers in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Directly compared to GNP and Prompt Tuning in experiments: consistently underperforms (often substantially) relative to learned graph-to-embedding GNP and even relative to simple Prompt Tuning; authors attribute poor performance to noise from irrelevant KG context and altered semantics when fed verbatim into LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Can introduce substantial noise from irrelevant triples; ordering heuristics may fail to surface critical relations; performance degrades compared to learned prompt/encoding approaches; scalability and truncation trade-offs when serializing large subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Neural Prompting with Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5349.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5349.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KAPING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KAPING (Knowledge-Augmented Prompting approach from Baek et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that injects important KG triples (one-hop or two-hop neighborhoods) as textual prompt tokens for LLMs; used as a baseline for direct triple injection into LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>KnowledgeAugmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG triple injection (one-hop/two-hop textual triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extracts KG triples connected to entities matched in the question and options, selects either one-hop (OH) or two-hop (TH) neighborhoods, and inserts those triples (textualized triples) directly into the LLM input as context/hard prompt, expecting the model to use them for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph subgraphs (one-hop or two-hop neighborhoods around linked entities)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Direct, interpretable triple textualization; simple retrieval-based augmentation; susceptible to extraneous context and noise; non-learned selection can lead to misleading or irrelevant facts being provided to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Multiple-choice QA (OBQA, ARC, PIQA, RiddleSense, PubMedQA, BioASQ) under LLM Frozen setting as evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy reported in Table 1. FLAN-T5 xlarge (3B) LLM Frozen totals: KAPING TH = 58.41, OH = 58.53. FLAN-T5 xxlarge (11B) totals: TH = 59.15, OH = 58.87. Per-dataset numbers are in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared directly in experiments: KAPING performs worse than KG Flattening and substantially worse than GNP and Prompt Tuning on the evaluated benchmarks; authors point to noise and the presence of irrelevant KG context as reasons for poor results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Feeding raw triples introduces noise and can alter semantics; selection heuristic (one-hop vs two-hop) does not sufficiently filter irrelevant or misleading facts; poor empirical performance in their multi-dataset evaluation suggests direct triple injection is brittle without learned filtering/encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Neural Prompting with Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>KnowledgeAugmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering <em>(Rating: 2)</em></li>
                <li>Deep bidirectional language-knowledge graph pretraining <em>(Rating: 2)</em></li>
                <li>GreaseLM: Graph REASoning Enhanced Language Models for Question Answering <em>(Rating: 1)</em></li>
                <li>QA-GNN: Reasoning with language models and knowledge graphs for question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5349",
    "paper_id": "paper-9a4e4ab77c3d836bab35e0578de68e8ce79af1e8",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GNP",
            "name_full": "Graph Neural Prompting",
            "brief_description": "A plug-and-play method that encodes retrieved KG subgraphs via a GNN and produces a graph-conditioned soft prompt (embedding) that is projected into the LLM token embedding space and concatenated with text inputs to guide frozen or tuned LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph Neural Prompt (graph-&gt;soft-prompt embedding)",
            "representation_description": "Retrieves a subgraph for each input (entity-linked, two-hop neighborhood), initializes node embeddings with pretrained entity vectors, encodes the subgraph with a Graph Attention Network (GAT) GNN to produce node embeddings H1; applies a self-attention layer to get H2, performs cross-modality attention between node embeddings and transformed text token embeddings (T') to produce H3, average-pools nodes to a graph-level embedding H4, then maps H4 via a domain projector (FFN layers + GELU) into the LLM dictionary dimension d_t to yield the Graph Neural Prompt Z — a sequence/vector of learnable embeddings concatenated to the LLM input. A self-supervised link-prediction objective (DistMult scoring) is jointly trained with the LLM loss to encourage structural learning.",
            "graph_type": "Knowledge graph (ConceptNet for commonsense experiments; UMLS for biomedical experiments); retrieved per-instance subgraphs (two-hop neighbors of linked entities)",
            "representation_properties": "Instance-level (per-example) soft prompt embedding; maps graph representation into LLM token embedding space (compatibility); retains structural information via GNN encoding and self-supervised link prediction; designed to reduce noise compared to naively inserting all triples; properties discussed: higher faithfulness for relevant facts, compactness (single graph-level embedding), expressivity via GNN, interpretability via retrieved subgraphs and attention, requires hyperparameter tuning (GNN layers, pooling layers, trade-off λ, link drop rate).",
            "evaluation_task": "Multiple-choice question answering (commonsense: OpenBookQA, ARC, PIQA, RiddleSense; biomedical: PubMedQA, BioASQ) under two settings: LLM Frozen (only prompt adapted) and LLM Tuned (LoRA or full finetuning).",
            "performance_metrics": "Accuracy on each dataset; reported totals in Table 1: FLAN-T5 xlarge (3B) LLM Frozen total accuracy = 74.36 (GNP) vs Prompt Tuning 65.95 and LLM-only 64.49; FLAN-T5 xxlarge (11B) LLM Frozen total = 77.84 (GNP) vs Prompt Tuning 68.55 and LLM-only 66.88. LLM Tuned (LoRA+GNP) totals: 3B = 77.43 vs LoRA 76.23; 11B = 80.83 vs LoRA 79.41. Per-dataset accuracies are reported in Table 1 (e.g., for 11B frozen: OBQA 87.20, ARC 78.20, PIQA 63.66, Riddle 70.98, PubMedQA 76.75, BioASQ 90.24).",
            "comparison_to_other_representations": "Direct comparisons in experiments show GNP substantially outperforms KG-flattening baselines and KAPING and yields large gains over Prompt Tuning when LLMs are frozen (e.g., average relative improvement Δ over Prompt Tuning: +13.54% for 11B frozen). When combined with LoRA, GNP improves LoRA slightly and in many cases matches or exceeds full finetuning (10/12 evaluations).",
            "limitations_or_challenges": "Requires retrieval and GNN encoding for each instance (computational overhead); sensitive to hyperparameters (number of GNN layers, pooling layers, λ, link drop rate); needs pretrained entity embeddings for initialization; still dependent on quality of entity linking and retrieved subgraph (irrelevant nodes can exist and must be filtered by attention); mapping to embedding space (domain projector) is essential—ablation shows large drop without it.",
            "uuid": "e5349.0",
            "source_info": {
                "paper_title": "Graph Neural Prompting with Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "KG Flattening (REL/BFS)",
            "name_full": "KG Flattening via node ordering (Relevance ranking or Breadth-First Search)",
            "brief_description": "A baseline approach that linearizes graph content by ordering nodes/triples and concatenating them into a textual sequence fed directly to the LLM; two orderings tested are relevance-score (REL) ranking and breadth-first search (BFS).",
            "citation_title": "Deep bidirectional language-knowledge graph pretraining",
            "mention_or_use": "use",
            "representation_name": "KG Flattening (linearization / serialization into text)",
            "representation_description": "Selects nodes/entities from the retrieved subgraph and flattens them into a sequence representation (textual sequence or triple list) according to an ordering strategy: (1) REL — rank nodes/triples by relevance score to the query (as in Yasunaga et al. 2022) and serialize them; (2) BFS — traverse the subgraph in breadth-first order and flatten encountered nodes/triples. The flattened sequence is directly concatenated as textual context (hard prompt) for the LLM.",
            "graph_type": "Knowledge graph subgraphs (retrieved per-question, two-hop neighborhoods of linked entities)",
            "representation_properties": "Simple, directly interpretable textualization; compactness depends on truncation; high potential to introduce noise and irrelevant context (as many triples may be unrelated); no learned graph encoder (non-parametric except for ranking heuristics), dataset- or instance-level ordering affects content; easy to implement but can degrade LLM performance due to noise and altered semantics.",
            "evaluation_task": "Multiple-choice QA (same benchmarks as GNP): OBQA, ARC, PIQA, RiddleSense, PubMedQA, BioASQ, evaluated under LLM Frozen setting in the paper.",
            "performance_metrics": "Accuracy reported in Table 1. FLAN-T5 xlarge (3B) LLM Frozen totals: KG Flattening REL = 60.18, BFS = 60.29 (per-dataset numbers provided in table; e.g., 3B REL OBQA 61.80, ARC 64.12, ...). FLAN-T5 xxlarge (11B) LLM Frozen totals: REL = 64.35, BFS = 64.20 (per-dataset numbers in Table 1).",
            "comparison_to_other_representations": "Directly compared to GNP and Prompt Tuning in experiments: consistently underperforms (often substantially) relative to learned graph-to-embedding GNP and even relative to simple Prompt Tuning; authors attribute poor performance to noise from irrelevant KG context and altered semantics when fed verbatim into LLM.",
            "limitations_or_challenges": "Can introduce substantial noise from irrelevant triples; ordering heuristics may fail to surface critical relations; performance degrades compared to learned prompt/encoding approaches; scalability and truncation trade-offs when serializing large subgraphs.",
            "uuid": "e5349.1",
            "source_info": {
                "paper_title": "Graph Neural Prompting with Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "KAPING",
            "name_full": "KAPING (Knowledge-Augmented Prompting approach from Baek et al. 2023)",
            "brief_description": "A method that injects important KG triples (one-hop or two-hop neighborhoods) as textual prompt tokens for LLMs; used as a baseline for direct triple injection into LLMs.",
            "citation_title": "KnowledgeAugmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
            "mention_or_use": "use",
            "representation_name": "KG triple injection (one-hop/two-hop textual triples)",
            "representation_description": "Extracts KG triples connected to entities matched in the question and options, selects either one-hop (OH) or two-hop (TH) neighborhoods, and inserts those triples (textualized triples) directly into the LLM input as context/hard prompt, expecting the model to use them for QA.",
            "graph_type": "Knowledge graph subgraphs (one-hop or two-hop neighborhoods around linked entities)",
            "representation_properties": "Direct, interpretable triple textualization; simple retrieval-based augmentation; susceptible to extraneous context and noise; non-learned selection can lead to misleading or irrelevant facts being provided to the LLM.",
            "evaluation_task": "Multiple-choice QA (OBQA, ARC, PIQA, RiddleSense, PubMedQA, BioASQ) under LLM Frozen setting as evaluated in the paper.",
            "performance_metrics": "Accuracy reported in Table 1. FLAN-T5 xlarge (3B) LLM Frozen totals: KAPING TH = 58.41, OH = 58.53. FLAN-T5 xxlarge (11B) totals: TH = 59.15, OH = 58.87. Per-dataset numbers are in Table 1.",
            "comparison_to_other_representations": "Compared directly in experiments: KAPING performs worse than KG Flattening and substantially worse than GNP and Prompt Tuning on the evaluated benchmarks; authors point to noise and the presence of irrelevant KG context as reasons for poor results.",
            "limitations_or_challenges": "Feeding raw triples introduces noise and can alter semantics; selection heuristic (one-hop vs two-hop) does not sufficiently filter irrelevant or misleading facts; poor empirical performance in their multi-dataset evaluation suggests direct triple injection is brittle without learned filtering/encoding.",
            "uuid": "e5349.2",
            "source_info": {
                "paper_title": "Graph Neural Prompting with Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "KnowledgeAugmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
            "rating": 2
        },
        {
            "paper_title": "Deep bidirectional language-knowledge graph pretraining",
            "rating": 2
        },
        {
            "paper_title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
            "rating": 1
        },
        {
            "paper_title": "QA-GNN: Reasoning with language models and knowledge graphs for question answering",
            "rating": 1
        }
    ],
    "cost": 0.01184025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph Neural Prompting with Large Language Models</h1>
<p>Yijun Tian ${ }^{1}$, Huan Song ${ }^{2}$, Zichen Wang ${ }^{2}$, Haozhu Wang ${ }^{2}$, Ziqing $\mathbf{H u}^{2}$, Fang Wang ${ }^{2}$, Nitesh V. Chawla ${ }^{1}$, Panpan $\mathbf{X u}^{2}$<br>${ }^{1}$ University of Notre Dame<br>${ }^{2}$ Amazon<br>{yijun.tian, nchawla}@nd.edu, {huanso, zichewan, haozhuw, ziqinghu, fwfang, xupanpan} @amazon.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a crossmodality pooling module, a domain projector, and a selfsupervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.</p>
<h2>Introduction</h2>
<p>Large Language Models (LLMs) have demonstrated exceptional performance and general capability in various NLP tasks and use cases such as question answering (Robinson, Rytting, and Wingate 2023) and text summarization (Zhang et al. 2023). Moreover, the significant growth in model size has further endowed LLMs with emergent capabilities (Wei et al. 2022b), laying the groundwork for exploring artificial general intelligence (Bubeck et al. 2023). Accordingly, LLMs have attracted tremendous interest from academia (Wei et al. 2022a; Zhao et al. 2023) and industry (Anil et al. 2023; OpenAI 2023).</p>
<p>Given the broad success of LLMs, many techniques have emerged to adapt these general-purpose models to downstream tasks. Beyond the conventional approach of model fine-tuning where all model parameters are adjusted (Howard and Ruder 2018), prompt-based adaptation methods are proposed to modulate a frozen LLM’s behavior through</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Result comparison across LLM Frozen (parameters unchanged) and LLM Tuned (parameters updated) settings. The proposed Graph Neural Prompting significantly improves the performance. Reported results are averaged across six datasets on two tasks for an 11B FLAN-T5 model.
prompts (Brown et al. 2020; Lester, Al-Rfou, and Constant 2021; Li and Liang 2021). Rather than adapt the parameters in LLMs, these methods freeze the LLMs and typically introduce additional trainable parameters. The idea of freezing LLMs is appealing, especially as the model size grows and the training resource dependency intensifies.</p>
<p>On the other hand, despite the success of LLMs in handling different real-world applications and the feasibility of adapting to specific downstream tasks, they still exhibit the inherent limitations of language modeling in accurately capturing and returning grounded knowledge (Lewis et al. 2020; Pan et al. 2023). Knowledge graphs (KGs), storing enormous facts, serve as a systematic way of representing knowledge (Ji et al. 2021). Consequently, existing methods have incorporated KGs to assist language modeling, often by designing customized model architectures to accommodate both KGs and textual data, followed by joint training sessions (Yasunaga et al. 2022; Zhang et al. 2022). Nonetheless, joint training KGs and text for LLMs is challenging due to the extensive parameters LLMs contain and the substantial computation resources they require. In addition, numerous pre-trained LLMs with exceptional capabilities are released. It becomes advantageous to employ these pre-existing LLMs, particularly beneficial if we can sidestep the need to craft a</p>
<p>specialized model and train it from scratch. A direct approach to employing KGs for retrieval-augmented generation (Lewis et al. 2020) is to feed the KG triples into LLMs directly (Baek, Aji, and Saffari 2023). However, this method can introduce substantial noise, given that KGs might contain various extraneous contexts. Therefore, we ask:</p>
<h2>Can we learn beneficial knowledge from KGs and integrate them into pre-trained LLMs?</h2>
<p>To answer the question, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP retrieves and encodes the pertinent grounded knowledge to derive Graph Neural Prompt, an embedding vector that can be sent into LLMs to provide guidance and instructions. In particular, GNP first utilizes a graph neural network (GNN) to capture and encode the intricate graph knowledge into entity/node embeddings. Then, a cross-modality pooling module is present to determine the most relevant node embeddings in relation to the text input, and consolidate these node embeddings into a holistic graph-level embedding. After that, GNP encompasses a domain projector to bridge the inherent disparities between the graph and text domains. Finally, a self-supervised link prediction objective is introduced to enhance the model comprehension of relationships between entities and capture graph knowledge in a self-supervised manner.</p>
<p>To fully evaluate our model, we conduct extensive experiments on multiple public benchmark datasets in the tasks of commonsense reasoning and biomedical reasoning. We further report the results across different LLM sizes and settings. We conclude that GNP can effectively encode intricate knowledge in KGs and significantly improve performance. Figure 1 shows the averaged performance improvement using our method across six datasets. Specifically, GNP improves the baseline by +13.5% when LLM is frozen, validating the superiority of our method in learning effective prompts. In addition, by using our method, fine-tuning LLMs with parameter-efficient approach LoRA (Hu et al. 2022) shows an improvement of +1.8%. More promisingly, compared to model full fine-tuning without leveraging any efficient tuning approaches, our method can achieve competitive or superior performance in 10 out of 12 evaluations, as shown in the experiment section. To summarize, our main contributions are:</p>
<ul>
<li>To the best of our knowledge, this is the first attempt to study the learning of beneficial knowledge from KGs for pre-trained LLMs.</li>
<li>We propose GNP, a novel plug-and-play method for pretrained LLMs to extract valuable knowledge from KGs. The proposed method contains various tailored designs, including a standard GNN, a cross-modality pooling module, a domain projector, and a self-supervised graph learning objective.</li>
<li>Extensive experiments demonstrate the superiority of GNP on multiple datasets across different settings. We also present the ablation study, model design comparison, parameter sensitivity analysis, case study and visualization to validate the effectiveness of GNP.</li>
</ul>
<h2>Related Work</h2>
<p>Large Language Models and Question Answering. Recently, various LLMs have been proposed (Chung et al. 2022; Touvron et al. 2023; Brown et al. 2020) and have demonstrated remarkable performance across different tasks (Shi et al. 2023; Chen et al. 2023b; Wei et al. 2024; Hong et al. 2023). Question answering, as a fundamental task, demands intricate reasoning and understanding comprehension skills to interpret the text and provide appropriate responses to the posed questions (Lu et al. 2022; Zhu et al. 2021; Wang et al. 2023; Chen et al. 2023a). Although LLMs have strong learning capabilities, they have the limitation of precisely capturing accurate factual knowledge and are susceptible to generating unfounded responses (Zhao et al. 2023; Ji et al. 2023; Bang et al. 2023). In addition, the enormous number of parameters in LLMs poses difficulties in adapting LLMs for downstream tasks (Scao et al. 2022; Smith et al. 2022). Correspondingly, various approaches are presented to alleviate the intensive training dependency and reduce the computational expenses (Lester, Al-Rfou, and Constant 2021; Li and Liang 2021; Hu et al. 2022). For instance, Prompt Tuning (Lester, Al-Rfou, and Constant 2021) introduces soft prompts to condition the pre-trained LLMs for downstream tasks. In our work, we propose to retrieve the factual knowledge from KGs to enhance LLMs, while still benefiting from circumventing the burdensome training expenses by using pre-trained LLMs.
Knowledge Graphs for Language Modeling. Many graph learning methods are proposed to encode graphs and KGs (Ji et al. 2021; Tian et al. 2023a,b; Tang et al. 2022; Wang, Jin, and Derr 2022; Xu et al. 2023; Kou et al. 2022). Recent studies indicate that KGs can enhance language modeling by providing background knowledge (Ren et al. 2021; Wang et al. 2019). One approach to achieve this is integrating KGs into the pre-training stage of language modeling. For instance, ERNIE (Sun et al. 2021), JAKET (Yu et al. 2022), and JointGT (Ke et al. 2021) develop pre-training objectives tailored for KG triples and the paired sentences. DRAGON (Yasunaga et al. 2022) introduces a customized fusion framework to jointly pre-train the model for KGs and text. Moreover, KGs are leveraged to assist language modeling for question answering (Lin et al. 2019; Lv et al. 2020; Feng et al. 2020; Mihaylov and Frank 2018). Specifically, GreaseLM (Zhang et al. 2022) and QAGNN (Yasunaga et al. 2021) suggest that KGs can scaffold reasoning about entities with the graph structure such as negation and multihop reasoning to facilitate complex question answering. To encode KGs, many works study methods to learn KG entity and relation embeddings, such as TransE (Bordes et al. 2013) and DistMult (Yang et al. 2015). Recently, with the aim of integrating KGs into the emerging domain of LLMs, given existing studies pose difficulties when applying, KAPING (Baek, Aji, and Saffari 2023) employs knowledge graphs to extract relevant triples. These triples correspond to the input question, with the expectation that directly feeding them into LLMs is beneficial, despite the presence of noise. In our work, we present a learning method for identifying beneficial knowledge from KGs, offering substantial benefits to LLMs.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: The overall framework. Given a multiple choice question, we first retrieve subgraphs from the knowledge graph based on the entities in the question and options. We then develop Graph Neural Prompting (GNP) to encode the pertinent factual knowledge and structural information to obtain the Graph Neural Prompt. GNP contains various designs including a GNN, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Later, the obtained Graph Neural Prompt is sent into LLM for inference along with the input text embedding. We utilize the standard maximum likelihood objective for downstream task adaptation, while LLM is kept frozen or tuned depending on different experimental settings.</p>
<h2>Preliminary</h2>
<p>In this section, we describe the knowledge graph and formally define the problem of multiple choice question answering.</p>
<p>Definition 1. Knowledge Graph. A knowledge graph is defined as $\mathcal{G}=(\mathcal{E},\mathcal{R},\mathcal{T})$, where $\mathcal{E}$ is the set of entities and $\mathcal{R}$ is the set of relations. $\mathcal{T}$ is the collection of fact triples $\left{\left(e_{h},r,e_{t}\right)\right} \in \mathcal{E} \times \mathcal{R} \times \mathcal{E}$, where $e_{h}$ denotes the head entity, $r$ is the relation, and $e_{t}$ indicates the tail entity.</p>
<p>Problem 1. Multiple Choice Question Answering. Given a question $Q$, a set of answer options $A=\left{a_{k}\right}<em _Theta="\Theta">{k=1}^{K}$, and an optional context $C$ depending on open-book or closebook, the task is to design a machine learning model $\mathcal{F}</em>$ to provide rich knowledge and assist the model to answer the question.}$ with parameters $\Theta$ that selects the best option to answer the question. Here $K$ denotes the total number of answer options and $a_{k}$ indicates the $k$-th answer option. The ground truth label $y \in A$ is the correct answer for $Q$. In addition, we use knowledge graph $\mathcal{G</p>
<h2>Methodology</h2>
<p>In this section, we introduce the techniques of prompting LLMs for question answering as well as subgraph retrieval. Additionally, we present Graph Neural Prompting and elaborate on its components and designs. Figure 2 illustrates the framework of our method.</p>
<h2>Prompting LLMs for Question Answering</h2>
<p>Prompting is the de facto approach to elicit responses from LLMs <em>Liu et al. (2023)</em>. The typical approach of prompting LLMs for multi-choice question answering is simple. Given a question $Q$, the optional context $C$, and the answer options $A$, we first tokenize the concatenation of $C, Q, A$ into a sequence of input text tokens $X$. We then design a series of prompt tokens, $P$, and prepend it to the input text tokens $X$, which is later considered as input for the LLM model to generate prediction $y^{\prime}=f([P, X])$. The LLM model can be trained for downstream task adaptation using a standard maximum likelihood loss using teacher forcing <em>Williams and Zipser (1989)</em> and a cross-entropy loss:</p>
<p>$$
\mathcal{L}_{l l m}=-\log p(y \mid X, \Theta)
$$</p>
<p>where $p$ is the probability distribution parameterized by the model. The prompt $P$ can be either a hard prompt in the form of textual input, or a soft prompt in the form of learnable embedding vectors.</p>
<p>Unlike existing methods that solely use a text string as the hard prompt, our Graph Neural Prompting approach encodes structural and factual information contained in the knowledge graph $\mathcal{G}$ into a soft prompt $P$, which is a sequence of trainable vectors that can be concatenated with the token embedding of $X$. The learning of $P$ is encouraged to provide rich structural information and knowledge from $\mathcal{G}$ as well as task instruction for each data instance.</p>
<h2>Subgraph Retrieval</h2>
<p>To semantically align the input text tokens $X$ with the massive knowledge graph $\mathcal{G}$ with millions of nodes, we retrieve subgraphs of $\mathcal{G}$ that contain the relevant entities to the tokens in $X$. In particular, for each answer option $a_{k}$ and its corresponding context $C$ and question $Q$, we first obtain a set of matched entities $\mathcal{E}_{\text {match }}$ via entity linking to match the</p>
<p>tokens in $X$ to the entities in $\mathcal{G}$. We then retrieve a subgraph $\mathcal{G}^{\prime}$ based on the entities in $\mathcal{E}_{\text {match }}$ by including their two-hop neighbors and the relations that connect them (Yasunaga et al. 2022). The retrieved subgraph contains the necessary content and knowledge to assist the model in answering $Q$.</p>
<h2>Graph Neural Prompting</h2>
<p>Graph Neural Prompting contains various designs, including a GNN encoder that embeds the knowledge graph, a crossmodality pooling module that determines the pertinent node embeddings, a domain projector that bridges the discrepancies between graph and text, and a self-supervised link prediction objective that encourages the model to recognize structural information.
GNN Encoder. Although the retrieved subgraph $\mathcal{G}^{\prime}$ contains rich contextual information regarding the question and answer choices, some entities and relations are not relevant to the actual question. Directly feeding every fact triples in $\mathcal{G}^{\prime}$ can introduce noise and prevent the LLM model from concentrating on the critical information. Therefore, we introduce a GNN to encode the most relevant knowledge and further integrate the complex relationships among the entities. In particular, we first initialize the node embeddings using pre-trained entity embeddings (Feng et al. 2020; Yasunaga, Leskovec, and Liang 2022). Next, we employ a standard graph attention network (Veličković et al. 2018) as our GNN encoder for the retrieved subgraph $\mathcal{G}^{\prime}$. The encoding process is formulated as follows:</p>
<p>$$
H_{1}=f_{G N N}\left(\mathcal{G}^{\prime}\right)
$$</p>
<p>where $H_{1} \in \mathbb{R}^{d_{g}}$ represents the node embeddings learned by GNN for every node in $\mathcal{G}^{\prime}$, and $d_{g}$ denotes the output dimension of the GNN encoder.
Cross-modality Pooling. With the aim of identifying the most pertinent nodes in relation to the question, and consolidating the node embeddings into a holistic graphlevel representation for subsequent use, we design the crossmodality pooling module. In particular, we first introduce a self-attention layer to dynamically identify node significance using the internal graph characteristics and the implicit interactions among nodes:</p>
<p>$$
H_{2}=\text { Self-Attn }\left(H_{1}\right)
$$</p>
<p>where $H_{2}$ is node embeddings obtained after calculating self-attention and Self-Attn indicates the self-attention component. Then, we leverage the textual prompt to calculate the importance of nodes within the graph. To ensure uniformity, we utilize the dictionary in the LLM to obtain the text embeddings $\mathcal{T} \in \mathbb{R}^{d_{t}}$ for every token in the input text, where $d_{t}$ denotes the dimension of the LLM dictionary. Concretely, we start by applying a transformation to the text embeddings $\mathcal{T}$ and obtain the transformed text embedding $\mathcal{T}^{\prime}$, ensuring that the dimension of $\mathcal{T}^{\prime}$ matches the dimension $d_{g}$ of node embeddings $H_{2}$. After that, we calculate the crossmodality attention using $H_{2}$ and $\mathcal{T}^{\prime}$. We use $H_{2}$ as the query and the $\mathcal{T}^{\prime}$ as the key and the value. The procedure is as follows:</p>
<p>$$
\begin{aligned}
\mathcal{T}^{\prime} &amp; =\operatorname{FFN}<em 2="2">{1}\left(\sigma\left(\operatorname{FFN}</em>)\right)\right) \
H_{3} &amp; =\operatorname{softmax}\left[H_{2} \cdot\left(\mathcal{T}^{\prime}\right)^{T} / \sqrt{d_{g}}\right] \cdot \mathcal{T}^{\prime}
\end{aligned}
$$}(\mathcal{T</p>
<p>where $\sigma$ is the GELU activation function, $\mathrm{FFN}<em 2="2">{1}$ and $\mathrm{FFN}</em>$ :}$ are feed-forward neural networks, and $H_{3}$ is the final node embeddings obtained with cross-modality attention considered. Next, we generate the graph-level embedding by average pooling the node embeddings $H_{3}$ in $\mathcal{G}^{\prime</p>
<p>$$
H_{4}=\operatorname{POOL}\left(H_{3}\right)
$$</p>
<p>where $H_{4}$ represents the graph-level embedding that takes into account the node significance in $\mathcal{G}^{\prime}$.
Domain Projector. In order to create a mapping between the graph-level embeddings and the text domain to facilitate comprehension by the LLM, we design a domain projector to align them. This projector aims to bridge the inherent disparities between the graph and text, allowing for more seamless integration. In addition, the projector maps the graph-level embeddings to the same dimension $d_{t}$ of LLM, which ensures compatibility and consistency when interfacing with the LLM's inherent structures. We design the projector as follows:</p>
<p>$$
Z=\operatorname{FFN}<em 4="4">{3}\left(\sigma\left(\operatorname{FFN}</em>\right)\right)\right)
$$}\left(H_{4</p>
<p>where $Z$ denotes Graph Neural Prompt, the final output of GNP, and $\mathrm{FFN}<em 4="4">{3}, \mathrm{FFN}</em>$ are feed-forward neural networks.
Self-supervised Link Prediction. While the downstream cross-entropy objective enables the model to learn and adapt to the target dataset, we design a link prediction task to further refine its understanding of relationships between entities and capture graph knowledge in a self-supervised manner. Specifically, we mask out some edges in $\mathcal{G}^{\prime}$ and enforce the model to predict them. This encourages the model to learn to use the partial graph content and structure to reason about the missing links. Concretely, we denote the set of masked-out edges as $\mathcal{E}<em 3="3">{\text {mask }} \subseteq \mathcal{E}$. Given the learned node embeddings of the head entity and tail entity in a triplet $\left{\mathbf{h}</em>}, \mathbf{t<em 3="3">{3}\right} \in H</em>}$, we adopt a widely-used knowledge graph embedding method DistMult (Yang et al. 2015) to map the entity embeddings and relation in the KG to vectors, $\mathbf{h}, \mathbf{r}, \mathbf{t}$. We then define the scoring function $\phi\left(e_{h}, e_{t}\right)=\langle\mathbf{h}, \mathbf{r}, \mathbf{t}\rangle$ to generate the scores for each triple, where $\langle\cdot, \cdot\rangle$ denotes the trilinear dot product, and $\mathbf{r}$ represents the relations in KGs. A higher $\phi$ indicates a higher chance of $\left(e_{h}, r, e_{t}\right)$ being a correct positive triple instead of an incorrect negative triple. We enforce the model to predict the masked edges in $\mathcal{E<em l="l" p="p">{\text {mask }}$ as positive and other random edges as negative. The link prediction loss $\mathcal{L}</em>$ is defined as follows:</p>
<p>$$
\mathcal{L}<em _left_e__h="\left(e_{h">{l p}=\sum</em>}, r, e_{t}\right) \in \mathcal{E<em _pos="{pos" _text="\text">{\text {mask }}}\left(S</em>\right)
$$}}+S_{\text {neg }</p>
<p>where $S_{\text {pos }}=-\log \sigma_{s}\left(\phi\left(e_{h}, e_{t}\right)+\gamma\right)$ indicates the score for correct positive triples, $\gamma$ is the margin, $\sigma_{s}$ is the sigmoid function, $\left{\left(e_{h}^{\prime}, r, e_{t}^{\prime}\right)\right}$ are $n$ negative triples corresponding to the positive triplet $\left(e_{h}, r, e_{t}\right)$, and $S_{\text {neg }}=$ $\frac{1}{n} \sum_{\left(e_{h}^{\prime}, r, e_{t}^{\prime}\right)} \log \sigma_{s}\left(\phi\left(e_{h}^{\prime}, e_{t}^{\prime}\right)+\gamma\right)$ is the score for incorrect negative triples. The final objective function $\mathcal{L}$ is defined as the weighted combination of $\mathcal{L}<em l="l" p="p">{l l m}$ and $\mathcal{L}</em>$ :</p>
<p>$$
\mathcal{L}=\mathcal{L}<em l="l" p="p">{l l m}+\lambda \mathcal{L}</em>
$$</p>
<p>where $\lambda$ is a trade-off weight for balancing two losses.</p>
<p>Table 1: Overall experimental results on commonsense reasoning and biomedical reasoning tasks. The best results across different LLM sizes and settings are highlighted in bold. $\Delta_{P T}$ and $\Delta_{L o R A}$ represent the relative performance improvement of our method to Prompt Tuning and LoRA, respectively. We also include the full fine-tuning result in gray color for further reference. * means multiple prompt design methods are evaluated while only the best result is reported. Accuracy is used as the evaluation metric.</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Setting</th>
<th>Method</th>
<th>Commonsense Reasoning</th>
<th></th>
<th></th>
<th></th>
<th>Biomedical Reasoning</th>
<th></th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>OBQA</td>
<td>ARC</td>
<td>PIQA</td>
<td>Riddle</td>
<td>PubMedQA</td>
<td>BioASQ</td>
<td></td>
</tr>
<tr>
<td>FLAN-T5 xlarge (3B)</td>
<td>LLM Frozen</td>
<td>LLM-only</td>
<td>69.20</td>
<td>68.24</td>
<td>58.43</td>
<td>53.73</td>
<td>71.50</td>
<td>65.85</td>
<td>64.49</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Prompt Designs*</td>
<td>72.20</td>
<td>70.99</td>
<td>60.94</td>
<td>52.75</td>
<td>70.50</td>
<td>67.48</td>
<td>65.33</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KG Flattening REL</td>
<td>61.80</td>
<td>64.12</td>
<td>57.56</td>
<td>43.33</td>
<td>69.25</td>
<td>65.04</td>
<td>60.18</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KG Flattening BFS</td>
<td>62.80</td>
<td>63.86</td>
<td>56.69</td>
<td>44.12</td>
<td>69.25</td>
<td>65.04</td>
<td>60.29</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KAPING TH</td>
<td>58.80</td>
<td>63.52</td>
<td>52.34</td>
<td>40.78</td>
<td>70.00</td>
<td>65.04</td>
<td>58.41</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KAPING OH</td>
<td>60.00</td>
<td>63.09</td>
<td>51.69</td>
<td>41.37</td>
<td>70.00</td>
<td>65.04</td>
<td>58.53</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Prompt Tuning</td>
<td>72.20</td>
<td>70.64</td>
<td>60.83</td>
<td>53.33</td>
<td>72.00</td>
<td>66.67</td>
<td>65.95</td>
</tr>
<tr>
<td></td>
<td></td>
<td>GNP</td>
<td>79.80</td>
<td>71.85</td>
<td>61.48</td>
<td>66.86</td>
<td>76.75</td>
<td>89.43</td>
<td>74.36</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\Delta_{P T}$</td>
<td>$\uparrow 10.53 \%$</td>
<td>$\uparrow 1.71 \%$</td>
<td>$\uparrow 1.07 \%$</td>
<td>$\uparrow 25.37 \%$</td>
<td>$\uparrow 6.60 \%$</td>
<td>$\uparrow 34.14 \%$</td>
<td>$\uparrow 12.76 \%$</td>
</tr>
<tr>
<td></td>
<td>LLM Tuned</td>
<td>Full Fine-tuning</td>
<td>82.80</td>
<td>73.30</td>
<td>63.55</td>
<td>74.12</td>
<td>76.25</td>
<td>91.06</td>
<td>76.85</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LoRA</td>
<td>80.40</td>
<td>71.33</td>
<td>63.76</td>
<td>72.94</td>
<td>76.25</td>
<td>92.68</td>
<td>76.23</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LoRA + GNP</td>
<td>83.40</td>
<td>72.45</td>
<td>64.31</td>
<td>75.49</td>
<td>76.25</td>
<td>92.68</td>
<td>77.43</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\Delta_{L o R A}$</td>
<td>$\uparrow 3.73 \%$</td>
<td>$\uparrow 1.57 \%$</td>
<td>$\uparrow 0.86 \%$</td>
<td>$\uparrow 3.50 \%$</td>
<td>$\uparrow 0.00 \%$</td>
<td>$\uparrow 0.00 \%$</td>
<td>$\uparrow 1.58 \%$</td>
</tr>
<tr>
<td>FLAN-T5 xxlarge (11B)</td>
<td>LLM Frozen</td>
<td>LLM-only</td>
<td>76.80</td>
<td>68.93</td>
<td>56.58</td>
<td>61.37</td>
<td>71.75</td>
<td>65.85</td>
<td>66.88</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Prompt Designs*</td>
<td>79.60</td>
<td>74.16</td>
<td>58.00</td>
<td>60.59</td>
<td>71.25</td>
<td>66.67</td>
<td>68.38</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KG Flattening REL</td>
<td>72.80</td>
<td>66.78</td>
<td>56.80</td>
<td>53.53</td>
<td>69.50</td>
<td>66.67</td>
<td>64.35</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KG Flattening BFS</td>
<td>72.40</td>
<td>66.95</td>
<td>56.37</td>
<td>54.90</td>
<td>68.75</td>
<td>65.85</td>
<td>64.20</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KAPING TH</td>
<td>60.60</td>
<td>57.25</td>
<td>53.21</td>
<td>48.43</td>
<td>68.75</td>
<td>66.67</td>
<td>59.15</td>
</tr>
<tr>
<td></td>
<td></td>
<td>KAPING OH</td>
<td>60.00</td>
<td>56.65</td>
<td>52.99</td>
<td>47.65</td>
<td>69.25</td>
<td>66.67</td>
<td>58.87</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Prompt Tuning</td>
<td>78.80</td>
<td>74.85</td>
<td>61.26</td>
<td>61.37</td>
<td>70.00</td>
<td>65.04</td>
<td>68.55</td>
</tr>
<tr>
<td></td>
<td></td>
<td>GNP</td>
<td>87.20</td>
<td>78.20</td>
<td>63.66</td>
<td>70.98</td>
<td>76.75</td>
<td>90.24</td>
<td>77.84</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\Delta_{P T}$</td>
<td>$\uparrow 10.66 \%$</td>
<td>$\uparrow 4.48 \%$</td>
<td>$\uparrow 3.92 \%$</td>
<td>$\uparrow 15.66 \%$</td>
<td>$\uparrow 9.64 \%$</td>
<td>$\uparrow 38.75 \%$</td>
<td>$\uparrow 13.54 \%$</td>
</tr>
<tr>
<td></td>
<td>LLM Tuned</td>
<td>Full Fine-tuning</td>
<td>89.40</td>
<td>76.82</td>
<td>65.61</td>
<td>80.78</td>
<td>78.00</td>
<td>92.68</td>
<td>80.55</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LoRA</td>
<td>88.60</td>
<td>78.54</td>
<td>65.61</td>
<td>74.90</td>
<td>77.75</td>
<td>91.06</td>
<td>79.41</td>
</tr>
<tr>
<td></td>
<td></td>
<td>LoRA + GNP</td>
<td>89.60</td>
<td>78.71</td>
<td>65.94</td>
<td>76.67</td>
<td>79.75</td>
<td>94.31</td>
<td>80.83</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\Delta_{L o R A}$</td>
<td>$\uparrow 1.13 \%$</td>
<td>$\uparrow 0.22 \%$</td>
<td>$\uparrow 0.50 \%$</td>
<td>$\uparrow 2.36 \%$</td>
<td>$\uparrow 2.57 \%$</td>
<td>$\uparrow 3.57 \%$</td>
<td>$\uparrow 1.79 \%$</td>
</tr>
</tbody>
</table>
<h2>Experiments</h2>
<p>In this section, we conduct extensive experiments to compare the performances of different models. We also show ablation study, model design comparison, and parameter sensitivity analysis to demonstrate the effectiveness of GNP. Moreover, we present case study and visualization to provide an intuitive understanding and illustrate how KGs benefit.</p>
<h2>Experiment setup</h2>
<p>Knowledge Graphs and Datasets. We conduct experiments on both the general domain (commonsense reasoning) and the biomedical domain (biomedical reasoning). For the used knowledge graphs, we consider ConceptNet (Speer, Chin, and Havasi 2017) that contains rich commonsense knowledge regarding the daily concepts, and Unified Medical Language System (UMLS) (Bodenreider 2004) that involves well-structured health and biomedical information. For datasets, we use four commonsense reasoning datasets, including OpenBookQA (OBQA) (Mihaylov et al. 2018), AI2 Reasoning Challenge (ARC) (Clark et al. 2018), Physical Interaction Question Answering (PIQA) (Bisk et al. 2020), and RiddleSense (Riddle) (Lin et al. 2021). In addition, we consider PubMedQA (Jin et al. 2019) and BioASQ (Tsatsaronis et al. 2015) for biomedical reasoning.
Two Settings: LLM Frozen vs. LLM Tuned. To fully
evaluate the model, we employ two settings: LLM Frozen and LLM Tuned. For LLM Frozen, we keep the parameters in LLM unchanged and only adapt the prompt. For LLM Tuned, the original LLM parameters are updated for downstream tasks by utilizing LoRA or full fine-tuning.
Baselines. In the setting of LLM Frozen, we compare with nine baselines, including LLM-only that uses no prompt, three prompt design methods that use different instructions as hard prompts, KG Flattening that flattens the nodes in the graph into a sequence via relevance score (REL) ranking (Yasunaga et al. 2022) or breadth-first search (BFS), KAPING (Baek, Aji, and Saffari 2023) that injects the important KG triples within one-hop (OH) and two-hop (TH) neighborhoods, and Prompt Tuning (Lester, Al-Rfou, and Constant 2021) that introduces soft prompts. In the setting of LLM Tuned, we compare with LoRA that updates partial LLM parameters. In addition, we include full model finetuning results as the referencing benchmark.
Implementation Details. For the proposed model, we set the learning rate to 1e-4, batch size to 8, hidden dimension of GNN to 1024, and training epochs to 50. In order to adapt the model effectively to each dataset, we search the GNN layers from 2 to 5, cross-modality pooling layers from 1 to 3, trade-off weight $\lambda$ from ${0.1,0.5}$, and link drop rate from ${0.1,0.3,0.7}$. We choose FLAN-T5 xlarge (3B parameters)</p>
<p>and xxlarge (11B parameters) as the LLMs used in this paper. We adjust the maximum sequence length of LLMs to best fit the question length for each dataset. We run all experiments on four NVIDIA Tesla V100 GPUs with 24GB RAM.</p>
<h2>Performance Comparison</h2>
<p>To comprehensively evaluate our model, we conduct rigorous experiments using various LLMs across two reasoning tasks under different settings. The results are reported in Table 1. According to the table, in the setting of LLM Frozen, we observe that the utilization of the prompt design instructions often yields performance improvement, compared to LLMonly that uses no instructions, though the enhancement is mostly marginal. Interestingly, the baseline methods that inject KG information directly (KG Flattening and KAPING) can significantly hurt the model performance. This aligns with our motivation that KGs contain irrelevant contexts for the downstream tasks that could introduce noises or even alter the semantics if not handled carefully. While Prompt Tuning shows improved outcomes using the trainable soft prompts, their improvement is trivial. In contrast, our GNP exhibits significant and notable performance improvements across various datasets, settings, and LLMs. For example, for the commonsense reasoning task, GNP provides $+25.37 \%$ improvement on Riddle for 3B LLM, and $+15.66 \%$ improvement for 11B LLM. In addition, for the biomedical reasoning task, GNP improves the performance by $+34.14 \%$ on BioASQ for 3B LLM and $+38.75 \%$ for 11B LLM. In general, GNP achieves an improvement of $\mathbf{+ 1 2 . 7 6 \%}$ and $+13.54 \%$ for 3B and 11B LLM, respectively.</p>
<p>In the setting of LLM Tuned, we first study the performance in comparison with LoRA and then report the model full fine-tuning for additional reference. As shown in the table, LoRA is a significantly more powerful approach than Prompt Tuning due to the direct update of the LLM internal parameters. Combining with the proposed GNP, the performance can be further improved. For example, GNP achieves $3.73 \%$ improvement on OBQA for 3B LLM, and $3.57 \%$ improvement on BioAQS for 11B LLM. Moreover, model full fine-tuning is an important reference to study the performance gap since LoRA only updates a small fraction of the model parameters. Surprisingly, we find that the incorporation of GNP can surpass the results of full finetuning. In contrast, relying solely on LoRA shows difficulties in achieving a comparable performance of full fine-tuning. In total, our final performance matches or surpasses model full fine-tuning in 10 out of 12 evaluations across different LLM sizes and datasets, as shown in Table 1.</p>
<h2>Ablation Study</h2>
<p>Since GNP contains various model components (i.e., crossmodality pooling (CMP), self-supervised link prediction (SLP), and domain projector (DP)), we conduct ablation studies to analyze the contributions of different components by removing each of them independently (see Table 2). Specifically, removing DP significantly affects the performance, showing that DP has a large contribution to the proposed method. In addition, the decreasing performances of removing CMP and SLP demonstrate the effectiveness</p>
<p>Table 2: Results of ablation study.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Variant</th>
<th style="text-align: center;">Commonsense</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Biomedical</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">PubMedQA</td>
<td style="text-align: center;">BioASQ</td>
</tr>
<tr>
<td style="text-align: center;">FLAN-T5 <br> xlarge (3B)</td>
<td style="text-align: center;">w/o CMP</td>
<td style="text-align: center;">78.00</td>
<td style="text-align: center;">69.44</td>
<td style="text-align: center;">76.00</td>
<td style="text-align: center;">86.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o SLP</td>
<td style="text-align: center;">78.80</td>
<td style="text-align: center;">69.18</td>
<td style="text-align: center;">75.75</td>
<td style="text-align: center;">88.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o DP</td>
<td style="text-align: center;">73.00</td>
<td style="text-align: center;">70.30</td>
<td style="text-align: center;">76.25</td>
<td style="text-align: center;">83.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GNP</td>
<td style="text-align: center;">79.80</td>
<td style="text-align: center;">71.85</td>
<td style="text-align: center;">76.75</td>
<td style="text-align: center;">89.43</td>
</tr>
<tr>
<td style="text-align: center;">FLAN-T5 <br> xxlarge (11B)</td>
<td style="text-align: center;">w/o CMP</td>
<td style="text-align: center;">85.20</td>
<td style="text-align: center;">76.91</td>
<td style="text-align: center;">75.75</td>
<td style="text-align: center;">87.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o SLP</td>
<td style="text-align: center;">83.60</td>
<td style="text-align: center;">76.74</td>
<td style="text-align: center;">73.25</td>
<td style="text-align: center;">89.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o DP</td>
<td style="text-align: center;">79.40</td>
<td style="text-align: center;">74.59</td>
<td style="text-align: center;">71.75</td>
<td style="text-align: center;">85.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GNP</td>
<td style="text-align: center;">87.20</td>
<td style="text-align: center;">78.20</td>
<td style="text-align: center;">76.25</td>
<td style="text-align: center;">90.24</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of integrating different model designs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Design</th>
<th style="text-align: center;">Commonsense</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Biomedical</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">PubMedQA</td>
<td style="text-align: center;">BioASQ</td>
</tr>
<tr>
<td style="text-align: center;">FLAN-T5 <br> xlarge (3B)</td>
<td style="text-align: center;">GNP</td>
<td style="text-align: center;">79.80</td>
<td style="text-align: center;">71.85</td>
<td style="text-align: center;">76.75</td>
<td style="text-align: center;">89.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DLP</td>
<td style="text-align: center;">79.80</td>
<td style="text-align: center;">70.30</td>
<td style="text-align: center;">75.50</td>
<td style="text-align: center;">89.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ RGNN</td>
<td style="text-align: center;">79.00</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">75.50</td>
<td style="text-align: center;">89.43</td>
</tr>
<tr>
<td style="text-align: center;">FLAN-T5 <br> xxlarge (11B)</td>
<td style="text-align: center;">GNP</td>
<td style="text-align: center;">87.20</td>
<td style="text-align: center;">78.20</td>
<td style="text-align: center;">76.25</td>
<td style="text-align: center;">90.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DLP</td>
<td style="text-align: center;">86.20</td>
<td style="text-align: center;">76.05</td>
<td style="text-align: center;">75.00</td>
<td style="text-align: center;">88.62</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ RGNN</td>
<td style="text-align: center;">85.20</td>
<td style="text-align: center;">76.48</td>
<td style="text-align: center;">75.25</td>
<td style="text-align: center;">89.43</td>
</tr>
</tbody>
</table>
<p>of CMP and SLP in enhancing the model. In most cases, SLP yields greater significance compared to CMP, while in BioASQ, CMP plays a more important role. Finally, the proposed GNP achieves the best results in all cases, indicating the strong capability of different components in our model.</p>
<h2>Model Design Comparison</h2>
<p>A salient property of GNP is the learning of Graph Neural Prompt for each data instance, i.e., various questions yield different retrieved subgraphs, resulting in unique prompts. Given its distinction to the dataset-level prompt (DLP) from Prompt Tuning that learns prompt for each dataset, we present the outcomes of integrating DLP for further investigation. As shown in Table 3, incorporating DLP cannot further boost the performance and might even diminish it in certain cases. This indicates that our instance-level prompt provides adequate guidance for LLM to perform well. In addition, we validate the importance of explicitly modeling relations using a widely-used Relational GNN (RGNN) (Zhang et al. 2022). The observed decline in performance suggests that a standard GNN is sufficient to capture the graph information, and explicitly modeling the relations might increase the difficulty of generating suitable guidance for the task.</p>
<h2>Parameter Sensitivity</h2>
<p>Next, we perform sensitivity analysis focusing on the following parameters: the number of GNN layers and the number of layers in the cross-modality pooling component. Impact of GNN layers. We evaluate the influence of GNN layers for both 3B and 11B models in Figure 3. According to the figure, we have the following observations. First, various datasets have different optimal numbers of GNN layers. To illustrate, for ARC, 3 layers can achieve the optimal performance while 4 layers perform the best for PubMedQA. Second, the optimal number of GNN layers for 3B and 11B</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance w.r.t. different number of GNN layers.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance w.r.t. different number of cross-modality pooling layers.</p>
<p>LLMs differ. For example, for OBQA, 3 layers work best for 3B LLM, while 11B LLM reaches its top performance when using 5 layers. Third, choosing different GNN layers can have a weak impact on some datasets while can also drastically affect the performance on other datasets. To demonstrate, increasing from 3 layers to 5 layers for 11B LLM can decrease the performance on ARC by a large margin (from 78.1 to 74.3), while adjusting the layers for BioASQ may not lead to a big change in the performance.</p>
<p><strong>Impact of cross-modality pooling layers.</strong> We report the performance of different cross-modality pooling layers in Figure 4. As shown in the figure, we observe that the commonsense reasoning dataset OBQA and biomedical reasoning dataset BioASQ demonstrate different reactions to layer numbers. Specifically, for OBQA, the performance of the larger 11B LLM increases with more layers, while the performance of the smaller 3B LLM decreases. On the other hand, for BioASQ, the larger 11B LLM tends to show a degraded performance when adding more layers, while the smaller 3B model presents an improved performance. This indicates that suitable cross-modality pooling layers can lead to the best model performance.</p>
<h3>Case Study and Visualization</h3>
<p>For a more intuitive understanding and comparison, we randomly select two examples from the OBQA dataset and visualize the retrieved subgraphs in Figure 5. For visualization clarity, we only show question entities and a limited number of their neighbors. We remarkably notice that the retrieved subgraphs encompass certain entities for the correct answer, and there exist edges connecting the question and answer entities, which makes the task of question answering easier by leveraging this information.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Case study on two QA examples from OBQA dataset. Question entities are marked in green and their subsampled neighbors in the KG are marked in blue. The entities appearing in the correct answer are marked in orange.</p>
<p>To answer the question "What is the best way to guess a babies eye color?", Prompt Tuning makes the wrong generation "Just take a random guess". On the other hand, our retrieved subgraph offers the links that directly relate the entity "babies" to "family", "record", and further to "genealogy", which all appear in the correct option (d). This important context provides valuable insights for the model. Note that the subgraph also contains irrelevant entities such as "round" and "nursery". This explains why directly using the knowledge graph can introduce noise. However, our GNP method possesses the capability to collect the most critical information in the graph to determine the correct answer.</p>
<p>The second question "were there fossil fuels in the ground when humans evolved?" requires correctly identifying the historical sequencing order between the entity "humans" and "fossil fuels". The retrieved subgraph contains the critical relation, i.e., "humans", "evolve", "prior", "fossil fuel". Nevertheless, the subgraph also contains the entity "created" that could confuse the model into selecting option (a). GNP is able to capture the structural proximity among the key entities and select the correct answer (c).</p>
<h3>Conclusion</h3>
<p>In this paper, we address the limitations of LLMs in precisely capturing and returning grounded knowledge. In particular, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. Extensive experiments on commonsense and biomedical reasoning tasks demonstrate that GNP can improve the performance by +13.5% when LLM is frozen, and +1.8% when LLM is tuned. In addition, we present ablation studies, model design comparison, parameter sensitivity, case study and visualization to validate the effectiveness of the proposed method.</p>
<h2>References</h2>
<p>Anil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.
Baek, J.; Aji, A. F.; and Saffari, A. 2023. KnowledgeAugmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. In ACL Workshop on Matching Entities.
Bang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie, B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.
Bisk, Y.; Zellers, R.; Gao, J.; Choi, Y.; et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In AAAI.
Bodenreider, O. 2004. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic acids research.
Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In NeurIPS.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. In NeurIPS.
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.
Chen, X.; Jiang, J.-Y.; Chang, W.-C.; Hsieh, C.-J.; Yu, H.-F.; and Wang, W. 2023a. MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering. arXiv preprint arXiv:2310.05007.
Chen, X.; Liu, Y.; Yang, Y.; Yuan, J.; You, Q.; Liu, L.-P.; and Yang, H. 2023b. Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-toImage Synthesis. arXiv preprint arXiv:2311.17126.
Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.
Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
Feng, Y.; Chen, X.; Lin, B. Y.; Wang, P.; Yan, J.; and Ren, X. 2020. Scalable multi-hop relational reasoning for knowledgeaware question answering. In EMNLP.
Hong, J.; Wang, J. T.; Zhang, C.; Li, Z.; Li, B.; and Wang, Z. 2023. DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer. arXiv preprint arXiv:2312.03724.
Howard, J.; and Ruder, S. 2018. Universal language model fine-tuning for text classification. In $A C L$.</p>
<p>Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR.
Ji, S.; Pan, S.; Cambria, E.; Marttinen, P.; and Philip, S. Y. 2021. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems.
Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.; Bang, Y. J.; Madotto, A.; and Fung, P. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys.
Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W. W.; and Lu, X. 2019. Pubmedqa: A dataset for biomedical research question answering. In EMNLP.
Ke, P.; Ji, H.; Ran, Y.; Cui, X.; Wang, L.; Song, L.; Zhu, X.; and Huang, M. 2021. Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. In ACLIJCNLP.
Kou, Z.; Zhang, Y.; Zhang, D.; and Wang, D. 2022. CrowdGraph: A Crowdsourcing Multi-Modal Knowledge Graph Approach to Explainable Fautography Detection. Proceedings of the ACM on Human-Computer Interaction.
Lester, B.; Al-Rfou, R.; and Constant, N. 2021. The power of scale for parameter-efficient prompt tuning. In EMNLP.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In NeurIPS.
Li, X. L.; and Liang, P. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In ACL-IJCNLP.
Lin, B. Y.; Chen, X.; Chen, J.; and Ren, X. 2019. Kagnet: Knowledge-aware graph networks for commonsense reasoning. In EMNLP.
Lin, B. Y.; Wu, Z.; Yang, Y.; Lee, D.-H.; and Ren, X. 2021. RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. In ACLIJCNLP.
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig, G. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys.
Lu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-C.; Tafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS.
Lv, S.; Guo, D.; Xu, J.; Tang, D.; Duan, N.; Gong, M.; Shou, L.; Jiang, D.; Cao, G.; and Hu, S. 2020. Graphbased reasoning over heterogeneous external knowledge for commonsense question answering. In AAAI.
Mihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.
Mihaylov, T.; and Frank, A. 2018. Knowledgeable reader: Enhancing cloze-style reading comprehension with external commonsense knowledge. In $A C L$.</p>
<p>OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.
Pan, S.; Luo, L.; Wang, Y.; Chen, C.; Wang, J.; and Wu, X. 2023. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv preprint arXiv:2306.08302.
Ren, H.; Dai, H.; Dai, B.; Chen, X.; Yasunaga, M.; Sun, H.; Schuurmans, D.; Leskovec, J.; and Zhou, D. 2021. Lego: Latent execution-guided reasoning for multi-hop question answering on knowledge graphs. In ICML.
Robinson, J.; Rytting, C. M.; and Wingate, D. 2023. Leveraging large language models for multiple choice question answering. In $I C L R$.
Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.; Hesslow, D.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.
Shi, Y.; Xu, S.; Liu, Z.; Liu, T.; Li, X.; and Liu, N. 2023. Mededit: Model editing for medical question answering with external knowledge bases. arXiv preprint arXiv:2309.16035.
Smith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhandari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.; Zerveas, G.; Korthikanti, V.; et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.
Speer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In AAAI.
Sun, Y.; Wang, S.; Feng, S.; Ding, S.; Pang, C.; Shang, J.; Liu, J.; Chen, X.; Zhao, Y.; Lu, Y.; et al. 2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137.
Tang, Z.; Pei, S.; Zhang, Z.; Zhu, Y.; Zhuang, F.; Hoehndorf, R.; and Zhang, X. 2022. Positive-unlabeled learning with adversarial data augmentation for knowledge graph completion. In IJCAI.
Tian, Y.; Dong, K.; Zhang, C.; Zhang, C.; and Chawla, N. V. 2023a. Heterogeneous Graph Masked Autoencoders. In AAAI.
Tian, Y.; Zhang, C.; Guo, Z.; Zhang, X.; and Chawla, N. 2023b. Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency. In ICLR.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
Tsatsaronis, G.; Balikas, G.; Malakasiotis, P.; Partalas, I.; Zschunke, M.; Alvers, M. R.; Weissenborn, D.; Krithara, A.; Petridis, S.; Polychronopoulos, D.; et al. 2015. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics.
Veličković, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and Bengio, Y. 2018. Graph attention networks. In ICLR.
Wang, X.; Kapanipathi, P.; Musa, R.; Yu, M.; Talamadupula, K.; Abdelaziz, I.; Chang, M.; Fokoue, A.; Makni, B.; Mattei,
N.; et al. 2019. Improving natural language inference using external knowledge in the science questions domain. In AAAI. Wang, Y.; Jin, W.; and Derr, T. 2022. Graph neural networks: Self-supervised learning. Graph Neural Networks: Foundations, Frontiers, and Applications.
Wang, Y.; Lipka, N.; Rossi, R. A.; Siu, A.; Zhang, R.; and Derr, T. 2023. Knowledge Graph Prompting for Multi-Document Question Answering. arXiv preprint arXiv:2308.11730.
Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester, B.; Du, N.; Dai, A. M.; and Le, Q. V. 2022a. Finetuned language models are zero-shot learners. In ICLR.
Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; et al. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research.
Wei, W.; Ren, X.; Tang, J.; Wang, Q.; Su, L.; Cheng, S.; Wang, J.; Yin, D.; and Huang, C. 2024. LLMRec: Large Language Models with Graph Augmentation for Recommendation. In WSDM.
Williams, R. J.; and Zipser, D. 1989. A learning algorithm for continually running fully recurrent neural networks. Neural computation.
Xu, Z.; Zeng, H.; Tan, J.; Fu, Z.; Zhang, Y.; and Ai, Q. 2023. A Reusable Model-agnostic Framework for Faithfully Explainable Recommendation and System Scrutability. ACM Transactions on Information Systems.
Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng, L. 2015. Embedding entities and relations for learning and inference in knowledge bases. In ICLR.
Yasunaga, M.; Bosselut, A.; Ren, H.; Zhang, X.; Manning, C. D.; Liang, P. S.; and Leskovec, J. 2022. Deep bidirectional language-knowledge graph pretraining. In NeurIPS.
Yasunaga, M.; Leskovec, J.; and Liang, P. 2022. Linkbert: Pretraining language models with document links. In ACL.
Yasunaga, M.; Ren, H.; Bosselut, A.; Liang, P.; and Leskovec, J. 2021. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In NAACL.
Yu, D.; Zhu, C.; Yang, Y.; and Zeng, M. 2022. Jaket: Joint pre-training of knowledge graph and language understanding. In AAAI.
Zhang, T.; Ladhak, F.; Durmus, E.; Liang, P.; McKeown, K.; and Hashimoto, T. B. 2023. Benchmarking large language models for news summarization. arXiv preprint arXiv:2301.13848.
Zhang, X.; Bosselut, A.; Yasunaga, M.; Ren, H.; Liang, P.; Manning, C. D.; and Leskovec, J. 2022. GreaseLM: Graph REASoning Enhanced Language Models for Question Answering. In ICLR.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223. Zhu, F.; Lei, W.; Wang, C.; Zheng, J.; Poria, S.; and Chua, T.-S. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
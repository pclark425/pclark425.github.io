<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1133 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1133</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1133</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-244113620</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2109.08518v1.pdf" target="_blank">Knowledge is reward: Learning optimal exploration by predictive reward cashing</a></p>
                <p><strong>Paper Abstract:</strong> There is a strong link between the general concept of intelligence and the ability to collect and use information. The theory of Bayes-adaptive exploration offers an attractive optimality framework for training machines to perform complex information gathering tasks. However, the computational complexity of the resulting optimal control problem has limited the diffusion of the theory to mainstream deep AI research. In this paper we exploit the inherent mathematical structure of Bayes-adaptive problems in order to dramatically simplify the problem by making the reward structure denser while simultaneously decoupling the learning of exploitation and exploration policies. The key to this simplification comes from the novel concept of cross-value (i.e. the value of being in an environment while acting optimally according to another), which we use to quantify the value of currently available information. This results in a new denser reward structure that"cashes in"all future rewards that can be predicted from the current information state. In a set of experiments we show that the approach makes it possible to learn challenging information gathering tasks without the use of shaping and heuristic bonuses in situations where the standard RL algorithms fail.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1133.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1133.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCR-TD (tabular, T-maze)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictively Cashed Reward Temporal-Difference learning (tabular) applied to T-maze disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tabular instantiation of the Predictively Cashed Reward (PCR) method that decomposes belief-augmented value into value-of-current-information (cross-values) plus value-of-future-information, and trains a TD learner on a dense, predictively-cashed reward to drive information-seeking actions in partially observable environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PCR-TD (tabular)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular Q-learning / TD agent that stores v_f (value of future information) in a table (only nonzero for prior belief state) and computes predictively-cashed reward λ_t from sampled cross-values; cross-values learned via belief-horizon sampling from belief-augmented transitions. Policy is greedy w.r.t. v* = v_c + v_f during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayes-adaptive exploration via belief-augmentation and Thompson-style posterior sampling (operationalized as sampling cross-values) with a predictively-cashed reward shaping (PCR)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent maintains a belief over discrete environments (here only two environments). It samples environments from its belief to compute cross-values (value-of-current-information) and forms a dense predictively-cashed reward λ_t = r_t + γ v_c(next) − v_c(current). It updates v_f via TD on λ_t, causing the agent to prefer actions (e.g., visiting the disambiguation cue) that immediately increase expected exploitative value once belief collapses.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>T-shaped maze disambiguation (two-environment ambiguous cue)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (latent environment parameter indicating which arm has reward), deterministic transitions and observations for cue and rewards, discrete state space, sparse true exploitation reward until disambiguation is performed.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Small discrete maze; belief has 3 states {prior 0.5, collapsed left, collapsed right}; episode length T = 20; S (maze length) variable (state-space size = S), action set: move up/down/left/right depending on location.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>PCR-TD learned the optimal information-gathering policy and achieved near-optimal cumulative reward in most independent repetitions (explicit numeric averages not provided for this task in the paper; described qualitatively as 'optimal').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Standard tabular Q-learning baseline (belief-augmented TD with ε-greedy) learned deterministic dynamics but failed to learn information gathering, resulting in zero total collected reward across episodes (qualitative failure).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Tabular PCR-TD learned effective information-gathering within the reported training regime used across repetitions; exact sample counts not tabulated but episodes were T=20 and training proceeded across epochs until convergence (figures show rapid learning relative to baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly decoupled: exploitation value handled by v_c (cross-values) estimated from sampled environments; exploration objective learned as v_f via TD on predictively-cashed reward. This turns long-horizon information value into immediate dense rewards that drive exploratory actions until belief collapses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Baseline tabular Q-learning (belief-augmented TD with ε-greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Predictive reward cashing converts sparse information value into an immediate reward signal (λ_t) that enables TD learners to acquire targeted information (visit cue) and then exploit; in the T-maze PCR-TD attained the optimal behaviour while standard TD failed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>This tabular success relies on deterministic inference with a small finite set of environments (belief space collapse to a one-hot posterior); the tabular algorithm exploits that property (S+1 belief states). The paper notes that tabular approaches are infeasible for large/infinite belief spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge is reward: Learning optimal exploration by predictive reward cashing', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1133.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1133.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCR-TD (conv, Treasure Map)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictively Cashed Reward Temporal-Difference learning with convolutional value-of-future-information network applied to spatial 'treasure map' task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep RL implementation of PCR where v_f is approximated by a small convolutional network over belief parameters, cross-values are computed via value-iteration (when possible) or belief-horizon sampling, and the agent is trained with TD on predictively-cashed rewards to actively seek an information-bearing 'map' cell before exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PCR-TD (conv)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular architecture: (1) cross-values computed via value-iteration applied to expected environment parameters (or learned by value iteration / belief-horizon sampling), (2) a convolutional network mapping the spatial belief (beta posterior parameters per cell) to v_f outputs (one per spatial location), (3) TD learning on predictively-cashed reward λ_t, and (4) an ε-greedy controller using v* = v_c + v_f for action selection. Cross-value sampling for λ_t used N = 80 samples in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian active information acquisition using belief-augmented RL: cross-values (posterior-sampled optimal values) + predictively-cashed reward to incentivize visiting information-rich 'map' cell; uses sampling from belief and Thompson-style elements for stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Agent maintains independent Beta posteriors per cell (analytic belief). When on a cell it observes reward samples and additional simulated 'pulls' (information). The central 'treasure map' cell, if visited, provides simulated observations for all cells, drastically reducing posterior uncertainty. The agent uses v_c (estimated via cross-values) to compute immediate gains from belief updates and uses v_f approximator to learn the policy to visit the map when beneficial, then exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Treasure map spatial multi-armed bandit (grid-world 3×3, 5×5, 7×7)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (unknown Bernoulli reward probabilities per cell), stochastic rewards (Bernoulli), discrete spatial grid, sparse global structure where one special cell yields broad information (map), episode length T = 25, per-step discounted reward γ = 0.96.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Grid sizes tested: 3×3, 5×5, 7×7. Action space: up to 8-neighbor moves + stay (variable at borders). Each episode T=25 steps. Belief state: Beta counts per cell (analytically updatable). Training: 2000 epochs, batches of 10 environments per epoch; test: 20 trials per environment batch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Quantitative: Average (non-discounted) total rewards (mean ± std) at test: 3×3: 10.78 ± 0.24; 5×5: 15.17 ± 0.22; 7×7: 15.33 ± 0.16. Only PCR-TD learned to systematically visit the 'map' cell and then exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (same evaluation): Standard TD (deep belief-augmented TD): 3×3: 7.30 ± 0.31; 5×5: 10.46 ± 0.37; 7×7: 10.42 ± 0.26. VI-TD (value-iteration + learned correction): 3×3: 8.28 ± 0.21; 5×5: 11.02 ± 0.75; 7×7: 9.55 ± 0.82. VI-Thompson: 3×3: 10.41 ± 0.16; 5×5: 11.72 ± 0.12; 7×7: 10.41 ± 0.15. VI-Greedy: 3×3: 9.39 ± 0.16; 5×5: 9.91 ± 0.11; 7×7: 9.64 ± 0.10.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training ran for 2000 epochs with batches of 10 sampled environments per epoch; cross-value estimation used N=80 samples for λ_t. PCR-TD achieved substantially higher test rewards within this regime while baselines did not learn the map-seeking strategy within the same budget.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Decoupled: v_c (cross-values) provides an exploitation baseline computed from sampled full-information optimal policies; v_f learned from predictively-cashed rewards gives credit for information acquisition. This makes information-seeking immediately rewarding and enables switching from exploration (visiting map) to exploitation (collecting high-probability cells) when beliefs change.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Baseline deep belief-augmented TD (TD), VI-TD (value-iteration + learned correction), VI-Thompson (Thompson/ posterior-sampling on VI values), VI-Greedy (greedy on VI values).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>PCR-TD is the only method among tested baselines that reliably learned to visit the central treasure-map cell and subsequently exploit, producing substantially higher cumulative rewards across grid sizes; predictive reward cashing converts future expected information gains into immediate training signal, alleviating sparsity and the chicken-and-egg exploration/exploitation coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>PCR-TD relies on either computable cross-values (value-iteration) or accurate cross-value estimates (belief-horizon sampling); computing cross-values can be expensive if transition/reward models are unknown and value-iteration must be approximated. The method's effectiveness depends on the ability to estimate v_c reasonably (the paper uses N=80 samples / value-iteration), and performance in high-dimensional or non-conjugate inference settings requires approximate inference modules (not extensively evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge is reward: Learning optimal exploration by predictive reward cashing', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1133.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1133.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief Horizon Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief Horizon Sampling (training method for cross-values)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training procedure introduced in this paper that samples a 'ground truth' environment from the terminal belief state of an episode and uses it as if it were the true environment to update cross-value estimates, enabling learning of cross-values when the actual environment is not directly observed during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief Horizon Sampling (used for cross-value learning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent per se but a training method: at episode end the terminal posterior b_T is used to sample an environment e, which is then used as the 'ground truth' to index and update cross-value tables or networks via TD updates; this approximates training cross-values as T→∞ and can converge under deterministic inference assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-training technique for Bayesian posterior-sampled evaluation (enables learning cross-values for posterior-sampled environments)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses the agent's accumulated observations in an episode to sample a terminal-belief environment and then treats that sampled environment as ground truth to perform cross-value TD updates, thereby adapting cross-value estimates to the distributions generated by the agent's behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Method applies when agent operates in partially observable environments where the environment parameter can be represented as a finite set or where posterior concentrates; paper proves convergence under deterministic inference (belief collapse to one-hot posteriors).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Applicable for problems where belief state dynamics lead to terminal beliefs (collapse) or where episodes are long enough for posterior concentration; complexity depends on environment's posterior convergence behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Enables learning of cross-values when ground truth environment is not available at training time; in experiments it allowed PCR-TD to succeed in tabular T-maze and to jointly train cross-values and v_f (qualitative/indirect quantitative improvements reported via downstream agent performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Without such a method cross-values would require explicit ground-truth environment labels or costly model-based planning; paper shows baselines lacking PCR reward fail to learn map-seeking behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Belief horizon sampling re-uses agent episodes for cross-value updates; sample efficiency depends on episode length and posterior concentration; converges under standard TD conditions in deterministic-inference settings.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>This method helps bootstrap cross-value estimates so the agent receives meaningful v_c signals earlier, thereby helping the PCR mechanism balance exploration and exploitation faster.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Not directly compared to other cross-value estimation schemes in experiments, but paper contrasts it with having direct access to ground-truth e during training or computing cross-values by explicit value-iteration when models are known.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Belief horizon sampling converges to true cross-values in deterministic-inference settings and works well in practice to jointly train cross-values and v_f from belief-augmented transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Convergence proof is informal and relies on deterministic inference (beliefs collapse to one-hot posteriors); when posterior does not concentrate within episode length, sampled terminal beliefs may be noisy and cross-value updates noisier, potentially slowing convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge is reward: Learning optimal exploration by predictive reward cashing', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1133.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1133.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard belief-augmented TD (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Belief-augmented Temporal-Difference / Q-learning baseline (ε-greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional TD / Q-learning approach applied to the belief-augmented MDP (value function depends on state x and belief b), trained on the original sparse reward structure without predictive reward cashing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief-augmented TD (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular (in maze experiment) or deep (in treasure map) value function approximator trained with TD on the true sparse rewards, using ε-greedy exploration; in deep case the network outputs a correction f_jk added to expected reward probability or value-iteration baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>None specific — uses ε-greedy (stochastic exploration) and in some variants uses Thompson/VI baselines; not explicitly optimizing value of information.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Explores via ε-greedy random actions (tabular baseline used ε = 0.5 during q-learning); does not explicitly use belief-derived immediate information value to decide exploratory actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>T-maze (tabular) and Treasure map (deep baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as experimental tasks: partially observable / unknown environment parameters; sparse rewards for information-gathering actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as above (T-maze: S states, T=20; treasure map: 3×3–7×7 grids, T=25).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>As baseline, achieved lower cumulative rewards: in treasure map tests: 3×3: 7.30 ± 0.31; 5×5: 10.46 ± 0.37; 7×7: 10.42 ± 0.26. In T-maze it failed to learn information gathering, producing zero total reward.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Failed to learn targeted exploration within the training budgets used (2000 epochs for deep; epochs unspecified for tabular), indicating poor sample efficiency for learning information-seeking behaviour with sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Uses ε-greedy: fixed probability of random exploration versus greedy exploitation; does not adapt exploration based on expected information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to PCR-TD, VI-TD, VI-Thompson, VI-Greedy in treasure map experiments and to PCR-TD in tabular T-maze.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Baseline TD methods fail to reliably learn targeted information-gathering behaviour in these sparse-reward belief-augmented tasks; replacing sparse rewards with predictively-cashed rewards (PCR) remedies this.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fails on tasks where information-gathering actions have no immediate reward and only indirectly enable future exploitation (the chicken-and-egg problem).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge is reward: Learning optimal exploration by predictive reward cashing', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1133.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1133.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VI-TD / VI-Thompson / VI-Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value-Iteration augmented TD and greedy/Thompson variants (comparative baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that combine value-iteration computed on expected reward probabilities (from belief) with learned corrections and different decision rules: VI-TD adds a learned correction to the VI value, VI-Thompson uses posterior sampling on VI values, and VI-Greedy acts greedily on VI values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VI-TD / VI-Thompson / VI-Greedy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>VI-TD: belief-augmented value = value-iteration solution on expected rewards + a learned correction network (FC); VI-Thompson: perform Thompson sampling on VI-derived environment values; VI-Greedy: greedy policy on VI values. Architectures: VI-TD uses a fully-connected network with large hidden layers; VI variants leverage model-based VI where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Model-based exploitation of expected-value via value-iteration; VI-Thompson adds posterior sampling (Thompson) over VI-derived environments as an exploration method.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>VI-based methods compute exploitation value from expected reward probabilities (belief), and adapt behavior by either acting greedily on that VI value, sampling environment models from the belief (Thompson on VI) or learning corrections to VI via a learned function approximator (VI-TD). They do not use PCR immediate information-crediting mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Treasure map spatial multi-armed bandit (3×3, 5×5, 7×7)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable, stochastic Bernoulli rewards, discrete grid, analytic Beta beliefs over cell probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as treasure map: grids 3–7 size, T=25, action space up to 9 moves, training 2000 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>VI-Thompson: 3×3: 10.41 ± 0.16; 5×5: 11.72 ± 0.12; 7×7: 10.41 ± 0.15. VI-Greedy and VI-TD had lower performance: VI-Greedy 3×3: 9.39 ± 0.16; 5×5: 9.91 ± 0.11; 7×7: 9.64 ± 0.10. VI-TD: 3×3: 8.28 ± 0.21; 5×5: 11.02 ± 0.75; 7×7: 9.55 ± 0.82.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Operates with model-based VI that can be computed quickly when expected parameters are known; however, VI-based methods here did not match PCR-TD in producing reliable map-seeking behaviours within the same training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>VI-Thompson explicitly uses posterior sampling to encourage exploration; VI-Greedy uses no explicit exploration other than any ε-greedy training schedule; VI-TD tries to learn residual exploration/exploitation complexity via function approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against PCR-TD and standard TD in treasure map experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>VI-Thompson sometimes attains performance near PCR-TD for small grid (3×3) but fails on larger grids (5×5,7×7) to consistently learn map-seeking behaviour; VI-Greedy and VI-TD underperform PCR-TD across grid sizes. PCR-TD outperforms these VI-based baselines overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>VI methods can fail because the representation of exploration value is not directly rewarded; VI-TD relies on learned corrections which can be insufficient to shape exploration for tasks with delayed information payoff. VI-Thompson shows partial success but is inconsistent across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge is reward: Learning optimal exploration by predictive reward cashing', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Varibad: A very good method for bayes-adaptive deep RL via meta-learning <em>(Rating: 2)</em></li>
                <li>Exploration in approximate hyper-state space for meta reinforcement learning <em>(Rating: 2)</em></li>
                <li>Decoupling exploration and exploitation for meta-reinforcement learning without sacrifices <em>(Rating: 2)</em></li>
                <li>Efficient Bayes-adaptive reinforcement learning using sample-based search <em>(Rating: 2)</em></li>
                <li>Bootstrapped Thompson sampling and deep exploration <em>(Rating: 1)</em></li>
                <li>Learning is planning: Near Bayes-optimal reinforcement learning via Monte-Carlo tree search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1133",
    "paper_id": "paper-244113620",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "PCR-TD (tabular, T-maze)",
            "name_full": "Predictively Cashed Reward Temporal-Difference learning (tabular) applied to T-maze disambiguation",
            "brief_description": "A tabular instantiation of the Predictively Cashed Reward (PCR) method that decomposes belief-augmented value into value-of-current-information (cross-values) plus value-of-future-information, and trains a TD learner on a dense, predictively-cashed reward to drive information-seeking actions in partially observable environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PCR-TD (tabular)",
            "agent_description": "Tabular Q-learning / TD agent that stores v_f (value of future information) in a table (only nonzero for prior belief state) and computes predictively-cashed reward λ_t from sampled cross-values; cross-values learned via belief-horizon sampling from belief-augmented transitions. Policy is greedy w.r.t. v* = v_c + v_f during evaluation.",
            "adaptive_design_method": "Bayes-adaptive exploration via belief-augmentation and Thompson-style posterior sampling (operationalized as sampling cross-values) with a predictively-cashed reward shaping (PCR)",
            "adaptation_strategy_description": "Agent maintains a belief over discrete environments (here only two environments). It samples environments from its belief to compute cross-values (value-of-current-information) and forms a dense predictively-cashed reward λ_t = r_t + γ v_c(next) − v_c(current). It updates v_f via TD on λ_t, causing the agent to prefer actions (e.g., visiting the disambiguation cue) that immediately increase expected exploitative value once belief collapses.",
            "environment_name": "T-shaped maze disambiguation (two-environment ambiguous cue)",
            "environment_characteristics": "Partially observable (latent environment parameter indicating which arm has reward), deterministic transitions and observations for cue and rewards, discrete state space, sparse true exploitation reward until disambiguation is performed.",
            "environment_complexity": "Small discrete maze; belief has 3 states {prior 0.5, collapsed left, collapsed right}; episode length T = 20; S (maze length) variable (state-space size = S), action set: move up/down/left/right depending on location.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "PCR-TD learned the optimal information-gathering policy and achieved near-optimal cumulative reward in most independent repetitions (explicit numeric averages not provided for this task in the paper; described qualitatively as 'optimal').",
            "performance_without_adaptation": "Standard tabular Q-learning baseline (belief-augmented TD with ε-greedy) learned deterministic dynamics but failed to learn information gathering, resulting in zero total collected reward across episodes (qualitative failure).",
            "sample_efficiency": "Tabular PCR-TD learned effective information-gathering within the reported training regime used across repetitions; exact sample counts not tabulated but episodes were T=20 and training proceeded across epochs until convergence (figures show rapid learning relative to baseline).",
            "exploration_exploitation_tradeoff": "Explicitly decoupled: exploitation value handled by v_c (cross-values) estimated from sampled environments; exploration objective learned as v_f via TD on predictively-cashed reward. This turns long-horizon information value into immediate dense rewards that drive exploratory actions until belief collapses.",
            "comparison_methods": "Baseline tabular Q-learning (belief-augmented TD with ε-greedy).",
            "key_results": "Predictive reward cashing converts sparse information value into an immediate reward signal (λ_t) that enables TD learners to acquire targeted information (visit cue) and then exploit; in the T-maze PCR-TD attained the optimal behaviour while standard TD failed.",
            "limitations_or_failures": "This tabular success relies on deterministic inference with a small finite set of environments (belief space collapse to a one-hot posterior); the tabular algorithm exploits that property (S+1 belief states). The paper notes that tabular approaches are infeasible for large/infinite belief spaces.",
            "uuid": "e1133.0",
            "source_info": {
                "paper_title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "PCR-TD (conv, Treasure Map)",
            "name_full": "Predictively Cashed Reward Temporal-Difference learning with convolutional value-of-future-information network applied to spatial 'treasure map' task",
            "brief_description": "A deep RL implementation of PCR where v_f is approximated by a small convolutional network over belief parameters, cross-values are computed via value-iteration (when possible) or belief-horizon sampling, and the agent is trained with TD on predictively-cashed rewards to actively seek an information-bearing 'map' cell before exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PCR-TD (conv)",
            "agent_description": "Modular architecture: (1) cross-values computed via value-iteration applied to expected environment parameters (or learned by value iteration / belief-horizon sampling), (2) a convolutional network mapping the spatial belief (beta posterior parameters per cell) to v_f outputs (one per spatial location), (3) TD learning on predictively-cashed reward λ_t, and (4) an ε-greedy controller using v* = v_c + v_f for action selection. Cross-value sampling for λ_t used N = 80 samples in experiments.",
            "adaptive_design_method": "Bayesian active information acquisition using belief-augmented RL: cross-values (posterior-sampled optimal values) + predictively-cashed reward to incentivize visiting information-rich 'map' cell; uses sampling from belief and Thompson-style elements for stochasticity.",
            "adaptation_strategy_description": "Agent maintains independent Beta posteriors per cell (analytic belief). When on a cell it observes reward samples and additional simulated 'pulls' (information). The central 'treasure map' cell, if visited, provides simulated observations for all cells, drastically reducing posterior uncertainty. The agent uses v_c (estimated via cross-values) to compute immediate gains from belief updates and uses v_f approximator to learn the policy to visit the map when beneficial, then exploit.",
            "environment_name": "Treasure map spatial multi-armed bandit (grid-world 3×3, 5×5, 7×7)",
            "environment_characteristics": "Partially observable (unknown Bernoulli reward probabilities per cell), stochastic rewards (Bernoulli), discrete spatial grid, sparse global structure where one special cell yields broad information (map), episode length T = 25, per-step discounted reward γ = 0.96.",
            "environment_complexity": "Grid sizes tested: 3×3, 5×5, 7×7. Action space: up to 8-neighbor moves + stay (variable at borders). Each episode T=25 steps. Belief state: Beta counts per cell (analytically updatable). Training: 2000 epochs, batches of 10 environments per epoch; test: 20 trials per environment batch.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Quantitative: Average (non-discounted) total rewards (mean ± std) at test: 3×3: 10.78 ± 0.24; 5×5: 15.17 ± 0.22; 7×7: 15.33 ± 0.16. Only PCR-TD learned to systematically visit the 'map' cell and then exploit.",
            "performance_without_adaptation": "Baselines (same evaluation): Standard TD (deep belief-augmented TD): 3×3: 7.30 ± 0.31; 5×5: 10.46 ± 0.37; 7×7: 10.42 ± 0.26. VI-TD (value-iteration + learned correction): 3×3: 8.28 ± 0.21; 5×5: 11.02 ± 0.75; 7×7: 9.55 ± 0.82. VI-Thompson: 3×3: 10.41 ± 0.16; 5×5: 11.72 ± 0.12; 7×7: 10.41 ± 0.15. VI-Greedy: 3×3: 9.39 ± 0.16; 5×5: 9.91 ± 0.11; 7×7: 9.64 ± 0.10.",
            "sample_efficiency": "Training ran for 2000 epochs with batches of 10 sampled environments per epoch; cross-value estimation used N=80 samples for λ_t. PCR-TD achieved substantially higher test rewards within this regime while baselines did not learn the map-seeking strategy within the same budget.",
            "exploration_exploitation_tradeoff": "Decoupled: v_c (cross-values) provides an exploitation baseline computed from sampled full-information optimal policies; v_f learned from predictively-cashed rewards gives credit for information acquisition. This makes information-seeking immediately rewarding and enables switching from exploration (visiting map) to exploitation (collecting high-probability cells) when beliefs change.",
            "comparison_methods": "Baseline deep belief-augmented TD (TD), VI-TD (value-iteration + learned correction), VI-Thompson (Thompson/ posterior-sampling on VI values), VI-Greedy (greedy on VI values).",
            "key_results": "PCR-TD is the only method among tested baselines that reliably learned to visit the central treasure-map cell and subsequently exploit, producing substantially higher cumulative rewards across grid sizes; predictive reward cashing converts future expected information gains into immediate training signal, alleviating sparsity and the chicken-and-egg exploration/exploitation coupling.",
            "limitations_or_failures": "PCR-TD relies on either computable cross-values (value-iteration) or accurate cross-value estimates (belief-horizon sampling); computing cross-values can be expensive if transition/reward models are unknown and value-iteration must be approximated. The method's effectiveness depends on the ability to estimate v_c reasonably (the paper uses N=80 samples / value-iteration), and performance in high-dimensional or non-conjugate inference settings requires approximate inference modules (not extensively evaluated here).",
            "uuid": "e1133.1",
            "source_info": {
                "paper_title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Belief Horizon Sampling",
            "name_full": "Belief Horizon Sampling (training method for cross-values)",
            "brief_description": "A training procedure introduced in this paper that samples a 'ground truth' environment from the terminal belief state of an episode and uses it as if it were the true environment to update cross-value estimates, enabling learning of cross-values when the actual environment is not directly observed during training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief Horizon Sampling (used for cross-value learning)",
            "agent_description": "Not an agent per se but a training method: at episode end the terminal posterior b_T is used to sample an environment e, which is then used as the 'ground truth' to index and update cross-value tables or networks via TD updates; this approximates training cross-values as T→∞ and can converge under deterministic inference assumptions.",
            "adaptive_design_method": "Meta-training technique for Bayesian posterior-sampled evaluation (enables learning cross-values for posterior-sampled environments)",
            "adaptation_strategy_description": "Uses the agent's accumulated observations in an episode to sample a terminal-belief environment and then treats that sampled environment as ground truth to perform cross-value TD updates, thereby adapting cross-value estimates to the distributions generated by the agent's behaviour.",
            "environment_name": null,
            "environment_characteristics": "Method applies when agent operates in partially observable environments where the environment parameter can be represented as a finite set or where posterior concentrates; paper proves convergence under deterministic inference (belief collapse to one-hot posteriors).",
            "environment_complexity": "Applicable for problems where belief state dynamics lead to terminal beliefs (collapse) or where episodes are long enough for posterior concentration; complexity depends on environment's posterior convergence behavior.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Enables learning of cross-values when ground truth environment is not available at training time; in experiments it allowed PCR-TD to succeed in tabular T-maze and to jointly train cross-values and v_f (qualitative/indirect quantitative improvements reported via downstream agent performance).",
            "performance_without_adaptation": "Without such a method cross-values would require explicit ground-truth environment labels or costly model-based planning; paper shows baselines lacking PCR reward fail to learn map-seeking behaviour.",
            "sample_efficiency": "Belief horizon sampling re-uses agent episodes for cross-value updates; sample efficiency depends on episode length and posterior concentration; converges under standard TD conditions in deterministic-inference settings.",
            "exploration_exploitation_tradeoff": "This method helps bootstrap cross-value estimates so the agent receives meaningful v_c signals earlier, thereby helping the PCR mechanism balance exploration and exploitation faster.",
            "comparison_methods": "Not directly compared to other cross-value estimation schemes in experiments, but paper contrasts it with having direct access to ground-truth e during training or computing cross-values by explicit value-iteration when models are known.",
            "key_results": "Belief horizon sampling converges to true cross-values in deterministic-inference settings and works well in practice to jointly train cross-values and v_f from belief-augmented transitions.",
            "limitations_or_failures": "Convergence proof is informal and relies on deterministic inference (beliefs collapse to one-hot posteriors); when posterior does not concentrate within episode length, sampled terminal beliefs may be noisy and cross-value updates noisier, potentially slowing convergence.",
            "uuid": "e1133.2",
            "source_info": {
                "paper_title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Standard belief-augmented TD (baseline)",
            "name_full": "Belief-augmented Temporal-Difference / Q-learning baseline (ε-greedy)",
            "brief_description": "A conventional TD / Q-learning approach applied to the belief-augmented MDP (value function depends on state x and belief b), trained on the original sparse reward structure without predictive reward cashing.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Belief-augmented TD (baseline)",
            "agent_description": "Tabular (in maze experiment) or deep (in treasure map) value function approximator trained with TD on the true sparse rewards, using ε-greedy exploration; in deep case the network outputs a correction f_jk added to expected reward probability or value-iteration baseline.",
            "adaptive_design_method": "None specific — uses ε-greedy (stochastic exploration) and in some variants uses Thompson/VI baselines; not explicitly optimizing value of information.",
            "adaptation_strategy_description": "Explores via ε-greedy random actions (tabular baseline used ε = 0.5 during q-learning); does not explicitly use belief-derived immediate information value to decide exploratory actions.",
            "environment_name": "T-maze (tabular) and Treasure map (deep baseline)",
            "environment_characteristics": "Same as experimental tasks: partially observable / unknown environment parameters; sparse rewards for information-gathering actions.",
            "environment_complexity": "Same as above (T-maze: S states, T=20; treasure map: 3×3–7×7 grids, T=25).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "As baseline, achieved lower cumulative rewards: in treasure map tests: 3×3: 7.30 ± 0.31; 5×5: 10.46 ± 0.37; 7×7: 10.42 ± 0.26. In T-maze it failed to learn information gathering, producing zero total reward.",
            "sample_efficiency": "Failed to learn targeted exploration within the training budgets used (2000 epochs for deep; epochs unspecified for tabular), indicating poor sample efficiency for learning information-seeking behaviour with sparse rewards.",
            "exploration_exploitation_tradeoff": "Uses ε-greedy: fixed probability of random exploration versus greedy exploitation; does not adapt exploration based on expected information gain.",
            "comparison_methods": "Compared directly to PCR-TD, VI-TD, VI-Thompson, VI-Greedy in treasure map experiments and to PCR-TD in tabular T-maze.",
            "key_results": "Baseline TD methods fail to reliably learn targeted information-gathering behaviour in these sparse-reward belief-augmented tasks; replacing sparse rewards with predictively-cashed rewards (PCR) remedies this.",
            "limitations_or_failures": "Fails on tasks where information-gathering actions have no immediate reward and only indirectly enable future exploitation (the chicken-and-egg problem).",
            "uuid": "e1133.3",
            "source_info": {
                "paper_title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "VI-TD / VI-Thompson / VI-Greedy",
            "name_full": "Value-Iteration augmented TD and greedy/Thompson variants (comparative baselines)",
            "brief_description": "Baselines that combine value-iteration computed on expected reward probabilities (from belief) with learned corrections and different decision rules: VI-TD adds a learned correction to the VI value, VI-Thompson uses posterior sampling on VI values, and VI-Greedy acts greedily on VI values.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "VI-TD / VI-Thompson / VI-Greedy",
            "agent_description": "VI-TD: belief-augmented value = value-iteration solution on expected rewards + a learned correction network (FC); VI-Thompson: perform Thompson sampling on VI-derived environment values; VI-Greedy: greedy policy on VI values. Architectures: VI-TD uses a fully-connected network with large hidden layers; VI variants leverage model-based VI where possible.",
            "adaptive_design_method": "Model-based exploitation of expected-value via value-iteration; VI-Thompson adds posterior sampling (Thompson) over VI-derived environments as an exploration method.",
            "adaptation_strategy_description": "VI-based methods compute exploitation value from expected reward probabilities (belief), and adapt behavior by either acting greedily on that VI value, sampling environment models from the belief (Thompson on VI) or learning corrections to VI via a learned function approximator (VI-TD). They do not use PCR immediate information-crediting mechanism.",
            "environment_name": "Treasure map spatial multi-armed bandit (3×3, 5×5, 7×7)",
            "environment_characteristics": "Partially observable, stochastic Bernoulli rewards, discrete grid, analytic Beta beliefs over cell probabilities.",
            "environment_complexity": "Same as treasure map: grids 3–7 size, T=25, action space up to 9 moves, training 2000 epochs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "VI-Thompson: 3×3: 10.41 ± 0.16; 5×5: 11.72 ± 0.12; 7×7: 10.41 ± 0.15. VI-Greedy and VI-TD had lower performance: VI-Greedy 3×3: 9.39 ± 0.16; 5×5: 9.91 ± 0.11; 7×7: 9.64 ± 0.10. VI-TD: 3×3: 8.28 ± 0.21; 5×5: 11.02 ± 0.75; 7×7: 9.55 ± 0.82.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Operates with model-based VI that can be computed quickly when expected parameters are known; however, VI-based methods here did not match PCR-TD in producing reliable map-seeking behaviours within the same training budget.",
            "exploration_exploitation_tradeoff": "VI-Thompson explicitly uses posterior sampling to encourage exploration; VI-Greedy uses no explicit exploration other than any ε-greedy training schedule; VI-TD tries to learn residual exploration/exploitation complexity via function approximation.",
            "comparison_methods": "Compared against PCR-TD and standard TD in treasure map experiments.",
            "key_results": "VI-Thompson sometimes attains performance near PCR-TD for small grid (3×3) but fails on larger grids (5×5,7×7) to consistently learn map-seeking behaviour; VI-Greedy and VI-TD underperform PCR-TD across grid sizes. PCR-TD outperforms these VI-based baselines overall.",
            "limitations_or_failures": "VI methods can fail because the representation of exploration value is not directly rewarded; VI-TD relies on learned corrections which can be insufficient to shape exploration for tasks with delayed information payoff. VI-Thompson shows partial success but is inconsistent across sizes.",
            "uuid": "e1133.4",
            "source_info": {
                "paper_title": "Knowledge is reward: Learning optimal exploration by predictive reward cashing",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Varibad: A very good method for bayes-adaptive deep RL via meta-learning",
            "rating": 2,
            "sanitized_title": "varibad_a_very_good_method_for_bayesadaptive_deep_rl_via_metalearning"
        },
        {
            "paper_title": "Exploration in approximate hyper-state space for meta reinforcement learning",
            "rating": 2,
            "sanitized_title": "exploration_in_approximate_hyperstate_space_for_meta_reinforcement_learning"
        },
        {
            "paper_title": "Decoupling exploration and exploitation for meta-reinforcement learning without sacrifices",
            "rating": 2,
            "sanitized_title": "decoupling_exploration_and_exploitation_for_metareinforcement_learning_without_sacrifices"
        },
        {
            "paper_title": "Efficient Bayes-adaptive reinforcement learning using sample-based search",
            "rating": 2,
            "sanitized_title": "efficient_bayesadaptive_reinforcement_learning_using_samplebased_search"
        },
        {
            "paper_title": "Bootstrapped Thompson sampling and deep exploration",
            "rating": 1,
            "sanitized_title": "bootstrapped_thompson_sampling_and_deep_exploration"
        },
        {
            "paper_title": "Learning is planning: Near Bayes-optimal reinforcement learning via Monte-Carlo tree search",
            "rating": 1,
            "sanitized_title": "learning_is_planning_near_bayesoptimal_reinforcement_learning_via_montecarlo_tree_search"
        }
    ],
    "cost": 0.0163425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>KNOWLEDGE IS REWARD: LEARNING OPTIMAL EX- PLORATION BY PREDICTIVE REWARD CASHING</p>
<p>Luca Ambrogioni l.ambrogioni@donders.ru.nl 
Donders Institute Radboud University
NijmegenNetherlands</p>
<p>KNOWLEDGE IS REWARD: LEARNING OPTIMAL EX- PLORATION BY PREDICTIVE REWARD CASHING</p>
<p>There is a strong link between the general concept of intelligence and the ability to collect and use information. The theory of Bayes-adaptive exploration offers an attractive optimality framework for training machines to perform complex information gathering tasks. However, the computational complexity of the resulting optimal control problem has limited the diffusion of the theory to mainstream deep AI research. In this paper we exploit the inherent mathematical structure of Bayes-adaptive problems in order to dramatically simplify the problem by making the reward structure denser while simultaneously decoupling the learning of exploitation and exploration policies. The key to this simplification comes from the novel concept of cross-value (i.e. the value of being in an environment while acting optimally according to another), which we use to quantify the value of currently available information. This results in a new denser reward structure that "cashes in" all future rewards that can be predicted from the current information state. In a set of experiments we show that the approach makes it possible to learn challenging information gathering tasks without the use of shaping and heuristic bonuses in situations where the standard RL algorithms fail.</p>
<p>INTRODUCTION</p>
<p>We live in a world of information. In our daily lives, almost every novel task inevitably starts with a series of targeted information retrieval actions, whether performing an internet search or just asking a question to a friend. It is therefore imperative for the deployment of artificial intelligence in the real world to develop efficient algorithms capable of learning how to find and use information in complex environments. In the context of Bayesian reinforcement learning (RL), information and knowledge are formalized as the concept of belief state (also known as information state) (Ghavamzadeh et al., 2015). One of the key results of Bayesian RL is that optimal exploration (also known as Bayes adaptive exploration), and consequently optimal information retrieval, can be achieved by augmenting the state space with the belief state and solving a standard control problem on this augmented space (Ghavamzadeh et al., 2015). In spite of its elegance, the very high dimensionality of belief-augmented problems, together with the intrinsic intractability of Bayesian inference, limits the range of applicability of this approach. Perhaps for this reason, most of the modern deep reinforcement learning literature adopts heuristic exploration methods such as -greedy, Thompson sampling (Osband &amp; Van Roy, 2015;Osband et al., 2016;Azizzadenesheli et al., 2018), noisy networks (Fortunato et al., 2018), random network exploration bonuses (Burda et al., 2019) and curiosity-based approaches (Still &amp; Precup, 2012;Frank et al., 2014;Zhelo et al., 2018). Recent developments in variational inference and deep reinforcement learning re-ignited the interest in approximate Bayes-adaptive exploration as a form of meta-reinforcement learning (Duan et al., 2016;Zintgraf et al., 2020;Dorfman et al., 2020). Nevertheless, learning targeted exploratory actions remains a very challenging task even with modern deep RL techniques, given the high dimensionality of the belief space and the fact that information gathering actions are only very indirectly related to the collection of reward. Fortunately the situation is less dire than it could superficially appear as belief-augmented RL problems inherit a considerable amount of structure from the underlying full information control problems and the nature of iterated Bayesian inference. In this paper we exploit this mathematical structure in order to simultaneously address two related sub-problems that make this setting hard: I) The sparsity of the reward structure (Zintgraf et al., 2021) and II) the so called "chicken-and-egg problem" of exploration and exploitation (Liu et al., 2021). This latter problem arises from the fact that the value of information gathering depends on its use to collect reward but a proper exploitation policy depends on the use of the available information. The other main problem limiting the use of Bayes-adaptive learning is the inherent computational complexity of non-conjugate iterated Bayesian inference. This paper does not deal with this problem and we recommend the reader to the extensive literature on approximate Bayesian inference Blei et al., 2017;Betancourt, 2017;Särkkä, 2013). Therefore, we limit our analysis to conjugate Bayesian models where inference can be performed using a simple update rule. However, all techniques introduced here can be straightforwardly applied to approximate inference settings, including modern approaches using variational autoencoders (VAEs) such as (Zintgraf et al., 2020;Dorfman et al., 2020), possibly in combination with training bonuses methods such as in (Zintgraf et al., 2021).</p>
<p>RELATED WORK</p>
<p>The theoretical framework for optimal control on a belief-augmented state space was first developed in the control theory literature under the name of dual control theory (Feldbaum, 1960a;1961;Filatov &amp; Unbehauen, 2000). The high computational complexity of the approach, stemming from the very high dimensionality of the state space, limited the diffusion of the theory. However, the approach was re-introduced in the context of model-based Bayesian reinforcement learning (Rieder, 1975;Ross et al., 2007) where the domain of applicability was greatly increased by the introduction of Monte Carlo tree search methods (Guez et al., 2012;Asmuth &amp; Littman, 2012;Katt et al., 2017). The use of belief-augmentation in model-free RL methods is very limited. To the best of our knowledge, the first use of temporal difference (TD) learning to solve dual control problems is in Santamar &amp; Ram (1997). Until recently, the deep reinforcement learning literature ignored the concept of Bayes-adaptive exploration, belief-augmentation and dual control. However, important progresses have been made in the related field of model-free meta-RL (Wang et al., 2016;Duan et al., 2016;Gupta et al., 2018). For example, RL 2 involves the training of a recurrent meta-network in order to learn optimal exploration in novel environments (Duan et al., 2016). This can be seen as a model-free form of Bayes-optimal learning since the information encoded in a Bayesian belief state is fully contained in the sequence of past transitions that are fed into the recurrent network. However, this results in redundant encoding as it ignores the exchangability of the observations, which in turn can make an already difficult learning problem into something substantially harder. The recently proposed variBAD method is the first work using modern deep learning techniques in an explicitly belief-augmented setting Zintgraf et al. (2020). This work uses VAEs for performing the approximate inference and hence obtaining the belief-state and then exploits a policy gradient method in order to obtain an approximate Bayes-adaptive policy. Unfortunately, the extremely sparse reward structure of most useful optimal exploration tasks makes methods such as variBAD very challenging to train. The HyperX method addresses this problem with a shaping approach by adding a series of reward bonuses to promote (meta-)exploration of the belief-augmented state space during training (Zintgraf et al., 2021). While our paper deals with the same underlying problem, our solution involves the reformulation of the reward structure into an equivalent one without the use of any heuristic shaping bonus. In this sense, our approach and HyperX are orthogonal and potentially complementary and can be used together to solve more challenging targeted exploration tasks. Similarly to our work, the recently introduced DREAM method decouples the learning of the exploitation and exploration modules so as to ameliorate the "chicken-and-egg" problem that arises from their interdependence (Liu et al., 2021). However, the approach used in DREAM is radically different from ours, as it exploits the environment ID and uses an information bottleneck loss to extract task relevant information. Our predictively cashed reward bears some superficial resemblance to curiosity-based approaches such as (Schmidhuber, 1991;Oudeyer et al., 2007;Frank et al., 2014;Stadie et al., 2015;Bellemare et al., 2016;Tang et al., 2017;Ostrovski et al., 2017;Zhao &amp; Tresp, 2019) since it directly rewards the discovery of new information. However, while curiosity rewards information in itself, the predictively cashed reward only values the difference in predicted reward that the new information allows to collect.</p>
<p>PRELIMINARIES</p>
<p>Markov decision processes (MDPs) are a mathematical formalization of sequential decision making in stochastic environments. A MDP is defined by: 1) a family of stochastic processes specified by a set of transition probabilities p(x t+1 | x t , a t ) between states x t that are conditional on a sequence of action variables a t and 2) a family of reward probabilities p(r t | x t , a t ). In a RL problem, transition and reward probabilities are not known in advance. Without loss of generality, we can formalize this uncertainty by making the transition and reward probabilities dependent on a set of "environment" parameters e:
p (x t+1 | x t , a t ) = T (x t+1 | x t , a t , e) ,(1)p (r t | x t , a t ) = R (r t | x t , a t , e) .
(2)</p>
<p>This model describes a family of Markov decision processes parameterized by the environment variable e, which is not directly accessible to the agent and can only be inferred from the observed transitions/rewards. A policy π(a t | { x τ , a τ , r τ } t−1 τ =0 ) is a distribution over actions conditional on a previous sequence of state transitions. Every policy π defines a π-controlled process where the actions are sampled according to the policy and state transitions and rewards are sampled accordingly. We denote the expectation under a π-controlled process in an environment e as E π|e [·]. The discounted expected value (often simply referred to as just the value) of a policy π in an environment e is defined as:
v π (x t ; e) = E π|e ∞ τ =t γ τ −t r t ,(3)
where γ ∈ (0, 1) is a discounting factor and the sequence of states and rewards is sampled from the π-controlled stochastic process. The optimal value v e (x t ; e) is defined as the global optimum of Eq. 3 with respect to the policy (the reason for this unusual notation will be clear later on). Now assume that environment e is sampled from a prior distribution p(e; b 0 ), parameterized by the initial "belief state" b 0 . This initial belief state summarizes all the information available to the agent at time zero. After each transition x t , a t , r t → x t+1 the belief state is updated using Bayes rule:
p(e; b t+1 ) = p(e | x t , x t+1 , a t , r t ) ∝ R (r t | x t , a t , e) T (x t+1 | x t , a t , e) p(e; b t ) .(4)
Here we assumed that the posterior distribution can be parameterized by the belief variables b t+1 given any possible transition. This is only approximately possible in non-conjugate cases, unless we allow for the use of infinite dimensional belief states (in that case the belief state can be chosen to be the posterior density itself). The appropriate objective function for learning optimal exploration is then simply the average of the environment specific values under the belief state:
v π (x t , b t ) = E e∼bt [v π t (x t ; e)] = E e∼bt E π|e ∞ τ =t γ τ −t r τ .(5)
A Bayes-adaptive policy is a global maximizer of Eq. 5. We denote the optimal belief-augmented value as v * (x t , b t ). In order to maximize the expected value, the agent needs to perform exploratory actions to update its belief state and then use the information to collect reward. Roughly speaking, these dynamics can be separated into an initial exploration phase and a subsequent exploitation phase. The optimal policy is non-Markov with respect to the state variable since the information at time t concerning the environment e depends on past transitions. However, the policy is Markov if state x t is augmented by the belief state b t . The computational challenge of belief-augmented reinforcement learning led the community towards more tractable alternatives. A simple option is Thompson sampling, where the belief-augmented policy is approximated by the optimal policy of an environment e sampled from the belief state: π(x t , b t ) ≈ π(x t ; e ), with e ∼ b t . However, a posterior sampling agent cannot learn how to perform actions that are not optimal in any known environment in the full information regime. In other words, the agent cannot learn to perform actions such as asking questions, consulting a map or performing an internet search as these actions are solely aimed at acquiring new information. Belief-augmentation allows us to solve the optimal exploration problem, also known as the exploration/exploitation dilemma, using standard reinforcement learning techniques. For example, given a belief-augmented transition x t , b t , a t , r t → x t+1 , b t+1 under a belief-augmented policy π(x t , b t ), we can learn the expected value of the policy using the standard TD learning (tabular) update rule
v π (x t , b t ) v π (x t , b t ) + η r t + γv π t+1 (x t+1 , b t+1 ) − v π (x t , b t ) ,(6)
where η is a learning rate. The key difference when compared with standard TD learning is that the value is now a function of the belief state. This allows to assign high values to belief states where the agent has precise information concerning how to collect reward. For example, the value of being on a Caribbean island and knowing that it hides a buried pirate treasure is higher than the value of being in the same island without that knowledge. This is true both because the belief correlates with the actual state of the world and because it affects the optimal policy (e.g. digging vs sunbathing).</p>
<p>CROSS-VALUES AND THE VALUE OF CURRENT AND FUTURE INFORMATION</p>
<p>We are now in the position to introduce our main contribution. Here we will show that a simplified version of the posterior sampling policy can be used to quantify the value of the information currently held by the agent. The expected value of a posterior sampling policy is not easy to quantify as each action leads to new belief updates. However, we can evaluate the value of a simplified sampling policy where the environment is sampled once from the belief and then the policy acts optimally until the end of the episode according to the sampled environment. In this case, the expected value is
E e ∼bt v e (x t ; e) .(7)
We refer to the quantity v e (x t ; e) as the cross-value, which is defined as the expected reward collected in environment e under the optimal policy of environment e . The cross-value plays a crucial role in the present work as it will allow us to quantify the penalty that the agent has to pay when acting under "wrong beliefs". Our starting point is Eq. 5, which expresses the optimal belief-augmented value as an average of environment-specific values:
v * (x t , b t ) = E e∼bt E π|bt;e ∞ τ =t γ τ −t r τ = E e∼bt [v * (x t , b t | e)] ,(8)
where
v * (x t , b t | e)
is the value of the optimal belief-augmented policy when the agent is in environment e. The value v * (x t , b t , a t | e) still depends on the belief state through the beliefaugmented policy. However, an interesting belief independent approximation can be obtained by replacing the belief-augmented value v * (x t , b t , a t | e) with the posterior sampling value given in Eq. 7:
v c (x t , b t ) = E e∼bt E e ∼bt v e (x t ; e) = E e,e ∼ iid bt v e (x t ; e) .(9)
We refer to v c as the value of current information as it can it interpreted as the value of a policy that stops acquiring information and starts acting on a fixed environment sampled according to its current belief. This quantity has some important and intuitive properties. If we denote a deterministic belief state as δ e (dirac measure), we have that v c (x t , δ e ) = v e (x t ; e). In words, the value of future information is equal to the optimal value in the full information regime. More generally, the value of current information tends to increase as the entropy of the belief state decreases since the cross-values tend to converge to the optimal values. From the optimality of the value in Eq. 8 it follows that the value of current information is a lower bound on the optimal belief-augmented value:
v c (x t , b t ) ≤ v * (x t , b t ) .(10)
Therefore, we can express the optimal belief-augmented value as a sum of the value of current information and a positive-valued term:
v * (x t , b t ) = v c (x t , b t ) + v f (x t , b t ) .(11)
We refer to v f (x t , b t , a t ) as the value of future information.</p>
<p>Since the value of current information can be computed in a non-augmented state space, the belief-augmented problem reduces to learning the value of future information.</p>
<p>Predictive reward cashing. The central result of this paper is that, once we have an expression for the cross-values, the full belief-augmented problem can be reduced to a simpler problem where information acquisition is directly rewarded. We can achieve this by plugging the decomposition of the value in Eq. 11 into the belief-augmented update rule in Eq. 6. This results in a similar update rule for the value of future information:
v f (x t , b t ) v f (x t , b t ) + η λ t + γv f t+1 (x t+1 , b t+1 ) − v f (x t , b t ) ,(12)
where λ t is the predictively cashed reward
λ t = r t + γv c (x t+1 , b t+1 ) − v c (x t , b t ) .(13)
This can been seen by plugging Eq. 11 into the TD target:
r t + γv π t+1 (x t+1 , b t+1 ) − v π (x t , b t ) = r t + γ v c (x t+1 , b t+1 ) + v f (x t+1 , b t+1 ) − v c (x t , b t ) + v f (x t , b t ) = λ t + γv f t+1 (x t+1 , b t+1 ) − v f (x t , b t ) .
This quantity can be interpreted as the difference between the future expected reward collected in the past and present belief states if the agent were to stop updating its belief and act optimally according to an environment sampled from the belief. This can be seen by evaluating its expected value:
E rt,xt+1 [λ t ] = E rt [r t ] + γv c (x t+1 , b t+1 ) − v c (x t , b t ) = E e,e E rt|e [r t ] + γE xt+1|π(e) v e (x t+1 ; e) − v c (x t , b t ) ≈ E e,e E rt|e [r t ] + γE xt+1|π e (e) v e (x t+1 ; e) − v c (x t , b t ) = v c (x t+1 , b t ) − v c (x t , b t ) ,(14)
where the approximation comes from the fact that we took the expectation with respect to the optimal policy under the environment e instead of the Bayes-adaptive policy. Therefore, the predictively cashed reward allows to immediately "cash in" future reward coming from an increase of available information. In practice, this transformation converts the very sparse reward structure of a beliefaugmented problem into a much denser reward structure where reward is delivered directly as soon as relevant information is acquired.</p>
<p>A key feature of the value of future information is that it can be bounded by full information quantities (i.e. quantities that can be computed without solving the belief-augmented problem). This is important as it means that we can bound function approximations of the value of future information without need for training, thereby ensuring the proper convergence of the belief-augmented values to the environment-specific optimal values as soon as the agent acquires information. The starting point is the inequality:
v * (x t , b t ) = v c (x t , b t ) + v f (x t , b t ) ≤ E e∼bt [v e (x t ; e)] .(15)
This result is obvious, it simply says that on average an optimal agent with full information collects at least as much reward as a belief-augmented agent with potentially incomplete knowledge of its environment. Therefore,
0 ≤ v f (x t , b t ) ≤ B(x t , b t ) ,
where the upper bound is given by
B(x t , b t ) = E e∼bt v e (x t ; e) − E e ∼bt v e (x t ; e) .(16)
This is the difference between the average expected future reward that can be acquired under the current information state and the average optimal expected reward.</p>
<p>PRACTICAL ALGORITHMS</p>
<p>The predictively cashed reward (13) can rarely be computed analytically even when the cross-value function is available. However, it is straightforward to obtain an unbiased stochastic estimator by replacing the exact expectation with an average over a finite number of samples: λ
(M,N ) t = r t + 1 M N M m=1 N n=1 v en (x t ; e m )
, with all the e k sampled independently from the belief state. We can therefore use this stochastic estimator as reward during training in place of the true λ t without affecting the asymptotic convergence. This is equivalent to switching from expected rewards to sampled rewards in conventional RL algorithms. In practice, we suggest to use M = 1. This generally provides a strong reward signal even when N is small since the cross-value usually decreases sharply as the difference between the two environments increases, delivering strong reward signals when relevant information is acquired. All standard value-based RL algorithms can be used to learn the value of future information from this modified reward structure. The main difference between the predictively cashed training and standard training is that the policy should maximize the total
value v * (x t , b t ) = v c (x t , b t ) + v f (x t , b t ) instead of just the learned value v f (x t , b t ). In practice, v c (x t , b t )
has to be estimated from the cross-values using a finite number of samples. This leads to a stochasticity in the policy similar to standard Thompson sampling, which can potentially facilitate (meta-)exploration during training. Convergence of the tabular algorithm follows from the standard convergence results of RL. Tabular approaches in finite deterministic inference settings: For the sake of simplicity, so far we presented the approach by its tabular update rule. Unfortunately, tabular algorithms are rarely feasible in a belief-augmented setting as there usually are infinitely many belief states even for very simple inference problems. However, there is an interesting class of problems where the full belief-augmented state space has a finite number of states. This happens when there is a finite number of possible environments and inference is deterministic, meaning that the belief variable can only transition from its prior value to a full information state. For a set of S possible environments, the belief state can be parameterized by an array of posterior probabilities b t = (ρ (1) t , ..., ρ (S) t ). When inference is deterministic, the belief state can either stay on its prior value or "collapse" on a one-hot-encoded array representing a full information state. This implies that the belief space has S + 1 different states and, assuming that the environments have H many states, the belief-augmented value can be represented by a table of H numbers. This is true because we know that the value of future information is zero for full information states. Thus, the only non-zero values are in the starting prior belief state. While deterministic inference can sound very restrictive, it can effectively represent many different experimental behavioral and cognitive tasks in animals and humans where cues offer an unambiguous disambiguation of the latent state (Friston et al., 2016). Function approximations with approximate value bound: In most realistic scenarios the beliefaugmented state space has an exponentially large or infinite number of states and tabular algorithms become unfeasible. In these situations we can use standard function approximation techniques to estimate v f (x t , b t ). We can also obtain a functional approximation of the value of future information that automatically satisfies the bound:
v f (x t , b t ) = B(x t , b t )w(x t , b t ) ,(17)
where w(x t , b t ) ∈ (0, 1) is the output of a parameterized function approximation such as a deep neural architecture. In practice, the bound B(x t , b t ) can be estimated using a finite number of samples. This approximation ensures that the policy switches from exploratory to the optimal exploitative policy automatically as soon as enough relevant information has been collected. This is because the belief-augmented value is the sum of the value of current information, which converges to the optimal full-information value as the belief converges to a deterministic measure, and a term v f ≤ B that is guaranteed to vanish under this limit. Additionally and perhaps more importantly, the use of the bound in a functional approximation means that the agent does not need to learn how the predictively cashed reward coming from the same state is down-scaled when information is acquired. This is crucially important as the apparent inconsistency of the reward schedule makes the learning problem particularly hard.</p>
<p>LEARNING THE CROSS-VALUES</p>
<p>So far we assumed the cross-values to already be available before training. Here we will present some methods to either pre-compute the cross-values or to jointly train them together with the value of future information. When the expected reward and transition distributions are known, the optimal expected values can be obtained by solving the Bellman equation. It is straightforward to extend the approach in order to compute the cross-values, as described in Appendix A. Importantly, the resulting recursive system of equations can be efficiently parallelized on GPUs in a manner similar to convolutional neural networks. This allows to efficiently compute a batch of cross-values in order to estimate the expectations in Eq. 13 and Eq. 16. In the stationary infinite horizon case, the time dependent system of equations is replaced by steady state value iterations as usual. When the reward and transition distributions are not available the cross-values can be learned using standard on-policy RL methods from sampled transitions. If the ground truth environment e of each transition is available, the cross-value with respect to an arbitrary environment e can be updated using the
tabular rule v e (x t ; e) v e (x t ; e) + η r t + γv e t+1 (x t+1 ; e) − v e (x t ; e) ,(18)
where the action a t is chosen to maximize the current estimate of the optimal value v e (x t ; e ) through on-policy techniques such as policy gradients. In practice, the environment e can be sampled before each action from the belief state b t , resulting in a Thompson sampling policy.</p>
<p>Belief horizon sampling. As we have shown, it is straightforward to estimate the cross-value function when the ground truth environment e is available at training time. However, in the most interesting settings the true environment can only be inferred through the agent's observations. In this situation, we propose the use of a method we call belief horizon sampling where the "ground truth" environment is sampled from the terminal belief state of the episode b T , with T the last time point of the episode. This is equivalent to training with the actual ground truth for T tending to infinity, as long as the π-controlled process satisfies the convergence conditions of the posterior to the maximum likelihood estimate. However, this approach also works well when the posterior is far from convergence since, loosely speaking, the sequence { x τ , a τ , r τ } T τ =0 has high probability under all the conditional distributions p({ x τ , a τ , r τ } T τ =0 | e ) with e in the high probability region of the posterior p(e | { x τ , a τ , r τ } T τ =0 ). In appendix B we show that this converges to the correct cross-values in the special case of deterministic inference.</p>
<p>MODULAR BAYES-ADAPTIVE DEEP REINFORCEMENT LEARNING</p>
<p>From the point of view of deep RL, the simultaneous learning of cross-values through belief horizon sampling and of the value of future information using predictively cashed rewards results in a modular architecture with components trained with separate loss functions from the same experienced transitions, as visualized in Fig. 1. On one hand, the cross-values are estimated by a deep net that takes e, e and x and outputs the cross-value (red module in the figure). On the other hand, another deep RL architecture is used to approximate the value of future information from predictively cashed rewards estimated using the cross-values (violet module). When inference cannot be performed in closed-form, an additional inference network needs to be trained using techniques such as variational inference Zintgraf et al. (2020) (blue module). Finally, the policy network is trained to maximize the estimated belief-augmented value (green module).</p>
<p>EXPERIMENTS</p>
<p>In this section we investigate the performance of the approach in a series of problems with closedform Bayesian updates. In particular, we focus on problems where there is either an action or a state whose purpose is to reveal the location of reward.</p>
<p>Disambiguation from a finite set of environments. We start with a single experimental set up inspired by animal neuroscience research (Friston et al., 2016;Wauthier et al., 2021). The environment is a T-shaped maze with the bifurcation at the upper end (see Fig. 2 A). Each arm either contains a reward (+1) or a punishment (-7) while the other arm has the opposite. The bottom part of the maze contains a disambiguation cue that allows to distinguish whether the reward is in the right or in the left arm. Since the are only two possible environments (which we assume to have equal prior) and the cue and rewards are deterministic, the belief state is a single number in {0, 0.5, 1}. This allows us to organize the belief-augmented values into a 3 × S table, where S is the number in the maze with left reward. The terminal black dot signifies that the agent stays at the location until the end of the episode. The PCR-TD policy is optimal. The TD policy learns to approach the reward region and then stops as it cannot disambiguate reward from punishment, resulting in zero total collected reward.</p>
<p>of states (length of the maze). In this case, the dynamics of the belief state are very simple. The belief transitions from its starting 0.5 to either 0 or 1 once either the cue or the reward/punishment locations are visited. The agent always starts in the middle of the maze, equally distant from the cue end and the reward/punishment ends. The agent can move up or down in the middle of the maze and down, left or right in the upper end. Each episode lasts T = 20 time steps. The agent collects reward/punishment at each time point as long as it is in the proper location in the maze. The optimal course of action is to reach the cue (information gathering) and then move to the other end to collect reward. In this problem there are S × 3 possible belief-augmented states, where S is the number of locations. As baseline, we use the classical (belief-augmented) tabular q-learning algorithm (Eq. 6) with -greedy policy ( = 0.5) and decreasing learning rate η n = 0.01/(1 + 0.001n), where n is the epoch number. Our method is a tabular q-learning algorithm with PCR rewards. The value of future information is stored as a table of S values since only the belief 0.5 has non-zero value of future information. The cross-values are learned together with the value of future information from belief-augmented transitions without accessing the known ground truth environment using our belief horizon sampling approach. The value of future information is updated only after 100 epochs so as to ensure some convergence of the cross q-values. Learning rates and were equal to the vanilla baseline. The results are visualized in Fig. 2. These results are obtained using the greedy policy. The baseline algorithm quickly learns the correct deterministic dynamics but it does not learn the information gathering policy, leading to zero reward collection (see Fig.2 C). On the other hand, q-learning with PCR reward quickly learns the correct value function, leading to optimal reward collection in most repetitions (see Fig.2 B).</p>
<p>The treasure map problem. We now move to a more complex task where the agent needs to find a "treasure map" that reveals the reward distribution in a 2d environment. This is conceptually very similar to the "treasure mountain" problem used in (Zintgraf et al., 2021). We formalize this task as a spatial multi-armed bandit, where the rewards follow Bernoulli distributions and each cell in the grid-world has an independent reward probability. As in the conventional Bernoulli multi-armed bandit problem, the belief state can be updated analytically and is given by the counting parameters of the beta posterior distribution of each cell. When the agent visits a cell, reward is sampled from the appropriate Bernoulli distribution. Furthermore, the agent also observes 5 additional simulated "pulls" that update the belief state but do not give reward (this is an operationalization of the act of looking around and exploring the cell). Importantly, the central cell is a "treasure map" that allows the agent to observe 5 simulated "pulls" for each cell. In our proposed method (denoted as PRC-TD), the value of future information is given by the output of a convolutional network with two hidden layers that takes the spatial array of belief variables as input. Specifically, the network outputs the w(  learning the cross-values with belief horizon sampling we compute them with (cross-)value iterations (see Appendix A). As main baseline, we use a more standard deep belief-augmented TD learning approach where the total belief-augmented value is given by the output of a deep network trained on the original reward structure. Since our method exploits the output of the value iterations, we also compared the results with a more sophisticated baseline (VI-TD) with the belief-augmented value given by the sum of the network output and the output of the value iterations applied to the expected reward probabilities (as inferred by the belief state). We also included VI-Greedy and VI-Thompson baselines, which implement greedy and Thompson sampling policies respectively on the values obtained by value iteration. All networks were trained for 2000 epochs. In each epoch, a batch of 10 environments (reward probabilities) were generated from the beta prior (a = 0.1, b = 1) and controlled sequences were simulated from the -greedy policies. The TD loss was summed over all transitions and then backpropagated. Training performance was stored for each epoch. We selected the epoch with highest training performance for the test run. Test performance was quantified as (non-discounted) average total reward collected during 20 trials on newly generated environments (averaged over a batch of 10 environments per trial). Additional details about task, training and architectures are given in Appendix C. The total reward as function of the epoch can be seen in Fig. 3 A for the 7 × 7 environments. All results are given in Table 8 for the three environment sizes. Only our PCR-TD method was able to learn to visit the "treasure map" cell at the center in order to collect information and then switch to the optimal exploitation policy. A typical learned trajectory of this kind can be seen in Fig. 3 B.</p>
<p>CONCLUSION &amp; FUTURE WORK</p>
<p>In the paper we showed that predictive reward cashing, with its associated policy that decouples the exploitation and exploration component, dramatically facilitates learning the Bayes-adaptive policy. While we developed the methods with a modular deep learning architecture in mind (see Fig. 1), in order to isolate the key contribution of the paper we limited our attention to settings where inference can be performed in closed-form. However, all approaches introduced here can in principle work together with approximate inference networks such as those used in (Zintgraf et al., 2020) and (Zintgraf et al., 2021).</p>
<p>A COMPUTING CROSS-VALUES BY BELLMAN EQUATION AND VALUE</p>
<p>ITERATIONS</p>
<p>In a finite horizon problem with known transition and reward distributions, the cross-values can be computed by backward iterations that are a straightforward extension of the bellman equation:
     v e1 t (x t ; e 1 ) = R (r t | x t , a t , e 1 ) + γ xt+1 T (x t+1 | x t , a t , e 1 ) v e1 t+1 (x t+1 ; e 1 ) v e1 t (x t ; e 2 ) = R (r t | x t , a t , e 2 ) + γ xt+1 T (x t+1 | x t , a t , e 2 ) v e1 t+1 (x t+1 ; e 2 ) a t = argmax α R (r t | x t , a t , e 1 ) + γ xt+1 T (x t+1 | x t , a t , e 1 ) v e1 t+1 (x t+1 ; e 1 ) .(19)
This algorithm allows to efficiently compute the cross-values for an arbitrary pair of environments e 1 and e 2 in a highly parallelizable manner. As usual, the system of equations can be turned into value iterations in the infinite horizon setting simply by dropping the explicit time dependency from the value function.</p>
<p>B CONVERGENCE OF BELIEF HORIZON SAMPLING</p>
<p>Here we will prove that tabular TD learning training with belief horizon sampling convergences to the true cross-value table under the usual conditions of TD learning. Like the rest of this paper, this section will have a rather informal tone. However, all reasoning can be easily rigorously formalized.</p>
<p>We define the deterministic inference setting as a belief-augmented Markov decision process where the belief state can only transition from its initial prior state b 0 into one of the S possible deterministic states, here denoted as δ j . These terminal states correspond to posterior distributions with zero entropy. In other words, each of them corresponds to a unique (non-augmented) Markov decision process. After a sequence of observed transitions { x τ , a τ , r τ } t−1 τ =0 , the belief state b τ can either be equal to b 0 or to one of the δ j s. In the latter case, at least one of the observed transitions has zero probability under all environments but one. On the other hand, in the former case all transitions have equal probability under all environments (this follows from the fact that the prior belief state has not been updated).</p>
<p>In belief horizon sampling, the ground truth environment e 1 is sampled from the terminal belief state b T . This environment is then used to select the proper entry of the cross-values table and to perform a TD update. The sampled environment e 1 can either be equal or different from the (inaccessible) true environment e * that generated the transitions. If it is equal then standard TD convergence results apply directly. Since all the δ j s represent deterministic posterior distributions, e 1 can be different from e * only when b T is equal to b 0 . In this case, the sequence { x τ , a τ , r τ } t−1 τ =0 could be generated by any of the possible environments with equal probability and, consequently, the transitions can be used to update any of the cross-values tables as if they were sampled from their own environments without affecting convergence.</p>
<p>C DETAILS OF THE TREASURE MAP EXPERIMENT</p>
<p>Task details. In each episode, the agent starts in a random location in the grid-world and stays in the environment for T = 25 time steps. At each time step, the agent can move to each of the 8 (except at the borders/corners) neighboring cells or stay in place. Reward is collected at each time step and is discounted with γ = 0.96.</p>
<p>Architecture details. The network output is a 2d array of value of future information, one for each spatial location. The network input was a tensor with two spatial dimensions and two channels, one for each parameter of the beta distributions. The network had two convolutional layers, the first with kernel sizes (3, 3) and 20 output channels and the second with kernel sizes (1, 1) and 1 output channels. Relu activation functions were applied after the first convolutional layer. The output tensor was scaled by 0.01 and then summed to a learnable constant table of values, one for each spatial location.</p>
<p>Policy details. Predictively cashed rewardλ t was computed from the cross-values using the approximation in Eq. 14 and N = 80 samples. We used an -greedy training policy with respect to the value function. This does not require the training of a policy network since the transition model is known and deterministic. The TD loss for each transition was L(w) = (λ t + γv(x t+1 , b t+1 ; w) − v(x t , b t ; w)) 2 .</p>
<p>Baseline architecture details. The baseline TD algorithm has to learn a substantially more complex value function which, loosely speaking, includes both the exploitation and the exploration part of the value. For these reasons, we thought it more effective to use a fully connected architecture as it is in theory capable of learning arbitrarily complex dependencies between the input variables. We therefore used a two-layers fully connected architecture and 4H 2 hidden layers, where H is the linear size of the grid-world. To facilitate learning, the output of the network corresponding to the j, k − th grid point was ρ jk + f jk (b t ), where f jk (b t ) denotes the output of the network and ρ jk is the expected reward probability of the cell given the current belief. This significantly improves performance. Since our method uses the solution of the value iterations, we also used an alternative method (VI-TD) with the belief-augmented value given by v(ρ jk ) + f jk (b t ), with v(ρ jk ) being the value obtained from the expected values by value iteration.</p>
<p>Figure 1 :
1Diagram of a modular predictively cashed RL architecture.</p>
<p>Figure 2 :
2Maze disambiguation experiment. A) The two possible environments. The green square represents reward (+1) while the red square represents punishment (-7). The blue/purple circles are disambiguation cues that indicate the location of the reward. The optimal course of action is to visit the cue and then go towards the reward. B) Results of tabular PCR q-learning. The black line denotes the median collected reward (q-loss for bottom panel) over 15 repetitions as a function of epoch. The shaded blue area shows the median absolute deviation from the median. Individual transparent lines are the result of individual repetitions. C) Results of the standard tabular q-learning algorithm. D) Visualization of the (median) trained policy of PCR-TD (left diagram) and TD (right diagram)</p>
<p>Figure 3 :
3x t , b t ) function as in Eq. 17 and the bound B(x t , b t ) is estimated using M = 1 and N = 40. Instead of The "treasure map" experiment. A) Total training reward as function of the epoch of PCR-TD (red), TD (blue) and Vi-TD (green) for the three grid-world environments. The dashed line denotes the average over the runs. B) Example trajectory of trained PCR-TD. No other method learned how to systematically reach the map.PRC-TDTD VI-TD VI-Thompson VI-Greedy 3 × 3 10.78 ± 0.24 7.30 ± 0.31 8.28 ± 0.21 10.41 ± 0.16 9.39 ± 0.16 5 × 5 15.17 ± 0.22 10.46 ± 0.37 11.02 ± 0.75 11.72 ± 0.12 9.91 ± 0.11 7 × 7 15.33 ± 0.16 10.42 ± 0.26 9.55 ± 0.82 10.41 ± 0.15 9.64 ± 0.10</p>
<p>Table 1 :
1Quantitative results of the treasure map experiment.</p>
<p>Learning is planning: Near Bayes-optimal reinforcement learning via Monte-Carlo tree search. J Asmuth, M L Littman, arXiv:1202.3699arXiv preprintJ. Asmuth and M. L. Littman. Learning is planning: Near Bayes-optimal reinforcement learning via Monte-Carlo tree search. arXiv preprint arXiv:1202.3699, 2012.</p>
<p>Efficient exploration through bayesian deep q-networks. K Azizzadenesheli, E Brunskill, A Anandkumar, IEEE Information Theory and Applications Workshop. K. Azizzadenesheli, E. Brunskill, and A. Anandkumar. Efficient exploration through bayesian deep q-networks. IEEE Information Theory and Applications Workshop, 2018.</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. Advances in Neural Information Processing Systems, 2016.</p>
<p>A conceptual introduction to Hamiltonian Monte Carlo. M Betancourt, arXiv:1701.02434arXiv preprintM. Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint arXiv:1701.02434, 2017.</p>
<p>Variational inference: A review for statisticians. D M Blei, A Kucukelbir, J D Mcauliffe, Journal of the American statistical Association. 112518D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859-877, 2017.</p>
<p>Exploration by random network distillation. Y Burda, H Edwards, A Storkey, O Klimov, International Conference on Learning Representations. Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. International Conference on Learning Representations, 2019.</p>
<p>Offline meta learning of exploration. R Dorfman, I Shenfeld, A Tamar, arXiv:2008.02598arXiv preprintR. Dorfman, I. Shenfeld, and A. Tamar. Offline meta learning of exploration. arXiv preprint arXiv:2008.02598, 2020.</p>
<p>Y Duan, J Schulman, X Chen, P L Bartlett, I Sutskever, P Abbeel, arXiv:1611.02779RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprintY. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Dual control theory. I. Avtomatika i Telemekhanika. A A Feldbaum, A. A. Feldbaum. Dual control theory. I. Avtomatika i Telemekhanika, pp. 1240-1249, 1960a.</p>
<p>Dual control theory. II. Avtomatika i Telemekhanika. A A Feldbaum, A. A. Feldbaum. Dual control theory. II. Avtomatika i Telemekhanika, pp. 1453-1464, 1960b.</p>
<p>Dual control theory III. A A Feldbaum, Avtomatika i Telemekhanika. 22A. A. Feldbaum. Dual control theory III. Avtomatika i Telemekhanika, 22:1-12, 1961.</p>
<p>Survey of adaptive dual control methods. N M Filatov, H Unbehauen, IEEE Proceedings-Control Theory and Applications. 1471N. M. Filatov and H. Unbehauen. Survey of adaptive dual control methods. IEEE Proceedings-Control Theory and Applications, 147(1):118-128, 2000.</p>
<p>M Fortunato, Mohammad G Azar, B Piot, J Menick, I Osband, A Graves, V Mnih, R Munos, D Hassabis, O Pietquin, Noisy networks for exploration. International Conference on Learning Representations. M. Fortunato, Mohammad G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, and O. Pietquin. Noisy networks for exploration. International Conference on Learning Representations, 2018.</p>
<p>Curiosity driven reinforcement learning for motion planning on humanoids. M Frank, J Leitner, M Stollenga, A Förster, J Schmidhuber, Frontiers in Neurorobotics. 725M. Frank, J. Leitner, M. Stollenga, A. Förster, and J. Schmidhuber. Curiosity driven reinforcement learning for motion planning on humanoids. Frontiers in Neurorobotics, 7:25, 2014.</p>
<p>Active inference and learning. K Friston, T Fitzgerald, F Rigoli, P Schwartenbeck, G Pezzulo, Neuroscience &amp; Biobehavioral Reviews. 68K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo. Active inference and learning. Neuroscience &amp; Biobehavioral Reviews, 68:862-879, 2016.</p>
<p>Bayesian Reinforcement Learning: A Survey. M Ghavamzadeh, S Mannor, J Pineau, A Tamar, M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar. Bayesian Reinforcement Learning: A Survey. 2015.</p>
<p>Efficient Bayes-adaptive reinforcement learning using sample-based search. A Guez, D Silver, P Dayan, Neural Information Processing Systems. A. Guez, D. Silver, and P. Dayan. Efficient Bayes-adaptive reinforcement learning using sample-based search. Neural Information Processing Systems, 2012.</p>
<p>Scalable and efficient bayes-adaptive reinforcement learning based on Monte-Carlo tree search. A Guez, D Silver, P Dayan, Journal of Artificial Intelligence Research. 48A. Guez, D. Silver, and P. Dayan. Scalable and efficient bayes-adaptive reinforcement learning based on Monte-Carlo tree search. Journal of Artificial Intelligence Research, 48:841-883, 2013.</p>
<p>Meta-reinforcement learning of structured exploration strategies. A Gupta, R Mendonca, Y Liu, P Abbeel, S Levine, Advances in Neural Information Processing Systems. A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine. Meta-reinforcement learning of structured exploration strategies. Advances in Neural Information Processing Systems, 2018.</p>
<p>Learning in POMDPs with Monte Carlo tree search. S Katt, F Oliehoek, C Amato, International Conference on Machine Learning. S. Katt, F. A Oliehoek, and C. Amato. Learning in POMDPs with Monte Carlo tree search. Interna- tional Conference on Machine Learning, 2017.</p>
<p>Decoupling exploration and exploitation for meta-reinforcement learning without sacrifices. E Z Liu, A Raghunathan, P Liang, C Finn, International Conference on Machine Learning. E. Z. Liu, A. Raghunathan, P. Liang, and C. Finn. Decoupling exploration and exploitation for meta-reinforcement learning without sacrifices. International Conference on Machine Learning, 2021.</p>
<p>. I Osband, B Van Roy, arXiv:1507.00300Bootstrapped Thompson sampling and deep exploration. arXiv preprintI. Osband and B. Van Roy. Bootstrapped Thompson sampling and deep exploration. arXiv preprint arXiv:1507.00300, 2015.</p>
<p>Deep exploration via bootstrapped DQN. I Osband, C Blundell, A Pritzel, B Van Roy, Advances in Neural Information Processing Systems. I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped DQN. Advances in Neural Information Processing Systems, 2016.</p>
<p>Count-based exploration with neural density models. G Ostrovski, M G Bellemare, A Oord, R Munos, International Conference on Machine Learning. G. Ostrovski, M. G. Bellemare, A. Oord, and R. Munos. Count-based exploration with neural density models. International Conference on Machine Learning, 2017.</p>
<p>Intrinsic motivation systems for autonomous mental development. P Oudeyer, F Kaplan, V V Hafner, IEEE Transactions on Evolutionary Computation. 112P. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265-286, 2007.</p>
<p>Bayesian dynamic programming. U Rieder, Advances in Applied Probability. 72U. Rieder. Bayesian dynamic programming. Advances in Applied Probability, 7(2):330-348, 1975.</p>
<p>S Ross, B Chaib-Draa, J Pineau, Bayes-Adaptive POMDPs. Neural Information Processing Systems. S. Ross, B. Chaib-draa, and J. Pineau. Bayes-Adaptive POMDPs. Neural Information Processing Systems, 2007.</p>
<p>A new heuristic approach for dual control. J C Santamar, A Ram, Särkkä. Bayesian filtering and smoothing. Cambridge University PressJ. C. Santamar and A. Ram. A new heuristic approach for dual control. AAAI Technical Report, 1997. S. Särkkä. Bayesian filtering and smoothing. Cambridge University Press, 2013.</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. J Schmidhuber, International Conference on Simulation of Adaptive Behavior: From Animals to Animats. J. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In International Conference on Simulation of Adaptive Behavior: From Animals to Animats, 1991.</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. B C Stadie, S Levine, P Abbeel, arXiv:1507.00814arXiv preprintB. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.</p>
<p>An information-theoretic approach to curiosity-driven reinforcement learning. S Still, D Precup, Theory in Biosciences. 1313S. Still and D. Precup. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139-148, 2012.</p>
<h1>exploration: A study of count-based exploration for deep reinforcement learning. H Tang, R Houthooft, D Foote, A Stooke, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in Neural Information Processing Systems. H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. Advances in Neural Information Processing Systems, 2017.</h1>
<p>J X Wang, Z Kurth-Nelson, D Tirumala, H Soyer, Joel Z Leibo, R Munos, C Blundell, D Kumaran, M Botvinick, arXiv:1611.05763Learning to reinforcement learn. arXiv preprintJ. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, Joel Z. Leibo, R. Munos, C. Blundell, D.n Kumaran, and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.</p>
<p>S T Wauthier, P Mazzaglia, O , C De, T Boom, B Verbelen, Dhoedt, arXiv:2104.10995A learning gap between neuroscience and reinforcement learning. arXiv preprintS. T. Wauthier, P. Mazzaglia, O. Ç atal, C. De Boom, T. Verbelen, and B. Dhoedt. A learning gap between neuroscience and reinforcement learning. arXiv preprint arXiv:2104.10995, 2021.</p>
<p>Advances in variational inference. C Zhang, J Bütepage, H Kjellström, S Mandt, IEEE Transactions on Pattern Analysis and Machine Intelligence. 418C. Zhang, J. Bütepage, H. Kjellström, and S. Mandt. Advances in variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008-2026, 2018.</p>
<p>Curiosity-driven experience prioritization via density estimation. R Zhao, V Tresp, arXiv:1902.08039arXiv preprintR. Zhao and V. Tresp. Curiosity-driven experience prioritization via density estimation. arXiv preprint arXiv:1902.08039, 2019.</p>
<p>Curiosity-driven exploration for mapless navigation with deep reinforcement learning. Oleksii Zhelo, Jingwei Zhang, Lei Tai, Ming Liu, Wolfram Burgard, arXiv:1804.00456arXiv preprintOleksii Zhelo, Jingwei Zhang, Lei Tai, Ming Liu, and Wolfram Burgard. Curiosity-driven exploration for mapless navigation with deep reinforcement learning. arXiv preprint arXiv:1804.00456, 2018.</p>
<p>Varibad: A very good method for bayes-adaptive deep RL via meta-learning. L Zintgraf, K Shiarlis, M Igl, S Schulze, Y Gal, K Hofmann, S Whiteson, International Conference on Learning Representations. L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. Varibad: A very good method for bayes-adaptive deep RL via meta-learning. International Conference on Learning Representations, 2020.</p>
<p>Exploration in approximate hyper-state space for meta reinforcement learning. L M Zintgraf, L Feng, C Lu, M Igl, K Hartikainen, K Hofmann, S Whiteson, International Conference on Machine Learning. L. M. Zintgraf, L. Feng, C. Lu, M. Igl, K. Hartikainen, K. Hofmann, and S. Whiteson. Exploration in approximate hyper-state space for meta reinforcement learning. International Conference on Machine Learning, pp. 12991-13001, 2021.</p>            </div>
        </div>

    </div>
</body>
</html>
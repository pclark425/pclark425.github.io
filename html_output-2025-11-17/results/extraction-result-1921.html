<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1921 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1921</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1921</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-277634130</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.06585v1.pdf" target="_blank">Sim-to-Real of Humanoid Locomotion Policies via Joint Torque Space Perturbation Injection</a></p>
                <p><strong>Paper Abstract:</strong> This paper proposes a novel alternative to existing sim-to-real methods for training control policies with simulated experiences. Prior sim-to-real methods for legged robots mostly rely on the domain randomization approach, where a fixed finite set of simulation parameters is randomized during training. Instead, our method adds state-dependent perturbations to the input joint torque used for forward simulation during the training phase. These state-dependent perturbations are designed to simulate a broader range of reality gaps than those captured by randomizing a fixed set of simulation parameters. Experimental results show that our method enables humanoid locomotion policies that achieve greater robustness against complex reality gaps unseen in the training domain.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1921.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1921.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JT-Perturb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Joint Torque Space Perturbation Injection (state-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real training method that injects state-dependent perturbations in joint-torque space during forward simulation using randomly sampled neural networks, aiming to model a broader class of reality-gap torque perturbations than finite-parameter domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-Real of Humanoid Locomotion Policies via Joint Torque Space Perturbation Injection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Humanoid locomotion (velocity tracking / walking forward, turning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>≤15s (maximum episode length reported: 15s)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich (continuous foot-ground contact during walking)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Actuator armature (inertia), actuator damping, motor constant (scaled motor constant used in simulation), joint torque limits; torque-based control policy (τ_input = τ_limit ⊗ a).</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Real robot is driven by motor current while simulation uses commanded torque (mismatch); complex actuator dynamics beyond armature/damping and motor constant (e.g., non-linear friction models, gearbox compliance/backlash, electrical motor current dynamics) are not modeled in high fidelity; joint stiffness was not modeled during training (introduced only in test).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>High-dimensional rigid-body simulation (MuJoCo) with domain randomization of a finite set of parameters and additional low-level actuator parameter randomization; augmented by expressive, state-dependent neural-network torque perturbations (MLP) to emulate broader reality gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Command-tracking (forward, lateral, heading velocities) and binary 'walking without falling' (per-seed success). Reported: in real-robot tests (command 0.4 m/s) JT-Perturb: 3/3 seeds succeeded; DR baseline: 2/3 seeds succeeded; ERFI baseline: 0/3 seeds succeeded. In simulated robustness tests (unseen actuator stiffness = 250): DR 0/ N seeds; ERFI and JT-Perturb succeeded for all seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Not reported as a numeric gap; qualitative: JT-Perturb maintained command-tracking and succeeded in real robot across all seeds, while baselines failed or only partially succeeded (proposed method showed no task-performance trade-off in nominal sim).</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Yes — compared three approaches: (1) domain randomization (DR) over a finite set of simulator parameters, (2) ERFI (random force/torque injection + some DR), and (3) JT-Perturb (this paper). JT-Perturb achieved higher robustness to unseen actuator stiffness and contact softness (sim tests) and achieved consistent sim-to-real transfer (real tests) compared to DR and ERFI.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Table I ranges: terrain friction [0.6,1.4]×default; link mass [0.6,1.4]×default; link CoM offset ±0.03 m; actuator armature [0.6,1.4]×default; actuator damping [0.0,2.9]+c_damping_default (Nm·s/rad); random push velocity [0.0,0.5] m/s; motor constant [0.8,1.2]×c_motor_default; delay [0.0,10.0] ms. Additionally, ERFI baseline uses motor constant randomization and random force injection (half environments); JT-Perturb injects state-dependent torque perturbations (MLP) in half the environments. Max perturbation magnitudes: joint torque perturbations 50 Nm, base force perturbations 80 N.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Full-sized humanoid robot TOCABI (180 cm tall, ~100 kg) with harmonic-drive actuators (gear ratio 100); lower-body 12 controlled joints (torque-based policy), upper body PD-controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failures in baselines traced to unmodeled/unseen actuator and contact dynamics: (a) unexpected actuator stiffness (spring-like joint stiffness not modeled in training) caused DR to fail in simulation; (b) softened/ compliant ground contact (changed solref timeconst) caused DR and ERFI to fail; (c) mismatch between simulator torque control and real robot motor-current-based actuation contributes to reality gap; laboratory floor unevenness and slipperiness also contributed to failures in baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Actuator and contact dynamics that are difficult or impossible to parameterize exhaustively (e.g., unmodeled joint stiffness, low torque transparency from high gear ratio actuators, motor-current vs torque control mismatch, communication delays) are critical failure modes for sim-to-real transfer; representing reality-gap effects as expressive, state-dependent joint-torque perturbations (sampled neural-net functions) yields policies that generalize across unseen actuator/contact discrepancies and achieves consistent sim-to-real transfer without degrading nominal task performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Modelling generalized forces with reinforcement learning for sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>RMA: Rapid motor adaptation for legged robots <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1921",
    "paper_id": "paper-277634130",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "JT-Perturb",
            "name_full": "Joint Torque Space Perturbation Injection (state-dependent)",
            "brief_description": "A sim-to-real training method that injects state-dependent perturbations in joint-torque space during forward simulation using randomly sampled neural networks, aiming to model a broader class of reality-gap torque perturbations than finite-parameter domain randomization.",
            "citation_title": "Sim-to-Real of Humanoid Locomotion Policies via Joint Torque Space Perturbation Injection",
            "mention_or_use": "use",
            "task_name": "Humanoid locomotion (velocity tracking / walking forward, turning)",
            "task_timescale": "≤15s (maximum episode length reported: 15s)",
            "task_contact_ratio": "contact-rich (continuous foot-ground contact during walking)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "Actuator armature (inertia), actuator damping, motor constant (scaled motor constant used in simulation), joint torque limits; torque-based control policy (τ_input = τ_limit ⊗ a).",
            "actuator_parameters_simplified": "Real robot is driven by motor current while simulation uses commanded torque (mismatch); complex actuator dynamics beyond armature/damping and motor constant (e.g., non-linear friction models, gearbox compliance/backlash, electrical motor current dynamics) are not modeled in high fidelity; joint stiffness was not modeled during training (introduced only in test).",
            "fidelity_level_description": "High-dimensional rigid-body simulation (MuJoCo) with domain randomization of a finite set of parameters and additional low-level actuator parameter randomization; augmented by expressive, state-dependent neural-network torque perturbations (MLP) to emulate broader reality gaps.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Command-tracking (forward, lateral, heading velocities) and binary 'walking without falling' (per-seed success). Reported: in real-robot tests (command 0.4 m/s) JT-Perturb: 3/3 seeds succeeded; DR baseline: 2/3 seeds succeeded; ERFI baseline: 0/3 seeds succeeded. In simulated robustness tests (unseen actuator stiffness = 250): DR 0/ N seeds; ERFI and JT-Perturb succeeded for all seeds.",
            "sim_vs_real_performance": "Not reported as a numeric gap; qualitative: JT-Perturb maintained command-tracking and succeeded in real robot across all seeds, while baselines failed or only partially succeeded (proposed method showed no task-performance trade-off in nominal sim).",
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Yes — compared three approaches: (1) domain randomization (DR) over a finite set of simulator parameters, (2) ERFI (random force/torque injection + some DR), and (3) JT-Perturb (this paper). JT-Perturb achieved higher robustness to unseen actuator stiffness and contact softness (sim tests) and achieved consistent sim-to-real transfer (real tests) compared to DR and ERFI.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Table I ranges: terrain friction [0.6,1.4]×default; link mass [0.6,1.4]×default; link CoM offset ±0.03 m; actuator armature [0.6,1.4]×default; actuator damping [0.0,2.9]+c_damping_default (Nm·s/rad); random push velocity [0.0,0.5] m/s; motor constant [0.8,1.2]×c_motor_default; delay [0.0,10.0] ms. Additionally, ERFI baseline uses motor constant randomization and random force injection (half environments); JT-Perturb injects state-dependent torque perturbations (MLP) in half the environments. Max perturbation magnitudes: joint torque perturbations 50 Nm, base force perturbations 80 N.",
            "robot_type": "Full-sized humanoid robot TOCABI (180 cm tall, ~100 kg) with harmonic-drive actuators (gear ratio 100); lower-body 12 controlled joints (torque-based policy), upper body PD-controlled.",
            "transfer_failure_analysis": "Failures in baselines traced to unmodeled/unseen actuator and contact dynamics: (a) unexpected actuator stiffness (spring-like joint stiffness not modeled in training) caused DR to fail in simulation; (b) softened/ compliant ground contact (changed solref timeconst) caused DR and ERFI to fail; (c) mismatch between simulator torque control and real robot motor-current-based actuation contributes to reality gap; laboratory floor unevenness and slipperiness also contributed to failures in baselines.",
            "key_finding_for_theory": "Actuator and contact dynamics that are difficult or impossible to parameterize exhaustively (e.g., unmodeled joint stiffness, low torque transparency from high gear ratio actuators, motor-current vs torque control mismatch, communication delays) are critical failure modes for sim-to-real transfer; representing reality-gap effects as expressive, state-dependent joint-torque perturbations (sampled neural-net functions) yields policies that generalize across unseen actuator/contact discrepancies and achieves consistent sim-to-real transfer without degrading nominal task performance.",
            "uuid": "e1921.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Modelling generalized forces with reinforcement learning for sim-to-real transfer",
            "rating": 2
        },
        {
            "paper_title": "RMA: Rapid motor adaptation for legged robots",
            "rating": 2
        }
    ],
    "cost": 0.009219,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real of Humanoid Locomotion Policies via Joint Torque Space Perturbation Injection
9 Apr 2025</p>
<p>Woohyun Cha 
Junhyeok Cha 
Jaeyong Shin 
Donghyeon Kim 
Jaeheung Park park73@snu.ac.kr 
Department of Intelligence and Information
Graduate School of Conver gence Science and Technology
ASRI
AIIS
Seoul National University
Republic of Korea</p>
<p>and Advanced Institute of Convergence Technology (AICT)
SuwonRepublic of Korea</p>
<p>Seoul National University</p>
<p>Sim-to-Real of Humanoid Locomotion Policies via Joint Torque Space Perturbation Injection
9 Apr 202574188516B9A721E446DAFE7BFAE7F7D5arXiv:2504.06585v1[cs.RO]
This paper proposes a novel alternative to existing sim-to-real methods for training control policies with simulated experiences.Prior sim-to-real methods for legged robots mostly rely on the domain randomization approach, where a fixed finite set of simulation parameters is randomized during training.Instead, our method adds state-dependent perturbations to the input joint torque used for forward simulation during the training phase.These state-dependent perturbations are designed to simulate a broader range of reality gaps than those captured by randomizing a fixed set of simulation parameters.Experimental results show that our method enables humanoid locomotion policies that achieve greater robustness against complex reality gaps unseen in the training domain.lic of Korea {woohyun321, threeman1,jasonshin0537,</p>
<p>I. INTRODUCTION</p>
<p>Deep Reinforcement Learning (DRL) for robotic applications has gained significant attention due to its demonstrated robustness and versatility.Although DRL algorithms are capable of solving complex, high-dimensional control problems, commonly used on-policy methods often require a prohibitively large amount of data, posing a substantial challenge when collecting sufficient samples solely from real hardware.Moreover, the exploration process required for policy improvement in early training stages can raise safety concerns for both the physical robot and its operational environment.As a result, it is standard practice to train control policies in simulation by creating a model (e.g., URDF) of the robot, setting simulation parameters that approximate real world conditions, and collecting rollouts to iteratively update the policies.Nonetheless, an unavoidable discrepancy, which is often referred to as the reality gap, emerges between simulation and the real world, hindering direct policy deployment on physical systems.</p>
<p>A widely adopted strategy in robotics literature for mitigating the reality gap is the domain randomization approach [1], [2], which involves randomizing a fixed, finite set of simulation parameters during training.The goal is to encourage the resulting control policy to generalize over a broad range Fig. 1: Control policies trained in simulation cannot be directly deployed to different environments due to the reality gap rising from the discrepancies between environments.Sim-to-real methods are used to enable sim-to-sim or simto-real transfer.This work proposes a novel sim-to-real method that injects state-dependent joint torque perturbations for enhanced robustness against complex reality gaps.The experimental results demonstrate that our approach outperforms existing methods in both simulation and real world environments.</p>
<p>of domains, ideally including the real-world configuration.An effective deployment of domain randomization typically requires (1) constructing the nominal simulation environment to resemble the real system as closely as possible and (2) choosing an optimal randomization range that balances policy generalization and performance [3].Despite its success in numerous applications, domain randomization is inherently limited by the finite scope of parameters that can be randomized.Furthermore, many simulators either do not expose all necessary parameters or rely on dynamics formulations that lack sufficient expressiveness to capture real-world complexities.Consequently, control policies trained under these constraints may exhibit only limited robustness to real-world variations.</p>
<p>To overcome these shortcomings, this paper proposes a novel method that injects state-dependent perturbations in the joint torque space during training.Motivated by the observation that the reality gap can be conceived as variations in joint torque space, we employ universal function approximators to introduce perturbations that are more expressive than those derived from a fixed, finite set of simulation parameters.We illustrate an example method of implementing these statedependent joint torque perturbations and present experimental results showing that control policies trained with our method achieve greater robustness against diverse types of reality gaps, especially those that are challenging to address using conventional domain randomization techniques.</p>
<p>The remainder of this paper is organized as follows.</p>
<p>In Section II, we review the existing literature on sim-toreal transfer and legged locomotion.Section III provides formulations of the problem and describes the training and implementation methodologies employed in the proposed approach.Section IV presents experimental results obtained from both simulation and real-world environments, and Section V concludes the paper.</p>
<p>II. RELATED WORK</p>
<p>A. DRL in Legged Robots</p>
<p>Deep Reinforcement Learning (DRL) has shown significant promise in enhancing the robustness and versatility of legged robot control.Although some prior studies have employed off-policy DRL algorithms trained directly on physical platforms [4]- [6], most research on legged robots relies on on-policy methods, despite their high sample complexity, due to their ease of implementation, training stability, and the absence of hardware risks during the exploratory stages of learning.To address the large sample requirements of on-policy algorithms, researchers often leverage highfidelity simulators for data collection.However, policies trained exclusively in simulation do not readily transfer to physical systems due to discrepancies predominantly caused by actuator dynamics and communication latencies [7], [8].In addition, recent work has shown that mismatches in ground contact dynamics modeling significantly degrade the performance of locomotion policies [9].</p>
<p>B. Sim-to-Real Methods</p>
<p>A wide range of techniques have been explored to bridge the reality gap for successful policy deployment on physical robots.One straightforward approach is to calibrate the simulator so that it closely matches the real-world robot.For example, [10] conducted rigorous system identification to replicate the dynamics of a quadrupedal platform in simulation, achieving successful sim-to-real transfer.Other approaches learn correction models by training neural networks on real hardware data to estimate and compensate for the reality gap [11], [12], thereby improving simulation fidelity.However, these methods often require extensive data collection, which may be infeasible when the hardware lacks certain sensing capabilities such as link-side joint torque measurements [12], or when a partially functional policy is unavailable [11].Among sim-to-real techniques, domain randomization(DR) is the most widely adopted approach.DR involves randomizing selected simulation parameters related to dynamics [2], robot models [13], [14], and observation noise or system latencies [10] during training.DR optimizes the control policy's performance across these randomized domains, and therefore ideally requires that the real-world system is encompassed within the training distribution [3] for successful sim-to-real transfer.To this end, practitioners typically set nominal simulation parameters to match realworld conditions as closely as possible, based on system identification [15] or manufacturer data [16].Nonetheless, the range of DR remains constrained by: (1) the finite set of parameters exposed by the simulator, and (2) the assumed formulations that govern how these parameters affect the simulated physics.For example, simulators that provide limited access to contact dynamics or those that fail to accurately capture intricate actuator behaviors may yield control policies that fail under substantial real-world discrepancies.An alternative, though less prevalent, approach is random force injection (RFI) [15], [17], which introduces random biases and noise to the actuated components during forward simulation.By training the control policy in this stochastically perturbed environment, RFI achieves sim-toreal performance close to DR while reducing the complexity of parameter tuning.</p>
<p>C. Learning Humanoid Locomotion</p>
<p>Our experiments and validations employ TOCABI [18], a full-sized humanoid robot.Learning stable and natural locomotion for humanoids is particularly challenging due to the inherent instability of bipedal walking and the robot's high dimensionality, which allows for many asymmetric or unstable gait solutions [19].Existing work addresses these issues through extensive reward shaping [20]- [23] or motion imitation [24]- [28].While reward shaping demands domain expertise and heavy engineering effort to guide policies toward desired gait patterns, motion imitation requires highquality reference motion data.In this work, we adopt the Adversarial Motion Prior (AMP) framework [26]- [28] to train locomotion policies through motion imitation.</p>
<p>III. METHOD</p>
<p>A. Reinforcement Learning</p>
<p>We model the robot locomotion control problem as a discrete time partially observable Markov Decision Process (POMDP).At each time step t, the agent makes an observation o t that contains partial information of the system state s t .Given o t , the agent executes an action a t ∼ π θ (•|o t ) according to its control policy π θ (•|o).After the agent executes a t , the environment transitions to its next state s t+1 ∼ P (•|s t , a t ) and the agent receives a reward r t = r(s t , a t ).The agent's objective is to maximize the expected cumulative discounted reward over a finite horizon
T , J(π θ ) = E τ ∼p(τ |π θ ) [Σ T −1 t=0 γ t r t ]. 1)
Observation Space: The observation space O ⊂ R 48 consists of the base linear and angular velocities v base,t ∈ R 3 , ω base,t ∈ R 3 , base orientation in Euler angles [α t , β t , γ t ] ∈ R 3 , the lower body joint positions q t ∈ R 12 and velocities qt ∈ R 12 , the command vector c command = [v x,command , v y,command , ω z,command ] ∈ R 3 (base forward velocity command, base lateral velocity command, and base yaw angular velocity command), and the previously executed action a t−1 ∈ R 12 .
o t = [v base,t , ω base,t , α t , β t , γ t , q t , qt , c command , a t−1 ] (1)
In this work, the robot is trained to follow base forward commands within the range [0.0, 0.6] m/s, base lateral command of 0.0 m/s, and yaw angular velocity commands within the range [−1.0, 1.0] rad/s.The yaw angular velocity command is computed from the yaw angular heading command ϕ z,command ∈ [−3.14, 3.14] using a heuristic approach as follows:
ω z,command = clip [4(ϕ z,command − ϕ z ), −1, 1] (2)
During training, at the beginning of each episode, v x,command , v y,command , ϕ z,command are randomly sampled.ω z,command is computed at each action sampling step.</p>
<p>2) Action Space: The torque based control policy method from [16] is used in this work.The action space A ⊂ R 12 is composed of 12 actions, each used to generate the torque command to the corresponding lower body joint.The upper body joints are controlled to stay in their default positions using joint space PD control.The action is bounded in the range [−1, 1], and the lower body joint torque vector is computed by elementwise multiplying the lower body joint torque limits to the actions.
τ input,t = τ limit ⊗ a t <a href="3">N m</a>
The action is sampled in a frequency of 125Hz, which is the control frequency of the learned controllers used in this work.</p>
<p>3) Reward: The total reward is formulated as follows:
r total = αr task + (1 − α)r imitation(4)
α is a hyperparameter that determines the weights of the task reward and the imitation reward.The task reward is composed of the velocity command tracking reward and the energy minimization reward from [29].The imitation reward is calculated from how well the policy is able to mimic the reference motion data.4) Motion Imitation: This work leverages the adversarial motion prior framework [26], [27] with Wasserstein Adversarial Imitation [28] to guide the policy in learning stable locomotion with expert locomotion data as reference motion data.The input state vector for the discriminator network comprises
s t = <a href="5">h base , ψ base , v base , ω base , q, q, x base f eet , ψ base f eet </a>
Each term indicates the height of the base link, the orientation of the base link in quaternion representation, the velocities of the base link in base link frame coordinates, joint positions and velocities, the positions and orientations of the feet links in base frame coordinates, respectively.The reference motion data are acquired using the model predictive capture point control framework from [30].Three separate trajectories of walking forward, turning left and right in place are recorded by rolling out the controller in the Mujoco [31] simulation environment.</p>
<p>B. Domain Randomization in Joint Torque Space</p>
<p>This work focuses on overcoming the limitations of domain randomization methods for robot control.Consider the following dynamics equation used for forward simulation in environments, perturbed by randomizing a fixed finite set of simulation parameters.M (q; p DR )q + C(q, q; p DR ) + G(q; p DR ) + τ contact (s;
p DR ) = τ output (τ input , s; p DR )(6)
s is the full state of the simulation dynamics.The terms in the forward simulation equation are conditioned on the simulation parameter set instance p DR ∈ P simul , where P simul is the set of all possible simulation configurations.Consider the effects of the perturbation on simulation parameters, formulated as; M (q; p DR ) = M (q) + M (q; p DR ) C(q, q; p DR ) = C(q, q) + C(q, q; p DR ) G(q; p DR ) = Ḡ(q) + G(q; p DR )
τ contact (s; p DR ) = τcontact (s) + τcontact (s; p DR ) τ output (τ input , s; p DR ) = τ input + τactuator (τ input , s; p DR )(7)
where • indicates values from unperturbed simulation parameters and • represents deviations between the unperturbed parameters and their perturbed counterparts.From ( 6) and ( 7), we get the following joint space dynamics equation.M (q)q + C(q, q) + Ḡ(q) + τcontact (s)
= τ input − [ M (q; p DR ) + C(q, q; p DR ) + G(q; p DR ) +τ contact (s; p DR ) + τactuator (τ input , s; p DR )] = τ input + τ DR (s; p DR )(8)
The effects of the randomized simulation parameters are equivalent to the non-linear joint torque space perturbation function τ DR (•; p DR ), and the formulation of τ DR is fixed by the simulator, physics engine, etc.Consider the set of all possible joint torque space perturbation functions equivalent to domain randomization, F DR (P simul ) = {τ DR (•; p DR )|p simul ∈ P simul }.If the joint torque space perturbation function equivalent to the true reality gap is within the range of domain randomization, i.e. τ reality gap (•) ∈ F DR (P simul ), domain randomization would result in successful sim-to-real transfer.In other words, adding the appropriate joint torque space perturbation at the forward simulation phase is equivalent to simulating the real world dynamics accurately.Potential problems of the domain randomization approach are 1) limited access to P simul 2) the lack of expressiveness of the formulation of τ DR (•; p DR ).Most simulators not allowing changing the contact softness parameter mid-training is an example of case 1), and using simplified actuator models that do not accurately simulate complex actuator dynamics is an example of case 2).Such limitations may lead to performance degradation in sim-toreal transfer.</p>
<p>C. Training Robust Policies via Joint Torque Space Perturbations</p>
<p>This paper proposes replacing the limited F DR (P simul ) with sets of more expressive joint torque space perturbation functions, denoted as F JT , so that it is more likely that τ reality gap ∈ F JT .Practically, F JT is formed using some parameterized universal function approximator.In this work, neural networks are used to represent τ JT ∈ F JT to leverage their ease of implementation and theoretical backgrounds from the Universal Approximation theorem.Methods from [11] and [12] can be seen as learning τ reality gap (•) ≈ τ ϕ (•) ∈ F JT using real hardware data and a neural network parameterized by weight ϕ.Instead of collecting real hardware data to find the true joint torque space reality gap function, the goal of this work is to train control policies that are robust against arbitrary joint torque space perturbation functions within F JT .To this end, an example method of training control policies with perturbations from random sampled ϕ, namely τ ϕ (•), is presented.The proposed method is presented as a mere example of joint torque space perturbation injection, therefore there may be better methods of injecting joint torque space perturbation for sim-to-real robustness.</p>
<p>In order to train a policy that can generalize across a diverse set of τ ϕ , the weights ϕ are randomized at the beginning of each episode during training.The sampled τ ϕ (•) takes in a state vector of the simulation s simul as input and output the perturbation at each time step.The perturbation is added to the simulation by propagating the forward dynamics with the sum of the input torque from the control policy τ π and the perturbation.
(s t+1 , r t+1 ) = f orward(s t , τ π (o t ) + τ ϕ (s simul,t )) τ π (o t ) ∼ π θ (•|o t )(9)
In this work, τ ϕ is modeled as a Multi-Layer Perceptron (MLP) network, with two hidden layers of 256 ReLU units and tanh activation layer in between.A tanh layer is added to the end of the neural network to softly clamp the output.Then, a maximum perturbation coefficient σ lim is multiplied to the final output to compute the joint torque space perturbation at each time step.For stable output distribution, the input vector s simul is normalized and θ is random sampled using Xavier initialization [32].For practical implementation, s simul,t is replaced with a privileged observation vector o privileged = [ψ base , ω base , ωbase , v base , vbase , q, q, τ input , F contact ], where F contact is the contact wrench vector at the feet.
τ ϕ (o privileged ) = σ lim × tanh(M LP (ô privileged )) (10)
In our implementation, the privileged observation vector is normalized with its running standard deviation estimate σ o privileged , but not zero-centered.A zero-centered normalization would result in:
ô′ privileged = o privileged − µ o privileged σ o privileged(11)
, where µ o privileged is the running mean estimate of the privileged observation vector.Instead, the vector is normalized to be zero when its original value is zero as well:
ôprivileged = o privileged /σ o privileged .(12)
Also, τ ϕ is designed to have zero bias in all layers, so that zero input would always result in zero output.We assume that the reality gap is minimal when the robot is not in contact, not moving, not actuated, and in zero configuration.Such design choices and assumptions allow simulating more realistic reality gaps.Again, ϕ is not learned, as ϕ is randomly sampled at the beginning of each episode.In this work, control policies are trained for humanoid locomotion tasks.Since the robot has a floating base, the dynamics equation and the joint torque space perturbation injection method involve the virtual joints as well.Virtual joints are non-physical constructs introduced into the robot models of floating base robots that represent the motion of the robot base in the inertial frame.Therefore, injecting joint torque space perturbations includes perturbing the robot base.Although reality gaps in joint torque space include moment perturbations as well, in practice the robot base is perturbed only with force perturbations during training under the assumption that the moment perturbations equivalent to reality gaps are trivial.</p>
<p>D. Training Setup</p>
<p>Our method is compared to the domain randomization (DR) baseline and the random force injection (ERFI) baseline from [17].All policies are trained to produce natural and stable gait patterns that mimic the reference motion data while controlling the humanoid robot TOCABI to follow velocity commands c command = [v x,command , v y,command , ω z,command ].TOCABI is a fullsized robot that stands 180cm tall and weighs approximately 100kg.TOCABI has harmonic drives with the gear ratio of 100 as its actuators.Such a high gear ratio leads to lower torque transparency and higher uncertainty in actuator dynamics, and thus more challenging sim-to-real transfer.</p>
<p>The control policies are trained using the Proximal Policy Optimization (PPO) algorithm [33].In addition to the PPO loss function, the AMP loss function L AM P from [28] and the gradient penalty loss function L gradpen from [34] are jointly optimized for motion imitation and smooth policy behavior.The resulting loss function is as follows;
L total = L P P O + L AM P + 0.002L gradpen (13)
Both actor and critic networks are modeled as MLPs with two hidden layers of 512 ELU units.The policy takes as input the history of observations of 10 steps collected with a step skipped between.In addition to the history of observations, the long state history encoder from [35] is used as a means of adaptation.Without the encoder, the control policies trained with the baseline methods suffered from severe drifting problems in real robot experiments.The discriminator network, used in the AMP framework, is modeled as an MLP with two hidden layers of 256 ReLU units.Samples for policy updates are collected using the GPU accelerated simulator IssacGym [36], to simultaneously roll out 4096 agents in simulation and collect 98304 samples per policy update for a total of 5000 policy updates.The maximum episode length is 15 seconds on simulation time, with early termination triggered when undesired contact occurs at robot links other than the foot links or when the base link of the robot is below the threshold height.</p>
<p>Table I shows the list and ranges of domain randomization parameters.The gray sections indicate parameters that are randomized in only the DR baseline, the light gray section the parameter that is randomized in both the DR baseline and the ERFI baseline, and the white section the parameter that is randomized in all training schemes.Our method, as well as other baselines, are trained with delay randomization, which simulates latencies between action computation and execution.The ERFI baseline is trained with motor constant randomization.The motor constant is multiplied to τ π to form the final input joint torque in simulation.Since our real robot actually controls not the torque but the motor current, as opposed to the simulation, it is necessary to randomize the motor constant.This also accounts for the torque bias injection from [17].The other terms in Table I are parameters commonly randomized for DR implementations.Random push is implemented by setting the velocity of the base link in the transverse plane to a random value.Similar to [17], half of the parallelized environments are perturbed with random force injection in the ERFI baseline, and with the state-dependent joint torque space perturbation in our method.In our method, it was observed that perturbing half of the environments only is necessary in learning proper gait patterns.The maximum magnitudes of the perturbations in both the ERFI baseline and our method are set to 50Nm and 80N, for joint torque perturbations and robot base force perturbations respectively.</p>
<p>IV. EXPERIMENTAL RESULTS</p>
<p>In this section, the results of comparing our method with the baseline methods are presented.Simulation experiments are conducted using the Mujoco simulator as the test environment.Section IV-A.1 shows evaluations of command tracking performance in the test environment under nominal parameter settings.Sections IV-A.2 and IV-A.3 present evaluations of robustness against reality gaps that are not simulated during training.The robustness of the methods are tested by setting simulation parameters to emulate soft ground contact and unexpected actuator stiffness in the test environment.The stiffness parameter is assumed to be an aspect of the actuator dynamics that cannot be modeled in Fig. 2: Forward velocity (X), lateral velocity (Y), and heading command tracking performance when commanded to walk forward in 0.4m/s.No significant differences in command tracking performance are observed, which indicates that the enhanced robustness of our method does not come at the expense of task performance.Fig. 3: Forward velocity command tracking performances when the command velocity is 0.4m/s under the scenario where unseen actuator stiffness is introduced.All seeds of the DR baseline failed to produce proper gait patterns, and instead drastically leaned forward and lost balance.All seeds of our method and the ERFI baseline succeeded in producing proper gait patterns and following velocity commands.</p>
<p>the training environment, but exists in the real robot.Section IV-B illustrates results of sim-to-real transfer.Results from both simulation and real robot experiments show that our method is the most robust against unseen reality gaps without task performance trade-off.</p>
<p>A. Simulation 1) Sim-to-sim transfer: In this experiment, the policies control the robot to walk forward in 0.4m/s in the test environment.The simulation parameters are set to nominal values measured and calculated from rigorous inspections and experiments on real hardware and specification sheets.Fig. 2 shows the command tracking performance of the methods.No significant differences in command tracking performance are observed across the methods, indicating that the enhanced robustness of our method does not come at the expense of task performance.Fig. 4: Augmenting ground softness in Mujoco.Setting the timeconst in the solref parameter of the ground higher(right) in simulation causes objects to sink more into the ground before the reaction force is applied, which gives the impression of a softer ground and results in more contact penetration.Fig. 5: Forward velocity command tracking performances when the command velocity is 0.4m/s under the scenario where unseen ground contact dynamics is introduced.All seeds of the DR baseline and the ERFI baseline were not able to move forward without falling.All seeds of our method succeeded in walking forward without falling.</p>
<p>2) Walking against unseen actuator dynamics: In this experiment, the joint stiffness parameter of the simulated actuators is set in the test environment.The joint stiffness parameter creates a spring element in each joint whose equilibrium position is set as the initial joint position.This joint stiffness is not simulated during training.We assume that walking against such perturbed actuator dynamics demonstrates robustness against unexpected dynamics that is not or cannot be simulated during training.Fig. 3 shows the tracking performance when the commanded forward velocity is 0.4m/s and the joint stiffness parameter is set to 250.All seeds of the DR baseline failed to produce stable gait patterns, as they drastically lean forward and lose balance.All seeds of the ERFI baseline and our method successfully controlled the robot to produce proper gait patterns.</p>
<p>3) Walking against unseen contact dynamics: In this experiment, the timeconst in the solref parameter of the ground is set to 0.2 in the test environment.This results in the solver resolving the contact constraint more gradually, introducing compliance, and simulating soft contact.Fig. 4 shows how contact softness is emulated in the Mujoco simulator.The robot is commanded to walk forward in 0.4m/s.Likewise, walking against such perturbed contact dynamics indirectly demonstrates robustness against unexpected dynamics that cannot be simulated during training.Fig. 5 shows the com- Fig. 7: Sim-to-real experiment results.Forward velocity (X), lateral velocity (Y), and heading command tracking performance when commanded to walk forward in 0.4m/s.All seeds of the ERFI baseline failed to stabilize the robot.Two of the three seeds of the DR baseline succeeded in controlling the robot to walk without falling, while all seeds of our method successfully controlled the robot to walk without falling.The environment of the laboratory is not ideal, as the ground is slightly unleveled, sloped, and slippery.This makes the reality gap more complex and thus the sim-to-real transfer more challenging.</p>
<p>mand tracking performance of the methods.All seeds of the DR baseline and the ERFI baseline failed to walk without falling, while all seeds of our method were able to walk while following the velocity commands.Also, the DR baseline and the ERFI baseline failed to walk when smaller timeconst values were set (such as 0.15), while our method was able to walk when larger timeconst values were set (such as 0.25).</p>
<p>B. Sim-to-Real</p>
<p>Fig. 6(a) shows a snapshot of the successful sim-to-real experiment with the policy trained with our method.It is worth noting that the floor of our laboratory is slightly uneven and slippery, resulting in more complex reality gaps and challenging sim-to-real problems.Fig. 7 shows the command tracking performance of the methods controlling the real robot.Note that all seeds of the ERFI baseline, which demonstrated better robustness than the DR baseline in simulation experiments, failed to control the robot while some seeds of the DR baseline were able to control the robot to walk properly.Our method was the only one that succeeded in sim-to-real transfer across all seeds.Such results imply that there exist complex and unseen reality gaps in the hardware and the environment, such that our method can handle better than the other baseline methods.</p>
<p>V. CONCLUSIONS This paper introduced a novel alternative to existing simto-real techniques by injecting state-dependent perturbations in joint torque space during control policy training.Building on the insight that the reality gap can be equivalently modeled as perturbations in joint torque space, we demonstrated that randomizing a fixed finite set of simulation parameters can be replaced by more expressive, state-dependent joint torque space perturbations.An example implementation involving random sampling of non-linear joint torque space functions is presented.In contrast to domain randomization, where the range of robustness is inherently restricted by the finite set of sampled simulation parameters, our approach enables control policies to learn robust behaviors under a substantially broader spectrum of reality gaps.</p>
<p>We validated the proposed approach through locomotion experiments on a full-sized humanoid robot, both in simulation and with the real robot.Simulation results indicate that our methods outperform baseline techniques in addressing reality gaps arising from discrepancies that were not, or could not be, introduced during training, especially those regarding the actuator and contact dynamics.In real robot experiments, our method consistently showed enhanced robustness against the reality gap arising from current hardware and laboratory settings.These findings highlight the potential of our approach to generalize more effectively in cases where unmodeled dynamics or other complex factors play a critical role.Furthermore, it is highlighted that these robustness gains did not come at the expense of task performance in unperturbed domains.</p>
<p>Overall, our methods offer promising pathways for bridging the reality gap in a wide array of robotic systems and tasks, extending beyond humanoid locomotion.They can further support the adoption of deep reinforcement learning algorithms in real-world environments, particularly for highdimensional problems with steep sample complexity, by providing a robust framework for training policies that are inherently more adaptable to aspects of the dynamics that are not or cannot be modeled.</p>
<p>Fig. 6 :
6
Fig.6: Snapshots of a success case (our method, (a)) and a failure case (seed 3 of the DR baseline, (b)).All seeds trained with our method succeeded in sim-to-real transfer, while a seed of the DR baseline and all seeds of the ERFI baseline failed in sim-to-real transfer.</p>
<p>TABLE I :
I
Domain randomization range
ParameterRangeUnitTerrain friction[0.6, 1.4] × f def ault-Link mass[0.6, 1.4] × m def aultkgLink CoM[−0.03, 0.03] + x CoMmActuator armature[0.6, 1.4] × I armature,def aultkg•m 2Actuator damping[0.0, 2.9] + c damping,def aultNm • s/radRandom push[0.0, 0.5]m/sObs bias/noise--Motor constant[0.8, 1.2] × c motor,def ault-Delay[0.0, 10.0]ms
Department of Intelligence and Information, Graduate School of Convergence Science and Technology, Seoul National University, Repub-
*This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education(RS-2023-00274280
Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). </p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, arXiv:2107.040342021arXiv preprint</p>
<p>A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. L Smith, I Kostrikov, S Levine, arXiv:2208.078602022arXiv preprint</p>
<p>Soft actor-critic algorithms and applications. T Haarnoja, A Zhou, K Hartikainen, G Tucker, S Ha, J Tan, V Kumar, H Zhu, A Gupta, P Abbeel, arXiv:1812.059052018arXiv preprint</p>
<p>Daydreamer: World models for physical robot learning. P Wu, A Escontrela, D Hafner, P Abbeel, K Goldberg, Conference on robot learning. PMLR2023</p>
<p>Why off-the-shelf physics simulators fail in evaluating feedback controller performance-a case study for quadrupedal robots. M Neunert, T Boaventura, J Buchli, Advances in Cooperative Robotics. World Scientific2017</p>
<p>How to train your robot with deep reinforcement learning: lessons we have learned. J Ibarz, J Tan, C Finn, M Kalakrishnan, P Pastor, S Levine, The International Journal of Robotics Research. 404-52021</p>
<p>Learning quadrupedal locomotion on deformable terrain. S Choi, G Ji, J Park, H Kim, J Mun, J H Lee, J Hwangbo, Science Robotics. 874e22562023</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.103322018arXiv preprint</p>
<p>Modelling generalized forces with reinforcement learning for sim-to-real transfer. R Jeong, J Kay, F Romano, T Lampe, T Rothorl, A Abdolmaleki, T Erez, Y Tassa, F Nori, arXiv:1910.094712019arXiv preprint</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 42658722019</p>
<p>Genloco: Generalized locomotion controllers for quadrupedal robots. G Feng, H Zhang, Z Li, X B Peng, B Basireddy, L Yue, Z Song, L Yang, Y Liu, K Sreenath, Conference on Robot Learning. PMLR2023</p>
<p>Policy transfer via kinematic domain randomization and adaptation. I Exarchos, Y Jiang, W Yu, C K Liu, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Crossing the gap: A deep dive into zero-shot sim-to-real transfer for dynamics. E Valassakis, Z Ding, E Johns, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Torque-based deep reinforcement learning for task-and-robot agnostic learning on bipedal robots using sim-to-real transfer. D Kim, G Berseth, M Schwartz, J Park, IEEE Robotics and Automation Letters. 2023</p>
<p>Learning and deploying robust locomotion policies with minimal dynamics randomization. L Campanaro, S Gangapurwala, W Merkt, I Havoutis, 6th Annual Learning for Dynamics &amp; Control Conference. PMLR2024</p>
<p>Design of the humanoid robot tocabi. M Schwartz, J Sim, J Ahn, S Hwang, Y Lee, J Park, 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids). IEEE2022</p>
<p>Learning symmetric and low-energy locomotion. W Yu, G Turk, C K Liu, ACM Transactions on Graphics. 3742018</p>
<p>Learning task space actions for bipedal locomotion. H Duan, J Dao, K Green, T Apgar, A Fern, J Hurst, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Sim-to-real learning of all common bipedal gaits via periodic reward composition. J Siekmann, Y Godse, A Fern, J Hurst, 2021 IEEE International Conference on Robotics and Automation (ICRA). </p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. N Rudin, D Hoeller, P Reist, M Hutter, Conference on Robot Learning. PMLR2022</p>
<p>Learning humanoid locomotion over challenging terrain. I Radosavovic, S Kamat, T Darrell, J Malik, arXiv:2410.036542024arXiv preprint</p>
<p>Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. X B Peng, P Abbeel, S Levine, M Van De Panne, ACM Transactions On Graphics (TOG). 3742018</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, arXiv:2004.007842020arXiv preprint</p>
<p>Amp: Adversarial motion priors for stylized physics-based character control. X B Peng, Z Ma, P Abbeel, S Levine, A Kanazawa, ACM Transactions on Graphics (ToG). 4042021</p>
<p>Adversarial motion priors make good substitutes for complex reward functions. A Escontrela, X B Peng, W Yu, T Zhang, A Iscen, K Goldberg, P Abbeel, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Humanmimic: Learning natural locomotion and transitions for humanoid robot via wasserstein adversarial imitation. A Tang, T Hiraoka, N Hiraoka, F Shi, K Kawaharazuka, K Kojima, K Okada, M Inaba, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024114</p>
<p>Minimizing energy consumption leads to the emergence of gaits in legged robots. Z Fu, A Kumar, J Malik, D Pathak, arXiv:2111.016742021arXiv preprint</p>
<p>A model predictive capture point control framework for robust humanoid balancing via ankle, hip, and stepping strategies. M.-J Kim, D Lim, G Park, J Park, arXiv:2307.132432023arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ international conference on intelligent robots and systems. 2012</p>
<p>Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings2010</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Learning smooth humanoid locomotion through lipschitz-constrained policies. Z Chen, X He, Y.-J Wang, Q Liao, Y Ze, Z Li, S S Sastry, J Wu, K Sreenath, S Gupta, arXiv:2410.118252024arXiv preprint</p>
<p>Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. Z Li, X B Peng, P Abbeel, S Levine, G Berseth, K Sreenath, The International Journal of Robotics Research. 027836492412851612024</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
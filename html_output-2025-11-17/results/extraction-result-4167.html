<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4167 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4167</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4167</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-279155102</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.03587v1.pdf" target="_blank">Preface to the Special Issue of the TAL Journal on Scholarly Document Processing</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scholarly literature makes it increasingly difficult for researchers to keep up with new knowledge. Automated tools are now more essential than ever to help navigate and interpret this vast body of information. Scientific papers pose unique difficulties, with their complex language, specialized terminology, and diverse formats, requiring advanced methods to extract reliable and actionable insights. Large language models (LLMs) offer new opportunities, enabling tasks such as literature reviews, writing assistance, and interactive exploration of research. This special issue of the TAL journal highlights research addressing these challenges and, more broadly, research on natural language processing and information retrieval for scholarly and scientific documents.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4167.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4167.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArxivDIGESTables</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system (described in the cited work) that uses language models to synthesize information from scientific papers into structured tables, enabling extraction and aggregation of quantitative findings across literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ArxivDIGESTables</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in the cited paper as a language-model-based system that synthesizes scientific literature into tabular summaries to surface structured, primarily quantitative information across multiple papers. The preface does not provide implementation details (model architecture, prompting strategy, pipeline) beyond this high-level description.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-disciplinary scholarly literature (ArXiv / multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>extraction/synthesis of quantitative data and metrics into structured tabular summaries (i.e., distillation of numeric findings and relationships across papers)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Language-model-driven synthesis into tables (exact parsing methods for text, figures, or tables not specified in this preface; the system is described as generating tabular representations of literature).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Only high-level functionality is reported in the preface; specific limitations of ArxivDIGESTables are not described here. General challenges noted in the preface that are relevant include the multi-modal nature of papers (figures, tables), complex specialized terminology, and lack of detailed validation/benchmarking descriptions in this overview.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4167.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4167.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HoneyComb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HoneyComb: A Flexible LLM-Based Agent System for Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent system targeted at materials science that (according to the cited work) provides flexible, model-driven assistance to domain experts; implied application areas include discovery and exploration of materials relationships and patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HoneyComb: A Flexible LLM-Based Agent System for Materials Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HoneyComb</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referred to as an LLM-based agent system for materials science in the references; the preface does not include technical details such as architecture, prompting, pipelines, or concrete extraction methods for quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>potential discovery/identification of materials relationships or patterns (not specified in the preface)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>No system-specific limitations are reported in the preface; general domain challenges include domain-specific terminology, multimodality of scientific documents, and computational/environmental costs for LLM use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4167.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4167.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON: Scientific Inspiration Machines Optimized for Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for generating scientific inspiration optimized for novelty; implied to use machine learning/LLM techniques to surface novel research directions and possibly relationships across literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciMON: Scientific Inspiration Machines Optimized for Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in the references as a system for producing novel scientific inspiration; the preface does not provide specifics on how it extracts quantitative laws or patterns from literature, nor on its architecture or methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational linguistics / general scientific discovery (paper presented at ACL relates to NLP & discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>potential generation/discovery of novel relationships or hypotheses (not explicitly described as extracting formal quantitative laws in the preface)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not specified in the preface; general limitations for such systems include verifying novelty, validating generated hypotheses, and grounding outputs in quantitative evidence from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4167.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4167.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Survey_Scientific_LLMs_2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A survey paper that reviews scientific LLMs and their applications for scientific discovery, which includes accelerating discovery and analyzing multiple papers to synthesize insights; mentioned as supporting the claim that LLMs can accelerate scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This is a survey paper referenced in the preface; it summarizes various scientific LLMs and applications (including discovery) but the preface does not extract system-level details from it.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-disciplinary / scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>discusses LLM applications in scientific discovery broadly (may include extraction of relationships/patterns), but no specific quantitative-law examples are provided in the preface.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Survey-level discussion includes mentions of computational/environmental costs and ethical concerns; no specific empirical limitations for law-extraction tasks are detailed in the preface.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Preface to the Special Issue of the TAL Journal on Scholarly Document Processing', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models <em>(Rating: 2)</em></li>
                <li>HoneyComb: A Flexible LLM-Based Agent System for Materials Science <em>(Rating: 2)</em></li>
                <li>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery <em>(Rating: 2)</em></li>
                <li>SciMON: Scientific Inspiration Machines Optimized for Novelty <em>(Rating: 1)</em></li>
                <li>OpenResearcher: Unleashing AI for Accelerated Scientific Research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4167",
    "paper_id": "paper-279155102",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "ArxivDIGESTables",
            "name_full": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "brief_description": "A system (described in the cited work) that uses language models to synthesize information from scientific papers into structured tables, enabling extraction and aggregation of quantitative findings across literature.",
            "citation_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "mention_or_use": "mention",
            "system_name": "ArxivDIGESTables",
            "system_description": "Described in the cited paper as a language-model-based system that synthesizes scientific literature into tabular summaries to surface structured, primarily quantitative information across multiple papers. The preface does not provide implementation details (model architecture, prompting strategy, pipeline) beyond this high-level description.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "cross-disciplinary scholarly literature (ArXiv / multi-domain)",
            "number_of_papers": null,
            "law_type": "extraction/synthesis of quantitative data and metrics into structured tabular summaries (i.e., distillation of numeric findings and relationships across papers)",
            "law_examples": null,
            "extraction_method": "Language-model-driven synthesis into tables (exact parsing methods for text, figures, or tables not specified in this preface; the system is described as generating tabular representations of literature).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Only high-level functionality is reported in the preface; specific limitations of ArxivDIGESTables are not described here. General challenges noted in the preface that are relevant include the multi-modal nature of papers (figures, tables), complex specialized terminology, and lack of detailed validation/benchmarking descriptions in this overview.",
            "comparison_baseline": null,
            "uuid": "e4167.0",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "HoneyComb",
            "name_full": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "brief_description": "An LLM-based agent system targeted at materials science that (according to the cited work) provides flexible, model-driven assistance to domain experts; implied application areas include discovery and exploration of materials relationships and patterns.",
            "citation_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "mention_or_use": "mention",
            "system_name": "HoneyComb",
            "system_description": "Referred to as an LLM-based agent system for materials science in the references; the preface does not include technical details such as architecture, prompting, pipelines, or concrete extraction methods for quantitative laws.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "materials science",
            "number_of_papers": null,
            "law_type": "potential discovery/identification of materials relationships or patterns (not specified in the preface)",
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "No system-specific limitations are reported in the preface; general domain challenges include domain-specific terminology, multimodality of scientific documents, and computational/environmental costs for LLM use.",
            "comparison_baseline": null,
            "uuid": "e4167.1",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "brief_description": "A system for generating scientific inspiration optimized for novelty; implied to use machine learning/LLM techniques to surface novel research directions and possibly relationships across literature.",
            "citation_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "mention_or_use": "mention",
            "system_name": "SciMON",
            "system_description": "Mentioned in the references as a system for producing novel scientific inspiration; the preface does not provide specifics on how it extracts quantitative laws or patterns from literature, nor on its architecture or methods.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computational linguistics / general scientific discovery (paper presented at ACL relates to NLP & discovery)",
            "number_of_papers": null,
            "law_type": "potential generation/discovery of novel relationships or hypotheses (not explicitly described as extracting formal quantitative laws in the preface)",
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Not specified in the preface; general limitations for such systems include verifying novelty, validating generated hypotheses, and grounding outputs in quantitative evidence from literature.",
            "comparison_baseline": null,
            "uuid": "e4167.2",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Survey_Scientific_LLMs_2024",
            "name_full": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "brief_description": "A survey paper that reviews scientific LLMs and their applications for scientific discovery, which includes accelerating discovery and analyzing multiple papers to synthesize insights; mentioned as supporting the claim that LLMs can accelerate scientific discovery.",
            "citation_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": null,
            "system_description": "This is a survey paper referenced in the preface; it summarizes various scientific LLMs and applications (including discovery) but the preface does not extract system-level details from it.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "cross-disciplinary / scientific discovery",
            "number_of_papers": null,
            "law_type": "discusses LLM applications in scientific discovery broadly (may include extraction of relationships/patterns), but no specific quantitative-law examples are provided in the preface.",
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Survey-level discussion includes mentions of computational/environmental costs and ethical concerns; no specific empirical limitations for law-extraction tasks are detailed in the preface.",
            "comparison_baseline": null,
            "uuid": "e4167.3",
            "source_info": {
                "paper_title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
            "rating": 2,
            "sanitized_title": "honeycomb_a_flexible_llmbased_agent_system_for_materials_science"
        },
        {
            "paper_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "rating": 2,
            "sanitized_title": "a_comprehensive_survey_of_scientific_large_language_models_and_their_applications_in_scientific_discovery"
        },
        {
            "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "rating": 1,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
            "rating": 1,
            "sanitized_title": "openresearcher_unleashing_ai_for_accelerated_scientific_research"
        }
    ],
    "cost": 0.0095245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Preface to the Special Issue of the TAL Journal on Scholarly Document Processing
4 Jun 2025</p>
<p>Florian Boudin 
JFLI
CNRS
Nantes University
France</p>
<p>Akiko Aizawa 
National Institute of Informatics
Japan</p>
<p>Preface to the Special Issue of the TAL Journal on Scholarly Document Processing
4 Jun 202541F16CB88D7E3BD5A12DFCDDE286FDD4arXiv:2506.03587v1[cs.DL]Scholarly Document ProcessingNatural Language Processing for ScienceLarge Language Models (LLMs) MOTS-CLÉS : Traitement automatique de documents scientifiquesTraitement Automatique des Langue pour la scienceGrands modèles de langues (LLMs)
The rapid growth of scholarly literature makes it increasingly difficult for researchers to keep up with new knowledge.Automated tools are now more essential than ever to help navigate and interpret this vast body of information.Scientific papers pose unique difficulties, with their complex language, specialized terminology, and diverse formats, requiring advanced methods to extract reliable and actionable insights.Large language models (LLMs) offer new opportunities, enabling tasks such as literature reviews, writing assistance, and interactive exploration of research.This special issue of the TAL journal highlights research addressing these challenges and, more broadly, research on natural language processing and information retrieval for scholarly and scientific documents.RÉSUMÉ.La croissance rapide de la littérature scientifique rend de plus en plus difficile pour les chercheurs de suivre l'évolution des connaissances.Le recours à des outils automatisés est aujourd'hui indispensable pour naviguer et interpréter cette immense masse d'informations.Les articles scientifiques posent des difficultés uniques en raison de leur langage complexe, de leur terminologie spécialisée et de leurs formats variés, ce qui nécessite des méthodes avancées pour extraire des informations fiables et exploitables.Les grands modèles de langage (LLMs) ouvrent de nouvelles perspectives, permettant des tâches telles que les revues de littérature, l'assistance à la rédaction et l'exploration interactive des travaux scientifiques.Ce numéro spécial de la revue TAL met en lumière des recherches qui s'attaquent à ces défis et, plus largement, des recherches sur le traitement automatique des langues et la recherche d'information appliqués aux documents scientifiques et académiques.</p>
<p>Introduction</p>
<p>The volume of scholarly literature is expanding rapidly.A compelling example is the ACL Anthology 1 , a repository for scientific contributions within the fields of computational linguistics and Natural Language Processing (NLP), which recently surpassed 100,000 papers, doubling its size in just four years (Bollmann et al., 2023).As the rate of publication continues to accelerate, researchers and institutions face increasing challenges in keeping up with the flood of new knowledge.This highlights the critical need for automated methods to help navigate, understand and distill the growing body of scientific information.</p>
<p>To address this pressing challenge, researchers across various fields-including computational linguistics, NLP, text mining, information retrieval, digital libraries and scientometrics-have dedicated significant efforts into developing methods and resources designed to process scientific documents.This led to a surge in publications on the matter, alongside the successful hosting of numerous international events, such as the workshops Scholarly Document Processing (SDP) (Ghosal et al., 2024), SCIentific DOCument Analysis (SCIDOCA) (Nguyen and Matsumoto, 2024), Natural Language Processing for Scientific Text (SciNLP) (Cohan et al., 2021) and Bibliometricenhanced Information Retrieval (BIR) (Frommholz et al., 2024).</p>
<p>At the national level in France, scholarly document processing is also gaining momentum.This interest is exemplified by the success of the workshop Analyse et Recherche de Textes Scientifiques (ARTS) 2 (Boudin et al., 2023), held at the TALN-CORIA 2023 conference.The event, which saw the presentation of 12 papers and attracted over 40 participants, highlighted the relevance of the topic and prompted the call for this special issue of the TAL journal.</p>
<p>Scientific papers present unique challenges for document processing methods due to their inherent complexity.They are characterized by intricate technical language, discipline-specific terminology, distinct structural conventions, and frequent use of mathematical expressions, all of which pose significant challenges for current methods (Ramesh Kashyap et al., 2023).Additionally, the multi-modal nature of scientific papers, with their tables, figures and diagrams, further complicates their processing (Shen et al., 2022).Beyond these document-level challenges, effective methods should also account for features present at the collection level, such as citation networks, and leverage rich metadata, including authors, keywords, and publication venues, each introducing its own set of difficulties.</p>
<p>Developing methods to extract reliable, valuable and verifiable information from scientific papers is crucial for many downstream applications, including retrieval (Boudin et al., 2020;Wang et al., 2023), recommendation (Kreutz and Schenkel, 2022;Huang et al., 2024), summarization (Luo et al., 2023), questionanswering (Saikh et al., 2022;Auer et al., 2023) and document understanding (Wright and Augenstein, 2021;Veyseh et al., 2021).With the rise of large language models (LLMs) and their enhanced ability to analyze and synthetize insights across multiple scientific papers, new applications are continuously emerging.Promising developments include accelerating scientific discovery (Zhang et al., 2024b), generating novel research directions (Wang et al., 2024), reviewing of the literature (Newman et al., 2024), assisting scientific writing (Jourdan et al., 2024) and enabling interactive exploration of papers (Zheng et al., 2024).</p>
<p>LLMs are also being developed for specialized scientific domains, such as healthcare and medicine (Labrak et al., 2024) or material sciences (Zhang et al., 2024a).These domain-specific models assist experts and researchers with complex tasks, including drug discovery (Savage, 2023), diagnosis generation (Abdullahi et al., 2024), and science education (Cooper, 2023).</p>
<p>Efforts are also underway to reduce the growing computational and environmental costs associated with training and deploying LLMs (Hershcovich et al., 2022;Sadat Moosavi et al., 2023).At the same time, ethical concerns are being addressed, with research focused on the responsible use of LLMs in science, including issues of bias, fairness, and transparency in AI-driven research (Peled-Cohen et al., 2024).This special issue of the TAL journal focuses on research addressing these challenges, with an emphasis on NLP and information retrieval for scholarly and scientific documents.</p>
<p>Call, Reviewing and Selection of Papers</p>
<p>The call for submissions to this special issue of the TAL journal on scholarly document processing was announced in December 2023, and the submission platform 3 closed in March 2024.The scope of relevant topics extended beyond NLP and information retrieval tasks, tools, and resources designed for scientific documents, encompassing areas such as bibliometrics, scientometrics, citation analysis and recommendation, claim verification, plagiarism detection and scientific writing assistance.</p>
<p>A total of five articles were submitted (two in French and three in English) by authors from Iran, India and France.Each article was reviewed by three experts, two members of the scientific committee and one member of the Editorial Board.In May 2024, the Editorial Board and the guest editors met to discuss the first round of reviews and notified the authors of the outcomes.One paper was selected for a second round of reviews and was ultimately accepted, resulting in a selection rate of 20%.</p>
<ol>
<li>https://tal-65-2.sciencesconf.org/3.Accepted paperThis issue of the TAL journal features one paper: Évaluation de la qualité de rapport des essais cliniques avec des larges modèles de langue (Evaluating clinical trials research article quality with large language models) by Mathieu Laï-king and Patrick Paroubek.The paper focuses on the biomedical domain, specifically investigating the use of LLMs to evaluate the quality of Randomized Controlled Trials (RCTs), a type of clinical research article.The authors frame the evaluation task as a questionanswering problem, prompting LLMs to assess articles according to the Consolidated Standards of Reporting Trials (CONSORT) framework.Through extensive experiments, the study demonstrates the high effectiveness of LLMs, achieving an accuracy of up to 85%, thus paving the way for advancements in automating quality assessment in clinical research.</li>
</ol>
<p>AcknowledgementsWe would like to thank the editorial committee of the TAL journal for inviting us to coordinate the scientific committee for this special issue.We are especially grateful to the editors-in-chief for their support and invaluable assistance throughout this process.Finally, we would like to thank all the reviewers and members of the scientific committee who joined us for this special issue and generously volunteered their time to help us select the articles published here.
Learning to Make Rare and Complex Diagnoses With Generative AI Assistance: Qualitative Study of Popular Large Language Models. References Abdullahi, T Singh, R Eickhoff, C , JMIR Med Educ. 10e51391Feb, 2024</p>
<p>The sciqa scientific question answering benchmark for scholarly knowledge. S Auer, D A Barone, C Bartz, E G Cortes, M Y Jaradeh, O Karras, M Koubarakis, D Mouromtsev, D Pliukhin, D Radyush, Scientific Reports. 1372402023</p>
<p>M Bollmann, N Schneider, A Köhn, M Post, Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023). L Tan, D Milajevs, G Chauhan, J Gwinnup, E Rippeth, the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)SingaporeAssociation for Computational LinguisticsDecember, 2023Two Decades of the ACL Anthology: Development, Impact, and Open Challenges</p>
<p>F Boudin, B Daille, R Dufour, O El, M Houbre, L Jourdan, N Kooli, Actes de CORIA-TALN 2023. Actes de l'atelier "Analyse et Recherche de Textes Scientifiques" (ARTS)@TALN 2023. s de CORIA-TALN 2023. s de l'atelier "Analyse et Recherche de Textes Scientifiques" (ARTS)@TALN 2023Paris, France, 62023</p>
<p>Keyphrase Generation for Scientific Document Retrieval. F Boudin, Y Gallina, A Aizawa, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly, 2020</p>
<p>A Cohan, P Dasigi, T Hope, K Lo, S Mohan, A Wade, L Wang, I Williams, D Zhang, Proceedings of the 2nd Workshop on Natural Language Processing for Scientific Text. the 2nd Workshop on Natural Language Processing for Scientific TextSciNLP 2021. August, 2021</p>
<p>Examining science education in ChatGPT: An exploratory study of generative artificial intelligence. G Cooper, Journal of Science Education and Technology. 322023</p>
<p>I Frommholz, P Mayr, G Cabanac, Proceedings of the 14th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2024). S Verberne, the 14th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2024)Glasgow, ScotlandMarch, 2024</p>
<p>T Ghosal, A Singh, A Waard, P Mayr, A Naik, O Weller, Y Lee, S Shen, Y Qin, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)Bangkok, ThailandAugust, 2024</p>
<p>Towards Climate Awareness in NLP Research. D Hershcovich, N Webersinke, M Kraus, J Bingler, M Leippold, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember, 2022</p>
<p>A scientific paper recommendation method using the time decay heterogeneous graph. Z Huang, D Tang, R Zhao, W Rao, Scientometrics. 1292024</p>
<p>CASIMIR: A Corpus of Scientific Articles Enhanced with Multiple Author-Integrated Revisions. Jourdan L Boudin, F Hernandez, N Dufour, R , Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCL. N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCLTorino, ItaliaMay, 2024</p>
<p>Scientific paper recommendation systems: a literature review of recent publications. C K Kreutz, R Schenkel, International journal on digital libraries. 232022</p>
<p>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains. Y Labrak, A Bazoge, E Morin, P.-A Gourraud, M Rouvier, R Dufour, in L.-W</p>
<p>Findings of the Association for Computational Linguistics: ACL 2024. A Ku, V Martins, Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust, 2024</p>
<p>CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization. Z Luo, Q Xie, S Ananiadou, Proceedings of the ACM Web Conference 2023, WWW '23. the ACM Web Conference 2023, WWW '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models. B Newman, Y Lee, A Naik, P Siangliulue, R Fok, J Kim, D S Weld, J C Chang, K Lo, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember, 2024</p>
<p>M L Nguyen, Y Matsumoto, Proceedings of the Eighth International Workshop on SCIentific DOCument Analysis (SCIDOCA 2024). the Eighth International Workshop on SCIentific DOCument Analysis (SCIDOCA 2024)Hamamatsu, Shizuoka, JapanAugust, 2024</p>
<p>L Peled-Cohen, N Calderon, S Lissak, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). R Reichart, the 1st Workshop on NLP for Science (NLP4Science)Miami, FL, USAAssociation for Computational LinguisticsNovember, 2024</p>
<p>Scientific document processing: challenges for modern learning methods. Ramesh Kashyap, A Yang, Y Kan, M.-Y , Int. J. Digit. Libr. 24mar, 2023</p>
<p>Sadat Moosavi, N Gurevych, I Hou, Y Kim, G Kim, Y J Schuster, T , Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP). A Agrawal, The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)Toronto, CanadaAssociation for Computational LinguisticsJuly, 2023</p>
<p>Scienceqa: A novel resource for question answering on scholarly articles. T Saikh, T Ghosal, A Mittal, A Ekbal, P Bhattacharyya, International Journal on Digital Libraries. 2332022</p>
<p>Drug discovery companies are customizing ChatGPT: here's how. N Savage, Nat Biotechnol. 4152023</p>
<p>VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups. Z Shen, K Lo, L L Wang, B Kuehl, D S Weld, D Downey, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Acronym Identification and Disambiguation Shared Tasks for Scientific Document Understanding. A P B Veyseh, F Dernoncourt, T H Nguyen, W Chang, L A Celi, 2021</p>
<p>Scientific Document Retrieval using Multi-level Aspect-based Queries. J A Wang, K Wang, X Wang, P Naidu, L Bergen, R Paturi, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>SciMON: Scientific Inspiration Machines Optimized for Novelty. Q Wang, D Downey, H Ji, T Hope, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust, 20241</p>
<p>CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding. D Wright, I Augenstein, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, Association for Computational LinguisticsAugust, 2021</p>
<p>HoneyComb: A Flexible LLM-Based Agent System for Materials Science. H Zhang, Y Song, Z Hou, S Miret, B Liu, Findings of the Association for Computational Linguistics: EMNLP 2024. Y Al-Onaizan, M Bansal, Y.-N Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember, 2024a</p>
<p>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember, 2024b</p>
<p>OpenResearcher: Unleashing AI for Accelerated Scientific Research. Y Zheng, S Sun, L Qiu, D Ru, C Jiayang, X Li, J Lin, B Wang, Y Luo, R Pan, Y Xu, Q Min, Z Zhang, Y Wang, W Li, P Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. D I Hernandez Farias, T Hope, M Li, the 2024 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsMiami, Florida, USAAssociation for Computational LinguisticsNovember, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
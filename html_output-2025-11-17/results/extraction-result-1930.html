<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1930 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1930</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1930</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-282058662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.11027v1.pdf" target="_blank">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1930.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1930.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vlaser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation vision-language model fine-tuned for embodied reasoning and extended with a flow-matching action expert for end-to-end robot control; trained on the Vlaser-6M data engine combining large-scale grounding, spatial, planning and in-domain simulation data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser (Vision-Language-Action Model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal VLM backbone (InternVL3-derived InternViT vision encoder + Qwen2.5 LLM variants) supervised-fine-tuned on embodied datasets (Vlaser-6M) and extended with a flow-matching action expert that shares self-attention with the language model; processes RGB images, text instructions, robot state tokens and produces action chunks via denoising flow matching.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Supervised fine-tuning of a pre-trained vision-language model (InternVL3) on embodied multimodal datasets (Vlaser-6M); followed by supervised VLA fine-tuning of the action expert (flow matching) on robot episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Vlaser-6M: curated mixture including ~1.5M embodied grounding QA (bbox/point), ~1.2M RoboVQA items, ~500k spatial intelligence samples, ~400k planning items, 100k manually annotated 3D spatial samples from ScanNet/ScanNet++/ARKitScenes, 300k mask-derived bbox/point annotations from SA-1B, and 2M in-domain simulation question-answer pairs generated in SimplerEnv for WidowX and Google Robot; data contains object descriptions, spatial relations, grounding/affordance annotations, planning traces and robot-state/action context.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Embodied reasoning and closed-loop robotic manipulation (vision-language-action)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Closed-loop simulated robotic manipulation and embodied reasoning evaluated in SimplerEnv (WidowX and Google Robot setups). Action space is low-level continuous action chunks (denoised via flow-matching) with horizon H=4; single-frame visual observation + robot state input produce an action chunk for continuous control (inference uses 10 flow-integration steps). Tasks include manipulation primitives (pick/place/stack, fold, place on counter) and multi-step planning in simulated home scenes (SimplerEnv / ALFRED / Habitat evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper explicitly analyzes semantic alignment: large-scale internet/out-of-domain embodied reasoning data improves upstream reasoning but does not reliably transfer to low-level VLA; in-domain SimplerEnv data (robot viewpoint, embodiment) has higher overlap with target visual observations and action semantics and leads to better transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>On embodied reasoning benchmarks (12 tasks): Vlaser-2B average normalized score 45.3 vs InternVL3-2B 15.2; Vlaser-8B 51.3 vs InternVL3-8B 22.3 (normalized averages across listed benchmarks). For downstream closed-loop VLA: paper reports state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark when using the Vlaser data engine and in-domain fine-tuning; specific per-task success-rate numbers for Vlaser VLA are not reported numerically in the text (qualitative improvement described).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baseline InternVL3-2B/8B used as initialization: InternVL3-2B embodied reasoning avg 15.2; InternVL3-8B avg 22.3. For VLA fine-tuning, the authors report that using Vlaser-2B as backbone did not show a clear improvement over InternVL3-2B on WidowX and Google Robot unless the model was fine-tuned on in-domain (Vlaser-QA) data; no explicit numeric VLA baselines provided in the body text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper states the pre-trained Vlaser VLM "significantly accelerates convergence" for downstream VLA policy learning (e.g., faster convergence on WidowX) and that in-domain data accelerates convergence and increases success rates, but does not provide explicit numeric sample-efficiency ratios (no precise episodes-to-threshold comparisons reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-visualization or attention-pattern quantitative analysis is reported; architecture notes sharing self-attention between the language model and action expert but no attribution maps or attention heatmaps are presented in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No quantitative analysis of embedding space structure (e.g., clustering, PCA) is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — substantial evidence: Vlaser trained on 1.5M grounding QA (bbox/points) and 300k SA-1B derived bbox/point labels; paper includes examples where the model produces explicit point coordinates and bounding boxes (e.g., <point>[[701, 374]]</point>), and chain-of-thought style multimodal reasoning that links observed objects to next actions (the tomato-slice example). These demonstrate language-perception-to-action mapping capability in open-vocabulary grounding and affordance localization.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>The paper does not present layerwise feature analyses separating low-level vs high-level features; no explicit claim about which feature levels benefit most from language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Key conditions reported: high domain similarity (robot viewpoint, sensor characteristics, in-domain simulation episodes) strongly improves transfer; internet-scale or out-of-domain embodied reasoning data improves upstream metrics but may not translate to VLA success due to viewpoint/embodiment domain gap; alignment is more effective when in-domain robot interaction annotations are present.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit quantitative comparison reported between objects/actions present in pretraining vs novel ones at test time; paper does not provide numbers for novel-vs-familiar splits.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Paper does not claim zero-shot success on closed-loop VLA tasks; it reports strong generalization for open-loop inference and improved embodied reasoning, but closed-loop control benefits from in-domain fine-tuning rather than purely zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Authors note that in their SFT they kept all parameters trainable (LLM, vision projector, visual encoder) and that they systematically examined different VLM initializations for downstream VLA, but they do not report detailed per-layer ablations or freezing experiments in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — the paper documents that improvements on out-of-domain embodied reasoning benchmarks do not necessarily translate to downstream VLA success (i.e., limited or negative transfer attributable to robot-viewpoint domain shift), and that in-domain robot annotations produce the most effective transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct controlled comparison to vision-only pretraining (e.g., ImageNet-only backbones) is provided; comparisons are between different VLM initializations and different Vlaser data-stream variants.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Authors report that Vlaser pretraining accelerates convergence in downstream policy learning qualitatively and specify evaluation checkpoints used for fair comparison (WidowX checkpoint at 45,390 iterations and Google Robot at 36,970 iterations), but no learning curves or quantitative early-vs-late phase metrics are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension measurements are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1930.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL3 (open-source multimodal backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The open-source multimodal foundation model used as Vlaser’s backbone; provides the InternViT vision encoder and multimodal projector architecture that Vlaser fine-tunes for embodied reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal vision-language backbone (InternViT vision encoder + multimodal projector and LLM head) used as the initialization for Vlaser; available in multiple sizes (2B and 8B variants used).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Pretrained on large-scale multimodal internet data (original InternVL3 pretraining) and then supervised fine-tuned on embodied datasets in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>InternVL3 pretraining details are external to this paper (internet-scale image-text multimodal data); within this work it is further supervised-fine-tuned on Vlaser-6M embodied datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Embodied reasoning benchmarks and downstream VLA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on 12 embodied reasoning benchmarks and used as the VLM initialization for VLA policy learning in SimplerEnv (WidowX and Google Robot setups).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Serves as the out-of-domain baseline backbone; paper shows that naive InternVL3 initialization (internet-scale pretraining) has lower semantic alignment with robot-viewpoint embodied tasks compared to models fine-tuned with in-domain robot data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>InternVL3-2B baseline average normalized embodied reasoning score = 15.2; InternVL3-8B baseline = 22.3 (normalized averages across the 12 benchmarks reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency numbers reported for InternVL3 vs other initializations in VLA training (qualitative statements only).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>InternVL3 serves as a backbone; grounding capability improves after supervised fine-tuning on Vlaser-6M — raw InternVL3 baseline grounding scores are reported in Table 1 but no internal grounding analysis is shown.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper reports that direct InternVL3 initialization suffers from domain gap to real-robot viewpoints and in-domain simulation annotation is more effective for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No per-layer analysis for InternVL3 presented, authors fine-tune all parameters for adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Implicit: InternVL3 alone (internet-pretrained) yields much lower embodied-reasoning benchmark scores compared to Vlaser fine-tuned on embodied data, indicating potential mismatch/negative effects without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1930.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vlaser-QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser-QA (Vlaser variant fine-tuned on in-domain question-answer pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2B Vlaser variant fine-tuned specifically on in-domain question-answer pairs (Bridge/Fractal/Sim-per-env QA) to better align the VLM with the target robot observation/action domain and improve downstream VLA fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser-QA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as Vlaser (2B) but SFT-focused on in-domain QA pairs from SimplerEnv/Bridge datasets prior to VLA action-expert fine-tuning; intended to reduce viewpoint and semantic domain gap between VLM pretraining and robot-policy data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Supervised fine-tuning of VLM on in-domain simulation question-answer pairs (Bridge/Fractal-like QA and SimplerEnv generated QA).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Fine-tuned using in-domain QA from SimplerEnv (Bridge Q&A and Fractal-style Q&A) derived from WidowX/Google Robot simulation episodes; data includes robot-state queries, next-action plans and embodied grounding entries.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Downstream VLA closed-loop manipulation on WidowX and Google Robot (SimplerEnv)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Low-level continuous control tasks in simulation (WidowX/Google Robot) with action chunks predicted by flow-matching expert; tasks include manipulation primitives and compositional multi-step tasks in home-like scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High: paper reports Vlaser-QA (fine-tuned on in-domain QA) substantially reduces domain gap and yields significantly improved downstream VLA convergence and success compared to out-of-domain VLM initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Paper reports that Vlaser-QA produced a significant performance promotion on WidowX and Google Robot downstream VLA tasks relative to VLM initializations that lacked in-domain QA fine-tuning (numerical success rates are not provided in the text body).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Authors report accelerated convergence and higher success rates when using Vlaser-QA for VLA fine-tuning versus other initializations, but no quantitative episodes-to-threshold comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Vlaser-QA is explicitly trained on in-domain question-answer pairs that include grounding and planning components, providing direct paired language-to-perception/action supervision supporting language-action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>In-domain QA fine-tuning aligns sensor viewpoint and task distributions, improving transfer; authors emphasize that in-domain annotations are 'substantially more effective' for accelerating VLA fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Fine-tuning kept all parameters trainable; no per-layer ablations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1930.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vlaser-Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vlaser-Grounding (Vlaser variant fine-tuned on embodied grounding data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Vlaser 2B variant fine-tuned primarily on embodied grounding datasets (bounding boxes and point localization) to strengthen visual affordance and localization capabilities for downstream VLA training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vlaser-Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vlaser architecture fine-tuned on the Vlaser-6M grounding subset (1.5M grounding QA + 300k SA-1B derived bbox/point samples) to improve open-vocabulary localization and affordance prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Supervised fine-tuning of the VLM backbone on large-scale grounding data (bbox/point QA).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Grounding data sources: RoboPoint, ShareRobot, Pixmo-Points, Paco-LaVIS, RefSpatial and 300k high-precision masks->bbox/point annotations derived from SA-1B; data includes object descriptions and localization targets (bbox/center-point) normalized to a resolution-invariant scale.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Embodied grounding and downstream VLA manipulation (pointing/localized affordance conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks requiring pinpoint localization of actionable affordances (point/bbox outputs) used as inputs or auxiliary supervision for robotic manipulation policies in SimplerEnv and other embodied benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment for localization tasks because grounding data is explicitly constructed to match actionable affordances; however paper reports mixed downstream impact — grounding-only pretraining helps perceptual alignment but best VLA transfer occurs when combined with in-domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Vlaser variants incorporating grounding data show clear improvement on embodied grounding benchmarks (table-level results reported qualitatively); precise numeric downstream VLA success-rates for Vlaser-Grounding alone are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Direct: large grounding dataset and examples of point predictions (e.g., Where2Place/Pointarena outputs) constitute evidence that language-grounded spatial coordinates are learned and produced by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Grounding data improves perception-to-action alignment but is most effective when combined with in-domain robot-viewpoint simulation data to reduce domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1930.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Expert (flow-matching)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flow-matching Action Expert (VLA action module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flow-matching based action expert module that predicts short action-chunks conditioned on image observation, language instruction, and robot state by learning a denoising vector field; integrated with the VLM via shared self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flow-matching Action Expert (Vlaser VLA module)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encodes robot state as a state token and noisy actions as action tokens; trained to predict denoising vector fields (flow matching) that map noisy action chunks to true action chunks; uses non-causal attention for VLA stream and shares attention weights with the language model.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Supervised flow-matching training on robot episode trajectories sampled from SimplerEnv and Bridge/Google/WidowX episodes (robot-specific datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Robot episodes (SimplerEnv) with action chunks sampled from trajectories; authors reference over 5M images of SimplerEnv episodes available for training; data includes sequences of actions, robot state, observations and success signals.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Low-level continuous robot control (VLA policy execution)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Predicts action chunks A_t = [a_t, a_{t+1}, ..., a_{t+H-1}] (H=4 in experiments); continuous action space; inference integrates learned vector field from tau=0 to 1 starting from random noise, using Euler integration with 10 steps; evaluated in SimplerEnv on WidowX and Google Robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Action expert is trained on in-domain robot episodes so semantic alignment between visual observations, robot state and low-level actions is high when in-domain data is used; misalignment arises when upstream VLM data is out-of-domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When paired with VLMs fine-tuned on in-domain data (e.g., Vlaser-QA) the action expert yields significant downstream performance improvements; the paper reports qualitative and benchmark-level success but does not list per-task numeric success rates for the action expert alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Using the action expert with baseline InternVL3 backbone achieves comparable performance to previous approaches at the 2B backbone scale, but specific numeric comparisons are not provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency numbers, but the paper notes that in-domain VLM initialization (Vlaser-QA) accelerates convergence of the action-expert fine-tuning compared to out-of-domain initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Shares self-attention between LLM and action expert (architectural note); no detailed analyses of attention specific to the action expert are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Action expert is explicitly trained conditioned on grounding/spatial/planning annotations and robot state; the training pipeline retains executed action trajectories and environment feedback, linking language instructions and observations to motor outputs — presented as evidence for grounding of action semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when the VLM backbone and action expert are trained/fine-tuned with in-domain robot-viewpoint data; out-of-domain embodied reasoning improvements do not reliably transfer to low-level action prediction without in-domain alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Flow-matching inference uses integration over tau with 10 steps (numerical integration); no learning-dynamics plots presented.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1930.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimplerEnv</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimplerEnv (simulation benchmark and data engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source simulated suite used for training and evaluating robot manipulation policies with strong reported real-vs-sim correlation; used both to generate 2M in-domain VLM QA pairs and to benchmark VLA models on WidowX and Google Robot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating real-world robot manipulation policies in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimplerEnv (benchmark / simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Simulation environment suite providing nearly 150K episodes designed to reduce real-vs-sim gaps (control and vision) and to faithfully reflect real-robot outcomes for tasks across Google Robot and WidowX setups; used to generate in-domain training/QA data and to evaluate closed-loop policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>SimplerEnv provides recorded robot episodes (over millions of images available across episodes) and is the source of 2M in-domain multimodal QA pairs for VLM pretraining described in the paper; datasets include WidowX and Google Robot morphologies/kinematics.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Simulated closed-loop robot manipulation and embodied reasoning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Simulated manipulation episodes corresponding to real robot embodiments (WidowX and Google Robot) covering pick/place/stack and other manipulation primitives and multi-step tasks; used for both data generation (pretraining) and evaluation of VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to provide high semantic alignment with real-robot embodied tasks (captures robot viewpoint and control modalities), thereby reducing domain gap compared to internet image-text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Used as evaluation bed: Vlaser variants fine-tuned with SimplerEnv in-domain data show improved convergence and higher closed-loop success; paper reports state-of-the-art on WidowX and competitive Google Robot results when using SimplerEnv-sourced in-domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>SimplerEnv-generated QA pairs explicitly include grounding, spatial reasoning and next-action planning labels used to tie perception to action in VLM pretraining and action-expert training.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Acts as in-domain source that improves transfer; authors emphasize that models trained with SimplerEnv in-domain data reduce vision observation domain shift and thus improve VLA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1930.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1 / RT-1-X (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1 / RT-1-X (Robotics Transformer models used as baselines in the field)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Existing vision-language-action transformer-style models cited as baselines and comparators for generalist robotic control that transfer web-scale knowledge to robotic control; referenced in the paper's baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1 (and RT-1-X variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Published vision-language-action transformers for robot control (cited as prior work/baseline); used in literature as examples of web-data transfer to robot policies — cited by this paper for comparison but not re-implemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (prior work benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1930.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1930.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pi_0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pi_0: A vision-language-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flow-based VLA model referenced in the paper; cited as related work that uses flow/diffusion style generative modeling for action sequence prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pi_{0}: A vision-language-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pi_0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work employing flow-based generative modeling for vision-language-action control; cited as related work/baseline (the Vlaser action expert design references flow-matching and prior flow-based VLA work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>General robot control (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-1: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>pi_{0}: A vision-language-action flow model for general robot control <em>(Rating: 2)</em></li>
                <li>Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models <em>(Rating: 2)</em></li>
                <li>Embodied-R1: Reinforced embodied reasoning for general robotic manipulation <em>(Rating: 2)</em></li>
                <li>RoboBrain: A unified brain model for robotic manipulation from abstract to concrete <em>(Rating: 2)</em></li>
                <li>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1930",
    "paper_id": "paper-282058662",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "Vlaser",
            "name_full": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
            "brief_description": "A foundation vision-language model fine-tuned for embodied reasoning and extended with a flow-matching action expert for end-to-end robot control; trained on the Vlaser-6M data engine combining large-scale grounding, spatial, planning and in-domain simulation data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser (Vision-Language-Action Model)",
            "model_description": "A multimodal VLM backbone (InternVL3-derived InternViT vision encoder + Qwen2.5 LLM variants) supervised-fine-tuned on embodied datasets (Vlaser-6M) and extended with a flow-matching action expert that shares self-attention with the language model; processes RGB images, text instructions, robot state tokens and produces action chunks via denoising flow matching.",
            "pretraining_type": "Supervised fine-tuning of a pre-trained vision-language model (InternVL3) on embodied multimodal datasets (Vlaser-6M); followed by supervised VLA fine-tuning of the action expert (flow matching) on robot episodes.",
            "pretraining_data_description": "Vlaser-6M: curated mixture including ~1.5M embodied grounding QA (bbox/point), ~1.2M RoboVQA items, ~500k spatial intelligence samples, ~400k planning items, 100k manually annotated 3D spatial samples from ScanNet/ScanNet++/ARKitScenes, 300k mask-derived bbox/point annotations from SA-1B, and 2M in-domain simulation question-answer pairs generated in SimplerEnv for WidowX and Google Robot; data contains object descriptions, spatial relations, grounding/affordance annotations, planning traces and robot-state/action context.",
            "target_task_name": "Embodied reasoning and closed-loop robotic manipulation (vision-language-action)",
            "target_task_description": "Closed-loop simulated robotic manipulation and embodied reasoning evaluated in SimplerEnv (WidowX and Google Robot setups). Action space is low-level continuous action chunks (denoised via flow-matching) with horizon H=4; single-frame visual observation + robot state input produce an action chunk for continuous control (inference uses 10 flow-integration steps). Tasks include manipulation primitives (pick/place/stack, fold, place on counter) and multi-step planning in simulated home scenes (SimplerEnv / ALFRED / Habitat evaluations).",
            "semantic_alignment": "Paper explicitly analyzes semantic alignment: large-scale internet/out-of-domain embodied reasoning data improves upstream reasoning but does not reliably transfer to low-level VLA; in-domain SimplerEnv data (robot viewpoint, embodiment) has higher overlap with target visual observations and action semantics and leads to better transfer.",
            "performance_with_language_pretraining": "On embodied reasoning benchmarks (12 tasks): Vlaser-2B average normalized score 45.3 vs InternVL3-2B 15.2; Vlaser-8B 51.3 vs InternVL3-8B 22.3 (normalized averages across listed benchmarks). For downstream closed-loop VLA: paper reports state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark when using the Vlaser data engine and in-domain fine-tuning; specific per-task success-rate numbers for Vlaser VLA are not reported numerically in the text (qualitative improvement described).",
            "performance_without_language_pretraining": "Baseline InternVL3-2B/8B used as initialization: InternVL3-2B embodied reasoning avg 15.2; InternVL3-8B avg 22.3. For VLA fine-tuning, the authors report that using Vlaser-2B as backbone did not show a clear improvement over InternVL3-2B on WidowX and Google Robot unless the model was fine-tuned on in-domain (Vlaser-QA) data; no explicit numeric VLA baselines provided in the body text.",
            "sample_efficiency_comparison": "Paper states the pre-trained Vlaser VLM \"significantly accelerates convergence\" for downstream VLA policy learning (e.g., faster convergence on WidowX) and that in-domain data accelerates convergence and increases success rates, but does not provide explicit numeric sample-efficiency ratios (no precise episodes-to-threshold comparisons reported).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention-visualization or attention-pattern quantitative analysis is reported; architecture notes sharing self-attention between the language model and action expert but no attribution maps or attention heatmaps are presented in the paper.",
            "embedding_space_analysis": "No quantitative analysis of embedding space structure (e.g., clustering, PCA) is reported in the paper.",
            "action_grounding_evidence": "Yes — substantial evidence: Vlaser trained on 1.5M grounding QA (bbox/points) and 300k SA-1B derived bbox/point labels; paper includes examples where the model produces explicit point coordinates and bounding boxes (e.g., &lt;point&gt;[[701, 374]]&lt;/point&gt;), and chain-of-thought style multimodal reasoning that links observed objects to next actions (the tomato-slice example). These demonstrate language-perception-to-action mapping capability in open-vocabulary grounding and affordance localization.",
            "hierarchical_features_evidence": "The paper does not present layerwise feature analyses separating low-level vs high-level features; no explicit claim about which feature levels benefit most from language pretraining.",
            "transfer_conditions": "Key conditions reported: high domain similarity (robot viewpoint, sensor characteristics, in-domain simulation episodes) strongly improves transfer; internet-scale or out-of-domain embodied reasoning data improves upstream metrics but may not translate to VLA success due to viewpoint/embodiment domain gap; alignment is more effective when in-domain robot interaction annotations are present.",
            "novel_vs_familiar_objects": "No explicit quantitative comparison reported between objects/actions present in pretraining vs novel ones at test time; paper does not provide numbers for novel-vs-familiar splits.",
            "zero_shot_or_few_shot": "Paper does not claim zero-shot success on closed-loop VLA tasks; it reports strong generalization for open-loop inference and improved embodied reasoning, but closed-loop control benefits from in-domain fine-tuning rather than purely zero-shot transfer.",
            "layer_analysis": "Authors note that in their SFT they kept all parameters trainable (LLM, vision projector, visual encoder) and that they systematically examined different VLM initializations for downstream VLA, but they do not report detailed per-layer ablations or freezing experiments in the main text.",
            "negative_transfer_evidence": "Yes — the paper documents that improvements on out-of-domain embodied reasoning benchmarks do not necessarily translate to downstream VLA success (i.e., limited or negative transfer attributable to robot-viewpoint domain shift), and that in-domain robot annotations produce the most effective transfer.",
            "comparison_to_vision_only": "No direct controlled comparison to vision-only pretraining (e.g., ImageNet-only backbones) is provided; comparisons are between different VLM initializations and different Vlaser data-stream variants.",
            "temporal_dynamics": "Authors report that Vlaser pretraining accelerates convergence in downstream policy learning qualitatively and specify evaluation checkpoints used for fair comparison (WidowX checkpoint at 45,390 iterations and Google Robot at 36,970 iterations), but no learning curves or quantitative early-vs-late phase metrics are reported in the paper.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension measurements are reported.",
            "uuid": "e1930.0"
        },
        {
            "name_short": "InternVL3",
            "name_full": "InternVL3 (open-source multimodal backbone)",
            "brief_description": "The open-source multimodal foundation model used as Vlaser’s backbone; provides the InternViT vision encoder and multimodal projector architecture that Vlaser fine-tunes for embodied reasoning.",
            "citation_title": "Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models",
            "mention_or_use": "use",
            "model_name": "InternVL3",
            "model_description": "Multimodal vision-language backbone (InternViT vision encoder + multimodal projector and LLM head) used as the initialization for Vlaser; available in multiple sizes (2B and 8B variants used).",
            "pretraining_type": "Pretrained on large-scale multimodal internet data (original InternVL3 pretraining) and then supervised fine-tuned on embodied datasets in this work.",
            "pretraining_data_description": "InternVL3 pretraining details are external to this paper (internet-scale image-text multimodal data); within this work it is further supervised-fine-tuned on Vlaser-6M embodied datasets.",
            "target_task_name": "Embodied reasoning benchmarks and downstream VLA fine-tuning",
            "target_task_description": "Evaluated on 12 embodied reasoning benchmarks and used as the VLM initialization for VLA policy learning in SimplerEnv (WidowX and Google Robot setups).",
            "semantic_alignment": "Serves as the out-of-domain baseline backbone; paper shows that naive InternVL3 initialization (internet-scale pretraining) has lower semantic alignment with robot-viewpoint embodied tasks compared to models fine-tuned with in-domain robot data.",
            "performance_with_language_pretraining": "InternVL3-2B baseline average normalized embodied reasoning score = 15.2; InternVL3-8B baseline = 22.3 (normalized averages across the 12 benchmarks reported).",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "No explicit sample-efficiency numbers reported for InternVL3 vs other initializations in VLA training (qualitative statements only).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "InternVL3 serves as a backbone; grounding capability improves after supervised fine-tuning on Vlaser-6M — raw InternVL3 baseline grounding scores are reported in Table 1 but no internal grounding analysis is shown.",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "Paper reports that direct InternVL3 initialization suffers from domain gap to real-robot viewpoints and in-domain simulation annotation is more effective for transfer.",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "No per-layer analysis for InternVL3 presented, authors fine-tune all parameters for adaptation.",
            "negative_transfer_evidence": "Implicit: InternVL3 alone (internet-pretrained) yields much lower embodied-reasoning benchmark scores compared to Vlaser fine-tuned on embodied data, indicating potential mismatch/negative effects without adaptation.",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1930.1"
        },
        {
            "name_short": "Vlaser-QA",
            "name_full": "Vlaser-QA (Vlaser variant fine-tuned on in-domain question-answer pairs)",
            "brief_description": "A 2B Vlaser variant fine-tuned specifically on in-domain question-answer pairs (Bridge/Fractal/Sim-per-env QA) to better align the VLM with the target robot observation/action domain and improve downstream VLA fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser-QA",
            "model_description": "Same architecture as Vlaser (2B) but SFT-focused on in-domain QA pairs from SimplerEnv/Bridge datasets prior to VLA action-expert fine-tuning; intended to reduce viewpoint and semantic domain gap between VLM pretraining and robot-policy data.",
            "pretraining_type": "Supervised fine-tuning of VLM on in-domain simulation question-answer pairs (Bridge/Fractal-like QA and SimplerEnv generated QA).",
            "pretraining_data_description": "Fine-tuned using in-domain QA from SimplerEnv (Bridge Q&A and Fractal-style Q&A) derived from WidowX/Google Robot simulation episodes; data includes robot-state queries, next-action plans and embodied grounding entries.",
            "target_task_name": "Downstream VLA closed-loop manipulation on WidowX and Google Robot (SimplerEnv)",
            "target_task_description": "Low-level continuous control tasks in simulation (WidowX/Google Robot) with action chunks predicted by flow-matching expert; tasks include manipulation primitives and compositional multi-step tasks in home-like scenes.",
            "semantic_alignment": "High: paper reports Vlaser-QA (fine-tuned on in-domain QA) substantially reduces domain gap and yields significantly improved downstream VLA convergence and success compared to out-of-domain VLM initializations.",
            "performance_with_language_pretraining": "Paper reports that Vlaser-QA produced a significant performance promotion on WidowX and Google Robot downstream VLA tasks relative to VLM initializations that lacked in-domain QA fine-tuning (numerical success rates are not provided in the text body).",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Authors report accelerated convergence and higher success rates when using Vlaser-QA for VLA fine-tuning versus other initializations, but no quantitative episodes-to-threshold comparisons are provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "Vlaser-QA is explicitly trained on in-domain question-answer pairs that include grounding and planning components, providing direct paired language-to-perception/action supervision supporting language-action grounding.",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "In-domain QA fine-tuning aligns sensor viewpoint and task distributions, improving transfer; authors emphasize that in-domain annotations are 'substantially more effective' for accelerating VLA fine-tuning.",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "Fine-tuning kept all parameters trainable; no per-layer ablations reported.",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1930.2"
        },
        {
            "name_short": "Vlaser-Grounding",
            "name_full": "Vlaser-Grounding (Vlaser variant fine-tuned on embodied grounding data)",
            "brief_description": "A Vlaser 2B variant fine-tuned primarily on embodied grounding datasets (bounding boxes and point localization) to strengthen visual affordance and localization capabilities for downstream VLA training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vlaser-Grounding",
            "model_description": "Vlaser architecture fine-tuned on the Vlaser-6M grounding subset (1.5M grounding QA + 300k SA-1B derived bbox/point samples) to improve open-vocabulary localization and affordance prediction.",
            "pretraining_type": "Supervised fine-tuning of the VLM backbone on large-scale grounding data (bbox/point QA).",
            "pretraining_data_description": "Grounding data sources: RoboPoint, ShareRobot, Pixmo-Points, Paco-LaVIS, RefSpatial and 300k high-precision masks-&gt;bbox/point annotations derived from SA-1B; data includes object descriptions and localization targets (bbox/center-point) normalized to a resolution-invariant scale.",
            "target_task_name": "Embodied grounding and downstream VLA manipulation (pointing/localized affordance conditioning)",
            "target_task_description": "Tasks requiring pinpoint localization of actionable affordances (point/bbox outputs) used as inputs or auxiliary supervision for robotic manipulation policies in SimplerEnv and other embodied benchmarks.",
            "semantic_alignment": "High alignment for localization tasks because grounding data is explicitly constructed to match actionable affordances; however paper reports mixed downstream impact — grounding-only pretraining helps perceptual alignment but best VLA transfer occurs when combined with in-domain data.",
            "performance_with_language_pretraining": "Vlaser variants incorporating grounding data show clear improvement on embodied grounding benchmarks (table-level results reported qualitatively); precise numeric downstream VLA success-rates for Vlaser-Grounding alone are not provided in the text.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "Direct: large grounding dataset and examples of point predictions (e.g., Where2Place/Pointarena outputs) constitute evidence that language-grounded spatial coordinates are learned and produced by the model.",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "Grounding data improves perception-to-action alignment but is most effective when combined with in-domain robot-viewpoint simulation data to reduce domain gap.",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1930.3"
        },
        {
            "name_short": "Action Expert (flow-matching)",
            "name_full": "Flow-matching Action Expert (VLA action module)",
            "brief_description": "A flow-matching based action expert module that predicts short action-chunks conditioned on image observation, language instruction, and robot state by learning a denoising vector field; integrated with the VLM via shared self-attention.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flow-matching Action Expert (Vlaser VLA module)",
            "model_description": "Encodes robot state as a state token and noisy actions as action tokens; trained to predict denoising vector fields (flow matching) that map noisy action chunks to true action chunks; uses non-causal attention for VLA stream and shares attention weights with the language model.",
            "pretraining_type": "Supervised flow-matching training on robot episode trajectories sampled from SimplerEnv and Bridge/Google/WidowX episodes (robot-specific datasets).",
            "pretraining_data_description": "Robot episodes (SimplerEnv) with action chunks sampled from trajectories; authors reference over 5M images of SimplerEnv episodes available for training; data includes sequences of actions, robot state, observations and success signals.",
            "target_task_name": "Low-level continuous robot control (VLA policy execution)",
            "target_task_description": "Predicts action chunks A_t = [a_t, a_{t+1}, ..., a_{t+H-1}] (H=4 in experiments); continuous action space; inference integrates learned vector field from tau=0 to 1 starting from random noise, using Euler integration with 10 steps; evaluated in SimplerEnv on WidowX and Google Robot tasks.",
            "semantic_alignment": "Action expert is trained on in-domain robot episodes so semantic alignment between visual observations, robot state and low-level actions is high when in-domain data is used; misalignment arises when upstream VLM data is out-of-domain.",
            "performance_with_language_pretraining": "When paired with VLMs fine-tuned on in-domain data (e.g., Vlaser-QA) the action expert yields significant downstream performance improvements; the paper reports qualitative and benchmark-level success but does not list per-task numeric success rates for the action expert alone.",
            "performance_without_language_pretraining": "Using the action expert with baseline InternVL3 backbone achieves comparable performance to previous approaches at the 2B backbone scale, but specific numeric comparisons are not provided in the main text.",
            "sample_efficiency_comparison": "No explicit sample-efficiency numbers, but the paper notes that in-domain VLM initialization (Vlaser-QA) accelerates convergence of the action-expert fine-tuning compared to out-of-domain initializations.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Shares self-attention between LLM and action expert (architectural note); no detailed analyses of attention specific to the action expert are reported.",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "Action expert is explicitly trained conditioned on grounding/spatial/planning annotations and robot state; the training pipeline retains executed action trajectories and environment feedback, linking language instructions and observations to motor outputs — presented as evidence for grounding of action semantics.",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "Works best when the VLM backbone and action expert are trained/fine-tuned with in-domain robot-viewpoint data; out-of-domain embodied reasoning improvements do not reliably transfer to low-level action prediction without in-domain alignment.",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "Flow-matching inference uses integration over tau with 10 steps (numerical integration); no learning-dynamics plots presented.",
            "dimensionality_analysis": "",
            "uuid": "e1930.4"
        },
        {
            "name_short": "SimplerEnv",
            "name_full": "SimplerEnv (simulation benchmark and data engine)",
            "brief_description": "Open-source simulated suite used for training and evaluating robot manipulation policies with strong reported real-vs-sim correlation; used both to generate 2M in-domain VLM QA pairs and to benchmark VLA models on WidowX and Google Robot tasks.",
            "citation_title": "Evaluating real-world robot manipulation policies in simulation",
            "mention_or_use": "use",
            "model_name": "SimplerEnv (benchmark / simulator)",
            "model_description": "Simulation environment suite providing nearly 150K episodes designed to reduce real-vs-sim gaps (control and vision) and to faithfully reflect real-robot outcomes for tasks across Google Robot and WidowX setups; used to generate in-domain training/QA data and to evaluate closed-loop policies.",
            "pretraining_type": "",
            "pretraining_data_description": "SimplerEnv provides recorded robot episodes (over millions of images available across episodes) and is the source of 2M in-domain multimodal QA pairs for VLM pretraining described in the paper; datasets include WidowX and Google Robot morphologies/kinematics.",
            "target_task_name": "Simulated closed-loop robot manipulation and embodied reasoning evaluation",
            "target_task_description": "Simulated manipulation episodes corresponding to real robot embodiments (WidowX and Google Robot) covering pick/place/stack and other manipulation primitives and multi-step tasks; used for both data generation (pretraining) and evaluation of VLA models.",
            "semantic_alignment": "Designed to provide high semantic alignment with real-robot embodied tasks (captures robot viewpoint and control modalities), thereby reducing domain gap compared to internet image-text corpora.",
            "performance_with_language_pretraining": "Used as evaluation bed: Vlaser variants fine-tuned with SimplerEnv in-domain data show improved convergence and higher closed-loop success; paper reports state-of-the-art on WidowX and competitive Google Robot results when using SimplerEnv-sourced in-domain data.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "SimplerEnv-generated QA pairs explicitly include grounding, spatial reasoning and next-action planning labels used to tie perception to action in VLM pretraining and action-expert training.",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "Acts as in-domain source that improves transfer; authors emphasize that models trained with SimplerEnv in-domain data reduce vision observation domain shift and thus improve VLA performance.",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1930.5"
        },
        {
            "name_short": "RT-1 / RT-1-X (baselines)",
            "name_full": "RT-1 / RT-1-X (Robotics Transformer models used as baselines in the field)",
            "brief_description": "Existing vision-language-action transformer-style models cited as baselines and comparators for generalist robotic control that transfer web-scale knowledge to robotic control; referenced in the paper's baseline comparisons.",
            "citation_title": "Rt-1: Vision-language-action models transfer web knowledge to robotic control",
            "mention_or_use": "mention",
            "model_name": "RT-1 (and RT-1-X variant)",
            "model_description": "Published vision-language-action transformers for robot control (cited as prior work/baseline); used in literature as examples of web-data transfer to robot policies — cited by this paper for comparison but not re-implemented here.",
            "pretraining_type": "",
            "pretraining_data_description": "",
            "target_task_name": "Robotic manipulation (prior work benchmarks)",
            "target_task_description": "",
            "semantic_alignment": "",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1930.6"
        },
        {
            "name_short": "pi_0",
            "name_full": "pi_0: A vision-language-action flow model for general robot control",
            "brief_description": "A flow-based VLA model referenced in the paper; cited as related work that uses flow/diffusion style generative modeling for action sequence prediction.",
            "citation_title": "pi_{0}: A vision-language-action flow model for general robot control",
            "mention_or_use": "mention",
            "model_name": "pi_0",
            "model_description": "Prior work employing flow-based generative modeling for vision-language-action control; cited as related work/baseline (the Vlaser action expert design references flow-matching and prior flow-based VLA work).",
            "pretraining_type": "",
            "pretraining_data_description": "",
            "target_task_name": "General robot control (prior work)",
            "target_task_description": "",
            "semantic_alignment": "",
            "performance_with_language_pretraining": "",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "",
            "has_sample_efficiency_data": false,
            "attention_analysis": "",
            "embedding_space_analysis": "",
            "action_grounding_evidence": "",
            "hierarchical_features_evidence": "",
            "transfer_conditions": "",
            "novel_vs_familiar_objects": "",
            "zero_shot_or_few_shot": "",
            "layer_analysis": "",
            "negative_transfer_evidence": "",
            "comparison_to_vision_only": "",
            "temporal_dynamics": "",
            "dimensionality_analysis": "",
            "uuid": "e1930.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-1: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "pi_{0}: A vision-language-action flow model for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models",
            "rating": 2
        },
        {
            "paper_title": "Embodied-R1: Reinforced embodied reasoning for general robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "RoboBrain: A unified brain model for robotic manipulation from abstract to concrete",
            "rating": 2
        },
        {
            "paper_title": "Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration",
            "rating": 2
        }
    ],
    "cost": 0.024322749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning VLASER: VISION-LANGUAGE-ACTION MODEL WITH SYNERGISTIC EMBODIED REASONING
13 Oct 2025</p>
<p>Ganlin Yang 
University of Science and Technology
China</p>
<p>Shanghai AI Laboratory</p>
<p>Tianyi Zhang 
Shanghai AI Laboratory</p>
<p>Zhejiang University</p>
<p>Haoran Hao 
Shanghai AI Laboratory</p>
<p>Nanjing University</p>
<p>Weiyun Wang 
Shanghai AI Laboratory</p>
<p>Fudan University
7 Tsinghua University 8 NUS</p>
<p>Yibin Liu 
Shanghai Jiao Tong University</p>
<p>Northeastern University</p>
<p>Dehui Wang 
Shanghai Jiao Tong University</p>
<p>Guanzhou Chen 
Shanghai AI Laboratory</p>
<p>Shanghai Jiao Tong University</p>
<p>Zijian Cai 
Shanghai Jiao Tong University</p>
<p>Shenzhen University Project</p>
<p>Junting Chen 
Shanghai AI Laboratory</p>
<p>Weijie Su 
Shanghai AI Laboratory</p>
<p>Wengang Zhou 
University of Science and Technology
China</p>
<p>Shanghai AI Laboratory</p>
<p>Yu Qiao 
Shanghai AI Laboratory</p>
<p>Jifeng Dai 
Shanghai AI Laboratory</p>
<p>Jiangmiao Pang 
Shanghai AI Laboratory</p>
<p>Gen Luo 
Shanghai AI Laboratory</p>
<p>Wenhai Wang 
Shanghai AI Laboratory</p>
<p>Yao Mu 
Shanghai AI Laboratory</p>
<p>Shanghai Jiao Tong University</p>
<p>Zhi Hou 
Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning VLASER: VISION-LANGUAGE-ACTION MODEL WITH SYNERGISTIC EMBODIED REASONING
13 Oct 20259D8125D657A848A63DDC7F5C66932B0AarXiv:2510.11027v1[cs.CV]
While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning.In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser -a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents.Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks-including spatial reasoning, embodied grounding, embodied QA, and task planning.Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data.Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.The code, model and data are available at https://github.com/OpenGVLab/Vlaser/.</p>
<p>INTRODUCTION</p>
<p>Embodied artificial intelligence (AI) (Chrisley, 2003) aims to endow agents with the ability to perceive, understand, and act in the physical world.Achieving such intelligence requires not only accurate perception and language understanding but also embodied reasoning and effective control, which together define the paradigm of vision-language-action (VLA) models.Developing foundation models that possess strong reasoning and control capabilities is therefore an important advancement toward general-purpose embodied AI.</p>
<p>In this context, vision-language models (VLMs) (OpenAI, 2023;Liu et al., 2023;Chen et al., 2024;Bai et al., 2025;Team et al., 2023) emerge as natural candidates to enhance embodied agents in perception generalization and reasoning ability.Following this paradigm, extensive embodied visionlanguage models (Azzolini et al., 2025;Team et al., 2025c) emerge from enhancing the key ability for an embodied agent in grounding, planning, and spatial reasoning.Meanwhile, a significant body of work extends vision-language models (VLMs) into vision-language-action models (VLAs) (Kim et al., 2024;Intelligence et al., 2025;Driess et al., 2025) for robot control.While there are some approaches (Intelligence et al., 2025;Driess et al., 2025) that demonstrate the effectiveness of cotraining with web data for the generalization in robot manipulation, it remains poorly understood which multi-modal data streams/abilities are most critical for improving downstream VLA models.In this paper, we aim to construct Vlaser, an embodied vision-language model that possesses strong The pre-trained Vlaser VLM significantly accelerates convergence in downstream Vision-Language Action model (VLA) policy learning on WidowX platform (Walke et al., 2023a).Bottom-right: Successful closed-loop operation of an agent powered by Vlaser within the SimplerEnv benchmark (Li et al., 2024b).</p>
<p>embodied reasoning capabilities, and subsequently answer this question based on the corresponding vision-language-action models.</p>
<p>Despite advancements in vision-language models (Chen et al., 2024;Bai et al., 2025), the capabilities of operating as an embodied agent remain severely constrained.In particular, navigation and traditional manipulation approaches rely heavily on planning-based control (Huang et al., 2022;Gasparetto et al., 2015;Zhang et al., 2018), which requires a strong foundational ability in grounding and planning.Planning and Grounding are cornerstones of the agents embodied in the physical world.Meanwhile, spatial understanding increasingly attracts the interest of the community in addressing the spatial perception ability of VLM.To this end, we firstly aim to introduce an embodied visionlanguage model specifically enhanced for the aboved embodied reasoning capabilities.Specifically, we construct the Vlaser data engine, which enables the systematic construction of the Vlaser-6M dataset by curating, reorganizing, and annotating public datasets from the Internet.As illustrated in Figure 1, the resulting dataset spans a wide spectrum of embodied reasoning tasks-including general embodied QA, visual grounding, spatial intelligence, task planning, and in-domain simulation data.</p>
<p>Leveraging this comprehensive data foundation, Vlaser achieves state-of-the-art performance across a variety of embodied reasoning benchmarks, demonstrating strong generalization in both open-loop inference and closed-loop control settings.</p>
<p>Existing Vision-Language-Action (VLA) models (Black et al., 2024;Cheng et al., 2024;Kim et al., 2024;Intelligence et al., 2025) typically fine-tune pre-trained Vision-Language Models (VLMs) for robot control.However, the selection of an optimal VLM backbone -one that accelerates convergence and improves success rates when used as initialization for end-to-end VLA policy learning, remains an under-explored research problem.To address this gap, we systematically investigate the VLM-to-VLA adaptation paradigm using our enhanced embodied vision-language model and associated data engine.Our experiments reveal an important insight: although out-of-domain embodied reasoning data significantly improve upstream reasoning capabilities as measured by standard benchmarks, these gains may not translate directly or prominently to downstream VLA performance.In contrast, indomain data -annotated directly on robot interaction datasets such as Open X-Embodiment (O 'Neill et al., 2024) proves substantially more effective in accelerating convergence and increasing task success rates during VLA fine-tuning.We believe this observation provides significant insights for future embodied vision-language model construction: It is urgent to shrink the domain gap between current embodied perception and reasoning benchmarks to the real-world embodied tasks, and thus facilitate the closed-loop evaluation for the corresponding robot embodiment.</p>
<p>In summary, the principal contributions of Vlaser are as follows.</p>
<p>An open-source embodied vision-language model and dataset.We introduce Vlaser, an adaptable vision-language model that enhances InternVL with embodied reasoning capabilities and end-to-end robot control.The full model weights, modular data generation pipeline, training and evaluation code, and the accompanying Vlaser-6M dataset will be made publicly available to support reproducibility and future research.</p>
<p>Systematic analysis of data effectiveness for VLA transfer.We conduct a thorough investigation into which types of vision-language pretraining data contribute most effectively to downstream Vision-Language-Action (VLA) policy learning.Our findings offer practical insights for constructing task-aware data streams that bridge the gap between Internet-scale pretraining and embodied-specific fine-tuning.</p>
<p>State-of-the-art performance across embodied benchmarks.Among models of comparable scale, Vlaser achieves top-tier results on a comprehensive set of embodied reasoning benchmarks-spanning visual grounding, task planning, spatial reasoning, and simulation-based robot evaluation, demonstrating its strong generalization and applicability to both open-loop inference and closed-loop control scenarios.</p>
<p>METHOD</p>
<p>Vlaser aims to integrate embodied reasoning with end-to-end robot control for embodied agents, and identify the most crucial VLM data stream for VLA models.We first present the Vlaser structures in Section 2.1.Then, we illustrate the data engine in Section 2.2.Section 2.3 discusses the training recipe that includes embodied reasoning pretraining and vision-language-action finetuning.</p>
<p>MODEL STRUCTURE</p>
<p>The structure of Vlaser consists of two major components: the typical vision-language backbone (Chen et al., 2024;Liu et al., 2023) and the action expert for low-level control, as shown in Figure 2. We illustrate the two components respectively in this section.</p>
<p>VLM Backbone Vision-language models (VLMs) are key candidates for embodied agents, providing both perception and reasoning abilities.Vlaser , built on InternVL3 (Zhu et al., 2025), integrates embodied reasoning with robot control for embodied agents.While InternVL3 excels in multimodal and linguistic tasks across various model sizes, Vlaser focuses on two sizes-2B and 8B-optimized for the computational constraints of robots.These models utilize InternViT (Chen et al., 2024) as the vision encoder, paired with Qwen2.5-1.5B and Qwen2.5-7BLLMs (Qwen et al., 2025).Unlike typical multimodal MLLMs, Vlaser emphasizes embodied common-sense reasoning and end-to-end robot control capabilities.</p>
<p>Action Expert There are a large number of MLLMs (Team et al., 2025a;NVIDIA et al., 2025a) that enhance the ability of embodied common-sense reasoning for agents, while a few approaches equip the embodied MLLMs with end-to-end robot control.Vlaser extends the MLLMs with a low-level robot control and verifies the capability of different data streams in downstream VLA finetuning.Following (Intelligence et al., 2025), we design an action expert based on the opensource vision-language model (Chen et al., 2024;Zhu et al., 2025).Meanwhile, we utilize the flow matching (Lipman et al., 2023a) for action prediction based on the llava-like vision-language structure, while sharing the self-attention among the language model and action expert module.Specifically, we encode the robot state as a state token and noised actions as action tokens, and input them into the action expert.Meanwhile, we utilize non-causal attention for the VLA stream.During inference, we denoise the actions based on the image observation, language instruction, as well as the current robot state.</p>
<p>VLASER DATA ENGINE</p>
<p>This section outlines the composition of the Vlaser-6M data engine, a cornerstone for the model's embodied reasoning capabilities.Here we present the overall data scale and sources for each reasoning modality, while more details about the construction methodologies are provided in Appendix A.2.</p>
<p>Embodied Grounding Data The Vlaser dataset incorporates two distinct 2D grounding formats-bounding boxes and center points-both normalized to the range [0, 1000] to ensure consistent and resolution-invariant grounding predictions across diverse image resolutions.Specifically, we collect 1.5 million high-quality question-answer pairs that support multiple grounding tasks: predicting bounding boxes from open-vocabulary descriptions, localizing object center points based on textual descriptions, and identifying objects from given spatial coordinates.The data is sourced from several open embodied grounding datasets, including RoboPoint (Yuan et al., 2024), ShareRobot (Ji et al., 2025), Pixmo-Points (Deitke et al., 2025), Paco-LaVIS (Ramanathan et al., 2023), and RefSpatial (Zhou et al., 2025a).To further enhance generalization capabilities for open-world and open-vocabulary scenarios, we also generate an additional 300k point and bounding box annotations derived from segmentation masks in the SA-1B dataset (Kirillov et al., 2023).This combination of curated human annotations and synthetically enriched data aims to bolster both the diversity and scalability of visual grounding under real-world embodied settings.</p>
<p>General and Spatial Reasoning Data The Vlaser dataset integrates 1.2 million question-answer pairs dedicated to general Robotic Visual Question Answering (RoboVQA) tasks, along with an additional 500k data items specifically designed to enhance spatial intelligence.This comprehensive data composition substantially strengthens the model's capabilities in general state perception and 3D spatial reasoning.For the RoboVQA component, data is aggregated from multiple established sources, including RoboVQA (Sermanet et al., 2024), Robo2VLM (Chen et al., 2025b), RoboPoint (Yuan et al., 2024), RefSpatial (Zhou et al., 2025a), OWMM-Agent (Chen et al., 2025a), among others.To support spatial understanding and reasoning, we incorporate datasets such as SPAR (Zhang et al., 2025), SpaceR-151k (Ouyang et al., 2025), and VILASR (Wu et al., 2025).Furthermore, we augment these with 100k manually annotated spatial understanding samples generated from publicly available 3D scene datasets-including ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), CA-1M (Lazarow et al., 2025), and ARKitScenes (Baruch et al., 2021).The integration of these diverse and high-quality data sources effectively enhances the model's spatial awareness and supports more robust performance in complex embodied reasoning tasks.</p>
<p>Planning Data To tackle complex tasks, it is essential to decompose them into manageable sub-tasks and solve them step by step.This capability is commonly referred to as planning.Effective planning allows robots to combine basic skills and generalize to new scenarios.We collected 400k training data to strengthen the model's planning ability, encompassing both language-based planning data and multimodal tasks.These include Alpaca-15k-Instruction (Wu et al., 2023) and MuEP (Li et al., 2024a).To further enhance environmental understanding and reasoning for complex decision-making, we incorporated training data with detailed reasoning processes from WAP (Shi et al., 2025).To improve the model's ability to comprehend complex instructions and execute tasks, we followed the annotations of LLaRP (Szot et al., 2024) to initialize planning tasks in Habitat (Szot et al., 2021) and generate planning trajectories to accomplish these tasks.In addition, we integrated egocentric video datasets such as EgoPlan-IT (Chen et al., 2023) and EgoCOT (Mu et al., 2023), which closely align with the observational perspective of embodied agents and provide valuable planning examples.</p>
<p>In-Domain Data for downstream VLAs</p>
<p>To facilitate the end-to-end policy learning for Vision-Language Action Models (VLAs), we further generate 2 million in-domain multimodal questionanswer pairs tailored for VLM pretraining.These data are specifically designed to align with the embodied reasoning context and enhance the model's ability to perceive, reason, and plan in interactive environments.The in-domain data is sourced from simulation platforms SimplerEnv (Li et al., 2024c).Within SimplerEnv, data is generated for two distinct robotic embodiments: the Google Robot (Brohan et al., 2023b;a;O'Neill et al., 2024) and the WidowX Robot (Walke et al., 2023a), ensuring broad morphological and kinematic coverage.The question-answer pairs encompass the specialized categories including embodied grounding, spatial intelligence, planning and general VQA for robot states as described above.The detailed methodology for constructing each of the in-domain data in simulation is described in Appendix A.2.</p>
<p>TRAINING RECIPE</p>
<p>Vlaser adopts a two-stage training recipe, designed to optimize both embodied reasoning and robot control.It includes a VLM pretraining followed by a VLA finetuning.In this section, we elaborate on the training recipe among all phrases.</p>
<p>Vision-Language Pretraining Vlaser is developed by supervised fine-tuning (SFT) InternVL3 (Zhu et al., 2025) on embodied-related datasets, including those focused on grounding, planning, and spatial intelligence.In the first training phase, we fine-tune InternVL3 using auto-regressive language modeling loss.In particular, given the input images x ∈ R t×h×w×3 and textual prompt y ∈ R l , the language modeling loss L lm can be defined by
L lm = − log p(t N |F v (x; θ v ), F t (y), t 0:N −1 ; Θ),(1)
where p ∈ R m is the next-token probability and m denotes the vocabulary size.Here, F v (•) denotes the ViT and the MLP, and θ v is their parameters.F t (•) is the textual tokenizer.Θ are the parameters of the LLM.t i denotes the i-th predicted word.</p>
<p>Vision-Language-Action Finetuning For robot policy learning, we optimize the model using an action expert trained on robot-specific datasets.Vlaser integrates a flow-matching-based action expert to predict a sequence of future actions from a single-frame observation.Specifically, denote the action chunk A t = [a t , a t+1 , . . ., a t+H−1 ], where a t represents the action in the current timestep t and H represents the action horizon.Meanwhile, we encode each noisy action with an action encoder (i.e., an MLP projector) as a single token for the action expert.For the action chunk, A τ t = τ A t + (1 − τ )ϵ is the corresponding noisy action chunk, and we train the network to output v θ (A τ t , o t ) to match the denoising vector field u(A τ t |A t ) = ϵ − A t , where o t indicates the observations (e.g., image camera and robot state) at action timestep t, θ represents the network and τ ∈ [0, 1] represents the flow matching timesteps.Therefore, the VLA optimization loss is as follows,
L vla = ∥v θ (A τ t , o t ) − u(A τ t |A t )∥ 2 (2)
We sample the action chunks from the robot episodes and flow-matching timesteps to optimize the network.At inference, we generate actions by integrating the learned vector field from τ = 0 to τ = 1, starting with random noise A 0 t ∼ N (0, I), as follows,
A τ +δ t = A τ t + δv θ (A τ t , o t ) (3)
where δ is the integration step size.In our experiments, we set H as 4, and δ as 10.We aim to identify the most effective VLMs for downstream VLA fine-tuning and bridge the gap between foundational VLMs and their performance in downstream VLA tasks, thus shedding light on the future construction of embodied VLMs.Currently, the SimplerEnv benchmark, including Bridge (Walke et al., 2023b) and Google Robot (Jang et al., 2022;Brohan et al., 2023b) datasets, provides numerous training episodes (Over 5M images) and corresponding Real-to-Sim benchmarks.We thus majorly analyze the most effective data stream for VLA finetuning based on SimplerEnv.</p>
<p>EXPERIMENTS</p>
<p>PERFORMANCE ON EMBODIED REASONING CAPABILITY</p>
<p>Evaluation Datasets We conduct a comprehensive evaluation of embodied reasoning capabilities across a total of 12 benchmarks, covering a wide spectrum of tasks including embodied question answering, task planning, embodied grounding, spatial intelligence, and closed-loop simulation evaluation.The evaluated benchmarks consist of: ERQA (Team et al., 2025b), Ego-Plan2 (Qiu et al., 2024), Where2place (Yuan et al., 2024), Pointarena (Cheng et al., 2025), Paco-Lavis (Ramanathan et al., 2023), Pixmo-Points (Deitke et al., 2025), VSI-Bench (Yang et al., 2025b), RefSpatial-Bench (Zhou et al., 2025a), MMSI-Bench (Yang et al., 2025d), VLABench (Zhang et al., 2024), and EmbodiedBench (Yang et al., 2025c).For EmbodiedBench, we further assess performance in two simulation environments ALFRED (Shridhar et al., 2020) and Habitat (Szot et al., 2021).</p>
<p>Baselines Since our method, Vlaser is trained at two model scales -2B and 8B parameters, we categorize the compared baseline methods into three groups for a systematic evaluation:   (Brohan et al., 2023b), Octo-Base (Team et al., 2024), OpenVLA (Kim et al., 2024), RoboVLM (Liu et al., 2025) and SpatialVLA (Qu et al., 2025b) are from (Qu et al., 2025b) while the results of π 0 (Black et al., 2024) is from (open-pi zero, 2025).</p>
<p>Model</p>
<p>Carrot on plate Put eggplant in basket Spoon on towel Stack Cube Avg RT-1-X (35M) (Brohan et al., 2023b) 4.2% 0% 0% 0% 1.1% Octo-Base (93M) (Team et al., 2024) 8  (Bai et al., 2025), Embodied-R1-7B (Yuan et al., 2025), and RoboBrain2.0-7B(Team et al., 2025a).</p>
<p>The overall experimental results are presented in Table 1.As shown in Table 1, compared to the base models InternVL3-2B and InternVL3-8B used as initialization for our supervised finetuning, our Vlaser yields substantial improvements across all embodied reasoning capabilities, with particularly notable gains in embodied grounding and simulation-based evaluation.For example, the average score increases from 15.2 to 45.3 for the 2B model, and from 22.3 to 51.3 for the 8B model.These significant performance gains underscore the high quality and effectiveness of the Vlaser-6M dataset in enhancing embodied reasoning abilities.An interesting observation emerges that when finetuning on the same Vlaser-6M dataset, a smaller sized Vlaser-2B outperforms Vlaser-8B on simple point grounding tasks that require direct, short answers.Conversely, Vlaser-8B demonstrates superior performance on more complex tasks such as multi-step planning and closed-loop simulation evaluation, which often benefit from chain-of-thought (CoT) reasoning.This scaling behavior indicates the importance of appropriate model size selection based on target application requirements.</p>
<p>When compared against current state-of-the-art embodied-specific VLMs, including Robo-Brain2.0(Team et al., 2025a) and Embodied-R1 (Yuan et al., 2025), our method, Vlaser still achieves superior performance on the majority of benchmarks while remaining highly competitive on the remainder, ultimately attaining the highest overall score (by +10% margin overall).These results indicate that Vlaser delivers a well-balanced and robust capability set, performing strongly across multiple dimensions of embodied intelligence -from embodied question answering and state estimation to future action planning, visual grounding, spatial reasoning, and closed-loop simulation.Such comprehensive competence highlights its suitability as a versatile backbone for embodied AI brains.</p>
<p>In the following section, we further examine how these enhanced reasoning capabilities, embedded within VLMs, translate into improved performance when fine-tuned for downstream Vision-Language Action models (VLAs) in simulation manipulation scenarios.</p>
<p>PERFORMANCE ON DOWNSTREAM CLOSE-LOOP ROBOT TASKS</p>
<p>Finetuning Datasets We conduct extensive experiments on SimplerENV to evaluate the performance of Vlaser and Vlaser data engine on closed-loop robotic manipulation tasks.</p>
<p>SimplerENV is an open-source suite of purpose-built simulated environments with nearly 150K episodes for evaluating real-world robot manipulation policies in a scalable, reproducible way.It targets the key real-to-sim gaps -control and vision so that simulated performance reliably tracks real-robot outcomes.Across Google Robot and WidowX/BridgeData V2 setups, SimplerEnv reports strong real-vs-sim correlations and faithfully reflects behavior under distribution shifts, enabling fast, comparable policy assessment without full digital twins.As a result, SimplerENV has been widely adopted for evaluating VLA models and has proven to reliably reflect the performance of the models on the real robot platform.Baselines As mentioned before, we integrate Vlaser with an action expert utilizing flow matching for low-level control fine-tuning and evaluating the model on the WidowX and Google Robot datasets from SimplerEnv.We conduct experiments using InternVL3-2B, Vlaser, and the models based on Vlaser data engine for the in-domain robot data.Commonly, embodied reasoning can be marginally categorized into embodied QA (including planning), embodied grounding, and embodied spatial intelligence.We thus construct corresponding embodied VLMs based on the corresponding data stream: Vlaser-QA, Vlaser-Grounding, Vlaser-Spatial.Notably, all series of Vlaser models are based on the same architecture with 2B size.</p>
<p>The full experimental results are presented in Table 2 and Table 3.We observe the proposed vision-language-action architecture based on InternVL3-2B is capable of low-level robot control and achieves comparable performance to many previous approaches, though we utilize a 2B backbone.</p>
<p>Particularly, we did not observe any clean improvement when we used the Vlaser-2B as our initial backbone on both WidowX and Google Robot, while we achieved significant performance promotion with the Vlaser-QA, although the architecture and model size are the same between the two models.This observation illustrates the effectiveness of our Vlaser data engine, and meanwhile identifies that there is no positive correlation between common embodied reasoning benchmarks and the performance of closed-loop control of the lower level for the specific embodiment of the robot.We reckon it is the domain shift between the internet data and the corresponding robot embodiment (e.g., WidowX or Google Robot), and we find that the enhanced abilities in the same observation domain effectively facilitate the closed-loop success rate.Therefore, it is urgent to shrink the domain gap between the foundational models and real-world robot embodiment for closed-loop task completion.</p>
<p>In addition, we conducted a simple ablation study on the Google Robot Tasks to evaluate the effectiveness of different data annotations.The experimental results indicate that incorporating all types of data leads to significant improvements, achieving performance comparable to the baseline.</p>
<p>We attribute these improvements primarily to the reduction of the vision observation domain shift.</p>
<p>RELATED WORK</p>
<p>Vision-Language Model for Embodied Reasoning Enhancing the embodied reasoning capabilities of current state-of-the-art Vision-Language Models (VLMs) has emerged as a critical research direction.These capabilities encompass a range of competencies, including grounding (Yuan et al., 2024;Deitke et al., 2025;Cheng et al., 2025) which identifies affordances that enable embodied agents to perform manipulations, spatial intelligence (Yang et al., 2025b;d), such as object counting and spatial relationship understanding, as well as task planning (Chen et al., 2023;Qiu et al., 2024), which involves assessing the current state and determining subsequent actions to be executed.Gemini Robotics-ER (Team et al., 2025b) integrates embodied reasoning into its core visual-language model (VLM), demonstrating strong generalization across a variety of tasks such as 3D scene perception, visual pointing, state estimation, and affordance prediction.In parallel, a number of data-driven methodologies have emerged to support such reasoning capabilities.For instance, Cosmos-Reason1 (NVIDIA et al., 2025a), VeBrain (Luo et al., 2025), MolmoAct (Lee et al., 2025), and EmbodiedOneVision (Qu et al., 2025a) each contribute curated datasets specifically designed for embodied reasoning tasks, emphasizing aspects such as multi-modal instruction following and action-aware visual-language alignment.Further advancing this direction, several frameworksincluding the RoboBrain series (Ji et al., 2025;Team et al., 2025a), Embodied R1 (Yuan et al., 2025), and Robix (Fang et al., 2025) incorporate Reinforcement Fine-Tuning (RFT) and synthesize spatiotemporal reasoning datasets enriched with structured thought traces.These approaches aim to enhance models' capacity for causal reasoning and long-horizon task decomposition.Distinguished from these prior efforts, our work not only achieves competitive, and in some cases superior performance on established embodied reasoning benchmarks, but also provides an in-depth analysis of the synergistic relationship between pre-trained VLMs and downstream Vision-Language Action Models (VLAs), offering insights that bridge model capabilities and real-world deployment.</p>
<p>Vision-Language-Action models.Developing a generalist model remains a central challenge in robotics.Inspired by the strong generalization abilities of vision-language models (VLMs) (OpenAI, 2023;Chen et al., 2024;Bai et al., 2025;Team et al., 2023) trained on large-scale internet data, researchers have proposed vision-language-action (VLA) models, which have demonstrated promising performance (Brohan et al., 2023a;Kim et al., 2024;Qu et al., 2025b;Hou et al., 2025).</p>
<p>Compared to traditional robot policies, VLA models are pretrained on large-scale robotics datasets and exhibit improved generalization across object categories and visual observations.Building on recent progress, researchers have incorporated techniques such as diffusion (Ho et al., 2020;Rombach et al., 2022;Peebles &amp; Xie, 2023), flow matching (Lipman et al., 2023b), and mixture-of-experts (MoE) (Shazeer et al., 2017) into VLA models, and have adopted larger, more capable VLMs as their backbones.These advances have enabled VLA models to tackle a wider range of complex real-world manipulation tasks.Nevertheless, current VLA models remain limited in their generalization.In particular, they have not yet reached the level of general reasoning exhibited by VLMs, such as decompose a complex task into manageable sub-steps and complete the task in a zero-shot manner.</p>
<p>Efforts have been made to enhance specific reasoning abilities in embodied scenarios (Team et al., 2025b;Ji et al., 2025;NVIDIA et al., 2025a).In parallel, several studies (Intelligence et al., 2025;Driess et al., 2025;Zhou et al., 2025b) explore unified training frameworks for VLMs and VLAs to leverage the reasoning capacity of VLMs.However, the relationship between high-level multimodal reasoning and low-level control performance remains largely unexplored.It is still unclear which specific multimodal abilities such as spatial understanding, grounding, or planning and which types of training data most effectively enhance the control capabilities of a VLA model.In this work, we take an initial step toward analyzing this relationship by a systematic evaluation, and also propose the latest foundational model with strong embodied multimodal understanding and action prediction.</p>
<p>CONCLUSION AND DISCUSSION</p>
<p>We introduce Vlaser, a foundational vision-language-action model that extends vision-language models with embodied reasoning and end-to-end robot control capabilities.Powered by the Vlaser-6M dataset, the model establishes a new state of the art across a wide range of embodied reasoning benchmarks, including planning, grounding, spatial reasoning, and simulation-based tasks.Moreover, Vlaser reveals the most effective data streams for downstream VLA through its curated data pipeline, achieving state-of-the-art performance on Bridge and competitive results on Google Robot for end-to-end robot control.</p>
<p>In this work, we reveal that current embodied reasoning benchmarks exhibit a significant domain gap when compared to real-world robots.This core domain shift arises from the observation that robots have a fundamentally different viewpoint from that of internet datasets.Additionally, there are inherent limitations due to the lack of sufficient data from the robot's perspective, despite the abundance of vision datasets available.Therefore, we argue that it is essential to develop alignment techniques to bridge the domain gap in representations between the robot's viewpoint and that of internet datasets.</p>
<p>Enshen  et al., 2025)(InternVL3-2B and InternVL3-8B).To maximize adaptation to embodied reasoning tasks, we keep all parameters trainable, including those in the large language model, the vision-language projector, and the visual encoder, enabling comprehensive end-to-end learning.Further details regarding the training setup, including hyperparameters and optimization settings, are provided in Table 4.</p>
<p>Table 4: Hyper-parameters used in the VLM pretraining of Vlaser.</p>
<p>While using Vlaser as the base model for downstream VLA Policy fine-tuning, we optimize all parameters within both the VLM and the Action Expert.Additionally, we conduct comparative experiments using several different versions of base models, including InternVL3-2B, etc. Detailed information and related parameter settings can be found in Table 5.</p>
<p>A.2 DATA GENERATION DETAILS</p>
<p>Embodied Grounding Data To further enhance embodied grounding capabilities, we generate an additional 300k high-quality data samples from the SA-1B dataset (Kirillov et al., 2023).The data generation process consists of two main stages.First, we convert segmentation masks into bounding boxes and point annotations: bounding boxes are derived by computing the minimal axisaligned rectangle enclosing each mask, while point annotations are obtained by randomly sampling a coordinate within the mask region.To ensure annotation quality, we apply an IoU threshold of 0.9 to select high-precision masks; masks with lower IoU values are either excluded or assigned reduced sampling weight.From the over 1 billion available masks, we initially sample 1 million candidate instances.In the second stage, we employ a two-step captioning and refinement pipeline.Coarse captions are first generated using BLIP-2 (Li et al., 2023), followed by a filtering and refinement process using Qwen2.5-VL-7B(Bai et al., 2025) to eliminate low-quality items and produce more accurate and detailed descriptions.This rigorous pipeline ultimately yields 300k high-quality data samples tailored for embodied grounding tasks, significantly expanding the diversity and precision of our training corpus.</p>
<p>Spatial Reasoning Data To enhance spatial intelligence capabilities, we manually construct a dataset of 100k 3D spatial perception samples derived from ScanNet (Dai et al., 2017), ScanNet++ (Yeshwanth et al., 2023), and ARKitScenes (Baruch et al., 2021).Following methodologies established in prior data engines (Deng et al., 2025;Fan et al., 2025), we utilize both the 3D point cloud and video sequences of each scene to construct a spatio-temporal scene graph.This graph encapsulates structural and semantic information such as overall scene dimensions, room center coordinates, object category counts, and precise 3D bounding boxes for every object instance.Based on this representation, we generate spatial reasoning questions that probe layout properties and inter-object relationships.These include queries about the object counts, absolute and relative distances, object and room sizes, relative directions between objects, and other spatial attributes, using the same question template in VSI-Bench (Yang et al., 2025b).</p>
<p>Planning Data To improve the model's ability to comprehend complex instructions and execute tasks with environmental feedback, we curate additional planning data within the Habitat simulator (Szot et al., 2021).Specifically, we initialize each planning task following the annotations of LLaRP (Szot et al., 2024), which specify both the task goals and the set of permissible actions.An LLM agent based on gpt-4o (OpenAI, 2025) is then deployed to roll out the task.During each rollout, we record the task instruction, the sequence of actions taken, and the environment's feedback, including observations and success signal for each action.Both the executed action trajectories and the corresponding feedback are retained.Only trajectories that successfully accomplish the task are included in the final training set, providing rich paired data of instructions, execution processes, and environment responses for enhancing the model's planning capabilities in a complex environment.</p>
<p>In-Domain Data for downstream VLAs We generate 2 million in-domain multimodal data samples to facilitate direct transfer learning for downstream Vision-Language Agents (VLAs) during finetuning.These data are collected from two distinct robotic platforms: the WidowX Robot (Walke et al., 2023a) and the Google Robot (Brohan et al., 2023b;a) within the SimplerEnv (Li et al., 2024c) simulation framework.The dataset mainly encompasses three systematically designed questionanswer types: 1) General QA, which queries the robot's current state and requests next few action plans; 2) Grounding QA, which requires the robot to localize points and bounding boxes as the actionable affordances; 3) Spatial Reasoning QA, which probes relative spatial relationships and geometric properties of objects in the scene.Detailed prompt templates and representative examples for each data category are provided in Figure 3 (General QA), Figure 4 (Grounding QA), and Figure 5 (Spatial Reasoning QA), respectively.We use Qwen2.5VL-7B(Bai et al., 2025) as the base model to generate the above data items.Question:</p>
<p>You are a robot operating in a home.Given a task, you must accomplish the task using a defined set of actions to achieve the desired outcome.Now the human instruction is: Put a microwaved tomato slice on the counter.You need to output the reasoning process and an action to execute.</p>
<p>Vlaser-8B: Reasoning-step-1: To put a microwaved tomato slice on the counter, I need to start by slicing a tomato.To do this, I require a knife.In the image, there is a knife visible on the counter near the toaster.Finding the knife is the logical next step because it is essential for cutting the tomato into slices.Once I have the knife, I can proceed with slicing the tomato.Therefore, my next action is to find the knife.Action: find a knife.Reasoning-step-2: I have identified the knife on the counter near the sink.</p>
<p>Picking up the knife is the next logical step because it is necessary for slicing the tomato.Once I have the knife, I can proceed with cutting the tomato into slices.This aligns with the goal of preparing a tomato slice for microwaving and then placing it on the counter.Action: pick up the knife.</p>
<p>Reasoning-step-3: I have successfully located the knife in the drawer.Now, I need to find a tomato to slice.The image shows a kitchen counter with various items, but no tomato is immediately visible.Tomatoes are commonly stored in the refrigerator or on the counter.Since the refrigerator is partially visible on the left side of the image, it makes sense to check there first.If the tomato isn ẗ in the fridge, I can then look around the counter more thoroughly.Therefore, my next step is to find a tomato.Action: find a tomato.... Reasoning-step-9: The current visual state shows the microwave with its door closed, and the tomato slice is on the counter.The task is to place a microwaved tomato slice on the counter, which has already been accomplished.Given that the tomato slice is already on the counter, the next logical step is to confirm its placement by finding a suitable counter top.This ensures that the tomato slice is securely and appropriately positioned for the task.Therefore, the next step is to find a counter top.</p>
<p>Action: find a counter top.</p>
<p>Reasoning-step-10: The current visual state shows a microwave with its door closed, and two tomatoes are visible on the counter.One tomato is already sliced, indicating that the previous steps involving slicing and microwaving have been completed.Since the tomato is currently being held, the next logical step is to put it down on the counter.This action will free up the hand to handle the tomato slice and place it on the counter as required by the task.Therefore, the appropriate next action is to put down the tomato.Action: put down the tomato.</p>
<p>Figure 1 :
1
Figure 1: Overall framework, capabilities, and evaluation of Vlaser.Top-left: Composition of the Vlaser-6M dataset, featuring multi-task embodied data-including QA, grounding, spatial reasoning, and planning-along with in-domain simulation-sourced pairs.Top-right: A LiDAR visualization illustrating the state-of-the-art embodied reasoning capability of the Vlaser VLM.Bottom-left: The pre-trained Vlaser VLM significantly accelerates convergence in downstream Vision-Language Action model (VLA) policy learning on WidowX platform (Walke et al., 2023a).Bottom-right: Successful closed-loop operation of an agent powered by Vlaser within the SimplerEnv benchmark (Li et al., 2024b).</p>
<p>Figure 2 :
2
Figure 2: An illustration of Vlaser architecture.Vlaser includes two components and corresponding training phases: 1) the Multimodal Pretraining is for embodied reasoning enhancement based on the corresponding data engine; 2) VLA training is performed on the action expert module, which handles low-level control based on flow matching action generation.</p>
<p>1) State-ofthe-art closed-source models, includingGPT-4o (OpenAI, 2025),Claude-3.7-Sonnet(Anthropic,  2025), and Gemini-2.5-Pro(Comanici et al., 2025); 2) Small-scale MLLMs (2B -3B parameters), comprising ChatVLA-2B(Zhou et al., 2025b), InternVL3-2B (Zhu et al., 2025), Qwen2.5-VL-3B(Bai et al., 2025), Embodied-R1-3B(Yuan et al., 2025), and RoboBrain2.0-3B(Teamet al.,</p>
<p>Figure 4 :
4
Figure 4: An illustration of Vlaser-6M data engine for in-domain embodied grounding QA sample in SimplerEnv.</p>
<p>Table 1 :
1
Comparison with existing close-sourced, open-sourced and embodied-related VLMs on 12 general embodied reasoning benchmarks, spanning from embodied QA, planning, embodied grounding to spatial intelligence and close-loop simulation evaluation.Avg denotes the normalized average performance of all the benchmarks.The best, second best and third best score among all the baselines are colored in red, orange and yellow.
ModelQA ERQA Ego-Plan2 Where2place Pointarena Paco-Lavis Pixmo-Points VSIBench RefSpatial MMSIBench VLABench EB-ALFRED EB-Habitat Planning Embodied Grounding Spatial Intelligence SimulationAvg▼ Closed-source MLLMs:GPT-4o-2024112047.041.829.129.516.210.842.58.830.339.356.359.034.2Claude-3.7-Sonnet35.541.325.622.212.47.247.07.730.241.767.065.733.6Gemini-2.5-Pro55.042.939.962.845.525.843.430.336.934.862.753.044.4▼ Small Size MLLMs:ChatVLA-2B34.325.33.710.110.22.12.40.920.10.00.00.09.1InternVL3-2B31.530.95.27.115.41.431.51.825.319.41.312.015.2Qwen2.5VL-3B35.330.331.041.767.436.627.924.926.531.36.719.731.6Embodied-R1-3B36.036.035.145.368.336.628.028.526.024.67.019.332.5RoboBrain2.0-3B37.341.864.246.067.636.928.846.526.818.10.010.035.3Vlaser-2B35.838.374.057.872.544.657.543.023.623.142.330.745.3▼ Medium Size MLLMs:Magma-8B29.327.910.929.615.310.112.74.526.28.50.00.014.6Cosmos-Reason1-7B 39.326.911.440.861.823.633.95.426.435.54.05.326.2VeBrain-7B38.327.333.138.955.120.139.920.628.325.95.712.328.8InternVL3-8B35.340.010.014.221.15.742.15.625.724.719.023.722.3Qwen2.5VL-7B39.329.731.156.368.043.538.232.125.936.410.018.335.7Embodied-R1-7B38.337.169.551.269.939.238.631.128.135.510.019.038.9RoboBrain2.0-7B42.033.263.649.573.137.836.132.526.56.614.029.337.0Vlaser-8B41.053.469.560.368.340.560.359.227.245.650.040.051.3</p>
<p>Table 2 :
2
SimplerEnv Evaluation on WidowX Robot Tasks.Avg indicates the average success rate among the four tasks.InternVL3-2B, Vlaser and Vlaser with Bridge Q&amp;A indicates the base model we select to fine-tune on the WidowX Robot Tasks.Particularlly, Vlaser-QA refers to the model which is fine-tuned on the Bridge question-answer pairs dataset.Model sizes are indicated within parentheses.The result of RT-1-X</p>
<p>Table 3 :
3
Comparison with existing methods in SimplerEnv on Google Robot tasks.Avg indicates the average success rate among the three tasks.In the last five lines in the table, we use the name of base model to indicate different evaluation settings.Vlaser-QA indicates the model which is fine-tuned on the Fractal question-answer pair dataset.Vlaser-Spatial and Vlaser-Grounding represent the model fine-tuned on the Spatial Reasoning Data and Embodied Grounding Data separately.The details of different sub-datasets can be find in the AppendixA.2.
Model sizes are indicated within</p>
<p>Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan  Wang, Tiejun Huang, Lu Sheng, et al.Roborefer: Towards spatial referring with reasoning in vision-language models for robotics.arXivpreprint arXiv:2506.04308,2025a.Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, et al.Chatvla: Unified multimodal understanding and robot control with vision-language-action model.arXiv preprint arXiv:2502.14420,2025b.
Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, YuchenDuan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes foropen-source multimodal models. arXiv preprint arXiv:2504.10479, 2025.A APPENDIXA.1 TRAINING DETAILSOur Vlaser is optimized in a fully supervised finetuning (SFT) manner based on InternVL3 series (Zhu</p>
<p>Table 10 :
10
Simulation planning Examples.</p>
<p>Simulation Planning: Example #1 from EmbodiedBench.</p>
<p>A.3 SIMULATION EVALUATION DETAILSWe fine-tune and evaluate the VLA models using various base models within the SimplerEnv simulation environment.To ensure fair evaluation, we use checkpoints with the same number of iterations for the WidowX Robot Task and the Google Robot Task, respectively.Specifically, for the WidowX Robot Tasks, we use a checkpoint after 45,390 iterations, while for the Google Robot Tasks, we use a checkpoint after 36,970 iterations.During evaluation, we utilize a single image and select an action chunk of size 2 for execution.In the flow matching configuration, we use 10 inference steps during the inference phase and apply Euler method as numerical integration method.We evaluate a sufficient number of samples to ensure the reliability of the tests.The exact number of test samples for each task is shown in the Table6.Qualitative Samples in SimplerEnv. Figure6and Figure7present qualitative examples from the evaluations conducted in the SimplerEnv.Specifically, Figure6showcases the performance of multiple models on the WidowX Robot Task, while Figure7demonstrates the results on the Google Robot Task.From these results, it is evident that, under the same test conditions, baseline models often fail the entire task due to slight positional errors.In contrast, our model shows significant improvements in this regard.Whether incorporating in-domain question-answer pair data, spatial intelligence data, or grounding data, the performance of the model shows clear enhancement.These results validate the effectiveness of our approach.GeneralWidowX Robot Google RobotQuestion: How can the robot arm effectively fold the green cloth from left to right as instructed?Answer: To fold the green cloth from left to right, the robot arm should first position itself so that the left edge of the cloth is aligned with the starting point of the fold.The arm should then move in a controlled manner, applying gentle pressure to fold the cloth over to the right edge.It's important to ensure that the cloth is not torn or damaged during the folding process.The robot arm should also be equipped with sensors to detect the cloth's texture and adjust its movements accordingly to maintain a smooth and accurate fold.Additionally, the robot arm should be programmed to stop folding once the right edge is reached, ensuring that the cloth is neatly folded without any wrinkles or creases.Question: What safety measures should be taken when using a robot arm to pick up cans like the can shown in the image?Answer: When using a robot arm to pick up cans, several safety measures should be considered to ensure the task is performed safely and efficiently.First, it's crucial to ensure that the robot arm is properly calibrated and that the gripper is securely attached to the arm.This prevents accidental drops or damage to the can.Additionally, the robot arm should be programmed with gentle and precise movements to avoid any impact that could damage the can or the arm itself.It's also important to ensure that the robot arm is operating within its designated safety boundaries to prevent any unintended movements that could cause harm.Furthermore, the environment should be free of obstacles that could interfere with the robot arm's movement, and the area should be well-lit to ensure the robot arm can accurately identify and grasp the can.Lastly, it's advisable to have a human supervisor or safety system in place to monitor the robot arm's actions and intervene if necessary.Make sure the question focuses on spatial reasoning, object relationships, positioning, or geometric understanding, and the answer provides precise spatial information about the robot's environment.Question: What is the relative position of the drawer compared to the robot arm?Answer: The drawer is positioned directly beneath the robot arm, with the arm's gripper currently interacting with the surface of the drawer.The drawer is oriented horizontally, and its front edge is aligned with the front edge of the robot arm's gripper.The drawer appears to be slightly tilted forward, suggesting that the robot arm is in the process of opening it.Question: What is the relative position of the coke can compared to the apple on the table?Answer: The coke can is positioned to the left of the apple on the table.The can is closer to the top left corner of the table, while the apple is slightly below and to the right of the can.Question:Point to the tool people use for writing.Vlaser-8B: <point>[[701, 374]]</point>.Embodied Grounding: Example #2 from Where2Place.Question:Please point out the free space between the black water bottle and the pot lid.Vlaser-8B: <point>[[293, 560]]</point>.
Claude 3.7 sonnet system card. 2025Sonnet Anthropic</p>
<p>Cosmos-reason1: From physical common sense to embodied reasoning. Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, arXiv:2503.155582025arXiv preprint</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025arXiv preprint</p>
<p>Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, arXiv:2111.08897A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. 2021arXiv preprint</p>
<p>pi_{0}: A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich, 2023a</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Robotics: Science and Systems XIX. 2023b</p>
<p>Owmm-agent: Open world mobile manipulation with multi-modal agentic data synthesis. Junting Chen, Haotian Liang, Lingxiao Du, Weiyun Wang, Mengkang Hu, Yao Mu, Wenhai Wang, Jifeng Dai, Ping Luo, Wenqi Shao, arXiv:2506.042172025aarXiv preprint</p>
<p>Kaiyuan Chen, Shuangyu Xie, Zehan Ma, Pannag R Sanketi, Ken Goldberg, arXiv:2505.15517Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets. 2025barXiv preprint</p>
<p>Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu, 2023CoRR</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2024</p>
<p>An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, Xiaolong Wang, Navila, arXiv:2412.04453Legged robot vision-language-action model for navigation. 2024arXiv preprint</p>
<p>Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, arXiv:2505.09990Probing multimodal grounding through language-guided pointing. 2025arXiv preprint</p>
<p>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Ron Chrisley, ; Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, arXiv:2507.06261Artificial intelligence. 14912003. 2025arXiv preprintEmbodied artificial intelligence</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Nianchen Deng, Lixin Gu, Shenglong Ye, Yinan He, Zhe Chen, Songze Li, Haomin Wang, Xingguang Wei, Tianshuo Yang, Min Dou, arXiv:2506.18385A comprehensive dataset for spatial reasoning in vision-language models. 2025arXiv preprint</p>
<p>Knowledge insulating vision-languageaction models: Train fast, run fast, generalize better. Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Homer Allen Z Ren, Quan Walke, Lucy Xiaoyang Vuong, Shi, arXiv:2505.237052025arXiv preprint</p>
<p>Vlm-3r: Vision-language models augmented with instructionaligned 3d reconstruction. Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, arXiv:2505.202792025arXiv preprint</p>
<p>Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li, arXiv:2509.01106Robix: A unified model for robot interaction, reasoning and planning. 2025arXiv preprint</p>
<p>Path planning and trajectory planning algorithms: A general overview. Motion and operation planning of robotic systems: Background and practical approaches. Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti, Renato Vidoni, 2015</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Scaling diffusion transformer for generalist vision-language-action policy. Zhi Hou, Tianyi Zhang, Yuwen Xiong, Haonan Duan, Hengjun Pu, Ronglei Tong, Chengyang Zhao, Xizhou Zhu, Yu Qiao, Jifeng Dai, arXiv:2503.197572025arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International conference on machine learning. PMLR2022</p>
<p>5}: a vision-languageaction model with open-world generalization. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, arXiv:2504.160542025arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Robobrain: A unified brain model for robotic manipulation from abstract to concrete. Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang, Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR). the Computer Vision and Pattern Recognition Conference (CVPR)June 2025</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Cubify anything: Scaling indoor 3d object detection. Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, Afshin Dehghan, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, arXiv:2508.07917Action reasoning models that can reason in space. 2025arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Muep: A multimodal benchmark for embodied planning with foundation models. Kanxue Li, Baosheng Yu, Qi Zheng, Yibing Zhan, Yuhui Zhang, Tianle Zhang, Yijun Yang, Yue Chen, Lei Sun, Qiong Cao, Li Shen, Lusong Li, Dapeng Tao, Xiaodong He, 10.24963/ijcai.2024/15Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. Kate Larson, the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-2482024</p>
<p>Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, Ted Xiao, arXiv:2405.05941Evaluating real-world robot manipulation policies in simulation. 2024barXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, arXiv:2405.059412024carXiv preprint</p>
<p>Flow matching for generative modeling. Yaron Lipman, Ricky T Q Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le, 2023a</p>
<p>Flow matching for generative modeling. Yaron Lipman, Ricky T Q Chen, Heli Ben-Hamu, Maximilian Nickel, Matthew Le, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202336</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, 2025</p>
<p>Visual embodied brain: Let multimodal large language models see, think, and control in spaces. Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, arXiv:2506.001232025arXiv preprint</p>
<p>EmbodiedGPT: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Cosmos-reason1: From physical common sense to embodied reasoning. Alisson Nvidia, Hannah Azzolini, Prithvijit Brandon, Huayu Chattopadhyay, Jinju Chen, Yin Chu, Jenna Cui, Yifan Diamond, Francesco Ding, Rama Ferroni, Jinwei Govindaraju, Siddharth Gu, Imad El Gururani, Zekun Hanafi, Jacob Hao, Jingyi Huffman, Brendan Jin, Rizwan Johnson, George Khan, Elena Kurian, Nayeon Lantz, Zhaoshuo Lee, Xuan Li, Tsung-Yi Li, Yen-Chen Lin, Ming-Yu Lin, Andrew Liu, Yun Mathau, Lindsey Ni, Wei Pavao, David W Ping, Misha Romero, Shuran Smelyanskiy, Lyne Song, Andrew Z Tchapmi, Boxin Wang, Haoxiang Wang, Fangyin Wang, Jiashu Wei, Yao Xu, Xiaodong Xu, Zhuolin Yang, Xiaohui Yang, Zhe Zeng, Zhang, 2025a</p>
<p>Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. Nikita Nvidia, Johan Cherniadev, Xingye Bjorck Andfernando Castañeda, Runyu Da, Ding, " Linxi, Yu Jim" Fan, Dieter Fang, Fengyuan Fox, Spencer Hu, Joel Huang, Zhenyu Jang, Jan Jiang, Kaushil Kautz, Lawrence Kundalia, Zhiqi Lao, Zongyu Li, Kevin Lin, Guilin Lin, Edith Liu, Loic Llontop, Ajay Magne, Avnish Mandlekar, Soroush Narayan, Scott Nasiriany, You Reed, Guanzhi Liang Tan, Zu Wang, Jing Wang, Qi Wang, Jiannan Wang, Yuqi Xiang, Yinzhen Xie, Zhenjia Xu, Seonghyeon Xu, Zhiding Ye, Ao Yu, Hao Zhang, Yizhou Zhang, Zhao, ArXiv Preprint. March 2025b. 2025open-pi zero. open-pi-zero</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAI</p>
<p>Gpt-4o system card. 2025OpenAI</p>
<p>Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun, arXiv:2504.01805Spacer: Reinforcing mllms in video spatial reasoning. 2025arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Egoplan-bench2: A benchmark for multimodal large language model planning in real-world scenarios. Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu, arXiv:2412.044472024arXiv preprint</p>
<p>Embodiedonevision: Interleaved vision-text-action pretraining for general robot control. Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, arXiv:2508.211122025aarXiv preprint</p>
<dl>
<dt>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-languageaction model. 2025barXiv preprint</dt>
<dd>
<p>Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2025</p>
</dd>
</dl>
<p>Parts and attributes of common objects. Anmol Vignesh Ramanathan, Vladan Kalia, Yi Petrovic, Baixue Wen, Baishan Zheng, Rui Guo, Aaron Wang, Rama Marquez, Abhishek Kovvuri, Kadian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Highresolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Multimodal long-horizon reasoning for robotics. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, J Nikhil, Joshi, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Noam Shazeer, Azalia Mirhoseini, Andy Krzysztof Maziarz, Quoc Davis, Geoffrey Le, Jeff Hinton, Dean, International Conference on Learning Representations. 2017</p>
<p>Jingjing Gong, and Xipeng Qiu. World-aware planning narratives enhance large vision-language model planner. Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, 2025</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Emma-x: An embodied multimodal action model with grounded chain of thought and look-ahead spatial reasoning. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, Deepanway Tan, Soujanya Ghosal, Poria, arXiv:2412.119742024arXiv preprint</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Large language models as generalizable policies for embodied tasks. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, Devon Hjelm, Alexander T Toshev, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. Mingyu Baai Robobrain Team, Huajie Cao, Yuheng Tan, Minglan Ji, Zhiyu Lin, Zhou Li, Pengwei Cao, Enshen Wang, Yi Zhou, Han, arXiv:2507.020292025aarXiv preprintet al. Robobrain 2.0 technical report</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025barXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025carXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023a</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023b</p>
<p>Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan, arXiv:2506.099652025arXiv preprint</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2305.03716Embodied task planning with large language models. 2023arXiv preprint</p>
<p>Magma: A foundation model for multimodal ai agents. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025a</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025b</p>
<p>Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, arXiv:2502.095602025carXiv preprint</p>
<p>Mmsi-bench: A benchmark for multi-image spatial intelligence. Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, arXiv:2505.237642025darXiv preprint</p>
<p>Scannet++: A highfidelity dataset of 3d indoor scenes. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, Angela Dai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao, arXiv:2508.139982025arXiv preprint</p>
<p>Han-Ye Zhang, Wei-Ming Lin, Ai-Xia Chen, Path planning for the mobile robot: A review. Symmetry. 201810450</p>
<p>From flatland to space: Teaching vision-language models to perceive and reason in 3d. Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, arXiv:2503.229762025arXiv preprint</p>
<p>Vlabench: A large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks. Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, arXiv:2412.181942024arXiv preprint</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, arXiv:2412.103452024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
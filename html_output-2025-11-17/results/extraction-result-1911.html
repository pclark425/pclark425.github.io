<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1911 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1911</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1911</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-281496722</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.18597v1.pdf" target="_blank">Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs)-based code generation for robotic manipulation has recently shown promise by directly translating human instructions into executable code, but existing methods remain noisy, constrained by fixed primitives and limited context windows, and struggle with long-horizon tasks. While closed-loop feedback has been explored, corrected knowledge is often stored in improper formats, restricting generalization and causing catastrophic forgetting, which highlights the need for learning reusable skills. Moreover, approaches that rely solely on LLM guidance frequently fail in extremely long-horizon scenarios due to LLMs'limited reasoning capability in the robotic domain, where such issues are often straightforward for humans to identify. To address these challenges, we propose a human-in-the-loop framework that encodes corrections into reusable skills, supported by external memory and Retrieval-Augmented Generation with a hint mechanism for dynamic reuse. Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world settings, show that our framework achieves a 0.93 success rate (up to 27% higher than baselines) and a 42% efficiency improvement in correction rounds. It can robustly solve extremely long-horizon tasks such as"build a house", which requires planning over 20 primitives.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1911.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1911.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LYRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LYRA: A Lifelong learning code sYnthesis framework with human-in-the-loop for Robotic long-horizon skill Acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human-in-the-loop code-generation framework that encodes human corrections into reusable, named skills stored in an external memory and uses RAG + hint guidance to enable robust long-horizon robotic manipulation planning and transfer to real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LYRA (framework using OpenAI GPT-4o for code-as-policies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system-level framework where an LLM (GPT-4o) synthesizes Python 'code-as-policies' that call deterministic perception and control primitives; the framework augments the prompt with retrieved few-shot examples and skill headers from an external memory (ChromaDB + text-embeddings-3) and supports human-in-the-loop iterative refinement and skill encapsulation (named Python functions) rather than only prompt edits.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>text-only language model (GPT-4o) used for code generation; retrieval uses text embedding models (text-embeddings-3).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; GPT-4o is used as an off-the-shelf LLM (implicitly pretrained on large code+text corpora); retrieval embeddings are from OpenAI text embeddings. The paper does not detail GPT-4o pretraining corpora or whether vision-language pretraining was used for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon robotic manipulation (tabletop rearrangement / structure building)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Benchmarks: customized Ravens (PyBullet), Franka Kitchen, MetaWorld, and real-world Franka FR3 deployments; tasks include pick-and-place primitives and long-horizon composition (e.g., 'build a house', 'stack a Jenga tower', writing 'ICLR'). Action space: continuous 6-DOF end-effector motions realized via deterministic primitives (pick/place, move_end_effector_to, etc.). Environments: simulation (privileged object state) and real world with vision-based perception; tasks require composing >20 primitives in extreme cases.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Discussed qualitatively: the framework enforces semantic alignment by (1) encoding human corrections as named skill functions (preserving semantics), (2) storing examples and skill docstrings in a retrieval database indexed by instruction-embeddings, and (3) using perception outputs (masks, 6D poses) to ground skill parameters. The paper does not present a quantitative overlap analysis between LLM pretraining data and task objects/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>LYRA (using GPT-4o) reported an overall success rate of 0.93 averaged across evaluated tasks, cited as up to 27% higher success rate than baselines, and a 42% improvement in feedback-round efficiency (fewer correction rounds). Metrics are reported as success rate (proportion of successful trials) and relative improvement in number of human/feedback rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>No experiment with a non-language pretrained core (e.g., training a model from scratch) is reported; baseline systems (CaP, LoHoRavens, DAHLIA) — which rely on LLMs or pretrained policies — achieve lower success rates (up to 27% lower than LYRA), but exact baseline numeric success rates for each task are not listed comprehensively in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>The paper reports a 42% efficiency improvement in correction rounds for LYRA (human-in-the-loop skill learning) vs LLM-based closed-loop baselines (fewer human correction interactions required). No absolute counts (e.g., episodes to reach X% success) are consistently reported across tasks in the main text; plots in the appendix/figures show average number of corrections but exact numeric tables are not inlined.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention- or saliency-style analyses are reported (the paper does not visualize LLM attention or attention over visual input).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No analysis of embedding space structure or clustering (beyond using text-embeddings-3 for RAG retrieval) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral/perceptual grounding evidence is indirect and functional: skills produced by the LLM call perception primitives (object masks/poses from vision models and core pick/place primitives) and are executed on the robot; successful transfer to a Franka FR3 and robust task completion (including long-horizon 'build a house') provide empirical evidence that language-conditioned code maps to perceptual/motor behavior, but the paper provides no internal representational analysis linking LLM tokens/embeddings to specific affordances or motor primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No neural hierarchical feature analysis is provided. The paper operationalizes hierarchy at the system level (skills can call other skills, nested functions), but does not analyze representation hierarchy in models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Qualitative factors affecting transfer success: (1) consistent primitive API between simulation and real robot (same function names and parameter conventions), (2) reliable perception pipeline (Grounded SAM 2 / GroundingDINO + FoundationPose) to estimate object masks and 6D poses, (3) user-designed curriculum and human hints to guide skill expansion. The paper reports successful sim-to-real transfer when these conditions hold, but does not provide an ablation quantifying sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No rigorous numeric comparison between objects/actions seen during skill learning vs held-out novel objects; qualitative claim: framework generalizes to unseen tasks such as 'build a temple' within ~5 rounds of user guidance by recombining existing skills.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>The system uses few-shot in-context learning: retrieval returns K=10 most similar examples to add to prompts. The paper demonstrates few-shot adaptation via retrieved examples and learned skills; pure zero-shot success is not claimed for complex long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise or component ablation analyses of the LLM are reported (the paper treats GPT-4o as an oracle/code generator).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit evidence of negative transfer reported. The paper does caution that naively overwriting prompts (or saving corrected flat code) can cause catastrophic forgetting, which LYRA mitigates by storing skills in external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct experimental comparison to vision-only pretrained models (e.g., ImageNet-pretrained perception+non-language controllers). The paper compares to other code-generation/LLM-based baselines rather than pure vision-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No analysis of representational or performance dynamics during fine-tuning of models; training is not performed on the core LLM—learning occurs by adding skills/examples to memory and iterative human-guided refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality or intrinsic-dimension analyses are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1911.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o (used as embodied code generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf large language model used in this work to generate Python code (policy programs) from natural language instructions and retrieved context; treated as the core code-generation and planning engine.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pretrained language model used to synthesize robot-executable Python code and perform task decomposition and planning via in-context learning and RAG-augmented prompts. The paper uses GPT-4o as a text/code generative model rather than performing additional multimodal pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Text/code language modeling pretraining (not trained in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper; implicitly code+text corpora used by GPT-family models. The paper does not describe GPT-4o's pretraining dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Code generation for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generates task-specific Python code calling perception and control primitives for tabletop manipulation tasks (sim and real), using in-context skill examples and retrieved skill headers.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed quantitatively in the paper; alignment is achieved procedurally by human corrections and encapsulating functionality into named skills to preserve semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As part of LYRA, GPT-4o yielded the framework's reported success rate (0.93) and sample-efficiency gains, but the paper does not separately report GPT-4o standalone metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided per-model (metrics reported at framework level).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Used in a code-as-policies pattern where generated code calls perception-to-action primitives; grounding is demonstrated behaviorally (successful task execution) but not analyzed at representational level.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No layer/feature analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper reports that GPT-4o-generated code transfers when backed by a stable primitive API and reliable perception (same prompt/skill headers across sim and real).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed for GPT-4o specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Operates with few-shot examples provided in the prompt; the system retrieves up to K=10 examples for in-context adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not shown.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable (GPT-4o is a language model).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1911.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grounded-SAM2+FoundationPose</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded SAM 2 / GroundingDINO + FoundationPose perception pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision foundation-model pipeline used to detect object masks/bounding boxes (Grounded SAM 2 / GroundingDINO) and estimate 6D object poses (FoundationPose) to provide open-world perceptual inputs for skill parameterization and sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grounded SAM 2 + GroundingDINO + FoundationPose</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision foundation models: Grounded SAM 2 and GroundingDINO produce segmentation masks and bounding boxes; FoundationPose recovers 6D poses by combining masks with CAD models. The paper uses these pre-trained vision models as a deterministic perception frontend to extract object centers and orientations that feed into code-generated skills.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Pretrained vision/vision-language foundation models for segmentation/detection and pose estimation (not trained in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; these models are standard open-world segmentation/detection and pose models trained on large vision datasets. The paper does not enumerate datasets or affordance annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Perception for real-world robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Object detection and 6D pose estimation in real tabletop scenes captured by a RealSense D435i, used to parameterize pick-and-place and assembly primitives for a Franka FR3 robot.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Implicit: these perception modules produce object-level masks and 6D poses that align with the skill API (object poses, sizes, colors). No quantitative overlap analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable (vision models). The paper reports successful use in real-world demos but does not quantify perception error rates or their effect on final task success in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported for the perception models.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Operational grounding: perception outputs are directly used to compute pick/place poses and feed deterministic primitives, providing the necessary perceptual-to-action mapping for the LLM-generated code to execute in the real world. No internal probing of joint vision-language representations is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Perception quality and calibration between camera and robot base are emphasized as necessary for sim-to-real transfer; the paper reports a calibrated extrinsic transform and describes pose estimation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Perception models operate zero-shot on open-world objects via grounding detectors and pose estimators; the paper uses them off-the-shelf.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1911.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CaP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-as-Policies (CaP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior code-generation approach that translates natural language instructions into executable robot code using fixed perception and control primitives in an open-loop manner; used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code as policies: Language model programs for embodied control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Code-as-Policies (CaP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach where an LLM generates procedural code (policies) that call a predefined set of perception/control primitives; operates open-loop without human-in-the-loop lifelong skill memory in the baseline setup.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (referenced work uses LLMs pretrained on text/code).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (code-as-policies baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as an open-loop code generation baseline for the Ravens and other manipulation tasks; generates flat task-specific code that calls primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper; CaP is described as relying on pre-defined primitives and prompts which can limit handling of ambiguities and long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported indirectly: CaP baseline performs worse than LYRA by up to ~27% (LYRA reports up to 27% higher success rate), but per-task numeric values for CaP are not exhaustively listed in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample efficiency numbers for CaP are provided in this paper; LYRA reports relative efficiency improvements compared to LLM-based closed-loop baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CaP relies on calling perception/control primitives; the paper does not analyze CaP's grounding mechanisms but contrasts CaP's open-loop code generation with LYRA's human-in-the-loop skill learning.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Uses in-context examples in prior work, but no specific zero/few-shot numbers reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper argues CaP-style approaches suffer on very long-horizon tasks due to noisy outputs and limited context windows; no quantitative negative-transfer measurements are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1911.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAHLIA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAHLIA (LLM-based closed-loop code generation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art code-generation framework with LLM-based closed-loop control used as a baseline in experiments; reported to sometimes produce false positives when LLM evaluates task success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DAHLIA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM-driven code-generation system that uses closed-loop LLM feedback for plan refinement; used here as a baseline for comparison on long-horizon Raven tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic code generation / long-horizon manipulation (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on customized Ravens tasks; the paper cites cases where LLM-only evaluation can produce false positives (incorrectly labeled successful executions).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not measured here; the paper highlights DAHLIA's tendency to overestimate success in some scenarios, indicating potential misalignment between LLM evaluation and human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported qualitatively as lower than LYRA; exact numeric success rates for DAHLIA on all tasks are not exhaustively reported within the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample efficiency numbers provided for DAHLIA in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not provided; paper contrasts DAHLIA's closed-loop LLM feedback (which can hallucinate) with human-in-the-loop skill acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not specifically quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper gives qualitative examples where DAHLIA's LLM-based verifier produces false positives, undermining reliability in long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1911.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoHoRavens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoHoRavens (long-horizon language-conditioned Ravens baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-horizon language-conditioned benchmark/approach for robotic tabletop manipulation; used here as a language-generation baseline that in prior work uses CLIPort as a pretrained RL execution policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LoHoRavens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoHoRavens (baseline using GPT + pretrained RL policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language-conditioned planning baseline: in prior work LoHoRavens uses LLMs to produce plans and uses a pretrained RL policy (CLIPort) for execution; in this paper LoHoRavens is included among baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Includes pretrained RL policy (CLIPort) for execution in referenced work; LoHoRavens itself is a benchmark/approach rather than a single pretrained multimodal model.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here; CLIPort and LoHoRavens pretrained on manipulation datasets in referenced works (details not enumerated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon tabletop manipulation (Ravens benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a baseline evaluated on the customized Ravens tasks; execution in prior LoHoRavens work uses a pretrained RL policy (CLIPort).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Included among baselines with lower performance than LYRA; exact numeric values not always listed in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not specifically reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>LoHoRavens in prior work grounds LLM plans via a pretrained RL execution policy, but this paper does not analyze that grounding in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>LoHoRavens uses few-shot in-context examples in related works; details not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1911.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven autonomous skill-learning agent originally developed for Minecraft; referenced as related work and as inspiration for an LLM-feedback variant of LYRA, but not directly used in robot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM-based agent which learns skills autonomously via environment interaction and automatic feedback (originally in Minecraft, Java-based); cited to contrast domain differences and motivate human-in-the-loop feedback for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not described in this paper (referenced work uses LLMs for interactive skill discovery in Minecraft).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Autonomous skill learning in embodied environments (Minecraft in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Originally designed for Minecraft with discrete action primitives; in this paper Voyager is referenced to argue that automatic LLM feedback transfers poorly to complex robotic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not applicable here; referenced qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper argues that directly porting Voyager-style LLM-only feedback to Python-based robotic manipulation is impractical and less reliable; no numerical comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not present here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not present here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Voyager's action grounding was in a simpler discrete-game domain; paper uses this contrast to justify human-in-the-loop learning for complex robot dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper notes differences in action complexity and dynamics between Minecraft and robotic manipulation hinder direct transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The paper qualitatively reports that relying solely on LLM feedback (Voyager-like) often fails for robotics due to complex dynamics and spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1911.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1911.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA / VLA mentions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-Language-Action (VLA) foundation models (e.g., OpenVLA, π0.5, GR00T)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced class of end-to-end vision-language-action foundation models that map multimodal inputs to motor commands or hierarchical plans; noted as promising but resource-intensive and data-starved for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vision-Language-Action foundation models (OpenVLA, π0.5, GR00T, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal models that aim to combine vision and language for robot control, either end-to-end or hierarchical; cited works include OpenVLA, π0.5 (Black et al.), and GR00T (Bjorck et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal vision-language-action pretraining (referenced papers); typically require large-scale multimodal robotic datasets or synthetic data; exact pretraining details are not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Paper states robotic data is far more limited than vision/text and that such VLA models typically need large-scale expert demonstrations and computational resources; no dataset names or content detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Generalist robotic control / vision-language-action tasks</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>End-to-end or hierarchical control for diverse robotic tasks; paper cites them as an alternative direction to LLM-as-code but notes practical constraints (data, compute, deployment on edge).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper discusses at a high level that VLA models aim to align perception and language but are limited by scarcity of robotic training data and compute requirements; no quantitative overlap analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper (these are cited references).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper argues that VLA approaches require large data/compute and suffer from data sparsity, but does not provide quantitative sample-efficiency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here (these are related works).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Cited as an active research direction; the paper does not report internal evidence from these models, only notes that they attempt integrated vision-language-action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper suggests transfer is limited by availability of large-scale robotic data and the difficulty of deploying very large models on edge devices.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper points out practical limitations (data scarcity, compute) but does not report negative-transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Code as policies: Language model programs for embodied control <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>LoHoRavens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation <em>(Rating: 2)</em></li>
                <li>0 : A vision-language-action flow model for general robot control <em>(Rating: 1)</em></li>
                <li>Gr00t n1: An open foundation model for generalist humanoid robots <em>(Rating: 1)</em></li>
                <li>OpenVLA: An open-source vision-language-action model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1911",
    "paper_id": "paper-281496722",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "LYRA",
            "name_full": "LYRA: A Lifelong learning code sYnthesis framework with human-in-the-loop for Robotic long-horizon skill Acquisition",
            "brief_description": "Human-in-the-loop code-generation framework that encodes human corrections into reusable, named skills stored in an external memory and uses RAG + hint guidance to enable robust long-horizon robotic manipulation planning and transfer to real robots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LYRA (framework using OpenAI GPT-4o for code-as-policies)",
            "model_description": "A system-level framework where an LLM (GPT-4o) synthesizes Python 'code-as-policies' that call deterministic perception and control primitives; the framework augments the prompt with retrieved few-shot examples and skill headers from an external memory (ChromaDB + text-embeddings-3) and supports human-in-the-loop iterative refinement and skill encapsulation (named Python functions) rather than only prompt edits.",
            "pretraining_type": "text-only language model (GPT-4o) used for code generation; retrieval uses text embedding models (text-embeddings-3).",
            "pretraining_data_description": "Not specified in this paper; GPT-4o is used as an off-the-shelf LLM (implicitly pretrained on large code+text corpora); retrieval embeddings are from OpenAI text embeddings. The paper does not detail GPT-4o pretraining corpora or whether vision-language pretraining was used for the LLM.",
            "target_task_name": "Long-horizon robotic manipulation (tabletop rearrangement / structure building)",
            "target_task_description": "Benchmarks: customized Ravens (PyBullet), Franka Kitchen, MetaWorld, and real-world Franka FR3 deployments; tasks include pick-and-place primitives and long-horizon composition (e.g., 'build a house', 'stack a Jenga tower', writing 'ICLR'). Action space: continuous 6-DOF end-effector motions realized via deterministic primitives (pick/place, move_end_effector_to, etc.). Environments: simulation (privileged object state) and real world with vision-based perception; tasks require composing &gt;20 primitives in extreme cases.",
            "semantic_alignment": "Discussed qualitatively: the framework enforces semantic alignment by (1) encoding human corrections as named skill functions (preserving semantics), (2) storing examples and skill docstrings in a retrieval database indexed by instruction-embeddings, and (3) using perception outputs (masks, 6D poses) to ground skill parameters. The paper does not present a quantitative overlap analysis between LLM pretraining data and task objects/actions.",
            "performance_with_language_pretraining": "LYRA (using GPT-4o) reported an overall success rate of 0.93 averaged across evaluated tasks, cited as up to 27% higher success rate than baselines, and a 42% improvement in feedback-round efficiency (fewer correction rounds). Metrics are reported as success rate (proportion of successful trials) and relative improvement in number of human/feedback rounds.",
            "performance_without_language_pretraining": "No experiment with a non-language pretrained core (e.g., training a model from scratch) is reported; baseline systems (CaP, LoHoRavens, DAHLIA) — which rely on LLMs or pretrained policies — achieve lower success rates (up to 27% lower than LYRA), but exact baseline numeric success rates for each task are not listed comprehensively in the main text.",
            "sample_efficiency_comparison": "The paper reports a 42% efficiency improvement in correction rounds for LYRA (human-in-the-loop skill learning) vs LLM-based closed-loop baselines (fewer human correction interactions required). No absolute counts (e.g., episodes to reach X% success) are consistently reported across tasks in the main text; plots in the appendix/figures show average number of corrections but exact numeric tables are not inlined.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention- or saliency-style analyses are reported (the paper does not visualize LLM attention or attention over visual input).",
            "embedding_space_analysis": "No analysis of embedding space structure or clustering (beyond using text-embeddings-3 for RAG retrieval) is reported.",
            "action_grounding_evidence": "Behavioral/perceptual grounding evidence is indirect and functional: skills produced by the LLM call perception primitives (object masks/poses from vision models and core pick/place primitives) and are executed on the robot; successful transfer to a Franka FR3 and robust task completion (including long-horizon 'build a house') provide empirical evidence that language-conditioned code maps to perceptual/motor behavior, but the paper provides no internal representational analysis linking LLM tokens/embeddings to specific affordances or motor primitives.",
            "hierarchical_features_evidence": "No neural hierarchical feature analysis is provided. The paper operationalizes hierarchy at the system level (skills can call other skills, nested functions), but does not analyze representation hierarchy in models.",
            "transfer_conditions": "Qualitative factors affecting transfer success: (1) consistent primitive API between simulation and real robot (same function names and parameter conventions), (2) reliable perception pipeline (Grounded SAM 2 / GroundingDINO + FoundationPose) to estimate object masks and 6D poses, (3) user-designed curriculum and human hints to guide skill expansion. The paper reports successful sim-to-real transfer when these conditions hold, but does not provide an ablation quantifying sensitivity.",
            "novel_vs_familiar_objects": "No rigorous numeric comparison between objects/actions seen during skill learning vs held-out novel objects; qualitative claim: framework generalizes to unseen tasks such as 'build a temple' within ~5 rounds of user guidance by recombining existing skills.",
            "zero_shot_or_few_shot": "The system uses few-shot in-context learning: retrieval returns K=10 most similar examples to add to prompts. The paper demonstrates few-shot adaptation via retrieved examples and learned skills; pure zero-shot success is not claimed for complex long-horizon tasks.",
            "layer_analysis": "No layer-wise or component ablation analyses of the LLM are reported (the paper treats GPT-4o as an oracle/code generator).",
            "negative_transfer_evidence": "No explicit evidence of negative transfer reported. The paper does caution that naively overwriting prompts (or saving corrected flat code) can cause catastrophic forgetting, which LYRA mitigates by storing skills in external memory.",
            "comparison_to_vision_only": "No direct experimental comparison to vision-only pretrained models (e.g., ImageNet-pretrained perception+non-language controllers). The paper compares to other code-generation/LLM-based baselines rather than pure vision-only baselines.",
            "temporal_dynamics": "No analysis of representational or performance dynamics during fine-tuning of models; training is not performed on the core LLM—learning occurs by adding skills/examples to memory and iterative human-guided refinement.",
            "dimensionality_analysis": "No dimensionality or intrinsic-dimension analyses are reported.",
            "uuid": "e1911.0"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "OpenAI GPT-4o (used as embodied code generator)",
            "brief_description": "Off-the-shelf large language model used in this work to generate Python code (policy programs) from natural language instructions and retrieved context; treated as the core code-generation and planning engine.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A large pretrained language model used to synthesize robot-executable Python code and perform task decomposition and planning via in-context learning and RAG-augmented prompts. The paper uses GPT-4o as a text/code generative model rather than performing additional multimodal pretraining.",
            "pretraining_type": "Text/code language modeling pretraining (not trained in this paper).",
            "pretraining_data_description": "Not detailed in this paper; implicitly code+text corpora used by GPT-family models. The paper does not describe GPT-4o's pretraining dataset.",
            "target_task_name": "Code generation for robotic manipulation",
            "target_task_description": "Generates task-specific Python code calling perception and control primitives for tabletop manipulation tasks (sim and real), using in-context skill examples and retrieved skill headers.",
            "semantic_alignment": "Not analyzed quantitatively in the paper; alignment is achieved procedurally by human corrections and encapsulating functionality into named skills to preserve semantics.",
            "performance_with_language_pretraining": "As part of LYRA, GPT-4o yielded the framework's reported success rate (0.93) and sample-efficiency gains, but the paper does not separately report GPT-4o standalone metrics.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "Not provided per-model (metrics reported at framework level).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not performed in this paper.",
            "embedding_space_analysis": "Not performed in this paper.",
            "action_grounding_evidence": "Used in a code-as-policies pattern where generated code calls perception-to-action primitives; grounding is demonstrated behaviorally (successful task execution) but not analyzed at representational level.",
            "hierarchical_features_evidence": "No layer/feature analysis reported.",
            "transfer_conditions": "Paper reports that GPT-4o-generated code transfers when backed by a stable primitive API and reliable perception (same prompt/skill headers across sim and real).",
            "novel_vs_familiar_objects": "Not analyzed for GPT-4o specifically.",
            "zero_shot_or_few_shot": "Operates with few-shot examples provided in the prompt; the system retrieves up to K=10 examples for in-context adaptation.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not shown.",
            "comparison_to_vision_only": "Not applicable (GPT-4o is a language model).",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1911.1"
        },
        {
            "name_short": "Grounded-SAM2+FoundationPose",
            "name_full": "Grounded SAM 2 / GroundingDINO + FoundationPose perception pipeline",
            "brief_description": "A vision foundation-model pipeline used to detect object masks/bounding boxes (Grounded SAM 2 / GroundingDINO) and estimate 6D object poses (FoundationPose) to provide open-world perceptual inputs for skill parameterization and sim-to-real transfer.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Grounded SAM 2 + GroundingDINO + FoundationPose",
            "model_description": "Vision foundation models: Grounded SAM 2 and GroundingDINO produce segmentation masks and bounding boxes; FoundationPose recovers 6D poses by combining masks with CAD models. The paper uses these pre-trained vision models as a deterministic perception frontend to extract object centers and orientations that feed into code-generated skills.",
            "pretraining_type": "Pretrained vision/vision-language foundation models for segmentation/detection and pose estimation (not trained in this paper).",
            "pretraining_data_description": "Not specified in this paper; these models are standard open-world segmentation/detection and pose models trained on large vision datasets. The paper does not enumerate datasets or affordance annotations.",
            "target_task_name": "Perception for real-world robotic manipulation",
            "target_task_description": "Object detection and 6D pose estimation in real tabletop scenes captured by a RealSense D435i, used to parameterize pick-and-place and assembly primitives for a Franka FR3 robot.",
            "semantic_alignment": "Implicit: these perception modules produce object-level masks and 6D poses that align with the skill API (object poses, sizes, colors). No quantitative overlap analysis is provided.",
            "performance_with_language_pretraining": "Not applicable (vision models). The paper reports successful use in real-world demos but does not quantify perception error rates or their effect on final task success in tables.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported for the perception models.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Operational grounding: perception outputs are directly used to compute pick/place poses and feed deterministic primitives, providing the necessary perceptual-to-action mapping for the LLM-generated code to execute in the real world. No internal probing of joint vision-language representations is provided.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Perception quality and calibration between camera and robot base are emphasized as necessary for sim-to-real transfer; the paper reports a calibrated extrinsic transform and describes pose estimation pipeline.",
            "novel_vs_familiar_objects": "Not analyzed quantitatively.",
            "zero_shot_or_few_shot": "Perception models operate zero-shot on open-world objects via grounding detectors and pose estimators; the paper uses them off-the-shelf.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1911.2"
        },
        {
            "name_short": "CaP",
            "name_full": "Code-as-Policies (CaP)",
            "brief_description": "Prior code-generation approach that translates natural language instructions into executable robot code using fixed perception and control primitives in an open-loop manner; used here as a baseline.",
            "citation_title": "Code as policies: Language model programs for embodied control",
            "mention_or_use": "use",
            "model_name": "Code-as-Policies (CaP)",
            "model_description": "An approach where an LLM generates procedural code (policies) that call a predefined set of perception/control primitives; operates open-loop without human-in-the-loop lifelong skill memory in the baseline setup.",
            "pretraining_type": "Not specified in this paper (referenced work uses LLMs pretrained on text/code).",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "Robotic manipulation (code-as-policies baseline)",
            "target_task_description": "Used as an open-loop code generation baseline for the Ravens and other manipulation tasks; generates flat task-specific code that calls primitives.",
            "semantic_alignment": "Not analyzed in this paper; CaP is described as relying on pre-defined primitives and prompts which can limit handling of ambiguities and long-horizon tasks.",
            "performance_with_language_pretraining": "Reported indirectly: CaP baseline performs worse than LYRA by up to ~27% (LYRA reports up to 27% higher success rate), but per-task numeric values for CaP are not exhaustively listed in the main text.",
            "performance_without_language_pretraining": "Not applicable here.",
            "sample_efficiency_comparison": "No explicit sample efficiency numbers for CaP are provided in this paper; LYRA reports relative efficiency improvements compared to LLM-based closed-loop baselines.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "CaP relies on calling perception/control primitives; the paper does not analyze CaP's grounding mechanisms but contrasts CaP's open-loop code generation with LYRA's human-in-the-loop skill learning.",
            "hierarchical_features_evidence": "Not discussed.",
            "transfer_conditions": "Not analyzed in this paper.",
            "novel_vs_familiar_objects": "Not analyzed.",
            "zero_shot_or_few_shot": "Uses in-context examples in prior work, but no specific zero/few-shot numbers reported here.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Paper argues CaP-style approaches suffer on very long-horizon tasks due to noisy outputs and limited context windows; no quantitative negative-transfer measurements are provided.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1911.3"
        },
        {
            "name_short": "DAHLIA",
            "name_full": "DAHLIA (LLM-based closed-loop code generation framework)",
            "brief_description": "A state-of-the-art code-generation framework with LLM-based closed-loop control used as a baseline in experiments; reported to sometimes produce false positives when LLM evaluates task success.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DAHLIA",
            "model_description": "An LLM-driven code-generation system that uses closed-loop LLM feedback for plan refinement; used here as a baseline for comparison on long-horizon Raven tasks.",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "Robotic code generation / long-horizon manipulation (baseline)",
            "target_task_description": "Evaluated on customized Ravens tasks; the paper cites cases where LLM-only evaluation can produce false positives (incorrectly labeled successful executions).",
            "semantic_alignment": "Not measured here; the paper highlights DAHLIA's tendency to overestimate success in some scenarios, indicating potential misalignment between LLM evaluation and human preferences.",
            "performance_with_language_pretraining": "Reported qualitatively as lower than LYRA; exact numeric success rates for DAHLIA on all tasks are not exhaustively reported within the main text.",
            "performance_without_language_pretraining": "Not applicable here.",
            "sample_efficiency_comparison": "No explicit sample efficiency numbers provided for DAHLIA in this work.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not provided; paper contrasts DAHLIA's closed-loop LLM feedback (which can hallucinate) with human-in-the-loop skill acquisition.",
            "hierarchical_features_evidence": "Not present.",
            "transfer_conditions": "Not discussed.",
            "novel_vs_familiar_objects": "Not analyzed.",
            "zero_shot_or_few_shot": "Not specifically quantified here.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Paper gives qualitative examples where DAHLIA's LLM-based verifier produces false positives, undermining reliability in long-horizon tasks.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1911.4"
        },
        {
            "name_short": "LoHoRavens",
            "name_full": "LoHoRavens (long-horizon language-conditioned Ravens baseline)",
            "brief_description": "A long-horizon language-conditioned benchmark/approach for robotic tabletop manipulation; used here as a language-generation baseline that in prior work uses CLIPort as a pretrained RL execution policy.",
            "citation_title": "LoHoRavens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation",
            "mention_or_use": "use",
            "model_name": "LoHoRavens (baseline using GPT + pretrained RL policy)",
            "model_description": "Language-conditioned planning baseline: in prior work LoHoRavens uses LLMs to produce plans and uses a pretrained RL policy (CLIPort) for execution; in this paper LoHoRavens is included among baseline comparisons.",
            "pretraining_type": "Includes pretrained RL policy (CLIPort) for execution in referenced work; LoHoRavens itself is a benchmark/approach rather than a single pretrained multimodal model.",
            "pretraining_data_description": "Not specified here; CLIPort and LoHoRavens pretrained on manipulation datasets in referenced works (details not enumerated in this paper).",
            "target_task_name": "Long-horizon tabletop manipulation (Ravens benchmark)",
            "target_task_description": "Used as a baseline evaluated on the customized Ravens tasks; execution in prior LoHoRavens work uses a pretrained RL policy (CLIPort).",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Included among baselines with lower performance than LYRA; exact numeric values not always listed in main text.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "Not specifically reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "LoHoRavens in prior work grounds LLM plans via a pretrained RL execution policy, but this paper does not analyze that grounding in detail.",
            "hierarchical_features_evidence": "Not present.",
            "transfer_conditions": "Not discussed in this paper.",
            "novel_vs_familiar_objects": "Not analyzed.",
            "zero_shot_or_few_shot": "LoHoRavens uses few-shot in-context examples in related works; details not enumerated here.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not discussed here.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1911.5"
        },
        {
            "name_short": "Voyager (mentioned)",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An LLM-driven autonomous skill-learning agent originally developed for Minecraft; referenced as related work and as inspiration for an LLM-feedback variant of LYRA, but not directly used in robot experiments.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "model_name": "Voyager",
            "model_description": "An LLM-based agent which learns skills autonomously via environment interaction and automatic feedback (originally in Minecraft, Java-based); cited to contrast domain differences and motivate human-in-the-loop feedback for robotics.",
            "pretraining_type": "Not described in this paper (referenced work uses LLMs for interactive skill discovery in Minecraft).",
            "pretraining_data_description": "Not provided here.",
            "target_task_name": "Autonomous skill learning in embodied environments (Minecraft in original work)",
            "target_task_description": "Originally designed for Minecraft with discrete action primitives; in this paper Voyager is referenced to argue that automatic LLM feedback transfers poorly to complex robotic dynamics.",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Not applicable here; referenced qualitatively.",
            "performance_without_language_pretraining": "Not applicable.",
            "sample_efficiency_comparison": "Paper argues that directly porting Voyager-style LLM-only feedback to Python-based robotic manipulation is impractical and less reliable; no numerical comparison provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not present here.",
            "embedding_space_analysis": "Not present here.",
            "action_grounding_evidence": "Voyager's action grounding was in a simpler discrete-game domain; paper uses this contrast to justify human-in-the-loop learning for complex robot dynamics.",
            "hierarchical_features_evidence": "Not discussed here.",
            "transfer_conditions": "Paper notes differences in action complexity and dynamics between Minecraft and robotic manipulation hinder direct transfer.",
            "novel_vs_familiar_objects": "Not applicable.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "The paper qualitatively reports that relying solely on LLM feedback (Voyager-like) often fails for robotics due to complex dynamics and spatial reasoning.",
            "comparison_to_vision_only": "Not applicable.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1911.6"
        },
        {
            "name_short": "OpenVLA / VLA mentions",
            "name_full": "Vision-Language-Action (VLA) foundation models (e.g., OpenVLA, π0.5, GR00T)",
            "brief_description": "Referenced class of end-to-end vision-language-action foundation models that map multimodal inputs to motor commands or hierarchical plans; noted as promising but resource-intensive and data-starved for robotics.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Vision-Language-Action foundation models (OpenVLA, π0.5, GR00T, etc.)",
            "model_description": "Large multimodal models that aim to combine vision and language for robot control, either end-to-end or hierarchical; cited works include OpenVLA, π0.5 (Black et al.), and GR00T (Bjorck et al.).",
            "pretraining_type": "Multimodal vision-language-action pretraining (referenced papers); typically require large-scale multimodal robotic datasets or synthetic data; exact pretraining details are not given in this paper.",
            "pretraining_data_description": "Paper states robotic data is far more limited than vision/text and that such VLA models typically need large-scale expert demonstrations and computational resources; no dataset names or content detailed here.",
            "target_task_name": "Generalist robotic control / vision-language-action tasks",
            "target_task_description": "End-to-end or hierarchical control for diverse robotic tasks; paper cites them as an alternative direction to LLM-as-code but notes practical constraints (data, compute, deployment on edge).",
            "semantic_alignment": "Paper discusses at a high level that VLA models aim to align perception and language but are limited by scarcity of robotic training data and compute requirements; no quantitative overlap analysis is provided.",
            "performance_with_language_pretraining": "Not reported in this paper (these are cited references).",
            "performance_without_language_pretraining": "Not applicable.",
            "sample_efficiency_comparison": "Paper argues that VLA approaches require large data/compute and suffer from data sparsity, but does not provide quantitative sample-efficiency comparisons.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here (these are related works).",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Cited as an active research direction; the paper does not report internal evidence from these models, only notes that they attempt integrated vision-language-action grounding.",
            "hierarchical_features_evidence": "Not discussed here.",
            "transfer_conditions": "Paper suggests transfer is limited by availability of large-scale robotic data and the difficulty of deploying very large models on edge devices.",
            "novel_vs_familiar_objects": "Not discussed here.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Paper points out practical limitations (data scarcity, compute) but does not report negative-transfer experiments.",
            "comparison_to_vision_only": "Not directly compared here.",
            "temporal_dynamics": "Not analyzed here.",
            "dimensionality_analysis": "Not analyzed here.",
            "uuid": "e1911.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Code as policies: Language model programs for embodied control",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "LoHoRavens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation",
            "rating": 2
        },
        {
            "paper_title": "0 : A vision-language-action flow model for general robot control",
            "rating": 1
        },
        {
            "paper_title": "Gr00t n1: An open foundation model for generalist humanoid robots",
            "rating": 1
        },
        {
            "paper_title": "OpenVLA: An open-source vision-language-action model",
            "rating": 1
        }
    ],
    "cost": 0.022983749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Under review as a conference paper GROWING WITH YOUR EMBODIED AGENT: A HUMAN-IN-THE-LOOP LIFELONG CODE GENERATION FRAMEWORK FOR LONG-HORIZON MANIPULATION SKILLS
25 Sep 2025</p>
<p>Yuan Meng y.meng@tum.de 
Zhenguo Sun zgsun@baai.ac.cn 
Max Fest max.fest@tum.de 
Xukun Li xukun.li@gmail.com 
Zhenshan Bing bing@nju.edu.cn 
Alois Knoll </p>
<p>School of Computation, Information and Technology
Technical University of Munich Munich
Germany</p>
<p>Beijing Academy of Artificial Intelligence (BAAI)
BeijingChina</p>
<p>School of Computation, Information and Technology
Technical University of Munich Munich
Germany</p>
<p>School of Computation, Information and Technology
Technical University of Munich Munich
Germany</p>
<p>The State Key Laboratory for Novel Software Technology
Nanjing University
SuzhouChina</p>
<p>School of Computation, Information and Technology
Technical University of Munich Munich
Germany</p>
<p>Under review as a conference paper GROWING WITH YOUR EMBODIED AGENT: A HUMAN-IN-THE-LOOP LIFELONG CODE GENERATION FRAMEWORK FOR LONG-HORIZON MANIPULATION SKILLS
25 Sep 2025DE39874912B1CAFFB16043EE3C676D17arXiv:2509.18597v2[cs.RO]
Large language models (LLMs)-based code generation for robotic manipulation has recently shown promise by directly translating human instructions into executable code, but existing approaches are limited by language ambiguity, noisy outputs, and limited context windows, which makes long-horizon tasks hard to solve.While closed-loop feedback has been explored, approaches that rely solely on LLM guidance frequently fail in extremely long-horizon scenarios due to LLMs' limited reasoning capability in the robotic domain, where such issues are often simple for humans to identify.Moreover, corrected knowledge is often stored in improper formats, restricting generalization and causing catastrophic forgetting, which highlights the need for learning reusable and extendable skills.To address these issues, we propose a human-in-the-loop lifelong skill learning and code generation framework that encodes feedback into reusable skills and extends their functionality over time.An external memory with Retrieval-Augmented Generation and a hint mechanism supports dynamic reuse, enabling robust performance on long-horizon tasks.Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world settings, show that our framework achieves a 0.93 success rate (up to 27% higher than baselines) and a 42% efficiency improvement in feedback rounds.It can robustly solve extremely long-horizon tasks such as "build a house", which requires planning over 20 primitives. 1 * Corresponding author 1 Code will be open-sourced upon acceptance.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) and vision-language models (VLMs) have become integral to robotic manipulation due to their robust commonsense knowledge and advanced reasoning capabilities.Early approaches Co-Reyes et al. (2018); Lynch et al. (2023); Liu et al. (2023) relied on language embeddings conditioned within reinforcement learning or imitation learning to align robot actions with human commands.These methods often struggled with limited data efficiency and poor generalization.With the rapid progress of LLMs such as GPT, a natural direction has been to integrate them into the pipeline for task decomposition and language grounding Zhang et al. (2023); Huang et al. (2023); Guo et al. (2024).In this setting, an LLM decomposes a complex manipulation task into sub-tasks and invokes a pre-trained language-conditioned policy to execute low-level primitives.This approach assumes that the pre-trained policy can carry out each motion precisely, yet in practice, this is rarely possible due to environmental perturbations and imperfect policy design.Another direction for advancing human-level robotic manipulation is to adopt LLM or VLM backbones for large-scale pretraining on robotic data, creating end-to-end vision-language-action (VLA) foundation models Kim et al. (2024); Black et al. (2024); Bjorck et al. (2025).However, robotic data is far more limited than in computer vision or natural language, leading to data sparsity.Training such large models (often with at least 7B parameters) also demands enormous computational resources and makes deployment on edge devices difficult.These factors have slowed progress in this line of research.Alternatively, well-pretrained LLMs already possess strong instruction-following capabilities and can directly generate policy code as an action representation for robot control.</p>
<p>Using LLMs for code generation in robotic manipulation Zhang et al. (2025) has shown strong potential in embodied AI.Approaches like Code-as-Policies (CaP) Liang et al. (2023); Chen et al. (2024); Mu et al. (2024) translate human instructions into executable Python code with fixed sets of perception and control primitives, enabling rapid deployment of behaviors.Despite this progress, current methods face key limitations: (1) LLM outputs are often noisy and error-prone in longhorizon planning.(2) Natural language is inherently ambiguous, so the agent may not always capture the user's intention.(3) Agents are restricted by pre-defined primitives and handcrafted prompts, and the limited context window prevents scaling with many examples.As a result, agents struggle with complex long-horizon tasks.Prior works Liang et al. (2024); Zhi et al. (2025); Meng et al. (2025) incorporate feedback to improve robustness, but these do not constitute true "learning", since verifiers must repeat the same feedback for seen tasks in the future.In code generation, learning can be enabled by dynamically adjusting prompts to support LLM in-context adaptation.This raises two key questions.The first question is how to store and reuse knowledge from feedback.Updating the prompt directly Arenas et al. (2024) can cause catastrophic forgetting, where performance on earlier tasks drops sharply.Extracting insights across tasks Zhao et al. (2024); Zha et al. (2024) produces ambiguous, task-specific knowledge, lacking in generalization to unseen tasks.Saving flat code offers flexibility for modification, but repeated changes often introduce noise and errors, making long-horizon generation unstable.A more stable alternative is to encapsulate temporally extended behaviors as skill functions, where adjusting function parameters offers both flexibility and generalization while preserving stability.This makes skills a promising choice for storing feedback knowledge.The second question is what type of feedback is most efficient during interaction.Language feedback is a convenient interface, but feedback generated solely by LLMs Arenas et al. (2024); Wang et al. (2023); Meng et al. (2025) is often unreliable, as it may diverge from human preferences or fail in extremely long-horizon reasoning Wang et al. (2023).Human-provided feed-back, in contrast, is more robust: errors in robotic tasks are usually easy for humans to identify, evaluate, and correct.Moreover, the feedback can be tested and verified within seconds in robotic code generation, whereas training traditional deep learning methods often requires hours or even days.This reliability and efficiency highlight the unique advantage of involving humans to guide robotic skill learning.</p>
<p>Motivated by these questions, we propose a human-in-the-loop framework that encodes user feedback into reusable skills and extends their functionality through a user-designed curriculum, enabling preference-aligned lifelong skill learning (Fig. 1).Learned skills and examples are stored in an external memory to prevent catastrophic forgetting and support reuse.For long-horizon task planning, our framework employs Retrieval-Augmented Generation (RAG) to retrieve relevant skills and examples, enabling dynamic in-context learning.In addition, a simple yet effective hint mechanism allows users to guide the agent when retrieval alone is insufficient, ensuring that only the most relevant skills are applied, thereby improving efficiency and reducing interference from irrelevant data.We validate the effectiveness of the framework through experiments in both simulation benchmarks (Ravens, Franka-Kitchen, and MetaWorld) and real-world settings using a Franka FR3 across diverse long-horizon tasks.Empirical analysis shows a 0.93 success rate, up to 27% higher than baselines, and a 42% efficiency improvement in correction rounds compared with LLM-based closed-loop methods.Notably, the framework can robustly solve extremely long-horizon tasks such as "build a house", which requires planning over 20 primitives.</p>
<p>To summarize, our contributions are threefold: (1) Human-in-the-loop lifelong skill learning: A framework that incorporates human corrections and a user-designed curriculum to extend skills while preserving previous functionalities, ensuring preference-aligned lifelong learning.(2) Memory-augmented skill retrieval: An external memory that stores learned skills and examples, combined with RAG and user-provided "hints" for dynamic in-context learning.(3) Challenging extreme long-horizon tasks: Demonstrating the first successful solution to the "build a house" task requiring over 20 primitives, validated across both simulation and real-world settings.We call our framework LYRA: A Lifelong learning code sYnthesis framework with human-in-the-loop for Robotic long-horizon skill Acquisition.</p>
<p>METHOD</p>
<p>In this section, we first introduce the preliminaries, then describe how our framework learns skills and applies them to downstream tasks.</p>
<p>PRELIMINARIES</p>
<p>Code generation for robotic manipulation can be modelled as using an LLM f to map a natural language instruction l to a robot control code c, i.e., f (l) = c, where f is termed a Language Model Program (LMP) in prior work Liang et al. (2023).Each code c is achieved by multiple behaviours b; thus, f can also be viewed as a mapping from l to b.A behaviour b is defined as a desirable or semantically meaningful motion (e.g., picking up an object), and the set of all such behaviours is denoted by B. In our work, we argue that behaviours b requiring no variation can be encapsulated as skills that reliably elicit these behaviours.We denote the skill space as Z and state parameters as s.Importantly, a skill z ∈ Z may also build on other skills:
Definition 1: A skill z ∈ Z is a function b ∼ z(s) which induces a specific behaviour b.
It is worth noting that although the task code c generated by the agent represents a sequence of behaviors to accomplish a task, it is not equivalent to a skill.We define a task (l, s 0 ) by a natural language instruction l and an initial environment state s 0 , which together form the main interface between the user and the agent.When a task is successfully completed with user feedback, we extend this tuple to (l, s 0 , c, s T ), where c is the task-specific code (often flat, process-oriented) and s T is the achieved final state.In practice, c may call skills z to complete the task, and its execution should produce an evaluable task result.</p>
<p>To align the skill output with the user's desired behaviour b, the agent adjusts the function details under user guidance.We call this process learning the skill.Importantly, here, learning does not refer to neural network fitting on data, but instead follows a broader concept in computer science Mitchell (2006): Definition 2: An agent is said to learn if it produces better behavioural responses b to an instruction l, as evaluated by a human.</p>
<p>Based on these definitions, the learning pipeline should answer: (1) How to encode human-provided feedback into skills that align with user preferences?(2) How to extend skills through lifelong learning while enabling dynamic in-context adaptation?(3) How to reuse and transfer learned skills to unseen long-horizon tasks?</p>
<p>Meta-prompt</p>
<p>You should try to preserve the previous functionality.</p>
<p>Real-world scenarios validation</p>
<p>PREFERENCE-ALIGNED SKILL ACQUISITION</p>
<p>Based on the above questions, we design a three-phase pipeline to preserve and utilize human-in-theloop corrections for preference-aligned lifelong skill learning in long-horizon manipulation (Fig. 2).The phases are: (1) preference-aligned skill acquisition, (2) lifelong capability extension, and (3) task-specific retrieval and planning.</p>
<p>To learn a new skill, three elements must be clarified:</p>
<p>(1) what skill to learn, (2) the initial task environment, and (3) the expected functionality or behavior.Asking an LLM to infer all of this at once is impractical due to language ambiguity, so we adopt multi-turn interaction.The agent is initiated with a fixed set of skills z ∈ Z 0 , along with examples e = (l, c) ∈ E 0 that demonstrate their use, enabling basic environment interaction.The user first describes the skill in natural language, and the LLM invokes the "skill parse" API to generate a Python function header with suggested parameters (e.g., "def stack blocks(blocks, start pose):").The user can accept or refine this definition.Once accepted, the LLM requests a base environment setup, and generates a corresponding task scenario.The complexity of the task is user-controlled, but overly complex tasks (over 6-8 steps) hinder skill acquisition when the agent's ability is still limited.We therefore recommend two principles: (1) tasks should be simple, ideally completed within 1-4 primitives, and (2) tasks should allow flexibility so that one skill can adapt to diverse requirements.After the setup, the user and the LLM agent will work together to "learn" the skill.The agent generates candidate code, deploys it in simulation, and displays results in real time.The user then accepts, rejects, or provides free-form feedback to refine the skill.User tests multiple requirements within the same scenario to check if the skill and its parameters generalize.For example, in Fig. 2 Phase I, the user guides the agent to stack blocks under different conditions such as edge alignment, tower positioning, or rotation.During this phase, the agent can freely modify implementation details under the reserved skill name, with the objective of producing a functional, preference-aligned skill.This forms the foundation for later phases of lifelong skill learning and capability extension.</p>
<p>LIFELONG LEARNING-ORIENTED CAPABILITY EXTENSION</p>
<p>Learning skills from a single task scenario is insufficient for complex long-horizon tasks, for two main reasons: (1) Simple skills cannot handle variations beyond their learning distribution.For example, a skill like "stack blocks" for stacking four blocks at a fixed pose cannot generalize to tasks such as building a {i×j ×k} structure or stacking multiple towers by color or size.</p>
<p>(2) Directly inserting learned skills into prompts risks hallucination, where LLMs may rewrite or fabricate code during long-horizon interactive planning, leading to catastrophic forgetting of existing skills.</p>
<p>Inspired by lifelong learning, we extend the pipeline with a user-designed curriculum that enables bottom-up skill functionality expansion (Fig. 2 Phase II).In this phase, the agent must: (1) solve more complex user-defined task variations, (2) preserve and extend skill functionality to unseen cases, and (3) store skills permanently for reuse.The learning process mirrors Phase I, with an added meta-prompt that explicitly asks the agent to preserve prior functionality while adapting to new tasks.Users should design tasks outside the original distribution and aligned with long-term goals, guiding the agent to acquire foundational skills for future complex tasks.The agent can either (1) create a new named skill that calls existing ones as nested functions, or (2) extend the current skill using modularized code (e.g., if-else or match-case) to avoid overwriting.After adapting to new tasks, the agent is re-evaluated on prior tasks; if performance is preserved, we can say that lifelong learning is achieved.To further prevent forgetting and hallucination, both learned skills z and explored successful examples (including task instruction and task plan) (l, c) ∈ E are stored in external memory and retrieved via RAG during task planning.</p>
<p>TASK-SPECIFIC-ORIENTED RETRIEVAL AND PLANNING</p>
<p>In Fig. 2 Phase III, we enable long-horizon task-specific planning through in-context learning, adapting general-purpose LLMs to specific tasks using two main prompt inputs: (1) few-shot examples E = {(l 1 , c 1 ), ..., (l M , c M )} that show mappings from instructions to task-specific code plan, and (2) a set of skills Z = {z 1 , ..., z N } that the agent can call to compose the plan c.Because of context window limits, both |E| = M and |Z| = N are fixed.The goal is that the examples and skills are representative enough for the agent to generalize to unseen pairs (l ′ , c ′ ), assuming l ′ reflects the human's intent and the LLM can interpolate from E and Z to produce the correct c ′ .</p>
<p>Key to our approach is dynamically managing the context window.As the number of examples grows, appending all of them to the prompt is infeasible, and many are irrelevant.For example, when solving "press the button", examples about "open the drawer" add noise and overwhelm the prompt.To address this, we use an external memory module with two vector databases: one for fewshot examples (l, c) indexed by their instructions, and one for skills z indexed by their docstrings.We implement this using ChromaDB and compute embeddings with OpenAI's text-embeddings-3 Neelakantan et al. (2022).For a new instruction l ′ , we retrieve the K = 10 most similar examples based on cosine similarity and append them to the prompt.We also retrieve the relevant skill headers from memory and include them in the prompt.Additionally, our framework introduces a simple yet efficient mechanism for guiding the agent with a shared language format: hints.Hints allow users to trigger retrieval from the library of known behaviours by specifying which previously learned skill may help with the current task.This is especially useful during skill learning and task planning, when the agent encounters unfamiliar instructions and must infer the required substeps, as in LLMbased planning.As the number of skills |Z| and behaviours |E| grows, most are irrelevant, and hints provide a way for the user to direct the agent toward the correct subset.If a required sub-behaviour</p>
<p>has not yet been learned and cannot be resolved with a hint, the failure signals the user to pause the current skill and teach the missing sub-behaviour first.</p>
<p>With this pipeline, our framework can tackle extremely challenging long-horizon tasks such as "build a house" and "stack a jenga tower", which require more than 20 planning steps (Fig. 2 Phase III left).We further validate our method on widely used benchmarks, including Metaworld and Franka Kitchen, covering over 20 task variations where the framework consistently solves all tasks (Fig. 2 Phase III middle).Finally, we deploy the framework in real-world settings on a franka FR3, successfully completing diverse tasks such as building a house, writing "ICLR", and generalizing to the novel task "build a temple" (Fig. 2 Phase III right).It is worth noting that although we present the learning pipeline in three phases, the process is not strictly linear.Whenever the agent cannot handle the current task, the user can roll back to the skill learning or extension phase and iterate until the agent meets the requirements.The overall skill learning process is summarized in Appendix A.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENT SETUP</p>
<p>Our work focuses on tabletop long-horizon manipulation tasks.For fair comparison, we include several code and language generation baselines in our experiments (More setup details in Appendix B).Recent work, Voyager Wang et al. (2023), explores skill learning with LLM-based automatic feedback, but it is designed for the Java-based game "Minecraft" with simple action primitives.</p>
<p>Adapting it for Python-based robotic manipulation is costly and impractical.Thus, we design a variation of our framework that mirrors their idea, where an LLM provides feedback instead of humans for final task plan generation.We also include a w/o memory version of our framework for ablation that simulates the human-in-the-loop updates at the prompt level Arenas et al. ( 2024 LYRA (Ours) w/ LLM feedback: A variation of our framework that relies on LLM feedback, where RGB-D scenes before and after execution are given to LLM to determine task success and provide feedback.This variation has access to our well-learned memory database.In the real world, we deploy the agents on a Franka FR3 and validate performance on challenging longhorizon tasks.As our simulation tasks rely on privileged information (e.g., object pose and size), we adopt state-of-the-art perception foundation models Ren et al. (2024); Wen et al. (2024) to obtain open-world object data.Details of the real-world deployment are provided in Appendix C.</p>
<p>LYRA (Ours</p>
<p>SKILL LEARNING WITH HUMAN-IN-THE-LOOP</p>
<p>To analyze how human-in-the-loop skill learning improves performance, we evaluate all models on the customized Ravens with six long-horizon tasks for a case study (Fig. 3 Relying only on success rate is limited, as it cannot capture alignment with user preferences.A key contribution of our approach is representing human preferences from language corrections in a form that can be preserved and reused through skill inheritance.Snapshots in Fig. 3 h), (i), the baseline places blocks at the target positions, but their poses and distances do not match user expectations.Unlike LLM-only feedback, which often labels sub-optimal results as successful, user interventions guide the agent to acquire precise skills (e.g., stacking corner-to-corner or aligning blocks with fixed spacing) that lead to more reliable and preference-aligned outcomes.Fig. 3(j), (k) vs. (l), (m) further highlights the reliability of human-in-the-loop skill learning.In this example case, we observe false positives from LLM evaluation, where incomplete tasks are incorrectly marked as successful.For instance, in Fig. 3(j), DAHLIA stacks long blocks in the same direction for a Jenga tower.Although clearly incorrect to a human, the LLM still labels it as correct, raising doubts about evaluation reliability.In contrast, our framework reuses the "build jenga layer" skill, which was learned earlier with human guidance in a simpler scene, to stack a stable Jenga tower.Results for all Ravens tasks refer to Appendix D and supplementary video.</p>
<p>Skill-oriented human-in-the-loop feedback is more efficient than task-specific flat code feedback from LLMs.Fig. 4 Average number of corrections (NoC) (↓) † Some tasks may not be completed within 10 feedback rounds.For example, in the hammer task, an incorrect grasp may cause the hammer to drop, which LLM-based closed-loop feedback systems may struggle to correct, often leading to task failure.</p>
<p>CAN YOU BUILD A HOUSE?</p>
<p>To demonstrate how user-guided lifelong learning expands skill capabilities and enables challenging long-horizon tasks, we present the case study "build a house."As shown in Fig. 5(a), the task required 12 skills developed bottom-up from core primitives (orange), including 7 learned specifically for house building (light green) and others inherited from prior tasks (yellow).Starting from the chaotic initial scene (Fig. database, enabling relevant information to be retrieved and appended to the prompt.This prevents catastrophic forgetting and gradually builds a complex skill tree for task planning.This integration of top-down planning and bottom-up skill learning relies on human verifiers' global awareness of which skills are necessary, a capability current LLM verifiers lack, as they tend to decompose tasks in a single direction and often get stuck in sub-optimal solutions.To the best of our knowledge, our framework is the first to successfully accomplish the "build a house" task, with full implementation details provided in Appendix E.</p>
<p>REAL-WORLD PERFORMANCE</p>
<p>The skills learned through our pipeline are embodiment-agnostic and can be deployed on heterogeneous robot arms, enabling smooth transfer to real-world settings.We provide ROS2-based control APIs for the Franka FR3 with consistent naming and variables to match the simulation environment.</p>
<p>As shown in Fig. 6(a)-(d), our framework allows the agent to robustly perform challenging longhorizon real-world tasks, including building a house, stacking a Jenga tower, and writing "ICLR".These tasks can last over 12 minutes, highlighting the framework's effectiveness in real-world deployment.We also test generalization to unseen tasks, such as "construct a temple".Within five rounds of user guidance, the agent reorganizes the scene, builds the temple base, layers, and head by retrieving and combining existing skills, and successfully completes the task.Full results, including "make a human face" and comparisons with baseline models in real-world settings, are provided in Appendix F and supplementary video.(2024).These approaches often suffer from environmental perturbations and the limited performance of pretrained policies.Some works also explore humans as verifiers who patch planning mistakes.For example, LMPC  2024), but the retrieved data often fail to stay aligned with the robot's current view, state, and goal.Our framework does not try to outcompute these giants; instead, it offers a different path by producing generalized reusable code skills that can be recombined on the fly, giving robust behaviour in unstructured environments.</p>
<p>CONCLUSION</p>
<p>We presented a human-in-the-loop lifelong skill learning framework that encodes user feedback into reusable and extendable skills, enabling agents to preserve knowledge, extend capabilities, and solve challenging long-horizon tasks such as building a house.Unlike prior closed-loop methods, our approach integrates skill inheritance, external memory, and a hint-guided retrieval mechanism to align learning with human preferences.Experiments across Ravens, Franka Kitchen, and Meta-World benchmarks, as well as real-world settings, demonstrate strong performance, achieving a 0.93 success rate (up to 27% higher than baselines) and a 42% efficiency improvement in correction rounds.Our framework robustly handles long-horizon tasks in both simulation and reality, including building a house, stacking a Jenga tower, writing "ICLR," and generalizing to constructing a temple.Future work will focus on incorporating advanced multimodal RAG for retrieval and extending to dual-arm collaboration and more complex robotic setups.</p>
<p>LLM USAGE STATEMENT</p>
<p>In this study, we employ OpenAI GPT-4o as the embodied agent for robot skill code generation.OpenAI ChatGPT was used solely for sentence-level polishing and grammar correction.All polished content was carefully proofread and verified by the authors for accuracy.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>An anonymous code repository is provided as an attachment.To support reproducibility, we include a pre-built skill memory ("trained") that allows readers to replicate our results.This code and data will also be open-sourced upon acceptance.put first on second The main pick-and-place primitive.It picks up an object at the specified Pose, lifts it vertically to a specified height, moves along the x-y plane to a point directly above the place Pose, then moves it down until it detects contact.</p>
<p>Appendices</p>
<p>move end effector to Moves the end effector the specified position, and suction gripper rotation.</p>
<p>We use python format strings for our prompts.</p>
<p>Listing 1: Main Actor prompts actor_system_prompt = f""" You write python code to control a robotic arm in a simulated environment, building on an existing API.Listing 2: Skill Learning prompts actor_skill_learning_system_prompt = f""" You write python code to control a robotic arm in a simulated environment, building on an existing API.</p>
<p>We are trying to learn skills, and are using different tasks to test and effectively learn a specific skill.</p>
<p>You will be given:</p>
<p>-a task for the robotic agent to solve -the skill you are supposed to use to solve the task You are supposed to complete the function, as well as flat, task-specific code, as follows:</p>
<p>def given_function(...) -&gt; ...: \"\"\" ... \"\"\" <function code> <task-specific code></p>
<p>For example:</p>
<p>-------------------------------------------------IN: task: "put the red block on the green block" skill: def put_block_on_other_block(block: TaskObject, otherBlock: TaskObject): \"\"\" places the block on top of otherBlock \"\"\" pass OUT: def put_block_on_other_block(block: TaskObject, otherBlock: TaskObject): \"\"\" places the block on top of otherBlock \"\"\" put_first_on_second(get_object_pose(block), get_object_pose(otherBlock)) red_block = get_block(color="red") green_block = get_block(color="green") put_block_on_other_block(red_block, green_block) - ------------------------------------------------If the new task requires you to rewrite the function header, you may do so, for example, to add arguments, or to update the docstring with important usage information.</p>
<p>You should try to preserve the previous functionality though, since the function might have previously been used to solve other tasks, which should remain solvable after changes.</p>
<p>DO NOT make any imports.DO NOT write any functions other than the given one.</p>
<p>Adhere to the following basic types: {get_core_types_text()} """ def skill_learning_prompt( task, few_shot_examples: list[TaskExample], skill: Skill, other_useful_skills: list[Skill], ):</p>
<p>return f""" The task is: {task} The function you are supposed to implement is:
{str(skill)} --------------------------------------------------------------- {get_few_shot_examples_string(few_shot_examples)}
The following skills may be useful in your implementation: {"\n\n".join([skill.descriptionfor skill in other_useful_skills])} - ---------------------------------------------------------------Implement the function and solve the task, while trying to ensure that prior tasks remain solvable."""</p>
<p>Listing 3: Task Setup prompts task_setup_api_string = """ def add_block( self, env: Environment, color=None, size: tuple[float, float, float] = (0.04, 0.04, 0.04), pose: Pose=None ): \"\"\" adds a block of a given size and color to the environment If the pose is left unspecified, a random collision-free pose is selected \"\"\" def add_zone( self, env: Environment, color: str, scale: float = 1, pose: Pose = None ): \"\"\" adds a zone of a given size and color to the environment If the pose is left unspecified, a random pose in the workspace is selected \"\"\" def add_cylinder(self, env: Environment, color: str = "red", scale: float = 0.5): \"\"\" adds a cylinder of a given scale and color to the environment \"\"\" """ task_setup_system_prompt = f""" You are writing python code to setup a simulated environment, translating user instructions into executable code, based on an existing API.------------------------------------------------write a function header for the prompt: {prompt}.""" def refine_function_header_prompt(function_code, refinement): return f""" Your role is to refine an existing python function, for example by adding a function argument or changing the name.If the function is implemented (i.e.not just "pass"), you should also alter the implementation accordingly, making as little changes and assumptions as possible.</p>
<p>Revise the following python function according to the user instructions: {function_code} Refinement prompt: {refinement} Do not make any assumptions.""" class ParsedList(BaseModel): parsed_list: list [str] def parse_hint_to_list_prompt(hint): return f"""</p>
<p>The user provided a list of tasks that are similar to the one you are currently trying to solve, in a single string.Retrieve each of the task descriptions from this string and return them as a list.This is the string: {hint} """</p>
<p>Listing 5: Core primitives
""" -----------------------------------------------------------------------------
the following functions require an initialised environmentthe agent doesn't need to know anything about the environment, only what methods are available to it we are responsible for properly initialising the environment, and ensuring that the agent has access to it """</p>
<p>""" IMPORTANT -pybullet can only handle one server at a time, if this is not commented out, this is the environment being used """ # env = Environment( # "/Users/maxfest/vscode/thesis/thesis/environments/assets", # disp=True, # shared_memory=False, # hz=480, # record_cfg={ # "save_video": False, # "save_video_path": "${data_dir}/${task}-cap/videos/", # "add_text": True, # "add_task_text": True, # "fps": 20, # "video_height": 640, # "video_width": 720, # }, # ) # from tasks.tasks.place_blocksimport Place5Blocks</p>
<h1>task = Place5Blocks() # env.set_task(task) # env.reset() """-----------------------------------------------------------------------------""" <strong>all</strong> = [</h1>
<p>"get_objects", "get_object_size", "get_object_pose", "get_object_color", "get_end_effector_pose", "put_first_on_second", "move_end_effector_to", "get_bbox", "get_point_at_distance_and_rotation_from_point", ] def get_objects() -&gt; list[TaskObject]:</p>
<p>"""gets all objects in the environment""" return env.task.taskObjectsdef get_object_size(task_object: TaskObject) -&gt; tuple[float, float, float]: """Returns the size of the given TaskObject as a tuple (width, depth, height)."""return task_object.sizedef get_object_pose(obj: TaskObject) -&gt; Pose: """returns the pose (Point3d, Rotation) of a given object in the environment."""return _from_pybullet_pose(env.get_object_pose(obj.id))</p>
<p>def get_object_color(task_object: TaskObject) -&gt; str: return task_object.colordef get_end_effector_pose() -&gt; Pose: """gets the current pose of the end effector""" return _from_pybullet_pose(env.get_ee_pose())</p>
<p>def put_first_on_second(pickPose: Pose, placePose: Pose):</p>
<p>""" This is the main pick-and-place primitive.</p>
<p>It allows you to pick up the TaskObject at 'pickPose', and place it at the Pose specified by 'placePose'.</p>
<p>If 'placePose' is occupied, it places the object on top of 'placePose.""" return env.step( action={ "pose0": _to_pybullet_pose(pickPose), "pose1": _to_pybullet_pose(placePose), } ) def move_end_effector_to(pose: Pose, speed=0.001):</p>
<p>"""moves the end effector from its current Pose to a given new Pose""" env.movep(_to_pybullet_pose(pose), speed=speed) def get_bbox(obj: TaskObject) -&gt; AABBBoundingBox: """gets the axis-aligned bounding box of an object -this is useful primarily for collision detection""" aabb_min, aabb_max = env.get_bounding_box(obj.id)return AABBBoundingBox(Point3D.from_xyz(aabb_min), Point3D.from_xyz(aabb_max))</p>
<p>def get_point_at_distance_and_rotation_from_point( point: Point3D, rotation: Rotation, distance: float, direction=np.array([1,0, 0]) ) -&gt; Point3D:</p>
<p>"""compute a point that is at a specific 'distance' from 'point', at a specified 'rotation'</p>
<p>The direction specifies the base direction in which to apply the rotation.This is useful for placing objects relative to other objects.""" rotated_direction = rotation.apply(direction)new_point = point.np_vec+ distance * rotated_direction return Point3D.from_xyz(new_point)</p>
<p>B.2 BASELINE MODEL SETUP</p>
<p>To ensure a fair comparison, we leverage state-of-the-art open-source code and language generation repositories.We adapt the baseline implementations and provide wrappers for direct integration with our APIs:</p>
<p>• CaP: https://github.com/google-research/google-research/tree/master/code_as_policies • LoHoRavens: https://github.com/Shengqiang-Zhang/LoHo-Ravens• DAHLIA: https://github.com/Ghiara/DAHLIAWe also consider Voyager Wang et al. (2023), which employs LLMs for automatic skill learning and RAG retrieval in Minecraft.However, Voyager is built for a Java-based game environment, and adapting it to our Python-based robotic control is impractical.To enable comparison, we design a variation of our framework where LLMs provide feedback (LYRA (Ours) w/ LLM feedback), keeping close to Voyager's concept.Following prior works Zhang et al. (2023); Meng et al. (2025); Wang et al. (2023), the LLM evaluates task success by receiving RGB-D observations before and after execution and providing corrective feedback if needed.In practice, we find that letting the LLM directly discover missing capabilities and learn new skills is difficult, since robotic manipulation involves more complex dynamics and spatial reasoning than Minecraft.Therefore, in this variation, we do not attempt direct skill learning but instead rely on the learned skills and example databases from our framework.The LLM focuses on reasoning about the current task, providing feedback to adjust the plan, and retrieving relevant items from memory.</p>
<p>For ablation, we also test a version without the retrieval module (LYRA (Ours) w/o memory).</p>
<p>Here, learned skills and examples are randomly sampled from our databases and appended to the prompt until the context window is filled.This simulates catastrophic forgetting in approaches that only modify prompts with human-in-the-loop Arenas et al. (2024).Specifically, we include 25 out of 49 learned skills and 25 out of 86 examples for Ravens tasks.This setup allows us to study how learned skills contribute when recent works rely solely on examples for in-context learning.Although performance drops on more complex tasks due to limited access to the full skill library, this variation still outperforms baseline models.</p>
<p>Since both the baseline models and our framework rely on few-shot in-context learning for reasoning and task planning, we configure the few-shot examples and/or skills for the baselines as follows:</p>
<p>• Customized Ravens: All models, including ours, start with 9 core primitives as initial skills, which are also appended to the baselines for in-context learning.In the Experiment section, we compare the success rates across 6 customized Ravens tasks as a case study.The tasks are set up as follows:</p>
<p>• Place block next to reference: A scene with two large blocks (red and blue).The agent must place the red block next to the blue one without collision.Alignment details (e.g., corner-to-corner) are not required for success.This task helps the agent acquire foundational spatial skills.</p>
<p>• Stack blocks: A scene with four medium-sized colored blocks.The agent must stack all blocks into a tower.Precise alignment is not required for success evaluation; towers with random positions or orientations are also considered successful.This provides a base skill for more complex behaviors.</p>
<p>• Build {i × j × k} structure: A scene with multiple medium-sized colored blocks.The agent must build structures as instructed by the user.Two example plans are provided in the prompt: (1) a {2 × 2 × 2} cube and (2) a {3 × 2 × 2} pyramid.The agent must also generalize to unseen tasks: (3) a {4 × 3 × 3} pyramid, (4) a {1 × 3 × 3} wall, and (5) a {4 × 4 × 1} base.</p>
<p>• Make a human face: A scene with several medium-sized colored blocks and one rectangular block.The agent must build a circle and use two blocks plus the rectangle to form facial features inside.Orientation details are not required for success.</p>
<p>• Build a Jenga tower: A scene with multiple rectangular blocks of uniform or mixed colors.The agent must stack them in alternating orientations to construct a stable Jenga tower.</p>
<p>• Build a house: A scene with blocks of various colors and shapes.The agent must use yellow blocks for the house base, brown blocks for the roof base, and red plates for the roof tiles.</p>
<p>C REAL-WORLD EXPERIMENT SETUP</p>
<p>C.1 HARDWARE SETTING</p>
<p>In the real-world demonstrations, we employ a Franka FR3 manipulator paired with an Intel Re-alSense D435i depth camera mounted at the table edge to evaluate the proposed framework.As illustrated in Fig. S1, the camera is positioned approximately 1 m in front of the robot, closely mirroring the configuration used in simulation.The robot's base coordinate system, O base , is defined at the center of joint 0, with the x-axis directed toward the table and the z-axis aligned vertically upward from the table surface.For object manipulation, the robot is equipped with a Franka two-finger gripper, in contrast to the simulation benchmark, which employs a suction gripper for pick-and-place tasks.To ensure seamless deployment across embodiments and gripper types, we maintain consistent primitive APIs between the real-world and simulated settings, enabling skills developed in simulation to transfer directly to the physical robot.</p>
<p>RealSense D435i</p>
<p>Franka FR3 O base In our real-world setup, the calibrated extrinsic transformation between the camera and the robot base frame is obtained as: In simulation, privileged object information (such as type, size, pose, and color) can be directly accessed through predefined APIs, e.g., get objects, get object size, and get object pose.In contrast, acquiring such information in the real world is more challenging.To address this, we employ recent vision foundation models to perceive and parse the task scene.For open-world object pose estimation, we use Grounded SAM 2 Ren et al. (2024) to detect target object masks and their bounding boxes (with SAM 2.1.hiera.largeand GroundingDINO.swint.ogc).The object center is estimated by computing the mean of all mask pixels and projecting it back into 3D space.For orientation estimation, we adopt FoundationPose Wen et al. (2024) in combination with object CAD models to recover the object's coordinate frame, particularly the x-and y-axis directions.By comparing the angle differences between the object and robot base coordinate frames, we determine the object's rotation pose.
T cam base =      6</p>
<p>DEPLOYMENT</p>
<p>We utilize the official Franka ROS2 library on Ubuntu 22.04 to control the Franka FR3 manipulator.</p>
<p>The action space of our framework is defined in a 6-dimensional vector space, representing endeffector motions composed of linear displacements along the x, y, and z axes, as well as rotational changes in roll, pitch, and yaw.By default, Franka ROS2 relies on OMPL-based motion planning.</p>
<p>To better align with our simulation environment and enable more intuitive trajectory control, we extend this setup by customizing a Cartesian linear motion planning instance, which for straightforward and precise linear path execution.To ensure reliable operation in the real-world hardware setup, we carefully configure the planning parameters, including detection tolerances, velocity and acceleration scaling factors, and orientation/pose constraints.The complete parameter configuration for our motion planning setup is summarized in Table S2, which provides the exact values adopted in our experiments.The following is an example prompt for generating a task-specific code plan that can be deployed in the ROS2 environment.</p>
<p>Listing 6: Demo task script structure for ROS2 deployment from utils.core_types import Workspace # The task script should be sumamrized as follows to enable a direct deployment in ROS2 environment:
def demo(record_cfg, camera_args): # -------------------------------------------------------------------------------------- # Do# --------------------------------------------------------------------------------------- try:</p>
<h1>...You can implement your task specific code at here ... # ... flat code ... # ... call your skill_function(...) to accomplish the task ...  # ---------------------------------------------------------------------------------  """Identifies and returns a list of TaskObjects that are categorized as roof tiles from a given list of objects.A roof tile is characterized by having one dimension smaller than 0.02 and being red in color.Args:</h1>
<h1>----------------------------------------------------------------------------------- # You</h1>
<p>-objects (list[TaskObject]): A list of TaskObjects to be analyzed for identification of roof tiles.Returns:</p>
<p>-list[TaskObject]: A list of TaskObjects that are identified as roof tiles, based on the specified characteristics.""" roof_tiles = [] for obj in objects: if obj.color.lower()== "red" and any(dim &lt; 0.02 for dim in obj.size): roof_tiles.append(obj)return roof_tiles</p>
<p>Listing 10: Function identify beam block def identify_beam_block(blocks: list[TaskObject]) -&gt; TaskObject:</p>
<p>"""Identifies the beam block from a list of blocks.</p>
<p>A beam block is defined by the following criteria: -It must have the color 'brown'.</p>
<p>-It has one square side, meaning two side lengths must be the same.</p>
<p>-The third side should be at least 3 times as long as the square sides.Args: blocks (list[TaskObject]): The list of block objects to be evaluated.Returns: TaskObject: Returns the TaskObject identified as a beam block.</p>
<p>If no beam block is found, returns None.""" for block in blocks: if block.color!= "brown": continue width, depth, height = sorted(block.size)if width == depth and height &gt;= 3 * width: return block return None</p>
<p>Listing 11: Function identify roof base def identify_roof_base(objects: list[TaskObject]) -&gt; TaskObject:</p>
<p>""" Identifies and returns the TaskObject that serves as the base for a roof in a given list of objects.</p>
<p>A roof base is characterized by being brown in color and by having two dimensions that are at least 10 times larger than the third dimension.</p>
<p>LYRAFigure 1 :
1
Figure 1: Framework overview: human-in-the-loop lifelong skill learning and task deployment.</p>
<p>.Pose, block.Pose ) learn a skill that stack the blocks, use 4 random blocks for initial setup.def stack blocks(blocks): • • • cur pose = blocks[0].Pose for block in blocks[1:]: put first on second( block.Pose, cur pose ) cur pose.z+= block.size[2]This is not stack.You need to stack ALL the blocks in a tower.def stack blocks(blocks): • • • cur pose.rot= np.deg2rad(45)cur pose = Pose( • • • rot=cur pose.rot ) • • • Please stack the blocks corner-to-corner with required angle, e.g., 45 degree.def stack blocks(blocks, start pose): • • • cur pose = start pose</p>
<p>Figure 2 :
2
Figure 2: Structure of the proposed human-in-the-loop lifelong skill learning pipeline.</p>
<p>): Code as Policy (CaP) Liang et al. (2023): A representative baseline for LLM-based open-loop code generation without correction or retrieval.LoHoRavens (GPT-4o) Zhang et al. (2023): A language-generation baseline with explicit LLM feedback, using a pretrained RL policy CLIPort for execution.DAHLIA Meng et al. (2025): A state-of-the-art code generation framework with LLM-based closed-loop control and incremental examples for in-context learning.</p>
<p>) w/o memory: A variant without the retrieval module, where 25 out of 49 learned skills and 25 out of 86 explored examples in Ravens are randomly sampled and appended until the context window is filled.For Franka Kitchen and MetaWorld, we sample half of the learned skills and examples at random to evaluate how irrelevant data affects feedback efficiency.For simulation, we build on the PyBullet-based Ravens benchmark Zeng et al. (2021); Zhang et al. (2023), where a UE5 robot manipulates multiple tabletop objects.Following prior work Zhang et al. (2023), we allow LLMs to set up tasks so scenarios can be modified by user intention.To test scalability, we also evaluate on Franka Kitchen Gupta et al. (2020) (long-horizon tasks with a Franka Panda in a kitchen scene) and Metaworld Yu et al. (2020) (Sawyer with diverse tabletop tasks).</p>
<p>Figure 3 :
3
Figure 3: Empirical analysis of skill learning.(a) Case study: Baseline comparison on Ravens longhorizon tasks, reported as the average success rate over 20 attempts.(b)-(m) Snapshots comparison: why human-in-the-loop is essential for skill learning and how it improves performance.</p>
<p>(b)-(m) show how user feedback improves task performance beyond what SR reflects.For example, in (b) vs. (d), (e), and (f) vs. (</p>
<p>Figure 4 :
4
Figure 4: Scalability and feedback efficiency validation.(a) Average number of corrections for LLM-based flat code generation versus our human-in-the-loop skill code generation.(b)-(i) Snapshots from Franka Kitchen and MetaWorld; additional results are provided in Appendix D.</p>
<p>5(b)), the task can be decomposed top-down into three subtasks: organizing the scene (Fig. 5(c)), building the base (Fig. 5(d)), and constructing the roof (Fig. 5(e)).Unlike LLM-only closed-loop methods that directly generate flat code without awareness of required skills, our framework acquires intermediate skills such as stacking blocks corner-to-corner or aligning them with precise distance and orientation, which can be further extended to build cubes and more complex structures.Through lifelong learning, users introduce increasingly complex tasks, and the agent expands its capability by reusing existing skills, either nesting them within new ones or modularizing them to develop extended functionality.Both learned skills and examples are stored in an external</p>
<p>Figure 5 :
5
Figure 5: (a) Skill tree of build a house.(b)-(i) Snapshots of task build a house, where (c)-(e) show sub-skills acquired from bottom-up lifelong skill expansion, and (f)-(h) show how they are deployed and reused in the task-specific plan generation.Full code implementation in Appendix E.</p>
<p>Figure 6 :
6
Figure 6: Real-world demonstrations.Snapshots of (a) building a house, (b) stacking a Jenga tower, (c) writing "ICLR", and (d) constructing a temple.Full real-world results refer to Appendix F.</p>
<p>Figure S1 :
S1
Figure S1: Real-world hardware setup.A Franka FR3 serves as the agent embodiment, with an Intel RealSense D435i placed at the table edge to capture privileged scene information.</p>
<p>.12323400e − 17 7.37277337e − 01 −6.75590208e − 01 1.06 1.00000000e + 00 −4.51452165e − 17 4.13679693e − 17 0.16 −0.00000000e + 00 −6.75590208e − 01 −7.37277337e − 01 0</p>
<p>Figure S2 :Figure S3 :Figure S4 :Figure S5 :Figure S6 :Figure S7 :Figure S8 :
S2S3S4S5S6S7S8
Figure S2: Snapshots of the "stack blocks" task and its variations under user-guided lifelong learning.(a) Stack 4 blocks corner-to-corner at the workspace center.(b) Stack 4 blocks with a 45°r otation at the back-right table corner.(c) Stack blocks by color, with the yellow block on top.(d) Stack blocks from largest to smallest.(e) Stack blocks from smallest to largest.(f) Stack blocks into a zigzag tower.(g) Build two towers sorted by color.</p>
<p>Framework</p>
<p>-wall MW -pick-place-wall MW -door-open MW -faucet-open MW -drawer-close MW -button-press MW -drawer-open MW -window-close DAHLIA 6 -into MW -window-open MW -disassembly MW -basketball MW -pick-out-of-hole MW -coffee-pull MW [2:4], Pose(next_stack_position, startingPose.rotation))# make lines towards the back of the workspace from each of the stacks make_line_of_blocks_next_to(blocks[4:6], blocks[0], "back") make_line_of_blocks_next_to(blocks[6:8], blocks[3], "back") # make the back wall of the house back_wall_start_pos = get_point_at_distance_and_rotation_from_point( get_object_pose(blocks[5]).position,startingPose.rotation,block_width + 0.005, direction=(-1, 0, 0), ) build_structure_from_blocks( blocks[8:14], (1, 3, 2), Pose(back_wall_start_pos, startingPose.rotation)) Listing 9: Function identify roof tiles def identify_roof_tiles(objects: list[TaskObject]) -&gt; list[TaskObject]:</p>
<p>Figure S10: Real-world demonstration for task "build a house".</p>
<p>Figure S11 :
S11
Figure S11: Real-world demonstration for task "stack a jenga tower".</p>
<p>Figure S12 :
S12
Figure S12: Real-world demonstration for task "write a ICLR".</p>
<p>Figure S13 :
S13
Figure S13: Real-world demonstration for task "build a temple".</p>
<p>Figure S15 :
S15
Figure S15: Real-world demonstration for task "stack blocks" with corner-to-corner aligned.</p>
<p>Figure S16 :
S16
Figure S16: Real-world demonstration for task "stack blocks" with shift 0.5cm from each other.</p>
<p>Figure S17 :
S17
Figure S17: Real-world demonstration for task "stack blocks" from big to small.</p>
<p>Figure S18 :
S18
Figure S18: Real-world demonstration for task "stack blocks" from small to big.</p>
<p>Figure S19 :
S19
Figure S19: Real-world demonstration of the "stack blocks" task, showcasing the performance of the baseline models, e.g., CaP or DAHLIA.</p>
<p>Phase II: Lifelong Learning-oriented Capability Extension stack
• • •
small2big (l1, s10) stack zigzag tower (l2, s20) stack two towers by color (li, si0) make cross around blue block (lj , sj0) build a {i * k} pyramid (lk, sk0) arrange in a circle (ln, sn0) Agent Agent User User User User-designed curriculum Base skill: stack blocks User-designed curriculum Base skill: move to reference Memory Examples (l, c) ∈ E Skills z ∈ Z</p>
<p>oriented Retrieval and Planning Agent Agent Agent Retrival Generation Challenging extreme long-horizon tasks
write "hello"make smiley faceTask Plansbuild housestack jenga tower
• • • Phase III: Task-specific-</p>
<p>Transfering to cross-embodiment
place the kettleuse hammerTask Plansopen cabinetassembly</p>
<p>skills learned specifically for building a house skills learned independently of building a house Core primitives
build houseget blocks by colorbuild house baseidentify roof tilesidentify beam blockidentify roof baseget objectsassemble roofbuild structure from blocksget object colorclear tablebuild house basebuild house roofplace roof tilesmake line of blocks next tomake line with blocksstack blocksmove block next to referenceget object poseput first on secondget object size</p>
<p>Liang et al. (2024)fine-tunes a code-writing model with teacher chat corrections to reduce the number of attempts, but it only generalizes to the taught task and requires large interactive training datasets.PromptBook Arenas et al. (2024) leverages human corrections to refine LLM prompts, achieving robust performance in real-world pick-and-place tasks.However, a slower high-level planner on top of a fast control policy.Typical examples include RT-H Belkhale et al. (2024), OpenVLA Kim et al. (2024; 2025), π 0.5 Black et al. (2024); Intelligence et al. (2025), Gemini-Robotics Team et al. (2025) and GR00T N1 Bjorck et al. (2025) etc.Such models require massive expert demonstrations and large-scale computational resources for pre-training, which limits their generalization performance.A few studies try to boost data coverage with RAG techniques that pull extra non-robot demonstrations from an external database Ju et al. (2024); Xu et al. (2024); Kuang et al. (</p>
<p>Ma et al. (2024)s is limited to the trained scenarios, and it may suffer from catastrophic forgetting when prompts are overwritten.Overall, most methods integrate feedback at the data or prompt level, which makes them prone to catastrophic forgetting in long-horizon tasks generation requiring multi-round iterations.Large-scale robotic foundation models.VLA modelsMa et al. (2024)roughly come in two categories: end-to-end models that map multi-modal inputs straight to motor commands and hierarchical models that add</p>
<p>Table S1 :
S1
Baseline model setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6 B.3 Raven tasks setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7 List of the core-primitives for our agent to build on
B FRAMEWORK SETUPB.1 FRAMEWORK PROMPTINGCONTENTS Our framework and the baseline code generation methods rely on dynamic prompting for in-contextlearning. Table S1 lists the core primitives used in our framework, and the corresponding promptsA Pseudo code of algorithm are shown below.1B Framework setup2B.1 Framework prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Name Function2get objects B.2 C Real-world experiment setup get object colorThis function gets all objects in the environment. The agent can retrieve specific properties of these objects with the functions below. Returns the color of the block.8get object sizeReturns the size of the block.get object poseReturns the pose of the block, given as a 3-dimensionalposition vector, and a 4 dimensional quaternion rotation.get bboxReturns the axis-aligned bounding box of an object, tosimplify collision queries.D Simulation performance11E Build a house14F Real-world demonstrations20A PSEUDO CODE OF ALGORITHMAlgorithm 1 Interactive skill learning1: Initialize Skill Library Z 0 and Examples E 02: z ← SkillParser(skill description)▷ Choose current skill to learn3: while True do4:l ← task description▷ Provide task instruction5:s 0 ← TaskSetup(initial state description)▷ Set up the environment6:correction ← ∅▷ Set initial correction7:c ← ∅▷ Set initial task-specific code8:while True do9:examples ← (l ′ i , c ′ i ) i=1,...,K ∈ E▷ Retrieval based on l and correction10:c, z ← Agent(l, c, z, correction, examples)11:s T ← Rollout(c)▷ Roll out policy code12:if s T is aligned with l then13:E = E ∪ (l, c)▷ Add example to library14:break15:end if16:update correction based on s T17:end while18: end while19: Update z in Z▷ Update skill library
C.1 Hardware setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8 C.2 Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8 C.3 Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9</p>
<p>For few-shot adaptation, all baselines are provided with the following example task plans and are expected to generalize to unseen scenarios, either in an open-loop manner or under LLM guidance:stack blocks (4 colors) put the yellow block next to the green block build a {2 × 2 × 2} cube with 8 blocks build a {3 × 2 × 2} pyramid with 8 blocks put the red block in the middle of the workspace rotate the blue block by 45 degrees move the end effector to the center of the workspace move the smallest block 10cm to the left arrange the blocks around a circle• Franka Kitchen &amp; Metaworld: All models, including ours, start with 9 core primitives as initial skills (shared across both benchmarks via unified API wrappers).
For few-shot adap-tation, baselines are given the following example task plans and are expected to generalizeto new scenarios:-reach a goal-push a puck to a goal-pick and place a puck in a goal-open a door (revolute joint)-close a window (slide joint)B.3 RAVEN TASKS SETUP</p>
<p>Table S2 :
S2
Hardware parameter setup
ParametersValuesRealSense x-axis detection tolarance0.01 [m]RealSense y-axis detection tolarance0.01 [m]RealSense z-axis detection tolarance0.01 [m]Feasible factor of cartesian linear planning0.9Max velocity scaling factor0.1Max acceleration scaling factor0.1Absolute x-axis orientation tolerance0.02Absolute y-axis orientation tolerance0.02Absolute z-axis orientation tolerance0.02Target pose constratint in x-axis0.005 [m]Target pose constratint in y-axis0.005 [m]Target pose constratint in z-axis0.005 [m]Workspace in x-axis+0.25 [m] -+0.8 [m]Workspace in y-axis-0.55 [m] -+0.3 [m]Workspace in z-axis+0.01 [m] -+0.65 [m]
Peiyuan Zhi, Zhiyuan Zhang, Yu Zhao, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, and Siyuan Huang.Closed-loop open-vocabulary mobile manipulation with gpt-4v.In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp.4761-4767.IEEE, 2025.E BUILD A HOUSEFigureS9: Snapshots of "build a house".The implementation details of the skill "build a house" are demonstrated as follows:Listing 7: Function build house from utils.core_types import Workspace def build_house():"""Builds a house in the middle of the workspace.Assumes all the necessary objects are available in the workspace, and moves them out of the way before building the house.""" objects = get_blocks_by_color() base_blocks = get_blocks_by_color("yellow") if len(base_blocks) != 14: raise Exception("Not enough blocks to build the house") roof_base = identify_roof_base(objects) if not roof_base:raise Exception("Can't find the roof base") roof_beam = identify_beam_block(objects) if not roof_beam:raise Exception("Can't find the roof beam") roof_tiles = identify_roof_tiles(objects) if not roof_tiles:raise Exception("Can't find the roof tiles") -TaskObject: The TaskObject identified as the roof base, based on the specified characteristics.Listing 13: Function assemble roof def assemble_roof(base: TaskObject, roof_beam: TaskObject, roof_tiles: list[TaskObject], overall_pose: Pose): """ Assembles a roof structure using a designated base, a roof beam, and a list of roof tiles, starting from a given overall pose.The base acts as the foundation while the roof beam provides structural support and the roof tiles are placed on top to complete the structure.Valid directions are "front", "back", "left", and "right".gap (float): The gap between the reference block and the first block in the line, and between consecutive blocks.This function will arrange the specified blocks in a single line, starting from the chosen side of the reference block, following the given direction along the x or y axis in the workspace, depending on the specified direction.""" axis = '' if direction == "front": axis = 'x' elif direction == "back": axis = '-x' elif direction == "left": axis = '-y' """Arranges the given blocks in a straight line starting from the specified start pose.Args: blocks (list[TaskObject]): A list of block objects to be arranged in a line.start_pose (Pose): The pose in the workspace where the line of blocks should start.The position will be used as the starting point, and the rotation will be used as the direction vector.gap (float): The gap between consecutive blocks.Note:The function places the blocks in the order in which they are passed.""" current_block = blocks[0] put_first_on_second(get_object_pose(current_block), start_pose) for block in blocks Listing 19: Function move block next to reference def move_block_next_to_reference( block: TaskObject, referenceBlock: TaskObject, axis: str = "x", gap: float = 0.005 ):"""Moves the block next to the referenceBlock such that their edges are aligned along the specified axis with a small gap.block (TaskObject): The block object to be moved and aligned.referenceBlock (TaskObject): The block object that remains stationary and serves as the reference.axis (str): The axis along which to align the blocks.Should be 'x', '-x', 'y', or '-y'.gap (float, optional): The small gap to leave between the blocks.Defaults to 0.005 meters.Raises:ValueError: If the specified axis is not 'x', '-x', 'y', or '-y'.""" if axis not in ["x", "-x", "y", "-y"]:raise ValueError("Axis must be either 'x', '-x', 'y', or '-y'.Listing 20: Applied core primitives def get_object_pose(obj: TaskObject) -&gt; Pose: """returns the pose (Point3d, Rotation) of a given object in the environment."""return _from_pybullet_pose(env.get_object_pose(obj.id))def put_first_on_second(pickPose: Pose, placePose: Pose):""" This is the main pick-and-place primitive.It allows you to pick up the TaskObject at 'pickPose', and place it at the Pose specified by 'placePose'.If 'placePose' is occupied, it places the object on top of 'placePose.""" return env.step( action={ "pose0": _to_pybullet_pose(pickPose), "pose1": _to_pybullet_pose(placePose), } ) def get_object_size(task_object: TaskObject) -&gt; tuple[float, float, float]:"""Returns the size of the given TaskObject as a tuple (width, depth, height)."""return task_object.sizedef get_object_color(task_object: TaskObject) -&gt; str: """Returns the color of the given TaskObject."""return task_object.colordef get_objects() -&gt; list[TaskObject]: """gets all objects in the environment""" return env.task.taskObjects
How to prompt your robot: A promptbook for manipulation skills with code as policies. Montserrat Gonzalez Arenas, Ted Xiao, Sumeet Singh, Vidhi Jain, Allen Ren, Quan Vuong, Jake Varley, Alexander Herzog, Isabel Leal, Sean Kirmani, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Chebotar, arXiv:2403.01823Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, arXiv:2503.147342025arXiv preprint</p>
<p>0 : A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Roboscript: Code generation for free-form manipulation tasks across real and simulation. Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, 2024CoRR</p>
<p>Correct me if i am wrong: Interactive learning for robotic manipulation. Eugenio Chisari, Tim Welschehold, Joschka Boedecker, Wolfram Burgard, Abhinav Valada, IEEE Robotics and Automation Letters. 722022</p>
<p>Guiding policies with language via meta-learning. Abhishek John D Co-Reyes, Suvansh Gupta, Nick Sanjeev, Jacob Altieri, John Andreas, Pieter Denero, Sergey Abbeel, Levine, International Conference on Learning Representations. 2018</p>
<p>No, to the right: Online language corrections for robotic manipulation via shared autonomy. Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, Dorsa Sadigh, Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction. the 2023 ACM/IEEE International Conference on Human-Robot Interaction2023</p>
<p>Grounding language model by detecting and recovering from plan-execution misalignment. Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Jianyu Chen, Doremi, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman, Conference on Robot Learning. PMLR2020</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2504.16054π 0.5 : a vision-language-action model with open-world generalization. Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Kevin Black, Noah Brown, James DarpinianPMLR2023. 2025arXiv preprintConference on Robot Learning</p>
<p>Robo-abc: Affordance generalization beyond categories via semantic correspondence for robot manipulation. Yuanchen Ju, Kaizhe Hu, Guowei Zhang, Gu Zhang, Mingrun Jiang, Huazhe Xu, European Conference on Computer Vision. Springer2024</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan P Rafailov, Foster, Quan Pannag R Sanketi, Vuong, 8th Annual Conference on Robot Learning. 2024</p>
<p>Fine-tuning vision-language-action models: Optimizing speed and success. Jin Moo, Chelsea Kim, Percy Finn, Liang, arXiv:2502.196452025arXiv preprint</p>
<p>Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang, 8th Annual Conference on Robot Learning. 2024</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Learning to learn faster from human feedback with language model predictive control. Jacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat Gonzalez Arenas, Maria Attarian, Maria Bauzá, Matthew Bennice, Alex Bewley, Adil Dostmohamed, 2024CoRR</p>
<p>Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao Bao, Yuke Zhu, Robotics: Science and Systems. 2023</p>
<p>Interactive language: Talking to robots in real time. Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, Pete Florence, IEEE Robotics and Automation Letters. 2023</p>
<p>A survey on visionlanguage-action models for embodied ai. Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King, arXiv:2405.140932024arXiv preprint</p>
<p>Data-agnostic robotic long-horizon manipulation with vision-language-guided closed-loop feedback. Yuan Meng, Xiangtong Yao, Haihui Ye, Yirui Zhou, Shengqiang Zhang, Zhenshan Bing, Alois Knoll, arXiv:2503.219692025arXiv preprint</p>
<p>The discipline of machine learning. Tom Michael, Mitchell , 20069Carnegie Mellon University, School of Computer Science, Machine Learning . . .</p>
<p>Robocodex: multimodal code generation for robotic behavior synthesis. Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, G E Chongjian, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning2024</p>
<p>Text and code embeddings by contrastive pre-training. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, arXiv:2201.100052022arXiv preprint</p>
<p>Grounded sam: Assembling open-world models for diverse visual tasks. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, arXiv:2401.141592024arXiv preprint</p>
<p>Saminda Gemini Robotics Team, Joshua Abeyruwan, Jean-Baptiste Ainslie, Montserrat Alayrac, Travis Gonzalez Arenas, Ashwin Armstrong, Robert Balakrishna, Maria Baruch, Michiel Bauza, Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Foundationpose: Unified 6d pose estimation and tracking of novel objects. Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>P-rag: Progressive retrieval augmented generation for planning on embodied everyday task. Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on robot learning. PMLR2020</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Conference on Robot Learning. PMLR2021</p>
<p>Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh, 2024 IEEE international conference on robotics and automation (ICRA). IEEE2024</p>
<p>Generative artificial intelligence in robotic manipulation: A survey. Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Yu Michael, Qifeng Wang, Chen, 2025CoRR</p>
<p>Lohoravens: A long-horizon languageconditioned benchmark for robotic tabletop manipulation. Shengqiang Zhang, Philipp Wicke, Lütfi Kerem, S ¸enel, Luis Figueredo, Abdeldjallil Naceri, Sami Haddadin, Barbara Plank, Hinrich Schütze, arXiv:2310.120202023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>            </div>
        </div>

    </div>
</body>
</html>
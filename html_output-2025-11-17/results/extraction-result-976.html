<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-976 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-976</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-976</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-350bdcdcec6c84a5859ebb76b4a62db2c4eaf2a2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/350bdcdcec6c84a5859ebb76b4a62db2c4eaf2a2" target="_blank">Invariant Policy Learning: A Causal Perspective</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This paper views the environmental shift problem through the lens of causality and proposes multi-environment contextual bandits that allow for changes in the underlying mechanisms and adopts the concept of invariance from the causality literature and introduces the notion of policy invariance.</p>
                <p><strong>Paper Abstract:</strong> Contextual bandit and reinforcement learning algorithms have been successfully used in various interactive learning systems such as online advertising, recommender systems, and dynamic pricing. However, they have yet to be widely adopted in high-stakes application domains, such as healthcare. One reason may be that existing approaches assume that the underlying mechanisms are static in the sense that they do not change over different environments. In many real-world systems, however, the mechanisms are subject to shifts across environments which may invalidate the static environment assumption. In this paper, we take a step toward tackling the problem of environmental shifts considering the framework of offline contextual bandits. We view the environmental shift problem through the lens of causality and propose multi-environment contextual bandits that allow for changes in the underlying mechanisms. We adopt the concept of invariance from the causality literature and introduce the notion of policy invariance. We argue that policy invariance is only relevant if unobserved variables are present and show that, in that case, an optimal invariant policy is guaranteed to generalize across environments under suitable assumptions.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e976.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e976.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An invariance-based causal discovery approach that identifies covariate sets whose conditional relationship to an outcome remains stable across environments; used as conceptual foundation for selecting non-spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal Inference by Invariant Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>ICP searches for subsets S of observed covariates such that the conditional distribution P(R | X^S) is invariant across multiple environments; under faithfulness, the intersection of accepted invariant sets yields variables that are (with high confidence) causal parents/ancestors of R. It operationalizes detection of spurious correlates by rejecting sets whose conditional distributions change across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-environment observational data / contextual bandit environments (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Sets of (possibly heterogeneous) environments modeled via SCMs; not an interactive virtual lab per se but a multi-environment observational setting where environment labels are observed and used for invariance testing. Can be applied to simulated environments or real cohorts (e.g., clustered continents in warfarin study).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via invariance testing across environments; intersection of accepted invariant sets yields robust predictors (removes features with environment-dependent associations).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Confounding (hidden common causes leading to environment-dependent correlations), distributional shift-induced spurious correlations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Test whether conditional distribution P(R | X^S) is equal across environments (in practice, test residual distributions after pooling and fitting conditional mean); non-invariance indicates spurious association.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation by statistical rejection of invariance hypotheses: if P(R|X^S) differs across environments, then S is not invariant (spurious). Use of multiple environments and intersection over accepted sets to rule out spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitatively: yields predictors that generalize across strong distributional shifts; in this paper, invariance-based selection led to bounded worst-case regret in simulations and improved robustness in semi-real experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Predictive methods that ignore invariance can perform well in training environments but may have large or unbounded worst-case regret when environment-induced confounding changes (shown in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICP-style invariance is an appropriate principle for removing variables whose association with R is unstable across environments; it forms the conceptual backbone of the paper's invariant policy approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e976.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e976.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Off-policy invariance test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Off-policy invariance test (resampling-based, Thams et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A resampling-based test that mimics target policies by weighted resampling of offline data, enabling testing of distributional invariance P(R | X^S) for policies different from the data-generating policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Off-policy invariance test (weighted resampling)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given offline data collected under an initial policy π^0, this method constructs a target sample that approximates draws under a candidate test policy π^S by weighted resampling with weights r_i = π^S(a_i | x_i^S) / π^0(a_i | x_i). The resampled target datasets (one per observed environment) are then passed to an invariance test (e.g., residual equality tests). Theoretical level guarantees for the asymptotic test are provided (based on Thams et al. 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline multi-environment contextual bandit data (simulations and case study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline logged data collected from multiple environments/policies; not active experimentation, but allows off-policy inference by resampling to emulate alternative policies.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects distractors by testing invariance of conditional reward distributions under policies that would use only candidate subsets S, thereby identifying variables whose predictive power is spurious (environment-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Confounding via unobserved U that induces environment-dependent correlations between some X and R; spurious correlates due to policy-induced paths.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Construct target samples via importance-weighted resampling to emulate π^S and then perform tests for equality of P(R | X^S) across environments (residual distribution tests).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Weighted resampling implicitly downweights examples unlikely under the test policy; inverse-propensity weighting (1/π^0(A|X)) is also used in estimators to recover pooled conditional means Q^S.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Reject invariance hypothesis for S if residual distributions differ across environments in the resampled data; this refutes that S is stable and hence refutes purported causal links that rely on S.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>No active interventions; optimization of test policy is performed in resampling space to increase test power, but real-world experiments are not collected adaptively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Empirical: high acceptance rates for true invariant sets as sample size increases; in simulations the off-policy test correctly accepts the true invariant set {X^2} while rejecting non-invariant sets with increasing power as n grows and with more training environments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline of testing invariance using the original logging policy π^0 can fail because π^0 may open policy-dependent paths, hiding true invariances; naive tests can wrongly reject invariant sets.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>In simulations: one strongly non-d-invariant variable (X^1); in semi-real warfarin: one injected non-invariant predictor</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Off-policy resampling enables valid invariance testing for candidate policies different from the data-collection policy; using resampling plus appropriate invariance tests recovers invariant sets even when data were collected under a policy that uses non-invariant covariates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e976.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e976.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant residual test (KW/MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant residual distribution test (Kruskal–Wallis / MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical invariance test that fits pooled conditional mean E[R | X^S] under a target policy and then tests whether residuals have the same distribution across environments using nonparametric tests like Kruskal–Wallis or kernel two-sample (MMD) tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant residual distribution test</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pool resampled target-sample data across environments to fit an estimator of E[R | X^S] (via regression). Compute residuals R - E_hat[R | X^S] and then perform a multi-sample test for equality of residual distributions across environments. Implementations include Kruskal–Wallis for mean differences and kernel two-sample tests (MMD) for broader alternatives, with multiple-testing corrections where appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Resampled offline multi-environment datasets (simulated bandits and warfarin case study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline datasets split by observed environment labels; test is applied to resampled data intended to approximate a particular test policy.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects distractors by revealing environment-conditional shifts in residuals after conditioning on X^S; when residual distributions differ, S is flagged as including spurious variables or missing confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-dependent confounding, hidden confounders causing shifts in conditional distribution of R given X^S, policy-induced spurious paths</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical hypothesis testing on residuals across environment groups (Kruskal–Wallis for mean differences; MMD or other kernel tests for distributional differences).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistically reject null of invariance (equal residual distributions), thereby refuting that S yields stable predictive relationship; used to exclude S from invariant candidate sets.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Simulation: Kruskal–Wallis-based test showed correct acceptances for the true invariant set and increasing power with sample size and number of environments; pointwise asymptotic level guarantees are provided in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>If the initial logging policy is used without resampling, residual tests can be confounded by policy-dependent paths and incorrectly reject true invariances.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Residual-based invariance tests (even simple nonparametric tests like Kruskal–Wallis) are effective when combined with off-policy resampling; kernel tests can improve power against richer alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e976.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e976.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weighted resampling (importance)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Importance-weighted resampling to emulate target policies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical resampling procedure that draws weighted samples from offline logged data to simulate draws from a different (target) policy, using weights r_i = π_target(a_i | x_i^S) / π^0(a_i | x_i).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Importance-weighted resampling for off-policy testing</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute per-example importance weights r_i proportional to the likelihood ratio between a target test policy π^S and the logging policy π^0, choose a resample size m_e per environment (GOF heuristic), and draw samples with probability proportional to product of selected weights. This yields a synthetic dataset approximating draws under π^S, which can be used for invariance tests and estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline multi-environment logged bandit data</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Logged datasets per environment collected under π^0; resampling is used to create target-samples for hypothetical policies without running new experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>By enabling invariance testing for candidate policies that condition only on S, it helps detect and thereby exclude distractor variables whose predictive relationship is not stable.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Policy-induced spurious associations, environment-dependent confounding</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Indirect: enables detection via construction of target datasets on which residual invariance tests are performed.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Resampling probabilities and inverse-propensity reweighting downweight examples unlikely under the target policy; estimators use 1/π^0(A|X) factors in weighted least squares.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Enables valid off-policy invariance testing with asymptotic level control (as proven/extended from Thams et al. 2021); empirically recovers invariant sets in simulation and real-data case study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Weighted resampling is a viable tool to emulate alternative policies for invariance tests; combined with sample-splitting it preserves level while allowing power optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e976.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e976.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Power-optim test policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-policy optimization via p-value minimization (log-derivative SGD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that chooses a test policy within a parametric family to maximize the power (or minimize expected p-value) of the invariance test by stochastic gradient optimization over resampling probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Test-policy power optimization</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Parameterize candidate test policies π_θ^S (e.g., linear softmax). Define objective J(θ) = E[ p-value(D^{π_θ^S}) | D ] and compute gradients via the log-derivative trick: ∇J = E[∇ log P(D^{π_θ^S} | D) * pv(D^{π_θ^S}) | D]. Estimate gradient by Monte Carlo resampling and apply stochastic gradient descent; use sample-splitting to avoid invalidating test level. The result is a policy that (in expectation) yields strong signals of non-invariance if present.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline multi-environment data used for resampling-based invariance testing</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>No active interactions; optimization takes place in the resampling/emulation space to increase statistical detectability of non-invariance under candidate S.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Improves detection sensitivity to spurious variables by searching for a policy that amplifies environment-dependent signals (making distractors easier to detect and reject).</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Environment-dependent confounding and policy-induced spurious associations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Indirect: produces target resampled datasets more likely to expose non-invariance, thereby increasing power of residual distribution tests to detect spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>By making p-values small for non-invariant S, the method increases the chance to statistically refute spurious relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Search over parametric policies (e.g., linear softmax), optimize expected p-value via SGD using log-derivative estimator; employs sample splitting to preserve test validity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Empirically increases power to detect non-invariance in resampled tests, especially with limited sample sizes; recommended to use sample-splitting to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Using a fixed or naive test policy can lead to lower power and failure to detect non-invariance in challenging settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Optimizing the test policy in resampling space is an effective technique to increase detection power for spurious, environment-dependent signals; sample-splitting is required to maintain valid type-I error guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e976.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e976.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IPW WLS Q-estimation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse-propensity-weighted weighted least squares estimator for Q^S</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An estimator that identifies pooled conditional means Q^S(x,a) across observed environments via reweighting observed data by 1/π^0(A|X) and fitting a parametric model with weighted least squares.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Inverse-propensity-weighted (IPW) weighted least squares for Q^S</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Under d-invariance, Q^S(x,a) = (1/L) ∑_ℓ E^{π^0,e_ℓ}[ R / π^0(A|X) | X^S=x, A=a ]. The paper estimates Q^S by minimizing a weighted squared-error objective with weights 1/π^0(A|X) across pooled environments, yielding a consistent estimator under regularity and overlap assumptions (π^0 bounded away from 0).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Offline multi-environment contextual bandit data</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Pooled offline datasets from several observed environments where logging policy probabilities π^0 are known/recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>When combined with invariant-set selection, estimator uses only X^S (d-invariant set) so it indirectly avoids modeling on distractors; the estimator itself does not specifically detect distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Policy confounding and environment-induced shifts handled via conditioning on d-invariant sets; estimator corrects for action-selection bias via IPW.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Inverse propensity weighting downweights examples with small logging probabilities; objective uses 1/π^0(A|X) as observation weights.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Consistent under stated assumptions (Proposition 3) when S is d-invariant and logging policy has overlap; in experiments yields good estimates for Q^S enabling robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IPW WLS provides an identifiable and consistent way to estimate pooled conditional means Q^S from logged multi-environment data, enabling policy optimization restricted to d-invariant covariate sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e976.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e976.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant policy learning (Alg1/2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Algorithm for learning optimal invariant policies (Algorithms 1 & 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical pipeline that tests all candidate covariate subsets for invariance using off-policy resampling tests, then performs off-policy optimization restricted to accepted invariant subsets to obtain policies with provable worst-case robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant policy learning algorithm (exhaustive/greedy search + off-policy invariance tests)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Iterate over subsets S of covariates (or use variable screening/greedy search), for each S: (1) use resampling to emulate a test policy π^S, (2) perform an invariance test (Algorithm 2) on the resampled data to accept/reject S, (3) if accepted, estimate Q^S via IPW WLS and derive optimal policy in Π^S via off-policy optimization; finally select the accepted S whose fitted policy has highest estimated reward. Uses sample-splitting, Bonferroni corrections, and GOF heuristics for resample sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated multi-environment contextual bandits and real-world/semi-real warfarin dosing datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline logged bandit datasets stratified by environment; experiments include linear SCM simulations and a semi-real clinical dosing dataset with introduced non-invariant predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit selection/removal of variables that are non-invariant across environments (i.e., likely distractors) using off-policy invariance tests; optional variable screening and greedy search to reduce combinatorics.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounding between X and R that varies with environment, policy-induced spurious associations, non-invariant predictors</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Resampling-based invariance testing (Algorithm 2) combined with residual equality tests; strong non-d-invariant variables identified via d-separation criteria and rejected by tests.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Inverse-propensity weighting in WLS estimation and weighted resampling implicitly downweight out-of-support or unlikely samples under target policies.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Statistical rejection of invariance for candidate S serves to refute that S contains stable causal predictors; under assumptions, accepted invariant policies are shown to be worst-case optimal (Proposition 1, Theorem 1).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Exhaustive or greedy subset search; optional optimization of test policy to maximize power (p-value minimization) before applying invariance test; sample-splitting to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Simulations: invariant policies (when S correctly identified) show bounded worst-case regret across unseen environments and outperform non-invariant policies in semi-real warfarin experiment with injected confounder. Warfarin original dataset: invariant method comparable to baselines (no loss of predictiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Policies trained using pooled data with non-invariant covariates can fail to generalize: in simulations, non-invariant policy's worst-case regret can be large/unbounded as environment shifts increase.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Experiments: single engineered non-invariant predictor in semi-real setup; simulations used one non-invariant X^1 variable alongside an invariant X^2</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining off-policy resampling-based invariance testing with IPW estimation and off-policy optimization yields policies that are provably robust under the paper's SCM assumptions; power of invariance detection increases with sample size and number of observed environments; sample-splitting and p-value-optimized test policies improve detectability while preserving test level.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Invariant Policy Learning: A Causal Perspective', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal Inference by Invariant Prediction <em>(Rating: 2)</em></li>
                <li>Invariant Risk Minimization <em>(Rating: 2)</em></li>
                <li>Off-policy invariance testing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-976",
    "paper_id": "paper-350bdcdcec6c84a5859ebb76b4a62db2c4eaf2a2",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "ICP",
            "name_full": "Invariant Causal Prediction",
            "brief_description": "An invariance-based causal discovery approach that identifies covariate sets whose conditional relationship to an outcome remains stable across environments; used as conceptual foundation for selecting non-spurious predictors.",
            "citation_title": "Causal Inference by Invariant Prediction",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "ICP searches for subsets S of observed covariates such that the conditional distribution P(R | X^S) is invariant across multiple environments; under faithfulness, the intersection of accepted invariant sets yields variables that are (with high confidence) causal parents/ancestors of R. It operationalizes detection of spurious correlates by rejecting sets whose conditional distributions change across environments.",
            "environment_name": "Multi-environment observational data / contextual bandit environments (general)",
            "environment_description": "Sets of (possibly heterogeneous) environments modeled via SCMs; not an interactive virtual lab per se but a multi-environment observational setting where environment labels are observed and used for invariance testing. Can be applied to simulated environments or real cohorts (e.g., clustered continents in warfarin study).",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via invariance testing across environments; intersection of accepted invariant sets yields robust predictors (removes features with environment-dependent associations).",
            "spurious_signal_types": "Confounding (hidden common causes leading to environment-dependent correlations), distributional shift-induced spurious correlations",
            "detection_method": "Test whether conditional distribution P(R | X^S) is equal across environments (in practice, test residual distributions after pooling and fitting conditional mean); non-invariance indicates spurious association.",
            "downweighting_method": null,
            "refutation_method": "Refutation by statistical rejection of invariance hypotheses: if P(R|X^S) differs across environments, then S is not invariant (spurious). Use of multiple environments and intersection over accepted sets to rule out spurious predictors.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Qualitatively: yields predictors that generalize across strong distributional shifts; in this paper, invariance-based selection led to bounded worst-case regret in simulations and improved robustness in semi-real experiment.",
            "performance_without_robustness": "Predictive methods that ignore invariance can perform well in training environments but may have large or unbounded worst-case regret when environment-induced confounding changes (shown in simulations).",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "ICP-style invariance is an appropriate principle for removing variables whose association with R is unstable across environments; it forms the conceptual backbone of the paper's invariant policy approach.",
            "uuid": "e976.0",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Off-policy invariance test",
            "name_full": "Off-policy invariance test (resampling-based, Thams et al. 2021)",
            "brief_description": "A resampling-based test that mimics target policies by weighted resampling of offline data, enabling testing of distributional invariance P(R | X^S) for policies different from the data-generating policy.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Off-policy invariance test (weighted resampling)",
            "method_description": "Given offline data collected under an initial policy π^0, this method constructs a target sample that approximates draws under a candidate test policy π^S by weighted resampling with weights r_i = π^S(a_i | x_i^S) / π^0(a_i | x_i). The resampled target datasets (one per observed environment) are then passed to an invariance test (e.g., residual equality tests). Theoretical level guarantees for the asymptotic test are provided (based on Thams et al. 2021).",
            "environment_name": "Offline multi-environment contextual bandit data (simulations and case study)",
            "environment_description": "Offline logged data collected from multiple environments/policies; not active experimentation, but allows off-policy inference by resampling to emulate alternative policies.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects distractors by testing invariance of conditional reward distributions under policies that would use only candidate subsets S, thereby identifying variables whose predictive power is spurious (environment-dependent).",
            "spurious_signal_types": "Confounding via unobserved U that induces environment-dependent correlations between some X and R; spurious correlates due to policy-induced paths.",
            "detection_method": "Construct target samples via importance-weighted resampling to emulate π^S and then perform tests for equality of P(R | X^S) across environments (residual distribution tests).",
            "downweighting_method": "Weighted resampling implicitly downweights examples unlikely under the test policy; inverse-propensity weighting (1/π^0(A|X)) is also used in estimators to recover pooled conditional means Q^S.",
            "refutation_method": "Reject invariance hypothesis for S if residual distributions differ across environments in the resampled data; this refutes that S is stable and hence refutes purported causal links that rely on S.",
            "uses_active_learning": false,
            "inquiry_strategy": "No active interventions; optimization of test policy is performed in resampling space to increase test power, but real-world experiments are not collected adaptively.",
            "performance_with_robustness": "Empirical: high acceptance rates for true invariant sets as sample size increases; in simulations the off-policy test correctly accepts the true invariant set {X^2} while rejecting non-invariant sets with increasing power as n grows and with more training environments.",
            "performance_without_robustness": "Baseline of testing invariance using the original logging policy π^0 can fail because π^0 may open policy-dependent paths, hiding true invariances; naive tests can wrongly reject invariant sets.",
            "has_ablation_study": true,
            "number_of_distractors": "In simulations: one strongly non-d-invariant variable (X^1); in semi-real warfarin: one injected non-invariant predictor",
            "key_findings": "Off-policy resampling enables valid invariance testing for candidate policies different from the data-collection policy; using resampling plus appropriate invariance tests recovers invariant sets even when data were collected under a policy that uses non-invariant covariates.",
            "uuid": "e976.1",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Invariant residual test (KW/MMD)",
            "name_full": "Invariant residual distribution test (Kruskal–Wallis / MMD)",
            "brief_description": "A practical invariance test that fits pooled conditional mean E[R | X^S] under a target policy and then tests whether residuals have the same distribution across environments using nonparametric tests like Kruskal–Wallis or kernel two-sample (MMD) tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Invariant residual distribution test",
            "method_description": "Pool resampled target-sample data across environments to fit an estimator of E[R | X^S] (via regression). Compute residuals R - E_hat[R | X^S] and then perform a multi-sample test for equality of residual distributions across environments. Implementations include Kruskal–Wallis for mean differences and kernel two-sample tests (MMD) for broader alternatives, with multiple-testing corrections where appropriate.",
            "environment_name": "Resampled offline multi-environment datasets (simulated bandits and warfarin case study)",
            "environment_description": "Offline datasets split by observed environment labels; test is applied to resampled data intended to approximate a particular test policy.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects distractors by revealing environment-conditional shifts in residuals after conditioning on X^S; when residual distributions differ, S is flagged as including spurious variables or missing confounders.",
            "spurious_signal_types": "Environment-dependent confounding, hidden confounders causing shifts in conditional distribution of R given X^S, policy-induced spurious paths",
            "detection_method": "Statistical hypothesis testing on residuals across environment groups (Kruskal–Wallis for mean differences; MMD or other kernel tests for distributional differences).",
            "downweighting_method": null,
            "refutation_method": "Statistically reject null of invariance (equal residual distributions), thereby refuting that S yields stable predictive relationship; used to exclude S from invariant candidate sets.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Simulation: Kruskal–Wallis-based test showed correct acceptances for the true invariant set and increasing power with sample size and number of environments; pointwise asymptotic level guarantees are provided in appendices.",
            "performance_without_robustness": "If the initial logging policy is used without resampling, residual tests can be confounded by policy-dependent paths and incorrectly reject true invariances.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Residual-based invariance tests (even simple nonparametric tests like Kruskal–Wallis) are effective when combined with off-policy resampling; kernel tests can improve power against richer alternatives.",
            "uuid": "e976.2",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Weighted resampling (importance)",
            "name_full": "Importance-weighted resampling to emulate target policies",
            "brief_description": "A practical resampling procedure that draws weighted samples from offline logged data to simulate draws from a different (target) policy, using weights r_i = π_target(a_i | x_i^S) / π^0(a_i | x_i).",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Importance-weighted resampling for off-policy testing",
            "method_description": "Compute per-example importance weights r_i proportional to the likelihood ratio between a target test policy π^S and the logging policy π^0, choose a resample size m_e per environment (GOF heuristic), and draw samples with probability proportional to product of selected weights. This yields a synthetic dataset approximating draws under π^S, which can be used for invariance tests and estimation.",
            "environment_name": "Offline multi-environment logged bandit data",
            "environment_description": "Logged datasets per environment collected under π^0; resampling is used to create target-samples for hypothetical policies without running new experiments.",
            "handles_distractors": true,
            "distractor_handling_technique": "By enabling invariance testing for candidate policies that condition only on S, it helps detect and thereby exclude distractor variables whose predictive relationship is not stable.",
            "spurious_signal_types": "Policy-induced spurious associations, environment-dependent confounding",
            "detection_method": "Indirect: enables detection via construction of target datasets on which residual invariance tests are performed.",
            "downweighting_method": "Resampling probabilities and inverse-propensity reweighting downweight examples unlikely under the target policy; estimators use 1/π^0(A|X) factors in weighted least squares.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Enables valid off-policy invariance testing with asymptotic level control (as proven/extended from Thams et al. 2021); empirically recovers invariant sets in simulation and real-data case study.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Weighted resampling is a viable tool to emulate alternative policies for invariance tests; combined with sample-splitting it preserves level while allowing power optimization.",
            "uuid": "e976.3",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Power-optim test policy",
            "name_full": "Test-policy optimization via p-value minimization (log-derivative SGD)",
            "brief_description": "A procedure that chooses a test policy within a parametric family to maximize the power (or minimize expected p-value) of the invariance test by stochastic gradient optimization over resampling probabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Test-policy power optimization",
            "method_description": "Parameterize candidate test policies π_θ^S (e.g., linear softmax). Define objective J(θ) = E[ p-value(D^{π_θ^S}) | D ] and compute gradients via the log-derivative trick: ∇J = E[∇ log P(D^{π_θ^S} | D) * pv(D^{π_θ^S}) | D]. Estimate gradient by Monte Carlo resampling and apply stochastic gradient descent; use sample-splitting to avoid invalidating test level. The result is a policy that (in expectation) yields strong signals of non-invariance if present.",
            "environment_name": "Offline multi-environment data used for resampling-based invariance testing",
            "environment_description": "No active interactions; optimization takes place in the resampling/emulation space to increase statistical detectability of non-invariance under candidate S.",
            "handles_distractors": true,
            "distractor_handling_technique": "Improves detection sensitivity to spurious variables by searching for a policy that amplifies environment-dependent signals (making distractors easier to detect and reject).",
            "spurious_signal_types": "Environment-dependent confounding and policy-induced spurious associations",
            "detection_method": "Indirect: produces target resampled datasets more likely to expose non-invariance, thereby increasing power of residual distribution tests to detect spurious signals.",
            "downweighting_method": null,
            "refutation_method": "By making p-values small for non-invariant S, the method increases the chance to statistically refute spurious relationships.",
            "uses_active_learning": false,
            "inquiry_strategy": "Search over parametric policies (e.g., linear softmax), optimize expected p-value via SGD using log-derivative estimator; employs sample splitting to preserve test validity.",
            "performance_with_robustness": "Empirically increases power to detect non-invariance in resampled tests, especially with limited sample sizes; recommended to use sample-splitting to avoid overfitting.",
            "performance_without_robustness": "Using a fixed or naive test policy can lead to lower power and failure to detect non-invariance in challenging settings.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Optimizing the test policy in resampling space is an effective technique to increase detection power for spurious, environment-dependent signals; sample-splitting is required to maintain valid type-I error guarantees.",
            "uuid": "e976.4",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "IPW WLS Q-estimation",
            "name_full": "Inverse-propensity-weighted weighted least squares estimator for Q^S",
            "brief_description": "An estimator that identifies pooled conditional means Q^S(x,a) across observed environments via reweighting observed data by 1/π^0(A|X) and fitting a parametric model with weighted least squares.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Inverse-propensity-weighted (IPW) weighted least squares for Q^S",
            "method_description": "Under d-invariance, Q^S(x,a) = (1/L) ∑_ℓ E^{π^0,e_ℓ}[ R / π^0(A|X) | X^S=x, A=a ]. The paper estimates Q^S by minimizing a weighted squared-error objective with weights 1/π^0(A|X) across pooled environments, yielding a consistent estimator under regularity and overlap assumptions (π^0 bounded away from 0).",
            "environment_name": "Offline multi-environment contextual bandit data",
            "environment_description": "Pooled offline datasets from several observed environments where logging policy probabilities π^0 are known/recorded.",
            "handles_distractors": null,
            "distractor_handling_technique": "When combined with invariant-set selection, estimator uses only X^S (d-invariant set) so it indirectly avoids modeling on distractors; the estimator itself does not specifically detect distractors.",
            "spurious_signal_types": "Policy confounding and environment-induced shifts handled via conditioning on d-invariant sets; estimator corrects for action-selection bias via IPW.",
            "detection_method": null,
            "downweighting_method": "Inverse propensity weighting downweights examples with small logging probabilities; objective uses 1/π^0(A|X) as observation weights.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Consistent under stated assumptions (Proposition 3) when S is d-invariant and logging policy has overlap; in experiments yields good estimates for Q^S enabling robust policies.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "IPW WLS provides an identifiable and consistent way to estimate pooled conditional means Q^S from logged multi-environment data, enabling policy optimization restricted to d-invariant covariate sets.",
            "uuid": "e976.5",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Invariant policy learning (Alg1/2)",
            "name_full": "Algorithm for learning optimal invariant policies (Algorithms 1 & 2)",
            "brief_description": "A practical pipeline that tests all candidate covariate subsets for invariance using off-policy resampling tests, then performs off-policy optimization restricted to accepted invariant subsets to obtain policies with provable worst-case robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Invariant policy learning algorithm (exhaustive/greedy search + off-policy invariance tests)",
            "method_description": "Iterate over subsets S of covariates (or use variable screening/greedy search), for each S: (1) use resampling to emulate a test policy π^S, (2) perform an invariance test (Algorithm 2) on the resampled data to accept/reject S, (3) if accepted, estimate Q^S via IPW WLS and derive optimal policy in Π^S via off-policy optimization; finally select the accepted S whose fitted policy has highest estimated reward. Uses sample-splitting, Bonferroni corrections, and GOF heuristics for resample sizes.",
            "environment_name": "Simulated multi-environment contextual bandits and real-world/semi-real warfarin dosing datasets",
            "environment_description": "Offline logged bandit datasets stratified by environment; experiments include linear SCM simulations and a semi-real clinical dosing dataset with introduced non-invariant predictor.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit selection/removal of variables that are non-invariant across environments (i.e., likely distractors) using off-policy invariance tests; optional variable screening and greedy search to reduce combinatorics.",
            "spurious_signal_types": "Hidden confounding between X and R that varies with environment, policy-induced spurious associations, non-invariant predictors",
            "detection_method": "Resampling-based invariance testing (Algorithm 2) combined with residual equality tests; strong non-d-invariant variables identified via d-separation criteria and rejected by tests.",
            "downweighting_method": "Inverse-propensity weighting in WLS estimation and weighted resampling implicitly downweight out-of-support or unlikely samples under target policies.",
            "refutation_method": "Statistical rejection of invariance for candidate S serves to refute that S contains stable causal predictors; under assumptions, accepted invariant policies are shown to be worst-case optimal (Proposition 1, Theorem 1).",
            "uses_active_learning": false,
            "inquiry_strategy": "Exhaustive or greedy subset search; optional optimization of test policy to maximize power (p-value minimization) before applying invariance test; sample-splitting to avoid overfitting.",
            "performance_with_robustness": "Simulations: invariant policies (when S correctly identified) show bounded worst-case regret across unseen environments and outperform non-invariant policies in semi-real warfarin experiment with injected confounder. Warfarin original dataset: invariant method comparable to baselines (no loss of predictiveness).",
            "performance_without_robustness": "Policies trained using pooled data with non-invariant covariates can fail to generalize: in simulations, non-invariant policy's worst-case regret can be large/unbounded as environment shifts increase.",
            "has_ablation_study": true,
            "number_of_distractors": "Experiments: single engineered non-invariant predictor in semi-real setup; simulations used one non-invariant X^1 variable alongside an invariant X^2",
            "key_findings": "Combining off-policy resampling-based invariance testing with IPW estimation and off-policy optimization yields policies that are provably robust under the paper's SCM assumptions; power of invariance detection increases with sample size and number of observed environments; sample-splitting and p-value-optimized test policies improve detectability while preserving test level.",
            "uuid": "e976.6",
            "source_info": {
                "paper_title": "Invariant Policy Learning: A Causal Perspective",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal Inference by Invariant Prediction",
            "rating": 2
        },
        {
            "paper_title": "Invariant Risk Minimization",
            "rating": 2
        },
        {
            "paper_title": "Off-policy invariance testing",
            "rating": 1
        }
    ],
    "cost": 0.0206795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Invariant Policy Learning: A Causal Perspective</h1>
<p>Sorawit Saengkyongam, Nikolaj Thams, Jonas Peters, and Niklas Pfister<br>University of Copenhagen, Denmark<br>Emails: {ss, thams, jonas.peters, np}@math.ku.dk</p>
<h4>Abstract</h4>
<p>Contextual bandit and reinforcement learning algorithms have been successfully used in various interactive learning systems such as online advertising, recommender systems, and dynamic pricing. However, they have yet to be widely adopted in high-stakes application domains, such as healthcare. One reason may be that existing approaches assume that the underlying mechanisms are static in the sense that they do not change over different environments. In many real-world systems, however, the mechanisms are subject to shifts across environments which may invalidate the static environment assumption. In this paper, we take a step toward tackling the problem of environmental shifts considering the framework of offline contextual bandits. We view the environmental shift problem through the lens of causality and propose multi-environment contextual bandits that allow for changes in the underlying mechanisms. We adopt the concept of invariance from the causality literature and introduce the notion of policy invariance. We argue that policy invariance is only relevant if unobserved variables are present and show that, in that case, an optimal invariant policy is guaranteed to generalize across environments under suitable assumptions. Our results establish concrete connections among causality, invariance, and contextual bandits.</p>
<h2>1 Introduction</h2>
<p>The problem of learning decision-making policies lies at the heart of learning systems. To adopt these learning systems in high-stakes application domains such as personalized medicine or autonomous driving, it is crucial that the learned policies are reliable even in environments that have never been encountered before. In this paper, we consider the problem of learning policies that are robust with respect to shifts across environments. We consider this question in the setup of offline contextual bandits, which provides a mathematical framework for tackling the above learning problems.</p>
<p>While recent studies in offline contextual bandits (Dudik et al., 2011; Bottou et al., 2013; Swaminathan and Joachims, 2015a,b; Kallus, 2018; Athey and Wager, 2021; Zhou et al., 2022) offer theoretical results and novel methodologies for policy learning from offline data, they primarily focus on a fixed-environment setting (from now on, we will refer to this as the equal distribution assumption) in which the underlying mechanisms do not change over time or over different environments. In practice, however, shifts between environments often occur, possibly invalidating the equal distribution assumption. In healthcare, for example, datasets from different hospitals may not come from the same underlying distribution. As a result, a learning agent that ignores environmental shifts may fail to generalize beyond the environments it was trained on.</p>
<p>In the supervised learning context, the environmental shift problem has been studied under different names, such as domain generalization, covariate shift adaptation, distributional robustness or out-of-distribution generalization (Sugiyama and Kawanabe, 2012; Muandet et al., 2013; Volpi et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2021). In domain generalization, the goal is to develop learning algorithms that are robust to changes in the test</p>
<p>distribution. Thus, a fundamental problem is how to characterize such changes. A promising direction relies on a causal framework to describe the changes through the concept of interventions (Schölkopf et al., 2012; Rojas-Carulla et al., 2018a; Magliacane et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2021). A key insight is that while purely predictive methods perform best if test and training distributions coincide, causal models generalize to arbitrarily strong interventions on the covariates because of the modularity property of structural causal models (see e.g., Pearl (2009)).</p>
<p>In real-world applications knowledge of the underlying causal graph and structural discrepancies between environments may not be available. In recent years, invariance-based methods have been exploited to learn the causal structure from data (Peters et al., 2016; Pfister et al., 2018; Heinze-Deml et al., 2018). In invariant causal prediction (Peters et al., 2016), for example, one assumes that the data are collected from different environments, each of which describes different underlying data-generating mechanisms, and uses this heterogeneity to learn the causal parents of an outcome variable $Y$. The underpinning assumption is the invariance assumption, which posits the existence of a set of covariates $X$ in which the mechanism between $X$ and $Y$ remains constant. A model based on such invariant covariates is guaranteed to generalize to all unseen environments.</p>
<p>Our paper delineates an explicit connection among causality, invariance, and the environmental shift problem in the context of contextual bandits. We develop a causal framework for characterizing the environmental shift problem, and provide a practical and theoretically sound solution based on the proposed framework.</p>
<p>Our contributions are threefold. First, we propose a multi-environment contextual bandit framework that represents mechanisms underlying a contextual bandit problem by structural causal models (SCMs; Pearl (2009)). The framework allows for changes in environments and thereby relaxes the equal distribution assumption. We define environments as different perturbations on the underlying SCM, and we evaluate the policy according to its worst-case performance in all environments. Second, using the proposed framework, we generalize the invariance assumption used in methods such as invariant causal prediction and define invariance properties for policies that, under certain assumptions, guarantee generalizability to unseen environments. Third, we develop an offline method for testing invariance under distributional (policy) shifts, and provide an algorithm for finding an optimal invariant policy. In addition, we highlight a setting in which causality and invariance are not necessary for solving the environmental shift problem. This insight takes us closer to understanding what causality can offer in offline contextual bandits.</p>
<p>The remainder of our paper is organized as follows. Sections 1.1 and 1.2 briefly review related work and introduces the offline contextual bandit problem. Section 2 formally defines a causal framework for multi-environment contextual bandits and the main objective of our problem's formulation. Drawing on the proposed framework, Section 3 introduces invariance properties for policies and provides the main theoretical contributions underpinning our solution for the environmental shift problem. Section 4 discusses the assumptions required to learn invariant policies from offline data and presents an algorithm for learning an optimal invariant policy. Section 5 provides simulation experiments that empirically verify our theoretical results. In Section 6, we apply our framework to a warfarin dosing study.</p>
<h1>1.1 Related Work</h1>
<p>Our work is most closely related to the line of work studying invariance and generalizability for prediction tasks in i.i.d. settings mentioned above (Rojas-Carulla et al., 2018a; Magliacane et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2021; Pfister et al., 2021). The environmental shift problem is also related to the problem of transportability in causal inference (Pearl and Bareinboim, 2011; Bareinboim and Pearl, 2014, 2016; Subbaswamy et al., 2019; Lee et al., 2020; Correa and Bareinboim, 2020) which aims to generalize causal findings from source environments to a target environment. Our work differs from the transportability literature: we neither assume prior knowledge of the underlying causal graph nor of the structural differences between environments. Instead, we only assume that invariances in the observed environments</p>
<p>are preserved in the target environment. Furthermore, while the goal in transportability is to derive whether and how one can identify a causal quantity (e.g., an interventional distribution) in the target environment based on data from the source environment, our goal is to learn worst-case optimal policies based on the source environments.</p>
<p>Graphical models have been used in reinforcement learning to represent the underlying Markov Decision Processes (MDP) under the framework of factored MDPs. Such methods, however, focus mainly on providing efficient planning algorithms rather than generalizing to a new environment (Kearns and Koller, 1999; Guestrin et al., 2003, 2002; Jonsson and Barto, 2006). Although some recent studies have explored the use of causality and invariance for tackling the environmental shift problem in contextual bandits and, more generally, reinforcement learning (Zhang et al., 2020; Sonar et al., 2021), the actual roles and benefits of causality and invariance remain unclear and under-explored.</p>
<p>Our framework differs from the framework of causal bandits (Lee and Bareinboim, 2018; Lattimore et al., 2016; Yabe et al., 2018; de Kroon et al., 2020). While causal bandits exploit causal knowledge (either assumed to be known a priori or estimated from data) for improving the finite sample performance in a single environment, our framework focuses on modeling distributional shifts and the ability to generalize to new environments. Another line of work has addressed the problem of policy evaluation and learning under unobserved confounding between the action and the reward variables (Bareinboim et al., 2015; Sen et al., 2017; Tennenholtz et al., 2020; Kallus and Zhou, 2020; Tennenholtz et al., 2021). In contrast, we consider the complementary problem of unobserved confounding between the covariates and the reward variables (see Section 3).</p>
<h1>1.2 Offline Contextual Bandits</h1>
<p>We briefly review the offline contextual bandit problem (Beygelzimer and Langford, 2009; Strehl et al., 2010), considering a setup in which some of the covariates (also known as context variables) are unobserved. More precisely, we assume that the covariates can be partitioned into observed and unobserved variables $X \in \boldsymbol{\mathcal { X }}$ and $U \in \boldsymbol{\mathcal { U }}$. Here, $\boldsymbol{X}$ and $\boldsymbol{\mathcal { U }}$ are metric spaces; the reader may think of $\boldsymbol{\mathcal { X }} \subseteq \mathbb{R}^{d}$ and $\boldsymbol{\mathcal { U }} \subseteq \mathbb{R}^{p}$. As in the standard contextual bandit setup (Langford and Zhang, 2008), for each round, we assume that the system generates a covariate vector $(X, U)$ and reveals only the observable $X$ to an agent. From the observed covariates $X$, the agent selects an action $A \in \mathcal{A}$ according to a policy $\pi: \boldsymbol{\mathcal { X }} \rightarrow \Delta(\mathcal{A})$ that maps the observed covariates to the probability simplex $\Delta(\mathcal{A})$ over the set of actions $\mathcal{A}$. (In this work, we assume $\mathcal{A}$ to be finite). Adapting commonly used notation, we write, for all $x \in \boldsymbol{\mathcal { X }}$ and $a \in \mathcal{A}, \pi(a \mid x):=\pi(x)(a)$. The agent then receives a reward $R$ depending on the chosen action $A$, and on both the observed and unobserved covariates $(X, U)$.</p>
<p>In the classical setting, one assumes that the covariates are drawn i.i.d. from a joint distribution $\mathbb{P}<em A="A" R="R" U_="U," X_="X," _mid="\mid">{X, U}$ (an assumption we will relax when introducing multi-environment contextual bandits in Section 2) and that the rewards are drawn from a conditional distribution $\mathbb{P}</em>$. The agent is evaluated based on the performance of its policy $\pi$ which is measured by the policy value:</p>
<p>$$
V(\pi):=\mathbb{E}<em U="U" X_="X,">{(X, U) \sim \mathbb{P}</em>}} \mathbb{E<em R="R" _mathbb_P="\mathbb{P" _sim="\sim">{A \sim \pi(X)} \mathbb{E}</em>[R]
$$}_{R \mid X, U, A}</p>
<p>The agent is now given a fixed training dataset that is collected offline: it consists of $n$ rounds from one or more different policies, i.e., $D:=\left{\left(X_{i}, A_{i}, R_{i}, \pi_{i}\left(X_{i}\right)\right)\right}<em i="i">{i=1}^{n}$, where $A</em> V(\pi)$.} \sim$ $\pi_{i}\left(X_{i}\right)^{1}$ for all $i \in{1, \ldots, n}$. The goal of the agent is then to find a policy $\pi$ that maximizes the policy value over a given policy class $\Pi$, i.e., $\pi^{*} \in \arg \max _{\pi \in \Pi</p>
<p>As mentioned, this setting assumes that the environment in which we deploy the agent is identical to the environment in which the training dataset was collected. Section 2 introduces a causal framework for multi-environment contextual bandits, a framework that relaxes the equal distribution assumption.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 A Causal Framework for Multi-environment Contextual Bandits</h1>
<p>Instead of having a fixed distribution $\mathbb{P}<em U="U" X_="X,">{X, U}$ over the covariates, we introduce a collection $\mathcal{E}$ of environments such that, in each round, the covariates are drawn from an environment-specific distribution $\mathbb{P}</em>$ in that round.}^{e}$ that depends on the environment $e \in \mathcal{E</p>
<p>In practice, the agent only observes part of the environments $\mathcal{E}^{\text {obs }} \subseteq \mathcal{E}$ and is expected to generalize well to all environments in $\mathcal{E}$ including the unseen environments $\mathcal{E} \backslash \mathcal{E}^{\text {obs }}$. To formalize the problem, we first introduce a model that puts assumptions on how environments change the distributions of $X, U$ and $R$. Specifically, an environment $e$ can only perturb the distribution of the reward $R$ through altering the distribution of the covariates $X$ and $U$. This constraint makes it possible to generalize information learned from one set of environments to another. In this formulation - even though the full conditional distribution of the reward $\mathbb{P}<em A="A" R="R" X_="X," _mid="\mid">{R \mid X, U, A}^{\pi, e}$ is assumed to be fixed across environments - the observable distribution $\mathbb{P}</em>$ after marginalizing out the unobserved $U$ may change from one environment to another (see, e.g., Figure 1(b))}^{\pi, e</p>
<p>Formally, the assumptions are constructed via an underlying class of SCMs indexed by the environment and policy. ${ }^{2}$</p>
<p>Setting 1 (Multi-environment (acyclic) SCMs for bandits). Let $\boldsymbol{\mathcal { X }}=\mathcal{X}^{1} \times \ldots \times \mathcal{X}^{d}$ and $\boldsymbol{\mathcal { U }}=\mathcal{U}^{1} \times \ldots \times \mathcal{U}^{p}$ be products of metric spaces, $\mathcal{A}=\left{a^{1}, \ldots, a^{k}\right}$ a discrete action space, $\Pi:={\boldsymbol{\mathcal { X }} \rightarrow \Delta(\mathcal{A})}$ the set of all policies, and $\mathcal{E}$ a collection of environments. For all $\pi \in \Pi$ and all $e \in \mathcal{E}$ we consider the following SCMs,</p>
<p>$$
\mathcal{S}(\pi, e): \quad\left{\begin{array}{l}
U:=s_{e}\left(X, U, \epsilon_{U}\right) \
X:=h_{e}\left(X, U, \epsilon_{X}\right) \
A:=g_{\pi}\left(X, \epsilon_{A}\right) \
R:=f\left(X, U, A, \epsilon_{R}\right)
\end{array}\right.
$$</p>
<p>where $(X, U, A, R) \in \boldsymbol{\mathcal { X }} \times \boldsymbol{\mathcal { U }} \times \mathcal{A} \times \mathbb{R},\left(s_{e}\right)<em e="e">{e \in \mathcal{E}},\left(h</em>\right)<em U="U">{e \in \mathcal{E}}$, and $f$ are measurable functions, $\epsilon=\left(\epsilon</em>$ (defined below) corresponding to the SCMs is acyclic, see Figure 1(b) and 1(c) for an example.}, \epsilon_{X}, \epsilon_{A}, \epsilon_{R}\right)$ is a random vector with independent components and a distribution $Q_{\epsilon}=$ $Q_{\epsilon_{U}} \otimes Q_{\epsilon_{X}} \otimes Q_{\epsilon_{A}} \otimes Q_{\epsilon_{R}}$, and $g_{\pi}$ and $Q_{\epsilon_{A}}$ are such that for all $x \in \boldsymbol{\mathcal { X }}$ it holds that $g_{\pi}\left(x, \epsilon_{A}\right)$ is a random variable on $\mathcal{A}$ with distribution $\pi(x)$. Figure 1(a) visualizes the coarse-grained structure of this setting. $U, X$, and $A$ should be thought of as random vectors. Accordingly, $h_{e}$, for example, is a function with a multivariate output; it is a short-hand notation in the sense that a component of $h_{e}$ does not need to depend on all $X$, for example. In particular, we assume that the graph $\mathcal{G</p>
<p>We assume there exists a probability measure $\mu$ on $\boldsymbol{\mathcal { X }} \times \boldsymbol{\mathcal { U }} \times \mathcal{A} \times \mathbb{R}$ such that for all $\pi \in \Pi$ and all $e \in \mathcal{E}$ the $\operatorname{SCM} \mathcal{S}(\pi, e)$ induces a unique distribution $\mathbb{P}^{\pi, e}$ over $(X, U, A, R)$ (see Bongers et al. (2016) for details) which is dominated by $\mu$ and marginally has full support on $\boldsymbol{\mathcal { X }}$. We denote the corresponding density by $p^{\pi, e}$ and the corresponding expectations by $\mathbb{E}^{\pi, e}$. Whenever a probability, density, or expectation does not depend on $\pi$, we omit $\pi$ and write $\mathbb{E}^{\pi}[X]$ rather than $\mathbb{E}^{\pi, e}[X]$, for example.</p>
<p>Some remarks regarding Setting 1 are in order: (1) We only use the SCMs as a flexible way of modeling the changes in the joint distribution with respect to the environment $e$ and the policy $\pi$. In particular, we do not use it to model any further intervention distributions that do not correspond to a change of policy or environment. (2) In practice, the precise form of the SCMs is unknown. Indeed, we will neither assume knowledge of the structural equations nor complete knowledge of the graph structure, except that the constraints induced by (1)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graphs summarizing different data-generating models. White and grey circles represent observed and hidden variables, respectively. (b) Here, {X<sup>2</sup>} is d-invariant, because R ⊥<sub>G(2)</sub> e | X<sup>S</sup>, see Definition 3. Any set S that contains X<sup>1</sup> is not d-invariant because of the open path e → X<sup>1</sup> ← U → R. In practice, we do not assume that the structure is known but test for invariances (12) from data. This requires testing under distributional shifts: even though {X<sup>2</sup>} is d-invariant, (12) may not hold for a policy π that depends on X<sup>1</sup> and X<sup>2</sup> because of the path e → X<sup>1</sup> → A → R. (c) A more complex model, where the environments do not act on all X variables. Although U has an edge into X<sup>3</sup>, the subset {X<sup>2</sup>, X<sup>3</sup>} is still a d-invariant set – there is no edge from e to X<sup>3</sup>. Again, every subset of variables containing X<sup>1</sup> is not d-invariant. (In fact, in examples (b) and (c), X<sup>1</sup> is a strongly non-d-invariant variable, see Definition 5, and cannot be part of a d-invariant set.)</p>
<p>hold. (3) The assumption of a dominating measure for all environments ensures that we can always assume the existence of densities while also switching across environments. In particular, this avoids any measure-theoretic difficulties regarding conditional distributions. (4) The assumption that the induced distributions over X have full support in all environments ensures that the generalization problem when moving from E<sup>obs</sup> to E does not involve any extrapolation. Additionally, it ensures that conditional expectations such as E<sup>π,e</sup>[R | X = x] can be uniquely defined for all x ∈ X as integrals of the conditional densities. (5) The environments are modelled fixed (and not random). However, we could also treat the environments as random variables which can be considered a special case of the fixed-environment setting (see Appendix E).</p>
<p>We now introduce the graph G over the variables (X<sup>1</sup>, . . . , X<sup>d</sup>, U<sup>1</sup>, . . . , U<sup>p</sup>, A, R) that visualizes the structure of the SCMs S(π,e) (for all π ∈ Π and e ∈ E). We draw edges corresponding to the assignments in (1), add edges from all X to A and add an environment node, which has edges into all variables whose assignments are not the same in all environments. This is similar to the selection diagrams in Pearl and Bareinboim (2011), with the difference that we consider multiple environments.</p>
<p>More precisely, G is constructed as follows: Each coordinate of the variables (X, U, A, R) corresponds to a node. The nodes are connected according to the assignments, that is, we draw a directed edge from a variable B to a variable C if, for at least one environment e ∈ E, the variable B appears on the right-hand side of the assignment of variable C (see Figure 1(b)</p>
<p>for an example). Let $\mathcal{I}<em U="U">{X} \subseteq{1, \ldots, d}$ and $\mathcal{I}</em>} \subseteq{1, \ldots, p}$ index the variables $X^{j}$ and $U^{\ell}$ for which the structural assignments $X^{j}:=h_{e}^{j}\left(X, U, \epsilon_{X}\right)$ and $U^{\ell}:=s_{e}^{\ell}\left(X, U, \epsilon_{U}\right)$ in (1) vary with $e$, i.e., where there exist $e, f \in \mathcal{E}$ such that $h_{e}^{j} \neq h_{f}^{j}$ or $s_{e}^{\ell} \neq s_{f}^{\ell}$, respectively. The environments $\mathcal{E}$ correspond to perturbations on variables $X^{\mathcal{I<em U="U">{X}}$ or $U^{\mathcal{I}</em>}}$, which implies that for each $e \in \mathcal{E}$ the distribution $\mathbb{P}^{\pi, e}\left(X^{\mathcal{I<em U="U">{X}}, U^{\mathcal{I}</em>}} \mid U^{{1, \ldots, p} \backslash \mathcal{I<em X="X">{U}}, X^{{1, \ldots, d} \backslash \mathcal{I}</em>}}\right)$ may vary. We augment the graph with a square node $e$ to represent the environments and draw a directed edge from the node $e$ to each of the perturbation targets $X^{\mathcal{I<em U="U">{X}}$ and $U^{\mathcal{I}</em>$. For completeness, we define $d$-separation in Appendix A.}}$. Furthermore, we draw edges from all nodes in $X$ to $A$ and mark them with $\pi$ (to represent their dependence on the policy). This graph $\mathcal{G}$ is assumed to be acyclic, that is, to not contain any directed cycles. By the Markov condition, which holds in SCMs (Peters et al., 2017), the graph $\mathcal{G}$ defined above encodes (conditional) independence statements, which we will see relate to invariance, through the concept of $d$-separation. More precisely, the Markov condition states that any $d$-separation statement in a graph implies conditional independence (Pearl, 2009; Lauritzen et al., 1990; Peters et al., 2017). Here, we refer to the standard definition of $d$-separation when not distinguishing between the different types of nodes and denote by $\Perp_{\mathcal{G}}$ a $d$-separation statement in a graph $\mathcal{G</p>
<p>For any $S \subseteq{1, \ldots, d}$, we also define $\mathcal{G}^{S}$ as the subgraph of $\mathcal{G}$, in which, instead of all $X$, only the covariates in $S$ point into $A$ :</p>
<p>$$
\mathcal{G}^{S}:=\text { subgraph of } \mathcal{G} \text { without edges } X^{{1, \ldots, d} \backslash S} \text { to } A
$$</p>
<p>Neither $\mathcal{G}$ nor $\mathcal{G}^{\mathcal{S}}$ depends on the choice of policy.
We are now ready to define contextual bandits with multiple environments.
Definition 1 (Multi-environment Contextual Bandits). Assume Setting 1. In a multi-environment contextual bandit setup, before the beginning of each round, the system is in an environment $e \in \mathcal{E}$. Then, the system generates a covariate vector $(X, U)$ and reveals only the observable $X$ and the environment label e to the agent. Based on the observed covariates $X$, the agent selects an action $A$ according to the policy $\pi: \boldsymbol{X} \rightarrow \Delta(\mathcal{A})$. The agent then receives a reward $R$, depending on the chosen action $A$ and on both the observed and unobserved covariates $(X, U)$. More precisely, we assume for all $i \in{1, \ldots, n}$ that $\left(X_{i}, U_{i}, A_{i}, R_{i}\right)$ are sampled independently according to $\mathbb{P}<em i="i">{X, U, A, R}^{\pi</em>|=1$, the setup reduces to a standard contextual bandit setup.}, e_{i}}$ (see Setting 1). The training data contains data from environments in $\mathcal{E}^{\text {obs }}$. When $\left|\mathcal{E}^{\text {obs }}\right|=|\mathcal{E</p>
<p>In the multi-environment contextual bandit setup, the covariates on different rounds are not identically distributed due to changes in the environments. We can thus use this framework to model situations, where the test environments differ from training environments. We illustrate this setting with the following example, which we will refer back to several times throughout the paper.</p>
<p>Example 1. Consider a linear multi-environment contextual bandit with the following underlying SCMs</p>
<p>$$
\mathcal{S}(\pi, e): \quad \begin{cases}U:=\epsilon_{U} &amp; \ X^{1}:=\gamma_{e} U+\epsilon_{X^{1}} &amp; \ X^{2}:=\alpha_{e}+\epsilon_{X^{2}} &amp; \ A:=g_{\pi}\left(X^{1}, X^{2}, \epsilon_{A}\right) &amp; \ R:= \begin{cases}\beta_{1} X^{2}+U+\epsilon_{R}, &amp; \text { if } A=0 \ \beta_{2} X^{2}-U+\epsilon_{R}, &amp; \text { if } A=1\end{cases}\end{cases}
$$</p>
<p>where $\epsilon_{R}, \epsilon_{A}, \epsilon_{X^{1}}, \epsilon_{X^{2}}$ are jointly independent noise variables with zero mean, $\gamma_{e}, \alpha_{e} \in \mathbb{R}$ for all $e \in \mathcal{E}, \beta_{1}, \beta_{2} \in \mathbb{R}$, and $\mathcal{A}={0,1}$. Figure 1(b) depicts the induced graph $\mathcal{G}$. In this example, the environments influence the observed covariates in two ways: (a) they change the mean of $X^{2}$ via $\alpha_{e}$ and (b) they change the conditional mean of $X^{1}$ given $U$ via $\gamma_{e}$, while the rest of the components remain fixed across different environments. Here, the environment-specific coefficient $\gamma_{e}$ modifies the correlation between the observable $X^{1}$ and the unobserved variable</p>
<p>$U$, and consequently between $X^{1}$ and the reward $R$. Thus, an agent that uses information from $X^{1}$ to predict the reward $R$ in the training environments may fail to generalize to other environments. To see this, consider a training environment $e=1$ and a test environment $e=2$ and let $\gamma_{1}=1, \gamma_{2}=-1$ be the environment-specific coefficients in the training and test environments, respectively. In the training environment, we have a large positive correlation between $X^{1}$ and $U$, and consequently the agent will learn that the action $A=0$ yields a higher expected reward when observing a positive value of $X^{1}$ (and $A=1$ otherwise). However, the correlation between $X^{1}$ and $U$ becomes negative (and large in absolute value) in the test environment, which means that the policy that the agent learned from the training environment will now be harmful. We will see in Section 3 that a policy that depends on a d-invariant set $\left(\left{X^{2}\right}\right.$ in this example) does not suffer from this generalization problem and is guaranteed to generalize across different environments.</p>
<p>A similar structure appears in the medical example discussed in Section 6. There, $A$ is the dose of a drug, $R$ is a response variable, $X$ are observed patient features and $U$ are unobserved genetic factors. The environment $e$ is (a proxy of) the continent on which the data was collected.</p>
<h1>2.1 Distributionally Robust Policies</h1>
<p>To evaluate the performance of an agent across different environments, we define a policy value that takes into account environments. In particular, we focus on the worst-case performance of an agent across environments.</p>
<p>Definition 2 (Robust Policy Value). For a fixed policy $\pi \in \Pi$, and a set of environments $\mathcal{E}$, we define the robust policy value $V^{\mathcal{E}}(\pi) \in \mathbb{R}$ as the worst-case expected reward</p>
<p>$$
V^{\mathcal{E}}(\pi):=\inf _{e \in \mathcal{E}} \mathbb{E}^{\pi, e}[R]
$$</p>
<p>Intuitively, an agent that maximizes the robust policy value is expected to perform well (relative to other agents) in the most harmful environment. The idea of optimizing worst-case performance has been suggested in the reinforcement learning literature (Garcia and Fernández, 2015; Amodei et al., 2016) to ensure safe behavior of an agent and prevent catastrophic events and has also been used to formulate adversarial training (Bai et al., 2021) as well as out-ofdistribution generalization (Ye et al., 2021).</p>
<p>We now assume that, for several observed environments, we are given an i.i.d. sample from a multi-environment contextual bandit, see Definition 1. More precisely, we assume to observe $D:=\left{\left(X_{i}, A_{i}, R_{i}, \pi_{i}\left(X_{i}\right), e_{i}\right)\right}<em i="i">{i=1}^{n}$, where $e</em>} \in \mathcal{E}^{\text {obs }}, A_{i} \sim \pi_{i}\left(X_{i}\right),\left(X_{i}, A_{i}, R_{i}\right) \stackrel{\text { inst. }}{=} \mathbb{P<em i="i">{X, A, R}^{\pi</em>$ :}, e_{i}}$ for all $i \in{1, \ldots, n}$. Using only $D$, we aim to solve the following maximin problem ${ }^{3</p>
<p>$$
\arg \max _{\pi \in \Pi} V^{\mathcal{E}}(\pi)
$$</p>
<p>If we do not observe all the environments, solving the maximin problem (4) is impossible without further assumptions. A baseline approach to this problem is to pool the data from all training environments and learn a policy that maximizes the policy value ignoring the environment structure. We show in Appendix B that this is indeed optimal if the observed covariates explain all of the environment based distributional shifts in $R$, e.g., if all relevant covariates have been observed. However, if for example, hidden variables are present, the pooling approach does not necessarily yield an optimal policy and the learned policy may fail to generalize to unseen test environments.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Invariant Policies for Distributional Robustness</h1>
<p>We now consider the general case in which the environment shifts may not be explained by the observed covariates. To this end, we introduce $d$-invariant sets and policies, and show that, under Setting 1, the maximin problem (4) can be reduced to finding an optimal $d$-invariant policy given certain assumptions, see Proposition 1 and Theorem 1. This becomes particularly relevant if important variables remain unobserved. If all variables are observed, it suffices to pool the observed environments.</p>
<p>Remark 1. If there are no hidden variables, one can solve the objective (4) by a standard policy optimization using all covariates $X$, without taking into account further concepts such as invariance or causality. This statement is made precise and proved as Proposition 5 in Appendix B.</p>
<p>Nevertheless, in more realistic cases (see e.g., Figures 1(b) and 1(c)), $d$-invariant sets and policies (introduced below) play a central role in solving the distributionally robust objective (4).</p>
<p>Definition 3 ( $d$-invariant Sets ${ }^{4}$ ). A subset $S \subseteq{1, \ldots, d}$ is said to be $d$-invariant if the following d-separation statement holds:</p>
<p>$$
R \Perp_{\mathcal{G}^{S}} e \mid X^{S}
$$</p>
<p>where $\mathcal{G}^{S}$ is defined in (2).
Our approach relies on the existence of a $d$-invariant set. We therefore make this assumption explicit.</p>
<p>Assumption 1. There exists a subset $S \subseteq{1, \ldots, d}$ such that $S$ is d-invariant.
Under faithfulness (Pearl, 2009), Assumption 1 is testable from the observed data (see Section 4.1). Next, we define $d$-invariant policies. For all subsets $S \subseteq{1, \ldots, d}$, let us denote the set of all policies that depend only on $X^{S}$ by $\Pi^{S}:=\left{\pi \in \Pi \mid \exists \pi^{S}: \boldsymbol{X}^{S} \rightarrow \Delta(\mathcal{A})\right.$ s.t. $\forall x \in$ $\mathcal{X}, \pi(\cdot \mid x)=\pi^{S}\left(\cdot \mid x^{S}\right)} \subseteq \Pi$.</p>
<p>Definition 4 ( $d$-invariant Policies). A policy $\pi$ is said to be $d$-invariant with respect to a subset $S \subseteq{1, \ldots, d}$ if $S$ is a $d$-invariant set and $\pi \in \Pi^{S}$.</p>
<p>We denote by $\mathbf{S}<em _inv="{inv" _text="\text">{\text {inv }}:={S \subseteq{1, \ldots, d} \mid S$ is $d$-invariant $}$ the collection of all $d$-invariant sets and $\Pi</em>$ from the observed data.}}:=\left{\pi \in \Pi \mid \exists S\right.$ s.t. $\pi$ is $d$-invariant w.r.t. $S}$ the collection of $d$-invariant policies. For now, we assume to have access to the set of $d$-invariant policies $\Pi_{\text {inv }}$. Section 4 discusses when and how we can learn $\Pi_{\text {inv }</p>
<p>Because of the hidden variables $U$, the conditional mean $\mathbb{E}^{\pi, e}[R \mid X=x]$ is not ensured to be stable across the environments. Nevertheless, a $d$-invariant policy ensures that parts of the conditional mean are unchanged across environments.</p>
<p>Lemma 1. Let $S \in \mathbf{S}_{\text {inv }}$ be a d-invariant set and $\pi \in \Pi^{S}$. It holds for all $e, f \in \mathcal{E}$ and $x \in \boldsymbol{X}^{S}$ that</p>
<p>$$
\mathbb{E}^{\pi, e}\left[R \mid X^{S}=x\right]=\mathbb{E}^{\pi, f}\left[R \mid X^{S}=x\right]
$$</p>
<p>Proof. See Appendix D.3.
For $S \in \mathbf{S}<em _inv="{inv" _text="\text">{\text {inv }}$, Lemma 1 implies that if a policy $\pi \in \Pi^{S}$ is optimal among $\Pi^{S}$ in the observed environments, then $\pi$ is also optimal among $\Pi^{S}$ in all environments (Proposition 1(i)). With the following assumption, we show in Proposition 1(ii) that the same holds when replacing $\Pi^{S}$ by $\Pi</em>$.}</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Assumption 2. Let $\mathcal{G}$ be the graph of the SCMs in Setting 1. Then, for all $\ell \in{1, \ldots, p}$, there must be an edge from $U^{\ell}$ to $R$ in $\mathcal{G}$.</p>
<p>Proposition 1. Assume Setting 1 and Assumption 1. Then the following statements hold.
(i) Let $S \in \mathbf{S}<em _opt="{opt" _text="\text">{\text {inv }}$ and $\pi</em> \in \arg \max }}^{S<em _in="\in" _mathcal_E="\mathcal{E" e="e">{\pi \in \Pi^{S}} \sum</em>[R]$. We then have}^{\text {obs }}} \mathbb{E}^{\pi, e</p>
<p>$$
\forall \pi \in \Pi^{S}: \quad V^{\mathcal{E}}(\pi) \leq V^{\mathcal{E}}\left(\pi_{\mathrm{opt}}^{S}\right)
$$</p>
<p>(ii) Let $\pi^{*} \in \arg \max <em _inv="{inv" _text="\text">{\pi \in \Pi</em>[R]$. If Assumption 2 holds, we have}}} \sum_{e \in \mathcal{E}^{\text {obs }}} \mathbb{E}^{\pi, e</p>
<p>$$
\forall \pi \in \Pi_{\mathrm{inv}}: \quad V^{\mathcal{E}}(\pi) \leq V^{\mathcal{E}}\left(\pi^{*}\right)
$$</p>
<p>Proof. See Appendix D.5.
Proposition 1 shows that a $d$-invariant policy that is optimal under the observed environments outperforms all other $d$-invariant policies, even on the test environments. But what about other policies that are not $d$-invariant? We will see in Theorem 1 that under certain assumptions on the set $\mathcal{E}$ of environments, they cannot perform better than the above $\pi^{*}$ either.</p>
<p>The key argument in the proof of Proposition 1(ii) is the identifiability of the optimal $d$ invariant set. Assumption 2 is necessary for this identifiablity: if the assumption is violated and there are multiple $d$-invariant sets, one can, in general, not say which of those $d$-invariant sets is optimal with respect to all environments $\mathcal{E}$ (see Appendix L for a more detailed discussion). While, without Assumption 2, the $d$-invariant set that is most predictive on $\mathcal{E}^{\text {obs }}$ is no longer guaranteed to be worst-case optimal, it still satisfies a weaker guarantee shown in Theorem 1(i) below.</p>
<p>We now outline the assumptions on the set $\mathcal{E}$ of environments facilitating this result. As seen in Example 1, the crucial difference between a $d$-invariant policy $\pi^{{2}}$ (a policy that only depends on $X^{2}$ ) and a non- $d$-invariant policy $\pi^{{1,2}}$ (a policy that depends on both $X^{1}$ and $X^{2}$ ) is that $\pi^{{1,2}}$ can use information related to variables confounded with the reward ( $X^{1}$ in this example) that may change across environments. In cases where the environments do not change the system 'too strongly' it can therefore happen that using such information is beneficial across all environments. In practice, however, one might not know how strong the test environments can change the system in which case such information can become useless or even harmful. Intuitively, this happens, for example, if environments exist where the non- $d$ invariant confounded variables no longer contain any information about the reward. Formally, we make the following definition.</p>
<p>Definition 5 (Confounding Removing Environments). For $j \in{1, \ldots, d}$, we say that the variable $X^{j}$ is strongly non-d-invariant if for all $S \subseteq{1, \ldots, d}$</p>
<p>$$
R \mathbb{L}_{\mathcal{G}^{S}} e \mid X^{S \cup{j}}
$$</p>
<p>An environment $e \in \mathcal{E}$ is said to be a confounding removing environment if for all $\pi \in \Pi$ it holds that</p>
<p>$$
X^{j} \Perp_{\mathcal{G}^{\pi, e}} U
$$</p>
<p>for all strongly non-d-invariant variables $X^{j}$, where $\mathcal{G}^{\pi, e}$ is the graph induced by the SCM $\mathcal{S}(\pi, e)$.</p>
<p>The two d-separation statements in Definition 5 are in different graphs: Both graphs $\mathcal{G}^{S}$ and $\mathcal{G}^{\pi, e}$ are subgraphs of $\mathcal{G}$. The distinction that is important for this definition is that while $\mathcal{G}^{S}$ contains all edges between the covariates $(X, U)$ that appear in at least one environment, the graph $\mathcal{G}^{\pi, e}$ only contains the edges that are active in the environment $e \in \mathcal{E}$. Furthermore, to provide more understanding of the strongly non- $d$-invariant variables, we characterize a graphical criterion for such variables in Appendix D.4. There we show that the strongly non- $d$ invariant variables are the variables that are directly affected by $e$ and are confounded with $R$</p>
<p>through $U$, and descendants of such variables. These strongly non- $d$-invariant variables should not be included if one wants to find $d$-invariant sets. For example in Figure 1(c), the variable $X^{1}$ is strongly non- $d$-invariant and the $d$-invariant sets $\left{X^{2}\right}$ and $\left{X^{2}, X^{3}\right}$ are the sets that do not contain $X^{1}$.</p>
<p>To give an example of a confounding removing environment, consider the graph $\mathcal{G}^{S}$ in Example 1 (see Figure 1(b)). For any subset $S$ where ${1} \subseteq S$ the path $e \rightarrow X^{1} \leftarrow U \rightarrow R$ is open, and therefore $X^{1}$ is strongly non- $d$-invariant. A confounding removing environment is an environment that removes the incoming edge from $U$ to $X^{1}$. In such an environment, the variable $X^{1}$ does not contain any information about the reward $R$. A similar notion of confounding removing environments is used in Christiansen et al. (2021) in the setting of prediction.</p>
<p>The existence of confounding removing environments implies that at least in some of the environments it is impossible to benefit from a non- $d$-invariant policy. To ensure that one cannot benefit in the worst-case, we therefore introduce the following assumption.</p>
<p>Assumption 3. For all $e \in \mathcal{E}$, there exists $f \in \mathcal{E}$ such that $f$ is a confounding removing environment and it holds that $\mathbb{P}<em X="X">{X}^{e}=\mathbb{P}</em>$.}^{f</p>
<p>To give an example, let $\mathcal{I} \subseteq{1, \ldots, d}$ index the variables $X^{j}$ for which there is an edge from $e$ to $X^{j}$ in the graph $\mathcal{G}$. If the set $\mathcal{E}$ of environments consists of arbitrary interventions on $X^{\mathcal{I}}$, then Assumption 3 is satisfied.</p>
<p>Theorem 1. Assume Setting 1 and Assumption 1. Let $\pi^{<em>}$ be an optimal d-invariant policy under the observed environments, $\pi^{</em>} \in \arg \max <em _inv="{inv" _text="\text">{\pi \in \Pi</em>[R]$. We then have the following statements.
(i) Let $\pi_{a}$ be the policy that always chooses an action $a \in \mathcal{A}$. We have for all $e \in \mathcal{E}$ that}}} \sum_{e \in \mathcal{E}^{\text {obs }}} \mathbb{E}^{\pi, e</p>
<p>$$
\max <em a="a">{a \in \mathcal{A}} \mathbb{E}^{\pi</em>[R]
$$}, e}[R]^{5} \leq \mathbb{E}^{\pi^{*}, e</p>
<p>(ii) If Assumptions 2 and 3 hold, we have</p>
<p>$$
\forall \pi \in \Pi: \quad V^{\mathcal{E}}(\pi) \leq V^{\mathcal{E}}\left(\pi^{*}\right)
$$</p>
<p>Proof. See Appendix D.6.
The first statement of Theorem 1 implies that in all environments the expected reward under an optimal $d$-invariant policy is larger than any optimal context-free policy. In other words, the information gained from the $d$-invariant set of covariates (the set that $\pi^{<em>}$ depends on) is generalizable across environments in the sense that it is not harmful in any environment. The second statement states that if the environments $\mathcal{E}$ are sufficiently strong (Assumption 3) then an optimal $d$-invariant policy $\pi^{</em>}$ maximizes the robust policy value $V^{\mathcal{E}}$.</p>
<p>The above results motivate a procedure to solve the distributionally robust objective (4). Proposition 1 implies that if we consider a policy class containing only the $d$-invariant policies, the maximin problem reduces to a standard policy optimization problem. Theorem 1 shows that an optimal $d$-invariant policy, under Assumption 3, is a solution to the distributionally robust objective. In other words, given a training dataset $D$, we seek to operationalize the following two steps: (a) find the set $\Pi_{\text {inv }}$ of all $d$-invariant policies (Section 4.1 discusses under which assumptions this is possible), (b) use offline policy optimization to solve $\arg \max <em _inv="{inv" _text="\text">{\pi \in \Pi</em>(\pi)$ on the dataset $D$.}}} V^{\mathcal{E}^{\text {obs }}</p>
<p>One of the key components of the proposed method is to test whether a policy $\pi$, which may be different from the policy generating the data, is $d$-invariant using data obtained from the observed environments $\mathcal{E}^{\text {obs }}$. The following section proposes such a test, discusses the assumptions required to learn the set of $d$-invariant policies, and gives a detailed description of the whole procedure.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4 Learning an Optimal Invariant Policy</h1>
<h3>4.1 Learning Invariant Sets</h3>
<p>Our theoretical results (Proposition 1 and Theorem 1) in the previous section assume that the set of all $d$-invariant policies $\Pi_{\text {inv }}$ is given. We now turn to the task of learning $\Pi_{\text {inv }}$ which boils down to searching for the collection of all $d$-invariant sets $\mathbf{S}^{\text {inv }}$ using data obtained from the observed environments $\mathcal{E}^{\text {obs }}$. To this end, we first define, for all $S \subseteq{1, \ldots, d}, \pi \in \Pi$ and $\mathcal{E}^{\prime} \subseteq \mathcal{E}$, the null hypothesis</p>
<p>$$
H_{0}\left(S, \pi, \mathcal{E}^{\prime}\right): \mathbb{P}_{R \mid X^{S}}^{\pi, e} \text { is the same for all } e \in \mathcal{E}^{\prime}
$$</p>
<p>In the case $\mathcal{E}^{\prime}=\mathcal{E}^{o b s}$, we refer to $H_{0}\left(S, \pi, \mathcal{E}^{o b s}\right)$ as $\mathcal{E}^{o b s}$-invariance (which does not consider the unseen environments). Furthermore, we call a set $S$ invariant if there exists $\pi \in \Pi^{S}$ such that $H_{0}\left(S, \pi, \mathcal{E}^{o b s}\right)$ holds and a policy $\pi$ invariant with respect to $S$ if $\pi \in \Pi^{S}$ and $S$ is invariant. We now state our core assumptions that make learning possible.</p>
<p>Assumption 4. For all $S \subseteq{1, \ldots, d}$, the following holds:
(i) $\exists \pi \in \Pi^{S}: H_{0}(S, \pi, \mathcal{E})$ true $\Longrightarrow R \Perp \mathcal{G}^{S} e \mid X^{S}$
(ii) $\forall \pi \in \Pi^{S}: H_{0}\left(S, \pi, \mathcal{E}^{\text {obs }}\right)$ true $\Longrightarrow H_{0}(S, \pi, \mathcal{E})$ true</p>
<p>Assumption 4(i) connects the conditional distribution invariance used in the null hypothesis (12) to the $d$-invariance condition given in (5) (The reversed implication follows by Lemma 3, Appendix D.3.) This assumption is a special case of the faithfulness assumption (Pearl, 2009) which is a fundamental assumption in causal discovery methods (e.g., Glymour et al. (2019)) that, in linear SCMs, holds with probability one if the linear coefficients are drawn from a distribution that is absolutely continuous with respect to Lebesgue measure (Meek, 1995; Spirtes et al., 2000). Assumption 4(ii) ensures that any invariance found in the observed environments $\mathcal{E}^{\text {obs }}$ can be generalized to all environments $\mathcal{E}$. Implicitly, it requires that the observed environments are sufficiently heterogeneous ${ }^{6}$. This type of assumption is also at the core of other invariance-based methods (Rojas-Carulla et al., 2018b; Magliacane et al., 2018; Arjovsky et al., 2019; Pfister et al., 2021).</p>
<p>At first glance, Assumption 4(i) suggests that we have to check the hypothesis $H_{0}(S, \pi, \mathcal{E})$ for all $\pi \in \Pi^{S}$ to conclude whether or not $S$ is $d$-invariant. Fortunately, as shown in Proposition 2, we actually only need to check the null hypothesis for a single $\pi \in \Pi^{S}$.</p>
<p>Proposition 2. Assume Setting 1 and Assumption 4. Then, for all subsets $S \subseteq{1, \ldots, d}$ and for all policies $\pi, \tilde{\pi} \in \Pi^{S}$, it holds that</p>
<p>$$
H_{0}(S, \pi, \mathcal{E}) \text { true } \Longleftrightarrow H_{0}(S, \tilde{\pi}, \mathcal{E}) \text { true. }
$$</p>
<p>Proof. See Appendix D.9.
Assumption 4 and Proposition 2 make the learning problem tractable. The task of testing whether a set $S$ is $d$-invariant boils down to testing the $\mathcal{E}^{\text {obs }}$-invariance hypothesis $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$ for a single $\pi^{S} \in \Pi^{S}$. We therefore have the flexibility to choose any $\pi^{S}$ from $\Pi^{S}$ to test the hypothesis (called the test policy). We discuss strategies for choosing the test policy in Section 4.4.</p>
<p>Testing $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$ for $\pi^{S} \in \Pi^{S}$ by directly checking for a change in the conditional distributions across environments in the observed data is, however, not in general possible. This is because the observed data may have been generated based on an initial policy $\pi^{0}$ that does not satisfy $\pi^{0} \in \Pi^{S}$. It can therefore happen that $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$ is true but $H_{0}\left(S, \pi^{0}, \mathcal{E}^{\text {obs }}\right)$ is not.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We illustrate this point using the example graph $\mathcal{G}$ given in Figure 1(b). For a policy depending only on $S={2}$ the environment $e$ is d-separated from $R$ given $X^{{2}}$ in $\mathcal{G}^{{2}}$, which implies that ${2}$ is $d$-invariant, and in particular that $H_{0}({2}, \pi^{{2}}, \mathcal{E}^{\text {obs }})$ is true by the Markov property (see Lemma 3 in Appendix D.3). However, if the initial policy $\pi^{0}$ depends on both $X^{1}$ and $X^{2}$, then the path $e \rightarrow X^{1} \rightarrow A \rightarrow R$ in Figure 1(b) is open, which implies, by Assumption 4, that $H_{0}({2}, \pi^{{1,2}}, \mathcal{E}^{\text {obs }})$ is not true. ${ }^{7}$</p>
<p>Thus, in general, we cannot directly test the $\mathcal{E}^{\text {obs }}$-invariance hypothesis of a set $S$ by using the observed data that were generated by the initial policy. Instead, we need to test $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$ for a policy $\pi^{S} \in \Pi^{S}$ that is different from the data-generating policy $\pi^{0}$ (by Proposition 2 it suffices to test a single policy). As we detail in the following section, we can do so by applying an off-policy test for invariance by resampling the data to mimic the policy $\pi^{S}$.</p>
<h1>4.2 Testing Invariance under Distributional Shifts</h1>
<p>Consider a fixed set $S \subseteq{1, \ldots, d}$ and a pre-specified test policy $\pi^{S} \in \Pi^{S}$ (see Section 4.4 for how to choose $\pi^{S}$ ). To test the hypothesis $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$, we apply the off-policy test from Thams et al. (2021), which draws a target sample from $\pi^{S}$ by resampling the offline data - drawn from $\pi^{0}$ - and then tests the invariance in this target sample. More formally, let $\mathcal{E}^{\text {obs }}:=\left{e_{1}, \ldots, e_{L}\right}$ and suppose that for every $e_{j} \in \mathcal{E}^{\text {obs }}$ a dataset $D^{e_{j}}$ consisting of $n_{e}$ observations $D^{e_{j}}=\left{\left(X_{i}^{e_{j}}, A_{i}^{e_{j}}, R_{i}^{e_{j}}, \pi^{0}\left(A_{i}^{e_{j}} \mid X_{i}^{e_{j}}\right)\right)\right}<em e__j="e_{j">{i=1}^{n</em>$ and show that the theoretical guarantees on the asymptotic level proved in Thams et al. (2021) also extend to our application.}}}$ is available. For each environment $e_{j}$, we draw a weighted resample $D^{e_{j}, \pi^{S}}$ of $D^{e_{j}}$ using the weighted resampling procedure introduced in Thams et al. (2021). ${ }^{8}$ We then apply an invariance test $\varphi^{S}\left(D^{e_{1}, \pi^{S}}, \ldots, D^{e_{L}, \pi^{S}}\right)$ to the resampled data, to test the $\mathcal{E}^{\text {obs }}$-invariance hypothesis $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$. An invariance hypothesis test $\varphi^{S}$ is a function (into ${0,1}$ ) that takes data from environments $e_{1}, \ldots, e_{L}$, each of size $m_{e_{i}}$, and tests whether $S$ is invariant. Here, $\varphi^{S}=1$ indicates that we reject the hypothesis of invariance. We detail a concrete test $\varphi^{S}$ in Section 4.4. In Appendix F, we provide details on the resampling scheme, that is, a formal definition of $D^{e_{j}, \pi^{S}</p>
<h3>4.3 Algorithm for Invariant Policy Learning</h3>
<p>The previous sections discuss finding invariant subsets $S$. We now discuss how to employ this in an algorithm that learns an optimal invariant policy. We assume that we are given an offpolicy optimization algorithm off_opt that takes as input a sample $D:=\left(D^{e_{1}}, \ldots, D^{e_{L}}\right)$ and a policy space $\Pi$, and returns an optimal policy $\pi^{<em>}$ and its estimated expected reward $\hat{\mathbb{E}}^{\pi^{</em>}}(R)$.</p>
<p>Here, we present one choice of off_opt that we use in the experimental section; our approach can also be applied with other off-policy optimization algorithms. Given a policy space $\Pi^{S}$, we consider an optimal policy of the form</p>
<p>$$
\pi^{S}(a \mid x):=\mathbb{1}\left[a=\underset{a^{\prime} \in \mathcal{A}}{\arg \max } Q^{S}\left(x, a^{\prime}\right)\right]
$$</p>
<p>where $Q^{S}(x, a):=\frac{1}{L} \sum_{\ell=1}^{L} \mathbb{E}^{\pi_{a}, e_{\ell}}\left[R \mid X^{S}=x\right]$ denotes the pooled conditional mean under the policy that always selects an action $a .{ }^{9}$</p>
<p>Let $\pi^{0}$ be an initial policy generating the sample $D$. By our assumption in Setting 1, the policy $\pi^{0}$ depends only on the observed covariates $X$. We therefore have that for all</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$S \subseteq{1, \ldots, d}$ the pooled conditional mean $Q^{S}(x, a)$ is identifiable for all $a \in \mathcal{A}$ and $x \in \mathcal{X}^{S}$ as shown in Lemma 2 below.</p>
<p>Lemma 2. Let $S \in \mathbf{S}_{\text {inv }}$ be a d-invariant set. It holds for all $x \in \mathcal{X}^{S}$ and all $a \in \mathcal{A}$ that</p>
<p>$$
Q^{S}(x, a)=\frac{1}{L} \sum_{\ell=1}^{L} \mathbb{E}^{\pi^{0}, e_{\ell}}\left[\frac{R}{\pi^{0}(A \mid X)} \mid X^{S}=x, A=a\right]
$$</p>
<p>Proof. See Appendix D.7.
Here, we express the causal quantity $Q^{S}(x, a)$ entirely in terms of expecations under the observed policy $\pi^{0}$ by using reweighting. Equivalently, one can also express $Q^{S}(x, a)$ with the backdoor adjustment formula (Pearl, 2009). While the two formulations are equivalent, the resulting estimators are different (see the discussion in Appendix C).</p>
<p>We propose to estimate $Q^{S}$ by a weighted least squares approach in which we consider a parameterized function class $\left{f_{\theta}: \boldsymbol{X}^{S} \times \mathcal{A} \rightarrow \mathbb{R} \mid \theta \in \Theta^{S}\right}$ and assume that there exists a unique $\theta_{0}^{S} \in \Theta^{S}$ such that for all $x \in \mathcal{X}^{S}$ and $a \in \mathcal{A}$ it holds that $Q^{S}(x, a)=f_{\theta_{0}^{S}}(x, a)$. That is, we consider</p>
<p>$$
\hat{\theta}<em _ell="1">{n}^{S}:=\underset{\theta \in \Theta^{S}}{\arg \min } \frac{1}{L} \sum</em>
$$}^{L} \frac{1}{n_{e_{\ell}}} \sum_{i=1}^{n_{e_{\ell}}}\frac{\left(f_{\theta}\left(A_{i}^{e_{\ell}}, X_{i}^{e_{\ell}} S\right)-R_{i}^{e_{\ell}}\right)^{2}}{\pi^{0}\left(A_{i}^{e_{\ell}} \mid X_{i}^{e_{\ell}}\right)</p>
<p>where $n:=\left(n_{e_{1}}, \ldots, n_{e_{L}}\right)$. We then plug the estimate $\widehat{Q}^{S}:=f_{\hat{\theta}<em n="n">{n}^{S}}$ into (14) to obtain our (estimated) optimal policy. Proposition 3 shows that, under some regularity conditions, $\hat{\theta}</em>$.}^{S}$ is a consistent estimate of $\theta_{0}^{S</p>
<p>Proposition 3. Assume Setting 1 and Assumption 1. Let $S \in \mathbf{S}^{\text {inv }}$ be a d-invariant set. Assume that
(i) $\Theta^{S}$ is compact,
(ii) there exists a unique $\theta_{0}^{S} \in \Theta^{S}$ s.t. $\forall x \in \mathcal{X}^{S}, \forall a \in \mathcal{A}: Q^{S}(x, a)=f_{\theta_{0}^{S}}(x, a) \mu$-a.s.,
(iii) $\forall x \in \mathcal{X}^{S}, \forall a \in \mathcal{A}: \theta \rightarrow f_{\theta}(x, a)$ is continuous on $\Theta^{S}$,
(iv) $\forall e \in \mathcal{E}^{\text {obs }}: \mathbb{E}^{\pi^{0}, e}\left[\sup <em _theta="\theta">{\theta \in \Theta^{S}}\left(R-f</em>\right]&lt;\infty$,
(v) $\exists \delta&gt;0$ s.t. $\forall x \in \mathcal{X}, \forall a \in \mathcal{A}: \pi^{0}(a \mid x) \geq \delta$.}(X, A)\right)^{2</p>
<p>Then, $\hat{\theta}<em 0="0">{n}^{S}$ is a consistent estimate of $\theta</em>}^{S}$, i.e., $\left|\hat{\theta<em 0="0">{n}^{S}-\theta</em>\right|}^{S<em e__1="e_{1">{\infty} \rightarrow 0$ in probability as $n</em> \rightarrow$ $\infty$.}}, \ldots, n_{e_{L}</p>
<p>Proof. See Appendix D.8.
We summarize the overall procedure for learning an optimal invariant policy, see Algorithm 1: The algorithm iterates over all subsets $S \subseteq{1, \ldots, d}$ and checks the invariance condition using the off-policy invariance test given in Algorithm 2. The choices of the hypothesis test $\psi^{S}$ and the test policy $\pi^{S}$ are discussed in Section 4.4. For each iteration, if the set $S$ is invariant, we learn an optimal policy $\pi_{S}^{<em>}$ within the policy space $\Pi^{S}$ and compute its estimated expected reward $\overline{\mathbb{E}}^{\pi_{S}^{</em>}}(R)$ using off_opt. Then, the algorithm returns an optimal policy $\pi_{S}^{<em>}$ such that the estimated expected reward $\overline{\mathbb{E}}^{\pi_{S}^{</em>}}(R)$ is maximized. Lastly, the algorithm returns null if no invariant sets are found.</p>
<p>Algorithm 1 requires us to iterate over all subset $S \subseteq{1, \ldots, d}$ which may be computationally intractable when $d$ is large. We suggest two approaches for reducing the computational complexity of the algorithm. First, one can use a variable screening method (e.g., Lasso regression Tibshirani (1996)) to filter out the variables that are not predictive of the reward. If an optimal invariant set is a subset of the Markov blanket $\mathrm{MB}(R)$ of the reward, applying a variable screening step prior to Algorithm 1 would not change the algorithm's output on the</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">Learning</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">optimal</span><span class="w"> </span><span class="n">invariant</span><span class="w"> </span><span class="n">policy</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">data</span><span class="w"> </span>\<span class="p">(</span><span class="n">D</span><span class="o">=</span>\<span class="n">left</span><span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e_</span><span class="p">{</span><span class="mi">1</span><span class="p">}},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e_</span><span class="p">{</span><span class="n">L</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">),</span><span class="w"> </span><span class="n">off</span><span class="o">-</span><span class="n">policy</span><span class="w"> </span><span class="n">optimization</span><span class="w"> </span><span class="n">off_opt</span><span class="p">,</span><span class="w"> </span><span class="n">hypothesis</span><span class="w"> </span><span class="n">tests</span>
<span class="w">        </span><span class="ow">and</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">policies</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span>\<span class="p">{</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">psi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">},</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span>\<span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">S</span><span class="w"> </span>\<span class="n">subseteq</span>\<span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">d</span>\<span class="p">}}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">initialize</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">reward</span><span class="w"> </span>\<span class="p">(</span>\<span class="nb">max</span><span class="w"> </span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">R</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="o">-</span>\<span class="n">infty</span>\<span class="p">);</span>
<span class="w">    </span><span class="n">initialize</span><span class="w"> </span><span class="n">optimal</span><span class="w"> </span><span class="n">invariant</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">inv</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="nb nb-Type">null</span><span class="w"> </span><span class="p">;</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">loop</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">subsets</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}(</span>\<span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">d</span>\<span class="p">})</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">invariance</span>
<span class="w">        </span><span class="n">is_inv</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">test_inv</span>\<span class="n">left</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">},</span><span class="w"> </span>\<span class="n">psi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">},</span><span class="w"> </span><span class="n">S</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="p">(</span><span class="n">see</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">update</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="n">invariant</span><span class="w"> </span><span class="n">set</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">is_inv</span><span class="w"> </span><span class="n">then</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">},</span><span class="w"> </span>\<span class="n">overline</span><span class="p">{</span>\<span class="n">mathbb</span><span class="p">{</span><span class="n">E</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}}(</span><span class="n">R</span><span class="p">)</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">off_opt</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="w"> </span>\<span class="n">Pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">);</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="nb">max</span><span class="w"> </span><span class="n">R</span><span class="o">&lt;</span>\<span class="n">overline</span><span class="p">{</span>\<span class="n">mathbb</span><span class="p">{</span><span class="n">E</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}}(</span><span class="n">R</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span>\<span class="p">(</span>\<span class="nb">max</span><span class="w"> </span>\<span class="n">mathrm</span><span class="p">{</span><span class="n">R</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">overline</span><span class="p">{</span>\<span class="n">mathbb</span><span class="p">{</span><span class="n">E</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span>\<span class="n">pi_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}}(</span><span class="n">R</span><span class="p">)</span>\<span class="p">);</span>
<span class="w">                </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">inv</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">pi_</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span>\<span class="p">);</span>
<span class="w">            </span><span class="n">end</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">    </span><span class="n">end</span>
<span class="w">    </span><span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">optimal</span><span class="w"> </span><span class="n">invariant</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">inv</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">*</span><span class="p">}</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="n">Testing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">invariance</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Function</span><span class="w"> </span><span class="n">test_inv</span><span class="p">(</span><span class="n">data</span><span class="w"> </span>\<span class="p">(</span><span class="n">D</span><span class="o">=</span>\<span class="n">left</span><span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e_</span><span class="p">{</span><span class="mi">1</span><span class="p">}},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e_</span><span class="p">{</span><span class="n">L</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">),</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">hypothesis</span><span class="w"> </span><span class="n">test</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">psi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">),</span>
<span class="w">    </span><span class="n">target</span><span class="w"> </span><span class="n">set</span><span class="w"> </span>\<span class="p">(</span><span class="n">S</span>\<span class="p">)</span><span class="w"> </span><span class="p">):</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">resampling</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">e</span><span class="o">=</span><span class="n">e_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">e_</span><span class="p">{</span><span class="n">L</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="o">|</span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="n">right</span><span class="o">|</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">compute</span><span class="w"> </span><span class="n">weights</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">frac</span><span class="p">{</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">a_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">}</span>\<span class="n">right</span><span class="p">)}{</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">a_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="n">right</span><span class="p">)}</span>\<span class="p">);</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">        </span><span class="n">choose</span><span class="w"> </span><span class="n">resampling</span><span class="w"> </span><span class="n">size</span><span class="w"> </span>\<span class="p">(</span><span class="n">m_</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">GOF</span><span class="o">-</span><span class="n">heuristic</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">Thams</span><span class="w"> </span><span class="n">et</span><span class="w"> </span><span class="n">al</span><span class="o">.</span><span class="w"> </span><span class="p">(</span><span class="mi">2021</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="w">        </span><span class="n">draw</span><span class="w"> </span>\<span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">,</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}}:</span><span class="o">=</span>\<span class="n">left</span><span class="p">(</span><span class="n">D_</span><span class="p">{</span><span class="n">i_</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">D_</span><span class="p">{</span><span class="n">i_</span><span class="p">{</span><span class="n">m_</span><span class="p">{</span><span class="n">e</span><span class="p">}}}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span>\<span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">prob</span><span class="o">.</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">propto</span><span class="w"> </span>\<span class="n">prod_</span><span class="p">{</span><span class="n">t</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">m_</span><span class="p">{</span><span class="n">e</span><span class="p">}}</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">i_</span><span class="p">{</span><span class="n">t</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="n">e</span><span class="p">}</span>\<span class="p">);</span>
<span class="w">    </span><span class="n">end</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="n">left</span><span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}},</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">D</span><span class="o">^</span><span class="p">{</span><span class="n">e_</span><span class="p">{</span><span class="n">L</span><span class="p">},</span><span class="w"> </span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">);</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">verifying</span><span class="w"> </span><span class="n">invariance</span><span class="w"> </span><span class="n">condition</span>
<span class="w">    </span><span class="n">is_invariant</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">psi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">D</span><span class="o">^</span><span class="p">{</span>\<span class="n">pi</span><span class="o">^</span><span class="p">{</span><span class="n">S</span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">is_invariant</span>
</code></pre></div>

<p>population level (see Peters et al. (2016); Rojas-Carulla et al. (2018b); Pfister et al. (2021)). This approach is particularly efficient when the Markov blanket is sparse, that is, $|\mathrm{MB}(R)| \ll d$.</p>
<p>Second, one may apply a greedy search instead of the exhaustive search in Algorithm 1. More specifically, we suggest to follow the greedy search introduced in Rojas-Carulla et al. (2018b). The greedy algorithm starts with an empty set $\bar{S}=\varnothing$. For each iteration, we search over the neighboring sets of the candidate set $\bar{S}$, which are obtained by adding or removing one predictor to or from $\bar{S}$. If any of the neighboring sets are accepted by the invaraince test, we select the one with the highest expected reward. If the test rejects all the neighbors, we select a neighbor that yields the largest p-value of the test.</p>
<h1>4.4 Specifications of the Target Test</h1>
<p>The resampling procedure detailed in Algorithm 2 requires a hypothesis test for the $\mathcal{E}^{\text {obs }}$ invariance null hypothesis that has power against the alternatives. We discuss one such test in Section 4.4.1 below. Moreover, in Sections 4.4.2 and 4.4.3, we discuss two choices of the test policy that aim to improve the power of the resampling test.</p>
<h3>4.4.1 Invariant residual distribution test</h3>
<p>We now detail a test $\varphi^{S}$ to test $\mathcal{E}^{\text {obs }}$-invariance in the target sample. We first pool data from all environments into one dataset and estimate the conditional $\mathbb{E}^{\pi^{S}}[R \mid X^{S}]$ using any prediction method (such as linear regression or a neural network). We then test whether the residuals $R-\mathbb{E}^{\pi^{S}}[R \mid X^{S}]$ are equally distributed across the environments $e \in \mathcal{E}$, i.e., we split the sample back into $L$ groups (corresponding to the environments) and test whether the residuals in these groups are equally distributed (see also Peters et al. (2016), for example). We then define $\varphi^{S}$ to be the composition of these operations, that is, $\varphi^{S}$ returns 1 if the test for equal distribution of the residuals is rejected.</p>
<p>In the simulation and the warfarin case study (Section 5 and 6), we use the KruskalWallis test (Kruskal and Wallis, 1952) to test whether the residuals have the same mean across environments; this test holds pointwise asymptotic level for all $\alpha \in(0,1)$ (see Proposition 7 in Appendix F). To obtain power against more alternatives, one could also use other tests, such as a two-sample kernel test with maximum mean discrepancy (Gretton et al., 2012) and then correct for the multiple testing using Bonferroni-corrections (see also Rojas-Carulla et al. (2018a), for example).</p>
<h3>4.4.2 Optimizing the test policy for power</h3>
<p>To check whether a subset $S$ is invariant, we only need to test the $\mathcal{E}^{\text {obs }}$-invariance for a single policy $\pi \in \Pi^{S}$ (see Proposition 2). This provides us with a degree of freedom that we can leverage. Intuitively, the non-invariance may be more easily detectable in some test policies compared to others. We can therefore try to find a policy that gives us the strongest signal for detecting non-invariance. We maximize the power of the test by minimizing the $p$-value of the test. In a population setting, this would return small $p$-values for non-invariant sets, whereas for invariant sets one would not be able to make the $p$-values arbitrarily small, since they are uniformly distributed. In a finite sample setting, this type of power optimization can lead to overfitting (which would break any level guarantees); to avoid this we use sample splitting.</p>
<p>As presented in Section 4.2, for each environment $e$, we obtain a target sample $D^{e, \pi^{S}}$ from a test policy $\pi^{S}$ by resampling the sample $D^{e}$ that was generated under the policy $\pi^{0}$, and then test $\mathcal{E}^{\text {obs }}$-invariance in the target sample. The probabilities for obtaining the reweighted sample conditioned on the original sample are given by the importance weights, see Appendix F. Here, we optimize the ability to detect non-invariance over a parameterized subclass of $\Pi^{S}$,</p>
<p>$$
\Pi_{S}^{\Theta}:=\left{\pi_{\theta}^{S} \mid \theta \in \Theta\right}
$$</p>
<p>where $\Theta=\times_{a \in \mathcal{A}} \mathbb{R}^{|S|}$ and $\pi_{\theta}^{S}$ is a linear softmax policy, i.e., for all $x^{S} \in \mathbb{R}^{|S|}$ and $a \in \mathcal{A}$ :</p>
<p>$$
\pi_{\theta}^{S}\left(a \mid x^{S}\right)=\frac{\exp \left(\theta_{a}^{\top} x^{S}\right)}{\sum_{a^{\prime}} \exp \left(\theta_{a^{\prime}}^{\top} x^{S}\right)}
$$</p>
<p>This is the parameterization we chose in the experiments below, but other choices work, too.
To check for the $\mathcal{E}^{\text {obs }}$-invariance condition of a subset $S$, the idea is then to find a policy $\pi_{\theta}^{S} \in \Pi_{S}^{\Theta}$ such that, in expectation, the test power is maximized, i.e., we need to solve the following optimization problem:</p>
<p>$$
\underset{\theta \in \Theta}{\arg \max } \mathbb{E}\left[\mathfrak{p w}\left(D^{\pi_{\theta}^{S}}\right) \mid D\right]
$$</p>
<p>where $D:=\left(D^{e_{1}}, \ldots, D^{e_{L}}\right)$ is all the observed data and pw is a function that takes as input the reweighted sample $D^{\pi_{\theta}^{S}}$ and outputs the power of the test. Since we condition on $D$, the expectation is only with respect to the resampling of $D^{\pi_{\theta}^{S}}$. For many invariance tests, the test power $\mathrm{pw}\left(D^{\pi_{\theta}^{S}}\right)$ cannot be directly obtained, but one can minimize the $p$-value of the test instead. This motivates the objective function</p>
<p>$$
\underset{\theta \in \Theta}{\arg \min } \mathbb{E}\left[\mathrm{pv}\left(D^{\pi_{\theta}^{S}}\right) \mid D\right]
$$</p>
<p>where pv is a function that takes as input the reweighted sample $D^{\pi_{\theta}^{S}}$ and outputs the pvalue of the test. We then employ gradient-based optimization algorithms to solve the above optimization problem, where the gradient is derived using the log-derivative. More precisely, let $J(\theta):=\mathbb{E}\left[\mathrm{pv}\left(D^{\pi_{\theta}^{S}}\right) \mid D\right]$ be our objective function which now depends on the parameters $\theta$. The gradient of the objective function $J(\theta)$ can be derived as follows</p>
<p>$$
\begin{aligned}
\nabla J(\theta) &amp; =\nabla \mathbb{E}\left[\mathrm{pv}\left(D^{\pi_{\theta}^{S}}\right) \mid D\right] \
&amp; =\nabla \sum_{d} \mathbb{P}\left(D^{\pi_{\theta}^{S}}=d \mid D\right) \mathrm{pv}(d) \
&amp; =\sum_{d} \mathbb{P}\left(D^{\pi_{\theta}^{S}}=d \mid D\right) \nabla \log \mathbb{P}\left(D^{\pi_{\theta}^{S}}=d \mid D\right) \mathrm{pv}(d) \
&amp; =\mathbb{E}\left[\nabla \log \mathbb{P}\left(D^{\pi_{\theta}^{S}} \mid D\right) \mathrm{pv}\left(D^{\pi_{\theta}^{S}}\right) \mid D\right]
\end{aligned}
$$</p>
<p>This expectation can be estimated by drawing repeated resamples $D^{\pi_{\theta}^{S}}$, where $\mathbb{P}\left(D^{\pi_{\theta}^{S}} \mid D\right)$ is determined by the resampling weights. In practice, we apply stochastic gradient descent (Zhang, 2004), i.e., at each iteration of the optimization we compute the gradient only from a single resample. As we argue in Appendix H, we can further speed up the optimization process substantially by a minor modification to the resampling weights, corresponding to sampling with replacement instead of distinct weights.</p>
<p>The optimization yields a policy $\pi_{\theta}^{s}$ that approximately satisfies $\pi_{\theta}^{s} \in \arg \min <em _theta="\theta">{\pi</em>$ as a test policy for testing the invariance of $S$. Lastly, to preserve the level of the statistical test, we split the original sample into two halves, perform the power optimization procedure on one half, and verify the invariance condition on the other half. The algorithm is presented in Algorithm 4 in Appendix I. We only use the approximation of the resampling weights for the power optimization and use the actual resampling weights for the final resampling, so the level guarantee of Proposition 7 in Appendix F still holds.} \in \Pi^{S}} J(\theta)$. We can then use $\pi_{\theta}^{s</p>
<h1>4.4.3 Using a uniform target distribution</h1>
<p>Since the procedure in Section 4.4.2 may be computationally challenging, especially if the algorithm is repeated many times as in Section 5. A computationally simpler approach is for each $a \in \mathcal{A}$ to test invariance under the test policy $\pi_{a} \in \Pi^{\varnothing}$, which always chooses the action $a$, and then combine the resulting $p$-values using Bonferroni corrections (Dunn, 1961). Beyond computational simplicity, this has an additional benefit: Across environments there may be a cancelling effect of the difference in means due to different dependencies on the action in each environment. By testing the invariance of the conditional mean of the reward in each action, such cancelling effects are accounted for.</p>
<h3>4.5 Learning Causal Ancestors under Distributional Shifts</h3>
<p>Sections 4.1 and 4.2 discuss an approach to learn invariant sets from off-policy data. The learned invariant sets are then used to find an optimal invariant policy as discussed in Section 4.3. Besides learning an optimal invariant policy, one can further use the proposed offpolicy invariance test to analyze the causal structure. More specifically, the learned invariant</p>
<p>sets allow us to look for potential observed causal ancestors $\operatorname{AN}(R)^{10}$ of $R$ by taking the intersection of the accepted sets. This approach is similar to invariant causal prediction (Peters et al., 2016), except that here, we employ the off-policy invariance test to account for the distributional shift between the initial and the test policies, and allow for hidden variables.</p>
<p>Now we outline a method for finding $\operatorname{AN}(R)$ from the offline data obtained from multiple environments $D^{e_{1}}, \ldots, D^{e_{L}}$. For all $e_{j} \in \mathcal{E}^{\text {obs }}$ and $S \subseteq{1, \ldots, d}$, let us denote by $D^{e_{j}, \pi^{S}}$ a weighted resample of $D^{e_{j}}$, and $\psi^{S}$ an invariance test for the $\mathcal{E}^{\text {obs }}$-invariance hypothesis $H_{0}\left(S, \pi^{S}, \mathcal{E}^{\text {obs }}\right)$ as discussed in Section 4.2 and Appendix F. For ease of presentation, we assume that $n_{e_{1}}=\cdots=n_{e_{L}}=: n$. Then, we propose to estimate the causal ancestors of $R$ by</p>
<p>$$
\hat{S}<em S:="S:" _psi_S="\psi^{S">{\mathrm{AN}}^{n}:=\bigcap</em> S
$$}\left(D^{e_{1}, \pi^{S}}, \ldots, D^{e_{L}, \pi^{S}}\right)=0</p>
<p>We detail the whole procedure in Algorithm 3 in Appendix G. Proposition 4 shows that this method controls the probability of wrongly selecting an incorrect variable.</p>
<p>Proposition 4. Assume Setting 1, and that $\mathbf{S}^{\text {inv }}$ is non-empty. Let $\hat{S}_{\mathrm{AN}}^{n}$ be the estimated set of causal ancestors given in (18) and assume that the invariance tests $\psi^{S}$ used in (18) have pointwise asymptotic level $\alpha \in(0,1)$. It then holds that</p>
<p>$$
\liminf <em _mathrm_AN="\mathrm{AN">{n \rightarrow \infty} \mathbb{P}\left(\hat{S}</em>(R)\right) \geq 1-\alpha
$$}}^{n} \subseteq \operatorname{AN</p>
<p>Proof. See Appendix D. 10 .</p>
<h1>5 Simulation Experiments</h1>
<p>To verify our theoretical findings we perform two simulation experiments, where we consider a linear multi-environment contextual bandit setting similar to Example 1 with the following SCM $\mathcal{S}(\pi, e)$ (which induces the graph shown in Figure 1(b)):</p>
<p>$$
\begin{gathered}
U:=\epsilon_{U}, \quad X^{1}:=\gamma_{e} U+\epsilon_{X^{1}}, \quad X^{2}:=\alpha_{e}+\epsilon_{X^{2}} \
A \sim \pi\left(A \mid X^{1}, X^{2}\right), \quad R:=\beta_{A, 1} X^{2}+\beta_{A, 2} U+\epsilon_{R}
\end{gathered}
$$</p>
<p>where $\epsilon_{U}, \epsilon_{X^{1}}, \epsilon_{X^{2}}, \epsilon_{R} \sim \mathcal{N}(0,1), A$ takes values in the space $\left{a_{1}, \ldots, a_{L}\right}, \gamma_{e}$ and $\alpha_{e}$ are parameters that depend on the environment $e$, and $\beta_{a_{1}, 1}, \ldots, \beta_{a_{L}, 1}, \beta_{a_{1}, 2}, \ldots, \beta_{a_{L}, 2}$ are parameters that are fixed across environments. Appendix J. 1 contains details on how the parameters are chosen in the experiments. The code for all the experiments is available at https://github.com/sorawitj/invariant-policy-learning.</p>
<h3>5.1 Generalization and Invariance</h3>
<p>We first consider an oracle setting, where we know a priori which subsets are invariant. From our data-generating process, it follows that $\left{X^{2}\right}$ is the only invariant set. We then compare an invariant policy which depends only on $X^{2}$ with a policy that uses both $X^{1}$ and $X^{2}$. We train both policies on a dataset of size $10^{\prime} 000$ obtained from multiple training environments under a fixed initial policy $\pi^{0}$ (see Appendix J.2). In both cases, we employ a weighted least squares to estimate the expected reward $\mathbb{E}\left[R \mid A, X^{S}\right]$, where $S$ is the subset that the policy uses. The policy then takes a greedy action w.r.t. the estimated expected reward, i.e., $\arg \max _{a} \hat{\mathbb{E}}\left[R \mid A=a, X^{S}\right]$ (see Section 4.3). Then we evaluate both policies on multiple unseen environments and compute the regret with respect to the policy that is optimal in each of the unseen environments. Figure 2 shows the results. Each data point represents the evaluation on an unseen environment. The $y$-axes show the regret value and the $x$-axes display the distance</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The generalization performance (in terms of regret) of the policy based on an invariant set {X<sup>2</sup>} and the policy based on a non-invariant set {X<sup>1</sup>, X<sup>2</sup>}. The left and the right plot show the results when the training environments consist of two and six different environments, respectively. In both cases, the worst-case regret for the invariant policy is upper bounded while this is not the case for the non-invariant policy.</p>
<p>From each unseen environment to the training environments (the distance is computed as the ℓ<sup>2</sup>-distance between the average value of the pairs (γ<sub>e<sub>t</sub></sub>, α<sub>e<sub>t</sub></sub>) in the training environments and the pair (γ<sub>e</sub>, α<sub>e</sub>) in the unseen test environment). The plot shows that the worst-case behavior of the invariant policy is smaller than the non-invariant one. In particular, for environments different from the training environments the gain can be significant. This empirically supports our result of Theorem 1.</p>
<h3>5.2 Learning Invariant Policies</h3>
<p>In practice, we do not know in advance which sets are invariant. We now aim to find an invariant policy from a dataset generated under an initial policy π<sup>0</sup> which takes both X<sup>1</sup> and X<sup>2</sup> as input. To do so, we employ the method proposed in Section 4.2 for testing invariance under distributional shifts. More precisely, we generate a dataset of size n from multiple training environments under the initial policy π<sup>0</sup> and apply the off-policy invariance test (see Section 4.4) to verify the invariance property of each subset in {∅, {X<sup>1</sup>}, {X<sup>2</sup>}, {X<sup>1</sup>, X<sup>2</sup>}}. We repeat the experiment 500 times and plot the acceptance rates at various sample sizes (n = 1<sup>′</sup>000, 3<sup>′</sup>000, 9<sup>′</sup>000, 27<sup>′</sup>000, 81<sup>′</sup>000) (these numbers denote the total sample size, that is, number of observations, summed over all environments). The resulting acceptance rates are shown in Figure 3. Our method yields high acceptance rates for the set {X<sup>2</sup>}, which indeed is invariant, while the acceptance rates for other sets gradually decrease as the sample size increases. Furthermore, we can see that our test is more powerful when the number of training environments increases (keeping the total number of observations fixed). Our test is conservative (the acceptance rate is above the 95% level in the left plot) because the target test is not exact (the true conditional expectation is not given). In Appendix J.3, we conduct the same experiment with an exact test, using the true conditional expectation, which shows the correct level.</p>
<h2>6 Warfarin Dosing Case Study</h2>
<p>We evaluate our proposed approach on the clinical task of warfarin dosing. Warfarin is a blood thinner medicine prescribed to patients at risk of blood clots. The appropriate dose of warfarin varies from patient to patient depending on various factors such as demographic and genetic information (Consortium, 2009). Our case study is based on the International Warfarin Pharmacogenetics Consortium (IWPC) dataset (Consortium, 2009) which consists of 5<sup>′</sup>700 patients who were treated with warfarin, collected from 21 research groups on 4 continents. The IWPC dataset contains the optimal dose of warfarin for each of the patients as well as their information on demographic characteristics, clinical and genetic factors. The warfarin dosing problem has been used in a number of previous works evaluating off-policy</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Acceptance rates for the off-policy invariance test proposed in Section 4.2 for varying sample sizes. With increasing sample size, only the invariant set {<em>X</em><sup>2</sup>*} is accepted. Here, more environments (right) seems to yield higher test power than fewer environments (left).</p>
<p>learning algorithms (Kallus and Zhou, 2018; Bertsimas and McCord, 2018; Zenati et al., 2020). Similarly to these works, we formulate the warfarin dosing problem as a multi-environment contextual bandit problem as follows.</p>
<ul>
<li>The covariates (<em>X</em>) are patient-level features including demographic, clinical and genetic factors.</li>
<li>The actions (<em>A</em>) are recommended warfarin doses output by a policy. We discretize the actions into three equal-sized buckets (low, medium, high) based on the quantiles of the optimal warfarin dose.</li>
<li>The reward (<em>R</em>) depends on the recommended dose and the optimal dose: For each patient <em>i</em>, the reward <em>R<sub>i</sub></em>(<em>a</em>) for an action <em>a</em> ∈ {low, medium, high} is computed as</li>
</ul>
<p>$$R_i(a) := |Y_i - m(a)|,\tag{20}$$</p>
<p>where <em>Y<sub>i</sub></em> is the optimal warfarin dose for a patient <em>i</em> and <em>m(a)</em> is a median value of the optimal warfarin doses within the bucket <em>a</em>. Here, we assume that neither the reward function nor the optimal warfarin doses are known to the agent. Instead, for each patient <em>i</em>, only the reward for the action <em>A<sub>i</sub></em> is observed, i.e., <em>R<sub>i</sub></em> := <em>R<sub>i</sub></em>(<em>A<sub>i</sub></em>).</p>
<ul>
<li>The environments (Ɛ) are proxies for continents. The continent information is not directly contained in the dataset, but we create proxies for the continent by clustering the 21 research groups into 4 clusters based on their proportion of the patients' race within each group. We believe that the resulting clusters roughly correspond to 4 different continents.</li>
</ul>
<p>To reduce the search space, we select the top 10 features that are most predictive for the optimal warfarin dose using the permutation feature importance method (Breiman, 2001). The top 10 features include 4 demographic variables, 4 clinical factors, and 2 genetic factors.</p>
<p>We consider two experimental setups to illustrate the benefits of our invariant learning approach. In the first setup, we directly apply our method to the IWPC dataset. Here, including invariance does not seem necessary in that our method performs similarly to other baselines (but not worse). It does, however, generate some causal insight into the problem. The second setup is a semi-real setting, where we introduce an artificial, non-invariant confounder.</p>
<p>We now outline our first experimental setup and the results. We first generate training data {(<em>X<sub>i</sub></em>, <em>A<sub>i</sub></em>, <em>R<sub>i</sub></em>, <em>e<sub>i</sub></em>)}_{<em>i</em>=1}^<em>n</em> by drawing actions <em>A<sub>i</sub></em> from a policy <em>π</em><sup>0</sup> ∈ Π<sup>BMI</sup> that is constructed from linear regression <em>Y<sub>i</sub></em> ≈ <em>f</em>(<em>X<sup>BMI</sup><sub>i</sub></em>) of the optimal dose onto the BMI (see Appendix K.1 for more details).</p>
<h3>6.1 Candidate Methods</h3>
<p>Using the generated training data, we empirically compare the performance of the following policy learning methods:</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Empirical results on the original dataset. Each point represents the expected reward of a policy on the corresponding test environment. The square points represent the mean value of the expected rewards. In this setup, all candidate methods yield similar performances on all of the test environments. This result indicates that the test environments may not be significantly different from the training environments.</p>
<ul>
<li>Invariant Policy Learning (Inv): This is our proposed method. We first perform the off-policy invariance test using the test described in Section 4.4.3 to search for potential invariant sets. We then take the top 20 sets with the largest p-values $\mathbf{S}<em _text_inv="\text{inv">{\text{inv}}^{20}$ as the candidate invariant sets. For each $S$ in $\mathbf{S}</em>$ as the covariates (the same algorithm is also used in other candidate methods below). Lastly, we select the top 3 sets that yield the largest expected rewards (computed using 5-fold cross-validation).}}^{20}$, we fit the policy optimization algorithm described in Section 4.3 with $X^{S</li>
<li>Predictive Policy Learning (Pred): This method serves as a baseline for policy learning that solely maximizes the expected reward. For each subset $S$, we fit the policy optimization algorithm with $X^{S}$ as the covariates. We then take the policies corresponding to the top 3 sets with the largest expected rewards.</li>
<li>All Set Policy Learning (All): This method serves as another baseline where we take all of the patient's features and fit the policy optimization algorithm.</li>
</ul>
<h3>6.2 Evaluation Setup &amp; Results</h3>
<p>We compare the policy learning methods using the following 'leave-one-environment-out' evaluation procedure.</p>
<ol>
<li>Select $e \in \mathcal{E}={1, \ldots, 4}$ as a test environment. Split the training data into $D^{\text{tst}}:=$ $\left{\left(X_{i}, A_{i}, R_{i}, e_{i}\right)\right}<em _text="\text" _tot="{tot">{i=1}^{n</em>\right)\right}}}}$, where $e_{i}=e$ and $D^{\text {tr }}:=\left{\left(X_{i}, A_{i}, R_{i}, e_{i<em _text="\text" _tr="{tr">{i=1}^{n</em> \in{1, \ldots, 4} \backslash$ ${e}$.}}}$, where $e_{i</li>
<li>Using $D^{\text {tr }}$, train the policies with candidate methods detailed in Section 6.1.</li>
<li>Evaluate the fitted policies by computing the expected reward on $D^{\text {tst }}$ using the true reward function (20).</li>
</ol>
<p>We repeat the above procedure for each $e \in \mathcal{E}$ and display the evaluation result in Fig 4. The performances of all candidate methods are similar. Even though the proposed invariant approach does not yield a higher reward compared with the baselines, it does not worsen the performance, either. This suggests that we can gain the stability benefit of an invariant policy without having to sacrifice predictiveness. Indeed, the stability benefit could prevent the learned invariant policy from being suboptimal when a new test environment is sufficiently different from the training environments as we show in Section 6.4.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Empirical results on policy learning with a non-invariant predictor (see Section 6.4). Each point represents the expected reward of a policy on the corresponding test environment. In this setup, our proposed method (Inv) outperforms the two baselines (Pred and All) that ignore the environment structure, while approaching the performance of the invariant oracle (Oracle-Inv).</p>
<h3>6.3 Analyzing Invariant Sets</h3>
<p>In addition to learning an optimal invariant policy, we can use the invariance-based approach to further analyze the dependence between the patient's features and the reward as discussed in Section 4.5. In particular, we apply the off-policy invariant causal prediction algorithm (see Algorithm 3 in Appendix G) to find potential causal ancestors of the reward. On this dataset, with a confidence level of 5%, the algorithm returns the empty set, which can happen if the covariates are highly correlated, for example <em>Heinze-Deml et al. (2018)</em>. Nonetheless, we can still extract more information by obtaining the defining sets (see Section 2.2 in <em>Heinze-Deml et al. (2018)</em>). The resulting defining set of size 2 is {Race, VKORC1} (see Appendix K.2 for more details on the variables). These variables are potential causal ancestors in the sense that at least one variable in these sets is a causal ancestor.</p>
<h3>6.4 Semi-real experiment</h3>
<p>To further illustrate the benefits of the invariance-based learning approach, we consider a semi-real setup where we introduce hidden variables and a non-invariant predictor. We remove the two genetic factors from the patient's features and create a non-invariant predictor that depends on those two factors as follows.</p>
<p>We first fit a linear regression to estimate the optimal warfarin dose from the genetic factors and denote the resulting coefficients by β. To mimic environmental perturbations, we perturb β depending an environment e ∈ E resulting in βe := γeβ, where γe is an environment-specific parameter. We define the non-invariant predictor in the environment e ∈ E as Xn-inv := XG⁽ βe, where XG are the two genetic features. We then add Xn-inv as part of the patient's features and remove XG. The training data are generated in a similar fashion as in the first setup, except that the initial policy does not only depend on the BMI score XBMI but also on the non-invariant predictor Xn-inv.</p>
<p>In addition to the candidate methods described in Section 6.1, we introduce an additional baseline for this setup.</p>
<ul>
<li>Oracle invariant Policy (Oracle-Inv): By construction, we know that Xn-inv is a strongly non-d-invariant variable (see Definition 5). This method serves as an oracle version of the invariant policy learning method by searching for the top 3 sets that do not contain Xn-inv such that their corresponding policies yield the largest expected reward (the procedure is similar to the Pred method with Xn-inv being removed).</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Formally, $\operatorname{AN}(R) \subseteq{1, \ldots, d}$ is defined as the set of indicies $j$ for which there is a directed path from $X^{j}$ to $R$ in $\mathcal{G}$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4461 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4461</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4461</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-278769611</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14599v2.pdf" target="_blank">Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4461.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4461.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthHypo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Truth-Hypo benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical benchmark introduced to evaluate LLMs' ability to generate truthful scientific hypotheses using a temporally split PubTator-derived knowledge graph and controlled retrieval settings; includes negative samples and three relation-type tasks (Chemical–Gene, Disease–Gene, Gene–Gene).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>TruthHypo benchmark (link-level + relation-level evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Constructs hypothesis-generation queries from entity pairs and evaluates LLM outputs under four knowledge settings (parametric only; parametric+KG; parametric+Lit; parametric+KG+Lit). Uses link-level evaluation (whether a connection exists) via precision, recall, F1, and relation-level evaluation (correct relation label) via accuracy. The dataset is temporally partitioned into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) edges from PubTator 3.0, with unseen edges filtered to avoid overlap and only kept if supported by multiple papers. Negative (no-relation) cases are added proportionally per relation-type.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Link-level: precision, recall, F1 (measuring ability to hypothesize existence of links and control false positives/negatives); Relation-level: accuracy (measuring correctness of predicted relation type). Also evaluates groundedness indirectly by correlating with KnowHD scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1; GPT-4o (evaluated: Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 70B, mini, full (as reported for families)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (biomedical relations: chemicals, genes, diseases)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Relational/associative biomedical hypotheses (links between entities such as Chemical–Gene, Disease–Gene, Gene–Gene)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across tasks and settings, most LLMs struggled to generate fully truthful hypotheses. Example reported numbers: GPT-4o achieved mean accuracies exceeding 60% (overall mean accuracy up to 66.95% in Parametric+KG+Lit setting); link-level F1 scores were higher than relation-level accuracy, indicating models often found connections but mislabelled relation types. Smaller models (Llama-3.1-8B) showed high recall but low precision; larger models (GPT-4o) achieved higher precision. Dataset sizes: Chemical & Gene: 1209 instances; Disease & Gene: 268; Gene & Gene: 547.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: automated numeric metrics (precision/recall/F1/accuracy) for closed tasks; additionally correlated with KnowHD groundedness scores and validated via human expert annotations in open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical evaluation across multiple LLMs and settings; correlation analyses between groundedness scores and hypothesis accuracy; human expert annotation in open-ended tasks (80% agreement) used to validate selection based on groundedness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Verifying truthfulness requires temporal-split gold data and is resource-intensive. Benchmark focuses on a subset of biomedical relation types and relies on PubTator extraction quality; negative sampling may affect evaluation balance. LLMs sometimes echo context without real reasoning; smaller models may degrade when augmented with external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Derived from PubTator 3.0 with temporal split into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) edges, filtered to remove head/tail overlaps and retain only relations supported by multiple papers; augmented with controlled negative samples. Tasks: Chemical&Gene (1209 instances), Disease&Gene (268), Gene&Gene (547).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4461.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4461.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowHD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-based Hallucination Detection (KnowHD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that detects hallucinations in LLM-generated hypotheses by decomposing rationales into atomic claims and verifying each claim against literature (PubMed) and/or a knowledge graph (PubTator), producing a normalized groundedness score per hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>KnowHD (atomic-claim groundedness verification)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Parses each generated hypothesis and its reasoning chain into atomic claims via prompting; for each atomic claim p, retrieves supporting context either from literature (top-k BM25 from PubMed limited to pre-2023 articles, threshold τ) or from the knowledge graph (edges containing entities in the claim). Prompts an LLM to judge whether the concatenated context entails each claim. Computes groundedness(h) = (1/|C(h)|) * sum_{p in C(h)} 1[context(p) |= p], i.e., the fraction of atomic claims that are entailed by retrieved context. Can use context_D (literature), context_G (graph), or union.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Atomic-claim entailment (binary per claim) aggregated into a groundedness score (0–100%). Used as a filter/selection criterion: higher groundedness indicates higher probability that the hypothesis is truthful; also binned to examine relationship between groundedness level and empirical accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Framework is model-agnostic, but claim decomposition and entailment judgments are implemented by prompting the evaluated LLMs (e.g., GPT-4o-mini used for decomposition/verification in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied (experiments used Llama-3.1 and GPT-4o family; groundedness judgments implemented using GPT variants where specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (uses PubMed literature and PubTator KG for verification)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Verification of generated mechanistic/relational hypotheses via decomposition into atomic claims</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>KnowHD groundedness correlates positively with hypothesis truthfulness: e.g., for GPT-4o-mini on Chemical & Gene under combined KG+Literature, mean accuracy for all hypotheses was 60.96% while hypotheses with groundedness >80% reached 72.77%. In rationales, literature-based verification grounded 76.30% of claims for literature-augmented Chemical&Gene hypotheses, KG-only verification grounded 51.08% of claims, and combined KG+Literature produced the highest groundedness.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated claim-level entailment judgments (prompted LLMs) combined with retrieval; validated via human expert judgments correlating groundedness with perceived truthfulness in open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Analyzed correlation between groundedness bins and empirical accuracy; human expert annotation study on open-ended hypotheses (54 pairs filtered by groundedness differences) where experts preferred higher-groundedness hypotheses (80% inter-annotator agreement); statistical tests (Wilcoxon signed-rank, Z-test) reported for selection ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependent on retrieval quality (BM25 limits, k and τ choices); entailment judgments are performed by LLMs and can inherit judgment errors; incomplete knowledge bases mean true claims could be labeled ungrounded; computational cost for retrieval and multiple entailment queries; KG verification often weaker than literature for some tasks (low KG-grounding rate reported).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Verification uses PubMed corpus (documents limited to PMIDs ≤ 36600000) and PubTator 3.0 knowledge graph as knowledge sources; retrieval parameters: BM25 with k=32 for hypothesis generation, k=8 for claim verification, τ=0.0 (as used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4461.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4461.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Groundedness score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atomic-claim groundedness score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric that quantifies what fraction of decomposed atomic claims in a hypothesis are entailed by retrieved knowledge (literature and/or knowledge graph), producing a scalar groundedness percentage used to rank/filter hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Groundedness score (KnowHD aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For hypothesis h, decompose into set C(h) of atomic claims. For each claim p, retrieve context(p) from literature (BM25 top-k pre-2023) and/or KG. Use an LLM prompt to judge entailment context(p) |= p (binary). Compute groundedness(h) = (1/|C(h)|) * sum_{p in C(h)} 1[context(p) |= p]. Higher scores indicate more of the hypothesis is supported by existing knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fraction of atomic claims supported by retrieved evidence; thresholds/bins (e.g., >80%) used to select higher-probability truthful hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (as applied in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Verification metric for generated relational/mechanistic hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Groundedness score stratifies hypothesis accuracy: e.g., GPT-4o-mini combined setting: overall mean accuracy 60.96% for Chemical&Gene; groundedness>80% group accuracy 72.77%. Literature-only verification grounded ~76.30% of claims in literature-augmented rationales; KG-only ~51.08%. Using groundedness to select among candidate hypotheses improved final accuracy in external-knowledge settings (e.g., combined parametric+KG+Lit: GPT-4o-mini accuracy with groundedness selection 63.44%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (retrieval + LLM entailment) with human validation correlation analyses and expert annotation study.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical correlation with held-out relation-label accuracy and human expert preferences in open-ended tasks; statistical testing reported for selection preference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary entailment judgments may misrepresent partial support; reliant on retrieval coverage and quality; LLM-based entailment judgments can be noisy and self-referential; choice of decomposition into atomic claims affects score granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4461.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4461.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Link/Relation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Link-level (Precision/Recall/F1) and Relation-level (Accuracy) metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard evaluation metrics applied to hypothesis generation: link-level precision/recall/F1 measure whether an LLM hypothesizes a connection correctly (irrespective of relation type), while relation-level accuracy measures whether the predicted relation label matches ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Link-level precision/recall/F1 and relation-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Link-level: treat predicted existence of a relation between entities as a binary retrieval task and compute precision, recall, and F1. Relation-level: treat predicted relation label (multi-class including 'no relation') as classification and compute accuracy against ground truth relation labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, recall, F1 (trade-offs between false positives and coverage); accuracy (overall correctness of relation-type prediction). Used together to diagnose model behavior (e.g., high F1 but lower accuracy indicates correct link prediction but wrong relation label).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (applied in TruthHypo)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Relational hypothesis evaluation (link existence & relation type)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported across multiple models and settings: link-level F1 generally higher than relation-level accuracy. Example: Parametric setting GPT-4o had average F1 ~73.17 and average accuracy ~61.81 (see Table 2). Precision/recall breakdowns show smaller models had higher recall and lower precision; GPT-4o with literature achieved highest precision but lower recall, illustrating trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric-based evaluation for closed benchmark tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard supervised evaluation against temporally-split ground truth derived from PubTator edges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Accuracy alone can mask whether a model is producing plausible but incorrect relation types; link-level metrics do not capture quality of rationale or groundedness; dataset construction and negative sampling influence these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4461.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4461.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Groundedness-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Groundedness-based hypothesis selection (KnowHD selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that generates multiple candidate hypotheses per input and chooses the candidate with the highest KnowHD groundedness score as the final hypothesis, shown to improve truthfulness when external knowledge is available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Groundedness-based selection among candidate generations</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each input, prompt the LLM to produce N candidate hypotheses (N=5 in experiments). Compute KnowHD groundedness score for each candidate by decomposing into atomic claims and verifying against KG and/or literature. Select the candidate with the maximum groundedness score as final output. Compare performance to greedy decoding baseline and self-consistency (majority-voting) baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Final hypothesis accuracy (relation-level) and link-level metrics after selection; improvement over greedy or majority-voting baselines indicates the utility of groundedness-based ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, GPT-4o, Llama-3.1 models (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mini, full, 8B, 70B (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Generated relational hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Groundedness-based selection generally outperformed greedy and self-consistency baselines when external knowledge was provided. Example: In Parametric+KG+Lit, GPT-4o-mini achieved average accuracy 63.44% with groundedness-based selection (approaching GPT-4o performance). In the parametric-only setting majority-voting slightly outperformed groundedness (61.86% vs 59.83%), but groundedness selection improved performance in knowledge-augmented settings.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated selection based on KnowHD scores; validated by downstream relation-level accuracy and human expert studies in open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared selected outputs' accuracy to greedy and self-consistency baselines across LLMs and settings; correlated improvements with groundedness levels; human expert preference experiments on open-ended data provided complementary validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires multiple generation runs (computational overhead) and multiple retrieval/verification cycles per candidate; dependent on reliability of groundedness judgments; selection gains are smaller or absent when only parametric knowledge is used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4461.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4461.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claim decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis decomposition into atomic claims (for verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodological step to split a generated hypothesis and its reasoning chain into discrete atomic claims that can be independently verified against knowledge sources, enabling fine-grained grounding assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Atomic claim extraction and verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt LLMs with a template to summarize a hypothesis/rationale into a JSON list of atomic claims. Each claim is then individually matched to literature (BM25 retrieval) and/or KG edges for verification using an entailment prompt. This isolates unsupported components in a compound rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Claim-level entailment (binary) aggregated into groundedness; identification of unsupported steps in reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used for claim extraction and verification (GPT-based prompts were used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varied (GPT-4o-mini used for many prompt tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanistic/reasoning chains underlying relational hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Enables granular identification of hallucinated reasoning steps; used as core component of KnowHD. The paper reports that literature retrieval grounded ~76.30% of claims in some settings, while KG grounded ~51.08% for KG-only rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM-based decomposition followed by automated retrieval + entailment judgments; validated by observing correlation between proportion of grounded claims and overall hypothesis accuracy and by human expert annotation preference.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with hypothesis-level accuracy and human annotations; inter-annotator agreement used in human study for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of decomposition depends on prompting and model understanding; overly coarse or overly fine decompositions can bias groundedness; subsequent verification is limited by retrieval coverage and entailment decision noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4461.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4461.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG + KG/Lit settings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented Generation settings (Parametric, Parametric+KG, Parametric+Lit, Parametric+KG+Lit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation framework that tests LLM hypothesis generation under varying external-knowledge augmentations: no external knowledge (parametric), structured KG augmentation, unstructured literature retrieval augmentation, and combined KG+literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Controlled knowledge-augmentation experimental settings</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Defines four evaluation conditions to assess how external knowledge sources affect hypothesis truthfulness: (1) parametric only (model internal knowledge), (2) parametric+KG (graph multi-hop chains converted to text), (3) parametric+Lit (BM25 retrieval from PubMed with temporal restriction), (4) parametric+KG+Lit combined. Prompts standardized to include retrieved contexts. Retrieval used BM25 with specific k and τ parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effect on link-level F1, relation-level accuracy, groundedness of rationales, and precision/recall trade-offs across models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1 family, GPT-4o family (experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 70B, mini, full as used</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Relational hypotheses and supporting rationales</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>External knowledge generally improved groundedness and accuracy for larger models: combined KG+Literature often yielded best groundedness and higher accuracy (e.g., GPT-4o-mini accuracy improvements up to ~5.14% when augmented). KG alone sometimes improved precision for Disease&Gene and Gene&Gene tasks; literature often improved grounding of claims. Smaller models sometimes degraded with external context due to integration challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated experimental comparisons across settings with human validation on open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparative experiments reporting metrics (Table 2 and 6); analysis of groundedness and accuracy correlations; human expert selection study in open-ended setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Integration of heterogeneous external sources can harm smaller models; retrieval (BM25) has limitations (term-matching) and temporal restrictions may omit evidence; increased computational cost due to long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models as biomedical hypothesis generators: A comprehensive evaluation <em>(Rating: 2)</em></li>
                <li>Improving scientific hypothesis generation with knowledge grounded large language models <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Can chatgpt be used to generate scientific hypotheses <em>(Rating: 1)</em></li>
                <li>Generating scientific claims for zero-shot scientific fact checking <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4461",
    "paper_id": "paper-278769611",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "TruthHypo",
            "name_full": "Truth-Hypo benchmark",
            "brief_description": "A biomedical benchmark introduced to evaluate LLMs' ability to generate truthful scientific hypotheses using a temporally split PubTator-derived knowledge graph and controlled retrieval settings; includes negative samples and three relation-type tasks (Chemical–Gene, Disease–Gene, Gene–Gene).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "TruthHypo benchmark (link-level + relation-level evaluation)",
            "evaluation_method_description": "Constructs hypothesis-generation queries from entity pairs and evaluates LLM outputs under four knowledge settings (parametric only; parametric+KG; parametric+Lit; parametric+KG+Lit). Uses link-level evaluation (whether a connection exists) via precision, recall, F1, and relation-level evaluation (correct relation label) via accuracy. The dataset is temporally partitioned into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) edges from PubTator 3.0, with unseen edges filtered to avoid overlap and only kept if supported by multiple papers. Negative (no-relation) cases are added proportionally per relation-type.",
            "evaluation_criteria": "Link-level: precision, recall, F1 (measuring ability to hypothesize existence of links and control false positives/negatives); Relation-level: accuracy (measuring correctness of predicted relation type). Also evaluates groundedness indirectly by correlating with KnowHD scores.",
            "model_name": "Llama-3.1; GPT-4o (evaluated: Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)",
            "model_size": "8B, 70B, mini, full (as reported for families)",
            "scientific_domain": "Biomedicine (biomedical relations: chemicals, genes, diseases)",
            "theory_type": "Relational/associative biomedical hypotheses (links between entities such as Chemical–Gene, Disease–Gene, Gene–Gene)",
            "human_comparison": false,
            "evaluation_results": "Across tasks and settings, most LLMs struggled to generate fully truthful hypotheses. Example reported numbers: GPT-4o achieved mean accuracies exceeding 60% (overall mean accuracy up to 66.95% in Parametric+KG+Lit setting); link-level F1 scores were higher than relation-level accuracy, indicating models often found connections but mislabelled relation types. Smaller models (Llama-3.1-8B) showed high recall but low precision; larger models (GPT-4o) achieved higher precision. Dataset sizes: Chemical & Gene: 1209 instances; Disease & Gene: 268; Gene & Gene: 547.",
            "automated_vs_human_evaluation": "Hybrid: automated numeric metrics (precision/recall/F1/accuracy) for closed tasks; additionally correlated with KnowHD groundedness scores and validated via human expert annotations in open-ended tasks.",
            "validation_method": "Empirical evaluation across multiple LLMs and settings; correlation analyses between groundedness scores and hypothesis accuracy; human expert annotation in open-ended tasks (80% agreement) used to validate selection based on groundedness.",
            "limitations_challenges": "Verifying truthfulness requires temporal-split gold data and is resource-intensive. Benchmark focuses on a subset of biomedical relation types and relies on PubTator extraction quality; negative sampling may affect evaluation balance. LLMs sometimes echo context without real reasoning; smaller models may degrade when augmented with external knowledge.",
            "benchmark_dataset": "Derived from PubTator 3.0 with temporal split into 'seen' (PMID ≤ 36600000) and 'unseen' (PMID ≥ 38200000) edges, filtered to remove head/tail overlaps and retain only relations supported by multiple papers; augmented with controlled negative samples. Tasks: Chemical&Gene (1209 instances), Disease&Gene (268), Gene&Gene (547).",
            "uuid": "e4461.0",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "KnowHD",
            "name_full": "Knowledge-based Hallucination Detection (KnowHD)",
            "brief_description": "A framework that detects hallucinations in LLM-generated hypotheses by decomposing rationales into atomic claims and verifying each claim against literature (PubMed) and/or a knowledge graph (PubTator), producing a normalized groundedness score per hypothesis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "KnowHD (atomic-claim groundedness verification)",
            "evaluation_method_description": "Parses each generated hypothesis and its reasoning chain into atomic claims via prompting; for each atomic claim p, retrieves supporting context either from literature (top-k BM25 from PubMed limited to pre-2023 articles, threshold τ) or from the knowledge graph (edges containing entities in the claim). Prompts an LLM to judge whether the concatenated context entails each claim. Computes groundedness(h) = (1/|C(h)|) * sum_{p in C(h)} 1[context(p) |= p], i.e., the fraction of atomic claims that are entailed by retrieved context. Can use context_D (literature), context_G (graph), or union.",
            "evaluation_criteria": "Atomic-claim entailment (binary per claim) aggregated into a groundedness score (0–100%). Used as a filter/selection criterion: higher groundedness indicates higher probability that the hypothesis is truthful; also binned to examine relationship between groundedness level and empirical accuracy.",
            "model_name": "Framework is model-agnostic, but claim decomposition and entailment judgments are implemented by prompting the evaluated LLMs (e.g., GPT-4o-mini used for decomposition/verification in experiments).",
            "model_size": "varied (experiments used Llama-3.1 and GPT-4o family; groundedness judgments implemented using GPT variants where specified)",
            "scientific_domain": "Biomedicine (uses PubMed literature and PubTator KG for verification)",
            "theory_type": "Verification of generated mechanistic/relational hypotheses via decomposition into atomic claims",
            "human_comparison": false,
            "evaluation_results": "KnowHD groundedness correlates positively with hypothesis truthfulness: e.g., for GPT-4o-mini on Chemical & Gene under combined KG+Literature, mean accuracy for all hypotheses was 60.96% while hypotheses with groundedness &gt;80% reached 72.77%. In rationales, literature-based verification grounded 76.30% of claims for literature-augmented Chemical&Gene hypotheses, KG-only verification grounded 51.08% of claims, and combined KG+Literature produced the highest groundedness.",
            "automated_vs_human_evaluation": "Automated claim-level entailment judgments (prompted LLMs) combined with retrieval; validated via human expert judgments correlating groundedness with perceived truthfulness in open-ended tasks.",
            "validation_method": "Analyzed correlation between groundedness bins and empirical accuracy; human expert annotation study on open-ended hypotheses (54 pairs filtered by groundedness differences) where experts preferred higher-groundedness hypotheses (80% inter-annotator agreement); statistical tests (Wilcoxon signed-rank, Z-test) reported for selection ratios.",
            "limitations_challenges": "Dependent on retrieval quality (BM25 limits, k and τ choices); entailment judgments are performed by LLMs and can inherit judgment errors; incomplete knowledge bases mean true claims could be labeled ungrounded; computational cost for retrieval and multiple entailment queries; KG verification often weaker than literature for some tasks (low KG-grounding rate reported).",
            "benchmark_dataset": "Verification uses PubMed corpus (documents limited to PMIDs ≤ 36600000) and PubTator 3.0 knowledge graph as knowledge sources; retrieval parameters: BM25 with k=32 for hypothesis generation, k=8 for claim verification, τ=0.0 (as used in experiments).",
            "uuid": "e4461.1",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Groundedness score",
            "name_full": "Atomic-claim groundedness score",
            "brief_description": "A metric that quantifies what fraction of decomposed atomic claims in a hypothesis are entailed by retrieved knowledge (literature and/or knowledge graph), producing a scalar groundedness percentage used to rank/filter hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Groundedness score (KnowHD aggregation)",
            "evaluation_method_description": "For hypothesis h, decompose into set C(h) of atomic claims. For each claim p, retrieve context(p) from literature (BM25 top-k pre-2023) and/or KG. Use an LLM prompt to judge entailment context(p) |= p (binary). Compute groundedness(h) = (1/|C(h)|) * sum_{p in C(h)} 1[context(p) |= p]. Higher scores indicate more of the hypothesis is supported by existing knowledge.",
            "evaluation_criteria": "Fraction of atomic claims supported by retrieved evidence; thresholds/bins (e.g., &gt;80%) used to select higher-probability truthful hypotheses.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedicine (as applied in this paper)",
            "theory_type": "Verification metric for generated relational/mechanistic hypotheses",
            "human_comparison": false,
            "evaluation_results": "Groundedness score stratifies hypothesis accuracy: e.g., GPT-4o-mini combined setting: overall mean accuracy 60.96% for Chemical&Gene; groundedness&gt;80% group accuracy 72.77%. Literature-only verification grounded ~76.30% of claims in literature-augmented rationales; KG-only ~51.08%. Using groundedness to select among candidate hypotheses improved final accuracy in external-knowledge settings (e.g., combined parametric+KG+Lit: GPT-4o-mini accuracy with groundedness selection 63.44%).",
            "automated_vs_human_evaluation": "Automated (retrieval + LLM entailment) with human validation correlation analyses and expert annotation study.",
            "validation_method": "Empirical correlation with held-out relation-label accuracy and human expert preferences in open-ended tasks; statistical testing reported for selection preference.",
            "limitations_challenges": "Binary entailment judgments may misrepresent partial support; reliant on retrieval coverage and quality; LLM-based entailment judgments can be noisy and self-referential; choice of decomposition into atomic claims affects score granularity.",
            "uuid": "e4461.2",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Link/Relation metrics",
            "name_full": "Link-level (Precision/Recall/F1) and Relation-level (Accuracy) metrics",
            "brief_description": "Standard evaluation metrics applied to hypothesis generation: link-level precision/recall/F1 measure whether an LLM hypothesizes a connection correctly (irrespective of relation type), while relation-level accuracy measures whether the predicted relation label matches ground truth.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Link-level precision/recall/F1 and relation-level accuracy",
            "evaluation_method_description": "Link-level: treat predicted existence of a relation between entities as a binary retrieval task and compute precision, recall, and F1. Relation-level: treat predicted relation label (multi-class including 'no relation') as classification and compute accuracy against ground truth relation labels.",
            "evaluation_criteria": "Precision, recall, F1 (trade-offs between false positives and coverage); accuracy (overall correctness of relation-type prediction). Used together to diagnose model behavior (e.g., high F1 but lower accuracy indicates correct link prediction but wrong relation label).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedicine (applied in TruthHypo)",
            "theory_type": "Relational hypothesis evaluation (link existence & relation type)",
            "human_comparison": false,
            "evaluation_results": "Reported across multiple models and settings: link-level F1 generally higher than relation-level accuracy. Example: Parametric setting GPT-4o had average F1 ~73.17 and average accuracy ~61.81 (see Table 2). Precision/recall breakdowns show smaller models had higher recall and lower precision; GPT-4o with literature achieved highest precision but lower recall, illustrating trade-offs.",
            "automated_vs_human_evaluation": "Automated metric-based evaluation for closed benchmark tasks.",
            "validation_method": "Standard supervised evaluation against temporally-split ground truth derived from PubTator edges.",
            "limitations_challenges": "Accuracy alone can mask whether a model is producing plausible but incorrect relation types; link-level metrics do not capture quality of rationale or groundedness; dataset construction and negative sampling influence these metrics.",
            "uuid": "e4461.3",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Groundedness-based selection",
            "name_full": "Groundedness-based hypothesis selection (KnowHD selection)",
            "brief_description": "A method that generates multiple candidate hypotheses per input and chooses the candidate with the highest KnowHD groundedness score as the final hypothesis, shown to improve truthfulness when external knowledge is available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Groundedness-based selection among candidate generations",
            "evaluation_method_description": "For each input, prompt the LLM to produce N candidate hypotheses (N=5 in experiments). Compute KnowHD groundedness score for each candidate by decomposing into atomic claims and verifying against KG and/or literature. Select the candidate with the maximum groundedness score as final output. Compare performance to greedy decoding baseline and self-consistency (majority-voting) baseline.",
            "evaluation_criteria": "Final hypothesis accuracy (relation-level) and link-level metrics after selection; improvement over greedy or majority-voting baselines indicates the utility of groundedness-based ranking.",
            "model_name": "GPT-4o-mini, GPT-4o, Llama-3.1 models (used in experiments)",
            "model_size": "mini, full, 8B, 70B (as used)",
            "scientific_domain": "Biomedicine",
            "theory_type": "Generated relational hypotheses",
            "human_comparison": false,
            "evaluation_results": "Groundedness-based selection generally outperformed greedy and self-consistency baselines when external knowledge was provided. Example: In Parametric+KG+Lit, GPT-4o-mini achieved average accuracy 63.44% with groundedness-based selection (approaching GPT-4o performance). In the parametric-only setting majority-voting slightly outperformed groundedness (61.86% vs 59.83%), but groundedness selection improved performance in knowledge-augmented settings.",
            "automated_vs_human_evaluation": "Automated selection based on KnowHD scores; validated by downstream relation-level accuracy and human expert studies in open-ended tasks.",
            "validation_method": "Compared selected outputs' accuracy to greedy and self-consistency baselines across LLMs and settings; correlated improvements with groundedness levels; human expert preference experiments on open-ended data provided complementary validation.",
            "limitations_challenges": "Requires multiple generation runs (computational overhead) and multiple retrieval/verification cycles per candidate; dependent on reliability of groundedness judgments; selection gains are smaller or absent when only parametric knowledge is used.",
            "uuid": "e4461.4",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claim decomposition",
            "name_full": "Hypothesis decomposition into atomic claims (for verification)",
            "brief_description": "A methodological step to split a generated hypothesis and its reasoning chain into discrete atomic claims that can be independently verified against knowledge sources, enabling fine-grained grounding assessment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Atomic claim extraction and verification",
            "evaluation_method_description": "Prompt LLMs with a template to summarize a hypothesis/rationale into a JSON list of atomic claims. Each claim is then individually matched to literature (BM25 retrieval) and/or KG edges for verification using an entailment prompt. This isolates unsupported components in a compound rationale.",
            "evaluation_criteria": "Claim-level entailment (binary) aggregated into groundedness; identification of unsupported steps in reasoning chains.",
            "model_name": "LLMs used for claim extraction and verification (GPT-based prompts were used in experiments)",
            "model_size": "varied (GPT-4o-mini used for many prompt tasks)",
            "scientific_domain": "Biomedicine",
            "theory_type": "Mechanistic/reasoning chains underlying relational hypotheses",
            "human_comparison": false,
            "evaluation_results": "Enables granular identification of hallucinated reasoning steps; used as core component of KnowHD. The paper reports that literature retrieval grounded ~76.30% of claims in some settings, while KG grounded ~51.08% for KG-only rationales.",
            "automated_vs_human_evaluation": "Automated LLM-based decomposition followed by automated retrieval + entailment judgments; validated by observing correlation between proportion of grounded claims and overall hypothesis accuracy and by human expert annotation preference.",
            "validation_method": "Correlation with hypothesis-level accuracy and human annotations; inter-annotator agreement used in human study for open-ended tasks.",
            "limitations_challenges": "Quality of decomposition depends on prompting and model understanding; overly coarse or overly fine decompositions can bias groundedness; subsequent verification is limited by retrieval coverage and entailment decision noise.",
            "uuid": "e4461.5",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RAG + KG/Lit settings",
            "name_full": "Retrieval-augmented Generation settings (Parametric, Parametric+KG, Parametric+Lit, Parametric+KG+Lit)",
            "brief_description": "Evaluation framework that tests LLM hypothesis generation under varying external-knowledge augmentations: no external knowledge (parametric), structured KG augmentation, unstructured literature retrieval augmentation, and combined KG+literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Controlled knowledge-augmentation experimental settings",
            "evaluation_method_description": "Defines four evaluation conditions to assess how external knowledge sources affect hypothesis truthfulness: (1) parametric only (model internal knowledge), (2) parametric+KG (graph multi-hop chains converted to text), (3) parametric+Lit (BM25 retrieval from PubMed with temporal restriction), (4) parametric+KG+Lit combined. Prompts standardized to include retrieved contexts. Retrieval used BM25 with specific k and τ parameters.",
            "evaluation_criteria": "Effect on link-level F1, relation-level accuracy, groundedness of rationales, and precision/recall trade-offs across models and tasks.",
            "model_name": "Llama-3.1 family, GPT-4o family (experiments)",
            "model_size": "8B, 70B, mini, full as used",
            "scientific_domain": "Biomedicine",
            "theory_type": "Relational hypotheses and supporting rationales",
            "human_comparison": false,
            "evaluation_results": "External knowledge generally improved groundedness and accuracy for larger models: combined KG+Literature often yielded best groundedness and higher accuracy (e.g., GPT-4o-mini accuracy improvements up to ~5.14% when augmented). KG alone sometimes improved precision for Disease&Gene and Gene&Gene tasks; literature often improved grounding of claims. Smaller models sometimes degraded with external context due to integration challenges.",
            "automated_vs_human_evaluation": "Automated experimental comparisons across settings with human validation on open-ended tasks.",
            "validation_method": "Comparative experiments reporting metrics (Table 2 and 6); analysis of groundedness and accuracy correlations; human expert selection study in open-ended setting.",
            "limitations_challenges": "Integration of heterogeneous external sources can harm smaller models; retrieval (BM25) has limitations (term-matching) and temporal restrictions may omit evidence; increased computational cost due to long contexts.",
            "uuid": "e4461.6",
            "source_info": {
                "paper_title": "Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models as biomedical hypothesis generators: A comprehensive evaluation",
            "rating": 2,
            "sanitized_title": "large_language_models_as_biomedical_hypothesis_generators_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Improving scientific hypothesis generation with knowledge grounded large language models",
            "rating": 2,
            "sanitized_title": "improving_scientific_hypothesis_generation_with_knowledge_grounded_large_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Can chatgpt be used to generate scientific hypotheses",
            "rating": 1,
            "sanitized_title": "can_chatgpt_be_used_to_generate_scientific_hypotheses"
        },
        {
            "paper_title": "Generating scientific claims for zero-shot scientific fact checking",
            "rating": 1,
            "sanitized_title": "generating_scientific_claims_for_zeroshot_scientific_fact_checking"
        }
    ],
    "cost": 0.0164715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
8 Jun 2025</p>
<p>Guangzhi Xiong aidong@virginia.edu 
University of Virginia</p>
<p>Eric Xie 
University of Virginia</p>
<p>Corey Williams 
University of Virginia</p>
<p>Myles Kim 
University of Virginia</p>
<p>Amir Hassan Shariatmadari 
University of Virginia</p>
<p>Sikun Guo 
University of Virginia</p>
<p>Stefan Bekiranov 
University of Virginia</p>
<p>Aidong Zhang 
University of Virginia</p>
<p>Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models
8 Jun 202527206A855BB799669A959D59A0E3FCB5arXiv:2505.14599v2[cs.CL]
Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions.However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources.Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability.To facilitate the systematic study of these challenges, we introduce Truth-Hypo, a benchmark for assessing the capabilities of LLMs in generating truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge.Our results show that LLMs struggle to generate truthful hypotheses.By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs.Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery.Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have transformed the landscape of artificial intelligence, demonstrating remarkable capabilities across diverse applications, from natural language understanding to creative content generation [Karanikolas et al., 2023;Franceschelli and Musolesi, 2024;Raiaan et al., 2024].These models, trained on extensive corpora of text, demonstrate an ability to analyze, summarize, and generate human-like text, enabling advancements across diverse domains.Recently, there has been a growing interest in leveraging LLMs for scientific discovery [Zhong et al., 2023;Yang et al., 2023;Kumar et al., 2023;Liu et al., 2024;Baek et al., 2024;Guo et al., 2024].Their capacity to process and synthesize vast amounts of scientific literature positions them as valuable tools in aiding researchers, particularly for tasks such as literature reviews, summarization, and even generating new hypotheses [Qi et al., 2023;Zhou et al., 2024;M. Bran et al., 2024;Wright et al., 2022;Zeng et al., 2023;D'Arcy et al., 2024;Ifargan et al., 2025;Yang et al., 2025].</p>
<p>One particularly promising application of LLMs is their use in scientific hypothesis generation, where they can assist in identifying promising research directions [Park et al., 2024;Si et al., 2024;Guo et al., 2025].By analyzing extensive scientific literature, LLMs can uncover gaps in existing knowledge and propose novel hypotheses that may not be immediately apparent to human researchers.For instance, LLMs have been successfully applied to propose novel drug combinations for breast cancer treatment, some of which were later validated in laboratory experiments, showcasing their potential to accelerate biomedical discoveries [Abdel-Rehim et al., 2024].</p>
<p>Despite these advancements, there are substantial challenges that limit the practical utility of LLMs in scientific hypothesis generation.A critical concern is the inability to evaluate the truthfulness of generated hypotheses.While LLMs can generate hypotheses that seem plausible, it remains uncertain whether these hypotheses are valid and grounded in existing knowledge or merely hallucinated and scientifically invalid.This issue is further exacerbated by the welldocumented "hallucination" problem, where LLMs confidently produce information that is factually inaccurate or unsupported, posing challenges to their reliability in scientific contexts [Jin et al., 2024].While current research has largely focused on improving the novelty and diversity of LLM-generated hypotheses, their truthfulness and grounding in established knowledge remain underexplored [Baek et al., 2024;Hu et al., 2024;Si et al., 2024].</p>
<p>To address these challenges, we introduce TruthHypo, a comprehensive benchmark for evaluating the ability of LLMs to generate truthful scientific hypotheses, and KnowHD, a knowledge-based hallucination detection framework designed to assess the groundedness of these hypotheses.Truth-Hypo, built on a biomedical knowledge graph along with a domain-specific corpus, provides a controlled environment to evaluate how well LLM-generated hypotheses align with established scientific knowledge.KnowHD focuses on analyzing the reasoning processes of LLMs to identify hypotheses that are likely hallucinated or untruthful.Our findings reveal that LLMs face significant challenges in generating truthful hypotheses.By analyzing hallucinations in the reasoning processes behind generated hypotheses, we demonstrate that groundedness scores from KnowHD serve as an effective signal for identifying truthful hypotheses from the diverse outputs of LLMs.</p>
<p>Human evaluations on open-ended hypothesis generation tasks further confirm the utility of KnowHD in identifying scientifically valid hypotheses.</p>
<p>Our main contributions are summarized as follows:</p>
<p>• We introduce TruthHypo, a comprehensive benchmark designed to evaluate the ability of LLMs to generate truthful scientific hypotheses.</p>
<p>• We propose KnowHD, a knowledge-based hallucination detection framework that assesses the groundedness of LLM-generated hypotheses and identifies hallucinated claims by analyzing the rationale behind the hypothesis generation.</p>
<p>• We provide an extensive analysis of existing LLMs on TruthHypo, highlighting their limitations and challenges in generating truthful hypotheses.</p>
<p>• Our evaluation further reveals the connection between hallucination and truthfulness of generated hypotheses, showing the effectiveness of using KnowHD to select truthful and grounded hypotheses.</p>
<p>Truthful Hypothesis Generation Benchmark</p>
<p>To systematically evaluate the ability of large language models (LLMs) to generate truthful scientific hypotheses, we introduce TruthHypo, a benchmark tailored for biomedical hypothesis generation.TruthHypo is designed to simulate realworld conditions by employing rigorous dataset construction, task formulation, and truthfulness evaluation metrics.An overview of the dataset construction, task formulation, and evaluation framework is depicted in Figure 1.</p>
<p>Dataset Construction</p>
<p>The dataset for TruthHypo is derived from PubTator 3.0 [Wei et al., 2024], a comprehensive biomedical knowledge graph that includes annotated relations (also called edges) extracted from scientific articles.To simulate the temporal progression of scientific discovery, we partitioned the graph into "seen" and "unseen" subsets based on the publication years of the corresponding articles.Relations in the "seen" subset were extracted from papers published before 2023, identified by PMIDs ≤ 366000001 .The "unseen" subset, designed to represent new discoveries, comprises relations extracted from papers published after 2024, identified by PMIDs ≥ 38200000.</p>
<p>To ensure no overlap between the two subsets, we removed the edges in the unseen subset that shared head and tail entities with those in the seen subset.In addition, to maintain quality and validity, only relations discovered by multiple articles in the test data were retained.This filtering process guarantees that the unseen subset exclusively contains knowledge unavailable before 2024, simulating the conditions of future scientific research.</p>
<p>In building the dataset, we focused on three key relation types: "Chemical &amp; Gene", "Disease &amp; Gene", and "Gene &amp; Gene".These relation types were chosen for their complementary nature, detailed annotations, and potential for objective evaluation.To construct comprehensive classification tasks for evaluating different LLMs, we augment the dataset with negative test cases to assess whether LLMs tend to make false-positive predictions on entity pairs that lack a direct relationship in the existing knowledge base.The number of negative samples (labeled as "no relation") for each relation type is controlled to align with the average number of instances across other labels of the same relation type.The final dataset has 1209 instances for the "Chemical &amp; Gene" task, 268 instances for the "Disease &amp; Gene" task, and 547 instances for the "Gene &amp; Gene" task.A summary of the dataset statistics is presented in Table 1</p>
<p>Task Formulation</p>
<p>The TruthHypo benchmark includes three tasks, corresponding to the selected relation types: "Chemical &amp; Gene", "Disease &amp; Gene", and "Gene &amp; Gene".For each task, the input is a hypothesis generation query with two entities (see Figure 5 in Appendix D for the template), and the LLM is required to hypothesize the potential relationship between them based on available knowledge and reasoning.</p>
<p>To comprehensively assess LLM performance, we evaluate their ability to generate hypotheses under various knowledge augmentation settings.In the first setting, LLMs rely solely on their parametric knowledge -information encoded in their parameters during pretraining on large corpora.This evaluates the model's intrinsic understanding and reasoning capabilities.</p>
<p>To enhance hypothesis generation, we introduce a second setting in which LLMs are augmented with structured knowledge from the "seen" knowledge graph.In this approach, key entities from the input are mapped to nodes in the graph, and multi-hop link chains connecting these nodes are explored.These chains, representing relevant relationships, are transformed into textual descriptions and provided as context for the model to use during hypothesis generation.</p>
<p>Another setting leverages information from biomedical literature using a retrieval-augmented generation (RAG) pipeline.Relevant documents are retrieved from the PubMed corpus 2 using BM25 [Robertson et al., 2009].To maintain consistency with the knowledge graph's temporal split, only articles with PMIDs ≤ 36600000 are included in the retrieval.This simulates the process of generating hypotheses based on literature available at a given point in time.</p>
<p>Finally, we consider a combined setting, where both structured knowledge from the graph and unstructured information from retrieved literature are used to support hypothesis generation.This comprehensive approach provides a more holistic context, enabling the model to reason across both sources.The LLM prompt templates we used to combine the external information with the original user instructions can be found in Figures 6, 8, 7, and 9 (Appendix D).</p>
<p>2 https://pubmed.ncbi.nlm.nih.gov/</p>
<p>Evaluation Metrics</p>
<p>To evaluate the quality of generated scientific hypotheses, we employ a set of complementary metrics tailored to different aspects of hypothesis generation.These metrics assess the performance of LLMs in identifying valid connections between entities (link-level evaluation) and predicting specific relations (relation-level evaluation).</p>
<p>For link-level evaluation, we focus on precision, recall, and F1 score.Precision measures the proportion of correctly identified connections among all hypothesized connections, emphasizing the reduction of false positives.Recall evaluates the model's ability to comprehensively identify all valid connections, capturing its sensitivity to true positives.The F1 score, as the harmonic mean of precision and recall, provides a balanced measure of performance, combining both the accuracy of predictions and the coverage of valid connections.These link-level metrics are critical for assessing the LLM's ability to hypothesize plausible relationships between entities, regardless of the specific relation type.</p>
<p>For relation-level evaluation, we employ accuracy to measure how often the generated hypotheses match the correct relation labels in the ground truth.Accuracy captures the overall correctness of hypotheses by considering both the existence of a connection and the predicted relation type.While precision, recall, and F1 focus on identifying potential connections, accuracy provides a finer-grained assessment of the model's capability to generate accurate relation labels.</p>
<p>By combining link-level and relation-level evaluations, the TruthHypo benchmark comprehensively measures the truthfulness of LLM-generated hypotheses, assessing the ability of LLMs to produce scientifically valid outputs.</p>
<p>Knowledge-based Hallucination Detection</p>
<p>As discussed earlier, a critical concern regarding the truthfulness of LLM-generated hypotheses is the occurrence of hallucinations, where models generate plausible-sounding but unsupported claims.To address this, we introduce KnowHD, a knowledge-based hallucination detection framework that evaluates the groundedness of LLM-generated hypotheses by analyzing the rationale behind their generation.KnowHD operates using scientific literature, knowledge graphs, or a combination of both as the knowledge base.An overview of the framework is presented in Figure 2.</p>
<p>To evaluate groundedness, each hypothesis and its reasoning chain are first decomposed into a set of atomic claims.This step is critical because hypotheses often consist of compound reasoning steps, some of which may be supported by existing knowledge while others may not.Parsing these into atomic claims allows a more granular evaluation of groundedness and isolates unsupported components.This step is implemented by prompting LLMs with the template shown in Figure 10 (Appendix D).</p>
<p>When using scientific literature as the knowledge base, relevant documents for each atomic claim are retrieved from the PubMed corpus, limited to articles published before 2023 (PMID ≤ 36600000).BM25 is employed to rank documents based on their relevance to the claim.To ensure computational efficiency and focus on the most relevant information, only the top-k documents are retained.The context retrieved from the literature corpus D for a claim p is defined as:</p>
<p>Rationale of Generated Hypothesis
context D (p) = {d 1 , d 2 , • • • , d k | d i ∈ D, BM25(p; d i ) ≥ τ, rank(d i ) ≤ k},(1)
where
d i represents a document in the corpus, BM25(p; d i )
is the relevance score assigned to the document for the claim p. τ is a threshold ensuring relevance, and rank(d i ) denotes the rank of d i in the BM25-retrieved list.When using a knowledge graph G as the knowledge base, the context for a claim is derived from the graph structure.For a claim p, relevant knowledge is extracted as:
context G (p) = {(e h , r, e t ) ∈ G |{e h , e t } ⊆ V(p) } , (2)
where (e h , r, e t ) represents an edge in the knowledge graph with head entity e h , tail entity e t , and relation r.The set V(p) contains all entities mentioned in the claim p.</p>
<p>The groundedness of a claim is determined based on whether the given context information (context D , context G , or context D ∪ context G ) can fully support the claim, which is implemented by prompting LLMs to provide a judgment using the template in Figure 12 (Appendix D).If the concatenated context collectively entails the claim, it is considered grounded.The overall groundedness of a hypothesis h is computed as:
groudedness(h) = 1 |C(h)| p∈C(h) 1[context(p) |= p], (3)
where C(h) represents the set of atomic claims for hypothesis h, and 1[x |= y] returns 1 if x entails y and 0 otherwise.The
context(p) can be context D (p), context G (p), or context D (p)∪ context G (p).
By offering both literature-based and graph-based contexts, KnowHD provides a robust framework for hallucination detection, offering flexibility to adapt to the available knowledge sources.This systematic evaluation of atomic claims enables a detailed assessment of the groundedness of hypotheses, identifying unsupported components and improving the reliability of LLM-generated outputs.</p>
<p>Benchmark Analysis on TruthHypo</p>
<p>Experiment Settings</p>
<p>To assess the ability of existing LLMs to generate truthful scientific hypotheses, we selected a diverse range of models varying in type and size.The Llama-3 family [Dubey et al., 2024] represents open-source LLMs, while the GPT-4 family [Achiam et al., 2023] exemplifies proprietary models.From each family, we evaluated two LLMs of different sizes (Llama-3.1-8B&amp; Llama-3.1-70B,GPT-4o-mini &amp; GPT-4o) to investigate size-related differences in performance.All LLMs were trained on the knowledge available before 2024, preventing recall of the exact knowledge for hypothesis generation.More implementation details are in Appendix A.</p>
<p>The TruthHypo benchmark evaluates LLMs across four distinct settings: (1) parametric knowledge only, (2) parametric knowledge with knowledge graphs (KG), (3) parametric knowledge with literature (Lit.), and (4) parametric knowledge with both KG and literature.These settings allow us to explore the impact of external knowledge sources on hypothesis generation.The F1 and accuracy scores of different models are reported in this section.More detailed results on the precision and recall can be found in Appendix C.</p>
<p>Comparison of LLMs in Truthful Hypothesis Generation</p>
<p>Table 2 presents the evaluation results for different LLMs and knowledge settings on TruthHypo.Across all tasks, the results indicate that most LLMs struggle to generate truthful scientific hypotheses, with only GPT-4o achieving mean accuracies exceeding 60%.Additionally, we can observe that link-level F1 scores are higher than relation-level accuracy scores, which indicates that LLMs can identify potential connections between entities but often fail to accurately predict the specific relationships.</p>
<p>For models from the same family with different sizes, larger LLMs tend to generate scientific hypotheses more likely to be truthful.This can be attributed to two main factors.First, larger LLMs generally perform better because they can store and leverage more knowledge in their parameters, as shown by the results of parametric knowledge-only setting.Second, LLMs of different sizes have diverse capabilities to process external knowledge for hypothesis generation.For example, GPT-4o-mini shows a modest 1.14% accuracy improvement when augmented with KG and literature, whereas GPT-4o achieves a more substantial 5.14% increase under the same conditions.This suggests that larger LLMs can better utilize additional context to reason about truthful scientific hypotheses.Similar trends are observed when comparing Llama-3.1-8B and Llama-3.1-70B.Interestingly, smaller models, such as Llama-3.1-8B,sometimes experience decreased performance when information from KG and literature is introduced.This degradation may stem from challenges in effectively integrating internal and external information, which can disrupt the model's reasoning processes.Performance differences are also observed across the three relation types: "Chemical &amp; Gene", "Disease &amp; Gene" and "Gene &amp; Gene".Notably, all larger models, including GPT-4o, GPT-4o-mini, and Llama-3.1-70B,tend to perform better on "Chemical &amp; Gene" tasks than on the other two types.This trend suggests that the "Chemical &amp; Gene" task may be more aligned with the pre-trained knowledge or reasoning capabilities of these models.In contrast, the smaller Llama-3.1-8Bshows a more inconsistent pattern, with performance varying across tasks and settings, likely reflecting its more limited parametric capacity and reasoning abilities.These variations in performance across relation types may be attributed to differences in training data distributions or the complexity of the relation types themselves.The relatively stronger performance on the "Chemical &amp; Gene" task highlights potential domain-specific biases or strengths in the LLMs, offering insights into their suitability for targeted applications in real-world scientific discovery.</p>
<p>Hallucination Detection on LLM-generated Hypotheses</p>
<p>To assess the groundedness of the generated hypotheses, we evaluated their rationales using KnowHD under various knowledge settings.KnowHD measures how well a hypothesis is supported by structured knowledge (KG), unstructured knowledge (literature), or both combined.The groundedness evaluation results for hypotheses generated by GPT-4o-mini are presented in Table 3.</p>
<p>The results demonstrate distinct contributions of KG and literature to grounding hypotheses.For example, KnowHD with the literature as the support knowledge base can verify 76.30% claims in the rationales of literature-augmented 'Chemical &amp; Gene" hypotheses.However, the hallucination detector can hardly verify the rationale generated based on adding KG information to parametric knowledge with only 51.08% of the claims being grounded.Combining KG and literature yields the highest groundedness scores, effectively leveraging the complementary strengths of both sources to identify grounded claims and detect hallucinations.</p>
<p>To further explore the relationship between hallucination and truthfulness, Figure 3 examines mean accuracy as a function of groundedness scores.Hypotheses were grouped based on their groundedness scores, and the average accuracy for each group was calculated.The figure reveals a positive correlation between groundedness scores and hypothesis truthfulness.As groundedness scores increase, the likelihood  of the hypothesis being truthful also increases.For example, GPT-4o-mini achieves a mean accuracy of 60.96% on "Chemical &amp; Gene" tasks under the combined KG + Literature setting, but this rises to 72.77% for hypotheses with groundedness scores above 80%.These findings underscore the potential of KnowHD to identify hypotheses with a higher probability of being truthful, particularly in contexts enriched with external knowledge.</p>
<p>Improving Generation of Truthful Hypotheses with KnowHD</p>
<p>To validate the utility of KnowHD on enhancing hypothesis generation, we prompted LLMs to generate five candidate hypotheses for each input and selected the one with the highest groundedness score as the final output.This approach was compared to two baselines: the greedy search method, where the hypothesis is generated using greedy next-token selection by the LLM, and the self-consistency method [Wang et al., 2022], which selects hypotheses based on majority voting across multiple predictions.As shown in Figure 4, groundedness-based hypothesis selection generally outperforms both the greedy search and majority-voting methods across most knowledge settings.In the parametric knowledge-only setting, the majority-voting method achieves slightly higher accuracy (61.86%) compared to groundedness-based selection (59.83%).However, as external knowledge is introduced, groundedness-based selection demonstrates consistent improvements over both baselines.For example, in the combined parametric + KG + Literature setting, GPT-4o-mini achieves an average accuracy of 63.44% when groundedness-based selection is used, approaching the performance of the larger GPT-4o model.</p>
<p>These results highlight the effectiveness of groundedness scores in scenarios where external knowledge is incorporated, as they help identify hypotheses that are more likely to be truthful.By detecting hallucinations in reasoning steps and focusing on grounded hypotheses, KnowHD provides a robust mechanism for enhancing the reliability and truthfulness of LLM-generated scientific hypotheses.</p>
<p>Human Study on Open-ended Tasks</p>
<p>To further assess the generalizability of KnowHD's effectiveness in selecting truthful hypotheses, we conducted experiments on open-ended hypothesis generation tasks.These tasks were designed to evaluate whether KnowHD could reliably identify hypotheses with a higher likelihood of truthfulness across broader and less structured generation scenarios.</p>
<p>For this analysis, we utilized the publicly available hypothesis generation dataset introduced by Qi et al. [2024], which involves generating free-form hypotheses based on given background information.We selected GPT-4o-mini as the tested LLM and enhanced its hypothesis generation process by incorporating external knowledge from scientific literature and knowledge graphs (KG).The model was prompted to generate five distinct scientific hypotheses for each input.These hypotheses were then evaluated by KnowHD, which assessed their groundedness based on their alignment with both structured (KG) and unstructured (literature) knowledge sources.</p>
<p>To analyze the relationship between groundedness scores and hypothesis truthfulness, we filtered generated hypotheses to create pairs with contrasting groundedness levels.For each input, we identified one hypothesis with the highest ground-  edness score and another with the lowest.We retained pairs where the higher groundedness score was at 30% greater than the lower score.This filtering resulted in 54 pairs of hypotheses with significant differences in groundedness levels.To validate KnowHD's effectiveness, we involved two domain experts to annotate each pair (80% agreement), selecting the hypothesis they deemed more likely truthful based on the given information.Additionally, GPT-4o was prompted to analyze the same pairs and provide its judgment.Results of this annotation study, summarized in Table 4, report the selection ratio for each group, defined as the proportion of hypotheses in each group identified as more truthful.</p>
<p>The results demonstrate a significant relationship between groundedness scores and the perceived truthfulness of hypotheses.Hypotheses with higher groundedness scores were consistently more likely to be selected as truthful by both human experts and GPT-4o, as indicated by the substantial differences in selection ratios.These findings highlight the utility of KnowHD in distinguishing truthful hypotheses, even in unstructured, open-ended generation tasks.By effectively leveraging groundedness as a criterion, KnowHD provides a robust mechanism for improving the reliability of LLMgenerated hypotheses, reinforcing its potential for facilitating real-world scientific discovery processes.</p>
<p>6 Related Work</p>
<p>Scientific Hypothesis Generation</p>
<p>The use of LLMs for scientific hypothesis generation is a rapidly growing field, leveraging the ability of these models to process and synthesize vast amounts of scientific literature [Qi et al., 2023;Yang et al., 2023;Zhou et al., 2024;Ciucȃ et al., 2023;Skarlinski et al., 2024;Radensky et al., 2024;Xiong et al., 2024c;Guo et al., 2024].LLMs have been applied in identifying research gaps and generating novel hypotheses, with notable successes in areas such as drug discovery, where generated hypotheses have led to experimentally validated drug combinations [Abdel-Rehim et al., 2024].Despite these advancements, most existing studies emphasize the novelty and diversity of hypotheses without addressing the critical aspect of truthfulness [Qi et al., 2024;Baek et al., 2024;Wang et al., 2023;Hu et al., 2024;Li et al., 2024].The prevalent hallucination problem exacerbates this issue, as LLMs often generate hypotheses that appear plausible but lack factual support [Huang et al., 2023].This gap motivates the development of TruthHypo, a benchmark explicitly designed to assess the ability of LLMs to generate truthful and grounded scientific hypotheses.</p>
<p>Knowledge Graph Reasoning</p>
<p>Knowledge graph reasoning involves inferring missing facts or relationships within a knowledge graph, with tasks such as link prediction, entity classification, and relation extraction being extensively studied [Nickel et al., 2015;Lin et al., 2015;Ji et al., 2021;Shu et al., 2024].Traditional link prediction focuses on predicting edges between entities based on graph structure.These tasks primarily target structured graph completion, emphasizing pattern detection rather than creative reasoning [Zhang and Chen, 2018;Krenn et al., 2023;Liu et al., 2023;Wu et al., 2023;Gu and Krenn, 2024].TruthHypo introduces a novel benchmark that centers on LLM-driven scientific hypothesis generation, leveraging LLMs' ability to flexibly integrate external knowledge through contextual inputs.Unlike static graph reasoning, TruthHypo evaluates how well LLMs generate grounded and truthful hypotheses.This shift highlights the growing role of LLMs in scientific discovery and bridges the gap between symbolic graph reasoning and natural language creativity.</p>
<p>Retrieval-augmented Generation</p>
<p>Retrieval-augmented generation (RAG) has emerged as a powerful approach for improving the factual accuracy and relevance of LLM outputs by integrating external knowledge during the generation process.This technique has been applied with literature retrieval, as demonstrated by [Lewis et al., 2020], to dynamically incorporate up-to-date information into model outputs.Retrieval-augmented generation methods enhance the ability of LLMs to ground their outputs in external knowledge, making them particularly valuable in tasks requiring factual accuracy, such as scientific text generation [Lála et al., 2023;Munikoti et al., 2023].In addition to literature retrieval, retrieval-augmented generation using knowledge graphs has gained attention for its potential to provide structured, domain-specific knowledge during text generation [Peng et al., 2024;Ma et al., 2024;Wang et al., 2025].Truth-Hypo builds on this paradigm by integrating both literature and knowledge graph retrieval to provide a robust evaluation of LLMs' ability to generate truthful scientific hypotheses.This dual approach enables a comprehensive analysis of the role of external knowledge in mitigating hallucinations and ensuring the groundedness of generated hypotheses.</p>
<p>Conclusion</p>
<p>We presented TruthHypo, a benchmark for evaluating the ability of LLMs to generate truthful scientific hypotheses, and KnowHD, a framework for detecting hallucinations by assessing groundedness in reasoning.Through extensive evaluation, we highlighted the limitations of existing LLMs and demonstrated that selecting highly grounded hypotheses improves truthfulness.These contributions offer valuable insights for improving the reliability and utility of LLMs in scientific discovery.</p>
<p>A Implementation Details</p>
<p>For the retrieval of external knowledge from scientific literature, we implemented the information retrieval system by adopting the BM25 retriever [Robertson et al., 2009] for processed PubMed chunks provided by the MedRAG toolkit [Xiong et al., 2024a,b].BM25 (Best Matching 25) is a probabilistic retrieval model that ranks documents based on term frequency, document length normalization, and the specificity of terms through inverse document frequency (IDF).We selected BM25 as our text retriever because it is particularly effective for the biomedical domain, where dense retrievers often struggle to encode the nuanced semantics of biomedical terms such as gene names [Luo et al., 2022].BM25's reliance on exact term matching with statistical weighting makes it well-suited for capturing term-specific relevance in structured biomedical text.In our experiments, τ in Equation ( 1) is set as 0.0.The number of retrieved documents is set as k = 32 for hypothesis generation, and k = 8 for claim verification.</p>
<p>To identify biomedical entities in a given claim, we used a two-step process.In the first step, we prompted LLMs to extract the entity mentions directly from the claim (Figure 11).This step focused on identifying relevant biomedical terms, such as gene names, proteins, or diseases, without additional processing or complex workflows.The extracted entity mentions were then used in the second step, where each mention was matched to its unified representation in the PubTator 3.0 knowledge graph.This matching was implemented using a BM25 retriever.For constructing the BM25 index, each piece of text, or "chunk", was designed by concatenating all possible mentions of a given entity stored in PubTator 3.0.By leveraging BM25's ranking capabilities, we retrieved the most relevant chunk corresponding to each entity mention, ensuring accurate alignment with PubTator's unified entities.</p>
<p>B Computational Cost</p>
<p>Table 5 shows the number of all tokens used in experiments for Table 2.It shows that the additional knowledge from either the knowledge graph (KG) or literature (Lit.) will significantly increase the number of input tokens.In particular, the literature brings more tokens than KG, as the knowledge in KG is always structured and summarized.While input lengths vary across different settings, output lengths are relatively stable, a consistent pattern shown in different LLMs.</p>
<p>C Additional Quantitative Results on TruthHypo</p>
<p>Table 2 presents the F1 score of various LLMs on the Truth-Hypo benchmark, evaluating their ability to identify the existence of a new relation given current knowledge.To provide a more granular analysis, Table 6 breaks down the results into precision and recall for different tasks, offering insights into the strengths and weaknesses of each model and knowledge augmentation setting.</p>
<p>From the results in Table 6, we observe that smaller LLMs, such as Llama-3.1-8B,tend to achieve higher recall scores across all tasks, along with relatively lower precision.This indicates that while these models can generate a comprehensive set of hypotheses, they are prone to a high false positive rate, which could pose challenges in real-world applications, such as scientific hypothesis generation, where precision is often critical.High false positive rates could result in wasted time and resources when pursuing hypotheses that are unlikely to hold upon experimental validation.</p>
<p>Given that validating new biomedical hypotheses often requires months or even years of research, ensuring high precision in hypothesis generation is of paramount importance.Among the tested models, GPT-4o with external knowledge from the literature achieved the highest precision across all tasks, demonstrating its ability to generate hypotheses with fewer false positives.However, this precision came at the expense of lower recall, especially when compared to GPT-4o with knowledge augmentations from both literature and knowledge graphs (KG).This trade-off highlights the importance of balancing precision and recall based on the specific requirements of a given application.</p>
<p>When comparing different knowledge settings, we found that the improvements provided by external knowledge sources varied across tasks and models.For example, knowledge graph (KG) information significantly enhanced the precision of all LLMs on tasks involving "Disease &amp; Gene" and "Gene &amp; Gene" relations, but it did not notably improve the precision of GPT-4o on the "Chemical &amp; Gene" task.In contrast, the literature knowledge augmentation slightly improved the precision of all LLMs except GPT-4o-mini.Interestingly, the setting that combined both knowledge sources provided a more balanced precision improvement, offering a middle ground between the individual benefits of KG and literature-based augmentations.</p>
<p>Additionally, Table 6 reveals that larger models such as GPT-4o consistently outperformed smaller models in precision, regardless of the knowledge setting, reflecting their ability to integrate complex external information effectively.This highlights the potential of larger models to better utilize structured and unstructured knowledge sources for hypothesis generation.However, smaller models, with their higher recall, may still serve as useful tools for exploratory or broad hypothesis generation tasks where exhaustive coverage is prioritized over precision.</p>
<p>Overall, the analysis demonstrates that the choice of LLM and knowledge augmentation strategy should be guided by the specific trade-offs between precision and recall that align with the requirements of the downstream task.For biomedi-cal applications, where precision is often paramount, leveraging models like GPT-4o with literature-based augmentations appears to be the most effective approach.</p>
<p>To further understand the limitations of hypothesis generation with high groundedness scores, we conducted an indepth analysis of the error patterns.We identified two representative types of errors: (1) cases where the LLM incorrectly infers that there is no association between the given entities, despite supporting evidence; and (2) cases where the model simply echoes or paraphrases the provided context without engaging in substantive reasoning or hypothesis formation.These findings highlight the need for more robust mechanisms to ensure both accurate association detection and genuine reasoning in hypothesis generation, enhancing the interpretability and trustworthiness of the overall system [Doshi-Velez and Kim, 2017;Loh et al., 2022;Miller, 2023;Sinha et al., 2024a,b].</p>
<p>D Prompt Templates for LLMs in Experiments</p>
<p>Figure 5 shows the template we used to construct a hypothesis generation query given two different entities.The prompt templates for the use of LLMs in the "Parametric", "Parametric + KG", "Parametric + Lit.", and "Parametric + KG + Lit." settings are presented in Figures 6, 7, 8, 9, respectively.These templates were designed to guide the LLMs in effectively leveraging various sources of knowledge while maintaining a pre-determined structure in the model output to facilitate consistent parsing and downstream analysis.</p>
<p>Figure 10 shows the template for LLMs to extract scientific claims from the entire rationale.For the identification of biomedical entities and the use of LLMs for claim verification, we employed the templates shown in Figures 11 and 12, respectively.The entity identification templates (Figure 11) were crafted to enable the LLMs to extract precise mentions of biomedical entities such as genes or diseases from textual claims.These prompts were carefully designed to minimize ambiguity, ensuring that entities sharing the same mention could be properly distinguished using their unique IDs.</p>
<p>Knowledge</p>
<p>Figure 1 :
1
Figure 1: Overview of the TruthHypo benchmark, including dataset construction, task formulation, and truthfulness evaluation.</p>
<p>Figure 2 :
2
Figure 2: Overview of the KnowHD hallucination detection framework.Hypotheses are parsed into atomic claims, which are then evaluated for groundedness using a knowledge graph, scientific literature or both as knowledge sources.</p>
<p>Figure 3 :
3
Figure 3: Mean accuracy corresponding to different levels of groundedness.Hypotheses are grouped based on their groundedness scores provided by KnowHD (KG + Literature).Only groups with no less than 10 hypotheses are shown in the plots.The dot size reflects the number of samples in each level of groundedness.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Prompt template for constructing user input with given entities.</p>
<p>Table 1 :
1
. Statistics of various tasks in the TruthHypo benchmark.
TaskLabel# Instancepositive correlate328Chemical &amp; Genenegative correlate478no relation403stimulate104Disease &amp; Geneinhibit75no relation89positive correlate247Gene &amp; Genenegative correlate118no relation182</p>
<p>Table 2 :
2
Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings.The metrics reported are link-level F1 and relation-level accuracy (Acc) for each task (Chemical &amp; Gene, Disease &amp; Gene, Gene &amp; Gene), as well as their averages."Param."denotes parametric knowledge, while "KG" and "Lit."refer to knowledge graphs and literature, respectively.All scores are percentages (%).
KnowledgeLLMChemical &amp; Gene Disease &amp; Gene Gene &amp; Gene F1 Acc F1 Acc F1 AccAverage F1 AccLlama-3.1-8B80.1642.4379.3741.0479.19 46.07 66.90 43.23ParametricLlama-3.1-70B 81.3652.4483.2954.4876.66 49.91 71.54 52.03[Wei et al., 2022]GPT-4o-mini83.3161.2981.8459.3379.32 53.02 75.49 58.79GPT-4o80.7466.1775.3854.8571.56 55.58 73.17 61.81Llama-3.1-8B81.3740.6179.5948.1379.61 48.45 70.65 43.73Parametric + KGLlama-3.1-70B 87.8562.8667.6252.2478.29 58.14 79.10 60.18[Baek et al., 2024]GPT-4o-mini86.4257.6574.1755.6081.65 62.34 79.40 58.65GPT-4o88.6663.8579.5056.7282.73 61.06 81.62 62.15Llama-3.1-8B80.7846.0780.4643.2879.91 42.60 68.58 44.76Parametric + Lit.Llama-3.1-70B 82.5656.7484.1652.9979.18 51.55 73.37 54.84[Lewis et al., 2020]GPT-4o-mini85.2859.8085.7153.7381.50 51.19 77.08 56.67GPT-4o79.5265.9275.8455.9764.69 51.92 71.84 60.82Llama-3.1-8B75.9836.4877.5841.4279.19 45.70 65.37 39.62Parametric + KGLlama-3.1-70B 84.8059.3177.6456.3481.24 55.76 77.37 57.95+ LiteratureGPT-4o-mini88.3460.9684.4758.2184.17 58.50 81.42 59.93GPT-4o89.7169.3182.8662.3185.91 63.99 83.55 66.95</p>
<p>Table 4 :
4
Resultsof analysis on open-ended hypothesis generation tasks."GPT" and "Human" denote the selection ratios by GPT-4o and human experts, respectively.All scores are percentages (%).pvalues were calculated using Wilcoxon signed-rank test and Z-test.</p>
<p>Table 5 :
5
Summary of #tokens used for all experiments in Table 2.
LLMTypeParam.+KGSetting +Lit.+KG+Lit.LlamaInput295.8k1.7M25.8M27.2M-3.1-8BOutput 811.1k1.0M782.5k1.2MLlamaInput295.8k1.7M25.8M27.2M-3.1-70BOutput 813.7k 881.2k 777.6k767.8kGPT-4oInput295.8k1.7M25.8M27.2M-miniOutput 751.6k 684.0k 787.2k707.1kGPT-4oInput Output 909.9k 839.1k 891.5k 295.8k 1.7M 25.8M27.2M 875.3k</p>
<p>Table 6 :
6
Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings, with precision and recall as the evaluation metrics."Prec"denotes the link-level precision, while "Recall" represents the link-level recall.Prompt template for constructing user input with given entitiesCan we hypothesize the potential relation between {{entity type 1}} {{entity name 1}} ({{entity ID 1}}) and {{entity type 2}} {{entity name 2}} ({{entity ID 2}})?The final hypothesis can be one of [{{relation label 1}}, {{relation label 2}}, 'no relation'].
LLMChemical &amp; Gene Disease &amp; Gene Gene &amp; Gene Prec Recall Prec Recall Prec Recall Prec Recall AverageLlama-3.1-8B 67.5798.5166.7997.7766.92 96.99 67.29 98.00ParametricLlama-3.1-70B 74.6989.3375.2393.3073.13 80.55 74.37 87.48Wei et al. [2022]GPT-4o-mini83.0083.6279.4784.3675.50 83.56 80.37 83.70GPT-4o90.5972.8382.6769.2783.27 62.74 87.60 69.63Llama-3.1-8B 71.4294.5474.0486.0372.16 88.77 71.93 91.85Parametric + KGLlama-3.1-70B 90.6385.2493.1453.0788.58 70.14 90.34 76.89Baek et al. [2024]GPT-4o-mini86.3786.4891.0662.5792.39 73.15 88.27 79.70GPT-4o86.2791.1991.3070.3987.96 78.08 87.21 84.89Llama-3.1-8B 68.8297.7768.3697.7768.49 95.89 68.67 97.26Parametric + Lit.Llama-3.1-70B 74.9291.9475.5694.9774.58 84.38 74.92 90.30Lewis et al. [2020]GPT-4o-mini78.1893.8080.1092.1874.94 89.32 77.55 92.37GPT-4o92.7369.6083.7869.2789.37 50.68 90.62 64.44Llama-3.1-8B 68.2185.7370.6486.0369.96 91.23 69.01 87.26Parametric + KGLlama-3.1-70B 84.1385.4887.4169.8380.05 82.47 83.33 82.59+ LiteratureGPT-4o-mini82.6194.9182.4586.5983.16 85.21 82.73 91.19GPT-4o86.6193.0584.8081.0187.50 84.38 86.61 89.11
PMID is the unique identifier of the paper where the edge was extracted.
AcknowledgementsThis work is supported in part by the US National Science Foundation under grants 2217071, 2213700, 2106913,  2008208, and NIH grant 1R01LM014012.Prompt template for hypothesis generation with knowledge from parameters and KG You are a scientist.Your task is to generate a scientific hypothesis following given instructions.Prompt template for claim identification ### Statement {{statement}}Summarize the statement as a list of claims which will be further verified by external resources.Output the summarized claims in the JSON format: '''json{"claims": ["claim1", ...]}'''Prompt template for entity recognition ### Background {{background}}Extract key entities from the background statement that will be used to search for relevant information in an external knowledge graph.Each entity should be extracted as "entity type (e.g., Disease/Chemical/-Gene/Mutation) entity name (entity id if presented)".Output the extracted entities in the JSON format: '''json{"entities": ["entity1", ...]}'''
Scientific hypothesis generation by a large language model: Laboratory validation in breast cancer treatment. Abbi Abdel-Rehim, Hector Zenil, Oghenejokpeme Orhobor, Marie Fisher, Ross J Collins, Elizabeth Bourne, Gareth W Fearnley, Emma Tate, Holly X Smith, Larisa N Soldatova, arXiv:2405.122582024arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. Ioana Ciucȃ, Yuan-Sen, Sandor Ting, Kartheik Kruk, Iyer, arXiv:2306.116482023arXiv preprint</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.086082017arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>On the creativity of large language models. Giorgio Franceschelli, Mirco Musolesi, AI &amp; SOCIETY. 2024</p>
<p>Forecasting high-impact research topics via machine learning on evolving knowledge graphs. Xuemei Gu, Mario Krenn, arXiv:2402.086402024arXiv preprint</p>
<p>Embracing foundation models for advancing scientific discovery. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Aidong Xiong, Zhang, 2024 IEEE International Conference on Big Data (BigData). IEEE2024</p>
<p>Ideabench: Benchmarking large language models for research idea generation. Sikun Guo, Hassan Amir, Guangzhi Shariatmadari, Albert Xiong, Myles Huang, Corey M Kim, Stefan Williams, Aidong Bekiranov, Zhang, 31st SIGKDD Conference on Knowledge Discovery and Data Mining -Datasets and Benchmarks Track. 2025</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.052322023arXiv preprint</p>
<p>Autonomous llm-driven research-from data to human-verifiable research papers. Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, Roy Kishony, NEJM AI. 21AIoa2400555, 2025</p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip Yu, IEEE transactions on neural networks and learning systems. 202133</p>
<p>Demystifying large language models for medicine: A primer. Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, arXiv:2410.188562024arXiv preprint</p>
<p>Large language models versus natural language understanding and generation. Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, Michael Vassilakopoulos, Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics. the 27th Pan-Hellenic Conference on Progress in Computing and Informatics2023</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, Nima João P Moutinho, Sanjabi, Nature Machine Intelligence. 5112023</p>
<p>Mycrunchgpt: A chatgpt assisted framework for scientific machine learning. Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, George Em Karniadakis, arXiv:2306.155512023arXiv preprint</p>
<p>Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, arXiv:2312.07559Paperqa: Retrieval-augmented generative agent for scientific research. 2023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024arXiv preprint</p>
<p>Learning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201529</p>
<p>A survey on graph classification and link prediction based on gnn. Xingyu Liu, Juan Chen, Quan Wen, arXiv:2307.008652023arXiv preprint</p>
<p>Conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Application of explainable artificial intelligence for healthcare: A systematic review of the last decade. Wen Hui, Loh, Ping Chui, Silvia Ooi, Prabal Seoni, Filippo Datta Barua, Molinari, Acharya Rajendra, Computer methods and programs in biomedicine. 2011-2022. 2022226107161</p>
<p>Improving biomedical information retrieval with neural retrievers. Man Luo, Arindam Mitra, Tejas Gokhale, Chitta Baral, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 2024</p>
<p>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo, Think-on-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. arXiv e-prints. 20242407</p>
<p>Explainable ai is dead, long live explainable ai! hypothesis-driven decision support using evaluative ai. Tim Miller, Proceedings of the 2023 ACM conference on fairness, accountability, and transparency. the 2023 ACM conference on fairness, accountability, and transparency2023</p>
<p>Evaluating the effectiveness of retrievalaugmented large language models in scientific document reasoning. Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana, arXiv:2311.043482023arXiv preprint</p>
<p>A review of relational machine learning for knowledge graphs. Maximilian Nickel, Kevin Murphy, Evgeniy Volker Tresp, Gabrilovich, Proceedings of the IEEE. 10412015</p>
<p>Can chatgpt be used to generate scientific hypotheses. Yang Jeong, Park , Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, Ju Li, Journal of Materiomics. 1032024</p>
<p>Graph retrieval-augmented generation: A survey. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang, arXiv:2408.089212024arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, First Conference on Language Modeling. 2024</p>
<p>Scideator: Humanllm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>Most Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam. A review on large language models: Architectures, applications, taxonomies, open issues and challenges. Mohaimenul Azam Khan Raiaan, Md Saddam Hossain, Kaniz Mukta, Nur Fatema, Sadman Mohammad Fahad, Sakib, 2024IEEE Access</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Knowledge graph large language model (kg-llm) for link prediction. Dong Shu, Tianle Chen, Mingyu Jin, Chong Zhang, Mengnan Du, Yongfeng Zhang, arXiv:2403.073112024arXiv preprint</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Colidr: Concept learning using aggregated disentangled representations. Sanchit Sinha, Guangzhi Xiong, Aidong Zhang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>A selfexplaining neural architecture for generalizable concept learning. Sanchit Sinha, Guangzhi Xiong, Aidong Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. Sam Michael D Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Hinks, J Michael, Manvitha Hammerling, Ponnapati, Andrew D Samuel G Rodriques, White, arXiv:2409.137402024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592023arXiv preprint</p>
<p>Knowledge graph retrievalaugmented generation for llm-based recommendation. Shijie Wang, Wenqi Fan, Yue Feng, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, arXiv:2501.022262025arXiv preprint</p>
<p>Pubtator 3.0: an ai-powered literature resource for unlocking biomedical knowledge. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Nucleic Acids Research. 35e2352022. 2024Chain-ofthought prompting elicits reasoning in large language models</p>
<p>Generating scientific claims for zero-shot scientific fact checking. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu, Wang , arXiv:2203.129902022arXiv preprint</p>
<p>Dynamic link prediction using graph representation learning with enhanced structure and temporal information. Chaokai Wu, Yansong Wang, Tao Jia, 2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD). IEEE2023</p>
<p>Benchmarking retrieval-augmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, Findings of the Association for Computational Linguistics: ACL 2024. 2024</p>
<p>Improving retrievalaugmented generation in medicine with iterative follow-up questions. Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang, Biocomputing 2025: Proceedings of the Pacific Symposium. World Scientific2024</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2025</p>
<p>Meta-review generation with checklist-guided iterative introspection. Qi Zeng, Mankeerat Sidhu, Pong Hou, Lu Chan, Heng Wang, Ji, arXiv:2305.146472023arXiv preprint</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 201831</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202336</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
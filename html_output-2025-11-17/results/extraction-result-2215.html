<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2215 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2215</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2215</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-281394034</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.14647v1.pdf" target="_blank">AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production</a></p>
                <p><strong>Paper Abstract:</strong> With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the framework's practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2215.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2215.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentCompass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FAGI-AgentCompass (AgentCompass)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented, multi-stage evaluation framework that analyzes agentic execution traces via a plan-and-execute LLM pipeline, hierarchical error taxonomy, trace-level clustering, and persistent episodic/semantic memory to produce quantitative quality scores and actionable remediation suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FAGI-AgentCompass</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AgentCompass transforms OpenTelemetry execution traces into structured representations, runs an LLM-driven plan-and-execute analytical pipeline (error identification & categorization → thematic clustering → quantitative scoring → synthesis), clusters errors across traces using transformer embeddings + HDBSCAN with probabilistic assignment, and maintains episodic and semantic memory to augment future analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Agentic AI evaluation / machine learning system monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Automated evaluation metrics: Categorization F1-Score (Cat.F1), Localization Accuracy (Loc.Acc.), Joint score, and system-generated overall quality score per trace</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Categorization F1: F1 measure comparing the agent's predicted error categories (mapped to TRAIL categories) to human annotations; Localization Accuracy: proportion of errors for which the system predicts the correct span location; Joint score: proportion where both category and location are correct; overall quality score: aggregate numeric score computed by AgentCompass from detected errors across predefined dimensions (factual grounding, safety, plan execution, etc.). These are computed by comparing AgentCompass outputs to TRAIL human annotations (after mapping taxonomies) and by internal scoring rules that weight detected issues.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate (data-driven evaluation metrics computed by model outputs vs human-annotated benchmark); includes ML-derived components (embedding + clustering) used to produce surrogate diagnostic groupings</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Human-annotated TRAIL labels (span ID + error category) and manual expert review</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>TRAIL benchmark provides human-generated annotations that locate the precise span(s) with errors and assign categories according to the TRAIL taxonomy; manual reviewer adjudication was used for qualitative inspection of false positives to determine whether AgentCompass found valid errors missed by annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Pearson correlation between AgentCompass overall quality scores and human overall quality scores: ρ = 0.430 (GAIA split), ρ = 0.408 (SWE Bench split). Comparative model gaps: Localization Accuracy difference on TRAIL (GAIA): AgentCompass 0.657 vs Gemini-2.5-Pro 0.546 (Δ = +0.111).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>On TRAIL benchmark: Localization Accuracy = 0.657 (GAIA), 0.250 (SWE Bench); Categorization F1 = 0.309 (GAIA), 0.232 (SWE Bench); Joint score = 0.239 (GAIA), 0.051 (SWE Bench).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Ground-truth is human annotation; not a performance metric. Human annotations serve as gold-standard labels for evaluation; manual review showed a non-trivial subset of system 'false positives' corresponded to valid errors omitted from the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>AgentCompass frequently identifies errors that are novel relative to the TRAIL human annotations (e.g., Safety & Security alerts, Reflection Gaps); these are 'novel with respect to the annotation schema' rather than scientific-discovery novelty—characterized as out-of-annotation-distribution / orthogonal to benchmark coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Qualitatively reported: the proxy-to-ground-truth mismatch increases when AgentCompass detects classes of errors (e.g., Safety & Security, Reflection Gaps) that were absent from the benchmark's taxonomy; no numerical stratification by novelty distance provided.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Methodological improvements to reduce proxy-to-ground-truth mismatch include: (1) plan-and-execute iterative decomposition (separate planning and execution phases) to reduce goal drift and improve localization; (2) memory augmentation (episodic + semantic) to use historical findings as priors; (3) trace-level clustering (transformer embeddings + HDBSCAN + probabilistic assignment) to surface recurring issues and reduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Empirical improvements reported: Localization Accuracy on GAIA improved to 0.657 (AgentCompass) vs 0.546 for Gemini-2.5-Pro (Δ +0.111). Other metrics (Categorization F1 and Joint) are top-tier or state-of-the-art on respective splits; no explicit ablation numbers isolating each method's contribution were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging — agentic workflow evaluation is an active, maturing area; benchmarking practices are evolving and existing human-annotation taxonomies are incomplete for production concerns like safety and reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Limited: uncertainty is present in soft clustering (probabilistic assignment of noise points to clusters) and in probabilistic cluster membership thresholds, but no calibration of confidence estimates against proxy-to-ground-truth gap (e.g., reliability diagrams or calibrated predictive intervals) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Multiple proxies (Cat.F1, Loc.Acc, Joint, overall quality score) are reported but the paper does not provide a formal analysis of correlations or independent failure modes between these proxies; qualitative tension is noted (e.g., high localization without commensurate categorization in some splits).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational evaluation pipeline → comparison against TRAIL human-annotated ground truth → manual review of false positives for additional validation. Errors discovered by AgentCompass can be converted into developer 'tickets' and tracked; no wet-lab or external experimental cascade present.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Key limitations include: taxonomy alignment mismatches (AgentCompass' granular taxonomy must be mapped to TRAIL categories, introducing metric distortion), moderate Pearson correlation with human scores because AgentCompass uncovers additional valid errors outside the benchmark, lack of numeric false-positive/false-negative rates reported, no ablation quantifying contributions of memory vs plan-and-execute, and absence of calibrated predictive uncertainty tied to ground-truth error detection.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Factors affecting proxy-to-ground-truth gap include taxonomy granularity and coverage (missing categories like Safety & Security in benchmarks), annotation incompleteness, emergent multi-agent error propagation (cascading failures), heterogeneity of trace data (GAIA vs SWE Bench splits), and ambiguity in mapping system-specific granular error types to benchmark labels.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Trail: Trace reasoning and agentic issue localization <em>(Rating: 2)</em></li>
                <li>Gaia: a benchmark for general ai assistants <em>(Rating: 2)</em></li>
                <li>Swe-bench: Can language models resolve real-world github issues? <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2215",
    "paper_id": "paper-281394034",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "AgentCompass",
            "name_full": "FAGI-AgentCompass (AgentCompass)",
            "brief_description": "A memory-augmented, multi-stage evaluation framework that analyzes agentic execution traces via a plan-and-execute LLM pipeline, hierarchical error taxonomy, trace-level clustering, and persistent episodic/semantic memory to produce quantitative quality scores and actionable remediation suggestions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "FAGI-AgentCompass",
            "system_description": "AgentCompass transforms OpenTelemetry execution traces into structured representations, runs an LLM-driven plan-and-execute analytical pipeline (error identification & categorization → thematic clustering → quantitative scoring → synthesis), clusters errors across traces using transformer embeddings + HDBSCAN with probabilistic assignment, and maintains episodic and semantic memory to augment future analyses.",
            "domain": "Agentic AI evaluation / machine learning system monitoring",
            "proxy_metric_name": "Automated evaluation metrics: Categorization F1-Score (Cat.F1), Localization Accuracy (Loc.Acc.), Joint score, and system-generated overall quality score per trace",
            "proxy_metric_description": "Categorization F1: F1 measure comparing the agent's predicted error categories (mapped to TRAIL categories) to human annotations; Localization Accuracy: proportion of errors for which the system predicts the correct span location; Joint score: proportion where both category and location are correct; overall quality score: aggregate numeric score computed by AgentCompass from detected errors across predefined dimensions (factual grounding, safety, plan execution, etc.). These are computed by comparing AgentCompass outputs to TRAIL human annotations (after mapping taxonomies) and by internal scoring rules that weight detected issues.",
            "proxy_metric_type": "empirical surrogate (data-driven evaluation metrics computed by model outputs vs human-annotated benchmark); includes ML-derived components (embedding + clustering) used to produce surrogate diagnostic groupings",
            "ground_truth_metric": "Human-annotated TRAIL labels (span ID + error category) and manual expert review",
            "ground_truth_description": "TRAIL benchmark provides human-generated annotations that locate the precise span(s) with errors and assign categories according to the TRAIL taxonomy; manual reviewer adjudication was used for qualitative inspection of false positives to determine whether AgentCompass found valid errors missed by annotators.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Pearson correlation between AgentCompass overall quality scores and human overall quality scores: ρ = 0.430 (GAIA split), ρ = 0.408 (SWE Bench split). Comparative model gaps: Localization Accuracy difference on TRAIL (GAIA): AgentCompass 0.657 vs Gemini-2.5-Pro 0.546 (Δ = +0.111).",
            "proxy_performance": "On TRAIL benchmark: Localization Accuracy = 0.657 (GAIA), 0.250 (SWE Bench); Categorization F1 = 0.309 (GAIA), 0.232 (SWE Bench); Joint score = 0.239 (GAIA), 0.051 (SWE Bench).",
            "ground_truth_performance": "Ground-truth is human annotation; not a performance metric. Human annotations serve as gold-standard labels for evaluation; manual review showed a non-trivial subset of system 'false positives' corresponded to valid errors omitted from the benchmark.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "AgentCompass frequently identifies errors that are novel relative to the TRAIL human annotations (e.g., Safety & Security alerts, Reflection Gaps); these are 'novel with respect to the annotation schema' rather than scientific-discovery novelty—characterized as out-of-annotation-distribution / orthogonal to benchmark coverage.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": "Qualitatively reported: the proxy-to-ground-truth mismatch increases when AgentCompass detects classes of errors (e.g., Safety & Security, Reflection Gaps) that were absent from the benchmark's taxonomy; no numerical stratification by novelty distance provided.",
            "gap_reduction_method": "Methodological improvements to reduce proxy-to-ground-truth mismatch include: (1) plan-and-execute iterative decomposition (separate planning and execution phases) to reduce goal drift and improve localization; (2) memory augmentation (episodic + semantic) to use historical findings as priors; (3) trace-level clustering (transformer embeddings + HDBSCAN + probabilistic assignment) to surface recurring issues and reduce noise.",
            "gap_reduction_effectiveness": "Empirical improvements reported: Localization Accuracy on GAIA improved to 0.657 (AgentCompass) vs 0.546 for Gemini-2.5-Pro (Δ +0.111). Other metrics (Categorization F1 and Joint) are top-tier or state-of-the-art on respective splits; no explicit ablation numbers isolating each method's contribution were reported.",
            "validation_cost_comparison": null,
            "temporal_validation": null,
            "domain_maturity": "Emerging — agentic workflow evaluation is an active, maturing area; benchmarking practices are evolving and existing human-annotation taxonomies are incomplete for production concerns like safety and reflection.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Limited: uncertainty is present in soft clustering (probabilistic assignment of noise points to clusters) and in probabilistic cluster membership thresholds, but no calibration of confidence estimates against proxy-to-ground-truth gap (e.g., reliability diagrams or calibrated predictive intervals) is reported.",
            "multiple_proxies": true,
            "proxy_correlation": "Multiple proxies (Cat.F1, Loc.Acc, Joint, overall quality score) are reported but the paper does not provide a formal analysis of correlations or independent failure modes between these proxies; qualitative tension is noted (e.g., high localization without commensurate categorization in some splits).",
            "validation_cascade": "Computational evaluation pipeline → comparison against TRAIL human-annotated ground truth → manual review of false positives for additional validation. Errors discovered by AgentCompass can be converted into developer 'tickets' and tracked; no wet-lab or external experimental cascade present.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Key limitations include: taxonomy alignment mismatches (AgentCompass' granular taxonomy must be mapped to TRAIL categories, introducing metric distortion), moderate Pearson correlation with human scores because AgentCompass uncovers additional valid errors outside the benchmark, lack of numeric false-positive/false-negative rates reported, no ablation quantifying contributions of memory vs plan-and-execute, and absence of calibrated predictive uncertainty tied to ground-truth error detection.",
            "domain_specific_factors": "Factors affecting proxy-to-ground-truth gap include taxonomy granularity and coverage (missing categories like Safety & Security in benchmarks), annotation incompleteness, emergent multi-agent error propagation (cascading failures), heterogeneity of trace data (GAIA vs SWE Bench splits), and ambiguity in mapping system-specific granular error types to benchmark labels.",
            "uuid": "e2215.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Trail: Trace reasoning and agentic issue localization",
            "rating": 2
        },
        {
            "paper_title": "Gaia: a benchmark for general ai assistants",
            "rating": 2
        },
        {
            "paper_title": "Swe-bench: Can language models resolve real-world github issues?",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 1
        }
    ],
    "cost": 0.00858075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production
18 Sep 2025</p>
<p>Nvjk Kartik kartik.nvj@futureagi.com 
Nikhil Pareek FutureAGI Inc</p>
<p>Garvit Sapra garvit.sapra@futureagi.com 
Nikhil Pareek FutureAGI Inc</p>
<p>Rishav Hada rishav@futureagi.com 
Nikhil Pareek FutureAGI Inc</p>
<p>AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production
18 Sep 20259ED843BBFC6530A0B1C49C8C2E8AB83DarXiv:2509.14647v1[cs.AI]
With the growing adoption of Large Language Models (LLMs) in automating complex, multiagent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture.We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows.AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization.The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions.Through collaborations with design partners, we demonstrate the framework's practical utility on realworld deployments, before establishing its efficacy against the publicly available TRAIL benchmark.AgentCompass achieves state-ofthe-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production.</p>
<p>Introduction</p>
<p>With recent advancements in Large Language Model's reasoning capabilities, they are increasingly being used for complex reasoning tasks such as agentic workflows.Agentic workflows allow users to automate certain tasks, based on internal thinking and reasoning capabilites of LLMs.These workflows range from being extremely simple such as retrieving a knowledge-base answer to a customer query, to very complex such as multi-step decision making in supply chain optimization or coordinating multiple specialized agents in a financial analysis pipeline.</p>
<p>Recent surveys by Wakefield Research (Pager-Duty, 2025) and other organizations (EIN Press-wire / Market.us,2025) show, agentic workflows have surged dramatically across industries in 2025, with statistics showing widespread adoption, hefty return on investment (ROI) gains, and rapid scaling in enterprise environments.Organizations are deploying agentic workflows across critical business functions.Automation enabled by agentic workflows has resulted in 20-30% cost savings for organizations, freeing resources for strategic growth (Belcak et al., 2025;Yu et al., 2025).However, there are some major roadblocks in widespread adoption of agentic workflows.Organizations face a range of technical, organizational, and operational roadblocks.Agents often make decisions using non-standardized paths, destabilizing established workflows and making process oversight difficult in regulated environments.Organizations are concerned about bias, and poor-quality or fragmented datasets slow reliable adoption and amplify systemic errors.Poor evaluation and systems breaking in production are major causes of financial and reputational damage to organizations adopting agentic AI (Wang et al., 2025;Liu et al., 2025;Chaudhry et al., 2025).Insufficient evaluation methods, over-reliance on technical benchmarks, and inadequate real-world robustness testing often leave organizations unprepared for edge cases and contextual failures.Most current evaluation frameworks prioritize technical metrics (accuracy, speed) and neglect stress tests for human-centered context, edge cases, and emotional intelligence, which are critical for user experience and trust.Errors and biases often compound and propagate through multiagent workflows, making correction and accountability complex.Unaddressed failures can cause legal liabilities and loss of competitive advantage (Moteki et al., 2025;Meimandi et al., 2025;Raza et al., 2025;Yehudai et al., 2025;Shukla, 2025).</p>
<p>To address this gap, recent research focuses on developing robust evaluation frameworks, that not only measure accuracy on a controlled dataset, but also includes real world scenarios with subjective evaluation metrics.For example, GAIA proposes a benchmark for General AI Assistants (Mialon et al., 2023).GAIA focuses on real-world tasks that require reasoning, multi-modality, web browsing, and effective use of external tools.This moves beyond traditional benchmarks, which often focus on narrow tasks (like text classification or image recognition), by emphasizing an AI agent's ability to generalize, adapt to changing scenarios, perform complex multi-step operations, and interact with diverse information sources.TRAIL (Deshpande et al., 2025) extends the GAIA benchmark by providing an error taxonomy for agentic executions with an annotated dataset demonstrating the practical usage of the proposed taxonomy.</p>
<p>Most organizations are unprepared for the complexities of agentic and multi-agentic AI risks, with governance blind spots multiplying postdeployment.These gaps often manifest as cascading failures, emergent behaviors, and unanticipated interactions with external systems, which existing monitoring and control frameworks struggle to detect or mitigate.In this work, we present Agent-Compass, the first evaluation framework purposebuilt for monitoring and debugging agentic workflows in production.Unlike prior approaches that rely on static benchmarks or single-pass LLM judgments, AgentCompass employs a recursive planand-execute reasoning cycle, a formal hierarchical error taxonomy tailored to business-critical issues, and a dual memory system that enables longitudinal analysis across executions.Its multi-stage analytical pipeline spans four key workflows: error identification and categorization, thematic error clustering, quantitative quality scoring, and synthesis with strategic summarization.To capture systemic risks, the framework further performs trace-level clustering via density-based analysis, surfacing recurring failure patterns that can be prioritized by engineering teams.Empirically, we validated Agent-Compass with design partners on real-world deployments and demonstrated its generality through the publicly available TRAIL benchmark, where it achieves state-of-the-art results in error localization and joint metrics while uncovering critical errors that human annotations miss, including safety and reflection gaps.Together, these contributions make AgentCompass a rigorous and practical foundation for real-time production monitoring and continual improvement of complex agentic systems.</p>
<p>Methodology</p>
<p>We propose an agentic framework for the automated analysis of traces.Our approach models the cognitive workflow of an expert human debugger, creating a structured, multi-stage analytical pipeline for data that is unstructured as traces stored in open-telemetry format.The core of our method is a recursive reasoning framework that enhances the reliability of Large Language Models for complex diagnostic tasks.This framework is augmented with a knowledge persistence layer for continual learning.Our method includes 3 core components: a multi-stage analytical pipeline, trace-level issue clustering, and knowledge persistence for continual learning.We describe each of these components in detail in this section.</p>
<p>The Multi-Stage Analytical Pipeline</p>
<p>Our method deconstructs the complex goal of trace analysis into a sequence of four distinct, progressively abstract stages.Each stage's output serves as the structured input for the next, creating a coherent analytical narrative from granular findings to a high-level strategic summary.</p>
<p>Error Identification and Categorization:</p>
<p>The initial stage performs a comprehensive scan of the entire execution trace to identify discrete errors.Each identified error is classified according to a formal, hierarchical error taxonomy, grounding the analysis in a predefined set of failure modes common to agentic systems.This taxonomy is described in detail in section 2.1.1.</p>
<p>Thematic Error Clustering:</p>
<p>The discrete errors identified in the previous stage are then subjected to a thematic analysis.The system groups individual errors into semantically coherent clusters, designed to reveal systemic issues, causal chains, or recurring patterns of failure that would not be apparent from isolated error events.</p>
<ol>
<li>Quantitative Quality Scoring: To move beyond qualitative error descriptions, this stage assesses the overall quality of the trace across several predefined dimensions (e.g., factual grounding, safety, plan execution).The system assigns a quantitative score to each dimension, providing a multi-faceted, objective measure of the agent's performance.</li>
</ol>
<p>Synthesis and Strategic Summarization:</p>
<p>The final stage synthesizes all preceding data -individual errors, thematic clusters, and quantitative scores into a final, actionable summary.This includes a single, aggregate quality score, key insights into the agent's behavior, and a recommended priority level for human intervention.</p>
<p>A Formal Taxonomy for Agentic Errors</p>
<p>Underpinning our entire methodology is a formal, hierarchical error taxonomy that provides a structured ontology for classifying failures in agentic systems.This taxonomy serves as the analytical foundation for the Error Identification stage and as a guiding schema for the agent's reasoning.We take inspiration from the taxonomy proposed in TRAIL (Deshpande et al., 2025).Our taxonomy is designed to be comprehensive in its coverage, developer-centric in its focus, and includes key business critical issues, categorizing errors in a way that maps directly to actionable interventions.</p>
<p>The taxonomy is organized into five high-level categories:</p>
<p>• Thinking &amp; Response Issues: Failures in the agent's core reasoning, such as hallucinations, misinterpretation of retrieved information, flawed decision-making, or violations of output formatting constraints.</p>
<p>• Safety &amp; Security Risks: Behaviors that could lead to harm, including the leakage of personally identifiable information (PII), the exposure of credentials, or the generation of biased or unsafe content.</p>
<p>• Tool &amp; System Failures: Errors originating from the agent's interaction with external tools or its execution environment.This includes API failures, misconfigurations, rate limits, and runtime exceptions.</p>
<p>• Workflow &amp; Task Gaps: Breakdowns in the execution of multi-step tasks, such as the loss of conversational context, goal drift, inefficient or redundant actions, and failures in task orchestration.</p>
<p>• Reflection Gaps: A meta-level category for failures in the agent's introspective or planning capabilities, such as a lack of selfcorrection after an error or taking action without evidence of reasoning (e.g., Chain-of-Thought).</p>
<p>Each category is further broken down into specific subcategories and granular error types, providing a multi-level classification schema that enables both high-level thematic analysis and fine-grained rootcause identification.Detailed taxonomy is shown in figure 1.</p>
<p>The Agentic Reasoning Framework</p>
<p>For our agentic workflows described in section 2.1 we adopt the ReAct framework (Yao et al., 2023).This framework is designed to steer and structure the inferential capabilities of LLMs.</p>
<p>The Plan-and-Execute Reasoning Cycle.At the heart of our methodology is a plan-and-execute cognitive cycle.Rather than tasking an LLM with solving a complex problem in a single step as described in TRAIL using LLM-as-a-judge, we decompose the reasoning for each pipeline stage into two distinct phases.First, in the Planning Phase, the agent uses an LLM to generate a structured, explicit strategy for the task at hand (e.g., "identify critical spans to analyze for tool-use errors").Second, in the Execution Phase, the agent provides this strategy back to the LLM as a directive for performing the actual analysis.This decomposition enforces a methodical approach, mitigates the risk of goal drift, and significantly improves the reliability and consistency of the analytical output compared to single-pass prompting.</p>
<p>Structured Representation of Trace Data.To make complex, non-linear execution traces legible to a language model, our method first transforms the raw trace data.It reconstructs the causal hierarchy of execution spans into a tree structure, which is then serialized into a numbered, hierarchical text format.This representation provides the LLM with both a high-level "outline" of the execution and detailed, step-by-step "details" for deeper inspection, effectively creating a structured document for the model to analyze.</p>
<p>Trace-level Issue Clustering via Density-Based Analysis</p>
<p>To provide developers with a higher-level understanding of recurring issues across multiple traces and over time, our methodology includes a crosstrace clustering stage.This moves beyond singletrace analysis to identify repeated failure modes that can help the developers identify and prevent it in future.Instead of simple rule-based grouping, we employ a sophisticated, unsupervised machine learning approach.</p>
<p>First, each identified error is converted into a high-dimensional vector representation using a transformer based encoder model.These embeddings are generated from a concatenation of some of the error's key semantic features.</p>
<p>Next, we apply the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm to this collection of error embeddings.We chose HDBSCAN for its ability to discover clusters of varying shapes and densities and its robustness to noise, making it well-suited for the diverse and often sparse nature of error data.The algorithm groups semantically similar error embeddings into dense clusters, effectively identifying groups of recurring, related issues.</p>
<p>Finally, to account for ambiguous cases, we employ a soft clustering technique.Errors that are initially classified as noise by HDBSCAN are probabilistically assigned to the most suitable existing cluster if their membership probability exceeds a predefined threshold.This allows the system to group peripherally related errors without forcing them into ill-fitting categories or just forming a singleton cluster, providing a more nuanced and comprehensive view of systemic issues.The resulting clusters represent high-level "issues" that can be tracked and prioritized by engineering teams in the form of 'tickets' or 'issues' which can be tracked in their agile development cycle.</p>
<p>Knowledge Persistence for Continual Learning</p>
<p>Most evaluation frameworks treat each agentic execution as independent, overlooking recurring error patterns and domain-specific nuances.In production, however, deployed agents repeatedly encounter similar edge cases, systemic errors, and domain-specific critical issues that cannot be fully anticipated at design time.By equipping our framework with memory, we enable longitudinal analysis: allowing the system to contextualize new traces with prior findings and continually refine its diagnostic heuristics, thereby improving accuracy and business relevance over time.</p>
<p>Episodic and Semantic Memory.The system maintains two distinct memory stores.Episodic Memory captures context and findings related to specific, individual traces, allowing for stateful, multi-turn analysis of a single execution.Semantic Memory, in contrast, stores generalized, cross-trace knowledge.The system abstracts highconfidence findings into durable error patterns or best practices, allowing it to recognize recurring issues across a fleet of agents and improve its diagnostic acumen over time.</p>
<p>Memory-Augmented Reasoning.Before initiating analysis on a new trace, the agent queries its memory stores for relevant prior knowledge.This retrieved context is then injected directly into the prompts used during the plan-and-execute cycle, priming the LLM with historical context and learned heuristics to guide its analysis.</p>
<p>Evaluation Study</p>
<p>To empirically validate the effectiveness of our agentic framework, we first collaborated with several customers as design partners, testing the system on real-world scenarios and validating its outputs against proprietary datasets.This early deployment phase demonstrated strong accuracy and practical utility in production-like settings.To further establish the general efficacy of our approach, we then evaluated it against the publicly available, human-annotated TRAIL benchmark.Our objectives were twofold: (i) to quantify performance in error identification and categorization against a standardized ground truth, and (ii) to qualitatively assess the framework's ability to surface valid errors overlooked during manual annotation.</p>
<p>Benchmark Dataset</p>
<p>For our evaluation, we utilized the TRAIL (Trace Reasoning and Agentic Issue Localization) benchmark (Deshpande et al., 2025), a publicly available dataset specifically designed for evaluating trace analysis systems.The dataset consists of 148 traces totaling 1,987 OpenTelemetry spans, of which 575 exhibit at least one error, sourced from two established agent benchmarks: GAIA (Mialon et al., 2023), which focuses on open-world information retrieval, and SWE-Bench (Jimenez et al., 2024), which involves software engineering tasks.Each trace in the TRAIL dataset is accompanied by a set of human-generated annotations.These annotations identify the precise location (span ID) and the specific category of each error according to the taxonomy proposed in their work.</p>
<p>Experimental Setup</p>
<p>Our experimental setup was designed to measure the agent's performance along the two primary axes defined by the TRAIL benchmark: error categorization and localization.</p>
<p>We ran our framework, with its full plan-andexecute and memory-augmented reasoning capabilities using our proprietary in-house fine-tuned Turing Large model1 , on each of the 148 traces from the TRAIL dataset.The agent's output was again then compared with human annotated ground truth.</p>
<p>Taxonomy Alignment.As our agent employs a more granular, hierarchical taxonomy (detailed in Section 2.1.1),we first established a semantic mapping to the TRAIL taxonomy to ensure a fair comparison.For example, multiple specific errors from our agent's taxonomy, such as 'Hallucinated Content' or 'Hallucinated Tool Result' were mapped to the corresponding TRAIL category of 'Languageonly'.This normalization enabled a direct and meaningful comparison of categorization accuracy.</p>
<p>From these counts, we calculated the overall Categorization F1-Score, providing a single, balanced measure of our agent's performance that is directly comparable to the results reported for other models in the TRAIL study.We also specifically isolated the set of False Positives for a subsequent qualitative analysis to identify valid errors our agent found that were not in the original benchmark.</p>
<p>Results and Analysis</p>
<p>Our evaluation demonstrates that the FAGI-AgentCompass framework achieves state-of-the-art performance on the TRAIL benchmark, significantly outperforming existing general-purpose Large Language Models on key metrics for trace debugging.The complete performance comparison is presented in Table 1.</p>
<p>Quantitative Performance</p>
<p>We report the performance on the same metrics that were used to evaluate the existing language models for the TRAIL Benchmark, Categorization F1-Score (Cat.F1), Localization Accuracy (Loc.Acc.), and the Joint score, which represents the accuracy of predicting both category and location correctly.</p>
<p>Localization Accuracy.The most significant result is our agent's superior performance in Localization Accuracy.On the TRAIL (GAIA split) dataset, FAGI-AgentCompass achieves a Localization Accuracy of 0.657, substantially outperforming the next best model, Gemini-2.5-Pro,which scored 0.546.This indicates that our agent's iterative planning and executing cycle along with memory augmentation allows it to pinpoint the precise event or set of spans when an error occured.Similarly, on the more complex TRAIL (SWE Bench Split) dataset, our agent achieves the highest Localization Accuracy of 0.250.Categorization F1-Score.In terms of Categorization F1-Score, our agent is highly competitive, achieving 0.309 on GAIA.While the Gemini-2.5-Promodel reports a higher F1 score on this specific dataset, this metric is sensitive to the inherent differences between our agent's granular, hierarchical taxonomy and the one used for the TRAIL benchmark annotations.Our primary goal was to validate our agent's performance against the established benchmark, which necessitated mapping our more detailed categories to their broader counterparts.This translation can naturally introduce discrepancies that affect the metric.Despite this, our agent was able to achieve a well balanced and top-tier performance.Furthermore, on the highly technical SWE Bench dataset, our agent again leads with the highest Categorization F1-Score of 0.232, highlighting its robustness on code-centric traces where general models often struggle.</p>
<p>Joint Performance.The Joint score, which is the strictest metric, further underscores the efficacy of our approach.Our agent sets a new state-of-the-art on both datasets, with a score of 0.239 on GAIA and 0.051 on SWE Bench.This demonstrates that the agent's ability to localize errors is not at the expense of categorization accuracy.The plan-andexecute cycle, which first identifies critical spans and then analyzes them for specific error types, leads to a higher probability of getting both the location and category correct simultaneously.</p>
<p>Score Correlation.The Pearson correlation coefficient (ρ), which measures the correlation between our agent's and human's overall quality scores for a trace, provides further insight into its analytical behavior.FAGI-AgentCompass reports ρ scores of 0.430 and 0.408 on GAIA and SWE Bench datasets respectively.Our framework employs a more systematic error taxonomy and broader evaluation criteria than those defined in the benchmark, which naturally leads to moderate (rather than high) correlation with human scores while capturing a richer spectrum of errors.Additionally, a deeper analysis reveals that our agent identified errors that were missed by human judgment.For example as shown in figure 6 in table 2 one of the categories in our taxonomy that the original benchmark study completely misses is 'Safety and Security Alerts' responsible for identifying outputs that may potentially cause harm, leak personal data or violate best security practices.When our agent detects additional, valid errors, it correctly assigns a lower overall quality score to the trace.This necessary and more accurate assessment naturally diverges from the human score, which was based on an incomplete view of the trace's failures.Therefore, a moderate correlation is an expected and even desirable artifact of a system designed to be more rigorous and developercentric than a manual annotation process using a limited taxonomy.Our agent's scoring is a function of the comprehensive set of issues it uncovers, not just a replication of a human's holistic impression.</p>
<p>Qualitative Analysis: Uncovering Novel Errors</p>
<p>Beyond quantitative metrics, a key finding of our study is the agent's ability to identify valid errors that were not included in the original humanannotated ground truth.These findings correspond to the False Positives (FP) in our evaluation metric, and a manual review revealed that a significant portion of these were not erroneous agent predictions, but rather novel, valid insights.We show a few samples in table 2. For instance, in one of the traces belonging to the benchmark dataset, the agent identified a 'Tool Selection Error' because the plan failed to specify which tool should be used to access external information from the cor-Trace Ground Truth Analysis Summary (from TRAIL)</p>
<p>FAGI-AgentCompass Analysis Summary</p>
<p>Figure 2 Identifies a Tool Selection Error, noting the agent failed to call the search_agent and instead printed a task string.</p>
<p>Correctly identifies the same tool misuse but also uncovers a Hallucination where the agent invents a final answer, and a Lack of Self-Correction as it repeatedly fails to use the tool correctly before fabricating the result.</p>
<p>Figure 3</p>
<p>Flags multiple Formatting Errors where the agent repeatedly calls the page_down tool with incorrect parameters.</p>
<p>Identifies the same formatting errors but provides a much deeper diagnosis.It flags a critical Lack of Self-Correction, as the agent ignores explicit error messages for 10 consecutive steps, and notes that this violates existing knowledge in its Semantic Memory, indicating a systemic failure to learn.</p>
<p>Figure 4</p>
<p>Notes a Poor Information Retrieval error, as the agent's web search returned an irrelevant result.</p>
<p>Pinpoints the sub-agent's tool failures ('Invalid Tool Params', 'Lack of Self-Correction') that caused the retrieval to fail.More critically, it uncovers the parent agent's subsequent Goal Drift and severe Hallucination, where it fabricates a numerical answer after receiving the sub-agent's failure report.</p>
<p>Figure 5</p>
<p>Identifies a Tool Selection Error, as the agent hallucinates a Tropicos ID instead of searching for it.</p>
<p>Correctly identifies the hallucination but also diagnoses a Task Orchestration Failure and a Missing ReAct Planning error.Our agent astutely notes the root cause: a failure to execute its own valid plan, which is a critical breakdown between the agent's reasoning and its subsequent actions.</p>
<p>Figure 6</p>
<p>Identifies an Instruction Non-compliance error because the agent prints 1000 characters of a file when the limit was 500.</p>
<p>Our agent identifies the same violation but classifies it with much higher severity as a Safety &amp; Security Risks &gt; Data Exposure error.It correctly reasons that violating an explicit negative constraint about data handling is a security failure, not just a formatting mistake, providing a more critical and accurate diagnosis.</p>
<p>Table 2: Qualitative comparison of error analysis on select traces from the TRAIL benchmark.The FAGI-AgentCompass consistently provides a deeper, more actionable root-cause analysis by identifying planning and meta-level reasoning failures that supplement the ground truth annotations.</p>
<p>responding database.While the human annotators did not label this as a distinct error, it represents a critical failure in agentic planning that leads to downstream hallucinations.Our agent's structured analysis of the entire reasoning chain, not just the final tool call, allows it to detect these subtle but crucial planning-phase failures that will potentially benefit the AI developers who might miss the underlying cause that triggers the issue in their agentic workflows.</p>
<p>As established in our analysis of the score correlation, our agent's comprehensive taxonomy allows it to identify critical issues missed during manual annotation, such as the 'Safety &amp; Security Risks' previously discussed.</p>
<p>In addition to security, our agent demonstrated a strong capability for identifying subtle failures in the agent's cognitive processes, a crucial area for modern reasoning-based systems.Our taxonomy includes a dedicated category for 'Reflection Gaps', which covers failures in an agent's introspective and planning capabilities.For example, in several complex traces, our agent flagged a 'Lack of Self-Correction'.It precisely identified moments where a sub-agent, after receiving an error from a tool, would immediately retry the exact same failed action without modifying its parameters or approach.This type of recursive, inefficient behavior is a significant flaw in agentic logic that a less granular analysis might miss.By identifying not just the direct tool failure but the meta-level reasoning failure that caused it, our framework provides a more profound and actionable diagnosis for developers seeking to build more robust and intelligent agents.Our framework not only identifies these errors, but also suggests actionable fixes to improve the system.Figures 7 and 8 show representative examples where the agent produces prescriptive "Fix Recipes" that map detected errors to targeted remediation strategies, such as adjusting tool invocation parameters, refining retrieval prompts, or restructuring orchestration logic.These recommendations provide developers with concrete next steps, reducing debugging effort and accelerating the de-</p>
<p>Conclusion</p>
<p>In this work, we introduced AgentCompass, a memory-augmented evaluation framework for diagnosing and improving agentic workflows postdeployment.By combining a structured error taxonomy, multi-stage analytical reasoning, tracelevel clustering, and persistent memory, our system provides deeper and more actionable insights than existing evaluation approaches.Our empirical study demonstrates strong accuracy in real-world deployments and state-of-the-art performance on the TRAIL benchmark, including the ability to uncover valid errors overlooked by human annotation.These findings highlight that moderate correlation with human judgments reflects not a shortcoming but a more rigorous, systematic evaluation process that captures the full spectrum of agentic failures.As organizations scale their use of multi-agent AI systems, AgentCompass offers a path toward trustworthy, production-grade monitoring and debugging, bridging the gap between technical benchmarks and the realities of enterprise deployment.</p>
<p>Figure 1 :
1
Figure 1: Detailed error taxonomy used in AgentCompass</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Execution trace for scenario 1</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Execution trace for scenario 3</p>
<p>Figure 6 :
6
Figure 6: Execution trace for scenario 4</p>
<p>Table 1 :
1
Performance across LLMs for Error Categorization &amp; Localization on TRAIL Benchmark.Note: Performance values for models other than FAGI-AgentCompass have been taken from Deshpande et al. (2025).
TRAIL (GAIA)TRAIL (SWE Bench)
https://docs.futureagi.com/future-agi/ get-started/evaluation/future-agi-models</p>
<p>Small language models are the future of agentic ai. Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, arXiv:2506.021532025PreprintYingyan Celine Lin, and Pavlo Molchanov</p>
<p>Murakkab: Resourceefficient agentic workflow orchestration in cloud platforms. Esha Gohar Irfan Chaudhry, Haoran Choukse, Íñigo Qiu, Rodrigo Goiri, Adam Fonseca, Ricardo Belay, Bianchini, arXiv:2508.182982025Preprint</p>
<p>Trail: Trace reasoning and agentic issue localization. Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, Rebecca Qian, arXiv:2505.086382025Preprint</p>
<p>Agentic ai market boosts by performing self actions grows by usd 196.6 billion by 2034, region at usd 1.58 billion. Press release. EIN Presswire / Market.us. 2025. February 4, 2025</p>
<p>Kexin Pei, Ofir Press, and Karthik Narasimhan. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues? Preprint. 2024</p>
<p>The real barrier to llm agent usability is agentic roi. Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, Weinan Zhang, arXiv:2505.177672025Preprint</p>
<p>The measurement imbalance in agentic ai evaluation undermines industry productivity claims. Kiana Jafari Meimandi, Gabriela Aránguiz-Dias, Grace Ra Kim, Lana Saadeddin, Mykel J Kochenderfer, arXiv:2506.020642025Preprint</p>
<p>Gaia: a benchmark for general ai assistants. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann Lecun, Thomas Scialom, arXiv:2311.129832023Preprint</p>
<p>Atsunori Moteki, Shoichi Masui, Fan Yang, Yueqi Song, Yonatan Bisk, Graham Neubig, Ikuo Kusajima, Yasuto Watanabe, Hiroyuki Ishida, Jun Takahashi, Shan Jiang, arXiv:2505.19662Fieldworkarena: Agentic ai benchmark for real field work tasks. 2025Preprint</p>
<p>2025 agentic ai roi survey results. Pagerduty, 2025</p>
<p>. Accessed, </p>
<p>Trism for agentic ai: A review of trust, risk, and security management in Figure 7: Recommended fixes by AgentCompass, example 1 Figure 8: Recommended fixes by AgentCompass, example 2 llm-based agentic multi-agent systems. Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis, arXiv:2506.041332025Preprint</p>
<p>Adaptive monitoring and realworld evaluation of agentic ai systems. Manish Shukla, arXiv:2509.001152025Preprint</p>
<p>Adaptive ai agent placement and migration in edge intelligence systems. Xingdan Wang, Jiayi He, Zhiqing Tang, Jianxiong Guo, Jiong Lou, Liping Qian, Tian Wang, Weijia Jia, arXiv:2508.033452025Preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023Preprint</p>
<p>Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, Michal Shmueli-Scheuer, arXiv:2503.16416Survey on evaluation of llmbased agents. 2025Preprint</p>
<p>A survey on agent workflow -status and future. Chaojia Yu, Zihan Cheng, Hanwen Cui, Yishuo Gao, Zexu Luo, Yijin Wang, Hangbin Zheng, Yong Zhao, 10.1109/icaibd64986.2025.110820762025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD). IEEE2025</p>            </div>
        </div>

    </div>
</body>
</html>
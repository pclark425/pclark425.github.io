<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-539 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-539</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-539</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-ada81a4de88a6ce474df2e2446ad11fea480616e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e" target="_blank">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people.</p>
                <p><strong>Paper Abstract:</strong> Large pretrained (e.g.,"foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e539.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e539.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socratic Models (SMs) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular zero-shot composition framework that uses language as the intermediate representation to connect pretrained models (LMs, VLMs, ALMs) and external APIs so they can exchange information and perform multimodal tasks without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (as the primary LM in examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive language model used off-the-shelf to reason over language prompts, generate candidate activities/captions/plans, and produce natural-language or pseudocode steps for downstream modules; used via prompting (zero-/few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General multimodal embodied reasoning (framework-level)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Framework-level task: compose pretrained VLMs/ALMs and LMs through language-based prompts to enable downstream applications such as egocentric video Q&A, multimodal assistive dialogue, video retrieval, and robot perception & planning, without training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / instruction following / perception-to-reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + (high-level) spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (LM); sensory modules (VLM/ALM) provide grounded observations which are converted to language for the LM</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting, few-shot prompting, structured multimodal prompt substitution (variable substitution), recursive Socratic exchanges between models</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>natural-language descriptions (prompts, event logs, action steps, pseudocode); LMs store commonsense/procedural knowledge implicitly in weights and expose it as language; edges carry language between modules</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-dependent metrics reported in applications (e.g., CIDEr for captioning, R@1 for video-to-text retrieval); framework evaluated by downstream task improvements when LMs are composed with VLMs/ALMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Varies by application (see specific entries). Framework-level claim: enabling new multimodal capabilities zero-shot, often improving over VLM-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables high-level procedural reasoning (multi-step plans and pseudocode), commonsense activity inference from lists of objects/places, summarization and QA over language event logs; improves multimodal retrieval when LM summarizes transcripts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Does not provide low-level geometric/sensorimotor grounding by itself; quality depends on accuracy of upstream VLM/ALM; LMs may hallucinate, produce biased or temporally imprecise answers, and miss fine visual attributes (colors, precise spatial coordinates).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared applications against task-specific baselines (e.g., CLIP-only baselines, finetuned captioning models). Improvements depend on combining LM outputs with VLM/ALM signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper reports ablations at application level (e.g., CLIP-only vs CLIP+LM+ALM for video retrieval; VLM+LM vs LM-only for contextual captioning). See application-specific entries for numeric effects.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language models encode rich procedural and object-relational commonsense in their weights that can be elicited via prompts; when fed language descriptions (instead of raw sensory input) LMs can perform high-level embodied reasoning (plans, activity inferences, Q&A, forecasting), but they lack precise low-level spatial/geometric representations and rely on VLMs/ALMs for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e539.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e539.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World-State History</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-based world-state history (event log)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporally ordered, concatenated sequence of natural-language summaries (captions) of key video frames serving as a textual representation of observed events for downstream LM reasoning (Q&A, forecasting, summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (LM) used to summarize and reason over the event log; CLIP / other VLMs produce frame summaries</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3); VLM sizes vary (e.g., CLIP ViT-* reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM consumes concatenated language summaries (generated by VLM+LM Socratic interaction) as context; it performs reading-comprehension-style reasoning (Q&A, temporal/cause inference, forecasting) purely from text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Egocentric video Q&A, summarization, and forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert egocentric video into a language-based event log (timestamped frame summaries) so an LM can answer free-form questions (who/what/when/why), summarize activities, and predict likely next events without accessing raw sensory data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction following / multi-step reasoning / temporal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + temporal (and some spatial high-level)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>VLM outputs converted to language; LM priors from pre-training on large text corpora enable commonsense inferences</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting to generate frame captions, recursive summarization, then prompting LM with '[world-state history] Q: {question} A:'</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit natural-language event log (textual state history); LM encodes and reasons with that text (implicit in weights plus context window)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative demonstrations (open-ended Q&A answers, summaries); no single numeric benchmark provided for this pipeline in paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative: LM produced meaningful recall, temporal, causal answers and forecasting when given the world-state history; performance depends on completeness and accuracy of VLM-generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>LMs reliably answer high-level recall and causal questions, count or temporal queries when the event log contains relevant entries, and produce coherent summaries/forecasts based on typical schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Errors when VLM summaries are incomplete or incorrect; LMs can hallucinate details, provide temporally imprecise answers or be biased by typical schedules from pretraining; fine-grained spatial relations and low-level sensorimotor detail are often missing.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not evaluated against trained video-QA models on standard benchmarks in paper; compared qualitatively to approaches that require domain-specific egocentric training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Authors note that quality improves with better VLMs and with including audio/text signals; unsupervised selection experiments (appendix) correlate VLM quality with downstream similarity to an sLM-generated pseudo-ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A textual event-log is an effective intermediate representation that allows LMs (operating without direct sensory input) to perform open-ended embodied reasoning (Q&A, summary, forecasting); however, the approach trades off low-level spatial/metric fidelity for linguistic interpretability and relies on upstream perceptual modules for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e539.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e539.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-based multi-step planner for robot manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an LM to convert VLM-detected objects and user instructions into multi-step natural-language or pseudocode plans which are then executed by pretrained language-conditioned robot policies (e.g., CLIPort-style policies).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (LM) interacting with ViLD (open-vocabulary VLM) and a pretrained language-conditioned robot policy (e.g., CLIPort-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (GPT-3); VLM/detector sizes per cited models (ViLD) - not all sizes listed</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM is prompted with VLM-produced object lists and high-level instructions; it generates step-by-step natural language or pseudocode commands (e.g., robot.pick_and_place("red block","blue block")) which are then passed to a pretrained policy that maps language tokens to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulated multi-step pick-and-place planning (UR5 + objects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a scene and a free-form human instruction (e.g., rearrange blocks, stack, undo), the LM generates a sequence of discrete manipulation commands (natural language or pseudocode) that a pretrained language-conditioned policy executes in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / multi-step planning / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + high-level spatial relations (e.g., 'on top', 'middle', 'top left corner')</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LM pretraining on large text corpora (commonsense procedural knowledge) + VLM detections for perceptual grounding; in-context prompt templates used to shape outputs</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting and code/pseudocode generation (LM writes executable-like strings), variable-substitution from VLM detections, chaining LM outputs to robot policy</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>action sequences represented as natural language steps or pseudocode strings; LM knowledge is implicit in weights and realized as generated textual plans</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative demonstration in simulation; no numeric success rates reported in paper for robot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Demonstrated generation of coherent multi-step plans that map to policy primitives; no quantitative task completion numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>LM generates semantically valid sequences of pick-and-place steps, can interpret free-form human requests and produce ordered commands, and leverage object-relational descriptions (e.g., object names, colors) to reference items.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LM does not provide precise continuous spatial coordinates or collision-aware trajectories; relies on the language-conditioned policy to map high-level commands to low-level motions; errors arise if VLM mislabels objects or LM misnames targets.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared quantitatively to end-to-end trained planners; compared conceptually to other works that ground language to affordances ([7] referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No formal ablation; paper highlights that chaining LM generation with a pretrained policy expands the policy's task repertoire without retraining but leaves geometric grounding to the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LMs can synthesize multi-step procedural plans and encode object-relational relationships in language which, when combined with language-conditioned robot policies, enable new manipulation behaviors zero-shot; however, LMs do not replace sensorimotor controllers and lack metric spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e539.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e539.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-summary-for-retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM summarization of ASR transcripts to augment video-to-text retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM summarizes speech-to-text transcripts (from an ALM/ASR) into short captions; these LM-generated summaries are used (via masked-LM sentence similarity) to re-weight CLIP-based video-caption rankings, improving zero-shot video-to-text retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (summarizer) + RoBERTa (similarity) + CLIP (visual-text similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3: 175B; RoBERTa: ~355M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 summarizes ASR transcripts using a short prompt; RoBERTa computes semantic similarity between the LM summary and candidate captions; CLIP provides visual-text similarity; scores are combined multiplicatively to re-rank captions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Video-to-text retrieval on MSR-VTT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a video query, retrieve the correct textual caption from a pool of captions (many-to-one); system uses CLIP visual-text similarity augmented with LM summaries of speech transcripts to re-weight rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>video understanding / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural (narrative of speech) + object-relational (entities mentioned in speech) + temporal context</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>ASR transcripts (ALM) and LM priors from pretraining; no finetuning on MSR-VTT (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (GPT-3 summary of ASR transcript), then embedding similarity (RoBERTa) combined with CLIP scores</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>LM-generated short textual summary of spoken content used as an intermediate semantic descriptor; vector embeddings used for matching</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Retrieval: Recall@1 (R@1), R@5, R@10, Median rank (MdR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot: CLIP-only baseline (Portillo-Quintero et al.) R@1 = 40.3; SMs (CLIP + ASR + GPT-3 + RoBERTa) R@1 = 44.7 on MSR-VTT full. On long-transcript subset: CLIP 41.5 -> SMs 54.9 R@1.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When videos contain substantive speech, LM summaries add discriminative semantic cues that significantly improve retrieval rankings (notably on long-transcript subset).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>No audio or short transcripts default to CLIP-only performance; improvements depend on quality of ASR and relevance of spoken content to captions; LM summarization can be noisy or omit visual details.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline: CLIP-only retrieval (Portillo-Quintero et al.) R@1 = 40.3; finetuned SOTA CLIP2Video reaches much higher R@1 (~54.6) but requires training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Adding ASR+GPT-3+RoBERTa re-weighting boosts R@1 from 40.3 to 44.7 overall and from 41.5 to 54.9 on long-transcript subset, showing LM summaries are the critical added component.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LMs can extract complementary semantic information from transcripts that VLM-only systems miss; when operating on language (transcripts) rather than raw audio, LMs improve retrieval, especially where speech content is informative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e539.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e539.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-activity-inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM as commonsense activity generator from object/place lists</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LM is prompted with VLM-produced lists of places and objects to generate candidate activities (commonsense inferences); those candidate activities are fed back to the VLM for re-ranking, producing likely activity labels for images/frames.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (LM) with CLIP or ViLD (VLM) for re-ranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3: 175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM produces open-vocabulary, commonsense activity descriptions (e.g., 'receiving a package') conditioned on place and object lists; VLM re-ranks candidate activities by visual relevance, creating an iterative Socratic loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Egocentric activity recognition / captioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a frame's detected place and objects, generate candidate activities and pick the most likely visually grounded activity using VLM re-ranking; produce a human-readable caption for the frame.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>activity recognition / captioning / object-relational inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + procedural (implicit activity sequences) + contextual/spatial (high-level place semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LM pretraining on text corpora (commonsense scripts) and VLM perceptual outputs; iterative prompting (LM -> VLM -> LM) supplies context</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting to generate activities, followed by VLM-based re-ranking of LM-generated candidates</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>LM-internal commonsense encoded implicitly in weights exposed as language strings (activity labels); the pipeline represents relations as textual triples (Places:, Objects:, Activities:).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative captioning and downstream benefits (e.g., improved contextual caption metrics on Concadia and more informative egocentric summaries); CIDEr reported for caption tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Concadia contextual captioning: SMs (VLM+LM) zero-shot CIDEr = 38.9; LM-only (no image) CIDEr = 40.1, showing LMs can generate plausible contextual captions from article text alone and can suggest activities from object/place context.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>LMs produce expressive, open-vocabulary activities beyond standard activity dataset classes (useful for egocentric domains), enable better contextual captions and event descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LM suggestions depend on quality of place/object lists; VLM mis-detections propagate to activity inference; LMs may output typical/biased activities not present in frame if context is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to Kreiss et al. (finetuned task-specific method) CIDEr 11.3; SMs 38.9 (zero-shot) substantially outperformed that baseline on Concadia.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Noted interesting ablation: LM-only (no image) achieved CIDEr 40.1 (slightly above SMs with image), indicating strong prior/statistical correlations in the LM; underscores that LM priors can dominate in contextual captioning when article text is informative.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LMs encode rich object-activity associations and can be prompted to generate plausible procedural/activity descriptions from object/place lists; these object-relational priors are usable for embodied reasoning when visual grounding is provided or even when images are absent, but over-reliance on LM priors can introduce dataset-overlap or bias artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Do as i can and not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 1)</em></li>
                <li>ClipCap: Clip prefix for image captioning <em>(Rating: 1)</em></li>
                <li>A straightforward framework for video retrieval using clip <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-539",
    "paper_id": "paper-ada81a4de88a6ce474df2e2446ad11fea480616e",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "SMs",
            "name_full": "Socratic Models (SMs) framework",
            "brief_description": "A modular zero-shot composition framework that uses language as the intermediate representation to connect pretrained models (LMs, VLMs, ALMs) and external APIs so they can exchange information and perform multimodal tasks without finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (as the primary LM in examples)",
            "model_size": "175B",
            "model_description": "Large autoregressive language model used off-the-shelf to reason over language prompts, generate candidate activities/captions/plans, and produce natural-language or pseudocode steps for downstream modules; used via prompting (zero-/few-shot).",
            "task_name": "General multimodal embodied reasoning (framework-level)",
            "task_description": "Framework-level task: compose pretrained VLMs/ALMs and LMs through language-based prompts to enable downstream applications such as egocentric video Q&A, multimodal assistive dialogue, video retrieval, and robot perception & planning, without training.",
            "task_type": "multi-step planning / instruction following / perception-to-reasoning",
            "knowledge_type": "procedural + object-relational + (high-level) spatial",
            "knowledge_source": "pre-training on large text corpora (LM); sensory modules (VLM/ALM) provide grounded observations which are converted to language for the LM",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting, few-shot prompting, structured multimodal prompt substitution (variable substitution), recursive Socratic exchanges between models",
            "knowledge_representation": "natural-language descriptions (prompts, event logs, action steps, pseudocode); LMs store commonsense/procedural knowledge implicitly in weights and expose it as language; edges carry language between modules",
            "performance_metric": "Task-dependent metrics reported in applications (e.g., CIDEr for captioning, R@1 for video-to-text retrieval); framework evaluated by downstream task improvements when LMs are composed with VLMs/ALMs",
            "performance_result": "Varies by application (see specific entries). Framework-level claim: enabling new multimodal capabilities zero-shot, often improving over VLM-only baselines.",
            "success_patterns": "Enables high-level procedural reasoning (multi-step plans and pseudocode), commonsense activity inference from lists of objects/places, summarization and QA over language event logs; improves multimodal retrieval when LM summarizes transcripts.",
            "failure_patterns": "Does not provide low-level geometric/sensorimotor grounding by itself; quality depends on accuracy of upstream VLM/ALM; LMs may hallucinate, produce biased or temporally imprecise answers, and miss fine visual attributes (colors, precise spatial coordinates).",
            "baseline_comparison": "Compared applications against task-specific baselines (e.g., CLIP-only baselines, finetuned captioning models). Improvements depend on combining LM outputs with VLM/ALM signals.",
            "ablation_results": "Paper reports ablations at application level (e.g., CLIP-only vs CLIP+LM+ALM for video retrieval; VLM+LM vs LM-only for contextual captioning). See application-specific entries for numeric effects.",
            "key_findings": "Language models encode rich procedural and object-relational commonsense in their weights that can be elicited via prompts; when fed language descriptions (instead of raw sensory input) LMs can perform high-level embodied reasoning (plans, activity inferences, Q&A, forecasting), but they lack precise low-level spatial/geometric representations and rely on VLMs/ALMs for grounding.",
            "uuid": "e539.0",
            "source_info": {
                "paper_title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "World-State History",
            "name_full": "Language-based world-state history (event log)",
            "brief_description": "A temporally ordered, concatenated sequence of natural-language summaries (captions) of key video frames serving as a textual representation of observed events for downstream LM reasoning (Q&A, forecasting, summarization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (LM) used to summarize and reason over the event log; CLIP / other VLMs produce frame summaries",
            "model_size": "175B (GPT-3); VLM sizes vary (e.g., CLIP ViT-* reported)",
            "model_description": "LM consumes concatenated language summaries (generated by VLM+LM Socratic interaction) as context; it performs reading-comprehension-style reasoning (Q&A, temporal/cause inference, forecasting) purely from text.",
            "task_name": "Egocentric video Q&A, summarization, and forecasting",
            "task_description": "Convert egocentric video into a language-based event log (timestamped frame summaries) so an LM can answer free-form questions (who/what/when/why), summarize activities, and predict likely next events without accessing raw sensory data.",
            "task_type": "instruction following / multi-step reasoning / temporal reasoning",
            "knowledge_type": "procedural + object-relational + temporal (and some spatial high-level)",
            "knowledge_source": "VLM outputs converted to language; LM priors from pre-training on large text corpora enable commonsense inferences",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting to generate frame captions, recursive summarization, then prompting LM with '[world-state history] Q: {question} A:'",
            "knowledge_representation": "explicit natural-language event log (textual state history); LM encodes and reasons with that text (implicit in weights plus context window)",
            "performance_metric": "Qualitative demonstrations (open-ended Q&A answers, summaries); no single numeric benchmark provided for this pipeline in paper",
            "performance_result": "Qualitative: LM produced meaningful recall, temporal, causal answers and forecasting when given the world-state history; performance depends on completeness and accuracy of VLM-generated summaries.",
            "success_patterns": "LMs reliably answer high-level recall and causal questions, count or temporal queries when the event log contains relevant entries, and produce coherent summaries/forecasts based on typical schedules.",
            "failure_patterns": "Errors when VLM summaries are incomplete or incorrect; LMs can hallucinate details, provide temporally imprecise answers or be biased by typical schedules from pretraining; fine-grained spatial relations and low-level sensorimotor detail are often missing.",
            "baseline_comparison": "Not evaluated against trained video-QA models on standard benchmarks in paper; compared qualitatively to approaches that require domain-specific egocentric training.",
            "ablation_results": "Authors note that quality improves with better VLMs and with including audio/text signals; unsupervised selection experiments (appendix) correlate VLM quality with downstream similarity to an sLM-generated pseudo-ground truth.",
            "key_findings": "A textual event-log is an effective intermediate representation that allows LMs (operating without direct sensory input) to perform open-ended embodied reasoning (Q&A, summary, forecasting); however, the approach trades off low-level spatial/metric fidelity for linguistic interpretability and relies on upstream perceptual modules for grounding.",
            "uuid": "e539.1",
            "source_info": {
                "paper_title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "LM-planner",
            "name_full": "LM-based multi-step planner for robot manipulation",
            "brief_description": "Using an LM to convert VLM-detected objects and user instructions into multi-step natural-language or pseudocode plans which are then executed by pretrained language-conditioned robot policies (e.g., CLIPort-style policies).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (LM) interacting with ViLD (open-vocabulary VLM) and a pretrained language-conditioned robot policy (e.g., CLIPort-like)",
            "model_size": "175B (GPT-3); VLM/detector sizes per cited models (ViLD) - not all sizes listed",
            "model_description": "LM is prompted with VLM-produced object lists and high-level instructions; it generates step-by-step natural language or pseudocode commands (e.g., robot.pick_and_place(\"red block\",\"blue block\")) which are then passed to a pretrained policy that maps language tokens to actions.",
            "task_name": "Simulated multi-step pick-and-place planning (UR5 + objects)",
            "task_description": "Given a scene and a free-form human instruction (e.g., rearrange blocks, stack, undo), the LM generates a sequence of discrete manipulation commands (natural language or pseudocode) that a pretrained language-conditioned policy executes in simulation.",
            "task_type": "object manipulation / multi-step planning / instruction following",
            "knowledge_type": "procedural + object-relational + high-level spatial relations (e.g., 'on top', 'middle', 'top left corner')",
            "knowledge_source": "LM pretraining on large text corpora (commonsense procedural knowledge) + VLM detections for perceptual grounding; in-context prompt templates used to shape outputs",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting and code/pseudocode generation (LM writes executable-like strings), variable-substitution from VLM detections, chaining LM outputs to robot policy",
            "knowledge_representation": "action sequences represented as natural language steps or pseudocode strings; LM knowledge is implicit in weights and realized as generated textual plans",
            "performance_metric": "Qualitative demonstration in simulation; no numeric success rates reported in paper for robot tasks",
            "performance_result": "Demonstrated generation of coherent multi-step plans that map to policy primitives; no quantitative task completion numbers reported.",
            "success_patterns": "LM generates semantically valid sequences of pick-and-place steps, can interpret free-form human requests and produce ordered commands, and leverage object-relational descriptions (e.g., object names, colors) to reference items.",
            "failure_patterns": "LM does not provide precise continuous spatial coordinates or collision-aware trajectories; relies on the language-conditioned policy to map high-level commands to low-level motions; errors arise if VLM mislabels objects or LM misnames targets.",
            "baseline_comparison": "Not compared quantitatively to end-to-end trained planners; compared conceptually to other works that ground language to affordances ([7] referenced).",
            "ablation_results": "No formal ablation; paper highlights that chaining LM generation with a pretrained policy expands the policy's task repertoire without retraining but leaves geometric grounding to the policy.",
            "key_findings": "LMs can synthesize multi-step procedural plans and encode object-relational relationships in language which, when combined with language-conditioned robot policies, enable new manipulation behaviors zero-shot; however, LMs do not replace sensorimotor controllers and lack metric spatial representations.",
            "uuid": "e539.2",
            "source_info": {
                "paper_title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "LM-summary-for-retrieval",
            "name_full": "LM summarization of ASR transcripts to augment video-to-text retrieval",
            "brief_description": "An LM summarizes speech-to-text transcripts (from an ALM/ASR) into short captions; these LM-generated summaries are used (via masked-LM sentence similarity) to re-weight CLIP-based video-caption rankings, improving zero-shot video-to-text retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (summarizer) + RoBERTa (similarity) + CLIP (visual-text similarity)",
            "model_size": "GPT-3: 175B; RoBERTa: ~355M",
            "model_description": "GPT-3 summarizes ASR transcripts using a short prompt; RoBERTa computes semantic similarity between the LM summary and candidate captions; CLIP provides visual-text similarity; scores are combined multiplicatively to re-rank captions.",
            "task_name": "Video-to-text retrieval on MSR-VTT",
            "task_description": "Given a video query, retrieve the correct textual caption from a pool of captions (many-to-one); system uses CLIP visual-text similarity augmented with LM summaries of speech transcripts to re-weight rankings.",
            "task_type": "video understanding / retrieval",
            "knowledge_type": "procedural (narrative of speech) + object-relational (entities mentioned in speech) + temporal context",
            "knowledge_source": "ASR transcripts (ALM) and LM priors from pretraining; no finetuning on MSR-VTT (zero-shot)",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting (GPT-3 summary of ASR transcript), then embedding similarity (RoBERTa) combined with CLIP scores",
            "knowledge_representation": "LM-generated short textual summary of spoken content used as an intermediate semantic descriptor; vector embeddings used for matching",
            "performance_metric": "Retrieval: Recall@1 (R@1), R@5, R@10, Median rank (MdR)",
            "performance_result": "Zero-shot: CLIP-only baseline (Portillo-Quintero et al.) R@1 = 40.3; SMs (CLIP + ASR + GPT-3 + RoBERTa) R@1 = 44.7 on MSR-VTT full. On long-transcript subset: CLIP 41.5 -&gt; SMs 54.9 R@1.",
            "success_patterns": "When videos contain substantive speech, LM summaries add discriminative semantic cues that significantly improve retrieval rankings (notably on long-transcript subset).",
            "failure_patterns": "No audio or short transcripts default to CLIP-only performance; improvements depend on quality of ASR and relevance of spoken content to captions; LM summarization can be noisy or omit visual details.",
            "baseline_comparison": "Baseline: CLIP-only retrieval (Portillo-Quintero et al.) R@1 = 40.3; finetuned SOTA CLIP2Video reaches much higher R@1 (~54.6) but requires training.",
            "ablation_results": "Adding ASR+GPT-3+RoBERTa re-weighting boosts R@1 from 40.3 to 44.7 overall and from 41.5 to 54.9 on long-transcript subset, showing LM summaries are the critical added component.",
            "key_findings": "LMs can extract complementary semantic information from transcripts that VLM-only systems miss; when operating on language (transcripts) rather than raw audio, LMs improve retrieval, especially where speech content is informative.",
            "uuid": "e539.3",
            "source_info": {
                "paper_title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "LM-activity-inference",
            "name_full": "LM as commonsense activity generator from object/place lists",
            "brief_description": "An LM is prompted with VLM-produced lists of places and objects to generate candidate activities (commonsense inferences); those candidate activities are fed back to the VLM for re-ranking, producing likely activity labels for images/frames.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (LM) with CLIP or ViLD (VLM) for re-ranking",
            "model_size": "GPT-3: 175B",
            "model_description": "LM produces open-vocabulary, commonsense activity descriptions (e.g., 'receiving a package') conditioned on place and object lists; VLM re-ranks candidate activities by visual relevance, creating an iterative Socratic loop.",
            "task_name": "Egocentric activity recognition / captioning",
            "task_description": "Given a frame's detected place and objects, generate candidate activities and pick the most likely visually grounded activity using VLM re-ranking; produce a human-readable caption for the frame.",
            "task_type": "activity recognition / captioning / object-relational inference",
            "knowledge_type": "object-relational + procedural (implicit activity sequences) + contextual/spatial (high-level place semantics)",
            "knowledge_source": "LM pretraining on text corpora (commonsense scripts) and VLM perceptual outputs; iterative prompting (LM -&gt; VLM -&gt; LM) supplies context",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting to generate activities, followed by VLM-based re-ranking of LM-generated candidates",
            "knowledge_representation": "LM-internal commonsense encoded implicitly in weights exposed as language strings (activity labels); the pipeline represents relations as textual triples (Places:, Objects:, Activities:).",
            "performance_metric": "Qualitative captioning and downstream benefits (e.g., improved contextual caption metrics on Concadia and more informative egocentric summaries); CIDEr reported for caption tasks.",
            "performance_result": "Concadia contextual captioning: SMs (VLM+LM) zero-shot CIDEr = 38.9; LM-only (no image) CIDEr = 40.1, showing LMs can generate plausible contextual captions from article text alone and can suggest activities from object/place context.",
            "success_patterns": "LMs produce expressive, open-vocabulary activities beyond standard activity dataset classes (useful for egocentric domains), enable better contextual captions and event descriptions.",
            "failure_patterns": "LM suggestions depend on quality of place/object lists; VLM mis-detections propagate to activity inference; LMs may output typical/biased activities not present in frame if context is ambiguous.",
            "baseline_comparison": "Compared to Kreiss et al. (finetuned task-specific method) CIDEr 11.3; SMs 38.9 (zero-shot) substantially outperformed that baseline on Concadia.",
            "ablation_results": "Noted interesting ablation: LM-only (no image) achieved CIDEr 40.1 (slightly above SMs with image), indicating strong prior/statistical correlations in the LM; underscores that LM priors can dominate in contextual captioning when article text is informative.",
            "key_findings": "LMs encode rich object-activity associations and can be prompted to generate plausible procedural/activity descriptions from object/place lists; these object-relational priors are usable for embodied reasoning when visual grounding is provided or even when images are absent, but over-reliance on LM priors can introduce dataset-overlap or bias artifacts.",
            "uuid": "e539.4",
            "source_info": {
                "paper_title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Do as i can and not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 1
        },
        {
            "paper_title": "ClipCap: Clip prefix for image captioning",
            "rating": 1
        },
        {
            "paper_title": "A straightforward framework for video retrieval using clip",
            "rating": 1
        }
    ],
    "cost": 0.01945375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</h1>
<p>Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence<br>Google</p>
<h4>Abstract</h4>
<p>Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodalinformed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.</p>
<h2>1 Introduction</h2>
<p>Large pretrained models (e.g., BERT [1], GPT-3 [2], CLIP [3]) have enabled impressive capabilities [4]: from zero-shot image classification [3, 5], to high-level planning [6, 7]. Their capabilities depend on their training data - while they may be broadly crawled from the web, their distributions remain distinct across domains. For example, in terms of linguistic data, visual-language models (VLMs) [8, 9] are trained on image and video captions, but large language models (LMs) [1, 10, 11] are additionally trained on a large corpora of other data such as spreadsheets, fictional novels, and standardized test questions. These different domains offer distinct commonsense knowledge: VLMs can ground text to visual content, but LMs can perform a variety of other linguistic tasks (e.g., reading comprehension [12]). In this work, we propose these model differences are complementary and can be jointly leveraged to compose (via prompting) new multimodal capabilities out-of-the-box. To this end, we present Socratic Models ${ }^{1}$ (SMs), a modular framework in which new tasks are formulated as a language-based exchange between pretrained models and other modules, without additional training or finetuning. These modules can either contain (i) large pretrained ("foundation" [4]) models, or (ii) APIs that interface with external capabilities or databases (e.g., web search, robot actions). Rather than scaling task-specific multimodal training data in the areas of overlap (e.g., alt-text captions [13]), or unifying model architectures for multitask learning [14], SMs embrace the zero-shot capabilities of pretrained models by prompt engineering guided multimodal discussions between the independent models to perform joint inference on a task-specific output.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Across a number of domains spanning vision, language, and audio modalities - and via a small amount of creative prompt-enabled multimodal composition - SMs are quantitatively competitive with zero-shot state-of-the-art on standard benchmarks including (i) image captioning on MS COCO [15, 16], (ii) contextual image captioning and description (improving 11.3 to 38.9 captioning CIDEr on Concadia [17]), and (iii) video understanding with video-to-text retrieval (from 40.7 to 44.7 zero-shot R@1 on MSR-VTT [18]). SMs also enable new capabilities across applications such as (i) open-ended reasoning for egocentric perception (Fig. 4), (ii) multimodal assistive dialogue to guide a user through a cooking recipe, and (iii) robot perceptiondriven planning for sequential pick and place. SMs give rise to new opportunities to address classically challenging problems in one domain, by reformulating it as a problem in another. For example, answering free-form questions about first-person videos (e.g., "why did I go to the front porch today?") was previously thought to be out-of-reach for egocentric perception without domain-specific data collection [19, 20]. We show that this is possible with SMs by assembling video into a language-based world-state history (in the form of a short story, or event log), then performing various types of open-ended text-prompted tasks (e.g., answering questions) about that world-state history - i.e., formulating video understanding as a reading comprehension problem, for which modern LMs are proficient.</p>
<p>The goal of this paper is (1) to discuss new perspectives on building AI systems that embrace the heterogeneity of pretrained models through structured Socratic dialogue, and (2) to give example demonstrations of what is already possible today with SMs on challenging multimodal tasks. Our primary contribution is (i) the Socratic Models framework, which proposes to compose multimodal pretrained models through language, without requiring training. The SMs framework contains key, enabling components such as the demonstrated (ii) multimodal prompting methods, including (iii) language-based world-state history for video understanding. Additional contributions include (iv) demonstrating strong quantitative performance of example SM systems, setting new zero-shot state-of-the-art on multiple tasks, including in image captioning and video understanding, and (v) providing additional application examples on open-ended egocentric perception, multimodal assistants, and robot perception and planning. Our demonstrated SM systems are not without limitations - we discuss the unreliability inherited from the models on which they are constructed, together with other potential broader impacts (Sec. 6). Code is available at socraticmodels.github.io.</p>
<h2>2 Problem Setting, Background, and Related Work</h2>
<p>Problem setting. We are interested in creating a variety of multimodal [21] applications enabled by large pretrained models, which can be viewed as a form of transfer [22, 23]: "knowledge" learned from a set of surrogate tasks (e.g., text completion, image-text similarity) is applied to new downstream target tasks (e.g., image captioning, robot planning). Consider a set of target tasks where each task $i$ seeks a desired map $f^{i}: \mathcal{X}^{i} \rightarrow \mathcal{Y}^{i}$. We are particularly interested in cases where: (i) each input $\mathcal{X}^{i}$ and/or output $\mathcal{Y}^{i}$ may contain multiple modalities e.g., from the power set of {language, vision, audio, robot actions}; (ii) there may be many such tasks; (iii) each target task may have little or no training data available; and (iv) models pretrained on the surrogate tasks are available.</p>
<p>Pretraining weights is a dominant paradigm for transfer learning with deep models, in which pretrained model weights (from surrogate tasks) are used to initialize some subset of parameters in the model for the target task, which are then either (a) left frozen, or (b) finetuned. Pretraining deep models has been studied extensively in the unsupervised setting [24, 25, 26, 27, 28], and in the supervised setting was perhaps most popularized by ImageNet [29] pretraining [30, 31, 32, 33],</p>
<p>Various forms of pretraining have been ubiquitous in NLP [34, 35, 36, 37, 38, 1, 2]. For each target task, model architectures and/or training procedures may need to be developed that are composed of these pretrained parameters, for which domain expertise may be advantageous. In multimodal training, it is common to leave sub-portions of models, for example ones associated with one but not other modalities, frozen for downstream tasks [39, 40, 41, 42, 43].
Joint training of all modalities on specific target tasks is a common approach to multimodal learning [42, 44, 45, 46, 47, 48]. For each task $i$ one may obtain a large multimodal dataset and train a taskspecific map $f_{k_{i}}^{i}$ with parameters $\theta_{i}$, some of which may come from pretrained weights, either frozen or finetuned. A benefit of this approach is that it follows the playbook of: (1) curate a big dataset, (2) train a big model, which given enough data and compute has proven to be formidable [49].
Combining both weights from large pretrained models with multimodal joint training, several works have achieved strong results for a number of downstream multimodal applications including VLMs with LMs for image captioning (e.g., CLIP with GPT-2) [45], video understanding (e.g., CLIP with BERT [46]), visual question answering e.g., [47] and ALMs and LMs for speech and text modeling e.g., $[50,51]$. These systems are often finetuned on task-specific data, and while this paradigm is likely to be preferred in domains for which data is abundant, our results suggest that SMs can be a strong alternative for applications in which data is less available or more expensive to obtain.
Multimodal probabilistic inference is an alternative e.g., Bayesian approach where one model is used as a prior and the other as evidence - with which models from different modalities may perform joint inference [52, 7]. One prominent example is in automatic speech recognition: different language models can be trained separately, then transfer knowledge to a speech-to-text system via priors [52].
The notion of "Mixture-of-Experts" ([53], see [54] for a review) is also a common paradigm for combining the outputs of multiple models - specifically, mixtures of experts across multimodal domains including vision and audio [55] have been studied. Further investigating these techniques in the context of recent pretrained foundation models may be a promising direction for future work.
Zero-shot or few-shot prompting recently has been shown, notably by Brown et al. [2], to be highly effective for transfer learning. In this approach, a large pretrained language model is zero-shot or few-shot prompted with several examples, without training, to perform a new task. Further methods such as chain-of-thought prompting [56] have shown that even simple prompting modifications can have a profound impact on target task performance [56, 57] and enable new capabilities. Our work builds on these works, by extending prompting methods into the multimodal domain.</p>
<h1>3 Socratic Models</h1>
<p>Socratic Models (SMs) is a framework in which multiple large pretrained models may be composed through language (via prompting) without requiring training, to perform new downstream multimodal tasks. This offers an alternative method for composing pretrained models that directly uses language as the intermediate representation by which the modules exchange information with each other. It is both distinct from, and may be complementary to, other multimodal approaches such as joint multimodal training (Sec. 2). SMs are perhaps most intuitively understood through examples, which are provided in Sec. 4 and 5, but a definition is as follows. A task-specific Socratic Model $f_{\mathrm{SM}}: \mathcal{X} \rightarrow \mathcal{Y}$ may be described as a computation graph, with nodes as a set of modules $\left{f_{\mathrm{AP}}^{\prime}\right}$, and the edges of the graph represent intermodule communication through language. Each $\mathcal{M}$ is some (multimodal) model or external API, and each module $f$ assists in transforming the output of one $f$ into a form of language that a connected $f^{\prime}$ may use for further inference. For visualization, outputs from LMs are blue, VLMs green, ALMs purple, prompt text gray, user inputs magenta, VLM-chosen LM outputs green-underlined blue, and ALM-chosen LM outputs purple-underlined blue.
A key component in SMs is multi-model multimodal prompting, in which information from a nonlanguage domain is substituted into a language prompt, which can be used by an LM for reasoning. One way to multimodal prompt is to variable-substitute language-described entities from other modalities into a prompt. An example of this is: activity $=f_{\mathrm{LM}}\left(f_{\mathrm{VLM}}\left(f_{\mathrm{LM}}\left(f_{\mathrm{LM}}\left(f_{\mathrm{UM}}(\text { video })\right)\right)\right)\right)$ shown in Fig. 2, where (i) the VLM detects visual entities, (ii) the LM suggests sounds that may be heard, (iii) the ALM chooses the most likely sound, (iv) the LM suggests possible activities, (v) the VLM ranks the most likely activity, (vi) the LM generates a summary of the Socratic interaction. Some form of such multimodal prompting is central to all of our demonstrated SM examples (Sec. 4</p>
<p>and 5). Note that this example involves multiple back-and-forth interactions, including calling the same model multiple times, forming a sort of "closed-loop" feedback between nodes in the SM graph.</p>
<p>Informally SMs may be interpreted as composing pretrained models to "talk to each other", but in practice certain models may need simple pre- and post-processing to produce language. For example, vision-text similarity VLMs, e.g., CLIP [3], do not inherently produce text, but can be made to perform zero-shot detection from a large pre-existing library of class category names, and return the top- $k$ detected categories. Accordingly, although our example SM systems required no training, the interactions between models are scripted with prompt templates. While in future work we are excited to explore learning the interactions (i.e., forms of each $f$, and edges), we also find practical benefits of a framework with no required task-specific training: new applications can be quickly targeted by just a small amount of creative programming.</p>
<p>SMs are in part a reaction to the constraints of the predominant "pretraining weights" (Sec. 2) paradigm to transfer learning with foundation models, which include: (i) expensive (at times prohibitively) to finetune large 100B+ parameter models both in terms of compute costs and data collection (can be challenging for new multimodal applications e.g., in AR or robotics), (ii) finetuning pretrained model weights may lose generality and robustness to distribution shifts [58], (iii) foundation models may store "stale" knowledge (due to training latencies), and lack access to dynamic online data or proprietary sources of information. Despite these limitations, large pretrained foundation models [4] are likely to serve as a backbone for many intelligent systems of the future - SMs is a systems approach (i.e., glue framework) that leans on their zero-shot and few-shot capabilities in aggregate as a means to address these limitations for new downstream multimodal tasks.</p>
<h1>4 Evaluation: Methods and Results</h1>
<p>In this section, we quantitatively evaluate Socratic Models on: image captioning [15, 16] (Sec. 3.1), contextual image captioning [17] (Sec. 3.2), and video-to-text retrieval [18] (Sec. 3.3). For each task, we (i) describe how we use the SMs framework, and (ii) discuss results.</p>
<h3>4.1 Socratic Image Captioning on MS COCO Captions: VLM + LM</h3>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Example: SM systems can be prompted to zeroshot annotate an egocentric image with a summary of the person's activities. Information from multiple modalities (language, audio) can help denoise predictions from any one specific modality (vision).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: SMs with VLM and LM prompting (left) can zero-shot generate captions for generic Internet images (e.g., from MS COCO), and can be as expressive as task-specific finetuned methods such as ClipCap [45].</p>
<p>Method. SMs can generate image captions by prompting a guided language-based exchange between a VLM and LM - i.e., via caption $=f_{\mathrm{VLM}}^{S}\left(f_{\mathrm{CLM}}^{s}\left(f_{\mathrm{VLM}}^{t}\right.\right.$ image $\left.\left.)\right)\right)$. First (1), the VLM is used to zeroshot detect different place categories (Places356 [59]), object categories (from Tencent ML-Images [60]), image type ( ${$ photo, cartoon, sketch, painting $}$ ) and the number of people ${$ are no people, is one person, are two people, are three people, are several people, are many people $}$. The top- $k$ ranked in each category can then be substituted into an LM prompt, as shown in Fig. 3, left. Second (2), given the VLM-informed language prompt, a causal LM (i.e., for text completion) generates several $n$ candidate captions. For this step, we use a non-zero next-token sampling temperature (e.g., 0.9 for</p>
<p>GPT-3), to return sufficiently diverse, but reasonable results across the $n$ candidates. Finally (3), these $n$ captions are then ranked by the VLM with the image, and the highest scoring caption is returned.</p>
<p>Results. Tab. 1 shows quantitative comparisons with state-of-the-art image captioning methods on MS COCO Captions dataset [15, 16]. We chose to evaluate over a random sampled subset of 100 images from the test split [63], so that GPT-3 API runtime costs are more affordable for reproducibility ( $\sim \$ 150$ USD per run with with $n=20$ generated candidate captions per image). Metrics from baselines are comparable to full-test-set metrics (see Appendix).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">CIDEr</th>
<th style="text-align: center;">SPICE</th>
<th style="text-align: center;">ROUGE-L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">${ }^{+}$ClipCap [45]</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">152.4</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: left;">${ }^{\dagger}$ MAGIC [61]</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">39.0</td>
</tr>
<tr>
<td style="text-align: left;">ZeroCap [62]</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">18.3</td>
</tr>
<tr>
<td style="text-align: left;">SMs 0-shot (ours)</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: left;">SMs 3-shot (ours)</td>
<td style="text-align: center;">$\mathbf{1 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{4 3 . 7}$</td>
</tr>
</tbody>
</table>
<p>${ }^{+}$finetuned on full training set with image-text pairs.
${ }^{\dagger}$ finetuned on unpaired training set, zero-shot on image-text pairs.
Table 1: Image captioning comparisons on a random subset of $N=100$ MS COCO test examples.</p>
<p>SMs substantially outperform the zero-shot state-of-the-art ZeroCap [62] with a CIDEr [64] score $18.0 \rightarrow 44.5$, but do not perform as well as methods such as ClipCap [45] which are directly finetuned on the training set. SMs tend to generate verbose and descriptive captions (see qualitative examples in Fig. 3), but may naturally score lower on captioning metrics if they do not match the dataset's distribution of caption labels. This performance gap narrows as SMs are few-shot prompted with 3 random captions from the training set, bringing CIDEr scores up to 76.3 , exceeding the performance of MAGIC [61] which finetunes the text generator on the training set's unpaired captions.</p>
<p>While these results are promising, the degree to which visual details are provided in the captions is largely limited by the capabilities of the VLM. For example, attributes (e.g., color of a shirt, a person's facial expression, or the spatial relationships between objects) are details not often captured in our particular system, which relies more on the contextual image classification capabilities of the VLM. Future captioning work with SMs may explore open-vocabulary object detectors [65, 66] to recover more salient details, or combine the outputs of multiple task-specific image captioning models with LMs to assemble a single rich and coherent caption.</p>
<h1>4.2 Socratic Contextual Image Description on Concadia: VLM + LM</h1>
<p>Method. Concadia [17] is a dataset for contextual image captioning and description, conditioned on the input image and an associated paragraph of article text. In particular, image descriptions describe the visual content in the image (e.g., "a portrait of a man with a beard in a suit") commonly used for accessibility, while image captions link images to article text (e.g., "a photo of Abraham Lincoln"). We evaluate SMs on both tasks, using a similar method to MS COCO captions (Sec. 4.1) but with articletext prompt-substitution (below), and no need for VLM re-ranking. $f_{\mathrm{LM}}^{2}\left(f_{\mathrm{VLM}}^{1}\right.$ (image), context):</p>
<div class="codehilite"><pre><span></span><code>I am an intelligent image captioning bot. The article is about: &quot;{article_text}&quot;. In this image, I think I see
a {object1}, {object2}, {object3},... A short caption for this image is:
</code></pre></div>

<p>Results. We evaluate SMs on the test split of Concadia with 9,691 images (shown in Tab. 2). Despite being zero-shot, SMs outperform the task-specific prior best method, Kreiss et al. [17], that directly finetunes on the training set of 77,534 images, with a CIDEr score improvement $11.3 \rightarrow 38.9$ for generated image captions, and $17.4 \rightarrow 22.6$ for generated image descriptions. We also report numbers for captioning generation conditioned on the image, article text, and ground truth description. This achieves a CIDEr score of 93.8 and suggests the upper bound of performance if SMs are used with VLMs that can produce accurate image descriptions. We also discuss interesting additional findings in the appendix, e.g., that LMs can perform comparably on contextual image captioning even without input images (i.e., only article text as input), which either (i) reflects a strong correlation between the distributions of captions and article texts, and/or (ii) indicates LM training set overlap. Overall, the results on Concadia are promising and suggest that SMs can be used to automatically generate descriptive texts that improve the accessibility of visual content on the web for the low vision community.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Caption Generation</th>
<th style="text-align: center;">Description Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Kreiss et al. [17]</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">17.4</td>
</tr>
<tr>
<td style="text-align: left;">SMs (ours)</td>
<td style="text-align: center;">$\mathbf{3 8 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 2 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">SMs w/ description</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: SMs zero-shot are competitive on contextual image captioning and description (measured with CIDEr) on the Concadia dataset, outperforming task-specific methods e.g., Kreiss et al. [17] which finetunes on the training set.</p>
<h1>4.3 Socratic Video-to-Text Retrieval: VLM + LM + ALM</h1>
<p>Method. Socratic Models can be adapted for video-to-text retrieval, a video understanding task commonly benchmarked on MSR-VTT [18]. Our approach leverages commonsense information from audio and language domains to augment the vision-based Portillo-Quintero et al. [67], which computes a similarity measure between the average VLM (i.e., CLIP) features of all video frames per video, and the VLM text features of captions - used to execute video-to-text retrieval with one-to-many nearest neighbor matching. Specifically, our system transcribes audio from the video with speech-to-text ALMs [51] for automatic speech recognition (ASR e.g., via Google Cloud speech-to-text API [68]), then summarizes the transcripts with an LM using the following prompt:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">I</span><span class="w"> </span><span class="nv">am</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">intelligent</span><span class="w"> </span><span class="nv">video</span><span class="w"> </span><span class="nv">captioning</span><span class="w"> </span><span class="nv">bot</span>.<span class="s1">&#39; I hear a person saying: &#39;</span>{<span class="nv">transcript</span>}<span class="s1">&#39;. Q: What&#39;</span><span class="nv">s</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">short</span><span class="w"> </span><span class="nv">video</span>
<span class="nv">caption</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">video</span>?<span class="w"> </span><span class="nv">A</span>:<span class="w"> </span><span class="nv">In</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">video</span>,
</code></pre></div>

<p>We compute similarity scores of the generated summary to the set of captions with a masked LM (e.g., similarity between sentence embeddings from RoBERTa [69]), and use those scores to re-weight the CLIP-based ranking from Portillo-Quintero et al. For videos with sufficiently-long transcripts ( $\geq 100$ characters), the matching score is: $\left(\right.$ CLIP $\left.^{\prime} \text { caption }\right) \cdot$ CLIP $\left(\right.$ video $\left.^{\prime}\right) \times\left(\right.$ RoBERTa $\left.^{\prime} \text { caption }\right) \cdot$ RoBERTa (GPT-3(prompt, Speech2Text (audio ${ }^{\prime}$ ()))), where - represents normalized dot product of embeddings, and $\times$ represents scalar multiplication. For a given video, if there is no audio or the transcript is too short, we default to Portillo-Quintero et al., which is just CLIP (caption) $\cdot$ CLIP (video'). Here, the Socratic interaction lies mainly between the ALM (speech-to-text) to the commonsense LM (GPT-3 to summarize transcriptions), and between the commonsense LM to the ranking based system that is a combination of the VLM (CLIP) and the masked LM (RoBERTa).</p>
<p>Results. We evaluate on MSR-VTT [18], which as noted in other recent works [46, 73] is the most popular benchmark for video-to-text retrieval. We compare our method with both zero-shot methods, as well as finetuned methods specifically trained on MSR-VTT. Results show that our method sets a new zero-shot state-of-the-art (Tab.3). Since our system uses PortilloQuintero et al. [67] to process CLIP features but additionally incorporates LM reasoning on speech-to-text transcripts, the increased measured performance of our method (i.e., $40.3 \rightarrow$ 44.7 R@1) directly reflects the added benefits of incorporating language-based multimodal reasoning. Additionally, to keep the comparison between our method and Portillo-Quintero et al. [67] as direct as possible, we maintain the usage of their precomputed CLIP features from ViT-B/32, but it is likely that performance can be improved with other recent more performant VLMs (e.g., LiT [39], CLIP with ViT-L/14).</p>
<p>Table 4 shows that on the subset of test videos that contain longtranscripts, we observe a more substantial increase in performance from 40.3 to 54.9 with our method compared to PortilloQuintero et al. [67]. Note that this is roughly comparable to the R@1 of the best finetuned-SOTA method, CLIP2Video [71], with 54.6 R@1 (Tab. 3). If we assume that for visual-only methods, the videos with-or-without transcripts are of roughly equal difficulty from a visual-only retrieval perspective, this suggests that on internet videos with sufficient speech present in the audio, our zero-shot SMs can nearly match the finetuned-SOTA methods for video-to-text retrieval.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MSR-VTT Full</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">R@1 $\uparrow$</td>
<td style="text-align: center;">R@5 $\uparrow$</td>
<td style="text-align: center;">R@10 $\uparrow$</td>
<td style="text-align: center;">MdR $\downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuned</td>
<td style="text-align: center;">JEMC [70]</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">16.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Collab. Experts [55]</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">8.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CLIP2Video [71]</td>
<td style="text-align: center;">$\mathbf{5 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">CLIP via [67]</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">$\mathbf{2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">SMs (ours)</td>
<td style="text-align: center;">$\mathbf{4 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Video-to-text retrieval results on MSR-VTT [18] dataset, both on the popular 1k-A [72] subset and the original 'full' test set. Differentiated are methods which train on the MSR-VTT dataset (finetuning), compared with zero-shot methods, which do not. Also noted: whether the methods use audio channels, and if CLIP [3] is used, which CLIP encoder is used.
[ Long-transcript subset of MSR-VTT Full
$R @ 1 \uparrow R @ 5 \uparrow R @ 10 \uparrow$ MdR $\downarrow$
CLIP via [67] 41.5 69.6 77.4 2.0
SMs (ours) $\mathbf{5 4 . 9} \mathbf{7 4 . 0} \mathbf{7 9 . 9} \mathbf{1 . 0}$
Table 4: SMs substantially improve on Portillo-Quintero et al. [67] for video-to-text retrieval on the MSR-VTT subset of videos for which long-transcripts are available ( $n=1,007$ out of 2,990 ).</p>
<h2>5 Applications: Methods and Demonstrations</h2>
<p>In this section, we describe several applications of SMs on (i) egocentric perception, (ii) multimodal assistive dialogue, and (iii) robot perception and planning. These applications each involve processing</p>
<p>user inputs/feedback, and serve as examples of integrating external modules (e.g., web search, robot policies) as additional participants to a Socratic discussion to enable new multimodal functionalities.</p>
<h1>5.1 Egocentric Perception: User + VLM + LM + ALM</h1>
<p>SMs can be prompted to perform various perceptual tasks on egocentric video: (i) summarizing content, (ii) answering free-form reasoning questions, (iii) and forecasting. Egocentric perception has downstream applications in AR and robotics, but remains challenging: the characteristics of first-person footage - from unusual viewpoints to lack of temporal curation - are not often found in existing datasets, which focus more on generic Internet content captured from third-person views [29, 16, 74]. This domain shift makes it difficult for data-driven egocentric models to benefit from the paradigm of pretraining on third person Internet data [75, 76]. SMs offer a zero-shot alternative to perform egocentric perceptual tasks without training on large domain-specific datasets [19, 20, 76].
For open-ended reasoning, a key aspect of our SMs-based approach is formulating video understanding as reading comprehension, i.e., re-framing "video Q\&amp;A" as a "short story Q\&amp;A" problem, which differs from common paradigms for video understanding that may involve supervising videotext models on labeled datasets or adversarial training (see [77] for a recent survey). To this end, we first extract a set of "key moments" throughout the video (e.g., via importance sampling, or video/audio search based on the input query, discussed in Appendix). We then caption the key frames indexed by these moments (using prompts similar to those in Sec.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: SMs with VLM, LM, and ALM can be prompted to generate a captions for key moments in videos, which can be assembled into a language-based world-state history (e.g., in the form of an event log) that the LM can answer free-form questions about.
4.1 and Sec. 4.2), and recursively summarize [78] them into a language-based record of events, which we term a language-based world-state history. This is then passed as context to an LM to perform various reasoning tasks via text completion such as Q\&amp;A, for which LMs have demonstrated strong zero-shot performance [2]. Drawing analogies to 3D vision and robotics, the world-state history can be thought of as building an on-the-fly reconstruction of events in the observable world with language, rather than other representations, such as dynamically-updated 3D meshes [79] or neural fields [80].
(i) Summarization enables augmenting human memory to recall events or life-log activities. Given world-state history constructed from SMs using a first-person POV video ${ }^{2}$, this can be implemented by prompting an LM to complete: "[world-state history] Summary of my day." to which it can respond with outputs like "I slept in a bed, made coffee, watched TV, did laundry, received a package, bench pressed, showered, ate a sandwich, worked on a computer, and drank wine."
(ii) Open-ended Q\&amp;A involves prompting the LM to complete the template: "[world-state history] Q: {question} A:". Conditioned on the quality (comprehensiveness) of the world-state history, LMs can generate surprisingly meaningful results to contextual recall questions (e.g., "what was I doing outdoors?" $\rightarrow$ "I was chopping wood in a yard.", "did I drive today?" $\rightarrow$ "no, I did not drive today."), temporal questions (e.g., "when did I last drink coffee?" $\rightarrow$ "I last drank coffee at 10:17 AM", "how many times did I receive a package today?" $\rightarrow$ "I received a package once today."), cause-and-effect questions (e.g., "why did I go to the front porch today?" $\rightarrow$ "I went to the front porch today to receive a package."). As in [81] we can also further prompt the LM to explain the answer by adding "This is because:" to which it can respond "I saw on the porch a package and knew that I was expecting it."
(iii) Forecasting of future events can be formulated as language-based world-state completion. Our system prompts the LM to complete the rest of an input event log. Timestamps of the predictions can be preemptively specified depending on the application needs. The completion results (example below on the right) are generative, and are more broad than binary event classification [82].</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Few-shot prompting the LM with additional examples of prior event logs most similar to the current one is likely to improve the precision of the results, which may be useful for assistive AR applications. Without additional context, the completions are likely biased towards typical schedules seen by the LM across Internet-scale data.</p>
<p>1:46 PM: I am eating a sandwich in a kitchen. 2:18 PM: I am checking time and working on a laptop in a clean room. 2:49 PM: I am buying produce from a grocery store or market. 3:31 PM: I am driving a car. 4:03 PM: I am in a park and see a playground. 4:35 PM: I am in a home and see a television.</p>
<h1>5.2 Multimodal Assistive Dialogue: User + VLM + LM + Web Search</h1>
<p>SMs can be adapted to engage in multimodal dialogue to assist people in doing every day tasks, such as cooking. Our example application here helps the user search for a recipe, then guides them through it step by step. The system allows the user to navigate recipe steps with casual dialogue, provides ingredient replacements or advice (using LM priors), and searches for visual references (in the form of images or videos) on user request. This is a case study in (i) prompting a dialogue LM [10] to produce key phrase tokens that elicit specific Socratic interactions (e.g., video search via a VLM to output visual data), and (ii) using a web crawler (outputs in red) as an additional module engaged in Socratic discussion with other models to retrieve information online. The approach preconditions an LM (e.g., GPT-3 [2]) with context that includes when and how key phrases should be referenced:</p>
<p>Alice is a an expert chef that will help Bob prepare a given recipe. If Bob asks for the next step, Alice will respond with "Step: " followed by the next step of the recipe. If Bob does not have the right ingredients, Alice will assist Bob in finding suitable replacements. If Bob asks Alice to describe something that is better shown visually, Alice will say "(image)" followed by a response. Ingredients: {ingredients} Recipe: {recipe}
where phrases such as "(image)" initiate a Socratic interaction where the web crawler returns a video of the recipe (if it exists), and the VLM runs visual search on it using the LM response to retrieve a related frame. We find that this can provide qualitatively meaningful guidance to a user, for example ${ }^{3}$ :</p>
<div class="codehilite"><pre><span></span><code><span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="n">Step</span><span class="o">:</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">medium</span><span class="w"> </span><span class="n">bowl</span><span class="o">,</span><span class="w"> </span><span class="n">beat</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">egg</span><span class="w"> </span><span class="n">whites</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">frothy</span><span class="o">.</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">eggs</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">egg</span><span class="w"> </span><span class="n">whites</span><span class="o">,</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">temperature</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">whats</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">temperature</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="n">Room</span><span class="w"> </span><span class="n">temperature</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">around</span><span class="w"> </span><span class="mi">70</span><span class="w"> </span><span class="n">degrees</span><span class="w"> </span><span class="n">Fahrenheit</span><span class="o">.</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">medium</span><span class="w"> </span><span class="n">bowl</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">medium</span><span class="w"> </span><span class="n">bowl</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bowl</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">big</span><span class="w"> </span><span class="n">enough</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">fit</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ingredients</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="k">this</span>
<span class="n">recipe</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">comfortable</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">mix</span><span class="w"> </span><span class="k">in</span><span class="o">.</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">exactly</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">big</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">be</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="n">image</span><span class="o">)</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">medium</span><span class="w"> </span><span class="n">bowl</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bowl</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">big</span><span class="w"> </span><span class="n">enough</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">fit</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ingredients</span><span class="w"> </span><span class="k">for</span>
<span class="k">this</span><span class="w"> </span><span class="n">recipe</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">comfortable</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">mix</span><span class="w"> </span><span class="k">in</span><span class="o">.</span>
<span class="o">(</span><span class="n">a</span><span class="o">)</span><span class="w"> </span><span class="n">video</span><span class="w"> </span><span class="n">search</span><span class="w"> </span><span class="o">(</span><span class="n">recipe</span><span class="w"> </span><span class="n">video</span><span class="o">)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="s2">&quot;A medium bowl is a bowl that is...&quot;</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">ok</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="n">im</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="n">whats</span><span class="w"> </span><span class="n">next</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="n">Step</span><span class="o">:</span><span class="w"> </span><span class="n">Keep</span><span class="w"> </span><span class="n">beating</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">slowly</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">sugar</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">stiff</span><span class="w"> </span><span class="n">peaks</span><span class="w"> </span><span class="n">form</span><span class="o">.</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">whats</span><span class="w"> </span><span class="n">stiff</span><span class="w"> </span><span class="n">peaks</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="n">Stiff</span><span class="w"> </span><span class="n">peaks</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">egg</span><span class="w"> </span><span class="n">whites</span><span class="w"> </span><span class="n">stand</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">peaks</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">fall</span><span class="w"> </span><span class="n">over</span><span class="o">.</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">show</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">looks</span><span class="w"> </span><span class="n">like</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="n">image</span><span class="o">)</span><span class="w"> </span><span class="n">Stiff</span><span class="w"> </span><span class="n">peaks</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">when</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">egg</span><span class="w"> </span><span class="n">whites</span><span class="w"> </span><span class="n">stand</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">peaks</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">fall</span>
<span class="n">over</span><span class="o">.</span>
<span class="o">(</span><span class="n">b</span><span class="o">)</span><span class="w"> </span><span class="n">video</span><span class="w"> </span><span class="n">search</span><span class="w"> </span><span class="o">(</span><span class="n">recipe</span><span class="w"> </span><span class="n">video</span><span class="o">)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="s2">&quot;Stiff peaks are when the egg whites...&quot;</span>
<span class="n">Bob</span><span class="o">:</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
<span class="n">Alice</span><span class="o">:</span><span class="w"> </span><span class="n">No</span><span class="o">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="n">water</span><span class="o">.</span>
</code></pre></div>

<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: SMs with VLM, Web Search, and LM prompting can enable multimodal dialogue applications such as guiding a user through online recipe steps and providing assistive visuals via video search.</p>
<h3>5.3 Robot Perception \&amp; Planning: User + VLM + LM + Policies</h3>
<p>SMs can be used to enable robots to perform language-conditioned tasks. Our example system uses a VLM (open-vocabulary object detection with ViLD [65]) to describe the objects in the scene, feeds that description as context to a LM as a multi-step planner [7, 6], that then generates the individual steps to be passed to a pretrained language-conditioned robot policy (e.g., models similar to CLIPort $[83,84]$ for open vocabulary pick-and-place). Steps can be represented in the form of natural language ("Pick the red block and place it on the blue block.") or in the form of pseudocode (to generate text with a fixed template e.g., "robot.pick_and_place("red block", "blue block")"), leveraging LM capacity to write code. We demonstrate this in the context of a simulated environment (shown in Fig.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>6) using a UR5 arm and and several objects (blocks, bowls). Distinct from [7], this uses cross-modal VLM-informed prompting and LM code generation, rather than joint probabilistic inference.</p>
<div class="codehilite"><pre><span></span><code>objects = (&quot;green block&quot;, &quot;blue block&quot;, &quot;yellow block&quot;, &quot;green
bowl&quot;, &quot;blue bowl&quot;, &quot;yellow bowl&quot;)
<span class="gh">#</span> move all the blocks to different corners.
Step 1. robot.pick_and_place(&quot;green block&quot;, &quot;top left corner&quot;)
Step 2. robot.pick_and_place(&quot;blue block&quot;, &quot;top right corner&quot;)
Step 3. robot.pick_and_place(&quot;yellow block&quot;, &quot;bottom left corner&quot;)
<span class="gh">#</span> now move the blue block to the middle.
Step 1. robot.pick_and_place(&quot;blue block&quot;, &quot;middle&quot;)
<span class="gh">#</span> stack the blocks on top of each other.
Step 1. robot.pick_and_place(&quot;yellow block&quot;, &quot;blue block&quot;)
Step 2. robot.pick_and_place(&quot;green block&quot;, &quot;yellow block&quot;)
<span class="gh">#</span> wait actually undo that last step.
Step 1. robot.pick_and_place(&quot;green block&quot;, &quot;top left corner&quot;)
<span class="gh">#</span> put the yellow block in the bowl you think it best fits.
Step 1. robot.pick_and_place(&quot;yellow block&quot;, &quot;yellow bowl&quot;)
<span class="gh">#</span> ok now sort the remaining blocks in the same way.
Step 1. robot.pick_and_place(&quot;blue block&quot;, &quot;blue bowl&quot;)
Step 2. robot.pick_and_place(&quot;green block&quot;, &quot;green bowl&quot;)
</code></pre></div>

<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: SMs can be engineered with VLM, LM, and language-conditioned robot policies (e.g., via CLIPort [83]) to enable robots to parse and generate plans from free-form human instructions (in magenta).</p>
<p>Chaining this system together expands the set of language-specified tasks beyond the original set of primitives trained by the policy, and enables applications involving human dialogue with the robot.</p>
<h1>6 Discussion</h1>
<p>Socratic Models is a modular framework that leverages structured dialogue (i.e., via prompting) between multiple large pretrained models to make joint predictions for new multimodal tasks. SMs leverage the commonsense knowledge already stored within foundation models pretrained on different domains of data (e.g., text-to-text, text-to-images, text-to-audio), which may include Internetscale data. Our shown systems for image captioning, video-to-text retrieval, egocentric perception, multimodal dialogue, robot perception and planning are just examples of the SMs framework, and may shed light on new opportunities to build simple systems that adapt pre-existing foundation models to (i) capture new multimodal functionalities zero-shot without having to rely on additional domain-specific data collection or model finetuning, and (ii) do so while retaining their robustness to distribution shifts (which is known to deteriorate after finetuning) [58]. Potential future work may involve meta-learning the Socratic interactions themselves, and extending the inter-module edges to include additional modalities beyond language, e.g., passing images between modules.</p>
<p>Broader Impacts. SMs offer new perspectives that encourage building AI systems using off-theshelf large pretrained models without additional data collection or model finetuning. This leads to several practical benefits, new applications, and risks as well. For one, SMs provide an interpretable window, through language, into the behavior of the systems (even for non-experts). Further, the barrier of entry for this technology is small: SMs can be engineered to capture new functionalities with minimal compute resources, and to tackle applications that have traditionally been data-scarce. No model training was used to create our demonstrated results. This can be enabling, but also raises potential risks, since it increases the flexibility of unintended end use applications, and should be carefully monitored over time. It is also important to note that the system may generate results that reflect unwanted biases found in the Internet-scale data on which incorporated models are trained, and should be used with caution (and checked for correctness) in downstream applications. We welcome broad discussion on how to maximize the potential positive impacts (enabling broad, new multimodal applications, with minimal new resources) while minimizing the capabilities of bad actors.</p>
<h1>Acknowledgments and Disclosure of Funding</h1>
<p>We thank Debidatta Dwibedi, Matthew O'Kelly, and Kevin Zakka for excellent feedback on improving this manuscript, Anelia Angelova, Jean-Jacques Slotine, Jonathan Tompson, Shuran Song, for fruitful technical discussions, Kan Huang for applications support, Ahmed Omran, Aren Jensen, Malcolm Slaney, Karolis Misiunas for advice on audio models, and Cody Wanner for YouTube videos.</p>
<h2>References</h2>
<p>[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[3] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
[4] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[5] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34, 2021.
[6] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.
[7] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2022.00000, 2022.
[8] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.
[9] A. Jain, M. Guo, K. Srinivasan, T. Chen, S. Kudugunta, C. Jia, Y. Yang, and J. Baldridge. Mural: multimodal, multitask retrieval across languages. arXiv preprint arXiv:2109.05125, 2021.
[10] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[11] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[12] P. Rajpurkar, R. Jia, and P. Liang. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.
[13] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.
[14] R. Hu and A. Singh. Transformer is all you need: Multimodal multitask learning with a unified transformer. arXiv e-prints, pages arXiv-2102, 2021.
[15] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollr, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.
[16] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.
[17] E. Kreiss, N. D. Goodman, and C. Potts. Concadia: Tackling image accessibility with context. arXiv preprint arXiv:2104.08376, 2021.
[18] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $5288-5296,2016$.</p>
<p>[19] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. arXiv preprint arXiv:2110.07058, 2021.
[20] D. Damen, H. Doughty, G. M. Farinella, A. Furnari, E. Kazakos, J. Ma, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al. Rescaling egocentric vision. arXiv preprint arXiv:2006.13256, 2020.
[21] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In ICML, 2011.
[22] R. Caruana. Multitask learning. Machine learning, 28(1):41-75, 1997.
[23] S. Thrun. Lifelong learning algorithms. In Learning to learn, pages 181-209. Springer, 1998.
[24] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527-1554, 2006.
[25] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19, 2006.
[26] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103, 2008.
[27] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: transfer learning from unlabeled data. In Proceedings of the 24th international conference on Machine learning, pages 759-766, 2007.
[28] G. Mesnil, Y. Dauphin, X. Glorot, S. Rifai, Y. Bengio, I. Goodfellow, E. Lavoie, X. Muller, G. Desjardins, D. Warde-Farley, et al. Unsupervised and transfer learning challenge: a deep learning approach. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pages 97-110. JMLR Workshop and Conference Proceedings, 2012.
[29] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.
[30] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580-587, 2014.
[31] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pages 647-655. PMLR, 2014.
[32] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818-833. Springer, 2014.
[33] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013.
[34] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.
[35] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.
[36] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. Advances in neural information processing systems, 28, 2015.
[37] P. Ramachandran, P. J. Liu, and Q. V. Le. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016.
[38] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. 2018.
[39] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer. Lit: Zero-shot transfer with locked-image text tuning. arXiv preprint arXiv:2111.07991, 2021.
[40] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman, and V. Mnih. Unsupervised learning of object keypoints for perception and control. Advances in neural information processing systems, 32, 2019.
[41] P. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomotor policy learning. IEEE Robotics and Automation Letters, 5(2):492-499, 2019.
[42] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200-212, 2021.
[43] K. Zakka, A. Zeng, P. Florence, J. Tompson, J. Bohg, and D. Dwibedi. Xirl: Cross-embodiment inverse reinforcement learning. In Conference on Robot Learning, pages 537-546. PMLR, 2022.</p>
<p>[44] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.
[45] R. Mokady, A. Hertz, and A. H. Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.
[46] Z. Gao, J. Liu, S. Chen, D. Chang, H. Zhang, and J. Yuan. Clip2tv: An empirical study on transformerbased methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 2021.
[47] H. Song, L. Dong, W.-N. Zhang, T. Liu, and F. Wei. Clip models are few-shot learners: Empirical studies on vqa and visual entailment. arXiv preprint arXiv:2203.07190, 2022.
[48] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and Y. Choi. Merlot reserve: Neural script knowledge through vision and language and sound. arXiv preprint arXiv:2201.02639, 2022.
[49] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.
[50] Y. Song, X. Fan, Y. Yang, G. Ren, and W. Pan. Large pretrained models on multimodal sentiment analysis. In Artificial Intelligence in China, pages 506-513. Springer, 2022.
[51] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau. mslam: Massively multilingual joint pre-training for speech and text. arXiv preprint arXiv:2202.01374, 2022.
[52] S. Karpagavalli and E. Chandra. A review on automatic speech recognition architecture and approaches. International Journal of Signal Processing, Image Processing and Pattern Recognition, 9(4):393-404, 2016.
[53] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, $6(2): 181-214,1994$.
[54] S. Masoudnia and R. Ebrahimpour. Mixture of experts: a literature survey. Artificial Intelligence Review, 42(2):275-293, 2014.
[55] Y. Liu, S. Albanie, A. Nagrani, and A. Zisserman. Use what you have: Video retrieval using representations from collaborative experts. BMVC, 2019.
[56] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[57] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[58] M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi, H. Namkoong, and L. Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021.
[59] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva. Places: An image database for deep scene understanding. arXiv preprint arXiv:1610.02055, 2016.
[60] B. Wu, W. Chen, Y. Fan, Y. Zhang, J. Hou, J. Liu, and T. Zhang. Tencent ml-images: A large-scale multi-label image database for visual representation learning. IEEE Access, 7:172683-172693, 2019.
[61] Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier. Language models can see: Plugging visual controls in text generation. arXiv preprint arXiv:2205.02655, 2022.
[62] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. arXiv preprint arXiv:2111.14447, 2021.
[63] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128-3137, 2015.
[64] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575, 2015.
[65] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.
[66] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780-1790, 2021.
[67] J. A. Portillo-Quintero, J. C. Ortiz-Bayliss, and H. Terashima-Marn. A straightforward framework for video retrieval using clip. In Mexican Conference on Pattern Recognition, pages 3-12. Springer, 2021.
[68] Speech-to-text: Automatic speech recognition I google cloud. https://cloud.google.com/ speech-to-text. Accessed: 2022-05-13.</p>
<p>[69] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[70] N. C. Mithun, J. Li, F. Metze, and A. K. Roy-Chowdhury. Learning joint embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pages 19-27, 2018.
[71] H. Fang, P. Xiong, L. Xu, and Y. Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.
[72] Y. Yu, J. Kim, and G. Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471-487, 2018.
[73] X. Cheng, H. Lin, X. Wu, F. Yang, and D. Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.
[74] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.
[75] Y. Li, T. Nagarajan, B. Xiong, and K. Grauman. Ego-exo: Transferring visual representations from third-person to first-person videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6943-6953, 2021.
[76] G. A. Sigurdsson, A. Gupta, C. Schmid, A. Farhadi, and K. Alahari. Charades-ego: A large-scale dataset of paired third and first person videos. arXiv preprint arXiv:1804.09626, 2018.
[77] D. Patel, R. Parikh, and Y. Shastri. Recent advances in video question answering: A review of datasets and methods. In International Conference on Pattern Recognition, pages 339-356. Springer, 2021.
[78] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.
[79] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe, P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. Davison, et al. Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages $559-568,2011$.
[80] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. arXiv preprint arXiv:2202.05263, 2022.
[81] Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. arXiv preprint arXiv:2109.05014, 2021.
[82] J. Lei, L. Yu, T. L. Berg, and M. Bansal. What is more likely to happen next? video-and-language future event prediction. arXiv preprint arXiv:2010.07999, 2020.
[83] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pages 894-906. PMLR, 2022.
[84] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. arXiv preprint arXiv:2010.14406, 2020.
[85] B. Strope, D. Beeferman, A. Gruenstein, and X. Lei. Unsupervised testing strategies for asr. 2011.
[86] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[87] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
[88] H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello. Wav2clip: Learning robust audio representations from clip. arXiv preprint arXiv:2110.11499, 2021.
[89] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Preprint, 2022.
[90] J. Dong, X. Li, C. Xu, S. Ji, Y. He, G. Yang, and X. Wang. Dual encoding for zero-example video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9346-9355, 2019.
[91] J. Dong, X. Li, and C. G. Snoek. Predicting visual features from text for image and video caption retrieval. IEEE Transactions on Multimedia, 20(12):3377-3388, 2018.
[92] M. Patrick, P.-Y. Huang, Y. Asano, F. Metze, A. Hauptmann, J. Henriques, and A. Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020.
[93] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.</p>
<p>[94] Q. Wang, Y. Zhang, Y. Zheng, P. Pan, and X.-S. Hua. Disentangled representation learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022.
[95] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze, L. Zettlemoyer, and C. Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021.
[96] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879-9889, 2020.
[97] M. Bain, A. Nagrani, G. Varol, and A. Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages $1728-1738,2021$.
[98] K. M. Kitani, T. Okabe, Y. Sato, and A. Sugimoto. Fast unsupervised ego-action learning for first-person sports videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3241-3248, 2011.
[99] M. S. Ryoo and L. Matthies. First-person activity recognition: What are they doing to me? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2730-2737, 2013.
[100] E. H. Spriggs, F. De La Torre, and M. Hebert. Temporal segmentation and activity classification from first-person sensing. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 17-24, 2009.
[101] Y. J. Lee, J. Ghosh, and K. Grauman. Discovering important people and objects for egocentric video summarization. In IEEE conference on computer vision and pattern recognition (CVPR), pages 13461353, 2012.
[102] A. Fathi, A. Farhadi, and J. M. Rehg. Understanding egocentric activities. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 407-414, 2011.
[103] H. Pirsiavash and D. Ramanan. Detecting activities of daily living in first-person camera views. In 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2847-2854, 2012.
[104] C. Li and K. M. Kitani. Pixel-level hand detection in ego-centric videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3570-3577, 2013.
[105] Y. J. Lee and K. Grauman. Predicting important objects for egocentric video summarization. International Journal of Computer Vision, 114(1):38-55, 2015.
[106] M. S. Ryoo, B. Rothrock, and L. Matthies. Pooled motion features for first-person videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 896-904, 2015.
[107] M. Ma, H. Fan, and K. M. Kitani. Going deeper into first-person activity recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1894-1903, 2016.
[108] S. Bambach, S. Lee, D. J. Crandall, and C. Yu. Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1949-1957, 2015.
[109] G. Garcia-Hernando, S. Yuan, S. Baek, and T.-K. Kim. First-person hand action benchmark with rgb-d videos and 3d hand pose annotations. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 409-419, 2018.
[110] Y. Li, M. Liu, and J. M. Rehg. In the eye of beholder: Joint learning of gaze and actions in first person video. In Proceedings of the European conference on computer vision (ECCV), pages 619-635, 2018.
[111] E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen. Epic-fusion: Audio-visual temporal binding for egocentric action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5492-5501, 2019.
[112] A. Furnari and G. M. Farinella. What would you expect? anticipating egocentric actions with rollingunrolling lstms and modality attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6252-6261, 2019.
[113] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European Conference on Computer Vision (ECCV), pages 720-736, 2018.
[114] L. Smaira, J. Carreira, E. Noland, E. Clancy, A. Wu, and A. Zisserman. A short note on the kinetics-7002020 human action dataset. arXiv preprint arXiv:2010.10864, 2020.
[115] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov, et al. The open images dataset v4. International Journal of Computer Vision, 128(7): 1956-1981, 2020.
[116] F. Petroni, T. Rocktschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.</p>
<p>[117] P. Agarwal, A. Betancourt, V. Panagiotou, and N. Daz-Rodrguez. Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models. arXiv preprint arXiv:2003.11743, 2020.
[118] C. Fan, Z. Zhang, and D. J. Crandall. Deepdiary: Lifelogging image captioning and summarization. Journal of Visual Communication and Image Representation, 55:40-55, 2018.
[119] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman. Vggsound: A large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721-725. IEEE, 2020.
[120] A.-M. Oncescu, A. Koepke, J. F. Henriques, Z. Akata, and S. Albanie. Audio retrieval with natural language queries. arXiv preprint arXiv:2105.02192, 2021.
[121] M. Barbieri, L. Agnihotri, and N. Dimitrova. Video summarization: methods and landscape. In Internet Multimedia Management Systems IV, volume 5242, pages 1-13. International Society for Optics and Photonics, 2003.
[122] A. G. Del Molino, C. Tan, J.-H. Lim, and A.-H. Tan. Summarization of egocentric videos: A comprehensive survey. IEEE Transactions on Human-Machine Systems, 47(1):65-76, 2016.
[123] E. Apostolidis, E. Adamantidou, A. I. Metsai, V. Mezaris, and I. Patras. Video summarization using deep neural networks: A survey. Proceedings of the IEEE, 109(11):1838-1863, 2021.
[124] M. S. Ryoo. Human activity prediction: Early recognition of ongoing activities from streaming videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1036-1043, 2011.
[125] M. Hoai and F. De la Torre. Max-margin early event detectors. International Journal of Computer Vision, 107(2):191-202, 2014.
[126] N. Rhinehart and K. M. Kitani. First-person activity forecasting with online inverse reinforcement learning. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 3696-3705, 2017.
[127] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert. Activity forecasting. In European Conference on Computer Vision (ECCV), pages 201-214, 2012.
[128] C. Vondrick, H. Pirsiavash, and A. Torralba. Anticipating visual representations from unlabeled video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 98-106, 2016.
[129] F. Abuzaid, G. Sethi, P. Bailis, and M. Zaharia. To index or not to index: Optimizing exact maximum inner product search. In 35th IEEE International Conference on Data Engineering, ICDE 2019, Macao, China, April 8-11, 2019, pages 1250-1261. IEEE, 2019. doi: 10.1109/ICDE.2019.00114. URL https: //doi.org/10.1109/ICDE.2019.00114.
[130] A. Shrivastava and P. Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2321-2329, 2014. URL https://proceedings.neurips.cc/paper/2014/hash/ 310ce61c90f3a46e340ee8257bc70e93-Abstract.html.
[131] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarls, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= Ua6zuk@WRN.
[132] H. Ramsauer, B. Schfl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, T. Adler, D. P. Kreil, M. K. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield networks is all you need. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=tL89RnzIiCd.
[133] A. S. Rawat, J. Chen, F. X. Yu, A. T. Suresh, and S. Kumar. Sampled softmax with random fourier features. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13834-13844, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ e43739bba7cdb577e9e3e4e42447f5a5-Abstract.html.
[134] K. Choromanski, H. Chen, H. Lin, Y. Ma, A. Sehanobish, D. Jain, M. S. Ryoo, J. Varley, A. Zeng, V. Likhosherstov, D. Kalashnikov, V. Sindhwani, and A. Weller. Hybrid random features. to appear in ICLR 2022, abs/2110.04367, 2021. URL https://arxiv.org/abs/2110.04367.
[135] A. Reuther, P. Michaleas, M. Jones, V. Gadepally, S. Samsi, and J. Kepner. Survey of machine learning accelerators. In 2020 IEEE high performance extreme computing conference (HPEC), pages 1-12. IEEE, 2020.
[136] X. Lin, Y. Rivenson, N. T. Yardimci, M. Veli, Y. Luo, M. Jarrahi, and A. Ozcan. All-optical machine learning using diffractive deep neural networks. Science, 361(6406):1004-1008, 2018.</p>
<h1>Appendix for Socratic Models</h1>
<h2>A Overview</h2>
<p>The appendix includes: (i) unsupervised evaluation for model selection, (ii) additional notes on main experiments, (iii) more details on applications to egocentric perception, (iv) scaling video search for language-based world-state history, (v) more details on robot perception and planning experiments, (vi) additional discussion on future work (e.g., SMs for deductive reasoning) and (vii) broader impacts (e.g., energy and resource consumption). For code, see socraticmodels.github.io.</p>
<h2>B Unsupervised Socratic Model Selection</h2>
<p>The combination of complementary models, in which one may compensate for the weaknesses of the other, opens an interesting avenue for unsupervised evaluation of model performance. Since our metric of interest is the combined performance of e.g., a VLM and a LM - rather than asking the question: '(A): how well does this VLM perform in absolute?' for SMs, we can instead ask: '(B): how well does this VLM compensate for the weakness of the LM?'.</p>
<p>Strope et al. [85] proposes a scheme which does so without requiring any evaluation ground truth. They also find that asking question (B) correlates well with answers to question (A), and is useful e.g., for model selection. The method assumes you have access to a weak (wLM) and a strong (sLM) LM (respectively VLM if evaluating the LM's performance). Asking "how well does this VLM compensate for the weaknesses of the LM" is equivalent to asking: "if we have a collection of VLMs, and we combine them with a weak LM, which model is going to perform the closest to the combination of the VLM with a strong LM?" If a VLM combined with a weak LM, instead of a strong one, makes up for the LM's shortcomings and still performs well in combination, then it may serve as a better component in the context of this combined system.</p>
<p>The benefit of this approach - while not entirely making up for doing absolute evaluations against a ground truth - is that because it only measures relative distance between model outputs, it can be performed unsupervised without annotated data: the distance between the output of the weak and strong combination can be measured using measures of semantic distance, for instance here by scoring them against a distinct, held-out language model.</p>
<p>As an example of using this approach, we extend the method in Strope et al. [85] to Socratic Models on egocentric perception, where we show it is possible to quantify the mutual dependence between foundation models without ground truth data. Specifically, to evaluate a new VLM (VLM') for generating language-based world-state history, we first use a baseline VLM VLM paired with the strong LM (sLM) to generate pseudo ground truth predictions $V L M \times s L M$. We then take both the baseline VLM VLM and new VLM $V L M^{\prime}$, and pair them with a weak LM wLM to generate predictions $V L M \times w L M$ and $V L M^{\prime} \times w L M$ respectively. We score these predictions (per image summary) against the pseudo ground truth $V L M \times s L M$. Since the outputs are linguistic, we can measure the similarity of a given prediction to the ground truth, by comparing their sentence embeddings produced by another language model e.g., RoBERTa [69]. It is important to use a distinct LM for scoring to avoid spurious correlations with the models under evaluation.</p>
<p>Tab. 5 shows example results of this analysis with GPT-3 "Davinci" as the $s L M$, and GPT-3 "Curie" as the $w L M$, to compare VLM (i.e., CLIP) variants with different backbones: vision transformers (ViT) [86] and ResNets (RN50) [87] with different model sizes. We find that this method can capture a correlation of ascending performance curve with increasingly better VLMs (e.g., better variants of CLIP) [3], as measured by zero-shot image classification accuracy on ImageNet [29] - with correlation coefficients of 0.41 and 0.46 between ImageNet accuracies and mean similarity to truth models via ViT-B/16 and RN50x16 respectively. We find that with our SM system for egocentric perception (and in contrast to the original setting in [85]), it is necessary to use a third baseline VLM $b V L M \times s L M$ to generate the pseudo ground truth, instead of $V L M \times s L M$. This is because the SM combinations that use the same VLM as the one that generates ground truth are biased to produce similar visual grounding results and can exhibit an unfair advantage during the comparisons. Those numbers in our tests have been grayed out in Tab. 5.</p>
<h1>C Additional Notes on Experiments</h1>
<p>Choice of models. There are many options of large pretrained "foundation" [4] models to choose from, but our experiments in the main paper use models that are publicly available, so that our systems can be made accessible to the community. In particular, we use CLIP [3] as the text-image similarity VLM (ViT-L/14 with 428M params, except on MSR-VTT which uses ViT-B/32), ViLD [65] as the open-vocabulary object detector VLM; Wav2CLIP [88] as the sound-critic ALM and Google Cloud Speech-to-text API [68] as the speech-to-text ALM; GPT-3 with 175B params [2, 89] and RoBERTa [69] with 355M params as the LMs. All pretrained models are used off-the-shelf with no additional finetuning. In terms of compute resources required, all experiments can be run on a single machine using an NVIDIA V100 GPU with internet access for outsourced API calls (e.g., GPT-3 and Google Cloud Speech-to-text).</p>
<h2>C. 1 Image Captioning on MS COCO</h2>
<p>For image captioning experiments on the MS COCO dataset $[15,16]$, we evaluate over a random sampled subset of 100 images from the test split [63], so that GPT-3 API runtime costs are more affordable for reproducibility ( $\sim \$ 150$ USD per run with with $n=20$ generated candidate captions per image). Metrics (shown in Tab. 6) from baselines reported on this subset of MS COCO test examples are comparable to the full test set metrics. Also, while the captions in Fig. 3, Section 4.1, were generated with the prompt "... creative short..." as noted in Fig. 3, for best quantitative MS-COCO captions we used the prompt "... short, likely...".</p>
<h2>C. 2 Contextual Image Captioning on Concadia</h2>
<p>Our experiments on Concadia [17] evaluate the extent to which SMs can generate captions and descriptions conditioned on input images and their associated article text. While our results show that the SM combination of VLMs and LMs can achieve strong results on the benchmark, we also observe that LMs (e.g., GPT-3) alone can return surprisingly competitive results too (Tab. 7). Specifically, using the same LM prompt from the SM approach, but leaving out information from the VLM:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">I am an intelligent image captioning bot. The article is about: '(article_text)'. In this image, I think I see a ${\underline{\text { object1} }},{\underline{\text { object2} }},{\underline{\text { object3} }} . . . .$ A short caption for this image is:</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Caption Generation</td>
<td style="text-align: center;">Description Generation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Kreiss et al. [17]</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SMs (ours)</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SMs (no image)</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SMs w/ description</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: SMs on zero-shot contextual image captioning and description tasks on the Concadia dataset.</p>
<p>texts that can be leveraged by an LM alone, and/or (iii) there may exist overlap between Concadia (e.g., Wikipedia articles) and the training set of the LM, which warrants further investigation to disentangle confounding variables.</p>
<h1>C. 3 Video-to-text Retrieval on MSR-VTT 1k-A</h1>
<p>We also report results in Tab. 8 on the popular MSR-VTT " $1 \mathrm{k}-\mathrm{A}$ " subset, introduced by Yu et al. [72] created via random sampling on the full test set. We follow the same evaluation protocol for video-to-text retrieval as used in prior work [55, 90, 91, 70], which reports the minimum rank among all valid text captions for a given video query, and each test video is associated with 20 captions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">MSR-VTT 1k-A</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Audio</th>
<th style="text-align: center;">CLIP enc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">R@1 $\uparrow$</td>
<td style="text-align: center;">R@5 $\uparrow$</td>
<td style="text-align: center;">R@10 $\uparrow$</td>
<td style="text-align: center;">MdR $\downarrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Finetuning</td>
<td style="text-align: center;">Collaborative Experts [55]</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SSB [92]</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP4Clip [93]</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-V/32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP2Video [71]</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-V/32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRL [94], ViT-B/32</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-V/32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAMoE [73]</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-B/32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP2TV [46]</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-B/16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRL [94], ViT-B/16 + QB-n</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-B/16</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot</td>
<td style="text-align: center;">SSB [92], zero-shot</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLIP via [67]</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">ViT-B/32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SMs (ours)</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">ViT-B/32</td>
</tr>
</tbody>
</table>
<p>Table 8: Video-to-text retrieval results on MSR-VTT [18] dataset on the 1k-A [72] subset. Differentiated are methods which train on the MSR-VTT dataset (finetuning), compared with zero-shot methods, which do not. Also noted: whether the methods use audio channels, and if CLIP [3] is used, which CLIP encoder is used.</p>
<p>Note that the original CLIP baseline for video-to-text retrieval via Portillo-Quintero et al. [67] reports R@1 to be 27.2, but this was computed with only 1 caption per video that was random sampled [72] from the original set of 20 captions (for text-to-video retrieval). This differs from the original evaluation protocol and may be sub-optimal since the sampled caption can be ambiguous or partial (generated from crowd compute). For example, videos may be paired with a vague caption "a person is explaining something" as ground truth, rather than one of the other (more precise) captions e.g., "a person is talking about importing music to a ipod". Upon correcting the evaluation protocol (i.e., increasing the number of associated captions per video to 20), R@1 for Portillo-Quintero et al. [67] improves to 58.0 , and SMs improve on top of that with LMs and ALMs ${ }^{4}$ to 60.7 R@1 zero-shot.</p>
<p>Other methods have also evaluated on zero-shot MSR-VTT text-to-video retrieval [95, 96, 97], but these have all been outperformed by Portillo-Quintero et al. [67]. Our method may be adapted as well to text-to-video, but due to our use of transcripts on only a subset of the videos, unlike in video-to-text, this creates an asymmetry which may require an unwieldy relative weighting for ranking videos with or without transcripts. Note that (Tab. 8) prior to the CLIP revolution in video-to-text retrieval, using the audio modality was not uncommon amongst competitive video-to-text retrieval methods [70, 55]. The trend over the past year, however, has been to instead focus on using only visual features, with all recent competitive methods being based off of CLIP, and not using audio data. Our approach, through leveraging commonsense reasoning stored in the LMs, is able to once again allow audio data to enable progress in this common video understanding task, beyond what CLIP alone can provide.</p>
<h2>D Egocentric Perception Appendix</h2>
<p>Background. Egocentric perception continues to be an important problem in computer vision. Early work in the area explores hand-designed first-person visual features for egocentric action recognition, object understanding, and video summarization. This includes ego-motion (e.g., optical flows) [98, 99] as well as features from human gaze, hands, and objects [100, 101, 102, 103, 104, 105]. Focusing on hand-designed features was common in early egocentric vision research, as the availability of data (or videos in general) was very limited. More recent approaches in egocentric perception leverage learned feature representations, utilizing pretrained convolutional network features [106], finetuning</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: On various egocentric perceptual tasks (shown), this work presents a case study of SMs with visual language models (VLMs, e.g., CLIP), large language models (LMs, e.g., GPT-3, RoBERTa), and audio language models (ALMs, e.g., Wav2CLIP, Speech2Text). From video search, to image captioning; from generating freeform answers to contextual reasoning questions, to forecasting future activities - SMs can provide meaningful results for complex tasks across classically challenging computer vision domains, without any model finetuning.
them [107, 48], or training them from scratch [108] with first-person videos. Similar to the topics explored in early work, learning of visual representations capturing human hands, objects, and eye gaze has been extensively studied [109, 110]. [111] learns multimodal embeddings (i.e., video + audio), and [112] studies future action anticipation from egocentric videos. Lack of sufficient data however, consistently remains a bottleneck - motivating researchers to construct new larger-scale egocentric video datasets including EPIC-Kitchens [113], Charades-Ego [76], and Ego4D [19].</p>
<h1>D. 1 Why Egocentric Perception?</h1>
<p>We highlight SMs on egocentric perception because it is an important yet challenging computer vision domain $[19,20,76]$ with downstream applications in augmented reality (AR) and robotics [7]. From unusual viewpoints to the lack of temporal curation - the characteristics of first-person videos are unique and not often found in existing datasets, which focus more on generic Internet content captured from third-person spectator views [29, 16, 74]. Notably, this domain shift makes it difficult for data-driven egocentric models to benefit from the standard paradigm of pretraining on third person Internet data [75, 76]. Overall, the key challenges have included how to acquire sufficient egocentric data, and/or how to make sufficient use of this data (either with dense labels, or otherwise).
Despite the challenges of egocentric perception, we find that SMs can reconcile the complementary strengths of pretrained foundation models to address these difficulties through contextual reasoning. For example, while modern activity recognition models trained on third person data might over-index to the motion of the primary person in video (making the models difficult to be adapted to first-person videos), we find that LMs like GPT-3 can suggest equally plausible activities (e.g., "receiving a package") that may be occurring given only a brief description of the scene (e.g., "front porch") and the objects detected in the image ("package, driveway, door") by a VLM. These activity suggestions are often more expressive than the class categories that can be found in typical activity recognition datasets (e.g., Charades [76], Kinetics [114]), and reflect the information already stored in the models, agnostic to the point of view. Our SM system for egocentric perception leverages these advantages, and also suggests future research directions in contextual reasoning that leverage existing language-based models without having to curate large annotated datasets.</p>
<h2>D. 2 Additional Details on Language-Based World-State History from Video</h2>
<p>In order to provide language-based reasoning capabilities for open-ended question-answering, a key aspect of our system is to describe the observed states of the world in language, with the goal of creating a language-based world-state history (Fig. 8) that can be used as context to an LM. To this end, a component of our method generates Socratic image summaries of individual video frames (Sec. 3.3-A), that can then be concatenated (along with timestamps) to form an event log (illustrated at the top and middle of Fig. 8).
3.3-A. Socratic Egocentric Image Summaries. Given an image frame as input, this component generates a natural language summary (e.g., caption) of what is occurring in the image. Our system</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Places: clean room. Objects: shorts, jeans, shirt. Common sense activities: getting dressed. Most likely: getting dressed.
SM (sure): I am getting dressed.
ClipCap: how to make a pair of jeans.</p>
<p>Places: kitchen. Objects: coffeemaker, waffle iron, kettle. Common sense activities: making coffee, making waffles. Most likely: making coffee.
SM (sure): I am making coffee, waffles, and tea.</p>
<p>ClipCap: how to clean a stove with a brush.</p>
<p>Places: living room. Objects: remote control, television, netflix. Common sense activities: watching netflix. Most likely: watching netflix.
SM (sure): I am watching netflix on the television.</p>
<p>ClipCap: this is what the house looks like from the inside.</p>
<p>Places: kitchen. Objects: refrigerator, refrigerator, dishwasher. Common sense activities: cooking, cleaning. Most likely: cooking.
SM (sure): I am cooking in a kitchen.</p>
<p>ClipCap: the refrigerator is full of food.</p>
<p>Places: kitchen. Objects: cooking spray, measuring cup, mixing bowl. Common sense activities: measuring, mixing. Most likely: mixing.
SM (sure): I am mixing a recipe.
ClipCap: the video shows the main running away from the camera.</p>
<p>ClipCap: the computer is now working on the screen.</p>
<p>Places: kitchen. Objects: sandwich, hamburger, kitchen \&amp; dining room table. Common sense activities: eating, sitting. Most likely: eating.
SM (sure): I am eating a sandwich in a kitchen.</p>
<p>ClipCap: how to make a mason jar with a lid.</p>
<p>Places: living room. Objects: fire, poster, computer monitor. Common sense activities: work on computer, look at flag, look at poster. Most likely: work on computer.
SM (sure): I am work on computer in home office.</p>
<p>ClipCap: the computer is now working on the screen.</p>
<p>Places: kitchen. Objects: sandwich, hamburger, kitchen \&amp; dining room table. Common sense activities: eating, sitting. Most likely: eating.
SM (sure): I am eating a sandwich in a kitchen.</p>
<p>ClipCap: the computer is now working on the screen.</p>
<p>Places: computer. Objects: fireplace, torch, wood-burning stove. Common sense activities: cooking, camping. Most likely: camping.
SM (sure): I am camping and can see a fireplace, torch, and wood-burning stove.</p>
<p>ClipCap: campfire in the night, slow motion.</p>
<h1>Generated Language-Based World-State History from Egocentric Video</h1>
<p>08:31 AM: Places: clean room. Objects: shorts, jeans, shirt. Activities: getting dressed. I was getting dressed.
10:17 AM: Places: kitchen. Objects: coffeemaker, waffle iron, kettle. Activities: making coffee. I was making coffee, waffles, and tea.
11:09 AM: Places: living room. Objects: remote control, television, netflix. Activities: watching netflix. I was watching netflix on the television.
21:17 PM: Places: staircase. Objects: stairs, hamster, human leg. Activities: ascending. I was ascending a staircase and see a hamster on the stairs and a human leg.
31:45 PM: Places: porch. Objects: package, porch, door. Activities: receiving. I was receiving a package.
33:24 PM: Places: kitchen. Objects: human hand, sole, human arm. Activities: washing dishes. I was washing dishes in a kitchen.
33:38 PM: Places: kitchen. Objects: refrigerator, refrigerator, dishwasher. Activities: cooking. I was cooking in a kitchen.
33:52 PM: Places: kitchen. Objects: cooking spray, measuring cup, mixing bowl. Activities: mixing. I was mixing a recipe.
34:24 PM: Places: shower. Objects: light switch, curtain, mirror. Activities: showering. I was showering and see the typical objects in a shower.
34:25 PM: Places: home office. Objects: flag, poster, computer monitor. Activities: work on computer. I was work on computer in home office.
35:16 PM: Places: kitchen. Objects: sandwich, hamburger, kitchen \&amp; dining room table. Activities: eating. I was eating a sandwich in a kitchen.
35:27 PM: Places: clean room. Objects: laptop, digital clock, desk. Activities: checking time. I was checking time and working on a laptop in a clean room.
36:31 PM: Places: field road. Objects: girl, worm, centipede. Activities: walking. I was walking in a field and see a girl and a worm.
37:01 PM: Places: yard. Objects: axe, axe, chairsae. Activities: chopping wood. I was chopping wood.
37:12 PM: Places: campsite. Objects: fireplace, torch, wood-burning stove. Activities: camping. I was camping and can see a fireplace, torch, and wood-burning stove.</p>
<h2>Open-Ended Reasoning</h2>
<h2>Recall Q\&amp;A</h2>
<p>Q: Did I eat dinner today? A: Yes, I ate dinner today. Explanation: I was seen eating a sandwich in a kitchen at 5:27 PM.
Q: Did I drive today? A: No, I did not drive today. Explanation: I was at home all day.
Q: Did I do laundry today? A: No, I did not do laundry today. Explanation: I was putting clothes away in a closet and wardrobe and drawer in a clean room.</p>
<h2>Contextual Reasoning Q\&amp;A</h2>
<p>Q: What was I doing outdoors? A: I was chopping wood in a yard. Explanation: I need wood to keep my fireplace going.
Q: Why was I chopping wood? A: I was chopping wood because I needed to get a fire going in the fireplace. Explanation: I was camping and it was getting cold.
Q: Why did I go to the front porch today? A: I went to the front porch today to receive a package. Explanation: I saw on the porch a package and knew that I was expecting it.
Q: Why were my wife and I in the living room this evening? A: My wife and I were in the living room this evening because we were watching netflix. Explanation: we have a television in the living room and netflix is a service that we can watch on it.</p>
<h2>Temporal Reasoning Q\&amp;A</h2>
<p>Q: When did I last wash my hands? A: I last washed my hands at 3:38 PM. Explanation: I was washing dishes in a kitchen.
Q: How many times did I receive a package today? A: I received a package once today. Explanation: I was receiving a package at 03:24 PM.
Q: When did I last drink coffee? A: I last drank coffee at 10:17 AM. Explanation: I was making coffee in the kitchen.
Q: When did I last watch Netflix? A: I last watched Netflix at 7:30 PM on April 6, 2020. Explanation: I was watching Netflix in a living room.
Q: How many times did I use electronics today? A: I used electronics six times today. Explanation: I was working on a laptop in a clean room, checking time and watching netflix on a television, and answering netflix on a computer monitor.
Q: When did I take a shower? A: I took a shower at 4:35 PM. Explanation: I was showering and see the typical objects in a shower.
Q: How long did I watch TV today? A: I watched TV for about 5 hours today. Explanation: I was watching netflix on the television for about 3 hours and then I watched a woman drink + in a living room for about 2 hours.</p>
<h2>Subjective Q\&amp;A</h2>
<p>Q: Was I happy today? A: I was generally happy today. Explanation: I was able to do some activities that I enjoy, such as watching netflix, working on my computer, and eating a sandwich.
Q: What was my favorite drink today? A: I drank wine in a living room with a woman. Explanation: I like to drink wine with friends.</p>
<p>Figure 8: An instantiation of the SMs framework for open-ended reasoning with egocentric perception. SMs can generate meaningful structured captions (top) for egocentric images through Socratic dialogue between VLMs (green) and LMs (blue), and qualitatively perform well versus state-of-the-art captioning models such as ClipCap [45]. Key moments from egocentric video are summarized with SMs into a language-based world-state history (middle), which can be provided as context to an LM for open-ended question answering. Results (bottom) for generated answers (blue) and model explanations (blue) suggest SMs are fairly capable of performing a variety of reasoning tasks including answering binary yes or no questions, contextual and temporal reasoning questions, as well as subjective questions.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Examples of guided multi-model exchanges (Socratic Models) for an egocentric perception system: (i, left) parsing a natural language question into search entities (with LM) to be used to find the most relevant key moments in the video (with VLM); (ii, middle) describing each key frame by detecting places and objects (VLM), suggesting commonsense activities (LM), pruning the most likely activity (VLM), then generating a natural language summary (LM) of the SM interaction; (iii, right) concatenating key frame summaries into a language-based world-state history that an LM can use as context to answer the original question.
uses a Socratic approach with guided multimodal multi-model discussion to provide answers to 3 questions that describe the visual scene: "where am I?", "what do I see?", and "what am I doing?", which are then summarized into a single caption per image frame.</p>
<ul>
<li>Where am I? For place recognition, we use a VLM to rank Places365 [59] scene categories against the image, with the top $n$ candidates (out of 365) inserted into a prefix: "Places: {place1}, {place2}, {place3}.".</li>
<li>What do I see? For object and people recognition, we use a VLM to rank OpenImages object categories [115] against the image, with the top $m$ categories (out of 600) inserted into a second prefix: "Objects: {object1}, {object2}, {object3}."</li>
<li>What am I doing? For activity recognition, we use a back-and-forth interaction between an LM and VLM: we first use an LM to infer the activities most related to the places and objects previously listed by the VLM (green):</li>
</ul>
<p>Places: {place1}, {place2}, {place3} Objects: {object1}, {object2}, {object3} Activities: activity_a, activity_b, activity_c.
We find that generating candidate activities using an LM yields more suitable descriptions of egocentric activities and interactions with first-person video, than using standard activity recognition dataset categories (e.g., from Charades or Kinetics). Activity recognition datasets are often tailored to third person videos, and can only cover a partial subset of human activities, which instead can be more holistically captured through LM reasoning [116] over the objects and places that the VLM perceives. For example, "receiving a package" is a common household activity not found in most datasets. After the LM generates candidate activities, these candidates are then fed back to the VLM and re-ranked to sort out the top $k$ activities by relevance to the key image frame: "Activities: {activity1}, {activity2}, {activity3}."</p>
<p>This process of generating candidate activities from places and objects is one way of extracting commonsense from LMs as knowledge bases [116]. Continuing the Socratic dialogue further, this can be repeated likewise to generate new relevant objects (conditioned on activities and places), as well as new places (conditioned on objects and activities). One can iterate the procedure (LM generate, VLM re-rank, repeat) to populate the set of places, objects, and activities until equilibrium (i.e., no more new entities), which generally helps to cover a broader set of places and objects that expand beyond the initial seed categories from Places365 and OpenImages. For example:</p>
<p>If I am making making pancakes, objects that I am likely to see include: a frying pan, a spatula, a bowl, milk, eggs, flour, sugar, baking powder, butter, a plate, syrup.</p>
<p>Given the final set of places, objects, and activities, we use the LM to generate an overall first-person summary of what is happening in the image. Specifically, the prompt is:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Key used parameters for Google Cloud Speech-to-Text API include 'model=video' and 'use_enhanced=True'. At 0.006 cents per 15 seconds, this represents an estimated speech-to-text processing cost of under 25 cents (USD) for all MSR-VTT test data.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
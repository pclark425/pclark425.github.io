<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-691 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-691</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-691</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-1df011fcb4c0ad43b5d1ce3b4d867ec1be114cd7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1df011fcb4c0ad43b5d1ce3b4d867ec1be114cd7" target="_blank">A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations.</p>
                <p><strong>Paper Abstract:</strong> Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less class-sensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e691.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e691.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interpretability claim vs implemented function</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claimed class-sensitive interpretability of backpropagation-based visualizations versus their implemented behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies a misalignment between the natural-language claims that backpropagation-based visualizations (saliency map, DeconvNet, Guided Backprop) reveal class-relevant pixels and the actual implemented behavior of some methods (GBP, DeconvNet) which perform (partial) image recovery and are largely class-insensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>backpropagation-based visualization pipeline (saliency map, DeconvNet, GBP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithms that produce pixel-space visualizations by propagating (modified) gradients from a class logit back to the input image; implementations differ in how forward/backward ReLU operations are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper claims / interpretability objective statements (methods and introduction)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>visualization algorithm implementations (research code for saliency map, DeconvNet, GBP)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>purpose-function mismatch ('claimed class-sensitive explanation' vs 'implemented image-recovery operation')</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers and descriptions present GBP and DeconvNet as methods to highlight class-relevant pixels (interpret model decisions). The implemented algorithms (via backward ReLU behavior and processing of gradients) instead largely recover input image structure independent of the target class, i.e., they act as (partial) image recovery filters and therefore are not faithful indicators of class-specific importance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model explanation component / visualization procedure (gradient modification rules, ReLU handling during backprop)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>theoretical analysis combined with empirical experiments: mathematical derivations (lemmas/theorems showing GBP approximates input recovery and DeconvNet becomes Gaussian noise or image recovery depending on pooling), plus empirical visual inspection across class logits, adversarial test, and ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified by (a) average L2 distances between visualizations computed for two different class logits per image (aggregated over 10k ImageNet images), showing saliency maps have much larger L2 changes than GBP/Deconv; (b) qualitative comparisons of visualizations (figures); (c) adversarial example comparison where saliency changes but GBP/Deconv remain almost unchanged; (d) theoretical bounds on number of filters N required for GBP recovery (N = Õ(p/ε^2)).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Conceptual and practical: using GBP or DeconvNet as evidence of why a network predicted a particular class can be misleading because their outputs are largely independent of the class logit; adversarial tests show GBP/Deconv outputs remain unchanged despite class changes, undermining interpretability. Quantitative impact shown via average L2 distances (saliency >> GBP/Deconv) across 10k images; theoretical guarantees show GBP can recover input when N large, explaining why outputs look like images rather than class explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple settings in the paper: trained VGG-16 and random VGG-16 (GBP and DeconvNet typically class-insensitive); prevalence depends on architecture components (e.g., GBP needs local convolutional structure and sufficient filters; DeconvNet becomes image-recovery when max-pooling is present). No global population prevalence percentage reported; statistical comparisons were performed on 10k ImageNet images.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch arises from simplified natural-language explanations (e.g., linear-model interpretation) that ignore critical implementation details—specifically how forward and backward ReLU gating is applied—and from implicit architectural dependencies (local connections, max-pooling) that dominate the visualization behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Proposed diagnostics and best practices: (1) evaluate class-sensitivity quantitatively (e.g., average L2 distance across different target logits on many images); (2) run adversarial-change tests to check whether visualizations change with class label; (3) perform ablation studies (remove pooling, vary number of filters, partially randomize weights) to reveal what parts of pipeline produce image recovery; (4) document exactly how gradient modifications (backward/forward ReLU) are implemented in code and emphasize limits of interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically benchmarked as a remedy in this paper — diagnostics were used to reveal the gap (effective for detection). The paper does not report a corrective visualization that restores faithful class-sensitivity while preserving visual quality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e691.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e691.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear-model explanation mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between linear approximation descriptions and nonlinear implementation details (ReLU handling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper points out that the common natural-language explanation that visualization methods 'approximate the network as linear' is insufficient and misrepresents the effect of nonlinearities, because different treatments of ReLU in implementations materially change visualization outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>theoretical / conceptual model descriptions vs actual gradient-modifying implementation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Natural-language explanations often describe gradient-based visualizations as visualizing learned linear weights (linearized model), while implementations differ in how nonlinear activation (ReLU) is handled during backpropagation (forward gating, backward gating, or both).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods section / conceptual explanation (linear approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>gradient-propagation implementation details (modified backprop with forward/backward ReLU gates)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/inaccurate theoretical description (oversimplified linear model ignoring nonlinear gating)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The linear-model description predicts that saliency map, DeconvNet and GBP reduce to the same output under a linear approximation; however, the implemented variants differ by applying forward ReLU masking, backward ReLU masking, or both — these non-linear gating decisions are not captured by the simplistic description and lead to qualitatively different outputs (noise vs image recovery).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model approximation and method description (theoretical justification / methods justification)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>analytical derivations (formal definitions of forward/backward ReLU functions and resulting formulae) and controlled experiments comparing methods on random and trained networks.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Mathematical theorems showing GBP approximates input x for large N (Theorem 1), while saliency and DeconvNet approximate Gaussian noise in random three-layer CNN (Theorem 2); empirical L2 and qualitative visual comparisons corroborate theoretical predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Misleading theoretical justification can cause incorrect expectations about what an implementation reveals (e.g., expecting weight visualization but getting image recovery). The paper demonstrates the impact by showing different visual outcomes and by providing proofs that the nonlinear gating (backward ReLU) changes the function class of the visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Conceptual gap affects any work relying on linear approximations of deep nets to justify visualization methods; demonstrated here across shallow and deep, random and trained CNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Over-simplification in natural-language theoretical descriptions that omit critical nonlinear implementation choices (forward vs backward ReLU gating).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Replace oversimplified linear explanations with analyses that explicitly include nonlinear operations used in implementations; when proposing visualizations, describe concretely the gating rules used and provide theoretical/empirical validation of what information is preserved.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The paper's corrected theoretical analysis successfully explains empirical observations, i.e., including backward ReLU and local connectivity in the theory explains why GBP/DeconvNet recover images.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning theory / interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e691.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e691.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Undocumented dependency on architecture (max-pooling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeconvNet's dependence on max-pooling and local connections for image recovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors identify a specific implementation-architecture dependency: DeconvNet produces image-recovery visualizations primarily when max-pooling and local convolutional structure are present, an effect not always stated in high-level descriptions of the method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeconvNet visualization implementation within CNN architectures</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DeconvNet is a modified backpropagation visualization that uses backward ReLU gating; its behavior depends on architectural components such as max-pooling and local receptive fields.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method description/algorithmic specification in papers</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>implemented DeconvNet visualization code (including handling of pooling switches and backward ReLU)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing/implicit dependency (implementation behavior depends on architectural component not emphasized in descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>DeconvNet's ability to produce human-interpretable (image-like) visualizations relies on the presence of max-pooling (which provides pooling switches) and local connections; without max-pooling DeconvNet in a random three-layer CNN yields Gaussian-like noise, contrary to general descriptions that present DeconvNet as inherently producing clean visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture and visualization interaction (max-pooling stage and gradient-routing through pooling)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>theoretical derivation extending three-layer analysis to include max-pooling, plus controlled experiments comparing DeconvNet outputs with and without max-pooling and across random vs trained networks.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative visualization comparisons and the same average-L2 statistics used in the paper; observation that DeconvNet visualizations shift from noise to image-recovery when max-pooling is present (figures and statistics presented), and theoretical approximation showing DeconvNet with pooling approximates GBP-like form.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Mischaracterizing DeconvNet as architecture-agnostic can mislead researchers and practitioners about its generalizability; practitioners may wrongly attribute interpretability to the algorithm rather than to architecture-specific artifacts. This undermines method portability across network designs (e.g., FCNs without local connections, networks without pooling).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Dependence observed whenever max-pooling and local convolutions are present (e.g., VGG-16, ResNet with pooling shows related effects); absence of pooling (or absence of local connections) removes the effect.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>High-level method descriptions omit explicit statement of architecture-dependent behavior (pooling switches and local receptive fields alter gradient routing and hence visualization output).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>When publishing or documenting visualization methods, explicitly state architectural dependencies (e.g., need for max-pooling or pooling switches), include ablation experiments showing behavior across architectures, and provide diagnostic tests (e.g., run method with pooling disabled) to verify origin of visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Paper's ablations (adding/removing pooling, comparing CNN vs FCN) effectively reveal the dependency; no downstream corrective algorithm proposed, but the diagnostics are effective for detecting the architecture-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations', 'publication_date_yy_mm': '2018-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep inside convolutional networks: Visualising image classification models and saliency maps <em>(Rating: 2)</em></li>
                <li>Visualizing and understanding convolutional networks <em>(Rating: 2)</em></li>
                <li>Striving for simplicity: The all convolutional net <em>(Rating: 2)</em></li>
                <li>Evaluating the visualization of what a deep neural network has learned <em>(Rating: 2)</em></li>
                <li>Deconvolution and checkerboard artifacts <em>(Rating: 1)</em></li>
                <li>PatternNet and PatternLRP: improving the interpretability of neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-691",
    "paper_id": "paper-1df011fcb4c0ad43b5d1ce3b4d867ec1be114cd7",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Interpretability claim vs implemented function",
            "name_full": "Claimed class-sensitive interpretability of backpropagation-based visualizations versus their implemented behavior",
            "brief_description": "The paper identifies a misalignment between the natural-language claims that backpropagation-based visualizations (saliency map, DeconvNet, Guided Backprop) reveal class-relevant pixels and the actual implemented behavior of some methods (GBP, DeconvNet) which perform (partial) image recovery and are largely class-insensitive.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "backpropagation-based visualization pipeline (saliency map, DeconvNet, GBP)",
            "system_description": "Algorithms that produce pixel-space visualizations by propagating (modified) gradients from a class logit back to the input image; implementations differ in how forward/backward ReLU operations are applied.",
            "nl_description_type": "research paper claims / interpretability objective statements (methods and introduction)",
            "code_implementation_type": "visualization algorithm implementations (research code for saliency map, DeconvNet, GBP)",
            "gap_type": "purpose-function mismatch ('claimed class-sensitive explanation' vs 'implemented image-recovery operation')",
            "gap_description": "Papers and descriptions present GBP and DeconvNet as methods to highlight class-relevant pixels (interpret model decisions). The implemented algorithms (via backward ReLU behavior and processing of gradients) instead largely recover input image structure independent of the target class, i.e., they act as (partial) image recovery filters and therefore are not faithful indicators of class-specific importance.",
            "gap_location": "model explanation component / visualization procedure (gradient modification rules, ReLU handling during backprop)",
            "detection_method": "theoretical analysis combined with empirical experiments: mathematical derivations (lemmas/theorems showing GBP approximates input recovery and DeconvNet becomes Gaussian noise or image recovery depending on pooling), plus empirical visual inspection across class logits, adversarial test, and ablations.",
            "measurement_method": "Quantified by (a) average L2 distances between visualizations computed for two different class logits per image (aggregated over 10k ImageNet images), showing saliency maps have much larger L2 changes than GBP/Deconv; (b) qualitative comparisons of visualizations (figures); (c) adversarial example comparison where saliency changes but GBP/Deconv remain almost unchanged; (d) theoretical bounds on number of filters N required for GBP recovery (N = Õ(p/ε^2)).",
            "impact_on_results": "Conceptual and practical: using GBP or DeconvNet as evidence of why a network predicted a particular class can be misleading because their outputs are largely independent of the class logit; adversarial tests show GBP/Deconv outputs remain unchanged despite class changes, undermining interpretability. Quantitative impact shown via average L2 distances (saliency &gt;&gt; GBP/Deconv) across 10k images; theoretical guarantees show GBP can recover input when N large, explaining why outputs look like images rather than class explanations.",
            "frequency_or_prevalence": "Observed across multiple settings in the paper: trained VGG-16 and random VGG-16 (GBP and DeconvNet typically class-insensitive); prevalence depends on architecture components (e.g., GBP needs local convolutional structure and sufficient filters; DeconvNet becomes image-recovery when max-pooling is present). No global population prevalence percentage reported; statistical comparisons were performed on 10k ImageNet images.",
            "root_cause": "Mismatch arises from simplified natural-language explanations (e.g., linear-model interpretation) that ignore critical implementation details—specifically how forward and backward ReLU gating is applied—and from implicit architectural dependencies (local connections, max-pooling) that dominate the visualization behavior.",
            "mitigation_approach": "Proposed diagnostics and best practices: (1) evaluate class-sensitivity quantitatively (e.g., average L2 distance across different target logits on many images); (2) run adversarial-change tests to check whether visualizations change with class label; (3) perform ablation studies (remove pooling, vary number of filters, partially randomize weights) to reveal what parts of pipeline produce image recovery; (4) document exactly how gradient modifications (backward/forward ReLU) are implemented in code and emphasize limits of interpretability.",
            "mitigation_effectiveness": "Not empirically benchmarked as a remedy in this paper — diagnostics were used to reveal the gap (effective for detection). The paper does not report a corrective visualization that restores faithful class-sensitivity while preserving visual quality.",
            "domain_or_field": "deep learning / explainable AI",
            "reproducibility_impact": true,
            "uuid": "e691.0",
            "source_info": {
                "paper_title": "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Linear-model explanation mismatch",
            "name_full": "Mismatch between linear approximation descriptions and nonlinear implementation details (ReLU handling)",
            "brief_description": "The paper points out that the common natural-language explanation that visualization methods 'approximate the network as linear' is insufficient and misrepresents the effect of nonlinearities, because different treatments of ReLU in implementations materially change visualization outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "theoretical / conceptual model descriptions vs actual gradient-modifying implementation",
            "system_description": "Natural-language explanations often describe gradient-based visualizations as visualizing learned linear weights (linearized model), while implementations differ in how nonlinear activation (ReLU) is handled during backpropagation (forward gating, backward gating, or both).",
            "nl_description_type": "methods section / conceptual explanation (linear approximation)",
            "code_implementation_type": "gradient-propagation implementation details (modified backprop with forward/backward ReLU gates)",
            "gap_type": "incomplete/inaccurate theoretical description (oversimplified linear model ignoring nonlinear gating)",
            "gap_description": "The linear-model description predicts that saliency map, DeconvNet and GBP reduce to the same output under a linear approximation; however, the implemented variants differ by applying forward ReLU masking, backward ReLU masking, or both — these non-linear gating decisions are not captured by the simplistic description and lead to qualitatively different outputs (noise vs image recovery).",
            "gap_location": "model approximation and method description (theoretical justification / methods justification)",
            "detection_method": "analytical derivations (formal definitions of forward/backward ReLU functions and resulting formulae) and controlled experiments comparing methods on random and trained networks.",
            "measurement_method": "Mathematical theorems showing GBP approximates input x for large N (Theorem 1), while saliency and DeconvNet approximate Gaussian noise in random three-layer CNN (Theorem 2); empirical L2 and qualitative visual comparisons corroborate theoretical predictions.",
            "impact_on_results": "Misleading theoretical justification can cause incorrect expectations about what an implementation reveals (e.g., expecting weight visualization but getting image recovery). The paper demonstrates the impact by showing different visual outcomes and by providing proofs that the nonlinear gating (backward ReLU) changes the function class of the visualization.",
            "frequency_or_prevalence": "Conceptual gap affects any work relying on linear approximations of deep nets to justify visualization methods; demonstrated here across shallow and deep, random and trained CNNs.",
            "root_cause": "Over-simplification in natural-language theoretical descriptions that omit critical nonlinear implementation choices (forward vs backward ReLU gating).",
            "mitigation_approach": "Replace oversimplified linear explanations with analyses that explicitly include nonlinear operations used in implementations; when proposing visualizations, describe concretely the gating rules used and provide theoretical/empirical validation of what information is preserved.",
            "mitigation_effectiveness": "The paper's corrected theoretical analysis successfully explains empirical observations, i.e., including backward ReLU and local connectivity in the theory explains why GBP/DeconvNet recover images.",
            "domain_or_field": "deep learning theory / interpretability",
            "reproducibility_impact": true,
            "uuid": "e691.1",
            "source_info": {
                "paper_title": "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations",
                "publication_date_yy_mm": "2018-05"
            }
        },
        {
            "name_short": "Undocumented dependency on architecture (max-pooling)",
            "name_full": "DeconvNet's dependence on max-pooling and local connections for image recovery",
            "brief_description": "The authors identify a specific implementation-architecture dependency: DeconvNet produces image-recovery visualizations primarily when max-pooling and local convolutional structure are present, an effect not always stated in high-level descriptions of the method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeconvNet visualization implementation within CNN architectures",
            "system_description": "DeconvNet is a modified backpropagation visualization that uses backward ReLU gating; its behavior depends on architectural components such as max-pooling and local receptive fields.",
            "nl_description_type": "method description/algorithmic specification in papers",
            "code_implementation_type": "implemented DeconvNet visualization code (including handling of pooling switches and backward ReLU)",
            "gap_type": "missing/implicit dependency (implementation behavior depends on architectural component not emphasized in descriptions)",
            "gap_description": "DeconvNet's ability to produce human-interpretable (image-like) visualizations relies on the presence of max-pooling (which provides pooling switches) and local connections; without max-pooling DeconvNet in a random three-layer CNN yields Gaussian-like noise, contrary to general descriptions that present DeconvNet as inherently producing clean visualizations.",
            "gap_location": "model architecture and visualization interaction (max-pooling stage and gradient-routing through pooling)",
            "detection_method": "theoretical derivation extending three-layer analysis to include max-pooling, plus controlled experiments comparing DeconvNet outputs with and without max-pooling and across random vs trained networks.",
            "measurement_method": "Qualitative visualization comparisons and the same average-L2 statistics used in the paper; observation that DeconvNet visualizations shift from noise to image-recovery when max-pooling is present (figures and statistics presented), and theoretical approximation showing DeconvNet with pooling approximates GBP-like form.",
            "impact_on_results": "Mischaracterizing DeconvNet as architecture-agnostic can mislead researchers and practitioners about its generalizability; practitioners may wrongly attribute interpretability to the algorithm rather than to architecture-specific artifacts. This undermines method portability across network designs (e.g., FCNs without local connections, networks without pooling).",
            "frequency_or_prevalence": "Dependence observed whenever max-pooling and local convolutions are present (e.g., VGG-16, ResNet with pooling shows related effects); absence of pooling (or absence of local connections) removes the effect.",
            "root_cause": "High-level method descriptions omit explicit statement of architecture-dependent behavior (pooling switches and local receptive fields alter gradient routing and hence visualization output).",
            "mitigation_approach": "When publishing or documenting visualization methods, explicitly state architectural dependencies (e.g., need for max-pooling or pooling switches), include ablation experiments showing behavior across architectures, and provide diagnostic tests (e.g., run method with pooling disabled) to verify origin of visual features.",
            "mitigation_effectiveness": "Paper's ablations (adding/removing pooling, comparing CNN vs FCN) effectively reveal the dependency; no downstream corrective algorithm proposed, but the diagnostics are effective for detecting the architecture-dependence.",
            "domain_or_field": "deep learning / explainable AI",
            "reproducibility_impact": true,
            "uuid": "e691.2",
            "source_info": {
                "paper_title": "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations",
                "publication_date_yy_mm": "2018-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "rating": 2
        },
        {
            "paper_title": "Visualizing and understanding convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Striving for simplicity: The all convolutional net",
            "rating": 2
        },
        {
            "paper_title": "Evaluating the visualization of what a deep neural network has learned",
            "rating": 2
        },
        {
            "paper_title": "Deconvolution and checkerboard artifacts",
            "rating": 1
        },
        {
            "paper_title": "PatternNet and PatternLRP: improving the interpretability of neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01436925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations</h1>
<p>Weili Nie ${ }^{1}$ Yang Zhang ${ }^{2}$ Ankit B. Patel ${ }^{13}$</p>
<h4>Abstract</h4>
<p>Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.</p>
<h2>1. Introduction</h2>
<p>Driven by massive data and computational resources, modern convolutional neural networks (CNNs) and other network architectures have achieved many outstanding results, such as image recognition (Krizhevsky et al., 2012), neural machine translation (Sutskever et al., 2014), and playing Go games (Silver et al., 2016), etc. Despite their extensive applications, these neural networks are always considered as black boxes. Interpretability used to be for its own sake; now, due to safety-critical applications such as self-driving cars and tumor diagnosis, it is no longer satisfying to have a black box that is unaccountable for its decisions. The demand for explainable artificial intelligence (XAI) (Gunning, 2017) - human interpretable explanations of model decisions - has driven the development of visualization techniques, including image synthesis via activation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>maximization (Simonyan et al., 2013; Johnson et al., 2016; Nguyen et al., 2016) and backpropagation-based visualizations (Simonyan et al., 2013; Zeiler \&amp; Fergus, 2014; Springenberg et al., 2014; Shrikumar et al., 2017; Kindermans et al., 2017).</p>
<p>The basic idea of backpropagation-based visualizations is to highlight class-relevant pixels by propagating the network output back to the input image space. The intensity changes of these pixels have the most significant impact on network decisions. Specifically, (Simonyan et al., 2013) visualizes the spatial support of a given class in a given image, i.e. saliency map, by using the true gradient which masks out negative entries of bottom data via the forward ReLU. Despite its simplicity, the results of saliency map are normally very noisy which makes the interpretation difficult. (Zeiler \&amp; Fergus, 2014) visualize the reverse mapping from feature activities back to the input pixel space with the deconvolutional network (DeconvNet) method. The basic idea of DeconvNet is to mask out negative entries of the top gradients by resorting to the backward ReLU. (Springenberg et al., 2014) proposed the Guided Backpropagation (GBP) method which combines the above two methods: by considering both the forward and backward ReLUs, it masks out the values for which either top gradients or bottom data are negative and produces sharper visualizations. More recently, DeepLift (Shrikumar et al., 2017) and PatternNet (Kindermans et al., 2017) have been proposed to further improve the visual quality of backpropagation-based methods.</p>
<p>This class of backpropagation-based visualizations, in particular GBP and DeconvNet, has attracted a lot of attention in both the deep learning community and other fields (Szegedy et al., 2013; Dosovitskiy \&amp; Brox, 2016; Selvaraju et al., 2016; Fong \&amp; Vedaldi, 2017; Kraus et al., 2016). Despite their good visual quality, the question of how they are actually related to the decision-making has remained largely unexplored. Do the pretty visualizations actually tell us reliably about what the network is doing internally? Our experiments have confirmed previous observations (Mahendran \&amp; Vedaldi, 2016; Selvaraju et al., 2016; Samek et al., 2017) that saliency map is indeed very sensitive to the change of class labels, while GBP and DeconvNet, though their visualization results are much cleaner than saliency map, remain almost the same given different class labels. It seems that</p>
<p>the visual quality improvement of backpropagation-based methods is sacrificing the ability of highlighting important pixels to a specific output class. In this sense, GBP and DeconvNet may be unreliable in interpreting how deep neural networks make classification decisions.</p>
<p>The most commonly used explanation for these visualizations is to approximate the neural networks with a linear function (Simonyan et al., 2013; Kindermans et al., 2017), where the derivative of output with respect to input image is just the weight vector of the model. In such sense, the backpropagation-based methods can be regarded as visualizing the learned weights. But apparently the approximate linear model is too simplistic to reflect the highly nonlinear property of deep neural networks. For example, GBP and DeconvNet essentially apply the same algorithm as saliency map, but treat ReLU, the nonlinear activation, differently. The linear model explanation thus cannot answer questions regarding why GBP and DeconvNet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear model. Therefore, we need a more complex model, which should at least capture the impact of both forward ReLU and backward ReLU, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, we can extract from these visualizations.</p>
<p>Our contributions. We provide a theoretical explanation for why GBP and DeconvNet generate more humaninterpretable but less class-sensitive visualizations than saliency map. Specifically, our analysis reveals that GBP and DeconvNet are essentially doing (partial) image recovery instead of highlighting class-relevant pixels or visualizing the learned weights, which means in principle they are unrelated to the decision-making of neural networks. We also find that it is the backward ReLU introduced by either GBP or DeconvNet, together with the local connections in CNNs that results in crisp visualizations. In particular, we explain how DeconvNet also relies on the max-pooling to recover the input. Finally, we do extensive experiments to support our theory and further reveal more detailed properties of these backpropagation-based visualizations ${ }^{1}$.</p>
<h2>2. Backpropagation-based Visualizations</h2>
<p>In this section, we first give formal definitions of backpropagation-based visualizations: saliency map, DeconvNet and GBP, and then compare their empirical behaviors.</p>
<h3>2.1. Formal Definitions</h3>
<p>The key difference of backpropagation-based methods is the way they propagate the output score back through the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Illustration of how backpropagation-based methods propagate back through the $i$-th nonlinear activation in the $l$-th layer with input $y_{i}^{(l)}$ and output $o_{i}^{(l)}$, where $T_{i}^{(l)}$ denotes the (modified) gradient after passing through the activation and $R_{i}^{(l)}$ denotes the top gradient before the activation.</p>
<p>ReLU activations. As illustrated by Figure 1, we consider the $i$-th ReLU activation in the $l$-th layer with its input $y_{i}^{(l)}$ and its output $o_{i}^{(l)}$ and denote by $\sigma(t)=\max (t, 0)$ the ReLU activation. Also, denote by $R_{i}^{(l)}$ the top gradient before activation, i.e., gradient of the output score with respect to $o_{i}^{(l)}$ and denote by $T_{i}^{(l)}$ the (modified) gradient after activation, i.e., gradient of the output score with respect to $y_{i}^{(l)}$. Then in the gradient calculations, the corresponding forward ReLU could be formally defined as a function</p>
<p>$$
\sigma_{f, i}^{(l)}(t) \triangleq \mathbb{I}\left(y_{i}^{(l)}\right) t
$$</p>
<p>where $\mathbb{I}(\cdot)$ is the indicator function and the corresponding backward ReLU could be formally defined as a function</p>
<p>$$
\sigma_{b, i}^{(l)}(t) \triangleq \mathbb{I}\left(R_{i}^{(l)}\right) t
$$</p>
<p>Therefore, the formal definition of backpropagation-based methods for propagating the output score back through the $i$-th ReLU activation in the $l$-th layer is</p>
<p>$$
T_{i}^{(l)}= \begin{cases}\sigma_{f, i}^{(l)}\left(R_{i}^{(l)}\right) &amp; \text { for saliency map } \ \sigma_{b, i}^{(l)}\left(R_{i}^{(l)}\right) &amp; \text { for DeconvNet } \ \sigma_{f, i}^{(l)}\left(\sigma_{b, i}^{(l)}\left(R_{i}^{(l)}\right)\right) &amp; \text { for GBP }\end{cases}
$$</p>
<p>which can be further uniformly formulated as</p>
<p>$$
T_{i}^{(l)}=h\left(R_{i}^{(l)}\right) \frac{\partial g\left(y_{i}^{(l)}\right)}{\partial y_{i}^{(l)}}
$$</p>
<p>where the two functions $h(\cdot)$ and $g(\cdot)$ are defined as</p>
<p>$$
\begin{aligned}
&amp; h(t)= \begin{cases}t &amp; \text { for saliency map } \
\sigma(t) &amp; \text { for DeconvNet and GBP }\end{cases} \
&amp; g(t)= \begin{cases}t &amp; \text { for DeconvNet } \
\sigma(t) &amp; \text { for saliency map and GBP }\end{cases}
\end{aligned}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Backpropagation-based visualizations for the trained VGG-16 net given an input "tabby". From top row to the last row, it is saliency map, DeconvNet and GBP, where "max" refers to computing the (modified) gradient for the maximum class logit and the number, say "482", refers to computing the (modified) gradient for the 482 -th logit. These numbers are randomly chosen for generality. Best viewed in the electronic version.</p>
<h3>2.2. Empirical Observations</h3>
<p>To be a good visualization method, a clean and visually human-interpretable result is very desirable. More importantly, it should also reveal how the neural networks make decisions. Based on this, we provide the empirical behaviors of the backpropagation-based visualizations for a pretrained VGG-16 net (Simonyan \&amp; Zisserman, 2014) in Figure 2. Without loss of generality, the visualizations are obtained by choosing one of the class logits (i.e. the unnormalized class probability output right before the softmax function) as the output score to be taken derivative with respect to the input image.</p>
<p>For the visual quality, saliency map is very noisy while DeconvNet and GBP produce human-interpretable visualizations with a subtle difference: DeconvNet unexpectedly produces some kind of texture-like pattern, and GBP is cleaner with some background information filtered out. For the class-sensitivity, saliency map changes greatly for different class logits while DeconvNet and GBP are almost invariant to which class logit we choose. This, together with more experiments, suggests that after introducing the backward ReLU, both DeconvNet and GBP modify the true gradient in a way that they create much cleaner results but their functionality as an indicator of important pixels to a specific class has disappeared. In the next section, we will explain these empirical behaviors and discuss the reason why GBP and DeconvNet differ greatly from saliency map.</p>
<h2>3. Theoretical Explanations</h2>
<p>We first analyze the backpropagation-based methods in a three-layer CNN with random Gaussian weights, which is then extended to more complicated models such as CNNs with max-pooling and deep CNNs. Besides, we also investigate their behaviors in well-trained CNNs.</p>
<h3>3.1. A Random Three-Layer CNN</h3>
<p>Consider a three-layer CNN, consisting of an input layer and a convolutional hidden layer, followed by a ReLU activation function and a fully connected layer of which its output is called class logits. Formally, let $x \in \mathbb{R}^{d}$ be a normalized input image with dimension $d$ and $|x|=1$, and let $W \in \mathbb{R}^{p \times N}$ be $N$ convolutional filters where each column $w^{(i)}$ denotes the $i$-th filter with size $p$. Note that here we use vectors to represent images and filters for simplicity, and the analysis also works for the more practical two-dimensional case. Then, we let $Y \in \mathbb{R}^{p \times J}$ be $J$ image patches extracted from $x$, and each column $y^{(j)}$ with size $p$ is generated by a linear function $y^{(j)}=D_{j} x$ where $D_{j} \triangleq\left[\begin{array}{lll}0_{p \times(j-1) b} &amp; I_{p \times p} &amp; 0_{p \times(d-(j-1) b-p)}\end{array}\right]$ with $b$ being the stride size ${ }^{2}$. For example, given a filter with size 3 and stride 1 , the resulting $j$-th patch $y^{(j)}$ is made of the $j$-th to $(j+2)$-th consecutive pixels. The weights in the fullyconnected layer can be represented by $V \in \mathbb{R}^{N J \times K}$ with $K$ being the number of output logits. Therefore, the $k$-th logit is represented by</p>
<p>$$
f_{k}(x)=\sum_{i=1}^{N} \sum_{j=1}^{J} V_{q_{i j}, k} \sigma\left(w^{(i) T} y^{(j)}\right)
$$</p>
<p>where the index $q_{i j}$ denotes the $((i-1) J+j)$-th entry in every column vector of weight matrix $V$.</p>
<p>Assume every entry of $V$ and $W$ is sampled from an i.i.d. Gaussian distribution $\mathcal{N}\left(0, c^{2}\right)$. The following lemma provides the formula for backpropagation-based visualizations in a random three-layer CNN. Note that the norm of the final results will be in the range of $[0,1]$ as we apply the normalization during visualizations.
Lemma 1. The backpropagation-based visualizations for the $k$-th logit in a random three-layer CNN is formalized as</p>
<p>$$
s_{k}(x)=\frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)}
$$</p>
<p>where $Z_{k}$ is the normalization coefficient to ensure $\left|s_{k}(x)\right| \in[0,1], h(\cdot)$ is given by Eq. (2) and
$\tilde{w}^{(i, j)}= \begin{cases}w^{(i)} &amp; \text { for DeconvNet } \ w^{(i)} \mathbb{I}\left(w^{(i) T} y^{(j)}\right) &amp; \text { for saliency map and } G B P\end{cases}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Proof. See Appendix A.
Next, we can analyze the different behaviors of these backpropagation-based methods case by case.</p>
<h3>3.1.1. GUIDED BACKPROPAGATION</h3>
<p>First, the behavior of GBP is given as follows.
Theorem 1. In a random three-layer CNN, if the number of filters $N$ is sufficiently large, GBP at the $k$-th logit can be approximated as</p>
<p>$$
s_{k}^{G B P}(x) \approx x
$$</p>
<p>Proof. See Appendix B.
The above theorem shows that after introducing the backward ReLU, the input image can be approximately recovered by GBP in a random three-layer CNN, regardless of the class label. However, according to the linear model explanation, backpropagation-based methods are visualizing learned weights, which should be random noise as they are all sampled from i.i.d Gaussians. Obviously, it is inconsistent with the actual behavior of GBP.</p>
<p>As the approximation in Eq. (5) builds on an assumption that the number of filters $N$ is sufficiently large, a key question is: How many filters are needed to guarantee an accurate recovery? From (Lugosi \&amp; Mendelson, 2017), we can set $N=\tilde{O}\left(\frac{p}{\epsilon^{2}}\right)$ such that with high probability $\left|\frac{1}{N} \sum_{i=1}^{N} \tilde{w}^{(i, j)}-\mathbb{E}\left[\tilde{w}^{(i, j)}\right]\right|&lt;\epsilon$, where $p$ denotes the filter size and $\tilde{O}(\cdot)$ hides some other factors. As an upper bound, it reveals that the number of convolutional filters needed heavily depends on the filter size $p$. As the filter size intrinsically determined by the local connections in CNNs is usually small, we could use a mild number of convolutional filters to recover the input image. For example, given a filter size $3 \times 3 \times 3$, we need at most $O\left(10^{3}\right)$ filters to achieve an estimation error $\epsilon$ less than 0.1 . This strongly suggests that GBP visualizations are human-interpretable in most of the CNNs, and thus the local connections property is another key factor underlying crisp visualizations.</p>
<h3>3.1.2. SALIENCY MAP AND DECONVNET</h3>
<p>Here we show the behaviors of saliency map and DeconvNet in a random three-layer CNN are largely different from GBP.
Theorem 2. In a random three-layer CNN, if the number of filters $N$ is sufficiently large, saliency map and DeconvNet are approximated as Gaussian random variables satisfying</p>
<p>$$
s_{k}^{\text {Sal }}(x), s_{k}^{\text {Deconv }}(x) \sim \mathcal{N}(0, I)
$$</p>
<p>Proof. See Appendix C.
The above theorem shows that both saliency map and DeconvNet visualizations will yield random noise, conveying
little information about the input image and class logits. For saliency map, it is easily understood since saliency map represents the true gradient of the class logit, which heavily depends on the weights. For DeconvNet, although its behavior appears similar to saliency map in this simplistic scenario, we will show later on that it behaves more similarly to GBP, in particular with the existence of max-pooling.</p>
<h3>3.2. Extensions to More Realistic Models</h3>
<p>In this section, we extend our analysis of a simple random three-layer CNN to other more realistic cases, including the max-pooling, deeper nets and trained weights.</p>
<h3>3.2.1. CNNS WITH MAX-POOLING</h3>
<p>If we add a max-pooling layer between the ReLU and the fully-connected layer, the $k$-th logit becomes</p>
<p>$$
f_{k}(x)=\sum_{i=1}^{N} \sum_{j=1}^{J} V_{\hat{q}_{i j}, k} \delta\left(\sigma\left(w^{(i) T} y^{(j)}\right)\right)
$$</p>
<p>where $\delta(\cdot)$ denotes the max-pooling, which successively selects the maximum value in a fixed-size pooling window, and the new index $\hat{q}<em i="i" j="j">{i j}$ is the down-sampled version of $q</em>$. Then the backpropagation-based visualizations for the $k$-th logit can be formulated as</p>
<p>$$
s_{k}(x)=\frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} h\left(\delta^{\prime}\left(o_{i j}\right) V_{\hat{q}_{i j}, k}\right) \tilde{w}^{(i, j)}
$$</p>
<p>where $o_{i j} \triangleq \sigma\left(w^{(i) T} y^{(j)}\right)$ is the output of each ReLU activation and $\delta^{\prime}\left(o_{i j}\right)$ denotes the derivative of $\delta(\cdot)$ evaluated at $o_{i j}$, which is</p>
<p>$$
\delta^{\prime}\left(o_{i j}\right)= \begin{cases}1 &amp; \text { if } o_{i j} \text { is chosen by max-pooling } \ 0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>Since $o_{i j} \geq 0$ with equality holds for $w^{(i) T} y^{(j)} \leq 0$, given a proper pooling window size, it is highly possible that $o_{i j}$ is chosen by the max-pooling if and only if $w^{(i) T} y^{(j)}&gt;0$. It means with high probability, Eq. (6) is approximated as</p>
<p>$$
s_{k}(x) \approx \frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} h\left(V_{\hat{q}_{i j}, k}\right) \tilde{w}^{(i, j)} \mathbb{I}\left(w^{(i) T} y^{(j)}\right)
$$</p>
<p>For saliency map and GBP, we know $\tilde{w}^{(i, j)} \mathbb{I}\left(w^{(i) T} y^{(j)}\right)=$ $\tilde{w}^{(i, j)}$ and thus Eq. (7) is further reduced to Eq. (4), which means the behaviors of saliency map and GBP remain the same after introducing the max-pooling. However, with high probability, DeconvNet at the $k$-th logit becomes</p>
<p>$$
s_{k}^{\text {Deconv }}(x) \approx \frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} \sigma\left(V_{\hat{q}_{i j}, k}\right) w^{(i)} \mathbb{I}\left(w^{(i) T} y^{(j)}\right)
$$</p>
<p>which is exactly the form of GBP in Eq. (4). Therefore, adding the max-pooling makes the DeconvNet behave like GBP - doing nothing but image recovery. This also explains and extends the previous intuitive claims in (Samek et al., 2017; Odena et al., 2016) that the image-specific information in DeconvNet comes from the max-pooling.</p>
<p>Note that that the approximation from Eq. (6) to Eq. (7) in DeconvNet with the max-pooling is essentially different from the approximations used in GBP. For GBP, the approximate gap can be made arbitrarily small by increasing the hidden layer size $N$, leading to a perfect recovery of the input. However, for DeconvNet, given any pooling window size, there might always exist at least one of the following two contradictory cases: it is possible that $a_{i j}$ is chosen by the max-pooling if $w^{(i) T} y^{(j)} \leq 0$, and also possible that $a_{i j}$ is not chosen if $w^{(i) T} y^{(j)}&gt;0$. This makes DeconvNet (with max-pooling), in theory, never recover input perfectly, which might explain why the unusual texture-like artifacts appear in the DeconvNet visualizations.</p>
<h3>3.2.2. DEEP CNNs</h3>
<p>The analysis for a three-layer CNN can be generalized to the multi-layer (or deeper) case. For clarity, we formulate the $k$-th logit of an $L$-layer deep CNN in a matrix form:</p>
<p>$$
f_{k}(x)=\Gamma_{k}^{(L) T} \sigma\left(\Gamma^{(L-1) T} \cdots \sigma\left(\Gamma^{(1) T} x\right)\right)
$$</p>
<p>where $\Gamma^{(l)} \in \mathbb{R}^{d_{l} \times d_{l+1}}$ denotes either the convolutional or fully-connected operator matrix in the $l$-th layer and $\Gamma_{k}^{(L)}$ is the $k$-th column of $\Gamma^{(L)}$. Denote by $o^{(l)}$ the output of ReLU activations in the $l$-th layer, i.e. $o^{(l)}=$ $\sigma\left(\Gamma^{(l) T} o^{(l-1)}\right), \forall l \in{1, \cdots, L-1}$ with $o^{(0)} \triangleq x$. Then backpropagation-based visualizations at the $k$-th logit in an $L$-layer deep CNN can be formulated as</p>
<p>$$
\begin{aligned}
s_{k}(x) &amp; =\frac{1}{Z_{k}} \frac{\partial \hat{o}^{(1)}}{\partial x} \cdot h\left(\hat{V}<em k="k">{\cdot, k}^{(1)}\right) \
&amp; \stackrel{(a)}{=} \frac{1}{Z</em>}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} h\left(\hat{V<em i="i" j="j">{q</em>
\end{aligned}
$$}, k}^{(1)}\right) \hat{w}^{(i, j)</p>
<p>with $\forall l \in{1, \cdots, L-1}$,
$\hat{V}<em k="k">{\cdot, k}^{(l)}=\frac{\partial \hat{o}^{(l+1)}}{\partial o^{(l)}} \cdot h\left(\frac{\partial \hat{o}^{(l+2)}}{\partial o^{(l+1)}} \cdots h\left(\frac{\partial \hat{o}^{(L-1)}}{\partial o^{(L-2)}} h\left(\Gamma</em>\right)\right)\right)$
where in $(a)$ we rewrite $s_{k}(x)$ in an expanded form, $\hat{o}^{(l)} \triangleq$ $g\left(\Gamma^{(l) T} o^{(l-1)}\right), w^{(i)}$ is the $i$-th filter encoded in $\Gamma^{(1)}$ and $N$ is the number of filters in the first convolutional layer. Also, $h(\cdot), g(\cdot)$ and $\hat{w}^{(i, j)}$ are defined in Eq. (2) and Lemma 1.}^{(L)</p>
<p>First, the approximate property of $\hat{V}_{\cdot, k}^{(1)}$ in the random deep CNN is given in the following proposition.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. (a) shows a two-dimensional toy example where $w^{(i)}$ 's are all in a cone (the orange area) and all the $y^{(j)}$ 's in another cone (the grey area) called "dead zone" will be filtered out by the ReLU. (b) and (c) show the histograms of all weights connected to the 26 -th activation and the 44 -th one, respectively, in the layer "fc1" of the trained VGG-16 net. Note that we randomly picked up two activations (i.e. 26 and 44 here) for comparison.</p>
<p>Proposition 1. For a random deep CNN where weights are i.i.d. Gaussians with zero mean, we can also approximate every entry of $\hat{V}_{\cdot, k}^{(1)}$ as i.i.d. Gaussian with zero mean.</p>
<p>Proof. See Appendix D.
Based on Proposition 1, we can see that the statistical properties of $\hat{V}<em i="i" j="j">{q</em>}, k}^{(1)}$ in Eq. (8) are approximately the same with those of $\hat{V<em i="i" j="j">{q</em>$ in Eq. (4), which means the analysis of backpropagation-based visualizations in a shallow threelayer CNN also applies to the deep CNN case. Therefore, the behaviors of these visualizations will barely change when increasing the depth of neural networks.}, k</p>
<h3>3.2.3. CNNs with Trained Weights</h3>
<p>The previous analysis for random CNNs does not apply to the trained case directly since the weights here may not be i.i.d. Gaussian distributed. For saliency map, which uses the true gradient, the trained weights are likely to impose a stronger bias towards some specific subset of the input pixels, and so they can highlight class-relevant pixels rather than producing random noise. For GBP and DeconvNet, the analysis is a little more involved.</p>
<p>On the one hand, the trained weights $w^{(i)}$ will only lie in a small subspace of the whole image patch space which will create some "dead zones", as illustrated in Figure 3 (a). That</p>
<p>is, all image patches lying in the “dead zone” will be filtered out by the forward ReLU. For example, it is well-known that the trained weights in the first convolutional layer are Gabor-like filters to detect the image patches containing edges <em>Yosinski et al. (2014); Zeiler &amp; Fergus (2014)</em>. That is, image patches without edges will probably be filtered out by the first convolutional layer. Also, the higher convolutional layers keep filtering out more image patches with certain patterns (e.g. Figure 9). See the supplementary material for a comparison between GBP and a linear edge detector.</p>
<p>On the other hand, as shown in Figure 3 (b) and (c), the histograms of weights connected to the respective one of any two different neurons in the first fully connected layer (called “fc1”) of the trained VGG-16 net are very similar to each other. Approximately, they form two very similar Gaussians with a small standard deviation, which means the (modified) gradients at any two different neurons in the layer “fc1” with respect to the input image are almost the same. Namely, $\frac{\partial\tilde{o}^{(\text{fcl})}}{\partial x}$ in Eq. (8) for GBP and DeconvNet (with max-pooling) satisfies</p>
<p>$\frac{\partial\tilde{o}<em _text_conv="\text{conv">{m}^{(\text{fcl})}}{\partial x}\approx F</em>(x),\forall m\in{1,\cdots,M}$}</p>
<p>where $\tilde{o}<em _text_conv="\text{conv">{m}^{(\text{fcl})}$ is the $m$-th entry of $\tilde{o}^{(\text{fcl})}$ and $F</em>$ denotes the (normalized) overall filtering effect of the convolutional layers and $M$ is the number of neurons in the layer “fc1”. Thus, Eq. (8) for GBP and DeconvNet (with max-pooling) in the trained CNN can be approximated as}}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{d</p>
<p>$$
\begin{aligned}
s_{k}(x) &amp; =\frac{1}{Z_{k}} \frac{\partial \tilde{o}^{(\text{fcl})}}{\partial x} \cdot h\left(\hat{V}<em k="k">{. . k}^{(\text{fcl})}\right) \
&amp; =\frac{1}{Z</em>}} \sum_{m=1}^{M} \frac{\partial \tilde{o<em k="k" m_="m,">{m}^{(\text{fcl})}}{\partial x} \cdot h\left(\hat{V}</em>\right) \
&amp; \stackrel{(a)}{\approx} F_{\text {conv }}(x)
\end{aligned}
$$}^{(\text{fcl})</p>
<p>where $(a)$ follows from setting the normalization coefficient to be $Z_{k}=\frac{1}{\sum_{m=1}^{M} h\left(\hat{V}_{m, k}^{(\text{fcl})}\right)}$.
It shows that GBP and DeconvNet (with max-pooling) in a trained CNN are actually doing the partial image recovery, where the trained weights control which image patch could form an active path to the class logit. More importantly, this filtering process is not class sensitive (e.g. the edge detector). In the end, only these “active” image patches are combined in the first fully connected layer to form the final visualization results. As the right side of (9) does not depend on $k$, it illustrates why the GBP and DecovNet visualizations in the trained VGG are not class-sensitive.</p>
<h2>4. Experiments</h2>
<p>To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Backpropagation-based visualizations in a random threelayer CNN (top row) and a random three-layer FCN (bottom row) given the input image “tabby”. From left to right, each column represents GBP, DeconvNet and saliency map, respectively. Only GBP visualization in the CNN is human-interpretable.
connected network (FCN) and a VGG-16 net. For a random network, their weights are all sampled from the truncated Gaussians with a zero-mean and standard deviation 0.1. Unless stated otherwise, the input is the image “tabby” from the ImageNet dataset <em>Deng et al. (2009)</em> with size $224 \times 224 \times 3$. See the supplementary materials for more results on other images and other neural network such as ResNet <em>He et al. (2016)</em>. In the three-layer CNN, the filter size is $7 \times 7 \times 3$, the number of filters is $N=256$, and the stride is 2 . In the three-layer FCN, the hidden layer size is set to $N_{h}=4096$. By default, the backpropagation-based visualizations are calculated with respect to the maximum class logit.</p>
<h3>4.1. Impact of Local Connections</h3>
<p>Figure 4 shows the backpropagation-based visualizations on a random three-layer CNN and a random three-layer FCN, respectively. We can see only GBP in the CNN can produce a human-interpretable visualization, while DeconvNet and saliency map in the CNN get random noise, which verifies our theoretical analysis in the section 3.1. In contrast, as local connections do not exist in the FCN and the input size (e.g. $224 \times 224 \times 3$ ) is extremely large, all the backpropagation-based methods (including GBP) in the FCN generate random noise. Particularly for GBP, the number of hidden neurons $N_{h}=4096$ is still not large enough to recover the image.</p>
<p>To further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters $N$ in the CNN and the number of hidden neurons $N_{h}$ in the FCN, respectively, while keep other parameters fixed. The results are given in Figure 5. Note that in the FCN, we have downsampled the input image to be of size $64 \times 64 \times 3$ due to computational limitations. We can see that as the number of filters $N$ increases (resp. the hidden layer size $N_{h}$ ), the vi-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. GBP visualizations given the input image "tabby" in a three-layer CNN (top row) by varying the number of filters $N$ and in a three-layer FCN (bottom row) by varying the number of hidden neurons $N_{h}$.
sual quality of GBP in the CNN (resp. in the FCN) becomes better. Interestingly, even by setting $N_{h}=70000$, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with $N=64$. Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP.</p>
<h3>4.2. Impact of Max-Pooling and Network Depth</h3>
<p>To show the impact of the max-pooling in backpropagationbased visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in Figure 6 (top row). As compared with the visualizations in Figure 4 (top row), neither GBP or saliency map is impacted by the max-pooling, whereas the DeconvNet visualization has now become human interpretable instead of being the random noise as before. It confirms that the max-pooling is critical in helping DeconvNet produce human-interpretable visualizations via image recovery, as predicted by our theoretical analysis in the section 3.2.1.</p>
<p>To show the impact of network depth, we also apply backpropagation-based visualizations in a random VGG-16 net, which also includes the max-pooling but is much deeper than the three-layer CNN. Figure 6 (bottom row) shows that only saliency map generates random noise while both GBP and DeconvNet could produce human-interpretable visualizations. Though there are subtle visual differences between the top row and bottom row of Figure 6, the behaviors of backpropagation-based methods are basically unchanged after increasing the network depth. In addition, both GBP and DeconvNet reconstruct every fine-grained detail of the input image in the random VGG, which is different from the trained VGG in Figure 2 where only those "active" image patches are preserved.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Backpropagation-based visualizations given the input image "tabby" in a random three layer CNN with the max-pooling (top row) and in a random VGG-16 net (bottom row). Now DeconvNet visualization also becomes human-interpretable.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Average $l_{2}$ distance statistics. For each input, we randomly choose two class logits to get corresponding visualizations and calculate their $l_{2}$ distances. The above is an average $l_{2}$ distance by using 10K images of the ImageNet for each backpropagationbased method in both random and trained VGG-16 net.</p>
<h3>4.3. Average $l_{2}$ Distance Statistics</h3>
<p>To quantitatively describe how backpropagation-based visualizations change with respect to different class logits, we also provide the average $l_{2}$ distance statistics as shown in Figure 7. Our results are obtained by first calculating the $l_{2}$ distance of two visualization results given two different class logits for each input image and then taking an average of those $l_{2}$ distances based on 10 K images from the ImageNet test set. The process is repeated for all backpropagationbased methods in both random and trained cases. As we can see, the average $l_{2}$ distance of saliency map is much larger than that of both GBP and DeconvNet in either a random VGG or a trained VGG, which clearly demonstrates that saliency map is class-sensitive but GBP and DeconvNet are not. Interestingly, in the trained VGG-16 net, the average $l_{2}$ distance of DeconvNet is slightly larger than that of GBP. It shows that the class insensitivity is exchanged for further improvement of visual quality.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Top row: the image "panda" and its backpropagationbased visualizations. Bottom row: the adversarial example misclassified as "busby" and its backpropagation-based visualizations. Both experiments are applied in the trained VGG-16 net.</p>
<h3>4.4. Adversarial Attack on VGG</h3>
<p>Adversarial attack provides another way of directly testing whether visualizations are class-sensitive or doing image recovery. The class-sensitive visualizations should change drastically as both the predicted class label and ReLU states of intermediate layers have changed, while the visualizations doing image recovery should change little as only a tiny adversarial perturbation is added into the input image. In this experiment, we first generate an adversarial example "busby" via the fast gradient sign method (FGSM) (Goodfellow et al., 2014) by feeding the image "panda" into the pretrained VGG-16 net. Next, we apply the backpropagationbased visualizations to the original image "panda" and its adversary "busby" in the trained VGG-16 net. As shown in Figure 8, the saliency map visualization changes significantly whereas the GBP and DeconvNet visualizations remain almost unchanged after replacing "panda" by its adversary "busby". Therefore, it further confirms that saliency map is class-sensitive in that it highlights important pixels in making classification decisions. However, GBP and DeconvNet are doing nothing but (partial) image recovery.</p>
<h3>4.5. VGG with Partly Trained Weights</h3>
<p>There exist some differences for backpropagation-based visualizations, GBP and DeconvNet in particular, between the random and trained cases. We take GBP as an example here to investigate the contributions of different layers in the trained VGG-16 net to these visual differences.</p>
<p>First, to isolate the impact of later layers, we load the trained weights up to a given layer and leave later layers randomly initialized. As shown in Figure 9 (top row), from "Conv1$1^{<em>}$ " to "Conv5-1</em>" GBP keeps filtering out more image patches as the number of trained convolutional layers increases. However, from "Conv5-1<em>" to "FC3</em>" (i.e., the
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Top row: load trained weights up to the indexed layer and leave the later layers to be randomly initialized (marked by star sign). Bottom row: load trained weights except for the indexed layer is randomly initialized instead (marked by diamond sign).
fully-trained case) GBP behaves almost the same, no matter weights in the dense layers are random or trained. Therefore, it is the trained weights in the convolutional layers rather than those in the dense layers that account for filtering out image patches. Also, it further confirms that GBP is class-insensitive. Furthermore, to reveal the impact of each layer, we load the trained weights for the whole VGG-16 net except for a given layer which is randomly initialized instead. The results are shown in Figure 9 (bottom row). We can see that the GBP visualization is blurry for "Conv1-1"", clean with much background information for "Conv3-1" and clean without background information for "Conv5-1", respectively. It means that the earlier convolutional layer has more important impact in the GBP visualization than the later convolutional layer.</p>
<h2>5. Conclusions</h2>
<p>In this paper, we proposed a theoretical explanation for backpropagation-based visualizations, where we started from a random three-layer CNN and later generalized it to more realistic cases. We showed that unlike saliency map, both GBP and DeconvNet are essentially doing (partial) image recovery, which verified their class-insensitive properties. We revealed that it is the backward ReLU, used by both GBP and DeconvNet, along with the local connections in CNNs, that is responsible for human-interpretable visualizations. We also explained how DeconvNet also relies on the max-pooling to recover the input. Our analysis was supported by extensive experiments. Finally, we hope our analysis can provide useful insights into developing better visualization methods for deep neural networks. A future direction is to understand how the GBP visualizations in the trained CNNs filter out image patches layer by layer.</p>
<h2>Acknowledgements</h2>
<p>Thanks to the anonymous reviewers for useful comments. WN, YZ and AB were supported by IARPA via DoI/IBC contract D16PC00003. Also, we thank Leon Sixt for pointing out an error in the proof of Theorem 1.</p>
<h2>References</h2>
<p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248-255. IEEE, 2009.</p>
<p>Dosovitskiy, A. and Brox, T. Inverting visual representations with convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4829-4837, 2016.</p>
<p>Fong, R. C. and Vedaldi, A. Interpretable explanations of black boxes by meaningful perturbation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3429-3437, 2017.</p>
<p>Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.</p>
<p>Gunning, D. Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web, 2017.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.</p>
<p>Johnson, J., Alahi, A., and Fei-Fei, L. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pp. 694-711. Springer, 2016.</p>
<p>Kindermans, P.-J., Schütt, K. T., Alber, M., Müller, K.R., and Dähne, S. Patternnet and patternlrp-improving the interpretability of neural networks. arXiv preprint arXiv:1705.05598, 2017.</p>
<p>Kraus, O. Z., Ba, J. L., and Frey, B. J. Classifying and segmenting microscopy images with deep multiple instance learning. Bioinformatics, 32(12):i52-i59, 2016.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.</p>
<p>Lugosi, G. and Mendelson, S. Sub-gaussian estimators of the mean of a random vector. arXiv preprint arXiv:1702.00482, 2017.</p>
<p>Mahendran, A. and Vedaldi, A. Salient deconvolutional networks. In European Conference on Computer Vision, pp. 120-135. Springer, 2016.</p>
<p>Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and Clune, J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In $A d$ vances in Neural Information Processing Systems, pp. 3387-3395, 2016.</p>
<p>Odena, A., Dumoulin, V., and Olah, C. Deconvolution and checkerboard artifacts. Distill, 2016. doi: 10.23915/ distill.00003. URL http://distill.pub/2016/ deconv-checkerboard.</p>
<p>Samek, W., Binder, A., Montavon, G., Lapuschkin, S., and Müller, K.-R. Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660-2673, 2017.</p>
<p>Selvaraju, R. R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., and Batra, D. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization. arXiv preprint arXiv:1610.02391, 2016.</p>
<p>Shrikumar, A., Greenside, P., and Kundaje, A. Learning important features through propagating activation differences. arXiv preprint arXiv:1704.02685, 2017.</p>
<p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.</p>
<p>Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<p>Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.</p>
<p>Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.</p>
<p>Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014.</p>
<p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.</p>
<p>Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320-3328, 2014.</p>
<p>Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In European conference on computer vision, pp. 818-833. Springer, 2014.</p>
<h2>Appendix</h2>
<h2>A. Proof of Lemma 1</h2>
<p>Proof: Saliency map includes only forward ReLUs without backward ReLUs, whereas DeconvNet includes only backward ReLUs without forward ReLUs. GBP has both types of ReLUs. Also, the norm of all the visualization results will be normalized to be in the range of $[0,1]$. Thus, by taking the (modified) derivative of $f_{k}(x)$ in Eq. (3) with respect to $x$ and applying the proper normalization, these backpropagation-based visualizations for the $k$-th logit can be unified as</p>
<p>$$
\begin{aligned}
&amp; s_{k}(x) \stackrel{(a)}{=} \frac{1}{Z_{k}} \sum_{i=1}^{N} \sum_{j=1}^{J} h\left(V_{q_{i j}, k}\right) \frac{\partial}{\partial x} g\left(w^{(i) T} y^{(j)}\right) \
&amp; \stackrel{(b)}{=} \frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} h\left(V_{q_{i j}, k}\right) \frac{\partial}{\partial y^{(j)}} g\left(w^{(i) T} y^{(j)}\right) \
&amp; \stackrel{(c)}{=} \frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \sum_{i=1}^{N} h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)}
\end{aligned}
$$</p>
<p>where $Z_{k}$ is the normalization coefficient to ensure $\left|s_{k}(x)\right| \in[0,1]$, (a) follows from the formal definitions of backpropagation-based visualization for a ReLU activation in Eq. (1) with $h(\cdot), g(\cdot)$ being given by Eq. (2), (b) is from applying $y^{(j)}=D_{j} x$ and swapping the two sums, and $(c)$ is from taking the derivative of $g(\cdot)$ in the three cases with
$\tilde{w}^{(i, j)}= \begin{cases}w^{(i)} &amp; \text { for DeconvNet } \ w^{(i)} \mathbb{I}\left(w^{(i) T} y^{(j)}\right) &amp; \text { for saliency map and GBP }\end{cases}$
as required.</p>
<h2>B. Proof of Theorem 1</h2>
<p>Proof: In a random neural network where every entry of both $V$ and $W$ is assumed to be independently Gaussian distributed with a zero mean and variance $c^{2}$, we have $V_{q_{i j}, k} \sim \mathcal{N}\left(0, c^{2}\right)$ and $w^{(i)} \sim \mathcal{N}\left(0, c^{2} I\right) \forall i \in$ ${1, \cdots, N}, j \in{1, \cdots, J}$. For GBP, in order to ensure $\left|s_{k}(x)\right| \in[0,1]$ we first set $Z_{k}=\tilde{Z}_{k} N$. Assuming the number of filters $N$ is sufficiently large (e.g. VGG-16 net usually has $N=256$ ), then from Eq. (10) we have</p>
<p>$$
\begin{aligned}
s_{k}(x) &amp; =\frac{1}{\tilde{Z}<em j="1">{k}} \sum</em> \
&amp; \stackrel{(a)}{=} \frac{1}{\tilde{Z}}^{J} D_{j}^{T} \frac{1}{N} \sum_{i=1}^{N} h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)<em j="1">{k}} \sum</em>\right] \
&amp; \stackrel{(b)}{=} \frac{1}{\tilde{Z}}^{J} D_{j}^{T} \mathbb{E}\left[h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)<em j="1">{k}} \sum</em>\right]
\end{aligned}
$$}^{J} D_{j}^{T} \mathbb{E}\left[h\left(V_{q_{i j}, k}\right)\right] \mathbb{E}\left[\tilde{w}^{(i, j)</p>
<p>where $(a)$ follows from the asymptotic approximation of sample mean to the expectation and $(b)$ follows from the fact that $V_{q_{i j}, k}$ and $w^{(i)}$ are independent.
For GBP, we have $h\left(V_{q_{i j}, k}\right)=\sigma\left(V_{q_{i j}, k}\right)$. Since we know $V_{q_{i j}, k} \sim \mathcal{N}\left(0, c^{2}\right)$, then $h\left(V_{q_{i j}, k}\right)$ follows one-dimensional rectified Gaussian distribution, and by its definition we can easily get $\mathbb{E}\left[h\left(V_{q_{i j}, k}\right)\right]=\sqrt{\frac{1}{2 \pi}} c$. Also, from the definition of $\tilde{w}^{(i, j)}$ for GBP, we know $\tilde{w}^{(i, j)}$ follows a $p$-dimensional rectified Gaussian distribution and its p.d.f. is</p>
<p>$$
p(w)=\frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{w^{T} w}{2 c^{2}}} u\left(y^{(j) T} w\right)+\frac{1}{2} p\left(w_{A}\right) \delta_{A}(w)
$$</p>
<p>where the manifold $A \triangleq\left{w \mid y^{(j) T} w=0\right}, p\left(w_{A}\right)$ is a $(p-1)$-dimensional Gaussian distribution projected from the $p$-dimensional Gaussian distribution onto the manifold $A$, with $w_{A} \in \mathbb{R}^{p-1}$ being the corresponding projected vector of $w$, and $u(t)$ and $\delta_{A}(w)$ are the unit step function and dirac delta function, respectively, i.e.,</p>
<p>$$
u(t)= \begin{cases}1, &amp; t&gt;0 \ 0, &amp; t \leq 0\end{cases} \quad \text { and } \quad \delta_{A}(w)= \begin{cases}+\infty, &amp; w \in A \ 0, &amp; w \notin A\end{cases}
$$</p>
<p>By definition, we have</p>
<p>$$
\int p\left(w_{A}\right) \delta_{A}(w) d w \triangleq \int p\left(w_{A}\right) d w_{A}=1
$$</p>
<p>such that it satisfies $\int p(w) d w=1$. Accordingly, its expectation is given by</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}\left[\tilde{w}^{(i, j)}\right] \
&amp; =\int_{y^{(j) T} w&gt;0} \frac{w}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{w^{T} w}{2 c^{2}}} d w+\int \frac{w}{2} p\left(w_{A}\right) \delta_{A}(w) d w \
&amp; \stackrel{(a)}{=} \int_{\phi_{p}&gt;0} U \phi \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}}|U| d \phi \
&amp; +\int \frac{1}{2} U \phi p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi)|U| d \phi \
&amp; \stackrel{(b)}{=} U \int_{\phi_{p}&gt;0} \phi \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi \
&amp; +\frac{1}{2} U \int \phi p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi) d \phi
\end{aligned}
$$</p>
<p>where $(a)$ follows from the change of variables $w=U \phi$ and $U$ is an unitary matrix satisfying the condition that $U^{T} \cdot \frac{y^{(j)}}{\left|y^{(j)}\right|<em 2="2">{2}}=e^{(p)}$ and $e^{(p)}$ is an unit vector with only the $p$-th entry being 1 . That is, $\frac{y^{(j)}}{\left|y^{(j)}\right|</em>\right|}}$ is the $p$-th column of $U$. Thus, $y^{(j) T} w=y^{(j) T} U \phi=e^{(p) T} \phi\left|y^{(j)<em p="p">{2}=\phi</em>\right|}\left|y^{(j)<em p="p">{2}$ with $\phi</em> w&gt;0$
is equivalent to $\phi_{p}&gt;0$. Also, by the change of variables in the integral, we have $d w=|U| d \phi$ where $|\cdot|$ denotes the determinant of a matrix. Accordingly, we define a new manifold $A_{p}={\phi \mid \phi_{p}=0}$. (b) follows from $|U|=1$ by the definition of an unitary matrix, and the swap between matrix multiplication and the integral.}$ being the $p$-th entry of $\phi$, which means $y^{(j) T</p>
<p>As $\phi$ is a $p$-dimensional vector, the first integral above can be evaluated at each entry, denoted by $\phi_{m}$, of $\phi$ separately. For $m \neq p$, we have</p>
<p>$$
\begin{aligned}
&amp; \int_{\phi_{p}&gt;0} \phi_{m} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi \
&amp; \stackrel{(a)}{=} \underbrace{\int_{-\infty}^{\infty} \frac{\phi_{m}}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{m}^{2}}{2 c^{2}}} d \phi_{m}}<em 0="0">{0} \cdot \int</em> \
&amp; =0
\end{aligned}
$$}^{\infty} \frac{1}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{p}^{2}}{2 c^{2}}} d \phi_{p</p>
<p>where $(a)$ follows from the expansion of the multiple integral, and all of the other $p-2$ integrals over $\phi_{k}$ for $k \notin{p, m}$ are 1 . For $m=p$, we have</p>
<p>$$
\begin{aligned}
&amp; \int_{\phi_{p}&gt;0} \phi_{p} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi \
&amp; \stackrel{(a)}{=} \int_{0}^{\infty} \phi_{p} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{p}^{2}}{2 c^{2}}} d \phi_{p} \
&amp; \stackrel{(b)}{=} \sqrt{\frac{1}{2 \pi}} c
\end{aligned}
$$</p>
<p>where $(a)$ also follows from the expansion of the multiple integral, and all other $p-1$ integrals over $\phi_{k}$ for $k \neq p$ are $1 ;(b)$ follows from evaluating the integral by the the change of variables $t=\frac{\phi_{m}^{2}}{2 c^{2}}$.
The second integral in Eq. (12) can also be evaluated at each entry in the following. First, we have</p>
<p>$$
p\left(\phi_{A_{p}}\right)=\frac{1}{\left(2 \pi c^{2}\right)^{\frac{p-1}{2}}} e^{-\frac{2 c p-1}{2 c^{2}} \frac{\phi_{1}^{2}}{2 c^{2}}}
$$</p>
<p>Then for $m \neq p$, we have</p>
<p>$$
\begin{aligned}
&amp; \int \phi_{m} p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi) d \phi \
&amp; \stackrel{(a)}{=} \int \phi_{m} \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p-1}{2}}} e^{-\frac{2 c p-1}{2 c^{2}} \frac{\phi_{1}^{2}}{2 c^{2}}} d \phi_{1} \cdots d \phi_{p-1} \
&amp; =\int \phi_{m} \frac{1}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{m}^{2}}{2 c^{2}}} d \phi_{m} \
&amp; =0
\end{aligned}
$$</p>
<p>where $(a)$ follows from the definition of $\delta_{A_{p}}(\phi)$. For $m=p$, we have</p>
<p>$$
\int \phi_{p} p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi) d \phi \stackrel{(a)}{=} 0
$$</p>
<p>where $(a)$ follows from $A_{p}=\left{\phi \mid \phi_{p}=0\right}$.
Putting them together, (12) becomes</p>
<p>$$
\mathbb{E}\left[\tilde{w}^{(i, j)}\right]=\sqrt{\frac{1}{2 \pi}} c \cdot U e^{(p)} \stackrel{(a)}{=} \sqrt{\frac{1}{2 \pi}} c \cdot \frac{y^{(j)}}{\left|y^{(j)}\right|_{2}}
$$</p>
<p>where $(a)$ follows from the the definition of the unitary matrix $U$ satisfying $U^{T} \cdot \frac{y^{(j)}}{\left|y^{(j)}\right|_{2}}=e^{(p)}$.
Therefore, GBP at the $k$-th logit can be approximated as</p>
<p>$$
\begin{aligned}
s_{k}^{\mathrm{GBP}}(x) &amp; \approx \frac{c^{2}}{2 \pi \hat{Z}<em j="1">{k}} \sum</em>\right|}^{J} \frac{1}{\left|y^{(j)<em j="j">{2}} D</em> \
&amp; \stackrel{(a)}{=} \frac{c^{2}}{2 \pi \hat{Z}}{ }^{T} y^{(j)<em j="1">{k}}\left(\sum</em>\right|}^{J} \frac{1}{\left|y^{(j)<em p__j="p_{j">{2}} \mathcal{I}</em>\right) x
\end{aligned}
$$}</p>
<p>where $(a)$ follows from the definition</p>
<p>$$
\mathcal{I}<em j="j">{p</em>
0_{(j-1) b \times(j-1) b} &amp; \
&amp; I_{p \times p} &amp; \
&amp; &amp; 0
\end{array}\right] \in \mathcal{R}^{d \times d}
$$}} \triangleq D_{j}^{T} D_{j}=\left[\begin{array}{ll</p>
<p>Ideally, if we assume $\left|y^{(j)}\right|<em 0="0">{2}=C</em>}, \forall j$ (a constant) and ignore the boundary points (Note that using the "SAME" padding method instead of the "VALID" one is supposed to alleviate the boundary inconsistency to some extent), then $\sum_{j=1}^{J} \mathcal{I<em j="j">{p</em>$ and thus we can further approximate the GBP as}} \approx p I_{d \times d</p>
<p>$$
s_{k}^{\mathrm{GBP}}(x) \approx \frac{c^{2} p}{2 \pi C_{0} \hat{Z}_{k}} x
$$</p>
<p>Thus, by setting the normalization coefficient $\hat{Z}<em 0="0">{k}=\frac{2 \pi C</em>$, we get the result.}}{c^{2} p</p>
<h2>C. Proof of Theorem 2</h2>
<p>In Eq. (4), we denote by $\Theta_{j}=\sum_{i=1}^{N} h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)}$, which is a sum of $N$ independent and identically distributed random variables. From the Central Limit Theorem, $\Theta_{j}$ is approximated as a Gaussian random variable if the number of filters $N$ is sufficiently large. Since $s_{k}(x)$ is a linear function of $\Theta_{j}$, i.e.</p>
<p>$$
s_{k}(x)=\frac{1}{Z_{k}} \sum_{j=1}^{J} D_{j}^{T} \Theta_{j}
$$</p>
<p>we have $s_{k}(x)$ can also be approximated as a Gaussian random variable for both saliency map and DeconvNet.</p>
<p>In the first part of the proof, we evaluate the mean and variance of saliency map.</p>
<p>Since for saliency map we know $\mathbb{E}\left[h\left(V_{q_{i j}, k}\right)\right]=$ $\mathbb{E}\left[V_{q_{i j}, k}\right]=0$, we can evaluate the mean of $\Theta_{j}$ as</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\Theta_{j}\right] &amp; =\sum_{i=1}^{N} \mathbb{E}\left[h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)}\right] \
&amp; \stackrel{(a)}{=} \sum_{i=1}^{N} \mathbb{E}\left[V_{q_{i j}, k}\right] \mathbb{E}\left[\tilde{w}^{(i, j)}\right] \
&amp; =0
\end{aligned}
$$</p>
<p>where $(a)$ is from the fact that $V_{q_{i j}, k}$ and $\tilde{w}^{(i, j)}$ are independent. Apparently, from Eq. (15) we have</p>
<p>$$
\mathbb{E}\left[s_{k}^{\mathrm{Sal}}(x)\right]=0
$$</p>
<p>Then to evaluate the variance of saliency map, we can also first evaluate the variance of $\Theta_{j}$ as</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Var}\left[\Theta_{j}\right]=N \cdot \operatorname{Var}\left[h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)}\right] \
&amp; \stackrel{(a)}{=} N \cdot\left{\operatorname{Var}\left[V_{q_{i j}, k}\right] \mathbb{E}\left[\tilde{w}^{(i, j)}\right]^{2}+\right. \
&amp; \left.\quad \operatorname{Var}\left[V_{q_{i j}, k}\right] \operatorname{Var}\left[\tilde{w}^{(i, j)}\right]+\operatorname{Var}\left[\tilde{w}^{(i, j)}\right] \mathbb{E}\left[V_{q_{i j}, k}\right]^{2}\right} \
&amp; \stackrel{(b)}{=} N \cdot c^{2} \mathbb{E}\left[\tilde{w}^{(i, j)} \tilde{w}^{(i, j) T}\right]
\end{aligned}
$$</p>
<p>where $(a)$ is also from the fact that $V_{q_{i j}, k}$ and $\tilde{w}^{(i, j)}$ are independent and $(b)$ follows from $\mathbb{E}\left[V_{q_{i j}, k}\right]=0$ and $\operatorname{Var}\left[V_{q_{i j}, k}\right]=c^{2}$. According to Eq. (11), we get</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}\left[\tilde{w}^{(i, j)} \tilde{w}^{(i, j) T}\right] \
&amp; =\int_{\left.y^{(j) T} w&gt;0\right.} w w^{T} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{w^{T} w}{2 c^{2}}} d w \
&amp; +\frac{1}{2} \int w w^{T} p\left(w_{A}\right) \delta_{A}(w) d w \
&amp; \stackrel{(a)}{=} \int_{\phi_{p}&gt;0} U \phi \phi^{T} U^{T} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}}|U| d \phi \
&amp; +\frac{1}{2} \int U \phi \phi^{T} U^{T} p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi)|U| d \phi \
&amp; =U \underbrace{\left[\int_{\phi_{p}&gt;0} \phi \phi^{T} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi\right]}<em A__p="A_{p">{B^{(0)}} U^{T} \
&amp; +\frac{1}{2} U \underbrace{\left[\int \phi \phi^{T} p\left(\phi</em>
\end{aligned}
$$}}\right) \delta_{A_{p}}(\phi) d \phi\right]}_{B^{(1)}} U^{T</p>
<p>where $(a)$ also follows from the change of variables $w=$ $U \phi$ and $U$ is an unitary matrix satisfying the condition that $U^{T} \cdot \frac{y^{(j)}}{\left|y^{(j)}\right|<em p="p">{2}}=e^{(p)}$, and $A</em>=0\right}$.}=\left{\phi \mid \phi_{p</p>
<p>As $\phi \phi^{T}$ is a $p \times p$ matrix, the integral above $B^{(0)}$ can be evaluated at each entry, denoted by $\phi_{m} \phi_{n}$, of $\phi \phi^{T}$ separately where $m, n \in{1, \cdots, p}$.
First, for $m \neq n \neq p$, we have</p>
<p>$$
\begin{aligned}
&amp; B_{m n}^{(0)}=\int_{\phi_{p}&gt;0} \phi_{m} \phi_{n} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi \
&amp; \stackrel{(a)}{=} \underbrace{\int_{-\infty}^{\infty} \frac{\phi_{m}}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{m}^{2}}{2 c^{2}}} d \phi_{m}}<em -_infty="-\infty">{0} \cdot \underbrace{\int</em> \
&amp; =0
\end{aligned}
$$}^{\infty} \frac{\phi_{n}}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{n}^{2}}{2 c^{2}}} d \phi_{n}}_{0</p>
<p>where $(a)$ follows from the expansion of the multiple integral, and all the other $p-2$ integrals over $\phi_{k}$ for $k \notin{m, n}$ are 1 . Similarly, we can easily get that $B_{m n}^{(0)}=0$ for $m \neq n$ with $m=p$ or $n=p$.
Also, for $m=n \neq p$, we have</p>
<p>$$
\begin{aligned}
&amp; B_{m n}^{(0)}=\int_{\phi_{p}&gt;0} \phi_{m}^{2} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi \
&amp; \stackrel{(a)}{=} \underbrace{\int_{-\infty}^{\infty} \frac{\phi_{m}^{2}}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{m}^{2}}{2 c^{2}}} d \phi_{m}}<em 0="0">{c^{2}} \cdot \underbrace{\int</em> \
&amp; =\frac{1}{2} c^{2}
\end{aligned}
$$}^{\infty} \frac{1}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{n}^{2}}{2 c^{2}}} d \phi_{p}}_{\frac{1}{2}</p>
<p>where $(a)$ follows from the expansion of the multiple integral, and all other $p-2$ integrals are 1.
Finally, for $m=n=p$, we have</p>
<p>$$
\begin{aligned}
B_{m n}^{(0)} &amp; =\int_{\phi_{p}&gt;0} \phi_{p}^{2} \cdot \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p}{2}}} e^{-\frac{\phi^{T} \phi}{2 c^{2}}} d \phi \
&amp; \stackrel{(a)}{=} \underbrace{\int_{0}^{\infty} \frac{\phi_{p}^{2}}{\left(2 \pi c^{2}\right)^{\frac{1}{2}}} e^{-\frac{\phi_{p}^{2}}{2 c^{2}}} d \phi_{p}}_{\frac{1}{2} c^{2}} \
&amp; =\frac{1}{2} c^{2}
\end{aligned}
$$</p>
<p>where $(a)$ follows from the expansion of the multiple integral, and all other $p-1$ integrals are 1.
Putting them together, we get $B^{(0)}=\frac{1}{2} c^{2} I$. Similarly, we can evaluate the integral $B^{(1)}$ in the following. For for $m=n \neq p$, we have</p>
<p>$$
\begin{aligned}
B_{m n}^{(1)} &amp; =\int \phi_{m} \phi_{n} p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi) d \phi \
&amp; \stackrel{(a)}{=} \int \phi_{m} \phi_{n} \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p-1}{2}}} e^{-\frac{2 c p-1}{2 c^{2}} \frac{\phi_{p}^{2}}{2}} d \phi_{1} \cdots d \phi_{p-1} \
&amp; =0
\end{aligned}
$$</p>
<p>where $(a)$ is from the definition of $\delta_{A_{p}}(\phi)$. For $m=n \neq p$, we have</p>
<p>$$
\begin{aligned}
B_{m n}^{(1)} &amp; =\int \phi_{m}^{2} p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi) d \phi \
&amp; =\int \phi_{m}^{2} \frac{1}{\left(2 \pi c^{2}\right)^{\frac{p-1}{2}}} e^{-\frac{2 c p-1}{2 c^{2}}} d \phi_{1} \cdots d \phi_{p-1} \
&amp; =c^{2}
\end{aligned}
$$</p>
<p>Finally, for $m=p$ (or $n=p$ ), we have</p>
<p>$$
B_{m n}^{(1)}=\int \phi_{p} \phi_{n} p\left(\phi_{A_{p}}\right) \delta_{A_{p}}(\phi) d \phi \stackrel{(a)}{=} 0
$$</p>
<p>where $(a)$ follows from $A_{p}=\left{\phi \mid \phi_{p}=0\right}$.
Putting them together, we have $B^{(1)}=c^{2}\left(I-e_{p} e_{p}^{T}\right)$, and thus $\mathbb{E}\left[\tilde{w}^{(i, j)} \tilde{w}^{(i, j) T}\right]=c^{2} U\left(I-\frac{1}{2} e_{p} e_{p}^{T}\right) U^{T}$ which further implies</p>
<p>$$
\operatorname{Var}\left[\Theta_{j}\right] \stackrel{(a)}{=} N c^{4}\left(I-\frac{y^{(j)} y^{(j) T}}{2\left|y^{(j)}\right|^{2}}\right)
$$</p>
<p>where $(a)$ is from $U e^{(p)}=\frac{y^{(j)}}{\left|y^{(j)}\right|_{2}}$. Accordingly, from Eq. (15) we have</p>
<p>$$
\begin{aligned}
\operatorname{Var}\left[s_{k}^{\text {ad }}(x)\right] &amp; =\frac{1}{Z_{k}^{2}} \sum_{j=1}^{J} D_{j}^{T} \operatorname{Var}\left[\Theta_{j}\right] D_{j} \
&amp; =\frac{N c^{4}}{Z_{k}^{2}} \sum_{j=1}^{J}\left(D_{j}^{T} D_{j}-\frac{D_{j}^{T} y^{(j)} y^{(j) T} D_{j}}{2\left|y^{(j)}\right|^{2}}\right) \
&amp; \stackrel{(a)}{\approx} \frac{N p c^{4}}{Z_{k}^{2}}\left(I-\frac{1}{2 p} \underbrace{\sum_{j=1}^{J} \frac{D_{j}^{T} y^{(j)} y^{(j) T} D_{j}}{\left|y^{(j)}\right|^{2}}}_{\Lambda}\right) \
&amp; \stackrel{(b)}{=} I-\frac{1}{2 p} \Lambda
\end{aligned}
$$</p>
<p>where $(a)$ is from the approximation that the patching matrix $D_{j}$ satisfies $\sum_{j=1}^{J} D_{j}^{T} D_{j} \approx p I$, and $(b)$ follows from setting the normalization coefficient to be $Z_{k}=c^{2} \sqrt{N p}$. Therefore, we have</p>
<p>$$
s_{k}^{\text {Sd }} \sim \mathcal{N}\left(0, I-\frac{1}{2 p} \Lambda\right)
$$</p>
<p>where $\Lambda \triangleq \sum_{j=1}^{J} \frac{D_{j}^{T} y^{(j)} y^{(j) T} D_{j}}{\left|y^{(j)}\right|^{2}}$ includes image information that is 'buried' in the noise. Let $\tilde{y}^{(j)} \triangleq D_{j}^{T} y^{(j)}$, a vectorized image patch, augmented with zeros representing all the other pixels. Then we have</p>
<p>$$
\Lambda=\sum_{j=1}^{J} \frac{\tilde{y}^{(j)} \tilde{y}^{(j) T}}{\left|\tilde{y}^{(j)}\right|^{2}}
$$</p>
<p>As we can see that $\operatorname{Trace}(\Lambda)=J$, we have $\operatorname{Trace}(I-$ $\frac{1}{2 p} \Lambda)=d-\frac{J}{2 p}$. Since the input image dimension $d \gg \frac{J}{2 p}$ (which works for a typical image patch size of $3 \times 3$ or $7 \times 7$ ), we have $\operatorname{Trace}\left(I-\frac{1}{2 p} \Lambda\right) \approx d$. That is, the identity term $I$ dominates the term $\frac{1}{2 p} \Lambda$ with image information. This implies that for an input image with reasonably large dimension $d$, we approximately have</p>
<p>$$
s_{k}^{\text {Sal }} \sim \mathcal{N}(0, I)
$$</p>
<p>In the second part of the proof, we evaluate the mean and variance of the DeconvNet.
Similarly, for DeconvNet we have $\mathbb{E}\left[\tilde{w}^{(i, j)}\right]=\mathbb{E}\left[w^{(i)}\right]=$ 0 , then we can evaluate the mean of $\Theta_{j}$ as</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\Theta_{j}\right] &amp; =\sum_{i=1}^{N} \mathbb{E}\left[h\left(V_{q_{i j}, k}\right) \tilde{w}^{(i, j)}\right] \
&amp; \stackrel{(a)}{=} \sum_{i=1}^{N} \mathbb{E}\left[\sigma\left(V_{q_{i j}, k}\right)\right] \mathbb{E}\left[w^{(i)}\right] \
&amp; =0
\end{aligned}
$$</p>
<p>where $(a)$ is from the fact that $V_{q_{i j}, k}$ and $\tilde{w}^{(i, j)}$ are independent. Apparently, from Eq. (15) we have</p>
<p>$$
\mathbb{E}\left[s_{k}^{\text {Deconv }}(x)\right]=0
$$</p>
<p>Then to evaluate the variance of DeconvNet, we can also first evaluate the variance of $\Theta_{j}$ as</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Var}\left[\Theta_{j}\right]=N \cdot \operatorname{Var}\left[\sigma\left(V_{q_{i j}, k}\right) w^{(i)}\right] \
&amp; \stackrel{(a)}{=} N \cdot\left{\operatorname{Var}\left[\sigma\left(V_{q_{i j}, k}\right)\right] \operatorname{Var}\left[w^{(i)}\right]+\right. \
&amp; \left.\quad \operatorname{Var}\left[\sigma\left(V_{q_{i j}, k}\right)\right] \mathbb{E}\left[w^{(i)}\right]^{2}+\operatorname{Var}\left[w^{(i)}\right] \mathbb{E}\left[\sigma\left(V_{q_{i j}, k}\right)\right]^{2}\right} \
&amp; \stackrel{(b)}{=} N c^{2} \mathbb{E}\left[\sigma\left(V_{q_{i j}, k}\right)^{2}\right] \cdot I \
&amp; \stackrel{(c)}{=} N c^{4} I
\end{aligned}
$$</p>
<p>where $(a)$ is also from the fact that $\sigma\left(V_{q_{i j}, k}\right)$ and $w^{(i)}$ are independent, $(b)$ follows from $\mathbb{E}\left[V_{q_{i j}, k}\right]=0$ and $\operatorname{Var}\left[w^{(i)}\right]=c^{2} I$ and $(c)$ follows from the fact that $\mathbb{E}\left[\sigma\left(V_{q_{i j}, k}\right)^{2}\right]=c^{2}$. Then, the rest of the proof follows the same derivations with saliency map, which yields</p>
<p>$$
s_{k}^{\text {Deconv }} \sim \mathcal{N}(0, I)
$$</p>
<p>Thus, we finish our proof by showing that both saliency map and DeconvNet are standard Gaussians which preserve no input information.</p>
<h2>D. Proof of Proposition 1</h2>
<p>First, let us focus on the GBP case. From Eq. (9), we know</p>
<p>$$
\hat{V}<em k="k">{, k}^{(2)} \triangleq \frac{\partial o^{(3)}}{\partial o^{(2)}} \cdots \sigma\left(\frac{\partial o^{(L-1)}}{\partial o^{(L-2)}} \sigma\left(\Gamma</em>\right)\right)
$$}^{(L)</p>
<p>where $\hat{V}<em i="i" j="j">{q</em>$ for GBP becomes}, k}^{(2)} \in \mathbb{R}^{d_{3} \times 1}$ as we know $\Gamma^{(l)} \in \mathbb{R}^{d_{l} \times d_{l+1}}$ in the $l$-th layer, and then $\hat{V}_{, k}^{(1)</p>
<p>$$
\begin{aligned}
\hat{V}<em _="," k="k">{, k}^{(1)} &amp; =\frac{\partial \sigma\left(\Gamma^{(2) T} o^{(1)}\right)}{\partial o^{(1)}} \sigma\left(\hat{V}</em>\right) \
&amp; =\left[\Gamma_{-1}^{(2)} \mathbb{I}\left(\Gamma_{-1}^{(2) T} o^{(1)}\right) \quad \cdots \quad \Gamma_{-d_{3}}^{(2)} \mathbb{I}\left(\Gamma_{-d_{3}}^{(2) T} o^{(1)}\right)\right] \sigma\left(\hat{V}}^{(2)<em t="1">{, k}^{(2)}\right) \
&amp; =\sum</em>\right)
\end{aligned}
$$}^{d_{3}} \Gamma_{-t}^{(2)} \mathbb{I}\left(\Gamma_{-t}^{(2) T} o^{(1)}\right) \sigma\left(\hat{V}_{t, k}^{(2)</p>
<p>which yields</p>
<p>$$
\hat{V}<em i="i" j="j">{q</em>\right)
$$}, k}^{(1)}=\sum_{t=1}^{d_{3}} \Gamma_{q_{i j}, t}^{(2)} \mathbb{I}\left(\Gamma_{-t}^{(2) T} o^{(1)}\right) \sigma\left(\hat{V}_{t, k}^{(2)</p>
<p>Since every entry of $\Gamma^{(2)}$ is i.i.d. Gaussian distributed with zero-mean, we have</p>
<p>$$
\begin{aligned}
\mathbb{I}\left(\Gamma_{-t}^{(2) T} o^{(1)}\right) &amp; =\mathbb{I}\left(\Gamma_{q_{i j}, t}^{(2)} o_{q_{i j}}^{(1)}+\sum_{v \neq q_{i j}} \Gamma_{v, t}^{(2)} o_{v}^{(1)}\right) \
&amp; \stackrel{(a)}{\approx} \mathbb{I}\left(\underbrace{\sum_{v \neq q_{i j}} \Gamma_{v, t}^{(2)} o_{v}^{(1)}}<em t="t">{b</em>\right)
\end{aligned}
$$}</p>
<p>where $(a)$ follows from the assumption of the dimension $d_{2}$ is sufficiently high in the CNN, and thus the impact of $\Gamma_{q_{i j}, t}^{(2)} o_{q_{i j}}^{(1)}$ can be ignored. Therefore, $b_{t}$ is independent of $\Gamma_{q_{i j}, t}^{(2)}$. Similarly, $\sigma\left(\hat{V}<em j="j" q__i="q_{i">{t, k}^{(2)}\right)$ is also independent of $\Gamma</em>$ under the same approximation.
As $d_{3}$ is also sufficiently large and $\hat{V}}, t}^{(2)<em i="i" j="j">{q</em>$ for GBP becomes}, k}^{(1)</p>
<p>$$
\hat{V}<em i="i" j="j">{q</em>\right)
$$}, k}^{(1)} \approx \sum_{t=1}^{d_{3}} \Gamma_{q_{i j}, t}^{(2)} b_{t} \sigma\left(\hat{V}_{t, k}^{(2)</p>
<p>which approximately is a Gaussian random variable with zero mean due to the central limit theorem. Next, in order to show the independence of two Gaussian random variables, it is equivalent to show they are uncorrelated. Since for any</p>
<p>$q^{\prime} \neq q_{i j}$, we know</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}\left[\hat{V}<em j="j" q__i="q_{i">{q^{\prime}, k}^{(1)} \hat{V}</em>\right] \
&amp; \approx \mathbb{E}\left[\sum_{t^{\prime}=1}^{d_{3}} \Gamma_{q^{\prime}, t^{\prime}}^{(2)} b_{t^{\prime}} \sigma\left(\hat{V}}, k}^{(1)<em t="1">{t^{\prime}, k}^{(2)}\right) \sum</em>}^{d_{3}} \Gamma_{q_{i j}, t}^{(2)} b_{t} \sigma\left(\hat{V<em t_prime="t^{\prime">{t, k}^{(2)}\right)\right] \
&amp; \stackrel{(a)}{=} \sum</em>}=1}^{d_{3}} \sum_{t=1}^{d_{3}} \mathbb{E}\left[\Gamma_{q^{\prime}, t^{\prime}}^{(2)} \Gamma_{q_{i j}, t}^{(2)}\right] \mathbb{E}\left[b_{t^{\prime}} b_{t}\right] \mathbb{E}\left[\sigma\left(\hat{V<em k="k" t_="t,">{t^{\prime}, k}^{(2)}\right) \sigma\left(\hat{V}</em>\right)\right] \
&amp; \stackrel{(b)}{=} 0
\end{aligned}
$$}^{(2)</p>
<p>where $(a)$ is from the mutual independence of $b_{t}, \Gamma_{q_{i j}, t}^{(2)}$ and $\sigma\left(\hat{V}<em q_prime="q^{\prime">{t, k}^{(2)}\right)$, and $(b)$ is from the independence of two i.i.d. zero-mean Gaussians $\Gamma</em>}, t^{\prime}}^{(2)}$ and $\Gamma_{q_{i j}, t}^{(2)}$, by our assumption. Therefore, $\hat{V<em j="j" q__i="q_{i">{q^{\prime}, k}^{(1)}$ and $\hat{V}</em>$, as desired.
Second, we consider the saliency map and DeconvNet cases. As $\hat{V}}, k}^{(1)}$ are uncorrelated with each other for any $q^{\prime} \neq q_{i j<em i="i" j="j">{q</em>$ for saliency map becomes}, k}^{(1)</p>
<p>$$
\hat{V}<em i="i" j="j">{q</em>
$$}, k}^{(1)} \approx \sum_{t=1}^{d_{3}} \Gamma_{q_{i j}, t}^{(2)} b_{t} \hat{V}_{t, k}^{(2)</p>
<p>where $\hat{V}_{t, k}^{(2)}$ comes from the definition</p>
<p>$$
\hat{V}<em k="k">{, k}^{(2)}=\frac{\partial o^{(3)}}{\partial o^{(2)}} \cdots \frac{\partial o^{(L-1)}}{\partial o^{(L-2)}} \cdot \Gamma</em>
$$}^{(L)</p>
<p>which is also approximately independent of $\Gamma_{q_{i j}, t}^{(2)}$ as before, and the other parameters are exactly the same with the GBP case, the independence approximation also holds for saliency map.
For DeconvNet, $\hat{V}<em i="i" j="j">{q</em>$ becomes}, k}^{(1)</p>
<p>$$
\hat{V}<em i="i" j="j">{q</em>
$$}, k}^{(1)} \approx \sum_{t=1}^{d_{3}} \Gamma_{q_{i j}, t}^{(2)} \hat{V}_{t, k}^{(2)</p>
<p>where $\hat{V}_{t, k}^{(2)}$ is defined identically as (16) for GBP. Since this is a special case of GBP, the analysis in the case trivially holds for DeconvNet as well.</p>
<h2>E. More Experiments on Random/Trained VGG-16 Net</h2>
<p>We provide more results for backpropagation-based visualizations including saliency map, DeconvNet and GBP in both untrained (randomly initialized) and trained VGG-16 net. The input images - labeled as "dog", "panda", "forest" and "mastiff" - are randomly chosen from the ImageNet dataset. As we can see, all the results (Figures 10 - 17) are consistent with our previous empirical observations that GBP and DeconvNet are more visually compelling but less class-sensitive than saliency map.</p>
<h2>F. Comparison Between GBP and Edge Detector</h2>
<p>Here we compare the GBP visualization with a linear vertical edge detector, as shown in Figure 18. At the first glance, the GBP visualizations in a trained VGG-16 net are very similar to the results of an edge detector. In other words, GBP indeed pays much attention to the edge information like a Gabor filter. However, there exist subtle differences between GBP and linear edge detectors. As we can see, the linear vertical edge detector will highlight all the horizontal intensity changes, while GBP has the additional ability to filter out some background image patches.</p>
<h2>G. More Experiments on Partly Trained VGG-16 Net</h2>
<p>In this section, we provide more GBP visualizations by feeding more images to a partly trained VGG-16 net. Specifically, we consider two kinds of weights loading strategies for the VGG-16 net. The first one is to load trained weights up to a given layer as shown in Figure 19. The second one is to load trained weights for all the layers except for a given layer as shown in Figure 20. The results are consistent with our previous analysis: it is the trained weights in the convolutional layers rather than those in the dense layers that account for filtering out image patches. Also, earlier convolutional layers have a greater impact on the GBP visualization than later convolutional layers.</p>
<h2>H. More Experiments on ResNet</h2>
<p>Our theoretical analysis shows that for GBP it is the local connections in CNNs, together with the backward ReLU, that contribute to the clean-looking visualizations. Here we further investigate backpropagation-based visualizations on both randomly initialized (Figure 20) and trained (Figure 21) ResNet-50. In general, the results are very similar to those in the VGG-16 net. However, we do observe some additional grid-like textures here and we conjecture that this deterioration of visual quality is due to the skip connections, as we have shown earlier that network structure has a significant impact on the visualizations. We leave the rigorous analysis of this phenomenon for future work.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Saliency map, DeconvNet and GBP visualizations for the random VGG-16 net with the input image "dog".
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Saliency map, DeconvNet and GBP visualizations for the trained VGG-16 net with the input image "dog".</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. Saliency map, DeconvNet and GBP visualizations for the random VGG-16 net with the input image "panda".
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. Saliency map, DeconvNet and GBP visualizations for the trained VGG-16 net with the input image "panda".</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. Saliency map, DeconvNet and GBP visualizations for the random VGG-16 net with the input image "forest".
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15. Saliency map, DeconvNet and GBP visualizations for the trained VGG-16 net with the input image "forest".</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16. Saliency map, DeconvNet and GBP visualizations for the random VGG-16 net with the input image "mastiff".
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17. Saliency map, DeconvNet and GBP visualizations for the trained VGG-16 net with the input image "mastiff".</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18. Comparison between the GBP visualization and the linear edge detector. The left column contains three sample inputs. The middle column contains the GBP visualization for each input. The right column is a linear vertical edge detector applied to each input. Specifically, the edge detector is designed by taking each pixel in the image and subtracting the neighboring pixel on the left.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19. Load the trained weights of the VGG-16 net up to the indexed layer and leave the rest layers to be randomly initialized (denoted by the star sign) with different input images.
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20. Load the trained weights of the VGG-16 net except for the indexed layer which is randomly initialized instead (denoted by the diamond sign) with different input images.</p>
<p><img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 21. Saliency map, DeconvNet and GBP visualizations for the random ResNet-50 with the input image "tabby".
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 22. Saliency map, DeconvNet and GBP visualizations for the trained ResNet-50 net with the input image "tabby".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Here we assume a VALID padding method implicitly, and other padding methods do not impact our analysis.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
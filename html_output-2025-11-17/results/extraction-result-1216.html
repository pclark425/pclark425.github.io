<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1216 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1216</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1216</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-238634678</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2110.06149v2.pdf" target="_blank">Planning from Pixels in Environments with Combinatorially Hard Search Spaces</a></p>
                <p><strong>Paper Abstract:</strong> The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artificial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this field brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without significant issues, such as limited generalization capabilities and difficulties when dealing with combinatorially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentification to reduce the complexity of finding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans. Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as"one-shot"planning, or in an offline RL paradigm which only provides low-quality trajectories.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1216.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1216.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPGS world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PPGS latent world model (encoder + hybrid forward + inverse)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A jointly trained latent world model used by PPGS that maps 64x64 RGB observations to a d=16 hyperspherical latent space via a ResNet encoder, predicts next embeddings with a hybrid forward model that conditions on a context image, and uses an inverse model plus a margin loss to enforce a discrete, reidentifiable latent structure for graph-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PPGS latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional encoder (ResNet-18 backbone) producing 16-D unit-norm embeddings on a hypersphere; a hybrid forward model that transforms the embedding through a deconvolutional network, concatenates it with an RGB context observation and an action one-hot, processes the result with a second ResNet-18 and outputs the next embedding; a probabilistic inverse model (one-layer MLP, 32 units) trained to predict actions between embeddings; training optimizes three losses (one-step L2 prediction loss on embeddings L_FW, inverse-model cross-entropy L_CE, and a margin loss L_margin that enforces minimum Euclidean distance ε=0.1 between non-bisimilar states). No image decoder/reconstruction is used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / hybrid (observation-conditioned) model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Procedurally-generated combinatorial planning environments from pixels (ProcgenMaze, IceSlider, DigitJump)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>One-step embedding L2 prediction loss (Monte Carlo estimate over horizons), nearest-neighbor ranking metrics for autoregressive predictions H@K and MMR@K (rank of predicted embedding among true embeddings), and qualitative success rate of downstream planner</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: reported as 'high accuracy' sufficient for constructing latent graphs and enabling search; predictive accuracy degrades if the hybrid (context) input is removed. Exact numeric MSE / H@K / MMR@K values are not provided in the paper. Forward-model training used one-step L2 loss over embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: latent space is intentionally structured (hypersphere + margin) so that embeddings can be reidentified as discrete states, enabling construction and (visualizable) search of a latent graph; latent nodes correspond to approximate bisimilar environment states but latent dimensions are not claimed to correspond to specific physical variables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Enforced geometric structure (unit hypersphere, margin ε) for reidentification; inverse-model regularization to avoid collapse; visualization/inspection of latent graphs and nearest-neighbor ranking metrics (H@K, MMR@K) used in ablations (no explicit attention maps or symbolic extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: world model trained for 40 epochs, batch size 128, ~20 hours on a single NVIDIA Ampere GPU (authors' report). Data: typically 400k environment steps (20 trajectories × 20 steps × 1000 levels) used for default training. Inference/planning cost: each tree expansion requires one forward pass of the dynamics network; planning can be computationally heavy (per-expansion forward pass non-negligible); planning uses a cutoff of C=256 leaves and replan horizon T_max=10 in full planner.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample efficiency: far more sample-efficient than PPO in experiments (PPGS trains on 400k offline random steps versus PPO's 50M on-policy steps). Computational/time efficiency: higher per-decision compute due to explicit latent graph expansion and many forward-model calls, but this is traded off for lower sample complexity and stronger generalization on combinatorial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Empirical: PPGS outperforms baselines (PPO, DreamerV2, GLAMOR, and a GS-on-images baseline) across the three evaluated environments; achieves markedly better generalization when trained on few levels (paper states PPGS surpasses PPO having seen 10 levels versus PPO needing ~1000 levels in ProcgenMaze). Exact per-task numeric success percentages are reported in the paper's tables/figures but not transcribed in text body; overall claims are strong superiority on combinatorially hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High predictive fidelity in latent embeddings (especially with context) translates to high utility for combinatorial planning: accurate one-step predictions and reliable reidentification enable breadth-first search in latent graph to succeed. The authors emphasize that task-relevant features are prioritized (no reconstruction loss), which helps for planning but sacrifices visual reconstruction interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Design trades: (1) hybrid forward model (adds context) improves fidelity (especially for tasks requiring global structure) but increases computational cost per expansion; (2) margin ε enforces discreteness and enables reidentification but constrains packing capacity (though capacity grows with latent dimensionality via kissing number); (3) avoiding image reconstruction reduces vulnerability to visual noise and forces task-focused latents but removes pixel-level interpretability; (4) explicit graph search is sample-efficient but higher in runtime per decision compared to model-free policies.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: hyperspherical latent space (d=16) with unit L2 normalization; margin ε=0.1 to define reidentification; hybrid forward model conditioning on a context observation s_c to recover invariant structure; inverse model cross-entropy loss to prevent collapse and regularize latent structure; no decoder/reconstruction loss; combined loss L = α L_FW + β L_CE + L_margin with α=10, β=1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to model-free PPO and model-based DreamerV2 and to GLAMOR: PPGS is more sample-efficient (400k offline vs 50M on-policy for PPO) and generalizes better on combinatorial tasks; compared to GS-on-images (no learning), PPGS attains better robustness to visual variability and can generalize from training levels; compared to MuZero-like approaches, PPGS focuses on recovering explicit discrete latent graphs via reidentification instead of MCTS-guided recurrent latent planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends: (a) use a context-conditioned (hybrid) forward model when global/invariant structure matters, (b) jointly train an inverse model to avoid collapse and structure the latent space, (c) enforce a margin on hyperspherical embeddings for reliable reidentification, (d) combine one-shot planning with MPC-style replanning and a lookup of observed transitions to mitigate accumulated model errors. Authors note limitations: scale to huge action spaces and very large grid sizes remains challenging.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1216.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1216.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (latent imagination / model-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL approach that learns a latent dynamics model and uses latent imagination for policy learning and planning, cited and used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent latent dynamics model trained with a variational objective that includes reconstruction (decoder) paths, and uses latent rollouts ('imagination') for policy and value learning; relies on a learned world model and value/policy networks trained on imagined trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (variational / recurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Discrete games and continuous control (Atari and control benchmarks); used here as a baseline on pixel-based procedurally generated environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured via reconstruction losses and prediction accuracy over latent sequences (not explicitly enumerated in this paper). In this paper the authors do not report DreamerV2 internal fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; DreamerV2 was run for a reduced budget (4M environment steps) in the experiments and struggled to generalize on the procedurally generated combinatorial tasks under the experimental budgets used.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural latent model with reconstruction objective; latent states are not explicitly discrete or reidentifiable as graph nodes in DreamerV2's standard formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No specific interpretability methods mentioned in this paper for DreamerV2; the paper emphasizes differences (PPGS avoids reconstruction, enforces discrete reidentifiable latent nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: DreamerV2 has large computational requirements; in this paper it was run for 4M steps only due to compute limits (authors state this is less than typical for full convergence). Exact training time / GPU usage not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less sample-efficient than PPGS on the combinatorial tasks in this paper's experiments given budgets used; required more compute and training steps in prior work to reach top performance on Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Under the experimental budget reported here (4M steps) DreamerV2 struggled across the three combinatorial environments and achieved lower success rates than PPGS (exact numbers provided in paper tables/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>While DreamerV2 learns usable latent dynamics for many control tasks, in this paper its learned model and policy did not generalize as well as PPGS's latent model plus explicit graph search for combinatorially hard, procedurally generated levels.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>DreamerV2 relies on reconstruction objectives and stochastic/deterministic latent paths that can be computationally expensive to train; its learned latent may be expressive but not structured for discrete-state reidentification needed for graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses a VAE-style latent objective with reconstruction; uses imagination rollouts for policy/value learning; not tailored to enforce discrete margins or explicit reidentification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to PPGS in this paper: DreamerV2 had lower task success under the authors' compute/data budgets; DreamerV2 is more general-purpose but less sample-efficient and less directly suitable for explicit graph search without additional structure.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in detail in this paper; authors note DreamerV2 was run with limited steps and likely would need larger budgets to improve performance on the evaluated tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1216.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1216.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero (Schrittwieser et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (planning with a learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent world model trained to predict value, policy and dynamics to guide Monte Carlo Tree Search (MCTS); cited as a strong example of coupling learned world models with classical planners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent latent prediction model trained to predict future rewards, policies and dynamics (no pixel reconstruction), and used to guide MCTS for planning; hidden states are optimized to be predictive of future signals relevant to planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model coupled to search (MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games (Go, chess, shogi), Atari; general game playing with planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not detailed in this paper; MuZero trains hidden state predictors for value/policy/reward prediction rather than explicit next-observation reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not provided in this paper (MuZero is only discussed in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Hidden states are optimized for planning signals; not explicitly interpretable or discrete for state reidentification as in PPGS.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No interpretability methods discussed in this paper regarding MuZero.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MuZero is computationally intensive (MCTS + recurrent model), but specifics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as related work demonstrating coupling of learned models and classical planners; PPGS differs by explicitly enforcing discrete latent structure for efficient graph search rather than MCTS-style search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated here; MuZero is cited for strong performance on large discrete games in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero shows that learned dynamics tailored to planning signals can guide powerful search (MCTS); authors contrast MuZero's approach with their focus on recovering discrete latent graphs and not relying on reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not directly evaluated here; MuZero-type approaches require heavy compute (MCTS) and are optimized for rewards, whereas PPGS focuses on visual-goal planning without rewards by enforcing explicit reidentification.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Optimize hidden states to predict reward/policy/value signals; use MCTS to plan in hidden-state space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually: MuZero uses MCTS and reward-driven hidden-state training; PPGS enforces discrete reidentification and breadth-first-like graph search, and trains only on dynamics (no reward supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper beyond the conceptual contrast.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1216.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1216.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ha & Schmidhuber 'World Models'</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent world models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early influential approach that learns a compact latent world model (VAE + recurrent dynamics) and then trains controllers operating on latent representations; cited as seminal work on learning world models in low-dimensional latent spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent world models facilitate policy evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Train a VAE to reconstruct observations and a recurrent model to predict latent dynamics; use the learned latent dynamics to train controllers (e.g., via evolution) in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + recurrent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Simulated environments and control tasks (e.g., car racing, VizDoom)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss and predictive performance of recurrent latent dynamics (not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported here; referenced historically as successful demonstration that compact latent dynamics can be learned.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent is learned for reconstruction and control; not explicitly discrete/reidentifiable for graph search as in PPGS.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>VAE reconstruction provides pixel-level check; no discrete reidentification enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed in this paper; earlier work is generally cheaper than modern large models but requires training VAE + recurrent network.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared numerically here; PPGS departs by removing reconstruction and adding inverse/margin losses to yield a discrete-like latent for graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper; cited as prior foundational work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Shows latent models can be useful for control; PPGS builds on the idea of latent dynamics but modifies objectives to prioritize reidentification and planning utility rather than reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>VAE-based models encourage reconstruction (may capture irrelevant visual detail) whereas PPGS avoids decoder to focus on task-relevant features and robust planning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE for encoder/decoder; recurrent latent dynamics for imagination-based control.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PPGS differs by removing reconstruction and adding inverse and margin losses to create a discrete reidentifiable latent structured for explicit graph search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1216.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1216.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepMDP: Learning continuous latent space models for representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that learns latent spaces by predicting rewards and action-conditional state distributions to induce MDP-consistent representations; cited as related work on objective design for latent world models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepMDP: Learning continuous latent space models for representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepMDP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns latent representations by training to predict rewards and transitions in latent space, encouraging MDP-like structure (bisimulation-inspired objectives); typically includes learned predictors on latent states rather than pixel reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (MDP-consistent objective)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Representation learning for RL from pixels (general)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Latent predictive objectives and downstream task performance (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not provided in this paper; cited for conceptual relevance to learning predictive latent representations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Encourages semantically meaningful latent structure (bisimulation-like), but not explicitly discrete; interpretability depends on learned structure and downstream analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Objective enforces MDP-consistent predictions (rewards/transitions) rather than pixel reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Conceptually similar motivation to PPGS in learning task-relevant latent structure; PPGS differs by enforcing discrete margins and using inverse model for collapse prevention.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>DeepMDP objectives aim to create latents useful for control; PPGS leverages a related idea but implements a margin/inverse loss and context-conditioned forward model specifically for combinatorial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>DeepMDP stresses reward/transition prediction; PPGS prioritizes reidentifiability for graph search and avoids reconstruction to focus on planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train latent predictors for rewards and transitions; encourage bisimulation-like properties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PPGS chooses different losses (inverse CE + margin + forward L2) and architecture (hybrid forward with context) geared to explicit reidentification and graph planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>DeepMDP: Learning continuous latent space models for representation learning <em>(Rating: 2)</em></li>
                <li>Learning invariant representations for reinforcement learning without reconstruction <em>(Rating: 2)</em></li>
                <li>World model as a graph: Learning latent landmarks for planning <em>(Rating: 2)</em></li>
                <li>Contrastive learning of structured world models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1216",
    "paper_id": "paper-238634678",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "PPGS world model",
            "name_full": "PPGS latent world model (encoder + hybrid forward + inverse)",
            "brief_description": "A jointly trained latent world model used by PPGS that maps 64x64 RGB observations to a d=16 hyperspherical latent space via a ResNet encoder, predicts next embeddings with a hybrid forward model that conditions on a context image, and uses an inverse model plus a margin loss to enforce a discrete, reidentifiable latent structure for graph-based planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PPGS latent world model",
            "model_description": "Convolutional encoder (ResNet-18 backbone) producing 16-D unit-norm embeddings on a hypersphere; a hybrid forward model that transforms the embedding through a deconvolutional network, concatenates it with an RGB context observation and an action one-hot, processes the result with a second ResNet-18 and outputs the next embedding; a probabilistic inverse model (one-layer MLP, 32 units) trained to predict actions between embeddings; training optimizes three losses (one-step L2 prediction loss on embeddings L_FW, inverse-model cross-entropy L_CE, and a margin loss L_margin that enforces minimum Euclidean distance ε=0.1 between non-bisimilar states). No image decoder/reconstruction is used.",
            "model_type": "latent world model / hybrid (observation-conditioned) model",
            "task_domain": "Procedurally-generated combinatorial planning environments from pixels (ProcgenMaze, IceSlider, DigitJump)",
            "fidelity_metric": "One-step embedding L2 prediction loss (Monte Carlo estimate over horizons), nearest-neighbor ranking metrics for autoregressive predictions H@K and MMR@K (rank of predicted embedding among true embeddings), and qualitative success rate of downstream planner",
            "fidelity_performance": "Qualitative: reported as 'high accuracy' sufficient for constructing latent graphs and enabling search; predictive accuracy degrades if the hybrid (context) input is removed. Exact numeric MSE / H@K / MMR@K values are not provided in the paper. Forward-model training used one-step L2 loss over embeddings.",
            "interpretability_assessment": "Partially interpretable: latent space is intentionally structured (hypersphere + margin) so that embeddings can be reidentified as discrete states, enabling construction and (visualizable) search of a latent graph; latent nodes correspond to approximate bisimilar environment states but latent dimensions are not claimed to correspond to specific physical variables.",
            "interpretability_method": "Enforced geometric structure (unit hypersphere, margin ε) for reidentification; inverse-model regularization to avoid collapse; visualization/inspection of latent graphs and nearest-neighbor ranking metrics (H@K, MMR@K) used in ablations (no explicit attention maps or symbolic extraction).",
            "computational_cost": "Training: world model trained for 40 epochs, batch size 128, ~20 hours on a single NVIDIA Ampere GPU (authors' report). Data: typically 400k environment steps (20 trajectories × 20 steps × 1000 levels) used for default training. Inference/planning cost: each tree expansion requires one forward pass of the dynamics network; planning can be computationally heavy (per-expansion forward pass non-negligible); planning uses a cutoff of C=256 leaves and replan horizon T_max=10 in full planner.",
            "efficiency_comparison": "Sample efficiency: far more sample-efficient than PPO in experiments (PPGS trains on 400k offline random steps versus PPO's 50M on-policy steps). Computational/time efficiency: higher per-decision compute due to explicit latent graph expansion and many forward-model calls, but this is traded off for lower sample complexity and stronger generalization on combinatorial tasks.",
            "task_performance": "Empirical: PPGS outperforms baselines (PPO, DreamerV2, GLAMOR, and a GS-on-images baseline) across the three evaluated environments; achieves markedly better generalization when trained on few levels (paper states PPGS surpasses PPO having seen 10 levels versus PPO needing ~1000 levels in ProcgenMaze). Exact per-task numeric success percentages are reported in the paper's tables/figures but not transcribed in text body; overall claims are strong superiority on combinatorially hard tasks.",
            "task_utility_analysis": "High predictive fidelity in latent embeddings (especially with context) translates to high utility for combinatorial planning: accurate one-step predictions and reliable reidentification enable breadth-first search in latent graph to succeed. The authors emphasize that task-relevant features are prioritized (no reconstruction loss), which helps for planning but sacrifices visual reconstruction interpretability.",
            "tradeoffs_observed": "Design trades: (1) hybrid forward model (adds context) improves fidelity (especially for tasks requiring global structure) but increases computational cost per expansion; (2) margin ε enforces discreteness and enables reidentification but constrains packing capacity (though capacity grows with latent dimensionality via kissing number); (3) avoiding image reconstruction reduces vulnerability to visual noise and forces task-focused latents but removes pixel-level interpretability; (4) explicit graph search is sample-efficient but higher in runtime per decision compared to model-free policies.",
            "design_choices": "Key choices: hyperspherical latent space (d=16) with unit L2 normalization; margin ε=0.1 to define reidentification; hybrid forward model conditioning on a context observation s_c to recover invariant structure; inverse model cross-entropy loss to prevent collapse and regularize latent structure; no decoder/reconstruction loss; combined loss L = α L_FW + β L_CE + L_margin with α=10, β=1.",
            "comparison_to_alternatives": "Compared experimentally to model-free PPO and model-based DreamerV2 and to GLAMOR: PPGS is more sample-efficient (400k offline vs 50M on-policy for PPO) and generalizes better on combinatorial tasks; compared to GS-on-images (no learning), PPGS attains better robustness to visual variability and can generalize from training levels; compared to MuZero-like approaches, PPGS focuses on recovering explicit discrete latent graphs via reidentification instead of MCTS-guided recurrent latent planning.",
            "optimal_configuration": "Paper recommends: (a) use a context-conditioned (hybrid) forward model when global/invariant structure matters, (b) jointly train an inverse model to avoid collapse and structure the latent space, (c) enforce a margin on hyperspherical embeddings for reliable reidentification, (d) combine one-shot planning with MPC-style replanning and a lookup of observed transitions to mitigate accumulated model errors. Authors note limitations: scale to huge action spaces and very large grid sizes remains challenging.",
            "uuid": "e1216.0"
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (latent imagination / model-based RL)",
            "brief_description": "A model-based RL approach that learns a latent dynamics model and uses latent imagination for policy learning and planning, cited and used as a baseline in experiments.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "use",
            "model_name": "DreamerV2",
            "model_description": "Recurrent latent dynamics model trained with a variational objective that includes reconstruction (decoder) paths, and uses latent rollouts ('imagination') for policy and value learning; relies on a learned world model and value/policy networks trained on imagined trajectories.",
            "model_type": "latent world model (variational / recurrent)",
            "task_domain": "Discrete games and continuous control (Atari and control benchmarks); used here as a baseline on pixel-based procedurally generated environments",
            "fidelity_metric": "Typically measured via reconstruction losses and prediction accuracy over latent sequences (not explicitly enumerated in this paper). In this paper the authors do not report DreamerV2 internal fidelity metrics.",
            "fidelity_performance": "Not reported in this paper; DreamerV2 was run for a reduced budget (4M environment steps) in the experiments and struggled to generalize on the procedurally generated combinatorial tasks under the experimental budgets used.",
            "interpretability_assessment": "Black-box neural latent model with reconstruction objective; latent states are not explicitly discrete or reidentifiable as graph nodes in DreamerV2's standard formulation.",
            "interpretability_method": "No specific interpretability methods mentioned in this paper for DreamerV2; the paper emphasizes differences (PPGS avoids reconstruction, enforces discrete reidentifiable latent nodes).",
            "computational_cost": "High: DreamerV2 has large computational requirements; in this paper it was run for 4M steps only due to compute limits (authors state this is less than typical for full convergence). Exact training time / GPU usage not reported here.",
            "efficiency_comparison": "Less sample-efficient than PPGS on the combinatorial tasks in this paper's experiments given budgets used; required more compute and training steps in prior work to reach top performance on Atari.",
            "task_performance": "Under the experimental budget reported here (4M steps) DreamerV2 struggled across the three combinatorial environments and achieved lower success rates than PPGS (exact numbers provided in paper tables/figures).",
            "task_utility_analysis": "While DreamerV2 learns usable latent dynamics for many control tasks, in this paper its learned model and policy did not generalize as well as PPGS's latent model plus explicit graph search for combinatorially hard, procedurally generated levels.",
            "tradeoffs_observed": "DreamerV2 relies on reconstruction objectives and stochastic/deterministic latent paths that can be computationally expensive to train; its learned latent may be expressive but not structured for discrete-state reidentification needed for graph search.",
            "design_choices": "Uses a VAE-style latent objective with reconstruction; uses imagination rollouts for policy/value learning; not tailored to enforce discrete margins or explicit reidentification.",
            "comparison_to_alternatives": "Compared to PPGS in this paper: DreamerV2 had lower task success under the authors' compute/data budgets; DreamerV2 is more general-purpose but less sample-efficient and less directly suitable for explicit graph search without additional structure.",
            "optimal_configuration": "Not discussed in detail in this paper; authors note DreamerV2 was run with limited steps and likely would need larger budgets to improve performance on the evaluated tasks.",
            "uuid": "e1216.1"
        },
        {
            "name_short": "MuZero (Schrittwieser et al.)",
            "name_full": "MuZero (planning with a learned model)",
            "brief_description": "A recurrent world model trained to predict value, policy and dynamics to guide Monte Carlo Tree Search (MCTS); cited as a strong example of coupling learned world models with classical planners.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero",
            "model_description": "Recurrent latent prediction model trained to predict future rewards, policies and dynamics (no pixel reconstruction), and used to guide MCTS for planning; hidden states are optimized to be predictive of future signals relevant to planning.",
            "model_type": "latent world model coupled to search (MCTS)",
            "task_domain": "Board games (Go, chess, shogi), Atari; general game playing with planning",
            "fidelity_metric": "Not detailed in this paper; MuZero trains hidden state predictors for value/policy/reward prediction rather than explicit next-observation reconstruction.",
            "fidelity_performance": "Not provided in this paper (MuZero is only discussed in related work).",
            "interpretability_assessment": "Hidden states are optimized for planning signals; not explicitly interpretable or discrete for state reidentification as in PPGS.",
            "interpretability_method": "No interpretability methods discussed in this paper regarding MuZero.",
            "computational_cost": "MuZero is computationally intensive (MCTS + recurrent model), but specifics are not provided in this paper.",
            "efficiency_comparison": "Mentioned as related work demonstrating coupling of learned models and classical planners; PPGS differs by explicitly enforcing discrete latent structure for efficient graph search rather than MCTS-style search.",
            "task_performance": "Not evaluated here; MuZero is cited for strong performance on large discrete games in prior work.",
            "task_utility_analysis": "MuZero shows that learned dynamics tailored to planning signals can guide powerful search (MCTS); authors contrast MuZero's approach with their focus on recovering discrete latent graphs and not relying on reward signals.",
            "tradeoffs_observed": "Not directly evaluated here; MuZero-type approaches require heavy compute (MCTS) and are optimized for rewards, whereas PPGS focuses on visual-goal planning without rewards by enforcing explicit reidentification.",
            "design_choices": "Optimize hidden states to predict reward/policy/value signals; use MCTS to plan in hidden-state space.",
            "comparison_to_alternatives": "Compared conceptually: MuZero uses MCTS and reward-driven hidden-state training; PPGS enforces discrete reidentification and breadth-first-like graph search, and trains only on dynamics (no reward supervision).",
            "optimal_configuration": "Not discussed in this paper beyond the conceptual contrast.",
            "uuid": "e1216.2"
        },
        {
            "name_short": "Ha & Schmidhuber 'World Models'",
            "name_full": "Recurrent world models (Ha & Schmidhuber)",
            "brief_description": "Early influential approach that learns a compact latent world model (VAE + recurrent dynamics) and then trains controllers operating on latent representations; cited as seminal work on learning world models in low-dimensional latent spaces.",
            "citation_title": "Recurrent world models facilitate policy evolution",
            "mention_or_use": "mention",
            "model_name": "World Models (Ha & Schmidhuber)",
            "model_description": "Train a VAE to reconstruct observations and a recurrent model to predict latent dynamics; use the learned latent dynamics to train controllers (e.g., via evolution) in latent space.",
            "model_type": "latent world model (VAE + recurrent dynamics)",
            "task_domain": "Simulated environments and control tasks (e.g., car racing, VizDoom)",
            "fidelity_metric": "Reconstruction loss and predictive performance of recurrent latent dynamics (not quantified in this paper).",
            "fidelity_performance": "Not reported here; referenced historically as successful demonstration that compact latent dynamics can be learned.",
            "interpretability_assessment": "Latent is learned for reconstruction and control; not explicitly discrete/reidentifiable for graph search as in PPGS.",
            "interpretability_method": "VAE reconstruction provides pixel-level check; no discrete reidentification enforced.",
            "computational_cost": "Not detailed in this paper; earlier work is generally cheaper than modern large models but requires training VAE + recurrent network.",
            "efficiency_comparison": "Not directly compared numerically here; PPGS departs by removing reconstruction and adding inverse/margin losses to yield a discrete-like latent for graph search.",
            "task_performance": "Not evaluated in this paper; cited as prior foundational work.",
            "task_utility_analysis": "Shows latent models can be useful for control; PPGS builds on the idea of latent dynamics but modifies objectives to prioritize reidentification and planning utility rather than reconstruction.",
            "tradeoffs_observed": "VAE-based models encourage reconstruction (may capture irrelevant visual detail) whereas PPGS avoids decoder to focus on task-relevant features and robust planning.",
            "design_choices": "VAE for encoder/decoder; recurrent latent dynamics for imagination-based control.",
            "comparison_to_alternatives": "PPGS differs by removing reconstruction and adding inverse and margin losses to create a discrete reidentifiable latent structured for explicit graph search.",
            "uuid": "e1216.3"
        },
        {
            "name_short": "DeepMDP",
            "name_full": "DeepMDP: Learning continuous latent space models for representation learning",
            "brief_description": "A framework that learns latent spaces by predicting rewards and action-conditional state distributions to induce MDP-consistent representations; cited as related work on objective design for latent world models.",
            "citation_title": "DeepMDP: Learning continuous latent space models for representation learning",
            "mention_or_use": "mention",
            "model_name": "DeepMDP",
            "model_description": "Learns latent representations by training to predict rewards and transitions in latent space, encouraging MDP-like structure (bisimulation-inspired objectives); typically includes learned predictors on latent states rather than pixel reconstruction.",
            "model_type": "latent world model (MDP-consistent objective)",
            "task_domain": "Representation learning for RL from pixels (general)",
            "fidelity_metric": "Latent predictive objectives and downstream task performance (not quantified here).",
            "fidelity_performance": "Not provided in this paper; cited for conceptual relevance to learning predictive latent representations.",
            "interpretability_assessment": "Encourages semantically meaningful latent structure (bisimulation-like), but not explicitly discrete; interpretability depends on learned structure and downstream analyses.",
            "interpretability_method": "Objective enforces MDP-consistent predictions (rewards/transitions) rather than pixel reconstruction.",
            "computational_cost": "Not specified here.",
            "efficiency_comparison": "Conceptually similar motivation to PPGS in learning task-relevant latent structure; PPGS differs by enforcing discrete margins and using inverse model for collapse prevention.",
            "task_performance": "Not evaluated in this paper.",
            "task_utility_analysis": "DeepMDP objectives aim to create latents useful for control; PPGS leverages a related idea but implements a margin/inverse loss and context-conditioned forward model specifically for combinatorial planning.",
            "tradeoffs_observed": "DeepMDP stresses reward/transition prediction; PPGS prioritizes reidentifiability for graph search and avoids reconstruction to focus on planning utility.",
            "design_choices": "Train latent predictors for rewards and transitions; encourage bisimulation-like properties.",
            "comparison_to_alternatives": "PPGS chooses different losses (inverse CE + margin + forward L2) and architecture (hybrid forward with context) geared to explicit reidentification and graph planning.",
            "uuid": "e1216.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 2,
            "sanitized_title": "recurrent_world_models_facilitate_policy_evolution"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "DeepMDP: Learning continuous latent space models for representation learning",
            "rating": 2,
            "sanitized_title": "deepmdp_learning_continuous_latent_space_models_for_representation_learning"
        },
        {
            "paper_title": "Learning invariant representations for reinforcement learning without reconstruction",
            "rating": 2,
            "sanitized_title": "learning_invariant_representations_for_reinforcement_learning_without_reconstruction"
        },
        {
            "paper_title": "World model as a graph: Learning latent landmarks for planning",
            "rating": 2,
            "sanitized_title": "world_model_as_a_graph_learning_latent_landmarks_for_planning"
        },
        {
            "paper_title": "Contrastive learning of structured world models",
            "rating": 1,
            "sanitized_title": "contrastive_learning_of_structured_world_models"
        }
    ],
    "cost": 0.0176625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Planning from Pixels in Environments with Combinatorially Hard Search Spaces
10 10 10 10 18 Mar 2022</p>
<p>Marco Bagatella mbagatella@tue.mpg.de 
Max Planck Institute for Intelligent Systems
Computer Science Department University Innsbruck
Max Planck Institute for Intelligent Systems
Max Planck Institute for Intelligent Systems
Tübingen, Tübingen, TübingenGermany, Austria, Germany, Germany</p>
<p>Mirek Olšák 
Max Planck Institute for Intelligent Systems
Computer Science Department University Innsbruck
Max Planck Institute for Intelligent Systems
Max Planck Institute for Intelligent Systems
Tübingen, Tübingen, TübingenGermany, Austria, Germany, Germany</p>
<p>Michal Rolínek michal.rolinek@tue.mpg.de 
Max Planck Institute for Intelligent Systems
Computer Science Department University Innsbruck
Max Planck Institute for Intelligent Systems
Max Planck Institute for Intelligent Systems
Tübingen, Tübingen, TübingenGermany, Austria, Germany, Germany</p>
<p>Georg Martius georg.martius@tue.mpg.de 
Max Planck Institute for Intelligent Systems
Computer Science Department University Innsbruck
Max Planck Institute for Intelligent Systems
Max Planck Institute for Intelligent Systems
Tübingen, Tübingen, TübingenGermany, Austria, Germany, Germany</p>
<p>Planning from Pixels in Environments with Combinatorially Hard Search Spaces
10 10 10 10 18 Mar 20228 9 9 9 10 10 Figure 1: Planning from Pixels with Graph Search. Our method leverages learned latent dynamics to efficiently build and search a graph representation of the environment. Resulting policies show unrivaled performance across a distribution of hard combinatorial tasks. Preprint. Under review. arXiv:2110.06149v2 [cs.LG]
The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artificial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this field brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without significant issues, such as limited generalization capabilities and difficulties when dealing with combinatorially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentification to reduce the complexity of finding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans. Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as "one-shot" planning, or in an offline RL paradigm which only provides low-quality trajectories. breadth first search (with reidentification) latent graph space network margin IceSlider 0.0 0.2 0.4 0.6 0.8 1.0 PPGS (ours) GS on IMG PPO success rate 1 2 3 3 4 5 5 6 7 8</p>
<p>Abstract</p>
<p>The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artificial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this field brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without significant issues, such as limited generalization capabilities and difficulties when dealing with combinatorially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentification to reduce the complexity of finding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans. Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as "one-shot" planning, or in an offline RL paradigm which only provides low-quality trajectories. </p>
<p>Introduction</p>
<p>Decision problems with an underlying combinatorial structure pose a significant challenge for a learning agent, as they require both the ability to infer the true low-dimensional state of the environment and the application of abstract reasoning to master it. A traditional approach for common logic games, given that a simulator or a model of the game are available, consists in applying a graph search algorithm to the state diagram, effectively simulating several trajectories to find the optimal one. As long as the state space of the game grows at a polynomial rate with respect to the planning horizon, the solver is able to efficiently find the optimal solution to the problem. Of course, when this is not the case, heuristics can be introduced at the expense of optimality of solutions.</p>
<p>Learned world models [17,18] can learn to map complex observations to a lower-dimensional latent space and retrieve an approximate simulator of an environment. However, while the continuous structure of the latent space is suitable for training reinforcement learning agents [12,19] or applying heuristic search algorithms [38], it also prevents a straightforward application of simpler graph search techniques that rely on identifying and marking visited states.</p>
<p>Our work follows naturally from the following insight: a simple graph search might be sufficient for solving visually complex environments, as long as a world model is trained to realize a suitable structure in the latent space. Moreover, the complexity of the search can be reduced from exponential to linear by reidentifying visited latent states.</p>
<p>The method we propose is located at the intersection between classical planning, representation learning and model-based reinforcement learning. It relies on a novel low-dimensional world model trained through a combination of opposing losses without reconstructing observations. We show how learned latent representations allow a dynamics model to be trained to high accuracy, and how the dynamics model can then be used to reconstruct a latent graph representing environment states as vertices and transitions as edges. The resulting latent space structure enables powerful graph search algorithms to be deployed for planning with minimal modifications, solving challenging combinatorial environments from pixels. We name our method PPGS as it Plans from Pixels through Graph Search.</p>
<p>We design PPGS to be capable of generalizing to unseen variations of the environment, or equivalently across a distribution of levels [13]. This is in contrast with traditional benchmarks [7], which require the agent to be trained and tested on the same fixed environment.</p>
<p>We can describe the main contributions of this paper as follows: first, we introduce a suite of environments that highlights a weakness of modern reinforcement learning approaches, second, we introduce a simple but principled world model architecture that can accurately learn the latent dynamics of a complex system from high dimensional observations; third, we show how a planning module can simultaneously estimate the latent graph for previously unseen environments and deploy a breadth first search in the latent space to retrieve a competitive policy; fourth, we show how combining our insights leads to unrivaled performance and generalization on a challenging class of environments.</p>
<p>Method</p>
<p>For the purpose of this paper, each environment can be modeled as a family of fully-observable deterministic goal-conditioned Markov Decision Processes with discrete actions, that is the 6-tuples {(S, A, T, G, R, γ) i } 1...n where S i is the state set, A i is the action set, T i is a transition function T i : S i × A i → S i , G i is the goal set and R i is a reward function R i : S i × G i → R and γ i is the discount factor. We remark that each environment can also be modeled as a BlockMDP [14] in which the context space X corresponds to the state set S i we introduced.</p>
<p>In particular, we deal with families of procedurally generated environments. We refer to each of the n elements of a family as a level and omit the index i when dealing with a generic level. We assume that state spaces and action spaces share the same dimensionality across all levels, that is |S i | = |S j | and |A i | = |A j | for all 0 ≤ i, j ≤ n.</p>
<p>In our work the reward simplifies to an indicator function for goal achievement R(s, g) = 1 s=g with G ⊆ S. Given a goal distribution p(g), the objective is that of finding a goal-conditioned policy π g that maximizes the return  Figure 2: Architecture of the world model. A convolutional encoder extracts latent state representations from observations, while a forward model and an inverse model reconstruct latent dynamics by predicting state transitions and actions that cause them. The notation is introduced in Sec. 2.1
J π = E g∼p(g) E τ ∼p(τ |πg) t γ t R(s t , g)(1)
where τ ∼ p(τ |π g ) is a trajectory (s t , a t ) T t=1 sampled from the policy. Our environments of interest should challenge both perceptive and reasoning capabilities of an agent. In principle, they should be solvable through extensive search in hard combinatorial spaces. In order to master them, an agent should therefore be able to (i) identify pairs of bisimilar states [43], (ii) keep track of and reidentify states it has visited in the past and (iii) produce highly accurate predictions for non-trivial time horizons. These factors contribute to making such environments very challenging for existing methods. Our method is designed in light of these necessities; it has two integral parts, the world model and the planner, which we now introduce.</p>
<p>World Model</p>
<p>The world model relies solely on three jointly trained function approximators: an encoder, a forward model and an inverse model. Their overall orchestration is depicted in Fig. 2 and described in the following.</p>
<p>Encoder</p>
<p>Mapping highly redundant observations from an environment to a low-dimensional state space Z has several benefits [17,18]. Ideally, the projection should extract the compressed "true state" of the environment and ignore irrelevant visual cues, discarding all information that is useless for planning. For this purpose, our method relies on an encoder h θ , that is a neural function approximator mapping each observed state s ∈ S and a low-dimensional representation z ∈ Z (embedding). While there are many suitable choices for the structure of the latent space Z, we choose to map observations to points on an d-dimensional hypersphere taking inspiration from Liu et al. [29].</p>
<p>Forward Model</p>
<p>In order to plan ahead in the environment, it is crucial for an agent to estimate the transition function T . In fact, if a mapping to a low-dimensional latent space Z is available, learning directly the projected transition function T Z : Z × A → Z can be largely beneficial [17,18]. The deterministic latent transition function T Z can be learned by a neural function approximator f φ so that if T (s t , a t ) = s t+1 , then f φ (h θ (s t ), a t ) := f φ (z t , a t ) = h θ (s t+1 ). We refer to this component as forward model. Intuitively, it can be trained to retrieve the representation of the state of the MDP at time t + 1 given the representation of the state and the action taken at the previous time step t.</p>
<p>Due to the Markov property of the environment, an initial state embedding z t and the action sequence (a t , . . . , a t+k ) are sufficient to to predict the latent state at time t + k, as long as z t successfully captures all relevant information from the observed state s t . The amount of information to be embedded in z t and to be retained in autoregressive predictions is, however, in most cases, prohibitive. Take for example the case of a simple maze: z t would have to encode not only the position of the agent, but, as the predictive horizon increases, most of the structure of the maze.</p>
<p>Invariant Structure Recovery To allow the encoder to only focus on local information, we adopt an hybrid forward model which can recover the invariant structures in the environment from previous observations. The function that the forward model seeks to approximate can then include an additional input: f φ (z t , a t , s c ) = z t+1 , where s c ∈ S is a generic observation from the same environment and level. Through this context input the forward model can retrieve information that is constant across time steps (e.g. the location of walls in static mazes). In practice, we can use randomly sampled observation from the same level during training and use the latest observation during evaluation. This choice allows for more accurate and structure-aware predictions, as we show in the ablations in Suppl. A.</p>
<p>Given a trajectory (s t , a t ) T t=1 , the forward model can be trained to minimize some distance measure between state embeddings (z t+1 ) 1...T −1 = (h θ (s t+1 )) 1...T −1 and one-step predictions (f φ (h θ (s t ), a t , s c )) 1...T −1 . In practice, we choose to minimize a Monte Carlo estimate of the expected Euclidean distance over a finite time horizon, a set of trajectories and a set of levels. When training on a distribution of levels p(l), we extract K trajectories of length H from each level with a uniform random policy π and we minimize
L FW = E l∼p(l) 1 H − 1 H−1 h=1 E a h ∼π f φ (z l h , a h , s l c ) − z l h+1 2 2
(2) where the superscript indicates the level from which the embeddings are extracted.</p>
<p>Inverse Model and Collapse Prevention</p>
<p>Unfortunately, the loss landscape of Equation 2 presents a trivial minimum in case the encoder collapses all embeddings to a single point in the latent space. As embeddings of any pair of states could not be distinguished in this case, this is not a desirable solution. We remark that this is a known problem in metric learning and image retrieval [8], for which solutions ranging from siamese networks [9] to using a triplet loss [22] have been proposed.</p>
<p>The context of latent world models offers a natural solution that isn't available in the general embedding problem, which consists in additionally training a probabilistic inverse model p ω (a t | z t , z t+1 ) such that if T Z (z t , a t ) = z t+1 , then p ω (a t | z t , z t+1 ) &gt; p ω (a k | z t , z t+1 )∀a k = a t ∈ A. The inverse model, parameterized by ω, can be trained to predict the action a t that causes the latent transition between two embeddings z t , z t+1 by minimizing multi-class cross entropy.
L CE = E l∼p(l) 1 H − 1 H−1 h=1 E a h ∼π − log p ω (a h | z l h , z l h+1 ) .(3)
Intuitively, L CE increases as embeddings collapse, since it becomes harder for the inverse model to recover the actions responsible for latent transitions. For this reason, it mitigates unwanted local minima. Moreover, it is empirically observed to enforce a regular structure in the latent space that eases the training procedure, as argued in Sec. A of the Appendix. We note that this loss plays a similar role to the reconstruction loss in Hafner et al. [18]. However, L CE does not force the encoder network to embed information that helps with reconstructing irrelevant parts of the observation, unlike training methods relying on image reconstruction [11,[17][18][19][20].</p>
<p>While L CE is sufficient for preventing collapse of the latent space, a discrete structure needs to be recovered in order to deploy graph search in the latent space. In particular, it is still necessary to define a criterion to reidentify nodes during the search procedure, or to establish whether two embeddings (directly encoded from observations or imagined) represent the same true low-dimensional state.</p>
<p>A straightforward way to enforce this is by introducing a margin ε, representing a desirable minimum distance between embeddings of non-bisimilar states [43]. A third and final loss term can then be introduced to encourage margins in the latent space:
L margin = E l∼p(l) 1 H − 1 H−1 h=1 max 0, 1 − z l h+1 − z l h 2 2 ε 2 .(4)
... ... Figure 3: Overview of latent-space planning. One-shot planning is possible by (i) embedding the current observation and goal to the latent space and (ii) iteratively growing a latent graph until a vertex is reidentified with the goal.</p>
<p>We then propose to reidentify two embeddings as representing the same true state if their Euclidean distance is less than ε 2 . Adopting a latent margin effectively constrains the number of margin-separated states that can be represented on an hyperspherical latent space. However, this quantity is lower-bounded by the kissing number [41], that is the number of non-overlapping unit-spheres that can be tightly packed around one d dimensional sphere. The kissing number grows exponentially with the dimensionality d. Thus, the capacity of our d-dimensional unit sphere latent space (d = 16 in our case with margin ε = 0.1) is not overly restricted.</p>
<p>The world model can be trained jointly and end-to-end by simply minimizing a combination of the three loss functions:
L = αL FW + βL CE + L margin .(5)
To summarize, the three components are respectively encouraging accurate dynamics predictions, regularizing latent representations and enforcing a discrete structure for state reidentification.</p>
<p>Planning Regimes</p>
<p>A deterministic environment can be represented as a directed graph G whose vertices V represent states s ∈ S and whose edges E encode state transitions. An edge from a vertex representing a state s ∈ S to a vertex representing a state s ∈ S is present if and only if T (s, a) = s for some action a ∈ A, where T is the state transition function of the environment. This edge can then be labelled by action a. Our planning module relies on reconstructing the latent graph, which is a projection of graph G to the latent state Z. In this section we describe how a latent graph can be build from the predictions of the world model and efficiently searched to recover a plan, as illustrated in Fig. 3. This method can be used as a one-shot planner, which only needs access to a visual goal and the initial observation from a level. When iterated and augmented with online error correction, this procedure results in a powerful approach, which we refer to as full planner, or simply as PPGS.</p>
<p>One-shot Planner Breadth First Search (BFS) is a graph search algorithm that relies on a LIFO queue and on marking visited states to find an optimal path O(V + E) steps. Its simplicity makes it an ideal candidate for solving combinatorial games by exploring their latent graph. If the number of reachable states in the environment grows polynomially, the size of the graph to search will increase at a modest rate and the method can be applied efficiently.</p>
<p>We propose to execute a BFS-like algorithm on the latent graph, which is recovered by autoregressively simulating all transitions from visited states. As depicted in Fig. 3, at each step, the new set of leaves L is retrieved by feeding the leaves from the previous iteration through the forward model f φ . The efficiency of the search process can be improved as shown in Fig. 4, by exploiting the margin ε enforced by equation 4 to reidentify states and identify loops in the latent graph. We now provide a simplified description of the planning method in Algorithm 1, while details can be found in Suppl. C.2.</p>
<p>Algorithm 1 Simplified one-shot PPGS Input: Initial observed state s 1 , visual goal g, model parameters θ, φ 1:
z 1 , z g = h θ (s 1 ), h θ (g) project to latent space Z 2: L, V = {z 1 }
sets of leaves and visited vertices 3: for T M AX steps do 4:
L = {f φ (z, a, s 1 ) : ∃z ∈ L, a ∈ A} grow graph 5:
if z * ∈ L can be reidentified with z g then 6: return action sequence from z 1 to z * Full Planner The one-shot variant of PPGS largely relies on highly accurate autoregressive predictions, which a learned model cannot usually guarantee. We mitigate this issue by adopting a model predictive control-like approach [15]. PPGS recovers an initial guess on the best policy (a i ) 1,...,n simply by applying one-shot PPGS as described in the previous paragraph and in Algorithm 2. It then applies the policy step by step and projects new observations to the latent space. When new observations do not match with the latent trajectory, the policy is recomputed by applying one-shot PPGS from the latest observation. This happens when the autoregressive prediction of the current embedding (conditioned on the action sequence since the last planning iteration) can not be reidentified with the embedding of the current observation. Moreover, the algorithm stores all observed latent transitions in a lookup table and, when replanning, it only trusts the forward model on previously unseen observation/action pairs. A detailed description can be found in Suppl. C.2.</p>
<p>Environments</p>
<p>In order to benchmark both perception and abstract reasoning, we empirically show the feasibility of our method on three challenging procedurally generated environments. These include the Maze environment from the procgen suite [13], as well as DigitJump and IceSlider, two combinatorially hard environments which stress the reasoning capabilities of a learning agent, or even of an human player. In the context of our work, the term "combinatorial hardness" is used loosely. We refer to an environment as "combinatorially hard" if only very few of the exponentially many trajectories actually lead to the goal, while deviating from them often results in failure (e.g. DigitJump or IceSlider). Hence, some "intelligent" search algorithm is required. In this way, the process of retrieving a successful policy resembles that of a graph-traversing algorithm. The last two environments are made available in a public repository [1], where they can also be tested interactively. More details on their implementation are included in Suppl. D.</p>
<p>ProcGenMaze The ProcgenMaze environment consists of a family of procedurally generated 2D mazes. The agent starts in the bottom left corner of the grid and needs to reach a position marked by a piece of cheese. For each level, an unique shortest solution exists, and its length is usually distributed roughly between 1 and 40 steps. This environment presents significant intra-level variability, with different sizes, textures, and maze structures. While retrieving the optimal solution in this environment is already a non-trivial task, its dynamics are uniform and actions only cause local changes in the observations. Moreover, ProcgenMaze is a forgiving environment in which errors can always be recovered from. In the real world, many operations are irreversible, for instance, cutting/breaking objects, gluing parts, mixing liquids, etc. Environments containing remote controls, for example, show non-local effects. We use these insights to choose the additional environments. IceSlider IceSlider is in principle similar to ProcgenMaze, since it also consists of procedurally generated mazes. However, each action propels the agent in a direction until an obstacle (a rock or the borders of the environments) is met. We generate solvable but unforgiving levels that feature irreversible transitions, that, once taken, prevent the agent from ever reaching the goal.</p>
<p>DigitJump DigitJump features a distribution of randomly generated levels which consist of a 2D 8x8 grid of handwritten digits from 1 to 6. The agent needs to go from the top left corner to the bottom right corner. The 4 directional actions are available, but each of them causes the agent to move in that directions by the number of steps expressed by the digit on the starting cell. Therefore, a single action can easily transport the player across the board. This makes navigating the environment very challenging, despite the reduced cardinality of the state space. Moreover, the game presents many cells in which the agent can get irreversibly stuck.</p>
<p>Related Work</p>
<p>World Models and Reinforcement Learning The idea of learning to model an environment has been widely explored in recent years. Work by Oh et al. [32] and Chiappa et al. [11] has argued that modern machine learning architectures are capable of learning to model the dynamics of a generic environment reasonably well for non-trivial time horizons. The seminal work by Ha and Schmidhuber [17] built upon this by learning a world model in a low-dimensional latent space instead of conditioning predictions on observations. They achieved this by training a VAE on reconstructing observations and a recurrent network for sampling latent trajectories conditioned on an action sequence. Moreover, they showed how sample efficiency could be addressed by recovering a simple controller acting directly on latent representations through an evolutionary approach.</p>
<p>This initial idea was iteratively improved along two main directions. On one hand, some subsequent works focused on learning objectives and suggested to jointly train encoding and dynamics components. Hafner et al. [18] introduced a multi-step variational inference objective to encourage latent representations to be predictive of the future and propagate information through both deterministic and stochastic paths. On the other hand, authors proposed to learn to act in the latent space by using zero-order methods [18] such as CEM [36] or policy gradient techniques [19,20]. These improvements gradually led to strong model-based RL agents capable of achieving very competitive performance in continuous control tasks [19] and on the Atari Learning Environment [7,10,20].</p>
<p>Relying on image reconstruction can however lead to vulnerability to visual noise: to overcome this limitation Okada and Taniguchi [33] and Zhang et al. [43] forgo the decoder network, while the latter proposes to rely on the notion of bisimilarity to learn meaningful representations. Similarly, Gelada et al. [16] only learn to predict rewards and action-conditional state distributions, but only study this task as an additional loss to model-free reinforcement learning methods. Another relevant approach is that of [44], who propose to learn a discrete graph representation of the environment, but their final goal is that of recovering a series of subgoals for model-free RL.</p>
<p>A strong example of how world models can be coupled with classical planners is given by MuZero [38]. MuZero trains a recurrent world model to guide a Monte Carlo tree search by encouraging hidden states to be predictive of future states and a sparse reward signal. While we adopt a similar framework, we focus on recovering a discrete structure in the latent space in order to reidentify states and lower the complexity of the search procedure. Moreover, we do not rely on reward signals, but only focus on learning the dynamics of the environment.</p>
<p>Neuro-algorithmic Planning In recent years, several other authors have explored the intersection between representation learning and classical algorithms. This is the case, for instance, of Ichter and Pavone [23], Kumar et al. [26], Kuo et al. [27] who rely on sequence models or VAEs to propose trajectories for sampling-based planners. Within planning research, Yonetani et al. [42] introduce a differentiable version of the A* search algorithm that can learn suitable representations from images with supervision. The most relevant line of work to us is perhaps the one that attempts to learn representations that are suitable as an input for classical solvers. Within this area, Asai and Fukunaga [4], Asai and Muise [5] show how symbolic representations can be extracted from complex tasks in an end-to-end fashion and directly fed into off-the-shelf solvers. More recently, Vlastelica et al. [40] frames MDPs as shortest-path problems and trains a convolutional neural network to retrieve the weights of a fixed graph structure. The extracted graph representation can be solved with a combinatorial solver and trained end-to-end by leveraging the blackbox differentiation method [35].</p>
<p>Visual Goals A further direction of relevant research is that of planning to achieve multiple goals [30]. While the most common approaches involve learning a goal-conditioned policy with experience relabeling [3], the recently proposed GLAMOR [34] relies on learning inverse dynamics and retrieves policies through a recurrent network. By doing so, it can achieve visual goals without explicitly modeling a reward function, an approach that is sensibly closer to ours and can serve as a relevant comparison. Another method that sharing a similar setting to ours is LEAP [31], which also attempts to fuse reinforcement learning and planning; however, its approach is fundamentally different and designed for dense rewards and continuous control. Similarly, SPTM [37] pursues a similar direction, but requires exploratory traversals in the current environment, which would be particularly hard to obtain due to procedural generation.</p>
<p>Experiments</p>
<p>The purpose of the experimental section is to empirically verify the following claims: (i) PPGS is able to solve challenging environments with an underlying combinatorial structure and (ii) PPGS is able to generalize to unseen variations of the environments, even when trained on few levels. We aim to demonstrate that forming complex plans in these simple-looking environments is beyond the reach of the best suited state-of-the-art methods. Our approach, on the other hand, achieves non-trivial performance. With this in mind, we did not insist on perfect fairness of all comparisons, as the different methods have different type of access to the data and the environment. However, the largest disadvantage is arguably given to our own method.</p>
<p>While visual goals could be drawn from a distribution p(g), we evaluate a single goal for each test level matching the environment solution (or the only state that would give a positive reward in a sparse reinforcement working framework). This represents a very challenging task with respect to common visual goal achievement benchmarks [34], while also allowing comparisons with rewardbased approaches such as PPO [39]. We mainly evaluate the success rate, which is computed as the proportion of solved levels in a set of 100 unseen levels. A level is considered to be solved when the agent achieves the visual goal (or receives a non-zero reward) within 256 steps.</p>
<p>Choice of Baselines Our method learns to achieve visual goals by planning with a world model learned on a distribution of levels. To the best of our knowledge, no other method in the literature shares these exact settings. For this reason, we select three diverse and strong baselines and we make our best efforts for a fair comparison within our computational limits.</p>
<p>PPO [39] is a strong and scalable policy optimization method that has been applied in procedurally generated environments [13]. While PPGS requires a visual goal to be given, PPO relies on a (sparse) reward signal specializing on a unique goal per level. DreamerV2 [20] is a model-based RL approach that also relies on a reward signal, while GLAMOR [34] is more aligned with PPGS as it is also designed to reach visual goals in absence of a reward.</p>
<p>While we restrict PPGS to only access an offline dataset of low-quality random trajectories, all baselines are allowed to collect data on policy for a much larger number of environment steps. More considerations on these baselines and on the fairness of our comparison can be found in Suppl. B. Furthermore, we also consider a non-learning naive search algorithm (GS ON IMAGES) thoroughly described in C.3.</p>
<p>A comprehensive ablation study of PPGS can be found in Section A of the Appendix.</p>
<p>Comparison of Success Rates</p>
<p>ProcgenMaze Our first claim is supported by Figure 6. PPGS outperform its baselines across the three environments. The gap with baselines is smaller in ProcgenMaze, a forgiving environment for which accurate plans are not necessary. On the other hand, ProcgenMaze involves long-horizon planning, which can be seen as a limitation to one-shot PPGS. As the combinatorial nature of the environment becomes more important, the gap with all baselines increases drastically.</p>
<p>PPO performs fairly well with simple dynamics and long-term planning, but struggles more when combinatorial reasoning is necessary. GLAMOR and DreamerV2 struggle across the three environments, as they likely fail to generalize across a distribution of levels. The fact that GS ON IMAGES manages to rival other baselines is a testament to the harshness of the environments.</p>
<p>Analysis of Generalization</p>
<p>The inductive biases represented by the planning algorithm and our training procedure ensure good generalization from a minimal number of training levels. In Fig. 7, we compare solution rates between PPGS and PPO as the number of levels available for training increases. The same metric for larger training level sets is additionally available in Table 3. Our method generally outperforms its baselines across all environments. In ProcgenMaze, PPGS achieves better success rates than PPO after only  seeing two orders of magnitude less level, e.g. 10 levels instead of 1000. Note that PPGS uses only 400k samples from a random policy whereas PPO uses 50M on-policy samples. Due to the harshness of the remaining environments, PPO struggles to find a good policy and its solution rate on unseen levels improves slowly as the number of training levels increases. In IceSlider, PPGS is well above PPO for any size of the training set and a outperforms GS ON IMAGES when only having access to 2 training levels. While having a comparable performance to PPO on small training sets in DigitJump, our method severely outperforms it once approximately 200 levels are available. On the other hand, PPO's ability to generalize plateaus. These results show that PPGS quickly learns to extract meaningful representations that generalize well to unseen scenarios.</p>
<p>Discussion</p>
<p>Limitations The main limitations of our method regard the assumptions that characterize the class of environments we focus on, namely a slowly expanding state space and discrete actions. In general, due to the complexity of the search algorithms, scaling to very large action sets becomes challenging. Moreover, a single expansion of the search tree requires a forward pass of the dynamics network, which takes a non-negligible amount of time. Finally, the world model is a fundamental component and the accuracy of the forward model is vital to the planner. Training an accurate forward model can be hard when dealing with exceedingly complex observations: very large grid sizes in the environments are a significant obstacle. On the other hand, improvements in the world model would directly benefit the whole pipeline.</p>
<p>Conclusion Hard search from pixels is largely unexplored and unsolved, yet fundamental for future AI. In this paper we presented how powerful graph planners can be combined with learned perception modules to solve challenging environment with a hidden combinatorial nature. In particular, our training procedure and planning algorithm achieve this by (i) leveraging state reidentification to reduce planning complexity and (ii) overcoming the limitation posed by information-dense observations through an hybrid forward model. We validated our proposed method, PPGS, across three challenging environments in which we found state-of-the-art methods to struggle. We believe that our results represent a sensible argument in support of the integration of learning-based approaches and classical solvers.</p>
<p>Supplementary Material for: Planning from Pixels in Environments with Combinatorially Hard Search Spaces</p>
<p>A Ablation Study PPGS relies on crucial architectural choices that we now set out to motivate. We do so by performing an ablation study and questioning each of the choices individually to show its contribution to the final performance.</p>
<p>World Model We evaluate the impact of different choices on the world model by retraining it from scratch and reporting the success rate of the full planner in Table 1. We also compute two latent metrics, which are commonly used to benchmark latent predictions [25], see below.  Table 1: Ablations. We evaluate the success rate on two environments when removing important components of our world model and planner. For the world model modifications, we also report metrics for predictive accuracy explained in the text. All results are averaged over 3 seeds.</p>
<p>ProcgenMaze / DigitJump</p>
<p>In particular, given a planning horizon K, we first collect a random trajectory (s t , a t ) t=1...L of length L = 20 and extract latent embeddings {z t } t=1,...L through the encoder h θ . We then autoregressively estimate the embedding z K+1 using only the first embedding z 1 and the action sequence (a t ) t=1...K , obtaining a predictionẑ K+1 .</p>
<p>We repeat this process for N trajectories, obtaining N sequences of latent embeddings (z n t ) n=1...N t=1...L and N predictions {ẑ n K+1 } n=1...N . We compute rank(ẑ n K+1 ) as the lowest k such thatẑ n K+1 is in the k-nearest neighbors of z n K+1 considering all other embeddings {z n t } t=1,...L . We can finally compute H@K = 1 N N n=1 1 rank(ẑ n K+1 )=1 and MMR@K = 1 N N n=1 1 rank(ẑ n K+1 ) . We found that training an inverse model is crucial for learning good representations. Even if the uninformative loss L margin introduced in Equation 4 already helps with avoiding the collapse in the latent space, we were not successful in training the forward model to high predictive accuracy unless the inverse model was jointly trained, despite further hyperparameter tuning. We can hypothesize that the inverse model enforces a regular structure in the latent space, which is in turn helpful for training the rest of the world model.</p>
<p>On the other hand, we find that the contribution of an hybrid forward model is more environmentdependent. We ablate this by introducing a forward model that only takes the state embedding z t and an action a t as input to predict the next embedding z t+1 , without having access to a contextual observation s c . In this case, the forward model can be implemented as a layer-normalized MLP with 3 layers of 256 neurons. When adopting this fully latent forward model, predictive accuracy drops sensibly. While success rate also drops sharply in DigitJump, this is not the case for ProcgenMaze. We believe that this can be motivated by the fact that several levels of ProcgenMaze could be solved in few steps, only knowing the local structure of the maze with respect to the starting position of the agent. In DigitJump, on the other hand, the agent can easily move across the environment and needs global information to plan accurately.</p>
<p>Planner Evaluating the planning algorithm does not require retraining the world model. We first show the importance of the lookup table. Without correcting the world model's inaccuracies, the planner is not able to recover from incorrect trajectories. As a result, the success rate is comparable to that of one-shot planning. Finally, we empirically validate the importance of state reidentification: when disabled, the BFS procedure is forced to randomly discard new vertices due to the exponential increase in the size of the graph. Because of this, promising trajectories cannot be expanded and the planner is only effective on simple levels which only require short-term planning.</p>
<p>Failure Cases</p>
<p>We now present a visual rendition of failure cases for one-shot PPGS in Fig. 8, together with the correct policy retrieved by the full planner. Figure 8: Failure cases. On the first row, a level from each environment that one-shot PPGS fails to solve (the white arrows represent the policy). On the second row, the policies corrected by a full planner, which is able to solve all levels. A white circle is drawn when PPGS recomputes its policy.</p>
<p>Iterative Model Improvement In general settings, collecting training trajectories by sampling actions uniformly at random does not grant sufficient coverage of the state space. In this case, the planners designed for PPGS can be deployed for data collection while training the world model. To show the potential of this approach in our current setting, we train our method starting from a small number of random trajectories and iteratively collecting on-policy data. We compare the performance of one-shot planning on IceSlider when trained on 1k levels. We measure success rates on unseen levels for (A) the default setting (400k random training samples), (B) in a low-data scenario (100k random training samples), and (C) when iteratively adding on-policy transitions to a small initial training set of 100k random training samples. In this last case, we collect 100k additional on-policy samples every 5 epochs. At 20 epochs we observe that on-policy data collection is able to accelerate and stabilize learning (see Table 2). When training is completed (40 epochs), (A) and (C) reach the same performance, while (B) does not improve. We believe that this hints that • random trajectories are enough to eventually cover the state space of IceSlider well and,</p>
<p>• the planner can be effectively used for data collection and iterative improvements.</p>
<p>We therefore believe that collecting data on-policy in an RL-like loop is crucial in environments requiring more exploration and represents an interesting direction.  </p>
<p>B Choice of Baselines and Fairness</p>
<p>After introducing the fundamental reasons behind our choice of baselines in Sec. 5, we present our reasoning and experimental setup with respect to each of the methods.</p>
<p>PPO PPO [39] is a strong model-free RL algorithm. Unlike PPGS, PPO requires a reward signal instead of visual goals. We grant PPO an advantageous setting by allowing on-policy data collection for 50M environment steps, which is in stark contrast to the offline dataset of 400k random transitions that PPGS is trained on. We use the implementation and hyperparameters presented by Cobbe et al. [13] for the Procgen suite, due to its similarity to the rest of the environments. While PPGS is tuned on ProcgenMaze and keeps its hyperparameters fixed across environments, we favor PPO by tuning the number of timesteps per rollout according to the environment to account for the possibility of getting stuck in a funnel state.</p>
<p>GLAMOR GLAMOR [34] learns inverse dynamics to achieve visual goals in Atari games. Similarly to PPGS, GLAMOR does not require a reward signal but needs to receive a visual goal. The only difference with PPGS in terms of settings is that we allow GLAMOR to collect data on-policy and for more interactions (2M). At evaluation time we deploy a strictly more forgiving scheme for GLAMOR, which is described in the original paper [34]. As GLAMOR is designed to approximately reach its goals, we also accept trajectories that terminate near the actual goal as viable solutions. Hyperparameters for GLAMOR were tuned by the original authors in Atari, which is a visually comparable setting.</p>
<p>DreamerV2 DreamerV2 [20] is a model-based RL approach reaching state-of-the-art performance in discrete games and continuous control domains. We use the original implementation for Atari environments. Due to its large computational requirements, we are only able to run DreamerV2 for a reduced number of steps, totaling 4M. We remark that while this is not enough for performance on Atari to converge, it is shown by the original authors to be sufficient for solving a significant number of games.</p>
<p>All cited codebases that we use are publicly available under a MIT license.</p>
<p>C Further Implementation Details</p>
<p>In this section we report relevant implementation choices for PPGS. In any case, we refer to our code [2] for precise details.</p>
<p>C.1 World Model</p>
<p>Encoder. The encoding function h θ is learned by a convolutional neural network. The output of the convolutional backbone of a ResNet-18 [21] is fed through a single fully connected layer with d units, where d = 16 is the size of the latent space Z. The output of the network is normalized to unit L2 norm.</p>
<p>Forward Model. From an architectural perspective, our hybrid forward model transforms the state embedding z t through a deconvolutional network and concatenates it to the RGB observation s c and a batchwise one-hot tensor representing the action. The result is processed through a second ResNet-18 to predict the next embedding. We found it to be irrelevant whether to train the network to predict a state representation z t+1 or a latent offset z t+1 − z t : for our experiments we choose the former.</p>
<p>Similarly to Hafner et al. [19] we find that, in practice, explicitly encouraging representations to be predictive for longer horizons (for instance through a multi-step loss) does not appear to be helpful. For this reason, we only train for one-step predictions, as noted in Equation 2.</p>
<p>Inverse Model. To enforce a simpler structure in the latent space, we implement the inverse model p ω as a low-capacity one-layer MLP with ReLU activations, 32 neurons and layer normalization [6].</p>
<p>Hyperparameters Hyperparameters are fixed in all experiments unless explicitly mentioned. More importantly, we deploy the same set of hyperparameters across all three environments, after tuning them on ProcgenMaze via grid search. The latent margin ε is set to 0.1 and the dimensionality of the latent space d is set to 16. The world model we propose is optimized using Adam [24] with learning rate λ = 0.001 for all components and parameters ε = 0.00001, β 1 = 0.9, β 2 = 0.999. All components are trained for 40 epochs with a batch size of 128. The losses are combined as in Equation 5 with weights α = 10, β = 1, although our architecture shows robustness to this choice. Training the world model takes approximately 20 hours on a single NVIDIA ampere GPU.</p>
<p>C.2 Planner</p>
<p>In this subsection, we present both planners (one-shot and full) more in detail.</p>
<p>One-shot Planner Algorithm 2 offers a more thorough description of the one-shot planner introduced in Algorithm 1. Given a visual goal g and an initial observed state s, maximizing discounted rewards corresponds to recovering the shortest action sequence (a i ) 1,...,n such that s n = g.</p>
<p>For this purpose, Algorithm 2 efficiently builds and searches the latent graph. It has access to an initial high-dimensional state s and a visual goal g; it keeps track of a list of visited vertices V and of a set of leaf vertices L. For each visited latent embedding, the algorithm stores the action sequence leading to it in a dictionary D. A key part of the algorithm is represented by the filter function.</p>
<p>The filter function receives as input the new set of leaves L , from which vertices reidentifiable with visited states have already been discarded. The function removes elements of L until no pair of states is too close. This is done by building a secondary graph with the elements of L as vertices and edges between all pairs of vertices at less than 2 distance. A set of non-conflicting elements can then be recovered by approximately solving a minimum vertex cover problem. If the state space of the environment grows exponentially with the planning horizon, or if the world model fails to reidentify bisimilar states, L can still reach impractically large sizes. For this reason, after resolving conflicts, if its cardinality is larger than a cutoff C = 256, |L t+1 | − C elements are uniformly sampled and removed.</p>
<p>Full Planner The full planner used by PPGS introduces the possibility of online replanning in an MPC approach. It autoregressively computes a latent trajectory T conditioned on the action sequence P retrieved by one-shot planning. At each step, the current observation is projected to the latent space to check if it can be reidentified with the predicted embedding in T . When this is not possible, the action sequence P is recomputed. Moreover, the planner gradually fills a latent transition buffer B. Forward predictions are then computed according tof θ (z, a, s), which returns z if (z, z , a) ∈ B, otherwise it queries the learned forward model f θ . As a side note, when replanning while using the full planner, the planning horizon T max is set to 10 steps. We report the method in full in Algorithm 3.</p>
<p>C.3 GS ON IMAGES</p>
<p>Our baselines include a graph search algorithm in observation space that does not involve any learned component. We refer to this algorithm as GS ON IMAGES and it can be seen as a measure of how hard an environment is when relying on reconstructing the state diagram to solve it. GS ON IMAGES assumes solely on the deterministic nature of the environment. Given a starting state s, a goal state g and the action set A, GS ON IMAGES plans as shown in Algorithm 4. It relies on a dictionary A left which stores, for each visited state, the set of actions that have not been attempted yet, and on a graph representation of the environment G = (V, E), where V is the set of visited states and E contains the observed transitions between states and labeled by an action.</p>
<p>As a side note on this algorithm's performance, we remark that, unlike the remaining methods, it strongly depends on the absence of visual noise and distractors. In particular, this method is bound to Algorithm 2 One-shot PPGS Input: s, g Output: action sequence (a i ) 1,...,n 1: z, z g = h θ (s 1 ), h θ (g) project to latent space 2: V = L = for z ∈ L do grow the latent graph 7: for a ∈ A do 8:
z = f φ (z, a, s) 9:
if min v∈V z − v 2 &gt; ε 2 then skip if already visited 10: V = V ∪ L add leaves to visited set 21: end while fail in environments in which this assumption does not hold: visual noise would render reidentification meaningless for GS on Images and the baseline would not be able to avoid revisiting vertices of the latent graph.
L = L ∪</p>
<p>C.4 Data Collection</p>
<p>As our method solely relies on accurately modeling the dynamics of the environment, the only requirement for training data is sufficient coverage of the state space. In most cases, this is satisfied by collecting trajectories offline according to a uniformly random policy. The ability to leverage a fixed dataset of trajectories draws PPGS closer to off-policy methods or even batch reinforcement learning approaches.</p>
<p>In practice, unless specified otherwise, we collect 20 trajectories of 20 steps from a set of n = 1000 training levels, for a total of 400k environment steps. One exception is made for ProcgenMaze, for which we also set a random starting position at each episode, since uncorrelated exploration is not sufficient to cover significant parts of the state.</p>
<p>D Environments</p>
<p>In this section, we present a few remarks on the environments chosen. For ProcgenMaze, we choose what is reported as the easy distribution in Cobbe et al. [13]. This corresponds to grids of size n × n, with 3 ≤ n ≤ 15; each cell of the grid is either a piece of wall or a corridor. In IceSlider, the agent always starts in the top row and needs to descend to a goal on the bottom row. It is not sufficient to slide over the goal, but the agent needs to come to a full stop on the correct square. In DigitJump, the handwritten digits are the same across training and test levels. Their frequency and position does of course change.</p>
<p>All environments return observations as 64x64 RGB images. ProcgenMaze and IceSlider are rendered in a similar style to ATARI games, while the DigitJump is a grid of MNIST [28] digits that highlights the cell at which the agent is positioned. The action space in all cases is restricted to four cardinal actions (UP, DOWN, LEFT, RIGHT) and a no-op action, for a total of 5 actions.  Fig. 9. For more information on the environments, we refer the reader to our code [1].</p>
<p>E Numerical Results</p>
<p>We finally include the full numerical results from Fig. 6 and Fig. 7 in Table 3.</p>
<p>ProcgenMaze -Success % on unseen levels when training on n levels  Table 3: Generalization results. This table presents the numerical results used to produce Fig. 6 and Fig. 7. All metrics are averaged over 3 random seeds. </p>
<p>Figure 1 :
1Planning from Pixels with Graph Search. Our method leverages learned latent dynamics to efficiently build and search a graph representation of the environment. Resulting policies show unrivaled performance across a distribution of hard combinatorial tasks.</p>
<p>Figure 4 :
4Number of leaf vertices when planning in ProcgenMaze, averaged over 100 levels, with 90% confidence intervals.</p>
<p>L
= L \ V reidentify and discard visited vertices (details in Suppl. C.</p>
<p>Figure 5 :
5Environments. Initial observations and one-shot PPGS's solution (arrows) of a random level of each of the three environments. ProcgenMaze is from[13]. DigitJump and IceSlider are proposed by us and can be accessed at [1].</p>
<p>Figure 6 :
6Success rates across the three environments. One-shot planning is competitive with the full method on shorter time horizons.</p>
<p>Figure 7 :
7Solution rates of PPGS and PPO as a function of the cardinality of the set of training levels.</p>
<p>{z} 3 :
3D = {z : []} 4: while for T MAX steps do</p>
<p>expert trajectories are shown in</p>
<p>Table 2 :
2Performance when training with datasets of various sizes and with on-policy data collection. Collecting trajectories generated by the planner can accelerate learning.</p>
<p>{z } L = filter(L )select the largest group of elements such that no pair is too close16:    z = arg min z∈L z − z g 211: </p>
<p>D[z ] = D[z] + [a] </p>
<p>12: </p>
<p>end if </p>
<p>13: </p>
<p>end for </p>
<p>14: </p>
<p>end for </p>
<p>15: </p>
<p>17: </p>
<p>if z − z g 2 ≤ ε 
2 then 
if z can be reidentified with the goal </p>
<p>18: </p>
<p>return D[z ] </p>
<p>19: </p>
<p>end if </p>
<p>20: </p>
<p>Algorithm 4 GS ON IMAGESInput: s, g 1: A left = {s : A} 2: V = {s} 3: E = ∅ 4: while s = g do if A left [s] = ∅ then if ∃s ∈ V such that A left [s ] = ∅and s is reachable from s then 7: find and apply action sequence to reach closest s ∈ V A left [s] = A left [s] \ {a} 15: take action a and reach state s V = V ∪ {s } 18: A left [s] = A E = E ∪ (s, s , a)5: </p>
<p>6: </p>
<p>8: </p>
<p>s = s </p>
<p>9: </p>
<p>else </p>
<p>10: </p>
<p>return </p>
<p>11: </p>
<p>end if </p>
<p>12: </p>
<p>else </p>
<p>13: </p>
<p>a ∼ U(A left [s]) 
uniformly sample among remaining actions </p>
<p>14: </p>
<p>16: </p>
<p>if s / 
∈ V then </p>
<p>17: </p>
<p>19: </p>
<p>end if </p>
<p>20: </p>
<p>21: </p>
<p>end if 
22: end while </p>
<p>Acknowledgments and Disclosure of Funding
Hindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O Pieter Abbeel, W Zaremba, Advances in Neural Information Processing Systems. M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Classical planning in deep latent space: Bridging the subsymbolicsymbolic boundary. M Asai, A Fukunaga, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). S. A. McIlraith and K. Q. Weinbergerthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAM. Asai and A. Fukunaga. Classical planning in deep latent space: Bridging the subsymbolic- symbolic boundary. In S. A. McIlraith and K. Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 6094-6101, 2018.</p>
<p>Learning neural-symbolic descriptive planning models via cube-space priors: The voyage home (to STRIPS). M Asai, C Muise, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. C. Bessierethe Twenty-Ninth International Joint Conference on Artificial Intelligence2020M. Asai and C. Muise. Learning neural-symbolic descriptive planning models via cube-space priors: The voyage home (to STRIPS). In C. Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 2676-2682, 2020.</p>
<p>J L Ba, J R Kiros, G E Hinton, Layer normalization. J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization, 2016.</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 47M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</p>
<p>A survey on metric learning for feature vectors and structured data. A Bellet, A Habrard, M Sebban, arXiv:1306.6709arXiv preprintA. Bellet, A. Habrard, and M. Sebban. A survey on metric learning for feature vectors and structured data. arXiv preprint arXiv:1306.6709, 2013.</p>
<p>Signature verification using a" siamese" time delay neural network. J Bromley, I Guyon, Y Lecun, E Säckinger, R Shah, Advances in neural information processing systems. 6J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature verification using a" siamese" time delay neural network. Advances in neural information processing systems, 6: 737-744, 1993.</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International conference on machine learning. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, 2020.</p>
<p>Recurrent environment simulators. S Chiappa, S Racanière, D Wierstra, S Mohamed, S. Chiappa, S. Racanière, D. Wierstra, and S. Mohamed. Recurrent environment simulators, 2017.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, Advances in Neural Information Processing. K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing (NeurIPS), 2018.</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, International conference on machine learning. K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 2048-2056, 2020.</p>
<p>Provably efficient rl with rich observations via latent state decoding. S Du, A Krishnamurthy, N Jiang, A Agarwal, M Dudik, J Langford, PMLRInternational Conference on Machine Learning. S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient rl with rich observations via latent state decoding. In International Conference on Machine Learning. PMLR, 2019.</p>
<p>Model predictive control: Theory and practice-a survey. C E Garcia, D M Prett, M Morari, Automatica. 253C. E. Garcia, D. M. Prett, and M. Morari. Model predictive control: Theory and practice-a survey. Automatica, 25(3):335-348, 1989.</p>
<p>DeepMDP: Learning continuous latent space models for representation learning. C Gelada, S Kumar, J Buckman, O Nachum, M G Bellemare, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. DeepMDP: Learning con- tinuous latent space models for representation learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 2170-2179, 2019.</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in Neural Information Processing Systems. D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 2555-2565, 2019.</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, International Conference on Learning Representations. D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, International Conference on Learning Representations. D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, abs/1512.03385CoRRK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.</p>
<p>Deep metric learning using triplet network. E Hoffer, N Ailon, International workshop on similarity-based pattern recognition. E. Hoffer and N. Ailon. Deep metric learning using triplet network. In International workshop on similarity-based pattern recognition, pages 84-92, 2015.</p>
<p>Robot motion planning in learned latent spaces. B Ichter, M Pavone, IEEE Robotics and Automation Letters. 43B. Ichter and M. Pavone. Robot motion planning in learned latent spaces. IEEE Robotics and Automation Letters, 4(3):2407-2414, 2019.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Contrastive learning of structured world models. T Kipf, E Van Der Pol, M Welling, International Conference on Learning Representations. T. Kipf, E. van der Pol, and M. Welling. Contrastive learning of structured world models. In International Conference on Learning Representations, 2020.</p>
<p>Lego: Leveraging experience in roadmap generation for sampling-based planning. R Kumar, A Mandalika, S Choudhury, S Srinivasa, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). R. Kumar, A. Mandalika, S. Choudhury, and S. Srinivasa. Lego: Leveraging experience in roadmap generation for sampling-based planning. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1488-1495, 2019.</p>
<p>Deep sequential models for sampling-based planning. Y.-L Kuo, A Barbu, B Katz, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEY.-L. Kuo, A. Barbu, and B. Katz. Deep sequential models for sampling-based planning. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6490-6497. IEEE, 2018.</p>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.</p>
<p>Sphereface: Deep hypersphere embedding for face recognition. W Liu, Y Wen, Z Yu, M Li, B Raj, L Song, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep hypersphere embedding for face recognition. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6738-6746, 2017.</p>
<p>Visual reinforcement learning with imagined goals. A V Nair, V Pong, M Dalal, S Bahl, S Lin, S Levine, Advances in Neural Information Processing Systems. 31A. V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine. Visual reinforcement learning with imagined goals. In Advances in Neural Information Processing Systems, volume 31, 2018.</p>
<p>Planning with goal-conditioned policies. S Nasiriany, V Pong, S Lin, S Levine, Advances in Neural Information Processing Systems. 32S. Nasiriany, V. Pong, S. Lin, and S. Levine. Planning with goal-conditioned policies. In Advances in Neural Information Processing Systems, volume 32, 2019.</p>
<p>Action-conditional video prediction using deep networks in atari games. J Oh, X Guo, H Lee, R L Lewis, S Singh, Advances in Neural Information Processing Systems. J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, 2015.</p>
<p>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. M Okada, T Taniguchi, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEEM. Okada and T. Taniguchi. Dreaming: Model-based reinforcement learning by latent imagina- tion without reconstruction. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 4209-4215. IEEE, 2021.</p>
<p>Planning from pixels using inverse dynamics models. K Paster, S A Mcilraith, J Ba, International Conference on Learning Representations. K. Paster, S. A. McIlraith, and J. Ba. Planning from pixels using inverse dynamics models. In International Conference on Learning Representations, 2021.</p>
<p>Differentiation of blackbox combinatorial solvers. M V Pogančić, A Paulus, V Musil, G Martius, M Rolinek, International Conference on Learning Representations. M. V. Pogančić, A. Paulus, V. Musil, G. Martius, and M. Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2020.</p>
<p>Optimization of computer simulation models with rare events. R Y Rubinstein, European Journal of Operational Research. 991R. Y. Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89-112, 1997.</p>
<p>Semi-parametric topological memory for navigation. N Savinov, A Dosovitskiy, V Koltun, International Conference on Learning Representations. N. Savinov, A. Dosovitskiy, and V. Koltun. Semi-parametric topological memory for navigation. In International Conference on Learning Representations, 2018.</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, Nature. 5887839J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock- hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, abs/1707.06347CoRRJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Neuro-algorithmic policies enable fast combinatorial generalization. M Vlastelica, M Rolínek, G Martius, M. Vlastelica, M. Rolínek, and G. Martius. Neuro-algorithmic policies enable fast combinatorial generalization, 2021.</p>
<p>Wikipedia contributors. Kissing number -Wikipedia, the free encyclopedia. Online; accessed 9Wikipedia contributors. Kissing number -Wikipedia, the free encyclopedia, 2021. URL https://en.wikipedia.org/w/index.php?title=Kissing_number&amp; oldid=1020394961. [Online; accessed 9-May-2021].</p>
<p>Path planning using neural a<em> search. R Yonetani, T Taniai, M Barekatain, M Nishimura, A Kanezaki, R. Yonetani, T. Taniai, M. Barekatain, M. Nishimura, and A. Kanezaki. Path planning using neural a</em> search, 2020.</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. A Zhang, R T Mcallister, R Calandra, Y Gal, S Levine, International Conference on Learning Representations. A. Zhang, R. T. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant repre- sentations for reinforcement learning without reconstruction. In International Conference on Learning Representations, 2021.</p>
<p>World model as a graph: Learning latent landmarks for planning. L Zhang, G Yang, B C Stadie, L. Zhang, G. Yang, and B. C. Stadie. World model as a graph: Learning latent landmarks for planning, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
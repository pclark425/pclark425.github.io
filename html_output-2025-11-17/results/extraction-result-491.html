<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-491 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-491</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-491</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-259342865</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.01898v2.pdf" target="_blank">Generative Artificial Intelligence Reproducibility and Consensus</a></p>
                <p><strong>Paper Abstract:</strong> We performed a billion locality sensitive hash comparisons between artificially generated data samples to answer the critical question - can we reproduce the results of generative AI models? Reproducibility is one of the pillars of scientific research for verifiability, benchmarking, trust, and transparency. Futhermore, we take this research to the next level by verifying the"correctness"of generative AI output in a non-deterministic, trustless, decentralized network. We generate millions of data samples from a variety of open source diffusion and large language models and describe the procedures and trade-offs between generating more verses less deterministic output. Additionally, we analyze the outputs to provide empirical evidence of different parameterizations of tolerance and error bounds for verification. For our results, we show that with a majority vote between three independent verifiers, we can detect image generated perceptual collisions in generated AI with over 99.89% probability and less than 0.0267% chance of intra-class collision. For large language models (LLMs), we are able to gain 100% consensus using greedy methods or n-way beam searches to generate consensus demonstrated on different LLMs. In the context of generative AI training, we pinpoint and minimize the major sources of stochasticity and present gossip and synchronization training techniques for verifiability. Thus, this work provides a practical, solid foundation for AI verification, reproducibility, and consensus for generative AI applications.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e491.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e491.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HardwareNonDet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hardware-induced floating-point non-determinism (NVIDIA Tensor Cores / GPU rounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-deterministic numerical differences produced by GPU hardware (e.g., probabilistic rounding in NVIDIA Tensor Cores) that cause bit-level differences across otherwise identical runs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>decentralized GPU inference/training network</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Heterogeneous network of consumer and datacenter GPUs (3080ti, 3070ti, 3060ti, 3090, A40) executing generative model inference and fine-tuning tasks in a decentralized setting.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / assumed deterministic experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>GPU-accelerated PyTorch implementation with CUDA/cuDNN kernels</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hardware-induced non-determinism (numeric rounding / execution order)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions or experimental protocols often assume that running the same seed and prompt yields identical outputs; in practice GPU hardware (e.g., Tensor Cores) can perform probabilistic rounding or non-deterministic floating-point reductions, producing slight numeric differences that can accumulate and flip outputs at the bit level.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>low-level numerical execution (hardware floating-point operations) and resulting model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical reproducibility tests across different GPUs and identical seeds/prompts showing bit-level hash mismatches; citation of hardware behavior (probabilistic rounding) as a source of non-determinism</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparison of cryptographic/perceptual hashes across identical seed runs on multiple GPUs; observation of differing hashes and hamming distances in perceptual hashes; qualitative measurement of bit-level differences implied by cryptographic hash avalanche effect</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Causes bit-level differences that make exact cryptographic verification impossible across nodes; motivates use of perceptual tolerance and multi-verifier consensus. Paper reports visual/perceptual hash hamming distances up to 5 in some examples, indicating observable output differences though perceptual similarity may remain.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as a common source of nondeterminism in parallel floating-point computation (no overall frequency statistic given), and observed across mixed GPU fleet used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Floating-point non-associativity and hardware-specific implementations (e.g., probabilistic rounding in Tensor Cores) leading to execution-dependent numeric differences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Avoid relying on bit-level cryptographic hashes; use perceptual/locality-sensitive hashing with tolerance; employ consensus among multiple independent verifiers rather than exact-match hashing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative: perceptual hashing combined with multi-verifier voting yields high verification probability despite hardware nondeterminism (see consensus probabilities reported elsewhere in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / distributed inference</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Artificial Intelligence Reproducibility and Consensus', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e491.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e491.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FrameworkNonDet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Framework-level non-determinism in deep learning libraries (PyTorch / cuDNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-deterministic behaviors exposed by ML frameworks (PyTorch, cuDNN) such as non-deterministic kernels, algorithm selection, and multithreading that cause runs with identical seeds to diverge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>experiment scripts / model inference and training using PyTorch + CUDA/cuDNN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PyTorch 2.0 on CUDA 12 executing diffusion and LLM inference and textual-inversion fine-tuning across heterogeneous GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / implementation details described in paper methods</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts and library code (PyTorch, cuDNN-backed ops)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>framework-induced non-determinism (algorithm selection, multithreading, nondeterministic kernels)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers and methods sometimes imply determinism when specifying seeds; however, PyTorch and underlying libraries (cuDNN) can select different algorithms (e.g., cuDNN benchmarking picks convolution algorithms) or use multithreaded kernels leading to race-ordered floating-point operations that differ between runs and machines, producing non-deterministic results despite identical seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model computation layer (convolutions, other cuDNN ops), multithreading and scheduler-level behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Repeated identical-seed experiments across multiple GPUs and machines; observation of differing outputs/hashes; citing known framework behaviors and prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Perceptual hash comparisons across image outputs (mode hash vs outliers), counting number of outlier hashes per class, average hamming distance of outliers, time per hash; for training, PCA projections of 768-d embeddings across repeated runs to quantify drift.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Introduces variability in model outputs and training checkpoints that can reduce reproducibility; in image experiments some classes produced perceptual-hash outliers (mode-based outliers counted across 7 generated images per class); in training, ablations allowing framework randomness caused embedding drift versus deterministic runs.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across experiments on a fleet of 22 GPUs and multiple machines; paper reports non-zero outliers across categories (exact percentage per hashing method reported in paper tables but not re-stated here).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implicit non-deterministic kernel implementations, benchmarking-driven algorithm choice, multithreading, and incomplete control of all nondeterministic settings in frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Set deterministic flags where available, disable non-deterministic benchmarking features (e.g., cuDNN benchmarking), minimize controllable randomness (seeds for shuffles, flips, noise), prefer deterministic decoding strategies for LLMs (greedy/beam), and use consensus / tolerance-based verification.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical: disabling stochastic decoding and minimizing controllable randomness led to near-deterministic training runs (deterministic textual-inversion runs mostly overlapped in PCA projection); for inference, greedy/beam decoding across machines was empirically deterministic in 4000 LLM generations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning software / experimental reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Artificial Intelligence Reproducibility and Consensus', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e491.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e491.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HashingMismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between cryptographic exact hashes and perceptual similarity measures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Exact cryptographic hashes are intolerant to tiny expected nondeterministic differences (avalanche effect), so the paper uses perceptual/locality-sensitive hashing to match similar generated artifacts rather than exact bitwise equality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>verification pipeline for generative model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Verification approach that computes perceptual hashes (aHash, pHash, dHash, cHash) on generated images and compares them across independent verifiers to determine whether outputs are 'the same' under tolerance.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithm specification / verification protocol described in paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>analysis scripts computing perceptual hashes and comparing Hamming distances across outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>mismatch between assumed equality semantics (exact hash match) and practical need for perceptual equivalence</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language expectation that validators can re-run tasks and match exact state via cryptographic hashes fails due to tiny numerical variations; the implementation uses perceptual hashing which intentionally increases collision probability for similar images, introducing a different matching semantics than exact hashes described in deterministic consensus protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>verification/evaluation step (output hashing and matching semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical demonstration that identical-seed runs do not produce identical cryptographic hashes (avalanche effect) and that perceptual hashes show small hamming distances; analysis of hash stability across GPU runs (Figure 1, Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Computed perceptual hashes on ~7,000 images per model variant, counted outliers vs majority hash per class, reported outlier counts with tolerances (Ot<=1, Ot<=2), average Hamming distance (aDist), and time per hash; also ran 1.3M-image experiment with ~0.85B comparisons to quantify intra-class collision probability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Enables practical verification despite low-level nondeterminism by accepting perceptual tolerance; using perceptual hashing plus multi-verifier consensus gives high verification probabilities. Paper reports intra-class perceptual hash collision probability < 0.0267% when allowing Hamming distance tolerance <= 2, and detection probabilities >99.843% with 3 verifiers (tolerance 2).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Perceptual hash outliers observed but rare; intra-class collision measured as <0.0267% in large-scale experiment (1.3M images).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incompatibility of exact-match cryptographic verification with small, expected nondeterministic differences in generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt locality-sensitive/perceptual hashes with a small Hamming-distance tolerance; use majority or super-majority consensus of independent verifiers; compute statistical detection probabilities using binomial model.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative: With tolerance of 2 and majority vote, verification probabilities reported (e.g., 99.843% for 3 verifiers); intra-class collision small (<0.0267%), indicating low false-accept risk for adversarial guessing under their tests.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>generative models verification / distributed consensus</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Artificial Intelligence Reproducibility and Consensus', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e491.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e491.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMDecodingDeterminism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding-strategy-dependent determinism in large language model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The choice of decoding algorithm determines whether generation is reproducible across machines: greedy and beam search decoding were empirically deterministic across different GPUs, while multinomial (sampling) decoding produced non-deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM inference across heterogeneous GPUs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Running open-source LLMs (Wizard Vicuna 13B, Vicuna 7B, RedPajama 7B/3B, GPT-J 6B) on different machines/GPUs using various decoding strategies to test cross-node determinism.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / decoding strategy specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>inference scripts invoking model decoding routines (greedy, beam search, multinomial) with fixed seeds</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>discrepancy between implied deterministic generation and actual stochasticity based on decoding choice</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers or documentation that imply determinism from fixed seeds can be misleading: decoding strategy matters â€” deterministic decoding (greedy, beam) produced consistent outputs across GPUs in the experiments, but sampling-based strategies produce expected non-deterministic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>decoding/decoding-configuration stage of inference</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Cross-machine empirical comparisons: 4,000 LLM descriptions generated across different hardware and decoding configurations; observed results.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Counted matching/mismatching outputs across runs and machines for each decoding strategy; table of 4,000 generations showed Yes (deterministic) for greedy/beam and No for multinomial.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>When deterministic decoding is used, cross-node reproducibility is effectively achieved (paper claims '100% consensus' for greedy/beam in their samples); multinomial sampling breaks reproducibility and thus cannot be used in a consensus-verification setting without extra measures.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Empirically, all greedy and beam search runs in their 4,000-sample experiment were deterministic across machines; multinomial was non-deterministic as expected.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Decoding algorithm semantics: sampling-based methods intentionally introduce randomness, while greedy/beam are deterministic given identical model state and computation path (modulo hardware/framework nondeterminism).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use deterministic decoding strategies (greedy or beam search) for verifiable LLM inference when cross-node reproducibility is required; avoid multinomial sampling for verification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical: greedy and beam search were observed deterministic in 4,000 generated descriptions across multiple models and machines; authors note a tiny non-zero chance at floating-point edge cases but treat it as negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language generation / reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Artificial Intelligence Reproducibility and Consensus', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e491.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e491.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextInvStochSources</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controllable sources of stochasticity in textual-inversion training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Identification of six controllable randomness sources in textual inversion fine-tuning (data flips, template choice, shuffling, latent gaussian noise, timestep choice, noise scheduler) that, if not controlled, produce non-reproducible training trajectories and embedding drift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>distributed textual-inversion fine-tuning workflow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Training textual inversion (new word embeddings) on frozen U-Net and VAE with small example image sets, using mixed precision, specific hyperparameters; performed multiple runs with controlled ablations of randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / training procedure description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training scripts and checkpointing code for textual inversion using PyTorch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing/incomplete specification of controllable randomness in training (implicit assumptions / omitted preprocessing randomness)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions of training that list hyperparameters and seeds may omit or understate multiple sources of randomness (horizontal flips, template sampling, data shuffling, gaussian latent noise, timestep sampling, noise scheduler behavior). If these are not fully controlled or specified, checkpoints and learned embeddings will diverge across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data augmentation and noise injection pipeline, checkpointing and sampling steps</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation study with six runs per concept: three fully controlled deterministic runs and three runs with individual randomness sources enabled (horizontal flip, dataset shuffle, no seed). PCA on final 768-dim embeddings across runs to visualize and quantify drift.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>PCA projection (principal components 1 and 2) of 768-dimensional embeddings from checkpoints; visual/quantitative assessment of overlap vs drift between runs; comparison between deterministic and ablation runs.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Allowing controllable randomness caused noticeable embedding drift in PCA projections; deterministic runs mostly overlapped while runs with introduced randomness diverged, implying loss of reproducibility in personalized embeddings and downstream generation consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed for textual-inversion runs across dozens of concepts sampled from the HuggingFace concepts library; deterministic behavior achieved when controlling all six sources, and drift seen when individual sources were enabled.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incomplete specification or control of commonly implicit randomness sources in training pipelines (data augmentation ordering, shuffle RNGs, noise RNGs, scheduler sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Minimize or fix all controllable randomness (set seeds, disable flips/shuffle, fix noise scheduler/timesteps) and employ gossip/synchronization training where checkpoints are periodically synchronized across nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical: deterministic runs largely overlap in PCA space; gossip/synchronized training (resync every 300 iterations) produced synchronized runs that were 'nearly identical' to the trainer by the end of 2000 iterations, tightening error bounds (qualitative effectiveness reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning training / transfer-fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Artificial Intelligence Reproducibility and Consensus', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Performance evaluation of cudnn convolution algorithms on nvidia volta gpus. <em>(Rating: 2)</em></li>
                <li>Recovering single precision accuracy from tensor cores while surpassing the fp32 theoretical peak performance. <em>(Rating: 2)</em></li>
                <li>Verification for machine learning, autonomy, and neural networks survey. <em>(Rating: 2)</em></li>
                <li>Scaling up trustless dnn inference with zero-knowledge proofs. <em>(Rating: 2)</em></li>
                <li>The challenge of verification and testing of machine learning. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-491",
    "paper_id": "paper-259342865",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "HardwareNonDet",
            "name_full": "Hardware-induced floating-point non-determinism (NVIDIA Tensor Cores / GPU rounding)",
            "brief_description": "Non-deterministic numerical differences produced by GPU hardware (e.g., probabilistic rounding in NVIDIA Tensor Cores) that cause bit-level differences across otherwise identical runs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "decentralized GPU inference/training network",
            "system_description": "Heterogeneous network of consumer and datacenter GPUs (3080ti, 3070ti, 3060ti, 3090, A40) executing generative model inference and fine-tuning tasks in a decentralized setting.",
            "nl_description_type": "research paper methods section / assumed deterministic experimental protocol",
            "code_implementation_type": "GPU-accelerated PyTorch implementation with CUDA/cuDNN kernels",
            "gap_type": "hardware-induced non-determinism (numeric rounding / execution order)",
            "gap_description": "Natural-language descriptions or experimental protocols often assume that running the same seed and prompt yields identical outputs; in practice GPU hardware (e.g., Tensor Cores) can perform probabilistic rounding or non-deterministic floating-point reductions, producing slight numeric differences that can accumulate and flip outputs at the bit level.",
            "gap_location": "low-level numerical execution (hardware floating-point operations) and resulting model outputs",
            "detection_method": "empirical reproducibility tests across different GPUs and identical seeds/prompts showing bit-level hash mismatches; citation of hardware behavior (probabilistic rounding) as a source of non-determinism",
            "measurement_method": "Comparison of cryptographic/perceptual hashes across identical seed runs on multiple GPUs; observation of differing hashes and hamming distances in perceptual hashes; qualitative measurement of bit-level differences implied by cryptographic hash avalanche effect",
            "impact_on_results": "Causes bit-level differences that make exact cryptographic verification impossible across nodes; motivates use of perceptual tolerance and multi-verifier consensus. Paper reports visual/perceptual hash hamming distances up to 5 in some examples, indicating observable output differences though perceptual similarity may remain.",
            "frequency_or_prevalence": "Described as a common source of nondeterminism in parallel floating-point computation (no overall frequency statistic given), and observed across mixed GPU fleet used in experiments.",
            "root_cause": "Floating-point non-associativity and hardware-specific implementations (e.g., probabilistic rounding in Tensor Cores) leading to execution-dependent numeric differences.",
            "mitigation_approach": "Avoid relying on bit-level cryptographic hashes; use perceptual/locality-sensitive hashing with tolerance; employ consensus among multiple independent verifiers rather than exact-match hashing.",
            "mitigation_effectiveness": "Qualitative: perceptual hashing combined with multi-verifier voting yields high verification probability despite hardware nondeterminism (see consensus probabilities reported elsewhere in paper).",
            "domain_or_field": "deep learning / distributed inference",
            "reproducibility_impact": true,
            "uuid": "e491.0",
            "source_info": {
                "paper_title": "Generative Artificial Intelligence Reproducibility and Consensus",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "FrameworkNonDet",
            "name_full": "Framework-level non-determinism in deep learning libraries (PyTorch / cuDNN)",
            "brief_description": "Non-deterministic behaviors exposed by ML frameworks (PyTorch, cuDNN) such as non-deterministic kernels, algorithm selection, and multithreading that cause runs with identical seeds to diverge.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "experiment scripts / model inference and training using PyTorch + CUDA/cuDNN",
            "system_description": "PyTorch 2.0 on CUDA 12 executing diffusion and LLM inference and textual-inversion fine-tuning across heterogeneous GPUs.",
            "nl_description_type": "experimental protocol / implementation details described in paper methods",
            "code_implementation_type": "experiment scripts and library code (PyTorch, cuDNN-backed ops)",
            "gap_type": "framework-induced non-determinism (algorithm selection, multithreading, nondeterministic kernels)",
            "gap_description": "Papers and methods sometimes imply determinism when specifying seeds; however, PyTorch and underlying libraries (cuDNN) can select different algorithms (e.g., cuDNN benchmarking picks convolution algorithms) or use multithreaded kernels leading to race-ordered floating-point operations that differ between runs and machines, producing non-deterministic results despite identical seeds.",
            "gap_location": "model computation layer (convolutions, other cuDNN ops), multithreading and scheduler-level behaviors",
            "detection_method": "Repeated identical-seed experiments across multiple GPUs and machines; observation of differing outputs/hashes; citing known framework behaviors and prior work.",
            "measurement_method": "Perceptual hash comparisons across image outputs (mode hash vs outliers), counting number of outlier hashes per class, average hamming distance of outliers, time per hash; for training, PCA projections of 768-d embeddings across repeated runs to quantify drift.",
            "impact_on_results": "Introduces variability in model outputs and training checkpoints that can reduce reproducibility; in image experiments some classes produced perceptual-hash outliers (mode-based outliers counted across 7 generated images per class); in training, ablations allowing framework randomness caused embedding drift versus deterministic runs.",
            "frequency_or_prevalence": "Observed across experiments on a fleet of 22 GPUs and multiple machines; paper reports non-zero outliers across categories (exact percentage per hashing method reported in paper tables but not re-stated here).",
            "root_cause": "Implicit non-deterministic kernel implementations, benchmarking-driven algorithm choice, multithreading, and incomplete control of all nondeterministic settings in frameworks.",
            "mitigation_approach": "Set deterministic flags where available, disable non-deterministic benchmarking features (e.g., cuDNN benchmarking), minimize controllable randomness (seeds for shuffles, flips, noise), prefer deterministic decoding strategies for LLMs (greedy/beam), and use consensus / tolerance-based verification.",
            "mitigation_effectiveness": "Empirical: disabling stochastic decoding and minimizing controllable randomness led to near-deterministic training runs (deterministic textual-inversion runs mostly overlapped in PCA projection); for inference, greedy/beam decoding across machines was empirically deterministic in 4000 LLM generations.",
            "domain_or_field": "machine learning software / experimental reproducibility",
            "reproducibility_impact": true,
            "uuid": "e491.1",
            "source_info": {
                "paper_title": "Generative Artificial Intelligence Reproducibility and Consensus",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "HashingMismatch",
            "name_full": "Mismatch between cryptographic exact hashes and perceptual similarity measures",
            "brief_description": "Exact cryptographic hashes are intolerant to tiny expected nondeterministic differences (avalanche effect), so the paper uses perceptual/locality-sensitive hashing to match similar generated artifacts rather than exact bitwise equality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "verification pipeline for generative model outputs",
            "system_description": "Verification approach that computes perceptual hashes (aHash, pHash, dHash, cHash) on generated images and compares them across independent verifiers to determine whether outputs are 'the same' under tolerance.",
            "nl_description_type": "algorithm specification / verification protocol described in paper",
            "code_implementation_type": "analysis scripts computing perceptual hashes and comparing Hamming distances across outputs",
            "gap_type": "mismatch between assumed equality semantics (exact hash match) and practical need for perceptual equivalence",
            "gap_description": "Natural-language expectation that validators can re-run tasks and match exact state via cryptographic hashes fails due to tiny numerical variations; the implementation uses perceptual hashing which intentionally increases collision probability for similar images, introducing a different matching semantics than exact hashes described in deterministic consensus protocols.",
            "gap_location": "verification/evaluation step (output hashing and matching semantics)",
            "detection_method": "Empirical demonstration that identical-seed runs do not produce identical cryptographic hashes (avalanche effect) and that perceptual hashes show small hamming distances; analysis of hash stability across GPU runs (Figure 1, Figure 2).",
            "measurement_method": "Computed perceptual hashes on ~7,000 images per model variant, counted outliers vs majority hash per class, reported outlier counts with tolerances (Ot&lt;=1, Ot&lt;=2), average Hamming distance (aDist), and time per hash; also ran 1.3M-image experiment with ~0.85B comparisons to quantify intra-class collision probability.",
            "impact_on_results": "Enables practical verification despite low-level nondeterminism by accepting perceptual tolerance; using perceptual hashing plus multi-verifier consensus gives high verification probabilities. Paper reports intra-class perceptual hash collision probability &lt; 0.0267% when allowing Hamming distance tolerance &lt;= 2, and detection probabilities &gt;99.843% with 3 verifiers (tolerance 2).",
            "frequency_or_prevalence": "Perceptual hash outliers observed but rare; intra-class collision measured as &lt;0.0267% in large-scale experiment (1.3M images).",
            "root_cause": "Incompatibility of exact-match cryptographic verification with small, expected nondeterministic differences in generated outputs.",
            "mitigation_approach": "Adopt locality-sensitive/perceptual hashes with a small Hamming-distance tolerance; use majority or super-majority consensus of independent verifiers; compute statistical detection probabilities using binomial model.",
            "mitigation_effectiveness": "Quantitative: With tolerance of 2 and majority vote, verification probabilities reported (e.g., 99.843% for 3 verifiers); intra-class collision small (&lt;0.0267%), indicating low false-accept risk for adversarial guessing under their tests.",
            "domain_or_field": "generative models verification / distributed consensus",
            "reproducibility_impact": true,
            "uuid": "e491.2",
            "source_info": {
                "paper_title": "Generative Artificial Intelligence Reproducibility and Consensus",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLMDecodingDeterminism",
            "name_full": "Decoding-strategy-dependent determinism in large language model outputs",
            "brief_description": "The choice of decoding algorithm determines whether generation is reproducible across machines: greedy and beam search decoding were empirically deterministic across different GPUs, while multinomial (sampling) decoding produced non-deterministic outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM inference across heterogeneous GPUs",
            "system_description": "Running open-source LLMs (Wizard Vicuna 13B, Vicuna 7B, RedPajama 7B/3B, GPT-J 6B) on different machines/GPUs using various decoding strategies to test cross-node determinism.",
            "nl_description_type": "experimental protocol / decoding strategy specification",
            "code_implementation_type": "inference scripts invoking model decoding routines (greedy, beam search, multinomial) with fixed seeds",
            "gap_type": "discrepancy between implied deterministic generation and actual stochasticity based on decoding choice",
            "gap_description": "Papers or documentation that imply determinism from fixed seeds can be misleading: decoding strategy matters â€” deterministic decoding (greedy, beam) produced consistent outputs across GPUs in the experiments, but sampling-based strategies produce expected non-deterministic variation.",
            "gap_location": "decoding/decoding-configuration stage of inference",
            "detection_method": "Cross-machine empirical comparisons: 4,000 LLM descriptions generated across different hardware and decoding configurations; observed results.",
            "measurement_method": "Counted matching/mismatching outputs across runs and machines for each decoding strategy; table of 4,000 generations showed Yes (deterministic) for greedy/beam and No for multinomial.",
            "impact_on_results": "When deterministic decoding is used, cross-node reproducibility is effectively achieved (paper claims '100% consensus' for greedy/beam in their samples); multinomial sampling breaks reproducibility and thus cannot be used in a consensus-verification setting without extra measures.",
            "frequency_or_prevalence": "Empirically, all greedy and beam search runs in their 4,000-sample experiment were deterministic across machines; multinomial was non-deterministic as expected.",
            "root_cause": "Decoding algorithm semantics: sampling-based methods intentionally introduce randomness, while greedy/beam are deterministic given identical model state and computation path (modulo hardware/framework nondeterminism).",
            "mitigation_approach": "Use deterministic decoding strategies (greedy or beam search) for verifiable LLM inference when cross-node reproducibility is required; avoid multinomial sampling for verification tasks.",
            "mitigation_effectiveness": "Empirical: greedy and beam search were observed deterministic in 4,000 generated descriptions across multiple models and machines; authors note a tiny non-zero chance at floating-point edge cases but treat it as negligible.",
            "domain_or_field": "natural language generation / reproducibility",
            "reproducibility_impact": true,
            "uuid": "e491.3",
            "source_info": {
                "paper_title": "Generative Artificial Intelligence Reproducibility and Consensus",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "TextInvStochSources",
            "name_full": "Controllable sources of stochasticity in textual-inversion training",
            "brief_description": "Identification of six controllable randomness sources in textual inversion fine-tuning (data flips, template choice, shuffling, latent gaussian noise, timestep choice, noise scheduler) that, if not controlled, produce non-reproducible training trajectories and embedding drift.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "distributed textual-inversion fine-tuning workflow",
            "system_description": "Training textual inversion (new word embeddings) on frozen U-Net and VAE with small example image sets, using mixed precision, specific hyperparameters; performed multiple runs with controlled ablations of randomness.",
            "nl_description_type": "experimental protocol / training procedure description",
            "code_implementation_type": "training scripts and checkpointing code for textual inversion using PyTorch",
            "gap_type": "missing/incomplete specification of controllable randomness in training (implicit assumptions / omitted preprocessing randomness)",
            "gap_description": "Natural-language descriptions of training that list hyperparameters and seeds may omit or understate multiple sources of randomness (horizontal flips, template sampling, data shuffling, gaussian latent noise, timestep sampling, noise scheduler behavior). If these are not fully controlled or specified, checkpoints and learned embeddings will diverge across runs.",
            "gap_location": "training data augmentation and noise injection pipeline, checkpointing and sampling steps",
            "detection_method": "Ablation study with six runs per concept: three fully controlled deterministic runs and three runs with individual randomness sources enabled (horizontal flip, dataset shuffle, no seed). PCA on final 768-dim embeddings across runs to visualize and quantify drift.",
            "measurement_method": "PCA projection (principal components 1 and 2) of 768-dimensional embeddings from checkpoints; visual/quantitative assessment of overlap vs drift between runs; comparison between deterministic and ablation runs.",
            "impact_on_results": "Allowing controllable randomness caused noticeable embedding drift in PCA projections; deterministic runs mostly overlapped while runs with introduced randomness diverged, implying loss of reproducibility in personalized embeddings and downstream generation consistency.",
            "frequency_or_prevalence": "Observed for textual-inversion runs across dozens of concepts sampled from the HuggingFace concepts library; deterministic behavior achieved when controlling all six sources, and drift seen when individual sources were enabled.",
            "root_cause": "Incomplete specification or control of commonly implicit randomness sources in training pipelines (data augmentation ordering, shuffle RNGs, noise RNGs, scheduler sampling).",
            "mitigation_approach": "Minimize or fix all controllable randomness (set seeds, disable flips/shuffle, fix noise scheduler/timesteps) and employ gossip/synchronization training where checkpoints are periodically synchronized across nodes.",
            "mitigation_effectiveness": "Empirical: deterministic runs largely overlap in PCA space; gossip/synchronized training (resync every 300 iterations) produced synchronized runs that were 'nearly identical' to the trainer by the end of 2000 iterations, tightening error bounds (qualitative effectiveness reported).",
            "domain_or_field": "deep learning training / transfer-fine-tuning",
            "reproducibility_impact": true,
            "uuid": "e491.4",
            "source_info": {
                "paper_title": "Generative Artificial Intelligence Reproducibility and Consensus",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Performance evaluation of cudnn convolution algorithms on nvidia volta gpus.",
            "rating": 2,
            "sanitized_title": "performance_evaluation_of_cudnn_convolution_algorithms_on_nvidia_volta_gpus"
        },
        {
            "paper_title": "Recovering single precision accuracy from tensor cores while surpassing the fp32 theoretical peak performance.",
            "rating": 2,
            "sanitized_title": "recovering_single_precision_accuracy_from_tensor_cores_while_surpassing_the_fp32_theoretical_peak_performance"
        },
        {
            "paper_title": "Verification for machine learning, autonomy, and neural networks survey.",
            "rating": 2,
            "sanitized_title": "verification_for_machine_learning_autonomy_and_neural_networks_survey"
        },
        {
            "paper_title": "Scaling up trustless dnn inference with zero-knowledge proofs.",
            "rating": 2,
            "sanitized_title": "scaling_up_trustless_dnn_inference_with_zeroknowledge_proofs"
        },
        {
            "paper_title": "The challenge of verification and testing of machine learning.",
            "rating": 2,
            "sanitized_title": "the_challenge_of_verification_and_testing_of_machine_learning"
        }
    ],
    "cost": 0.013952999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generative Artificial Intelligence Reproducibility and Consensus
5 Feb 2024</p>
<p>Edward Kim 
Department of Computer Science
Drexel University
PA</p>
<p>Isamu Isozaki 
Department of Computer Science
Drexel University
PA</p>
<p>Naomi Sirkin 
Department of Computer Science
Drexel University
PA</p>
<p>Michael Robson mrobson@smith.edu 
Department of Computer Science
Smith College
MassachusettsMA</p>
<p>Generative Artificial Intelligence Reproducibility and Consensus
5 Feb 2024BBEF5136ACB84CF9A5EE90A37A7B223DarXiv:2307.01898v2[cs.DC]
We performed a billion locality sensitive hash comparisons between artificially generated data samples to answer the critical question -can we reproduce the results of generative AI models?Reproducibility is one of the pillars of scientific research for verifiability, benchmarking, trust, and transparency.Futhermore, we take this research to the next level by verifying the "correctness" of generative AI output in a non-deterministic, trustless, decentralized network.We generate millions of data samples from a variety of open source diffusion and large language models and describe the procedures and trade-offs between generating more verses less deterministic output.Additionally, we analyze the outputs to provide empirical evidence of different parameterizations of tolerance and error bounds for verification.For our results, we show that with a majority vote between three independent verifiers, we can detect image generated perceptual collisions in generated AI with over 99.89% probability and less than 0.0267% chance of intra-class collision.For large language models (LLMs), we are able to gain 100% consensus using greedy methods or n-way beam searches to generate consensus demonstrated on different LLMs.In the context of generative AI training, we pinpoint and minimize the major sources of stochasticity and present gossip and synchronization training techniques for verifiability.Thus, this work provides a practical, solid foundation for AI verification, reproducibility, and consensus for generative AI applications.</p>
<p>Introduction</p>
<p>Generative Artificial Intelligence (GenAI) represents one of the most impactful and consumer pervasive advancements in artificial intelligence technology in recent years.This form of AI is designed to learn the distribution of the training data, and sample from the learned manifold to create content, e.g.text, images, music, or other complex signals.It a significant shift from traditional AI models that are primarily used to analyze and interpret data typically seen in discriminative supervised tasks such as classification or regression.This raises issues of scientific reproducibility due to the statistical nature of generative AI.Reproducibility is essential as it enables the independent verification of results, ensuring that findings from a machine learning model or algorithm are valid and reliable.This is also important for understanding of a model's behavior, its strengths, and weaknesses.Reproducibility allows for the independent and accurate benchmarking and comparison of ML models through the open sharing of datasets and code; however, given the current trajectory of large ML models, understanding the issues and challenges around reproducibility is increasingly important.</p>
<p>As a case in point, AI is growing exponentially in every aspect -in usage and adoption, as well as in cost to train, model parameterization, data, and compute.For example, ChatGPT reached 1 million users in just 5 days -a feat that took 3.5 years for Netflix, and 2 years for Twitter.OpenAI's GPT-3 required over over $12M USD in training costs, and the carbon footprint of training this model was equivalent to the output of 126 danish homes for an entire year [1].This does not even cover the cost of obtaining and labeling massive amounts of data.The cost, size, and compute necessary for GPT-4 is an order of magnitude larger, and intractable to train or deploy by anyone except for a handful of the largest industry players.However, advancements in model quantization, hardware, and cloud computing have made it possible to train and deploy increasingly complex models on consumer grade hardware.Thus, a significant effort is underway in the field to democratize artificial intelligence.</p>
<p>One of the main goals of the democratization of AI is to expand the benefits of AI beyond a small group of elite researchers and companies, and to ensure that everyone has access to the tools and resources they need to take advantage of AI.While open source software is a core component, there is also the need for "open source" hardware.This concept is not new and has been explored in decentralized cloud networks.For example, BOINC [2], which stands for Berkeley Open Infrastructure for Network Computing, is an open-source software platform for distributed computing.It allows volunteers to donate unused processing power from their personal computers to scientific research projects.The system is designed to manage and utilize the computing resources of thousands of volunteers across the globe, effectively creating a massive, distributed supercomputer.This allows researchers to conduct large-scale computations without the need for a dedicated, or centralized infrastructure.Some well-known projects include SETI@home [3], which searches for extraterrestrial intelligence, and Folding@home [4], which simulates protein folding for disease research.However, a decentralized cloud infrastructure must utilize trustless computing principles that do not normally apply when dealing with a known entity.In a decentralized ecosystem, you do not know who you are interacting with, and the dangers of malicious or adversarial actors increases by orders of magnitude.To address these issues, the contributions of this work are the following.We investigate and present techniques to scale up the computational capabilities of a decentralized network.We present algorithms that can execute ML tasks and monitor other nodes for fraudulent output and, in our results, provide empirical evidence of different parameterizations of tolerance and error bounds for verification.In the context of generative AI training, we pinpoint and minimize the major sources of stochasticity and present gossip and synchronization training techniques for verifiability.In essence, this work demonstrates that we can re-produce and verify generative AI work in both image and language generation with minimal overhead and extremely high precision in a decentralized, trustless machine learning network.</p>
<p>Background</p>
<p>Background in Generative AI</p>
<p>Algorithmically, deep learning currently dominates nearly all applications of artificial intelligence and machine learning, and has shown incredible success in the past several years.These improvements can be attributed to multiple factors, where two major contributors were access to large amounts of data, and large amounts of compute power.Today, training and running these models requires an enormous amount of compute, usually accelerated in cloud infrastructure using high-powered GPU hardware.</p>
<p>Since the introduction of Generative Adversarial Networks (GANs) [5], generative AI has made remarkable strides.Today, it's used in a wide range of applications, from creating realistic imagery and generating art to synthesizing high-quality speech and writing.Stable Diffusion [6], is a deep learning, textto-image model primarily used to generate detailed images conditioned on text descriptions.It uses a latent diffusion model, which involves training the model to remove successive applications of Gaussian noise on training images, functioning as a sequence of denoising autoencoders.The model consists of three parts: a variational autoencoder (VAE) [7], a U-Net [8], and text encoder [9].The VAE compresses an image from pixel space to a smaller dimensional latent space, capturing a more semantic meaning of the image.Gaussian noise is then iteratively applied to this compressed latent representation during forward diffusion.The U-Net block, composed of a ResNet [10] backbone, denoises the output from forward diffusion to obtain a latent representation.The VAE decoder then generates the final image by converting this representation back into pixel space.This process can be conditioned on a string of text, an image, or another modality.Large language models (LLMs) are another class of generative AI that have been trained on large textual datasets.These models have shifted the focus of natural language processing research away from training specialized supervised models for specific tasks.Despite being trained on simple tasks such as predicting the next word in a sentence, LLMs with sufficient training and parameter counts capture much of the syntax and semantics of human language and demonstrate considerable general knowledge within their training corpus.</p>
<p>Background in Decentralized Verification and Consensus</p>
<p>The field of machine learning verification is in its infancy.In the traditional definition, verification involves making a compelling argument that the system will not misbehave under a broad range of circumstances [11].This involves the need to consider unusual inputs crafted by an adversary, not just naturally occurring inputs as testing alone is insufficient to provide security guarantees [12].</p>
<p>However, we are dealing with a nuanced type of verification of correctness.In the context of a decentralized network, a consensus model is a mechanism that ensures all participants in a distributed network agree on the content of a shared database.It is the protocol by which the nodes in the network agree on a single version of the truth, despite the presence of faulty nodes or those with malicious intent.The consensus model is the core idea to minimize the need for trust in a blockchain system, where consensus ensures that every transaction is validated according to a set of agreed-upon rules.Unlike traditional centralized systems where a single authority validates transactions, in a blockchain, multiple nodes participate in the validation process [13].This decentralization enhances security and transparency but requires the following properties to be effective [14].(1) Agreement -all honest nodes must agree on the same value.(2) Validity -if all honest nodes propose the same value, they must decide on that value.(3) Termination -every honest node must eventually reach a decision.(4) Integrity -a node decides on a value at most once.In other words, once a node has made a decision, it cannot change it.And (5) Fault Tolerance -the consensus model should be able to function correctly even if some nodes fail or act maliciously.</p>
<p>Consensus is not the only way to verify in a trustless system; you can also use a cryptographic proof.A SNARK (Succinct Non-Interactive ARguments of Knowledge) is a cryptographic primitive that allows one party, called the prover, to convince another party, called the verifier, that a given statement is true, without the verifier needing to perform the actual computation.Cryptographic techniques like Zero-Knowledge Proofs (ZKPs) and verifiable computing enable a party to prove that a computation was performed correctly without revealing the details of the computation itself.These methods provide strong guarantees of correctness while preserving privacy.Despite these benefits, cryptographic methods are extremely computationally expensive, often times adding 10,000x more work on the prover.While existing SNARKs exist and have been demonstrated to work on smaller neural network models [15], their proving times are unusable for the large diffusion and language models being deployed today.Thus, reproducibility of machine learning goes beyond scientific inquiry; rather it also has applications in consensus mechanisms to verify generative machine learning tasks.</p>
<p>Why Reproducability in Generative AI is Hard</p>
<p>As stated previously, in order to have an effective consensus model, and ultimately verify the correctness of machine learning tasks in a distributed, decentralized system, honest nodes must come to agreement and must agree on the same value.However, in the realm of machine learning and deep learning, the parallel nature of the computations can introduce a slight non-determinism.This is because floating-point operations are not exactly associative, and when executed in parallel, the order in which they are executed can vary.This can cause tiny differences in the results, which may accumulate over time.Additionally, some GPUs and hardware accelerators have built-in randomness.For example, the NVIDIA Tensor Cores use probabilistic rounding [16], which can make results slightly different even if everything else is kept constant.</p>
<p>Software-wise, machine learning frameworks often introduce non-determinism.For example, PyTorch, like many other deep learning frameworks, involves operations that can yield different results across multiple executions, even when using identical seeds [17], see Figure 1.This non-deterministic behavior can be attributed to factors such as the use of multi-threading, which can lead to race conditions, or specific hardware and software configurations that introduce variability.While you can limit the sources of non-deterministic behavior or use deterministic algorithms instead of non-deterministic ones where available, this often comes at the cost of performance.As a concrete example, CUDA convolution operations, which use the cuDNN library, can be a source of nondeterminism.This is due to the benchmarking feature of cuDNN, which can select different algorithms for convolution operations based on the size parameters [18].Disabling this feature can lead to more deterministic but potentially slower performance.</p>
<p>Fig. 1.Identical runs of the same seed and prompt to generative AI does not yield the same bit level result, even on the same machine.Crytographic hashes that are designed for exact matches exhibit the avalanche effect, while perceptual hashes (aHash, pHash, dHash, cHash) are more stable and exhibit locality sensitive hashing.</p>
<p>Methodology</p>
<p>Assuming a set of machine learning nodes are performing inference or training work on generative AI, how do we know the machine learning node did what it was supposed to do? Can we reproduce its results?Recall in the typical deterministic consensus setting, multiple nodes can re-run the instructions and check the results.This checking mechanism is typically performed by computing a cryptographic hash of the output or state.If the instructions were all run correctly, then any validator should have identical state hashes.Any minor change in the state (like altering a single bit) leads to a substantial change in the output hash, i.e. each of the output bits changes with a 50% probability.This effect, avalanche effect, can be seen in common hash functions such as the SHA-1 or MD5 hash function.If a single bit is modified, the resulting hash sum becomes entirely different.This makes it extremely difficult to predict the output of the hash function based on a given input, which is a key aspect of its security.</p>
<p>Locality-Sensitive Hashing</p>
<p>In our case, we employ different algorithms that exhibit the Locality-sensitive hashing (LSH) property that probabilistically groups similar input items into the same "buckets".Unlike traditional hashing techniques that aim to minimize hash collisions, LSH intentionally maximizes them.This technique can also be viewed as a method for reducing the dimensionality of high-dimensional data, where high-dimensional input items are transformed into lower-dimensional versions while maintaining the relative distances between items.We utilize the following set of perceptual hashes (a hash string that approximates the visual characteristics of an image), that are commonly used in image hashing, Average Hash (aHash): This is a type of perceptual hash that works by resizing the image to a small, fixed resolution, converting it to grayscale, calculating the mean pixel value, and then generating a hash based on whether each pixel is above or below the mean.Perceptual Hash (pHash): This is a more complex type of perceptual hash that involves the Discrete Cosine Transform (DCT).The image is resized and converted to grayscale, the DCT is applied, the top-left portion of the DCT matrix (which represents the lowest frequencies) is retained, the mean value is calculated excluding the first element, and a hash is generated based on whether each value is above or below the mean.Difference Hash (dHash): This is another type of perceptual hash that works by comparing the relative gradients of the pixel values.The image is resized and converted to grayscale, each pixel is compared to its neighbor, and a hash is generated based on whether each pixel is greater than or less than its neighbor.Color Hash (cHash): This involves generating a hash based on the colors in an image.It involves resizing the image to a small, fixed resolution and generating a hash based on the quantized color values of each pixel.</p>
<p>These types of perceptual hashes are used in search by image, e.g.Google images searching [19], or things like identifying songs with the same fingerprint (Shazam) [20].For our purposes, the perceptual hashes work well, but there are some slight variations in the hash as shown by Figure 2.</p>
<p>Generative Image and Language Models</p>
<p>Our image generation experiments are centered around stable diffusion models and fine-tuned variants.Stable diffusion supports the ability to generate new images from scratch through the use of a text prompt describing elements to be included or omitted from the output.The architecture of the model includes a variational autoencoder (VAE), a U-Net, and text encoder.Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion.The U-Net denoises the output from forward diffusion backwards to obtain a latent representation.Finally, the VAE decoder generates the final image by converting the representation back into pixel space.Popular fine-tuning methods have been employed to personalize the diffusion models, including fine-tuning all of the weights, or low rank and textual finetuning methods.</p>
<p>Low-Rank Adapation</p>
<p>The LoRA, or Low-Rank Adaptation [21], proposes a new method for adapting large-scale pre-trained language models to specific tasks or domains.Full fine-tuning (retraining all model parameters) becomes less feasible as the size of models continue to expand, i.e. fine-tuning the 175 billion parameters in GPT-3 is prohibitively expensive.</p>
<p>Low-Rank Adaptation (LoRA) freezes the pre-trained model weights and introduces trainable rank decomposition matrices into each layer of the Transformer architecture.This approach significantly reduces the number of trainable parameters for downstream tasks.LoRA can reduce the number of trainable parameters by orders of magnitude.In the context of image generation, LoRAs have been used to guide the diffusion process towards particular concepts or visual representations.</p>
<p>Textual Inversion Textual inversions [22] are an alternative approach to personalizing text-to-image generation.Using only a small number of images (typically 3-5 images) of a user-provided concept, such as an object or a style, inversions learn to represent visual concepts through new "words" in the embedding space of a frozen text-to-image model.These words can be composed into natural language sentences, guiding personalized creation.Interestingly, a single word embedding is sufficient for capturing unique and varied concepts.</p>
<p>Large Language Model Generation</p>
<p>Text generation is crucial for many NLP tasks, including open-ended text generation, summarization, translation, and more.The text generation process, known as decoding, can be customized to improve the quality of the generated output and reduce repetition.However, in our case, our goal is to produce quality output that can be verified by other nodes.</p>
<p>The generation configuration that yields deterministic results uses a simple decoding strategy called greedy search, which picks the token with the highest probability as the next token.For a more globally "aware" generation strategy, we can also utilize beam-search decoding, which keeps several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence.</p>
<p>We minimize the stocasticity of the output by turning off stragegies such as multinomial sampling which randomly selects the next token based on the probability distribution over the entire vocabulary, and Beam-Search Multinomial Sampling.</p>
<p>Experiments and Results</p>
<p>For our experiments, we set up a heterogeneous, decentralized GPU network consisting of (8x) 3080ti GPUs running on 1x PCI-e risers, (4x) 3070ti GPUs running on GPU risers, (4x) 3060ti GPUs, and two other machines each running a single 3090 connected directly to the motherboard via PCI-e 16x slots.There are an additional (4x) A40 Nvidia GPUs connected in an NLink configuration.In total, the tasks described below were performed on a mix of 22 total GPUs.Unless specified in the experiment, the GPUs were selected to perform a task at random.Each machine was running in an Ubuntu 22 environment with CUDA 12 and torch 2.0.</p>
<p>We structure our results as follows: (1) We first compute the independent likelihoods of determinism in several image and language models.(2) We then use these likelihoods to compute an algorithm to verify machine learning inference tasks on a decentralized network.(3) Next, we describe how to verify in the training setting, and lastly (4) share the derived generated dataset for reproducibility and to benefit the community at large.</p>
<p>Consensus Likelihoods of Image and Language Generation</p>
<p>In our first experiment, generate images from different variants of stable diffusion [6], including the base v1.5 model, a fine-tuned version of stable diffusion, LoRA or low rank adapatations [21], and textual inversions [22].We use a template "A photo of {}", where the class variable inserted is a class from the ILSVRC challenge [23].We generate 7 images with identical prompts and seeds, for a total of approximately 7000 images per image model.The results of our generation can be seen in Table 1.</p>
<p>We compute four different types of perceptual hashes and compare the results.Each class (group of 7 images) is hashed and compared with each other.The mode hash is selected as the "correct" one, and the number of outliers are computed across all thousand categories and reported as the outliers.We also record the number of outliers when allowing for a tolerance of 1 (Ot&lt;=1) or 2 (Ot&lt;=2) hash discrepancies (hamming distance of 1 or 2).Additionally, we report the average hamming distance (aDist) of the outliers and the time to compute a single hash on a 512x512 image.While the color hash technique yields the best accuracy results, the average hash demonstrates nearly the same performance, yet with a speed up of approximately 3.5x.The consensus percentages are computed by the number of outliers over the total number of images hashed.This can be used as the likelihood that an image generated with the same prompt and seed will generate the same perceptual hash.</p>
<p>For language generation, we sample several open source LLMs and run the same prompt over four different machines and GPUs.The five LLMs tested were the Wizard Vicuna 13B [24] in 4-bit quantizaiton mode, the Vicuna 7B [25] in 8-bit quantization, Red Pajama 7B [26] in 8-bit quantization, the Red Pajama 3B [26], and GPT-J 6B [27] in 8-bit quantization.We utilized the "instruct" version of the LLM and provided a prompt, "### Human: Please write a description about {name} ### Assistant:", where the name comes from the 1000 ImageNet classes.A total of 4000 sentence generations were created across different decoding techniques and beam searches as shown in Table 2.Even with the GPU non-determinism, we saw no stocasticity in the LLM outputs when greedy or n-beam methods were used to decode.When explicitly specifying multinomial sampling where the model selects the next token based on the probability distribution over the entire vocabulary, we do observe the expected non-deterministic  1. Generated images from different variants of stable diffusion [6], including the base v1.5 model, a fine-tuned version of stable diffusion, LoRA or low rank adapatations [21], and textual inversions [22].Approximately 7 images with identical prompts and seeds were generated per 1000 classes in the ImageNet class categories.The number of outliers to the majority hash in the class is shown with tolerances of 0, 1 (Ot&lt;=1), and 2 (Ot&lt;=2).The average distance of all outliers is shown as aDist, and the time per image hash is presented in the last column.</p>
<p>behavior.As a final note, we believe that although we did not observe any mismatched tokens between GPUs, we believe that there is a non-zero chance of a token mismatch in the greedy and n-beam case.The nature of the floating point operations and error drift, with a special edge case probability that is near the border of two words, could possibly flip the generated next word; however, this would be an extremely rare edge case.</p>
<p>Method # Gen. Greedy(30) Greedy(60) Beam N=5 Beam N=10 Multin.Wizard Vicuna 13B [24]</p>
<p>Tolerance in Reproducability</p>
<p>Given the independent likelihoods of determinism computed from the previous experiment, we can now derive an machine learning algorithm for a decentralized network.We provide posterior probabilities of detecting incorrect or fraudulent behavior in the case that we assume the majority of nodes in the network are honest (Figure 3(a)), or the case where we require a super-majority (Figure 3(b)).We use, P (X = k) = n k p k (1 âˆ’ p) nâˆ’k , where the binomial distribution gives the probability of getting exactly k successes (defined as generating the correct perceptual hash) over n independent verifiers.The P (X = k) is the probability of getting exactly k verifications, where p is the probability of generating a correct output (derived from above).Verification of Correctness -Type I Error -For majority vote and tolerance of 2, we can achieve 99.843%, 99.988%, and 99.999% verification with 3, 5, and 7 independent verifiers.For super majority (greater than 2/3s) and tolerance of 2, we can achieve 99.692%, 99.960%, and 99.999% verification accuracy with 4, 7, and 10 independent verifiers; these probabilities demonstrate strong verification with a minimal redudant work.</p>
<p>(a) Simple Majority (b) 2/3 Super Majority Fig. 3. Graphs of probabilities that independent verifiers can spot incorrect or fraudlent behavior given different likelihoods of deterministic generation.In the simple majority case, we assume a majority of nodes are honest, and in the super majority case, we have a stricter assumption that over 2/3s are honest.</p>
<p>Another type of error is the accidental or malicious generation of a perceptual hash without performing the task, or guessing a hash based upon the given prompt.To assess the risk of this error, we generate over 1.3M images using stable diffusion.Our simulation mimics a scenario where a malicious actor sees the prompt, and tries to guess at a perceptual hash.Thus, these guess images are generated using identical prompts as the one provided (a photo of {}), but with a different seed.Verification of Correctness -Type II Error -For each category, we generated 1300 images (a total of 1.3M), and perform an all-to-all average hash comparison and count collisions.The total number of hash comparisons here is approximately 0.85 billion.The probability of an intra-class perceptual hash collision is less than 0.0267%, when allowing for a hamming distance tolerance of &lt;=2.This indicates that there is nearly zero percent chance that an adversary would be able to guess the perceptual hash -even with information about the prompt given.</p>
<p>Verification of Generative Training</p>
<p>We now turn our attention to the case of generative AI training.In particular, we focus on the textual inversion fine-tuning case; this is a likely scenario in a distributed, heterogeneous GPU network.Textual Inversion is a technique for capturing novel concepts from a small number of example images.It learns new "words" in the text encoder's embedding space, which are used within text prompts for personalized image generation.Importantly, the weights of the U-Net and VAE are frozen, thus, the only parameters that are able to change and reduce the loss are the 768-dimensional word embedding.</p>
<p>Similar to the inference case, we would need the ability to perform a fraud proof over training epochs.Thus, we need to identify the major sources of controllable stocasticity and minimize them to allow for replication.Figure 4, identifies and minimizes the six controllable sources of randomness including, horizontal flips of the data, random choice of the template, training data shuffling, gaussian noise for the latents, random choice of timestep, and the noise scheduler.The randomness is minimized at each of these six points in order to gain verifiability in the training process.Even with control over these parameters via seeds or parameter settings, the process still remains non-deterministic.</p>
<p>We remove these sources of stocasticity and train several dozens of textual inversions randomly sampled from the huggingface concepts library1 .For each concept, we present 5-10 exemplar images, and train the model with the following hyperparameters: learning rate is 5e-04, maximum train steps is 2000, batch size is 4, gradient accumulation is 1, and checkpoints are generated every 50 steps.The model is trained in mixed 16-bit precision.Each textual inversion train is performed six times.The first three times are deterministic runs -we minimize all controllable sources of stochasticity.The next three runs are ablations over the possible sources of randomness.For run 4, we allow horizontal flips in the training process, for run 5 we allow the input dataset to be shuffled, and for run 6, we do not set the seed for training and noise.We take the six runs and perform PCA over the 768-dimensional space and project the checkpoint embeddings onto principal components 1 and 2. The plots of the training checkpoints of all six runs can be seen in Figure 5.We observe that by minimizing the stocasticity leads to nearly deterministic training.There are cases in Figure 5(b) and Figure 5(e) where the random error begins to propagate and cause drift in the final textual embeddings.To control for this, we demonstrate a gossip training procedure in Figure 6 where the checkpoints between deterministic nodes can be synchronized every several checkpoints (here we do every 6 checkpoints, or 300 epochs).Given the gossip training mechanism, the end embedding weights of the verifiers follow the trajectory of the trainer and provide much tighter error bounds.</p>
<p>Conclusion</p>
<p>In conclusion, we demonstrate that we are able to reproduce and verify the correctness of machine learning tasks in a distributed, decentralized network.We tackle a particularly challenging task in ML of verifying both the inference and training of generative AI.We present the likelihoods of an honest node producing an output that can be verified by other independent nodes in the network and in the process generated millions of data samples and billions of hash comparisons.</p>
<p>We provided empirical evidence of different parameterizations of tolerance and error bounds for verification as the groundwork for a fraud proof in a blockchain network.Our results show that with minimal overhead and extremely high precision, we can verify generative AI work in both image and language generation, and a malicious actor has close to zero percent chance of exploiting the algorithm.The study also identified and minimized the major sources of stochasticity in generative AI training and presented gossip and synchronization training techniques for verifiability.</p>
<p>In summary, we provide a robust and practical foundation for AI verification, reproducibility, and consensus, significantly reducing the need for trust in a decentralized network.</p>
<p>Fig. 2 .
2
Fig. 2. Generated images from SD v1.5 on seven different GPUs (mix of 3060ti, 3070ti, 3080ti, and 3090) using the prompt, "A photo of {class}", where class is from the ImageNet dataset.The first three columns, class id (469) [caldron, cauldron], (361) [skunk, polecat], and (695) [padlock] have identical perceptual hashes.The last three rows, (168) [redbone], (555) [fire engine, fire truck], (686) [oil filter] have perceptual hashes with the most extreme hamming distances we observed in our generated data (up to 5).Visual differences can be seen in the writing, the color of the truck, and the circle in the oil filter.</p>
<p>Fig. 4 .
4
Fig. 4. Six major sources of stocasticity in the fine-tuning process of textual inversion.The randomness is minimized at each of these six points in order to gain verifiability in the training process.Even with control over these parameters via seeds or parameter settings, the process still remains non-deterministic.</p>
<p>Fig. 5 .
5
Fig.5.Textual inversion training results from randomly selected objects in the sd concepts library (https://huggingface.co/sd-concepts-library).The graphs show six runs of 2000 iterations fine-tuning a new token based upon a small set of images(3)(4)(5)(6)(7)(8)(9)(10).Three runs minimize the amount of stocasticity within the training process, "DeterR1,2,3"."StocHF0.5"includes a random horizontal flip of the image, "Shuffle" allows the dataset to be shuffled, and "Noseed" does not provide a random seed for the noise.The deterministic runs mostly overlap with some drift over the plot of the 768-dimensional vector projected on the 2 principal components of the training data.</p>
<p>Fig. 6 .
6
Fig. 6.Training in a textual inversion with resyncing of checkpoints every 300 iterations on the <kitchenrobot> inversion.(1) Three training instances (Sync Deter1,2,3) are introduced and synchronized to De-terR1 during training.(2) At the start, there is some amount of variation within the training but within an error bounds of all three Deterministic Runs.(3) By the end of training 2000 iterations, the Synchronized trainings and Deterministic R1, are nearly identical.The graph shows 4 overlapping vector projections from the DeterR1 and Sync Deter1,2,and 3.</p>
<p>Table 2 .
2
Generated 4,000 descriptions from different LLMs of different parameter sizes.Different decoing strategies include greedy methods of 30 max tokens and 60 max tokens, beam search of size 5 and 10, as well as multinomial decoding.All greedy and beam search strategies were empirically deterministic across different machines and GPUs.The multinomial case was non-deterministic as expected.A "Yes" indicates observed deterministic behavior.
4000YesYesYesYesNoVicuna 7B [25]4000YesYesYesYesNoRed Pajama 7B [26]4000YesYesYesYesNoRed Pajama 3B [26]4000YesYesYesYesNoGPT-J 6B [27]4000YesYesYesYesNo
https://huggingface.co/sd-concepts-library</p>
<p>Carbontracker: Tracking and predicting the carbon footprint of training deep learning models. Lasse F Wolff Anthony, Benjamin Kanding, Raghavendra Selvan, arXiv:2007.030512020arXiv preprint</p>
<p>Boinc: A system for public-resource computing and storage. Anderson David, Fifth IEEE/ACM international workshop on grid computing. IEEE2004</p>
<p>Seti@ home: an experiment in public-resource computing. Jeff David P Anderson, Eric Cobb, Matt Korpela, Dan Lebofsky, Werthimer, Communications of the ACM. 45112002</p>
<p>Folding@ home: Lessons from eight years of volunteer distributed computing. Adam L Beberg, Guha Daniel L Ensign, Siraj Jayachandran, Vijay S Khaliq, Pande, 2009 IEEE International Symposium on Parallel &amp; Distributed Processing. IEEE2009</p>
<p>Generative adversarial networks. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Communications of the ACM. 63112020</p>
<p>High-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2022</p>
<p>An introduction to variational autoencoders. Foundations and TrendsÂ® in Machine Learning. Max Diederik P Kingma, Welling, 201912</p>
<p>U-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, GermanySpringerOctober 5-9, 2015. 2015Proceedings, Part III 18</p>
<p>Learning visual n-grams from web data. Ang Li, Allan Jabri, Armand Joulin, Laurens Van Der Maaten, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2017</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Verification for machine learning, autonomy, and neural networks survey. Weiming Xiang, Patrick Musau, Ayana A Wild, Diego Manzanas Lopez, Nathaniel Hamilton, Xiaodong Yang, Joel Rosenfeld, Taylor T Johnson, arXiv:1810.019892018arXiv preprint</p>
<p>The challenge of verification and testing of machine learning. Ian Goodfellow, Nicolas Papernot, 2017Cleverhans-blog</p>
<p>Understanding blockchain consensus models. Arati Baliga, 2017414Persistent</p>
<p>Md Sadek Ferdous, Mohammad Jabed, Morshed Chowdhury, Mohammad A Hoque, Alan Colman, arXiv:2001.07091Blockchain consensus algorithms: A survey. 2020arXiv preprint</p>
<p>Scaling up trustless dnn inference with zero-knowledge proofs. Daniel Kang, Tatsunori Hashimoto, Ion Stoica, Yi Sun, arXiv:2210.086742022arXiv preprint</p>
<p>Recovering single precision accuracy from tensor cores while surpassing the fp32 theoretical peak performance. Hiroyuki Ootomo, Rio Yokota, The International Journal of High Performance Computing Applications. 3642022</p>
<p>Performance evaluation of cudnn convolution algorithms on nvidia volta gpus. Marc Jorda, Pedro Valero-Lara, Antonio J Pena, IEEE Access. 72019</p>
<p>Testing different image hash functions. </p>
<p>locality-sensitive-hashing-for-music-search-f2f1940ace23. Locality sensitive hashing for similar item search</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>An image is worth one word: Personalizing text-toimage generation using textual inversion. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, H Amit, Gal Bermano, Daniel Chechik, Cohen-Or, arXiv:2208.016182022arXiv preprint</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, 2023</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>Redpajama: An open source recipe to reproduce llama training dataset. 2023Together Computer</p>
<p>Ben Wang, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX. May 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4524 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4524</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4524</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-fede2b0d6f7803fb3b0d8b8d33387b7144290b80</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fede2b0d6f7803fb3b0d8b8d33387b7144290b80" target="_blank">Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning</a></p>
                <p><strong>Paper Venue:</strong> Bioinform.</p>
                <p><strong>Paper TL;DR:</strong> This method supports a general strategy of leveraging the language interpreting capabilities of LLMs to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the LLM.</p>
                <p><strong>Paper Abstract:</strong> Abstract Motivation Creating knowledge bases and ontologies is a time consuming task that relies on manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrarily complex nested knowledge schemas. Results Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against an LLM to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for matched elements. We present examples of applying SPIRES in different domains, including extraction of food recipes, multi-species cellular signaling pathways, disease treatments, multi-step drug mechanisms, and chemical to disease relationships. Current SPIRES accuracy is comparable to the mid-range of existing Relation Extraction methods, but greatly surpasses an LLM’s native capability of grounding entities with unique identifiers. SPIRES has the advantage of easy customization, flexibility, and, crucially, the ability to perform new tasks in the absence of any new training data. This method supports a general strategy of leveraging the language interpreting capabilities of LLMs to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the LLM. Availability and implementation SPIRES is available as part of the open source OntoGPT package: https://github.com/monarch-initiative/ontogpt.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4524.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4524.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPIRES / OntoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES) implemented in the OntoGPT package</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A schema-driven zero-shot information extraction method that uses LLM prompt completions (GPT-3.5/GPT-4 variants) to populate nested knowledge schemas from free text, with recursive prompting and post-hoc grounding to ontologies via OAKlib and external normalizers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-16k (gpt-3.5-turbo), gpt-4-1106-preview (gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SPIRES (Structured Prompt Interrogation and Recursive Extraction of Semantics); implementation: OntoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Zero-shot, schema-driven pipeline: (1) user provides a detailed LinkML knowledge schema and entry class; (2) GeneratePrompt constructs an instruction + a pseudo-YAML attribute template (AttributeTemplate) and concatenates input text; (3) the prompt is sent to an LLM for completion (CompletePrompt), requesting results conforming to the template; (4) ParseCompletion heuristically parses the LLM output (line-splitting, attribute matching, token delimiters), and recursively calls SPIRES for inlined class ranges; (5) Ground uses ontology annotators and OAKlib (Gilda, BioPortal, Ontology Lookup Service, NCATS NodeNormalizer, etc.) to map extracted strings to persistent identifiers and validate against allowed ID prefixes/value sets; (6) optional TranslateToOWL to convert instances to OWL using LinkML-OWL/ROBOT and run reasoning. Prompting strategy emphasizes constrained pseudo-YAML templates, low creativity settings, and recursive decomposition of nested schema elements. Two input-processing modes tested: chunking (sliding-window) and no-chunking (full text in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>500 abstracts (BC5CDR test set) plus multiple other textual inputs (e.g., scraped recipes); also evaluations using 100-term samples from several ontologies</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical / life sciences (chemical-disease relations, drug mechanisms, cellular signaling/pathways, disease treatments) and demonstration in food/recipes (food ontology)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Qualitative mechanistic/causal relationships and process descriptions: e.g., 'chemical induces disease' relations (causal/associative), multi-step mechanistic descriptions of drug actions and cellular signaling pathways (mechanistic explanations), and stepwise procedural knowledge (process steps in recipes).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Examples extracted in paper: (1) Cromakalim INDUCES vasodilation of large and small coronary vessels (PMID 2160002); (2) Lithium INDUCES hypercalcemia in chronic exposure (PMID 19154241); (3) Fluorouracil INDUCES transient brain disease/encephalopathy (PMID 10327032). Supplementary example: extraction of gene/pathway assertions such as 'β-catenin is required for cGAS/STING signaling' from a research abstract (shown in Supplementary).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to gold-standard relation triples in the BioCreative BC5CDR test set (comparison to ground truth), grounding evaluation versus curated ontology term lists (random 100-term samples from GO, EMAPA, MONDO), and ablation-style comparisons to direct LLM prompting without SPIRES grounding; no model fine-tuning (zero-shot) for main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Grounding accuracy on ontology term lists: GO: SPIRES returned 98/100 correct identifiers with GPT-3.5-turbo and 97/100 with GPT-4-turbo; EMAPA: 100/100 with SPIRES; MONDO: SPIRES with GPT-3.5-turbo 97/100, SPIRES with GPT-4-turbo 18/100. Direct prompting without SPIRES: at most 3/100 correct (GPT-3.5) for GO. BC5CDR relation extraction: chunking + GPT-3.5-turbo F1 = 41.16 (precision 0.43, recall 0.39); no-chunking GPT-3.5-turbo F1 = 36.64 (precision 0.63, recall 0.26); no-chunking GPT-4 F1 = 43.80 (precision 0.69, recall 0.32). Named-entity recognition (MeSH grounding) (no chunking): Chemical — GPT-3.5-turbo F1 69.70 (P 0.89, R 0.57); GPT-4 F1 73.69 (P 0.85, R 0.65). Disease — GPT-3.5-turbo F1 61.70 (P 0.87, R 0.48); GPT-4 F1 69.70 (P 0.88, R 0.56).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to direct LLM prompting without SPIRES grounding (LLM 'naive' prompts), SPIRES dramatically improved grounding to ontologies. Also compared qualitatively to prior trained RE systems on BC5CDR: SPIRES (zero-shot) achieves mid-range performance relative to teams that trained on task-specific data (best trained models reported F1 up to ~0.57; BioGPT reported F1 44.98).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Schema-constrained zero-shot prompting plus post-hoc ontology grounding yields substantial improvements in mapping text spans to persistent identifiers compared to naive LLM prompting; SPIRES can populate nested schema structures and extract qualifiers making relations richer than simple triples. Performance on relation extraction tasks is competitive with mid-range supervised methods without task-specific training. GPT-4 sometimes refused to provide identifiers (safety/knowledge-cutoff behaviour), whereas GPT-3.5 often produced identifiers but with varying quality; model choice and prompt engineering significantly affect grounding reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hallucination and incorrect relation extraction remain leading failure modes; some LLM responses refused to return identifiers (model-specific refusals), parsing errors for complex labels, variability across ontologies and models (e.g., GPT-4 underperformed for certain ontology grounding tasks), lower recall on relation extraction, dependency on external ontologies and normalization services (scalability/cost), and cost/opacity concerns with closed commercial APIs. Authors stress need for human validation before KB insertion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4524.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4524.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dunn & Dagdelen et al. (fine-tuned GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that fine-tuned GPT-3 to extract structured relationships from unstructured materials-science text using engineered schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuned GPT-3 with engineered schemas</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tuning GPT-3 on task-specific training data with engineered schema templates to extract structured relationships from complex materials-science text (as described in the referenced work and cited in Related Work). The SPIRES paper references this as a complementary supervised, fine-tuned approach that aligns extractions with engineered schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Materials chemistry / materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Structured relationships from scientific text (e.g., synthesis steps, materials properties and relations) — i.e., domain-specific structured knowledge rather than formal mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Positioned as a supervised, fine-tuned alternative to zero-shot LLM methods like SPIRES; argued to be effective for engineered-schema extraction in materials domain.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuned LLMs can be engineered to extract structured relationships from scientific text in domain-specific settings (materials), demonstrating high utility when task-specific training data is available.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires substantial task-specific training data and engineering; less flexible for arbitrary nested schema extraction without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4524.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4524.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs4OL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs4OL: Large language models for ontology learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study exploring the application of LLMs to ontology learning and extraction; concluded current models are not yet sufficiently flexible for ontology-driven needs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLMs4OL: Large language models for ontology learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLMs4OL (evaluation/analysis framework)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Exploratory evaluation of applying large language models to ontology learning tasks; discussed model capabilities and limitations for ontology-driven information extraction and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Semantic web / ontology learning (general domain)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Ontology axioms/structured schema elements (not explicitly qualitative laws, but relevant to extracting structured domain knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Concludes LLMs are not yet sufficiently flexible for ontology-driven needs, serving as a cautionary comparison to schema-centered methods such as SPIRES.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LMs show promise but lack sufficient flexibility and reliability for robust ontology learning without additional methods/engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Insufficient flexibility for ontology-driven extraction; likely to hallucinate or fail to meet strict ontology constraints without additional normalization/grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4524.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4524.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MapperGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MapperGPT: Large language models for linking and mapping entities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced preprint demonstrating the use of LLMs for entity linking and ontology mapping tasks, improving alignment of extracted entities to existing ontologies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MapperGPT: Large language models for linking and mapping entities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MapperGPT (LLM-based entity linking/mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses LLMs to assist linking and mapping of textual entities to ontology identifiers, improving grounding accuracy; cited in SPIRES as related work on improving grounding inherent to extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Biomedical ontologies / entity normalization</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Entity mappings and alignment (supports structured knowledge extraction indirectly, rather than extracting high-level qualitative laws itself)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based linking can noticeably improve ontology alignment and grounding accuracy when combined with external resources.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependent on prompt design and supplemental grounding tools; not a standalone solution for schema-population without integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models <em>(Rating: 2)</em></li>
                <li>BioGPT: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 2)</em></li>
                <li>MapperGPT: Large language models for linking and mapping entities <em>(Rating: 2)</em></li>
                <li>LLMs4OL: Large language models for ontology learning <em>(Rating: 2)</em></li>
                <li>Is ChatGPT a biomedical expert? - exploring the Zero-Shot performance of current GPT models in biomedical tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4524",
    "paper_id": "paper-fede2b0d6f7803fb3b0d8b8d33387b7144290b80",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "SPIRES / OntoGPT",
            "name_full": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES) implemented in the OntoGPT package",
            "brief_description": "A schema-driven zero-shot information extraction method that uses LLM prompt completions (GPT-3.5/GPT-4 variants) to populate nested knowledge schemas from free text, with recursive prompting and post-hoc grounding to ontologies via OAKlib and external normalizers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-16k (gpt-3.5-turbo), gpt-4-1106-preview (gpt-4-turbo)",
            "model_size": null,
            "method_name": "SPIRES (Structured Prompt Interrogation and Recursive Extraction of Semantics); implementation: OntoGPT",
            "method_description": "Zero-shot, schema-driven pipeline: (1) user provides a detailed LinkML knowledge schema and entry class; (2) GeneratePrompt constructs an instruction + a pseudo-YAML attribute template (AttributeTemplate) and concatenates input text; (3) the prompt is sent to an LLM for completion (CompletePrompt), requesting results conforming to the template; (4) ParseCompletion heuristically parses the LLM output (line-splitting, attribute matching, token delimiters), and recursively calls SPIRES for inlined class ranges; (5) Ground uses ontology annotators and OAKlib (Gilda, BioPortal, Ontology Lookup Service, NCATS NodeNormalizer, etc.) to map extracted strings to persistent identifiers and validate against allowed ID prefixes/value sets; (6) optional TranslateToOWL to convert instances to OWL using LinkML-OWL/ROBOT and run reasoning. Prompting strategy emphasizes constrained pseudo-YAML templates, low creativity settings, and recursive decomposition of nested schema elements. Two input-processing modes tested: chunking (sliding-window) and no-chunking (full text in prompt).",
            "number_of_papers": "500 abstracts (BC5CDR test set) plus multiple other textual inputs (e.g., scraped recipes); also evaluations using 100-term samples from several ontologies",
            "domain_or_field": "Biomedical / life sciences (chemical-disease relations, drug mechanisms, cellular signaling/pathways, disease treatments) and demonstration in food/recipes (food ontology)",
            "type_of_laws_extracted": "Qualitative mechanistic/causal relationships and process descriptions: e.g., 'chemical induces disease' relations (causal/associative), multi-step mechanistic descriptions of drug actions and cellular signaling pathways (mechanistic explanations), and stepwise procedural knowledge (process steps in recipes).",
            "example_laws_extracted": "Examples extracted in paper: (1) Cromakalim INDUCES vasodilation of large and small coronary vessels (PMID 2160002); (2) Lithium INDUCES hypercalcemia in chronic exposure (PMID 19154241); (3) Fluorouracil INDUCES transient brain disease/encephalopathy (PMID 10327032). Supplementary example: extraction of gene/pathway assertions such as 'β-catenin is required for cGAS/STING signaling' from a research abstract (shown in Supplementary).",
            "evaluation_method": "Comparison to gold-standard relation triples in the BioCreative BC5CDR test set (comparison to ground truth), grounding evaluation versus curated ontology term lists (random 100-term samples from GO, EMAPA, MONDO), and ablation-style comparisons to direct LLM prompting without SPIRES grounding; no model fine-tuning (zero-shot) for main experiments.",
            "performance_metrics": "Grounding accuracy on ontology term lists: GO: SPIRES returned 98/100 correct identifiers with GPT-3.5-turbo and 97/100 with GPT-4-turbo; EMAPA: 100/100 with SPIRES; MONDO: SPIRES with GPT-3.5-turbo 97/100, SPIRES with GPT-4-turbo 18/100. Direct prompting without SPIRES: at most 3/100 correct (GPT-3.5) for GO. BC5CDR relation extraction: chunking + GPT-3.5-turbo F1 = 41.16 (precision 0.43, recall 0.39); no-chunking GPT-3.5-turbo F1 = 36.64 (precision 0.63, recall 0.26); no-chunking GPT-4 F1 = 43.80 (precision 0.69, recall 0.32). Named-entity recognition (MeSH grounding) (no chunking): Chemical — GPT-3.5-turbo F1 69.70 (P 0.89, R 0.57); GPT-4 F1 73.69 (P 0.85, R 0.65). Disease — GPT-3.5-turbo F1 61.70 (P 0.87, R 0.48); GPT-4 F1 69.70 (P 0.88, R 0.56).",
            "comparison_baseline": "Compared to direct LLM prompting without SPIRES grounding (LLM 'naive' prompts), SPIRES dramatically improved grounding to ontologies. Also compared qualitatively to prior trained RE systems on BC5CDR: SPIRES (zero-shot) achieves mid-range performance relative to teams that trained on task-specific data (best trained models reported F1 up to ~0.57; BioGPT reported F1 44.98).",
            "key_findings": "Schema-constrained zero-shot prompting plus post-hoc ontology grounding yields substantial improvements in mapping text spans to persistent identifiers compared to naive LLM prompting; SPIRES can populate nested schema structures and extract qualifiers making relations richer than simple triples. Performance on relation extraction tasks is competitive with mid-range supervised methods without task-specific training. GPT-4 sometimes refused to provide identifiers (safety/knowledge-cutoff behaviour), whereas GPT-3.5 often produced identifiers but with varying quality; model choice and prompt engineering significantly affect grounding reliability.",
            "challenges_limitations": "Hallucination and incorrect relation extraction remain leading failure modes; some LLM responses refused to return identifiers (model-specific refusals), parsing errors for complex labels, variability across ontologies and models (e.g., GPT-4 underperformed for certain ontology grounding tasks), lower recall on relation extraction, dependency on external ontologies and normalization services (scalability/cost), and cost/opacity concerns with closed commercial APIs. Authors stress need for human validation before KB insertion.",
            "uuid": "e4524.0",
            "source_info": {
                "paper_title": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Dunn & Dagdelen et al. (fine-tuned GPT-3)",
            "name_full": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "brief_description": "A cited work that fine-tuned GPT-3 to extract structured relationships from unstructured materials-science text using engineered schemas.",
            "citation_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (fine-tuned)",
            "model_size": null,
            "method_name": "Fine-tuned GPT-3 with engineered schemas",
            "method_description": "Fine-tuning GPT-3 on task-specific training data with engineered schema templates to extract structured relationships from complex materials-science text (as described in the referenced work and cited in Related Work). The SPIRES paper references this as a complementary supervised, fine-tuned approach that aligns extractions with engineered schemas.",
            "number_of_papers": null,
            "domain_or_field": "Materials chemistry / materials science",
            "type_of_laws_extracted": "Structured relationships from scientific text (e.g., synthesis steps, materials properties and relations) — i.e., domain-specific structured knowledge rather than formal mathematical laws.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": "Positioned as a supervised, fine-tuned alternative to zero-shot LLM methods like SPIRES; argued to be effective for engineered-schema extraction in materials domain.",
            "key_findings": "Fine-tuned LLMs can be engineered to extract structured relationships from scientific text in domain-specific settings (materials), demonstrating high utility when task-specific training data is available.",
            "challenges_limitations": "Requires substantial task-specific training data and engineering; less flexible for arbitrary nested schema extraction without retraining.",
            "uuid": "e4524.1",
            "source_info": {
                "paper_title": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "LLMs4OL",
            "name_full": "LLMs4OL: Large language models for ontology learning",
            "brief_description": "A cited study exploring the application of LLMs to ontology learning and extraction; concluded current models are not yet sufficiently flexible for ontology-driven needs.",
            "citation_title": "LLMs4OL: Large language models for ontology learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "LLMs4OL (evaluation/analysis framework)",
            "method_description": "Exploratory evaluation of applying large language models to ontology learning tasks; discussed model capabilities and limitations for ontology-driven information extraction and alignment.",
            "number_of_papers": null,
            "domain_or_field": "Semantic web / ontology learning (general domain)",
            "type_of_laws_extracted": "Ontology axioms/structured schema elements (not explicitly qualitative laws, but relevant to extracting structured domain knowledge)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": "Concludes LLMs are not yet sufficiently flexible for ontology-driven needs, serving as a cautionary comparison to schema-centered methods such as SPIRES.",
            "key_findings": "Large LMs show promise but lack sufficient flexibility and reliability for robust ontology learning without additional methods/engineering.",
            "challenges_limitations": "Insufficient flexibility for ontology-driven extraction; likely to hallucinate or fail to meet strict ontology constraints without additional normalization/grounding.",
            "uuid": "e4524.2",
            "source_info": {
                "paper_title": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MapperGPT",
            "name_full": "MapperGPT: Large language models for linking and mapping entities",
            "brief_description": "A referenced preprint demonstrating the use of LLMs for entity linking and ontology mapping tasks, improving alignment of extracted entities to existing ontologies.",
            "citation_title": "MapperGPT: Large language models for linking and mapping entities",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "MapperGPT (LLM-based entity linking/mapping)",
            "method_description": "Uses LLMs to assist linking and mapping of textual entities to ontology identifiers, improving grounding accuracy; cited in SPIRES as related work on improving grounding inherent to extraction.",
            "number_of_papers": null,
            "domain_or_field": "Biomedical ontologies / entity normalization",
            "type_of_laws_extracted": "Entity mappings and alignment (supports structured knowledge extraction indirectly, rather than extracting high-level qualitative laws itself)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "LLM-based linking can noticeably improve ontology alignment and grounding accuracy when combined with external resources.",
            "challenges_limitations": "Dependent on prompt design and supplemental grounding tools; not a standalone solution for schema-population without integration.",
            "uuid": "e4524.3",
            "source_info": {
                "paper_title": "Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "rating": 2
        },
        {
            "paper_title": "BioGPT: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 2
        },
        {
            "paper_title": "MapperGPT: Large language models for linking and mapping entities",
            "rating": 2
        },
        {
            "paper_title": "LLMs4OL: Large language models for ontology learning",
            "rating": 2
        },
        {
            "paper_title": "Is ChatGPT a biomedical expert? - exploring the Zero-Shot performance of current GPT models in biomedical tasks",
            "rating": 1
        }
    ],
    "cost": 0.01491525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for POPULATING KNOWLEDGE BASES USING ZERO-SHOT LEARNING</h1>
<p>J. Harry Caufield ${ }^{1}$, Harshad Hegde ${ }^{1}$, Vincent Emonet ${ }^{2}$, Nomi L. Harris ${ }^{1}$, Marcin Joachimiak ${ }^{1}$, Nicolas Matentzoglu ${ }^{3}$, HyeongSik Kim ${ }^{4}$, Sierra Moxon ${ }^{1}$, Justin T. Reese ${ }^{1}$, Melissa A. Haendel ${ }^{5}$, Peter N. Robinson ${ }^{6}$, and Christopher J. Mungall ${ }^{1}$<br>${ }^{1}$ Division of Environmental Genomics and Systems Biology, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA<br>${ }^{2}$ Institute of Data Science, Faculty of Science and Engineering, Maastricht University, Maastricht, The Netherlands<br>${ }^{3}$ Semanticly Ltd, Athens, Greece<br>${ }^{4}$ Robert Bosch LLC, Sunnyvale, CA 94085, USA<br>${ }^{5}$ Anschutz Medical Campus, University of Colorado, Aurora, CO 80217, USA<br>${ }^{6}$ The Jackson Laboratory for Genomic Medicine, Farmington, CT 06032, USA</p>
<p>December 22, 2023</p>
<h4>Abstract</h4>
<p>Creating knowledge bases and ontologies is a time consuming task that relies on manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrarily complex nested knowledge schemas. Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against an LLM to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for matched elements. We present examples of applying SPIRES in different domains, including extraction of food recipes, multi-species cellular signaling pathways, disease treatments, multi-step drug mechanisms, and chemical to disease orange relationships. Current SPIRES accuracy is comparable to the mid-range of existing Relation Extraction (RE) methods, but greatly surpasses an LLM's native capability of grounding entities with unique identifiers. SPIRES has the advantage of easy customization, flexibility, and, crucially, the ability to perform new tasks in the absence of any new training data. This method supports a general strategy of leveraging the language interpreting capabilities of LLMs to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the LLM. SPIRES is available as part of the open source OntoGPT package: https://github.com/ monarch-initiative/ontogpt. Contact: jhc@lbl.gov</p>
<h1>1 Introduction</h1>
<p>Knowledge Bases and ontologies (here collectively referred to as KBs) encode domain knowledge in a structure that is amenable to precise querying and reasoning. General purpose KBs such as Wikidata [1] contain broad contextual knowledge, and are used for a wide variety of tasks, such as integrative analyses of otherwise disconnected data and enrichment of web applications (for example, a recipe website may want to dynamically query Wikidata to retrieve information about ingredients or country of origin). In the life sciences, KBs such as the Gene Ontology (GO) [2] and the Reactome biological pathway KB [3] contain extensive curated knowledge detailing cellular mechanisms that involve interacting gene products and molecules. These domain-specific KBs are used for tasks such as interpreting high-throughput experimental data. All KBs, whether general purpose or domain-specific, owe their existence to curation, often a concerted effort by human experts.
However, the vast majority of human knowledge is communicated via natural language, with scientific knowledge communicated textually in journal abstracts and articles, which has historically been largely opaque to machines. The latest Natural Language Processing (NLP) techniques making use of Large Language Models (LLMs) have shown great promise in interpreting highly technical language, as demonstrated by orange their performance on question-answering benchmarks [4]. These techniques have known limitations, such as being prone to hallucinations [5] (i.e., generating incorrect statements) and insensitivity to negations [6]. Applications such as clinical decision support require precision and reliability not yet demonstrated by LMs of any size, though recent demonstrations offer promise [7, 8, 9, 10].
If instead of passing the unfiltered results of LLM queries to users, we use LLMs to build KBs using NLP at the time of KB construction, then we can assist manual knowledge curation and acquisition while validating facts prior to insertion into the KB. NLP can assist KB construction at multiple stages. Literature triage aids selection of relevant texts to curate; Named Entity Recognition (NER) can identify textual spans mentioning relevant things or concepts such as genes or ingredients; grounding maps these spans to persistent identifiers in databases or ontologies; Relation Extraction (RE) connects named entities via predicates such as 'causes' into simple triple statements. Deep Learning methods such as autoregressive LMs [11] have made considerable gains in all these areas. The first generation of these methods relied heavily on task-specific training data, but the latest generation of LLMs such as GPT-3 and GPT-4 are able to generalize and perform zero-shot or few-shot learning on these tasks by reframing them as prompt-completion tasks [12].
Most KBs are built upon detailed knowledge schemas which prove challenging to populate. Schemas describe the forms in which data should be structured within a domain. For example, a food recipe KB may break a recipe down into a sequence of dependent steps, where each step is a complex knowledge structure involving an action, utensils, and quantified inputs and outputs. Inputs and outputs might be a tuple of a food type plus a state (e.g. cooked) (Figure 1). Ontologies such as FOODON [7] may be used to provide identifiers for any named entities. Similarly, a biological pathway database might break down a cellular program into subprocesses and further into individual steps, each step involving actions, subcellular locations, and inputs and outputs with activation states and stoichiometry. Adapting existing pipelines to custom KB schemas requires considerable engineering.
A schema provides a structure for data. For example, the recipe schema in Figure 1 could be used in a recipe database, with each record instantiating the recipe class, with additional linked records instantiating contained classes, e.g. individual ingredients or steps. Figure 2 shows an example of an instantiated schema class, rendered using YAML [13] syntax.
Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), an automated approach for population of custom schemas and ontology models. The objective of SPIRES is to generate an instance (i.e., an object) from a text, where that instance has a collection of attribute-value associations. Each value is either a primitive (e.g. string, number, or identifier) or another inlined instance (Figure 2). SPIRES integrates the flexibility of LLMs with the reliability of publicly-available databases and ontologies (Figure 3). This strategy allows SPIRES to fill out schemas with linked data while bypassing a need for training examples. A major advantage of SPIRES over more traditional RE is its ability to populate schemas that exhibit nesting, in which complex classes may have attributes whose ranges are themselves complex classes. SPIRES also makes use of a flexible grounding approach that can leverage over a thousand ontologies in the OntoPortal Alliance [17], as well as biomedical lexical grounders such as Gilda [18] and OGER [19]. This grounding method offers far more consistent mapping to unique identifiers than hallucination-prone LLM querying alone.</p>
<h2>2 System and Methods</h2>
<p>In SPIRES, A knowledge schema is a structure for constraining the shape of instances for a given domain. A schema is a collection of classes or templates, each of which can be instantiated by instances. Each class has a collection of attribute constraints, which control the attribute-value pairs that can be associated with each instance. The range of an</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example schema. Boxes denote classes and arrows denote attributes whose range are classes (compound attributes). Crows feet above boxes denote multivalued attributes. Attributes whose ranges are primitives or value sets are shown within each box. Here, the top level container class "Recipe" is composed of a label, description, categories, steps, and ingredients. Steps and ingredients are further decomposed into food items, quantities, etc.
attribute specifies the allowed value or values. A range can be either (1) a primitive type such as a string or number; (2) a class; or (3) an enumeration of permissible value tokens (e.g., an enumeration of days of the week may include "Monday", "Tuesday", and so on). Attributes also have cardinality, specifying the minimum and maximum number of values each instance can take. Additionally, each schema element can have arbitrary metadata associated with it.
Formally, a schema $S$ consists of $n$ classes:</p>
<p>$$
\text { Classes }(S)=\left{c_{1}, \cdots, c_{n}\right}
$$</p>
<p>Classes correspond to the kinds of entities present in a database (e.g. in a recipe database, this would include recipes, as well as ingredients and steps; see example in Figure 1).
Each class $c_{i}$ has an ordered list of attributes:</p>
<p>$$
\text { Attributes }\left(c_{i}\right)=\left{c_{i} a_{1}, \cdots, c_{i} a_{m}\right}
$$</p>
<p>Instances of $c_{i}$ may have values specified for each of these attributes. An attribute $a$ can have associated properties:</p>
<ul>
<li>$\operatorname{Name}(a)$ : the name of the attribute; for example, "summary" or "steps".</li>
<li>Multivalued $(a)={$ True, False $}$, indicating whether the value of a is a list, or single-valued. A recipe might have a single-valued attribute for the name of the recipe, and a multivalued attribute for the steps.</li>
</ul>
<p>On medium heat melt the butter and sautee the onion and bell peppers.
Add the hamburger meat and cook until meat is well done...
Ingredients: 1 small onion, 2 bell peppers, 2 tablespoons garlic powder...</p>
<div class="codehilite"><pre><span></span><code><span class="nl">label</span><span class="p">:</span><span class="w"> </span><span class="n">Simple</span><span class="w"> </span><span class="n">Spaghetti</span>
<span class="nl">description</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">tomato</span><span class="w"> </span><span class="n">sauce</span><span class="w"> </span><span class="n">spaghetti</span><span class="w"> </span><span class="n">dish</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">hamburger</span><span class="w"> </span><span class="n">meat</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">vegetables</span><span class="p">.</span>
<span class="nl">category</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nl">dbpedia</span><span class="p">:</span><span class="n">Main_course</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">dbpedia</span><span class="w"> </span><span class="n">ontology</span>
<span class="o">-</span><span class="w"> </span><span class="nl">dbpedia</span><span class="p">:</span><span class="n">Italian_cuisine</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">dbpedia</span><span class="w"> </span><span class="n">ontology</span>
<span class="nl">ingredients</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nl">food_item</span><span class="p">:</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03301704</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">onion</span><span class="w"> </span><span class="p">(</span><span class="n">whole</span><span class="p">,</span><span class="w"> </span><span class="n">raw</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="nl">foodity</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="o">-</span><span class="w"> </span><span class="nl">food_item</span><span class="p">:</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">00003485</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">sweet</span><span class="w"> </span><span class="n">red</span><span class="w"> </span><span class="n">bell</span><span class="w"> </span><span class="n">pepper</span><span class="w"> </span><span class="p">(</span><span class="n">whole</span><span class="p">)</span>
<span class="nl">quantity</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span>
<span class="o">-</span><span class="w"> </span><span class="nl">food_item</span><span class="p">:</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03301844</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">garlic</span><span class="w"> </span><span class="n">powder</span>
<span class="nl">quantity</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span>
<span class="nl">unit</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;[tbs_us]&quot;</span>
<span class="o">-</span><span class="w"> </span><span class="nl">food_item</span><span class="p">:</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03310351</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">UCUM</span><span class="w"> </span><span class="n">standard</span>
<span class="nl">quantity</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span>
<span class="nl">unit</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;[tbs_us]&quot;</span>
<span class="o">-</span><span class="w"> </span><span class="nl">food_item</span><span class="p">:</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">00001649</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">butter</span>
<span class="nl">quantity</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="nl">unit</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;[tbs_us]&quot;</span>
<span class="o">-</span><span class="w"> </span>
<span class="nl">steps</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="k">action</span><span class="err">:</span><span class="w"> </span><span class="n">chop</span>
<span class="nl">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03301704</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">onion</span><span class="w"> </span><span class="p">(</span><span class="n">whole</span><span class="p">,</span><span class="w"> </span><span class="n">raw</span><span class="p">)</span>
<span class="w">    </span><span class="nl">outputs</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="nl">_</span><span class="p">:</span><span class="n">ChoppedOnion</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="p">(</span><span class="k">no</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">ontology</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="k">action</span><span class="err">:</span><span class="w"> </span><span class="n">chop</span>
<span class="nl">inputs</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">00003485</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">sweet</span><span class="w"> </span><span class="n">red</span><span class="w"> </span><span class="n">bell</span><span class="w"> </span><span class="n">pepper</span><span class="w"> </span><span class="p">(</span><span class="n">whole</span><span class="p">)</span>
<span class="w">    </span><span class="nl">outputs</span><span class="p">:</span>
<span class="w">        </span><span class="o">-</span><span class="w"> </span><span class="nl">_</span><span class="p">:</span><span class="n">ChoppedBellPepper</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="p">(</span><span class="k">no</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">ontology</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span>
<span class="o">-</span><span class="w"> </span><span class="k">action</span><span class="err">:</span><span class="w"> </span><span class="k">add</span>
<span class="nl">inputs</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03301217</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">tomato</span><span class="w"> </span><span class="n">sauce</span>
<span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">00002221</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">salt</span><span class="w"> </span><span class="n">product</span>
<span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">00001649</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">black</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">white</span><span class="w"> </span><span class="n">pepper</span><span class="w"> </span><span class="n">product</span>
<span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03301644</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">garlic</span><span class="w"> </span><span class="n">powder</span>
<span class="w">    </span><span class="nl">outputs</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nl">FOODON</span><span class="p">:</span><span class="mi">03304014</span><span class="w"> </span><span class="err">##</span><span class="w"> </span><span class="n">spaghetti</span><span class="w"> </span><span class="n">sauce</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">meat</span>
</code></pre></div>

<h1>Figure 2: Example of a portion of text to parse and a corresponding instantiation of the recipe schema from Figure 1, using YAML syntax. Input text is truncated for brevity; the full input is available at https://github.com/ monarch-initiative/ontogpt/blob/main/tests/input/cases/recipe-spaghetti.txt. In each attributevalue pair, the attribute is shown in bold, followed by a colon and then the value or values. For multivalued attributes, each list element value is indicated with a hyphen at the beginning of the line. Terminal elements that are value sets from ontologies and standards such as FOODON [14], UCUM [15], and DBPedia [16] are shown here with their human-readable labels in blue after the double-hash comment symbol. Dynamic elements are indicated via RDF blank node syntax (e.g. _: ChoppedOnion does not correspond to a named entity and serves as a placeholder.</h1>
<ul>
<li>$\operatorname{Identifier}(a)={$ True, False $}$, indicating whether $a$ is a persistent identifier for instances, such as the FOODON identifiers in Figure 2.</li>
<li>$\operatorname{Prompt}(a)=$ string, which is a user-specified custom prompt for that attribute.</li>
<li>Range $(a)$ : the allowable values for this attribute; this can be a class $c$ in $S$, or a primitive type such as string or number, or a value set (see below). In Figure 1, the range of the ingredients attribute is Ingredient, and the range of the $i d$ attribute is a string.</li>
<li>ValueSets(c): a list of atomic values from which values of $a$ can be drawn, where a value set is either an extensional list (fixed/static) or intensional (specified by ontology query). For example, a value set for a food element in an ingredient may be drawn from the food branch of the Food Ontology.</li>
<li>Inlined $(a)={$ True, False $}$, indicating, when the range is a class, if the object should be nested/embedded, or passed by reference.</li>
</ul>
<p>Additionally, a class $c$ can include a set of identifier constraints:</p>
<p>$$
I D S p a c e s\left(c_{i}\right)=\left{\text { prefix }_{i}, \cdots, \text { prefix }\right}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Overview of the SPIRES approach. A knowledge schema and text containing instances defined in the schema are processed by OntoGPT, yielding a query for GPT-3 or newer, accessed through the OpenAI API. OntoGPT parses the result, grounding extracted instances with specific entries and terms retrieved from queries of databases and ontologies where possible. The final product is a set of structured data (instances and relationship) in the shapes defined by the schema. Icons by user Khoirin from the Noun Project (https://thenounproject.com/besticon/).</p>
<p>The constraint set is a list of strings that are the allowable prefixes that the identifier can take-for example, "WIKIDATA", "MESH", "GO", or "FOODON". The prefixes should come from a standard prefix registry such as BioRegistry [20] to ensure consistency across schemas and projects; SPIRES expects upper-case prefixes.</p>
<h1>2.1 Evaluation of Entity Grounding</h1>
<p>To determine the extent to which SPIRES improves entity grounding relative to prompting alone, we queried two GPT models with sets of ontology term labels with and without our grounding. We selected 100 terms at random from each of three ontologies: the Gene Ontology (GO), the Mouse Developmental Anatomy Ontology (EMAPA), and the MONDO Disease Ontology. The 16k GPT-3.5-turbo (gpt-3.5-turbo-16k) and the newly available GPT-4-turbo (gpt-4-1106-preview) models were each queried with the full term list in a single prompt each along with text requesting corresponding identifiers from the specified ontology (or, for SPIRES, a structured query based on a minimal schema). A match was considered successful for each pair of identifier and label in which the label text was parsed as a single entity, remained unchanged in the output, and matched to the correct identifier. The full evaluation and results are available in a code notebook online ${ }^{1}$.</p>
<h3>2.2 Evaluation Against Chemical Disease Relation Task</h3>
<p>We evaluated SPIRES on the Biocreative Chemical-Disease-Relation task [21]. We used all 500 abstracts of the BC5CDR test set and evaluated against the set of 1066 chemical-induces-disease (CID) triples. For each triple, the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>predicate is fixed, and the subject and object are always identifiers drawn from the Medical Subject Headings (MeSH) vocabulary [22]. Grounding was performed using multiple ontologies beyond MeSH, including three resources for chemical and drug information: Chemical Entities of Biological Interest (ChEBI) [23], DrugBank [24], and MedDRA [25] (See Table S1 for a full list of external resources used for grounding). We used the Translator NodeNormalizer [26] to normalize these to MeSH IDs to permit comparison with the test set. No fine tuning was performed. The training set was used to enhance our mappings of named entity spans to MeSH identifiers; after building this lexicon, the training set was discarded.</p>
<p>We provided SPIRES with a model of chemical to disease (CTD) associations based on the Biolink Model [27]. Biolink extends the simple triple model of associations to include qualifiers on the predicate, subject, and object. Subject and object qualifier information was discarded in this evaluation as extracting these details was not tested for in the original CDR benchmark. Statements with predicate qualifiers of "NOT" were discarded. We configured value sets for MeSH Disease and Chemical entries manually (see the full list of identifiers used to define these sets in Table S2). NER of chemical and disease entities was also evaluated based on ability to identify a corresponding MeSH. We compared two pre-processing approaches: a "chunking" approach in which input documents were processed as separate subsegments (essentially a sliding window approach) and a "no chunking" approach in which the entirety of the test corpus document title and abstract was passed in a prompt. Two OpenAI models were used in these comparisons: gpt-3.5-turbo and gpt-4.</p>
<h1>3 Algorithm</h1>
<p>The SPIRES extraction procedure takes as input (1) a schema $S$, (2) an entry point class $C$, and (3) a text $T$ (Figure 4, top). It returns a structured instance $i$ conforming to $S$, making use of a large language model (LLM) that allows prompt completion, such as GPT-3 and its more recent versions. The procedure is detailed below:
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Flowchart depicting the SPIRES algorithm.</p>
<h1>SPIRES $(S, C, T)$</h1>
<ol>
<li>Generate the prompt: $p=\operatorname{GeneratePrompt}(S, C, T)$</li>
<li>Perform prompt completion: $r=\operatorname{CompletePrompt}(p)$</li>
<li>Parse results and recurse over nested structures: $i u=\operatorname{ParseCompletion}(r, S, C)$</li>
<li>Ground results using ontologies: $i=\operatorname{Ground}(i u, S, C)$</li>
<li>(optional) translation to OWL: ont $=$ TranslateToOWL $(i)$</li>
</ol>
<h3>3.1 Step 1: Generate Prompt</h3>
<p>SPIRES first generates text for a prompt (Figure 4, Generate Prompt) to be provided to the LLM:</p>
<p>$$
\text { GeneratePrompt }(S, C, T)=\text { Instructions }()+ \text { AttributeTemplate }(S, C, T)+\text { TextIntro }()+T+\text { Break }())
$$</p>
<p>Here, the Instructions function returns a piece of text such as "From the text below, extract the following entities in the following format".
The AttributeTemplate function generates a pseudo-YAML structure that is a template for results. For each $a$ in Attributes $(C)$, we write:</p>
<p>$$
\text { Name }(a)+":"+\operatorname{Prompt}(a)+" \backslash n "
$$</p>
<p>If Prompt is undefined for attribute $a$, then it is automatically generated from the name. If Multivalued $(a)$ is True, then the text is preceded with "A semicolon-separated list".
The TextIntro function introduces a break between the template and the input text and is a fixed string "Text:". The Break function is also a fixed string that serves to demarcate the end of the text and is a sequence of three break characters, e.g. "===". As an example, when calling this function when $S=$ RecipeSchema, $C=$ Ingredient, and $T=$ "garlic powder ( 2 tablespoons)", the following prompt would be generated:</p>
<div class="codehilite"><pre><span></span><code>Split the following piece of text into fields in the
following format:
food_item: &lt;the food item&gt;
amount: &lt;the quantity of the ingredient&gt;
</code></pre></div>

<p>Text:
garlic powder (2 tablespoons)
$==$
Note that typical input texts will be larger, except when the function is called recursively.</p>
<h3>3.2 Step 2: Complete the Prompt</h3>
<p>The generated prompt is provided to the LLM using a completion API (Figure 4, Complete Prompt). The nature of the prompt can be adapted for different language models; the OntoGPT implementation defaults to the orange GPT-3.5-turbo model [28] but is compatible with any model capable of delivering a payload orange that conforms to a prompt-specified structure. The intended completion results are a pseudo-YAML structure conforming to the specified template. For example, when passing the example prompt in Step 1, the return payload may be the following text:
food_item: garlic powder
amount: 2 tablespoons</p>
<h3>3.3 Step 3: Completion Result Parsing and Recursive Extraction</h3>
<p>The $\operatorname{ParseCompletion}(r, S, C)$ function returns a pre-grounded instance object $i$ partially conforming to $C$. This step consists of two sub-steps: (1) parsing the pseudo-YAML; (2) recursively calling SPIRES on any inlined attributes.</p>
<p>For the parsing step (Figure 4, Parse Completion), the completion provided by the LLM is not guaranteed to be strict YAML or even conform directly to the specified template, so a heuristic approach is used. The response is separated by newlines into a list. Each line is split on the first instance of a " ;"; the part before is matched against the attribute name, and the part after is the value, which is parsed as detailed below. Attribute matching is case-insensitive. All whitespace is normalized to underscores.</p>
<p>Each value $v$ is parsed according to the range and cardinality of the matched attribute $a$, populating each attribute $a$ of $i$ :</p>
<p>$$
i[a]=\operatorname{ParseValue}(v)
$$</p>
<p>If $a$ is multivalued, then $v$ is first split according to a delimiter (default ";"), and the rules below are applied on each token; otherwise the rules below are applied directly.</p>
<p>Rule 1: If the range is a primitive data type (i.e. string, number, or boolean) then the value is returned as-is.
Rule 2: If the range of the attribute is a class, and the attribute is non-inlined (i.e. a reference) or an enumeration, then the value will be grounded, as specified in Step 4 below.</p>
<p>Rule 3: if the range of the attribute is an inlined class, then SPIRES is called recursively:</p>
<p>$$
\operatorname{SPIRES}(S, \operatorname{Range}(a), v)
$$</p>
<p>This proceeds until a non-inlined class is reached. For example, given the example payload from the previous step, the attribute food item is a reference to an ontology class, so the value "garlic powder" is grounded using the grounding procedure (Step 4). The attribute amount is a reference to an inlined class Quantity, so this will be recursively parsed by calling GeneratePrompt(RecipeSchema, Quantity, "2 tablespoons").</p>
<h1>3.4 Step 4: Grounding and Normalization</h1>
<p>All leaf nodes of the instance tree that correspond to named entities are grounded, i.e., mapped to an identifier in an existing vocabulary, ontology, or database (Figure 4, Ground). Classes representing named entities can each be annotated with one or more vocabularies. Each vocabulary is identified by a unique prefix. For example, in Figure 1, the FoodItem class could be annotated with both FOODON and Wikidata, indicating that grounding on labels can be performed using these vocabularies. Grounding on the string "garlic powder" may then yield FOODON:03301844 when the BioPortal [29] or AgroPortal annotator [30] is used, and WIKIDATA:Q10716334 when a Wikidata normalizer is used. The final results are normalized via validation against identifier constraints for the class. If $I D S p a c e s(c)$ is set, then the prefix of the identifier is checked against the list of valid prefixes. If ValueSets $(c)$ is set, then the value returned must be present in the value set.</p>
<h3>3.5 Step 5: Translation to OWL and Reasoning</h3>
<p>Step 4 produces an instance tree that can be directly represented in JSON or YAML syntax (both of which allow for arbitrary nesting of objects). For some KBs, this is sufficient. Further conversion to an ontological representation in OWL (Figure 4, Translate to OWL), and additional reasoning steps, then support checking for consistency and population of missing axioms. There are multiple methods for translating to OWL, including ROBOT templates [31], DOSDPs [32], and OTTR [33].</p>
<h2>4 Implementation</h2>
<p>We provide an implementation of SPIRES in Python as part of the OntoGPT Python package ${ }^{2}$, which provides both a command line interface (CLI) and a simple web application (Supplementary Figure S1). SPIRES uses LinkML [34] as its Knowledge Schema language. This allows for a full representation of the necessary schema elements while incorporating LinkML's powerful mechanism for specifying static and dynamic value sets. For example, a value set can be constructed as a declarative query of the form "include branches $A, B$ and $C$ from ontology $O_{1}$, excluding sub-branch $D$, and include all of ontology $O_{2}$ ". The LinkML framework also supports converting schemas to LinkML from forms such as SHACL [35], JSON-Schema [36], or SQL Data Definition Language, allowing their use with SPIRES.
SPIRES performs grounding and normalization with the Ontology Access Kit library (OAKlib) [37], which provides interfaces for multiple annotation tools (i.e., those providing links to external vocabularies and ontologies), including the Gilda entity normalization tool [18], the BioPortal annotator [38], and the Ontology Lookup Service [39]. For</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Pre-made schemas. Example use cases are included but are not comprehensive. Note the CTD schema is deliberately restricted to only use the MESH vocabulary for purposes of evaluation. Identifiers refers to all ontologies, value sets, and other unique term sets incorporated in a given schema.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Schema</th>
<th style="text-align: center;">Use Case</th>
<th style="text-align: center;">Identifiers</th>
<th style="text-align: center;">Text inputs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Food Recipes</td>
<td style="text-align: center;">Enforcing consistent structure on stepwise processes</td>
<td style="text-align: center;">FOODON, UO</td>
<td style="text-align: center;">Unstructured and semi-structured recipes</td>
</tr>
<tr>
<td style="text-align: center;">Drug mechanisms</td>
<td style="text-align: center;">Integrating drug descriptions</td>
<td style="text-align: center;">MONDO, <br> CHEBI, <br> MESH</td>
<td style="text-align: center;">Mechanism of Action (MOA) descriptions</td>
</tr>
<tr>
<td style="text-align: center;">Chemical-disease interactions</td>
<td style="text-align: center;">Assembling knowledge graphs of chemical-impacted phenotypes</td>
<td style="text-align: center;">MESH</td>
<td style="text-align: center;">Abstracts describing effects of chemicals on conditions</td>
</tr>
<tr>
<td style="text-align: center;">Metagenomic <br> Samples</td>
<td style="text-align: center;">Standardizing metadata for metagenomics</td>
<td style="text-align: center;">ENVO</td>
<td style="text-align: center;">Descriptions of environmental samples</td>
</tr>
<tr>
<td style="text-align: center;">Mendelian <br> Diseases</td>
<td style="text-align: center;">Extracting disease relationships from literature</td>
<td style="text-align: center;">MONDO, <br> HPO</td>
<td style="text-align: center;">Case studies or descriptions of Mendelian diseases</td>
</tr>
</tbody>
</table>
<p>identifier normalization a number of services can be used, including OntoPortal mappings, with the default being the NCATS Biomedical Translator Node Normalizer [26].
The results of extraction can optionally be further processed using LinkML-OWL [40], which generates an OWL representation of instance data using mappings specified in a LinkML schema. This OWL file can be used as an input to ROBOT [31] to run OWL reasoning to check for logical inconsistencies and perform automated classification.</p>
<h1>4.1 Standard Templates for Multiple Applications</h1>
<p>The SPIRES implementation comes with a growing collection of ready-made schemas for multiple applications. These are primarily life-science focused, for example, deriving a pathway from a Mechanism of Action description in a database such as DrugBank. We also include a schema for food recipes to demonstrate general applicability in domains beyond the environmental and life sciences. Table 1 lists a selection of the pre-made schemas.</p>
<h3>4.2 Extraction of Recipe Ontologies from Websites</h3>
<p>To demonstrate the full functionality of OntoGPT we created a pipeline for extracting recipes from websites and generating an OWL ontology from the combined outputs. Recipes are extracted using the recipe-scrapers Python module ${ }^{3}$. The pipeline takes the output of scraping, concatenates the results into a text, then feeds this to OntoGPT using the recipe template. We use LinkML-OWL to map the recipe template to OWL axioms, such that each recipe is represented as a class defined by its ingredients and its steps. We use ROBOT to extract the relevant parts of the FOODON ontology, and merge this with the extraction results, combined with a manually coded simple recipe classification with defined classes for groupings such as "Meat Recipe" and "Wheat Based Recipe". We use the Elk reasoner [41] to classify the results. The results of this process are highlighted in Figure S3.</p>
<h3>4.3 Entity Grounding</h3>
<p>Grounding entities with ontology terms is part of the core functionality of SPIRES and its value is well demonstrated in a direct comparison with the straightforward approach of directly querying an LLM with term descriptions. If we request the GO term for "integrase activity" we expect the response to include GO:0008907, for example. Of 100 GO terms chosen at random, SPIRES returned the correct identifiers for 98 when using GPT-3.5-turbo and 97 with GPT-4-turbo. Without SPIRES, GPT-3.5-turbo returned just 3 correct identifiers. Though it yielded 100 putative matches, few included correct GO identifiers. This "mass hallucination" may be an artifact of prompting with terms lacking surrounding context. Even so, it may be challenging to determine how much context is sufficient to improve grounding. GPT-4-turbo demonstrated a different challenge by consistently refusing to retrieve identifiers, returning responses such as "As an AI developed before 2023, I do not have real-time access to databases...". For the EMAPA mouse anatomy ontology, SPIRES returned correct identifiers for all 100 term descriptions, while GPT-3.5-turbo repeatedly provided identifiers from the EHDAA2 human anatomy ontology instead. GPT-4-turbo refused to ground EMAPA terms as it had with GO.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Extracted relation examples. All predicates are 'INDUCES'. Sources are PubMed identifiers (PMIDs). PMID 2160002 is "Vasodilation of large and small coronary vessels and hypotension induced by cromakalim and pinacidil" [42]. PMID 19154241 is a case report on lithium therapy [43]. PMID 10327032 is a study of hyperammonemic encephalopathy risks in cancer patients [44].</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Subject</th>
<th style="text-align: center;">Sub. qual.</th>
<th style="text-align: center;">Predicate</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Object qual.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2160002</td>
<td style="text-align: center;">MESH:D019806</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">INDUCES</td>
<td style="text-align: center;">MESH:D014664</td>
<td style="text-align: center;">large and small</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cromakalim</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vasodilation</td>
<td style="text-align: center;">coronary vessels</td>
</tr>
<tr>
<td style="text-align: center;">2160002</td>
<td style="text-align: center;">MESH:D020110</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">INDUCES</td>
<td style="text-align: center;">MESH:D014664</td>
<td style="text-align: center;">large and small</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pinacidil</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vasodilation</td>
<td style="text-align: center;">coronary vessels</td>
</tr>
<tr>
<td style="text-align: center;">19154241</td>
<td style="text-align: center;">MESH:D008094</td>
<td style="text-align: center;">Chronic</td>
<td style="text-align: center;">INDUCES</td>
<td style="text-align: center;">MESH:D006934</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lithium</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hypercalcemia</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10327032</td>
<td style="text-align: center;">MESH:D005472</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">INDUCES</td>
<td style="text-align: center;">MESH:D001927</td>
<td style="text-align: center;">Transient</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fluorouracil</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Brain Diseases</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>MONDO terms posed some surprising difficulty: SPIRES with GPT-3.5-turbo correctly returned 97 of 100 identifiers but SPIRES with GPT-4-turbo returned just 18 correct matches. In some cases, this may have been due to incorrectly parsing entities (e.g., parsing "UV-induced skin damage, susceptibility to" as "skin damage"). As with GO, prompting without SPIRES only returned one correct identifier at most from both GPT-3.5-turbo and GPT-4-turbo.</p>
<h1>4.4 Evaluation on BioCreative Chemical Disease Relation Task</h1>
<p>We evaluated SPIRES on the BioCreative Chemical-Disease-Relation (BC5CDR) task corpus. To demonstrate the zero-shot learning approach, we did not perform any fine tuning using the training set. The training set was used to enhance our mappings of named entity spans to MeSH identifiers and was then discarded. For our CTD schema (see Figure S2), we follow the Biolink Model [27] which extends the simple triple model of associations to include qualifiers on the predicate, subject, and object. This yields finer-grained predictions; for example, SPIRES correctly parses the statements in Table 2. In these cases, SPIRES grounds the drug entity Cromakalim to its corresponding MeSH identifier and extracts its relationship with vasodilation along with a qualifier noting the observation is specific to "large and small coronary vessels", an anatomical entity worthy of further grounding (though this was not explored within the original BC5CDR task). Similarly, the correctly extracted relationship between lithium and hypercalcemia includes the qualifier that the observation pertains to chronic lithium exposure.
When evaluating, we discard subject and object qualifier information, as this is not tested for in the original CDR benchmark. If the predicate qualifier is "NOT" then we discard the whole statement. Note that in the examples in Table 2 , even though we evaluated the first two statements to be a correct interpretation of the abstract, they were counted as false negatives; the corresponding triple was not in the test set, presumably an error of omission.
For SPIRES, we saw initially encouraging results on the BC5CDR task with chunking and GPT-3.5-turbo: we observed an F-score of 41.16, precision of 0.43 , and recall of 0.39 . Using the "no chunking" approach (i.e., no preprocessing of the test document) yielded an F-score of 36.64 (precision 0.63 , recall 0.26 ) with GPT-3.5-turbo and an F-score of 43.80 (precision 0.69 , recall 0.32 ). For NER results alone (i.e., correct grounding against MeSH for chemical and disease entities), see Table S3.
These results place SPIRES just below the average of all 18 teams that participated in the original CDR challenge. We assume all 18 teams used the full training set, whereas with SPIRES there was no task-specific training or fine tuning. For comparison, Luo et al. report an F-score of 44.98 on BC5CDR with their biomedical domain-specific, trained-from-scratch BioGPT model [9]. We note that the best-scoring relation extraction results from the CDR task achieved an impressive score of 0.57 , though with a model trained on a large and carefully engineered set of training examples [45]. SPIRES bypasses this step but may see further improvement with fine-tuned and/or domain-specific LLMs.</p>
<h2>5 Discussion</h2>
<h3>5.1 Comparable Methods</h3>
<p>SPIRES is a well-developed and generally model-agnostic approach for information extraction designed with structured schemas and standardized ontologies in mind. Some recent efforts have made great strides in leveraging the first type of resource, i.e., they address the task of aligning extracted information with pre-defined data models. The fine-tuned</p>
<p>GPT-3-based approach described by Dunn and Dagdelen et al. employs engineered schemas to extract structured relationships from unstructured text in materials chemistry [46]. The authors of the LLMs4OL approach also explored application of LLMs to information extraction, but concluded that the models are not yet sufficiently flexible for ontology-driven needs [47]. We also consider the task of ontology alignment to be related to our efforts; we have found that LLMs can noticeably improve accuracy in ontology alignment [48] and the development of general frameworks such as Agent-OM [49] may further improve the grounding inherent to information extraction.</p>
<h1>5.2 Choosing a Model</h1>
<p>OntoGPT currently supports both select open LLMs and the OpenAI API. Running OntoGPT across a large corpus with OpenAI models may be prohibitively expensive for some users. Additionally, the use of this API involves closed models with inscrutable training data, which may be plagued by biases [50]. Though our experiments here generally concern GPT-3 and 4, the rapid pace of model development will ensure access to progressively more capable (and ideally, more transparent) language models. Smaller LMs such as LLaMA have been shown to outperform models ten times their size [51], and it is possible to fine-tune these into instruction following models [52]. LLMs based on LLaMA2 and adapted for biomedical language, including BioMedGPT-LM [53] and Radiology-Llama2 [54], may complement the grounding provided through SPIRES.</p>
<h3>5.3 Reliability and Hallucinations</h3>
<p>A common problem with LLMs is hallucination of results (producing factually invalid statements that are not consistent with the input text) [5, 50]. We crafted prompts to limit hallucination, asking only for the LM to extract what was found in the text, and keeping default low-creativity settings. On examination we found that hallucinations were generally infrequent, with most false positives and negatives attributable to incorrect relation extraction. It is worth noting that LLM interfaces designed for direct function calling may duplicate some of the data structure enforcement afforded by SPIRES but do not alleviate the issue of hallucination: a model may still improperly associate real or fictional ontology identifiers with extracted entities when queried without aid of our approach.
Some text generation may yield technically correct results. For example, one result extracted from the title "Increased frequency and severity of angio-oedema related to long-term therapy with angiotensin-converting enzyme inhibitor in two patients", yielded "Lisinopril INDUCES angio-oedema". Lisinopril is in fact a subtype of ACE inhibitor, and the extracted association is supported by other literature. However, this more precise statement is not the one that is in the original text. Presumably the LM is substituting the class of drug with a specific member here, but it is unclear why it does it on this occasion. Until there are better methods to control this hallucination and explain justifications for statements in terms of the text and prior knowledge, results from LMs should be carefully validated before being entered into KBs.</p>
<p>SPIRES is a new approach to information extraction that leverages recent advances in large language models to populate complex knowledge schemas from unstructured text. It uses zero-shot learning to identify and extract relevant information from query text, which is then normalized and grounded using existing ontologies and vocabularies. SPIRES requires no model tuning or training data. The approach is customizable, flexible, and can be used to populate knowledge schemas across varied domains. We envision SPIRES being used not in isolation, but rather in synergistic strategies combining human expertise, linguistic pattern recognition, deep learning and classical deductive reasoning approaches. SPIRES is one component of a growing toolkit of methods for transforming noisy, heterogeneous information into actionable knowledge.</p>
<h2>6 Competing interests</h2>
<p>No competing interest is declared.</p>
<h2>7 Acknowledgements</h2>
<h3>7.1 Funding</h3>
<p>This work was supported by the National Institutes of Health National Human Genome Research Institute [RM1 HG010860]; National Institutes of Health Office of the Director [R24 OD011883]; and the Director, Office of Science, Office of Basic Energy Sciences, of the US Department of Energy [DE-AC0205CH11231 to J.H.C., H.H., N.L.H., M.J., S.M., J.T.R, and C.J.M.]. We also gratefully acknowledge Bosch Research for their support of this research project.</p>
<h1>References</h1>
<p>[1] Denny Vrandečić. Wikidata: A new platform for collaborative data collection. In Proceedings of the 21st International Conference on World Wide Web, WWW '12 Companion, pages 1063-1064, New York, NY, USA, 2012. ACM. ISBN 9781450312301. doi:10.1145/2187980.2188242.
[2] The Gene Ontology Consortium. The gene ontology resource: 20 years and still GOing strong. Nucleic Acids Res., 47(D1):D330-D338, January 2019. ISSN 0305-1048, 1362-4962. doi:10.1093/nar/gky1055.
[3] Antonio Fabregat, Steven Jupe, Lisa Matthews, Konstantinos Sidiropoulos, Marc Gillespie, Phani Garapati, Robin Haw, Bijay Jassal, Florian Korninger, Bruce May, Marija Milacic, Corina Duenas Roca, Karen Rothfels, Cristoffer Sevilla, Veronica Shamovsky, Solomon Shorser, Thawfeek Varusai, Guilherme Viteri, Joel Weiser, Guanming Wu, Lincoln Stein, Henning Hermjakob, and Peter D'Eustachio. The reactome pathway knowledgebase. Nucleic Acids Res., 46(D1):D649-D655, January 2018. ISSN 0305-1048, 1362-4962. doi:10.1093/nar/gkx1132.
[4] Samy Ateia and Udo Kruschwitz. Is ChatGPT a biomedical expert? - exploring the Zero-Shot performance of current GPT models in biomedical tasks. In CLEF 2023: Conference and Labs of the Evaluation Forum, June 2023. doi:10.48550/arXiv. 2306.16108.
[5] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. arXiv, February 2022. doi:10.1145/3571730.
[6] Allyson Ettinger. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Trans. Assoc. Comput. Linguist., 8:34-48, December 2020. ISSN 2307-387X. doi:10.1162/tacl_a_00298.
[7] Yanshan Wang, Sunyang Fu, Feichen Shen, Sam Henry, Ozlem Uzuner, and Hongfang Liu. The 2019 n2c2/OHNLP track on clinical semantic textual similarity: Overview. JMIR Med Inform, 8(11):e23375, November 2020. ISSN 2291-9694. doi:10.2196/23375.
[8] Mihir P Khambete, William Su, Juan C Garcia, and Marcus A Badgeley. Quantification of BERT diagnosis generalizability across medical specialties using semantic dataset distance. AMIA Jt Summits Transl Sci Proc, 2021:345-354, May 2021. ISSN 2153-4063. doi:10.1371/journal.pone. 0112774.
[9] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief. Bioinform., 23(6), November 2022. ISSN 1467-5463, 1477-4054. doi:10.1093/bib/bbac409.
[10] Robert M Wachter and Erik Brynjolfsson. Will generative artificial intelligence deliver on its promise in health care? JAMA, November 2023. ISSN 0098-7484, 1538-3598. doi:10.1001/jama.2023.25054.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, June 2017. doi:10.48550/arXiv.1706.03762.
[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are Few-Shot learners. arXiv, May 2020. doi:10.48550/arXiv.2005.14165.
[13] Oren Ben-Kiki, Clark Evans, and Ingy Döt Net. YAML ain't markup language (YAML ${ }^{\mathrm{TM}}$ ) version 1.2.2. https://yaml.org/spec/1.2.2/, 2021. Accessed: 2023-3-28.
[14] Damion M Dooley, Emma J Griffiths, Gurinder S Gosal, Pier L Buttigieg, Robert Hoehndorf, Matthew C Lange, Lynn M Schriml, Fiona S L Brinkman, and William W L Hsiao. FoodOn: a harmonized food ontology to increase global food traceability, quality control and data integration. NPJ Sci Food, 2:23, December 2018. ISSN 2396-8370. doi:10.1038/s41538-018-0032-6.
[15] G Schadow, C J McDonald, J G Suico, U Föhring, and T Tolxdorff. Units of measure in clinical information systems. J. Am. Med. Inform. Assoc., 6(2):151-162, 1999. ISSN 1067-5027. doi:10.1136/jamia.1999.0060151.
[16] Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sören Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. DBpedia - a crystallization point for the web of data. Journal of Web Semantics, 7(3):154-165, September 2009. ISSN 1570-8268. doi:10.1016/j.websem.2009.07.002.
[17] John Graybeal, Clement Jonquet, Nicola Fiore, and Mark A Musen. Adoption of BioPortal's ontology registry software: The emerging OntoPortal community. In RDA P13 2019 - 13th Research Data Alliance Plenary Meeting, April 2019.</p>
<p>[18] Benjamin M Gyori, Charles Tapley Hoyt, and Albert Steppi. Gilda: biomedical entity text normalization with machine-learned disambiguation as a service. Bioinformatics Advances, 2(1), January 2022. doi:10.1093/bioadv/vbac034.
[19] Lenz Furrer, Anna Jancso, Nicola Colic, and Fabio Rinaldi. OGER : hybrid multi-type entity recognition. Journal of Cheminformatics, 11(1), 2019. doi:10.1186/s13321-018-0326-3.
[20] Charles Tapley Hoyt, Meghan Balk, Tiffany J Callahan, Daniel Domingo-Fernández, Melissa A Haendel, Harshad B Hegde, Daniel S Himmelstein, Klas Karis, John Kunze, Tiago Lubiana, Nicolas Matentzoglu, Julie McMurry, Sierra Moxon, Christopher J Mungall, Adriano Rutz, Deepak R Unni, Egon Willighagen, Donald Winston, and Benjamin M Gyori. Unifying the identification of biomedical entities with the bioregistry. Sci Data, 9(1):714, November 2022. ISSN 2052-4463. doi:10.1038/s41597-022-01807-3.
[21] Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016:baw068, May 2016. ISSN 0162-4105, 1758-0463. doi:10.1093/database/baw068.
[22] C E Lipscomb. Medical subject headings (MeSH). Bull. Med. Libr. Assoc., 88(3):265-266, July 2000. ISSN 0025-7338.
[23] Janna Hastings, Gareth Owen, Adriano Dekker, Marcus Ennis, Namrata Kale, Venkatesh Muthukrishnan, Steve Turner, Neil Swainston, Pedro Mendes, and Christoph Steinbeck. ChEBI in 2016: Improved services and an expanding collection of metabolites. Nucleic Acids Res., 44(D1):D1214-9, January 2016. ISSN 0305-1048, 1362-4962. doi:10.1093/nar/gkv1031.
[24] David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, Nazanin Assempour, Ithayavani Iynkkaran, Yifeng Liu, Adam Maciejewski, Nicola Gale, Alex Wilson, Lucy Chin, Ryan Cummings, Diana Le, Allison Pon, Craig Knox, and Michael Wilson. DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic Acids Res., 46(D1):D1074-D1082, January 2018. ISSN 0305-1048. doi:10.1093/nar/gkx1037.
[25] Elliot G Brown, Louise Wood, and Sue Wood. The medical dictionary for regulatory activities (MedDRA). Drug Saf., 20(2):109-117, 1999. ISSN 0114-5916. doi:10.2165/00002018-199920020-00002.
[26] Karamarie Fecho, Anne T Thessen, Sergio E Baranzini, Chris Bizon, Jennifer J Hadlock, Sui Huang, Ryan T Roper, Noel Southall, Casey Ta, Paul B Watkins, Mark D Williams, Hao Xu, William Byrd, Vlado Dančík, Marc P Duby, Michel Dumontier, Gustavo Glusman, Nomi L Harris, Eugene W Hinderer, Greg Hyde, Adam Johs, Andrew Su, Guangrong Qin, Qian Zhu, and Biomedical Data Translator Consortium. Progress toward a universal biomedical data translator. Clin. Transl. Sci., May 2022. ISSN 1752-8054, 1752-8062. doi:10.1111/cts. 13301.
[27] Deepak R Unni, Sierra A T Moxon, Michael Bada, Matthew Brush, Richard Bruskiewich, J Harry Caufield, Paul A Clemons, Vlado Dancik, Michel Dumontier, Karamarie Fecho, Gustavo Glusman, Jennifer J Hadlock, Nomi L Harris, Arpita Joshi, Tim Putman, Guangrong Qin, Stephen A Ramsey, Kent A Shefchek, Harold Solbrig, Karthik Soman, Anne E Thessen, Melissa A Haendel, Chris Bizon, Christopher J Mungall, Liliana Acevedo, Stanley C Ahalt, John Alden, Ahmed Alkanaq, Nada Amin, Ricardo Avila, Jim Balhoff, Sergio E Baranzini, Andrew Baumgartner, William Baumgartner, Basazin Belhu, Mackenzie Brandes, Namdi Brandon, Noel Burtt, William Byrd, Jackson Callaghan, Marco Alvarado Cano, Steven Carrell, Remzi Celebi, James Champion, Zhehuan Chen, Mei-Jan Chen, Lawrence Chung, Kevin Cohen, Tom Conlin, Dan Corkill, Maria Costanzo, Steven Cox, Andrew Crouse, Camerron Crowder, Mary E Crumbley, Cheng Dai, Vlado Dančík, Ricardo De Miranda Azevedo, Eric Deutsch, Jennifer Dougherty, Marc P Duby, Venkata Duvvuri, Stephen Edwards, Vincent Emonet, Nathaniel Fehrmann, Jason Flannick, Aleksandra M Foksinska, Vicki Gardner, Edgar Gatica, Amy Glen, Prateek Goel, Joseph Gormley, Alon Greyber, Perry Haaland, Kristina Hanspers, Kaiwen He, Kaiwen He, Jeff Henrickson, Eugene W Hinderer, Maureen Hoatlin, Andrew Hoffman, Sui Huang, Conrad Huang, Robert Hubal, Kenneth Huellas-Bruskiewicz, Forest B Huls, Lawrence Hunter, Greg Hyde, Tursynay Issabekova, Matthew Jarrell, Lindsay Jenkins, Adam Johs, Jimin Kang, Richa Kanwar, Yaphet Kebede, Keum Joo Kim, Alexandria Kluge, Michael Knowles, Ryan Koesterer, Daniel Korn, David Koslicki, Ashok Krishnamurthy, Lindsey Kvarfordt, Jay Lee, Margaret Leigh, Jason Lin, Zheng Liu, Shaopeng Liu, Chunyu Ma, Andrew Magis, Tarun Mamidi, Meisha Mandal, Michelle Mantilla, Jeffrey Massung, Denise Mauldin, Jason McClelland, Julie McMurry, Philip Mease, Luis Mendoza, Marian Mersmann, Abrar Mesbah, Matthew Might, Kenny Morton, Sandrine Muller, Arun Teja Muluka, John Osborne, Phil Owen, Michael Patton, David B Peden, R Carter Peene, Bria Persaud, Emily Pfaff, Alexander Pico, Elizabeth Pollard, Guthrie Price, Shruti Raj, Jason Reilly, Anders Riutta, Jared Roach, Ryan T Roper, Greg Rosenblatt, Irit Rubin, Sienna Rucka, Nathaniel Rudavsky-Brody, Rayn Sakaguchi, Eugene Santos, Kevin Schaper, Charles P Schmitt, Shepherd Schurman, Erik Scott, Sarah Seitanakis, Priya Sharma, Ilya Shmulevich, Manil Shrestha, Shalki Shrivastava, Meghamala Sinha, Brett Smith, Noel Southall, Nicholas Southern, Lisa</p>
<p>Stillwell, Michael " Michi" Strasser, Andrew I Su, Casey Ta, Anne E Thessen, Jillian Tinglin, Lucas Tonstad, Thi Tran-Nguyen, Alexander Tropsha, Gaurav Vaidya, Luke Veenhuis, Adam Viola, Marcin Grotthuss, Max Wang, Patrick Wang, Paul B Watkins, Rosina Weber, Qi Wei, Chunhua Weng, Jordan Whitlock, Mark D Williams, Andrew Williams, Finn Womack, Erica Wood, Chunlei Wu, Jiwen Kevin Xin, Hao Xu, Colleen Xu, Chase Yakaboski, Yao Yao, Hong Yi, Arif Yilmaz, Marissa Zheng, Xinghua Zhou, Eric Zhou, Qian Zhu, Tom Zisk, and The Biomedical Data Translator Consortium. Biolink model: A universal schema for knowledge graphs in clinical, biomedical, and translational science. Clin. Transl. Sci., June 2022. ISSN 1752-8054, 1752-8062. doi:10.1111/cts. 13302 .
[28] OpenAI. OpenAI API. https://platform.openai.com/docs/models, 2023. Accessed: 2023-3-27.
[29] P L Whetzel, N F Noy, N H Shah, P R Alexander, C Nyulas, T Tudorache, and M A Musen. BioPortal: enhanced functionality via new web services from the national center for biomedical ontology to access and use ontologies in software applications. Nucleic Acids Res., 39(suppl):W541-W545, July 2011. ISSN 0305-1048. doi:10.1093/nar/gkr469.
[30] Clément Jonquet, Anne Toulet, Elizabeth Arnaud, Sophie Aubin, Esther Dzalé Yeumo, Vincent Emonet, John Graybeal, Marie-Angélique Laporte, Mark A Musen, Valeria Pesce, and Pierre Larmande. AgroPortal: A vocabulary and ontology repository for agronomy. Comput. Electron. Agric., 144:126-143, January 2018. ISSN 0168-1699. doi:10.1016/j.compag.2017.10.012.
[31] Rebecca C Jackson, James P Balhoff, Eric Douglass, Nomi L Harris, Christopher J Mungall, and James A Overton. ROBOT: A tool for automating ontology workflows. BMC Bioinformatics, 20(1):407, July 2019. ISSN 1471-2105. doi:10.1186/s12859-019-3002-3.
[32] David Osumi-Sutherland, Melanie Courtot, James P Balhoff, and Christopher Mungall. Dead simple OWL design patterns. J. Biomed. Semantics, 8(1):18, 2017. ISSN 2041-1480. doi:10.1186/s13326-017-0126-0.
[33] Christian Kindermann, Daniel P Lupp, Uli Sattler, and Evgenij Thorstensen. Generating ontologies from templates: A Rule-Based approach for capturing regularity. arXiv, page 13, 2018. doi:10.48550/arXiv.1809.10436.
[34] Sierra Moxon, Harold Solbrig, Deepak Unni, Dazhi Jiao, Richard Bruskiewich, James Balhoff, Gaurav Vaidya, William Duncan, Harshad Hegde, Mark Miller, and Others. The linked data modeling language (LinkML): A General-Purpose data modeling framework grounded in Machine-Readable semantics. In CEUR Workshop Proceedings, volume 3073, pages 148-151, 2021.
[35] Paolo Pareti and George Konstantinidis. A review of SHACL: From data validation to schema reasoning for RDF graphs. In Reasoning Web. Declarative Artificial Intelligence, pages 115-144. Springer International Publishing, 2022. doi:10.1007/978-3-030-95481-9_6.
[36] JSON schema. http://json-schema.org/, 2022. Accessed: 2023-3-28.
[37] Chris Mungall, Harshad, Patrick Kalita, Charles Tapley Hoyt, Sujay Patil, Marcin p Joachimiak, Joe Flack, David Linke, Deepak, Sierra Moxon, Nico Matentzoglu, Vinícius de Souza, Glass, Harry Caufield, Jules Jacobsen, Justin Reese, Nomi Harris, and Shawn Tan. INCATools/ontology-access-kit: v0.2.1. https://github.com/ INCATools/ontology-access-kit, March 2023.
[38] Clement Jonquet, Nigam H Shah, and Mark A Musen. The open biomedical annotator. Summit Transl Bioinform, 2009:56-60, March 2009. ISSN 2153-6430.
[39] Simon Jupp, Tony Burdett, James Malone, Catherine Leroy, Matt Pearce, Julie Mcmurry, and Helen Parkinson. A new ontology lookup service at EMBL-EBI. http://ceur-ws.org/Vol-1546/paper_29.pdf, 2015. Accessed: 2023-1-3.
[40] Chris Mungall, Sujay Patil, and Nomi Harris. linkml/linkml-owl: v0.2.4. https://zenodo.org/record/ 7384531, December 2022.
[41] Yevgeny Kazakov and Pavel Klinov. Advancing ELK: Not only performance matters. In Diego Calvanese and Boris Konev, editors, Proceedings of the 28th International Workshop on Description Logics (DL-15). CEUR Workshop Proceedings 2015., 2015.
[42] J F Giudicelli, C D la Rochelle, and A Berdeaux. Effects of cromakalim and pinacidil on large epicardial and small coronary arteries in conscious dogs. J. Pharmacol. Exp. Ther., 255(2):836-842, November 1990. ISSN 0022-3565.
[43] Mian M Rizwan and Nancy D Perrier. Long-term lithium therapy leading to hyperparathyroidism: a case report. Perspect. Psychiatr. Care, 45(1):62-65, January 2009. ISSN 0031-5990, 1744-6163. doi:10.1111/j.17446163.2009.00201.x.</p>
<p>[44] C C Liaw, H M Wang, C H Wang, T S Yang, J S Chen, H K Chang, Y C Lin, S J Liaw, and C T Yeh. Risk of transient hyperammonemic encephalopathy in cancer patients who received continuous infusion of 5-fluorouracil with the complication of dehydration and infection. Anticancer Drugs, 10(3):275-281, March 1999. ISSN 0959-4973. doi:10.1097/00001813-199903000-00004.
[45] Jun Xu, Yonghui Wu, Yaoyun Zhang, Jingqi Wang, Ruiling Liu, Qiang Wei, and Hua Xu. UTH-CCB@BioCreative V CDR task: Identifying chemical-induced disease relations in biomedical text. In Proceedings of the Fifth BioCreative Challenge Evaluation Workshop, pages 254-259, July 2015.
[46] Alexander Dunn, John Dagdelen, Nicholas Walker, Sanghoon Lee, Andrew S Rosen, Gerbrand Ceder, Kristin Persson, and Anubhav Jain. Structured information extraction from complex scientific text with fine-tuned large language models. arXiv, December 2022. doi:10.48550/arXiv.2212.05238.
[47] Hamed Babaei Giglou, Jennifer D'Souza, and Sören Auer. LLMs4OL: Large language models for ontology learning. In The Semantic Web - ISWC 2023, pages 408-427. Springer Nature Switzerland, 2023. doi:10.1007/978-3-031-47240-4_22.
[48] Nicolas Matentzoglu, J Harry Caufield, Harshad B Hegde, Justin T Reese, Sierra Moxon, Hyeongsik Kim, Nomi L Harris, Melissa A Haendel, and Christopher J Mungall. MapperGPT: Large language models for linking and mapping entities. arXiv, October 2023. doi:10.48550/arXiv. 2310.03666.
[49] Zhangcheng Qiang, Weiqing Wang, and Kerry Taylor. Agent-OM: Leveraging large language models for ontology matching. arXiv, December 2023. doi:10.48550/arXiv.2312.00326.
[50] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi:10.1145/3442188.3445922. URL https://doi.org/10. $1145 / 3442188.3445922$.
[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv, 2023. doi:10.48550/arXiv. 2302.13971.
[52] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv, 2023. doi:10.48550/arXiv. 2303.16199.
[53] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. BioMedGPT: Open multimodal generative pre-trained transformer for BioMedicine. arXiv, August 2023. doi:10.48550/arXiv.2308.09442.
[54] Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Jie Luo, Cheng Chen, Sekeun Kim, Jiang Hu, Haixing Dai, Lin Zhao, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Tianming Liu, Quanzheng Li, and Xiang Li. Radiology-Llama2: Best-in-Class large language model for radiology. arXiv, August 2023. doi:10.48550/arXiv. 2309.06419.
[55] Mark A Musen and Protégé Team. The protégé project: A look back and a look forward. AI Matters, 1(4):4-12, June 2015. ISSN 2372-3483. doi:10.1145/2757001.2757003.
[56] Nicholas Sioutos, Sherri de Coronado, Margaret W Haber, Frank W Hartel, Wen-Ling Shaiu, and Lawrence W Wright. NCI thesaurus: a semantic model integrating cancer-related clinical and molecular information. J. Biomed. Inform., 40(1):30-43, February 2007. ISSN 1532-0464, 1532-0480. doi:10.1016/j.jbi.2006.02.013.
[57] Christopher J Mungall, Julie A McMurry, Sebastian Köhler, James P Balhoff, Charles Borromeo, Matthew Brush, Seth Carbon, Tom Conlin, Nathan Dunn, Mark Engelstad, Erin Foster, J P Gourdine, Julius O B Jacobsen, Dan Keith, Bryan Laraway, Suzanna E Lewis, Jeremy NguyenXuan, Kent Shefchek, Nicole Vasilevsky, Zhou Yuan, Nicole Washington, Harry Hochheiser, Tudor Groza, Damian Smedley, Peter N Robinson, and Melissa A Haendel. The monarch initiative: an integrative data and analytic platform connecting phenotypes to genotypes across species. Nucleic Acids Res., 45(D1):D712-D722, January 2017. ISSN 0305-1048, 1362-4962. doi:10.1093/nar/gkw1128.
[58] Sebastian Köhler, Michael Gargano, Nicolas Matentzoglu, Leigh C Carmody, David Lewis-Smith, Nicole A Vasilevsky, Daniel Danis, Ganna Balagura, Gareth Baynam, Amy M Brower, Tiffany J Callahan, Christopher G Chute, Johanna L Est, Peter D Galer, Shiva Ganesan, Matthias Griese, Matthias Haimel, Julia Pazmandi, Marc Hanauer, Nomi L Harris, Michael J Hartnett, Maximilian Hastreiter, Fabian Hauck, Yongqun He, Tim Jeske, Hugh Kearney, Gerhard Kindle, Christoph Klein, Katrin Knoflach, Roland Krause, David Lagorce, Julie A McMurry, Jillian A Miller, Monica C Munoz-Torres, Rebecca L Peters, Christina K Rapp, Ana M Rath, Shahmir A</p>
<p>Rind, Avi Z Rosenberg, Michael M Segal, Markus G Seidel, Damian Smedley, Tomer Talmy, Yarlalu Thomas, Samuel A Wiafe, Julie Xian, Zafer Yüksel, Ingo Helbig, Christopher J Mungall, Melissa A Haendel, and Peter N Robinson. The human phenotype ontology in 2021. Nucleic Acids Res., 49(D1):D1207-D1217, January 2021. ISSN 0305-1048, 1362-4962. doi:10.1093/nar/gkaa1043.
[59] Lynn M Schriml, Elvira Mitraka, James Munro, Becky Tauber, Mike Schor, Lance Nickle, Victor Felix, Linda Jeng, Cynthia Bearer, Richard Lichenstein, Katharine Bisordi, Nicole Campion, Brooke Hyman, David Kurland, Connor Patrick Oates, Siobhan Kibbey, Poorna Sreekumar, Chris Le, Michelle Giglio, and Carol Greene. Human disease ontology 2018 update: classification, content and workflow expansion. Nucleic Acids Res., 47(D1): D955-D962, January 2019. ISSN 0305-1048, 1362-4962. doi:10.1093/nar/gky1032.</p>
<h1>8 Supplementary Data</h1>
<h2>a</h2>
<h2>Select Schema: gocam.GoCamAnnotations</h2>
<p>Title: $\beta$-Catenin Is Required for the cGAS/STING Signaling Pathway but Antagonized by the Herpes Simplex Virus 1 US3 Protein Text:
The cGAS/STING-mediated DNA-sensing signaling pathway is crucial for interferon (IFN) production and host antiviral responses. Herpes simplex virus I (HSV-1) is a DNA virus that has evolved multiple strategies to evade host immune responses. Here, we demonstrate that the highly conserved $\beta$-catenin protein in the Wnt signaling pathway is an important factor to enhance the transcription of type I interferon (IFN-I) in the cGAS/STING signaling pathway, and the production of IFN-I mediated by $\beta$-catenin was antagonized by HSV-1 US3 protein via its kinase activity. Infection by US3-deficienct HSV-1 and its kinase-dead variants failed to downregulate IFN-I and IFN-stimulated gene (ISG) production induced by $\beta$-catenin. Consistent with this, absence of $\beta$-catenin enhanced the replication of US3-deficienct HSV-1, but not wild-type HSV-1. The underlying mechanism was the interaction of US3 with $\beta$-catenin and its hyperphosphorylation of $\beta$-catenin at Thr556 to block its nuclear translocation. For the first time, HSV-1 US3 has been shown to inhibit IFN-I production</p>
<h2>Submit Query</h2>
<p>Powered by OntoGPT</p>
<h2>b</h2>
<h2>Results</h2>
<h2>genes</h2>
<ul>
<li>item: 1:
$\beta$-catenin HGNC:2514</li>
<li>item: 2:</li>
</ul>
<p>US3 HGNC:10420</p>
<ul>
<li>item: 3:</li>
</ul>
<p>IFN HGNC:5417</p>
<ul>
<li>item: 4:</li>
</ul>
<p>ISG</p>
<h2>organisms</h2>
<ul>
<li>item: 1:</li>
</ul>
<p>HSV-1 NCBITaxon: 10298</p>
<h2>gene_organisms</h2>
<ul>
<li>item: 1:</li>
<li>gene:
$\beta$-catenin HGNC:2514</li>
<li>organism:</li>
</ul>
<p>HSV-1 NCBITaxon:10298</p>
<ul>
<li>item: 2:</li>
<li>gene:</li>
</ul>
<p>US3 HGNC:10420</p>
<ul>
<li>organism:</li>
</ul>
<p>HSV-1 NCBITaxon:10298</p>
<h2>activities</h2>
<ul>
<li>item: 1:
transcription GO:0006351
Figure S1: Screenshot of web-ontogpt. (a) Form entry page, allowing selection of schema, plus input text. (b) Sample of results as structured object rendered as nested HTML. Note that both input text and results are truncated for brevity.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure S2: Chemical to Disease (CTD) schema (available from https://w3id.org/ontogpt/ctd).</p>
<p>Table S1: Resources used for grounding during evaluation of SPIRES with relations in the BC5CDR test corpus. These resources were used for initial annotation and are subsequently normalized to MeSH. Annotations from the Gilda text entity normalization tool are retrieved through its API (http://grounding.indra.bio/apidocs) using the Ontology Access Kit.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity type</th>
<th style="text-align: left;">Resource</th>
<th style="text-align: left;">Prefix</th>
<th style="text-align: left;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chemical</td>
<td style="text-align: left;">Medical Subject Headings 2022</td>
<td style="text-align: left;">MESH</td>
<td style="text-align: left;">$[22]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Chemical Entities of Biological Interest</td>
<td style="text-align: left;">CHEBI</td>
<td style="text-align: left;">$[23]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">National Cancer Institute Thesaurus</td>
<td style="text-align: left;">NCIT</td>
<td style="text-align: left;">$[56]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Mapping of Drug Names and MeSH 2022</td>
<td style="text-align: left;">MDM</td>
<td style="text-align: left;">$[22]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">DrugBank</td>
<td style="text-align: left;">DRUGBANK</td>
<td style="text-align: left;">$[24]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Gilda</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">$[18]$</td>
</tr>
<tr>
<td style="text-align: left;">Disease</td>
<td style="text-align: left;">Medical Subject Headings 2022</td>
<td style="text-align: left;">MESH</td>
<td style="text-align: left;">$[22]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Mondo Disease Ontology</td>
<td style="text-align: left;">MONDO</td>
<td style="text-align: left;">$[57]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Human Phenotype Ontology</td>
<td style="text-align: left;">HP</td>
<td style="text-align: left;">$[58]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">National Cancer Institute Thesaurus</td>
<td style="text-align: left;">NCIT</td>
<td style="text-align: left;">$[56]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Human Disease Ontology</td>
<td style="text-align: left;">DOID</td>
<td style="text-align: left;">$[59]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical Dictionary for Regulatory Activities</td>
<td style="text-align: left;">MEDDRA</td>
<td style="text-align: left;">$[25]$</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure S3: Screenshot of extracted recipes in a merged OWL file from the Protege ontology editor [55]. The "Simple Spaghetti" recipe is correctly classified under MeatRecipe, due to the presence of an ingredient that is classified as a meat-based product in FOODON. The right hand panel shows OWL logical axioms for the recipe, including its ingredients, and the steps involved.</p>
<p>Table S2: MeSH identifiers used to define value sets during evaluation of SPIRES with relations in the BC5CDR test corpus. All identifiers in this table were treated as root nodes of a hierarchy, i.e., the value sets include all child MeSH terms.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity type</th>
<th style="text-align: center;">MeSH identifier</th>
<th style="text-align: center;">MeSH term</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Chemical</td>
<td style="text-align: center;">D602</td>
<td style="text-align: center;">Amino Acids, Peptides, and Proteins</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D1685</td>
<td style="text-align: center;">Biological Factors</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D2241</td>
<td style="text-align: center;">Carbohydrates</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D4364</td>
<td style="text-align: center;">Pharmaceutical Preparations</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D6571</td>
<td style="text-align: center;">Heterocyclic Compounds</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D7287</td>
<td style="text-align: center;">Inorganic Chemicals</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D8055</td>
<td style="text-align: center;">Lipids</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D9706</td>
<td style="text-align: center;">Nucleic Acids, Nucleotides, and Nucleosides</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D9930</td>
<td style="text-align: center;">Organic Chemicals</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D11083</td>
<td style="text-align: center;">Polycyclic Compounds</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D13812</td>
<td style="text-align: center;">Therapeutics</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D19602</td>
<td style="text-align: center;">Food and Beverages</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D45424</td>
<td style="text-align: center;">Complex Mixtures</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D45762</td>
<td style="text-align: center;">Enzymes and Coenzymes</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D46911</td>
<td style="text-align: center;">Macromolecular Substances</td>
</tr>
<tr>
<td style="text-align: center;">Disease</td>
<td style="text-align: center;">D001423</td>
<td style="text-align: center;">Bacterial Infections and Mycoses</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D001523</td>
<td style="text-align: center;">Mental Disorders</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D002318</td>
<td style="text-align: center;">Cardiovascular Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D002943</td>
<td style="text-align: center;">Circulatory and Respiratory Physiological Phenomena</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D004066</td>
<td style="text-align: center;">Digestive System Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D004700</td>
<td style="text-align: center;">Endocrine System Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D005128</td>
<td style="text-align: center;">Eye Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D005261</td>
<td style="text-align: center;">Female Urogenital Diseases and Pregnancy Complications</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D006425</td>
<td style="text-align: center;">Hemic and Lymphatic Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D007154</td>
<td style="text-align: center;">Immune System Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D007280</td>
<td style="text-align: center;">Disorders of Environmental Origin</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009057</td>
<td style="text-align: center;">Stomatognathic Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009140</td>
<td style="text-align: center;">Musculoskeletal Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009358</td>
<td style="text-align: center;">Congenital, Hereditary, and Neonatal Diseases and Abnormalities</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009369</td>
<td style="text-align: center;">Neoplasms</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009422</td>
<td style="text-align: center;">Nervous System Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009750</td>
<td style="text-align: center;">Nutritional and Metabolic Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D009784</td>
<td style="text-align: center;">Occupational Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D010038</td>
<td style="text-align: center;">Otorhinolaryngologic Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D010272</td>
<td style="text-align: center;">Parasitic Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D012140</td>
<td style="text-align: center;">Respiratory Tract Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D013568</td>
<td style="text-align: center;">Pathological Conditions, Signs and Symptoms</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D014777</td>
<td style="text-align: center;">Virus Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D014947</td>
<td style="text-align: center;">Wounds and Injuries</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D017437</td>
<td style="text-align: center;">Skin and Connective Tissue Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D052801</td>
<td style="text-align: center;">Male Urogenital Diseases</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">D064419</td>
<td style="text-align: center;">Chemically-Induced Disorders</td>
</tr>
</tbody>
</table>
<p>Table S3: Results for named entity recognition evaluation of SPIRES on chemical and disease entities in the BC5CDR corpus. The chunking strategy was not used in this evaluation. Grounding was performed against MeSH only - further accuracy may be afforded by use of alternate ontology annotators such as CHEBI or MONDO for chemical and disease, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity type</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">F-score</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chemical</td>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: left;">69.70</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">0.57</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">73.69</td>
<td style="text-align: left;">0.85</td>
<td style="text-align: left;">0.65</td>
</tr>
<tr>
<td style="text-align: left;">Disease</td>
<td style="text-align: left;">GPT-3.5-turbo</td>
<td style="text-align: left;">61.70</td>
<td style="text-align: left;">0.87</td>
<td style="text-align: left;">0.48</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">69.70</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">0.56</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/hhursev/recipe-scrapers&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
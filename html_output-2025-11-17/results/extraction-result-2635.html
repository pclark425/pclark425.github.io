<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2635 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2635</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2635</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-9c2feda6ec5df0161e2cbeac5e46e6f0ce735424</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9c2feda6ec5df0161e2cbeac5e46e6f0ce735424" target="_blank">Bayesian Optimization for Adaptive Experimental Design: A Review</a></p>
                <p><strong>Paper Venue:</strong> IEEE Access</p>
                <p><strong>Paper TL;DR:</strong> This review considers the application of Bayesian optimisation to experimental design, in comparison to existing Design of Experiments (DOE) methods, and surveys for a range of core issues in experimental design.</p>
                <p><strong>Paper Abstract:</strong> Bayesian optimisation is a statistical method that efficiently models and optimises expensive “black-box” functions. This review considers the application of Bayesian optimisation to experimental design, in comparison to existing Design of Experiments (DOE) methods. Solutions are surveyed for a range of core issues in experimental design including: the incorporation of prior knowledge, high dimensional optimisation, constraints, batch evaluation, multiple objectives, multi-fidelity data, and mixed variable types.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2635.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2635.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based global optimisation framework that fits a surrogate (usually a Gaussian process) to expensive black-box functions and selects new evaluations by maximising an acquisition function that trades off predicted objective value and epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient global optimization of expensive black-box functions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimization (surrogate + acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintains a probabilistic surrogate model (Gaussian process) over the objective f(x) producing a posterior mean μ_t(x) and uncertainty σ_t(x). An acquisition function (e.g., Expected Improvement, Probability of Improvement, GP-UCB, Predictive Entropy Search, Knowledge Gradient) is computed from μ and σ and optimised cheaply to select the next point(s) to evaluate. Extensions handled in the review include multi-objective, constrained, batch/parallel, multi-fidelity, high-dimensional and mixed-type inputs. Key components: GP surrogate (kernel choice, hyperparameters), acquisition function, inner optimisation of acquisition, and optional mechanisms for transfer learning, constraint handling, multi-fidelity modelling and batch construction.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General experimental design and automated discovery (materials design, alloy optimisation, robotics, neuroscience, hyperparameter tuning, simulation-based engineering design).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequentially allocate expensive evaluation budget to input settings that maximise an acquisition function computed from the GP posterior. Variants allocate batches by sequential suppression/penalisation or multi-step lookahead; multi-fidelity variants choose among information sources of different costs and accuracies; budgeted-batch variants select sets under an explicit budget constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitly measured as number of expensive function evaluations and surrogate inference cost (GP kernel inversion: O(n^3) naive, O(n^2) incremental updates); also mentions wall-clock acceleration via GPUs for surrogate and acquisition optimisation.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Varies by acquisition: Expected Improvement (EI) measures expected utility improvement; Probability of Improvement (PI) measures probability of improvement; GP-UCB uses upper confidence bound (μ + κσ) linked to regret bounds; Predictive Entropy Search (PES) and Knowledge Gradient (KG) are information-theoretic / expected-value-of-information approaches (mutual information, expected reduction in entropy or expected increase in value).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Encoded in acquisition functions: EI/PI trade exploration via σ_t(x) and exploitation via μ_t(x); GP-UCB adds an explicit exploration weight κ_t on σ_t(x); PES/KG directly target information gain or expected value of information, providing principled exploration; batch methods combine exploitation-first points with pure-exploration or penalisation to diversify.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch construction methods promote diversity via local penalization, pure-exploration additions, arrays of strategies (multiple GP-UCB instances), multi-strategy ensembles, and strategy-space diversity heuristics to cover different modes of the acquisition landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments / limited evaluation budget (also time and computational resource concerns are discussed); multi-fidelity addresses per-evaluation monetary/compute cost tradeoffs; some methods consider process constraints within batches.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled by selecting acquisition targets under the available budget: sequential design until budget exhausted, budgeted-batch BO (explicit budget-aware batch selection), multi-fidelity selection to spend budget on cheaper approximations where informative, and constrained batch optimisation methods that enforce process constraints during batch composition.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement over current best (f(x^+)) for single-objective; Pareto-dominance and dominated hypervolume for multi-objective; information-theoretic metrics (entropy reduction, expected information gain) for information-directed searches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative claims of sample efficiency (BO minimises number of function evaluations). Reported empirical demonstration of high-dimensional BO up to ~25–34 intrinsic dimensions on real data and up to 50 dimensions on synthetic functions; computational complexity of GP inference O(n^3) (naive) reducible to O(n^2) incrementally. No unified numeric percent-gains reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Classical DOE methods (factorial, central composite, Latin hypercube / space-filling), derivative-free optimisers (COBYLA, DIRECT, evolutionary algorithms ISRES), random or grid search, and standard surrogate-free adaptive sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Review reports BO is far more sample-efficient than classical derivative-free optimisers and fixed space-filling designs for expensive black-box functions (qualitative statement); specific numeric comparisons are not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Described qualitatively as substantial reductions in expensive function evaluations (often requiring 'few' evaluations compared to other optimisers); no single aggregated numeric efficiency figure given.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper discusses tradeoffs: acquisition computation is cheap relative to objective evaluations but surrogate inference (GP inversion) scales cubically with data points; information-theoretic acquisitions (PES, KG) are more computationally expensive but target greater information gain; multi-objective acquisition cost grows rapidly with number of objectives; multi-fidelity methods trade cheaper low-fidelity evaluations against expensive high-fidelity ones to reduce total experimental cost; batch methods trade synchronous parallelism vs potential loss of adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key recommendations: use information-directed acquisitions (PES, KG) when information gain is the principal objective; use EI/GP-UCB when optimisation of the objective is primary; employ multi-fidelity or multi-source models to allocate budget across cheaper and expensive evaluations; use budgeted-batch or process-constrained batch BO when multiple parallel evaluations must be recommended under constraints; and compose specialised BO variants carefully to match problem structure (objectives, constraints, dimensionality, variable types).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2635.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that selects the next evaluation by maximising the expected positive improvement over the current best observed objective value, balancing magnitude and probability of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The application of Bayesian methods for seeking the extremum</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement (EI) acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes EI(x) = (μ_t(x) - f(x^+)) Φ(Z) + σ_t(x) φ(Z), where Z = (μ_t(x) - f(x^+))/σ_t(x). EI accounts for both probability and magnitude of improvement, enabling a natural trade-off between exploitation (high μ) and exploration (high σ). It is cheap to evaluate from the GP posterior and commonly used to allocate evaluation budget sequentially or in multi-point (batch) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General experimental optimisation and automated discovery across materials, simulations, lab experiments, and hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate next evaluation(s) to locations maximising EI; for batch selection, use multi-point EI approximations or heuristics to create diverse batches.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Acquisition evaluation cost is low (closed-form with GP posterior); overall cost dominated by GP inference (see BO entry).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not an information-theoretic metric; measures expected improvement (expected utility) rather than mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Trade-off embedded in balance between μ (exploitation) and σ (exploration) in the EI formula; tuning via added constants (e.g., ξ) can bias towards exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch EI variants and heuristics (multi-point EI, sequential fantasy/predictive samples) are used to create diverse batch selections; not inherently diversity-promoting in single-point EI.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget or batch size (implicit).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Used iteratively until budget exhausted; multi-point EI or sequential fantasies used to select multiple points per iteration when parallel resources are available.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Directly targets improvement over current best (f(x^+)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to PI, GP-UCB, information-theoretic acquisitions; review notes EI commonly used as a pragmatic balance of exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>EI is computationally cheap and often effective; however, it optimises expected objective improvement rather than information gain, so may be less efficient when the goal is information (e.g., model learning, Pareto set identification).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>EI is recommended when the optimisation objective is direct improvement of f and computational cost of acquisition must be kept low; for information-centric tasks, use PES/KG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2635.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predictive Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic acquisition that selects queries expected to maximally reduce the entropy of the location of the global optimum (i.e., maximises expected information gain about argmax f).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Predictive entropy search for Bayesian optimization with unknown constraints</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Predictive Entropy Search (PES) / Parallel PES</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PES formulates acquisition as the expected reduction in the posterior entropy of the argmax (or solution set) conditioned on a hypothetical observation, effectively selecting experiments that maximise mutual information about the optimizer. Parallel/Batch extensions (Parallel PES) approximate joint information gain for multiple evaluations and are used to allocate batches by maximising expected joint entropy reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design where learning the optimum location (or Pareto set) efficiently is important; applicable to constrained and multi-objective settings as extended in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate evaluations to points that maximise expected information gain about the optimizer (entropy reduction); in batch settings, approximate joint entropy reduction across batch elements.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Acquisition computation is more expensive than utility-based acquisitions due to entropy computations and approximations for joint batch gains; overall cost accounted as CPU/GPU time for acquisition and surrogate computations in addition to objective evaluation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Explicit mutual information / expected entropy reduction about argmax f; predictive entropy is the primary metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration is driven explicitly by information gain: points that most reduce uncertainty about the optimizer are preferred, naturally balancing exploration and exploitation based on their information value.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch PES constructs sets whose joint information is maximised, which implicitly promotes diversity among batch points to avoid redundant information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Used under limited expensive-evaluation budgets and for batch-parallel evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects points that maximise information per evaluation; parallel PES uses approximations to handle batch size constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reduction in posterior entropy of the optimizer and eventual identification of high-performing points (indirectly promotes breakthroughs by targeting informative regions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually or empirically in literature against EI, PI, GP-UCB and KG; review notes PES for constrained problems [86,87] and multi-objective extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>PES is computationally heavier but can achieve greater information per expensive evaluation; the review highlights the computational cost vs information gain tradeoff and cites parallel PES for batch settings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use PES when the objective is to rapidly reduce uncertainty about the optimizer (e.g., small expensive budgets), but be mindful of the higher acquisition computational cost; parallel PES is suitable for batch allocation when information-focused batches are required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2635.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition strategy that selects experiments by maximising the expected incremental value (expected improvement in decision quality) of a single evaluation, i.e., the expected increase in the solution’s value after observing the result.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The parallel knowledge gradient method for batch Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Gradient (KG) / Parallel Knowledge Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KG computes, for each candidate evaluation, the expected increase in the objective value of the best decision after obtaining the observation (expected value of information). The parallel KG extends the criterion to batches by estimating joint expected value improvements. KG is inherently decision-theoretic and can incorporate evaluation costs and budget considerations into its expected-utility calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Decision-focused experimental design and optimisation where each evaluation’s expected impact on final decision quality matters (general scientific experiments, materials, engineering).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments to maximise expected value-of-information — choose the point(s) with the largest expected uplift to the best-achievable outcome given the budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Acquisition computation cost (CPU/GPU time) can be substantial since KG requires nested expectations; objective evaluation counts remain the dominant experimental cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected value of information measured as expected increase in final decision utility (expected improvement in best-observed objective after sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>KG balances exploration/exploitation by explicitly valuing prospective information that leads to better final decisions; it may select exploratory points if their expected impact on the final choice is high.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Parallel KG estimates joint expected gains for batches and will select complementary points if they jointly increase expected decision value; implicit diversity arises from the joint-value calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budgets and parallel/batch constraints; KG can be adapted to include per-evaluation costs in its utility.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>KG naturally incorporates expected decision improvement per evaluation and so can be used to prioritise cheaper/higher-value evaluations under budget constraints; the parallel KG handles multi-evaluation batch selection.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected uplift in the utility of the final decision (e.g., improvement over current best), framed as expected value-of-information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to EI, PES and other acquisition methods; the review lists KG [101] among batch/parallel methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>KG is more computationally intensive for acquisition but targets experiments with maximal expected decision impact, trading internal computation for potentially fewer expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommended when explicit decision-theoretic allocation is desired (e.g., limited budget and need to maximise final decision quality); suitable for batch settings via parallel KG approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2635.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-fidelity Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO extension that models multiple information sources of varying cost and fidelity, allocating evaluations across them to reduce total expensive evaluation cost while still optimizing the high-fidelity objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-fidelity Bayesian optimisation with continuous approximations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-fidelity Bayesian Optimization (multi-source)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a joint surrogate over objective outputs at multiple fidelities (e.g., cheap simulations, approximations, and expensive physical experiments). The optimiser selects among sources and inputs by weighing the expected information/value for the high-fidelity objective against per-evaluation cost, often using co-kriging or structured GP kernels to model inter-fidelity correlation. The literature surveyed includes continuous-approximation multi-fidelity BO and other multi-information-source approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Optimization problems where cheaper approximations or simulations are available alongside costly high-fidelity experiments (materials design, simulation-driven engineering, reinforcement learning with simulators and real-world tests).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate budget across fidelities by selecting evaluations that maximise a utility per unit cost (e.g., expected information or expected improvement at high fidelity divided by source cost), sometimes via a knowledge-gradient-like or information ratio criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-evaluation monetary/compute cost of each fidelity (implicit); surrogate inference cost increases with multi-fidelity modelling complexity; overall budget treated in terms of number/cost of high-fidelity evaluations avoided.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Often uses expected improvement on the high-fidelity objective conditioned on multi-fidelity models, or other expected-information/value metrics that account for cross-fidelity correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration can occur at low-fidelity cheap sources to learn broad structure, while exploitation occurs at high-fidelity expensive evaluations to confirm promising candidates; acquisition balances expected contribution to the high-fidelity objective and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via selecting evaluations across different fidelities and input regions; explicit diversity mechanisms not emphasised but multi-source sampling can increase coverage cheaply.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/computational budget across heterogeneous evaluation sources; limited number of high-fidelity experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimiser trades off information/cost across sources by modelling expected benefit per unit cost and choosing cheaper informative evaluations where beneficial, reserving expensive evaluations for high-value confirmations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement in high-fidelity objective discovered with fewer expensive evaluations (often evaluated qualitatively); specific breakthrough metrics depend on application (e.g., material property thresholds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to single-fidelity BO (which uses only expensive evaluations) and to ad-hoc use of proxies or simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Explicitly analyses tradeoff between information (value toward high-fidelity objective) and evaluation cost; referenced works show that using cheaper approximations can accelerate search but require careful modelling of fidelity correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommend modelling cross-fidelity structure and choosing evaluations that maximise expected high-fidelity benefit per unit cost; multi-fidelity BO can substantially reduce costly experiments if low-fidelity sources are sufficiently informative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2635.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-IS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-information-source Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that coordinates sampling across multiple information sources (simulators, experiments, weak/strong labelers) with different costs and accuracies to optimise a target objective efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-information source optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-information-source Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models several correlated information sources and chooses where and which source to query by estimating the contribution of each query to reducing uncertainty or improving objective estimates. Can be cast as multi-fidelity optimisation or as active learning from weak/strong labelers, and uses acquisition criteria that account for cost and informativeness of each source.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Settings with heterogeneous data sources: simulation + experiment, weak vs strong labelers in active learning, multi-resolution simulations in engineering and materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate queries to sources and input settings that maximise expected information or expected improvement per unit cost, leveraging source correlation to prioritise cheap informative evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-source evaluation cost (time, compute, monetary), plus surrogate inference complexity; papers cited discuss trading off these costs explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected information gain about the high-fidelity objective or expected improvement adjusted by source reliability; may use mutual information or expected utility measures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration preferentially uses low-cost/high-uncertainty sources to learn broadly, exploitation uses higher-fidelity sources for confirmation; selection driven by information-per-cost criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit in sampling different sources and input locations; explicit diversity promotion not central but possible via batch criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/computational budget distributed across multiple sources; limited budget for high-fidelity queries.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimiser explicitly models cost of each source and selects queries to maximise information or improvement subject to a total cost/budget constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Identification of high-fidelity optima with fewer expensive queries; no single standard metric reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-source BO, naive use of only high-fidelity experiments or only simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review highlights that multi-source approaches trade per-query cost vs informativeness and can substantially accelerate search when low-cost sources correlate well with the high-fidelity objective; careful modelling of inter-source biases is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Prefer sampling low-cost sources early to map landscape, then allocate budget to high-fidelity queries in promising regions; choose queries by expected information or expected improvement per unit cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2635.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Budgeted-Batch BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Budgeted Batch Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch BO variant that composes batches of experiments under an explicit budget constraint, selecting points to maximise expected return while respecting total cost or size limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Budgeted batch Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Budgeted Batch Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs batches of candidate evaluations taking into account both the budget (e.g., total runtime, monetary cost, number of samples) and the need for diversity/informativeness within the batch. Methods include multi-step lookahead, heuristics, and explicit budget-aware selection heuristics to choose multiple points per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental pipelines where several runs must be scheduled at once (e.g., alloy casting batches, parallel lab assays, parallel simulations) under a global budget.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select a set of points whose joint expected utility (e.g., EI, KG, PES) per unit budget is maximised; often uses sequential suppression/fantasy sampling or penalisation to ensure diversity and spread of information across the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Batch size, per-evaluation cost, and total budget (time or monetary) are considered; acquisition computation for joint batch selection increases with batch size.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on the underlying acquisition used for batch scoring (EI, PES, KG, information-theoretic criteria); often optimises expected joint improvement or information per budget unit.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Batch composition mixes exploitation-focused and exploration-focused points (e.g., first point via GP-UCB, additional points via pure exploration or penalised acquisition) so exploration/exploitation tradeoff is balanced across the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity induced by penalisation of acquisition near already-selected batch points, lookahead/fantasies, or combining different acquisition strategies within a batch (arrays of strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch budget (number of parallel evaluations) and/or monetary/time budget for batch operations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Explicitly enforces budget by limiting batch size or selecting points to maximise expected returns subject to budget, using heuristics or lookahead to account for joint outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Joint expected improvement of the batch or expected number of candidates exceeding target thresholds; not given as standardized numeric metrics in review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Sequential single-point BO, naive parallel random sampling, multi-point EI without budget awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discusses trade-offs between parallel speed-up (running many experiments concurrently) and loss of adaptivity (making choices without intermediate feedback), and computational cost of joint acquisition optimisation vs experimental savings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When batches are required, construct them to mix exploitation and exploration, use penalisation or fantasising to avoid redundant queries, and explicitly account for budget constraints to prioritise highest-value points.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2635.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2635.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Strategy-space diversity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploiting Strategy-Space Diversity for Batch Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that generates batch candidates by combining several distinct batch-construction strategies to promote diverse and complementary experiments within a parallel batch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploiting strategy-space diversity for batch Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Strategy-space Diversity for Batch BO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a batch by running multiple different batch-selection strategies (e.g., GP-UCB seeds, pure-exploration points, penalised-EI points, different surrogate hyperparameters) in parallel to produce a set of complementary candidates. The goal is to span different modes of the acquisition landscape and increase hypothesis diversity in the batch, thereby improving the chance of discovering breakthroughs in parallel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch experimental design where parallel experiments should explore diverse hypotheses to increase discovery probability (materials, lab experiments, A/B tests).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate the available parallel evaluation slots across multiple strategies so that the batch contains complementary points; selection can be based on ranking or ensemble combination of candidate lists from different strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Extra acquisition computation due to running multiple strategies (CPU/GPU time); experimental cost measured by number of parallel evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not necessarily an explicit information-theoretic criterion; relies on ensemble of acquisition heuristics to cover both high-utility and high-uncertainty regions, indirectly increasing expected information gathered by the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Diversity achieved by mixing exploitation-focused strategies (e.g., GP-UCB) with exploration-focused ones (pure exploration or PE) so the batch collectively balances exploitation and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: combining outputs of multiple distinct batch-selection strategies to ensure the batch contains widely different candidate points and avoids redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Parallel batch size constraint (fixed number of concurrent evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Distributes the fixed batch slots across strategies to achieve coverage and diversity; selection may prioritise highest-scoring unique points from each strategy subject to batch size.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Increased probability of finding high-performing points in one batch due to strategy diversity; no single numeric breakthrough metric provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to single-strategy batch methods (e.g., local penalization, multi-point EI) and to sequential single-point BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Trade-off between increased acquisition computation (running multiple strategies) and higher chance of diverse, informative, or breakthrough experiments per batch; recommended when parallel resources exist and exploration diversity is valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Promote strategy diversity in batches to cover multiple hypotheses and reduce risk of redundant parallel experiments; combine exploitation and exploration strategies within the same batch for robust allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Optimization for Adaptive Experimental Design: A Review', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Predictive entropy search for Bayesian optimization with unknown constraints <em>(Rating: 2)</em></li>
                <li>Parallel predictive entropy search for batch global optimization of expensive objective functions <em>(Rating: 2)</em></li>
                <li>The parallel knowledge gradient method for batch Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Multi-fidelity Bayesian optimisation with continuous approximations <em>(Rating: 2)</em></li>
                <li>Multi-information source optimization <em>(Rating: 2)</em></li>
                <li>Budgeted batch Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Exploiting strategy-space diversity for batch Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Bayesian optimization with unknown constraints <em>(Rating: 1)</em></li>
                <li>Parallel Bayesian global optimization of expensive functions <em>(Rating: 1)</em></li>
                <li>Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2635",
    "paper_id": "paper-9c2feda6ec5df0161e2cbeac5e46e6f0ce735424",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BO",
            "name_full": "Bayesian Optimization",
            "brief_description": "A model-based global optimisation framework that fits a surrogate (usually a Gaussian process) to expensive black-box functions and selects new evaluations by maximising an acquisition function that trades off predicted objective value and epistemic uncertainty.",
            "citation_title": "Efficient global optimization of expensive black-box functions",
            "mention_or_use": "use",
            "system_name": "Bayesian Optimization (surrogate + acquisition)",
            "system_description": "Maintains a probabilistic surrogate model (Gaussian process) over the objective f(x) producing a posterior mean μ_t(x) and uncertainty σ_t(x). An acquisition function (e.g., Expected Improvement, Probability of Improvement, GP-UCB, Predictive Entropy Search, Knowledge Gradient) is computed from μ and σ and optimised cheaply to select the next point(s) to evaluate. Extensions handled in the review include multi-objective, constrained, batch/parallel, multi-fidelity, high-dimensional and mixed-type inputs. Key components: GP surrogate (kernel choice, hyperparameters), acquisition function, inner optimisation of acquisition, and optional mechanisms for transfer learning, constraint handling, multi-fidelity modelling and batch construction.",
            "application_domain": "General experimental design and automated discovery (materials design, alloy optimisation, robotics, neuroscience, hyperparameter tuning, simulation-based engineering design).",
            "resource_allocation_strategy": "Sequentially allocate expensive evaluation budget to input settings that maximise an acquisition function computed from the GP posterior. Variants allocate batches by sequential suppression/penalisation or multi-step lookahead; multi-fidelity variants choose among information sources of different costs and accuracies; budgeted-batch variants select sets under an explicit budget constraint.",
            "computational_cost_metric": "Implicitly measured as number of expensive function evaluations and surrogate inference cost (GP kernel inversion: O(n^3) naive, O(n^2) incremental updates); also mentions wall-clock acceleration via GPUs for surrogate and acquisition optimisation.",
            "information_gain_metric": "Varies by acquisition: Expected Improvement (EI) measures expected utility improvement; Probability of Improvement (PI) measures probability of improvement; GP-UCB uses upper confidence bound (μ + κσ) linked to regret bounds; Predictive Entropy Search (PES) and Knowledge Gradient (KG) are information-theoretic / expected-value-of-information approaches (mutual information, expected reduction in entropy or expected increase in value).",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Encoded in acquisition functions: EI/PI trade exploration via σ_t(x) and exploitation via μ_t(x); GP-UCB adds an explicit exploration weight κ_t on σ_t(x); PES/KG directly target information gain or expected value of information, providing principled exploration; batch methods combine exploitation-first points with pure-exploration or penalisation to diversify.",
            "diversity_mechanism": "Batch construction methods promote diversity via local penalization, pure-exploration additions, arrays of strategies (multiple GP-UCB instances), multi-strategy ensembles, and strategy-space diversity heuristics to cover different modes of the acquisition landscape.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experiments / limited evaluation budget (also time and computational resource concerns are discussed); multi-fidelity addresses per-evaluation monetary/compute cost tradeoffs; some methods consider process constraints within batches.",
            "budget_constraint_handling": "Handled by selecting acquisition targets under the available budget: sequential design until budget exhausted, budgeted-batch BO (explicit budget-aware batch selection), multi-fidelity selection to spend budget on cheaper approximations where informative, and constrained batch optimisation methods that enforce process constraints during batch composition.",
            "breakthrough_discovery_metric": "Improvement over current best (f(x^+)) for single-objective; Pareto-dominance and dominated hypervolume for multi-objective; information-theoretic metrics (entropy reduction, expected information gain) for information-directed searches.",
            "performance_metrics": "Qualitative claims of sample efficiency (BO minimises number of function evaluations). Reported empirical demonstration of high-dimensional BO up to ~25–34 intrinsic dimensions on real data and up to 50 dimensions on synthetic functions; computational complexity of GP inference O(n^3) (naive) reducible to O(n^2) incrementally. No unified numeric percent-gains reported in this review.",
            "comparison_baseline": "Classical DOE methods (factorial, central composite, Latin hypercube / space-filling), derivative-free optimisers (COBYLA, DIRECT, evolutionary algorithms ISRES), random or grid search, and standard surrogate-free adaptive sampling.",
            "performance_vs_baseline": "Review reports BO is far more sample-efficient than classical derivative-free optimisers and fixed space-filling designs for expensive black-box functions (qualitative statement); specific numeric comparisons are not provided in the review.",
            "efficiency_gain": "Described qualitatively as substantial reductions in expensive function evaluations (often requiring 'few' evaluations compared to other optimisers); no single aggregated numeric efficiency figure given.",
            "tradeoff_analysis": "The paper discusses tradeoffs: acquisition computation is cheap relative to objective evaluations but surrogate inference (GP inversion) scales cubically with data points; information-theoretic acquisitions (PES, KG) are more computationally expensive but target greater information gain; multi-objective acquisition cost grows rapidly with number of objectives; multi-fidelity methods trade cheaper low-fidelity evaluations against expensive high-fidelity ones to reduce total experimental cost; batch methods trade synchronous parallelism vs potential loss of adaptivity.",
            "optimal_allocation_findings": "Key recommendations: use information-directed acquisitions (PES, KG) when information gain is the principal objective; use EI/GP-UCB when optimisation of the objective is primary; employ multi-fidelity or multi-source models to allocate budget across cheaper and expensive evaluations; use budgeted-batch or process-constrained batch BO when multiple parallel evaluations must be recommended under constraints; and compose specialised BO variants carefully to match problem structure (objectives, constraints, dimensionality, variable types).",
            "uuid": "e2635.0",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "EI",
            "name_full": "Expected Improvement",
            "brief_description": "An acquisition function that selects the next evaluation by maximising the expected positive improvement over the current best observed objective value, balancing magnitude and probability of improvement.",
            "citation_title": "The application of Bayesian methods for seeking the extremum",
            "mention_or_use": "mention",
            "system_name": "Expected Improvement (EI) acquisition",
            "system_description": "Computes EI(x) = (μ_t(x) - f(x^+)) Φ(Z) + σ_t(x) φ(Z), where Z = (μ_t(x) - f(x^+))/σ_t(x). EI accounts for both probability and magnitude of improvement, enabling a natural trade-off between exploitation (high μ) and exploration (high σ). It is cheap to evaluate from the GP posterior and commonly used to allocate evaluation budget sequentially or in multi-point (batch) variants.",
            "application_domain": "General experimental optimisation and automated discovery across materials, simulations, lab experiments, and hyperparameter tuning.",
            "resource_allocation_strategy": "Allocate next evaluation(s) to locations maximising EI; for batch selection, use multi-point EI approximations or heuristics to create diverse batches.",
            "computational_cost_metric": "Acquisition evaluation cost is low (closed-form with GP posterior); overall cost dominated by GP inference (see BO entry).",
            "information_gain_metric": "Not an information-theoretic metric; measures expected improvement (expected utility) rather than mutual information.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Trade-off embedded in balance between μ (exploitation) and σ (exploration) in the EI formula; tuning via added constants (e.g., ξ) can bias towards exploration.",
            "diversity_mechanism": "Batch EI variants and heuristics (multi-point EI, sequential fantasy/predictive samples) are used to create diverse batch selections; not inherently diversity-promoting in single-point EI.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed evaluation budget or batch size (implicit).",
            "budget_constraint_handling": "Used iteratively until budget exhausted; multi-point EI or sequential fantasies used to select multiple points per iteration when parallel resources are available.",
            "breakthrough_discovery_metric": "Directly targets improvement over current best (f(x^+)).",
            "performance_metrics": "",
            "comparison_baseline": "Compared in literature to PI, GP-UCB, information-theoretic acquisitions; review notes EI commonly used as a pragmatic balance of exploration/exploitation.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "EI is computationally cheap and often effective; however, it optimises expected objective improvement rather than information gain, so may be less efficient when the goal is information (e.g., model learning, Pareto set identification).",
            "optimal_allocation_findings": "EI is recommended when the optimisation objective is direct improvement of f and computational cost of acquisition must be kept low; for information-centric tasks, use PES/KG.",
            "uuid": "e2635.1",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "PES",
            "name_full": "Predictive Entropy Search",
            "brief_description": "An information-theoretic acquisition that selects queries expected to maximally reduce the entropy of the location of the global optimum (i.e., maximises expected information gain about argmax f).",
            "citation_title": "Predictive entropy search for Bayesian optimization with unknown constraints",
            "mention_or_use": "mention",
            "system_name": "Predictive Entropy Search (PES) / Parallel PES",
            "system_description": "PES formulates acquisition as the expected reduction in the posterior entropy of the argmax (or solution set) conditioned on a hypothetical observation, effectively selecting experiments that maximise mutual information about the optimizer. Parallel/Batch extensions (Parallel PES) approximate joint information gain for multiple evaluations and are used to allocate batches by maximising expected joint entropy reduction.",
            "application_domain": "Experimental design where learning the optimum location (or Pareto set) efficiently is important; applicable to constrained and multi-objective settings as extended in the literature.",
            "resource_allocation_strategy": "Allocate evaluations to points that maximise expected information gain about the optimizer (entropy reduction); in batch settings, approximate joint entropy reduction across batch elements.",
            "computational_cost_metric": "Acquisition computation is more expensive than utility-based acquisitions due to entropy computations and approximations for joint batch gains; overall cost accounted as CPU/GPU time for acquisition and surrogate computations in addition to objective evaluation counts.",
            "information_gain_metric": "Explicit mutual information / expected entropy reduction about argmax f; predictive entropy is the primary metric.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration is driven explicitly by information gain: points that most reduce uncertainty about the optimizer are preferred, naturally balancing exploration and exploitation based on their information value.",
            "diversity_mechanism": "Batch PES constructs sets whose joint information is maximised, which implicitly promotes diversity among batch points to avoid redundant information.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Used under limited expensive-evaluation budgets and for batch-parallel evaluation budgets.",
            "budget_constraint_handling": "Selects points that maximise information per evaluation; parallel PES uses approximations to handle batch size constraints.",
            "breakthrough_discovery_metric": "Reduction in posterior entropy of the optimizer and eventual identification of high-performing points (indirectly promotes breakthroughs by targeting informative regions).",
            "performance_metrics": "",
            "comparison_baseline": "Compared conceptually or empirically in literature against EI, PI, GP-UCB and KG; review notes PES for constrained problems [86,87] and multi-objective extensions.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "PES is computationally heavier but can achieve greater information per expensive evaluation; the review highlights the computational cost vs information gain tradeoff and cites parallel PES for batch settings.",
            "optimal_allocation_findings": "Use PES when the objective is to rapidly reduce uncertainty about the optimizer (e.g., small expensive budgets), but be mindful of the higher acquisition computational cost; parallel PES is suitable for batch allocation when information-focused batches are required.",
            "uuid": "e2635.2",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "KG",
            "name_full": "Knowledge Gradient",
            "brief_description": "An acquisition strategy that selects experiments by maximising the expected incremental value (expected improvement in decision quality) of a single evaluation, i.e., the expected increase in the solution’s value after observing the result.",
            "citation_title": "The parallel knowledge gradient method for batch Bayesian optimization",
            "mention_or_use": "mention",
            "system_name": "Knowledge Gradient (KG) / Parallel Knowledge Gradient",
            "system_description": "KG computes, for each candidate evaluation, the expected increase in the objective value of the best decision after obtaining the observation (expected value of information). The parallel KG extends the criterion to batches by estimating joint expected value improvements. KG is inherently decision-theoretic and can incorporate evaluation costs and budget considerations into its expected-utility calculation.",
            "application_domain": "Decision-focused experimental design and optimisation where each evaluation’s expected impact on final decision quality matters (general scientific experiments, materials, engineering).",
            "resource_allocation_strategy": "Allocates experiments to maximise expected value-of-information — choose the point(s) with the largest expected uplift to the best-achievable outcome given the budget.",
            "computational_cost_metric": "Acquisition computation cost (CPU/GPU time) can be substantial since KG requires nested expectations; objective evaluation counts remain the dominant experimental cost.",
            "information_gain_metric": "Expected value of information measured as expected increase in final decision utility (expected improvement in best-observed objective after sampling).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "KG balances exploration/exploitation by explicitly valuing prospective information that leads to better final decisions; it may select exploratory points if their expected impact on the final choice is high.",
            "diversity_mechanism": "Parallel KG estimates joint expected gains for batches and will select complementary points if they jointly increase expected decision value; implicit diversity arises from the joint-value calculation.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budgets and parallel/batch constraints; KG can be adapted to include per-evaluation costs in its utility.",
            "budget_constraint_handling": "KG naturally incorporates expected decision improvement per evaluation and so can be used to prioritise cheaper/higher-value evaluations under budget constraints; the parallel KG handles multi-evaluation batch selection.",
            "breakthrough_discovery_metric": "Expected uplift in the utility of the final decision (e.g., improvement over current best), framed as expected value-of-information.",
            "performance_metrics": "",
            "comparison_baseline": "Compared in literature to EI, PES and other acquisition methods; the review lists KG [101] among batch/parallel methods.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "KG is more computationally intensive for acquisition but targets experiments with maximal expected decision impact, trading internal computation for potentially fewer expensive evaluations.",
            "optimal_allocation_findings": "Recommended when explicit decision-theoretic allocation is desired (e.g., limited budget and need to maximise final decision quality); suitable for batch settings via parallel KG approximations.",
            "uuid": "e2635.3",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Multi-fidelity BO",
            "name_full": "Multi-fidelity Bayesian Optimization",
            "brief_description": "A BO extension that models multiple information sources of varying cost and fidelity, allocating evaluations across them to reduce total expensive evaluation cost while still optimizing the high-fidelity objective.",
            "citation_title": "Multi-fidelity Bayesian optimisation with continuous approximations",
            "mention_or_use": "mention",
            "system_name": "Multi-fidelity Bayesian Optimization (multi-source)",
            "system_description": "Constructs a joint surrogate over objective outputs at multiple fidelities (e.g., cheap simulations, approximations, and expensive physical experiments). The optimiser selects among sources and inputs by weighing the expected information/value for the high-fidelity objective against per-evaluation cost, often using co-kriging or structured GP kernels to model inter-fidelity correlation. The literature surveyed includes continuous-approximation multi-fidelity BO and other multi-information-source approaches.",
            "application_domain": "Optimization problems where cheaper approximations or simulations are available alongside costly high-fidelity experiments (materials design, simulation-driven engineering, reinforcement learning with simulators and real-world tests).",
            "resource_allocation_strategy": "Allocate budget across fidelities by selecting evaluations that maximise a utility per unit cost (e.g., expected information or expected improvement at high fidelity divided by source cost), sometimes via a knowledge-gradient-like or information ratio criterion.",
            "computational_cost_metric": "Per-evaluation monetary/compute cost of each fidelity (implicit); surrogate inference cost increases with multi-fidelity modelling complexity; overall budget treated in terms of number/cost of high-fidelity evaluations avoided.",
            "information_gain_metric": "Often uses expected improvement on the high-fidelity objective conditioned on multi-fidelity models, or other expected-information/value metrics that account for cross-fidelity correlation.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration can occur at low-fidelity cheap sources to learn broad structure, while exploitation occurs at high-fidelity expensive evaluations to confirm promising candidates; acquisition balances expected contribution to the high-fidelity objective and cost.",
            "diversity_mechanism": "Implicit via selecting evaluations across different fidelities and input regions; explicit diversity mechanisms not emphasised but multi-source sampling can increase coverage cheaply.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Monetary/computational budget across heterogeneous evaluation sources; limited number of high-fidelity experiments.",
            "budget_constraint_handling": "Optimiser trades off information/cost across sources by modelling expected benefit per unit cost and choosing cheaper informative evaluations where beneficial, reserving expensive evaluations for high-value confirmations.",
            "breakthrough_discovery_metric": "Improvement in high-fidelity objective discovered with fewer expensive evaluations (often evaluated qualitatively); specific breakthrough metrics depend on application (e.g., material property thresholds).",
            "performance_metrics": "",
            "comparison_baseline": "Compared conceptually to single-fidelity BO (which uses only expensive evaluations) and to ad-hoc use of proxies or simulations.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Explicitly analyses tradeoff between information (value toward high-fidelity objective) and evaluation cost; referenced works show that using cheaper approximations can accelerate search but require careful modelling of fidelity correlations.",
            "optimal_allocation_findings": "Recommend modelling cross-fidelity structure and choosing evaluations that maximise expected high-fidelity benefit per unit cost; multi-fidelity BO can substantially reduce costly experiments if low-fidelity sources are sufficiently informative.",
            "uuid": "e2635.4",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Multi-IS",
            "name_full": "Multi-information-source Optimization",
            "brief_description": "An approach that coordinates sampling across multiple information sources (simulators, experiments, weak/strong labelers) with different costs and accuracies to optimise a target objective efficiently.",
            "citation_title": "Multi-information source optimization",
            "mention_or_use": "mention",
            "system_name": "Multi-information-source Optimization",
            "system_description": "Models several correlated information sources and chooses where and which source to query by estimating the contribution of each query to reducing uncertainty or improving objective estimates. Can be cast as multi-fidelity optimisation or as active learning from weak/strong labelers, and uses acquisition criteria that account for cost and informativeness of each source.",
            "application_domain": "Settings with heterogeneous data sources: simulation + experiment, weak vs strong labelers in active learning, multi-resolution simulations in engineering and materials discovery.",
            "resource_allocation_strategy": "Allocate queries to sources and input settings that maximise expected information or expected improvement per unit cost, leveraging source correlation to prioritise cheap informative evaluations.",
            "computational_cost_metric": "Per-source evaluation cost (time, compute, monetary), plus surrogate inference complexity; papers cited discuss trading off these costs explicitly.",
            "information_gain_metric": "Expected information gain about the high-fidelity objective or expected improvement adjusted by source reliability; may use mutual information or expected utility measures.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration preferentially uses low-cost/high-uncertainty sources to learn broadly, exploitation uses higher-fidelity sources for confirmation; selection driven by information-per-cost criteria.",
            "diversity_mechanism": "Implicit in sampling different sources and input locations; explicit diversity promotion not central but possible via batch criteria.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Monetary/computational budget distributed across multiple sources; limited budget for high-fidelity queries.",
            "budget_constraint_handling": "Optimiser explicitly models cost of each source and selects queries to maximise information or improvement subject to a total cost/budget constraint.",
            "breakthrough_discovery_metric": "Identification of high-fidelity optima with fewer expensive queries; no single standard metric reported in the review.",
            "performance_metrics": "",
            "comparison_baseline": "Single-source BO, naive use of only high-fidelity experiments or only simulations.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Review highlights that multi-source approaches trade per-query cost vs informativeness and can substantially accelerate search when low-cost sources correlate well with the high-fidelity objective; careful modelling of inter-source biases is necessary.",
            "optimal_allocation_findings": "Prefer sampling low-cost sources early to map landscape, then allocate budget to high-fidelity queries in promising regions; choose queries by expected information or expected improvement per unit cost.",
            "uuid": "e2635.5",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Budgeted-Batch BO",
            "name_full": "Budgeted Batch Bayesian Optimization",
            "brief_description": "A batch BO variant that composes batches of experiments under an explicit budget constraint, selecting points to maximise expected return while respecting total cost or size limits.",
            "citation_title": "Budgeted batch Bayesian optimization",
            "mention_or_use": "mention",
            "system_name": "Budgeted Batch Bayesian Optimization",
            "system_description": "Constructs batches of candidate evaluations taking into account both the budget (e.g., total runtime, monetary cost, number of samples) and the need for diversity/informativeness within the batch. Methods include multi-step lookahead, heuristics, and explicit budget-aware selection heuristics to choose multiple points per iteration.",
            "application_domain": "Experimental pipelines where several runs must be scheduled at once (e.g., alloy casting batches, parallel lab assays, parallel simulations) under a global budget.",
            "resource_allocation_strategy": "Select a set of points whose joint expected utility (e.g., EI, KG, PES) per unit budget is maximised; often uses sequential suppression/fantasy sampling or penalisation to ensure diversity and spread of information across the batch.",
            "computational_cost_metric": "Batch size, per-evaluation cost, and total budget (time or monetary) are considered; acquisition computation for joint batch selection increases with batch size.",
            "information_gain_metric": "Depends on the underlying acquisition used for batch scoring (EI, PES, KG, information-theoretic criteria); often optimises expected joint improvement or information per budget unit.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Batch composition mixes exploitation-focused and exploration-focused points (e.g., first point via GP-UCB, additional points via pure exploration or penalised acquisition) so exploration/exploitation tradeoff is balanced across the batch.",
            "diversity_mechanism": "Diversity induced by penalisation of acquisition near already-selected batch points, lookahead/fantasies, or combining different acquisition strategies within a batch (arrays of strategies).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch budget (number of parallel evaluations) and/or monetary/time budget for batch operations.",
            "budget_constraint_handling": "Explicitly enforces budget by limiting batch size or selecting points to maximise expected returns subject to budget, using heuristics or lookahead to account for joint outcomes.",
            "breakthrough_discovery_metric": "Joint expected improvement of the batch or expected number of candidates exceeding target thresholds; not given as standardized numeric metrics in review.",
            "performance_metrics": "",
            "comparison_baseline": "Sequential single-point BO, naive parallel random sampling, multi-point EI without budget awareness.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Discusses trade-offs between parallel speed-up (running many experiments concurrently) and loss of adaptivity (making choices without intermediate feedback), and computational cost of joint acquisition optimisation vs experimental savings.",
            "optimal_allocation_findings": "When batches are required, construct them to mix exploitation and exploration, use penalisation or fantasising to avoid redundant queries, and explicitly account for budget constraints to prioritise highest-value points.",
            "uuid": "e2635.6",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Strategy-space diversity",
            "name_full": "Exploiting Strategy-Space Diversity for Batch Bayesian Optimization",
            "brief_description": "A method that generates batch candidates by combining several distinct batch-construction strategies to promote diverse and complementary experiments within a parallel batch.",
            "citation_title": "Exploiting strategy-space diversity for batch Bayesian optimization",
            "mention_or_use": "mention",
            "system_name": "Strategy-space Diversity for Batch BO",
            "system_description": "Constructs a batch by running multiple different batch-selection strategies (e.g., GP-UCB seeds, pure-exploration points, penalised-EI points, different surrogate hyperparameters) in parallel to produce a set of complementary candidates. The goal is to span different modes of the acquisition landscape and increase hypothesis diversity in the batch, thereby improving the chance of discovering breakthroughs in parallel experiments.",
            "application_domain": "Batch experimental design where parallel experiments should explore diverse hypotheses to increase discovery probability (materials, lab experiments, A/B tests).",
            "resource_allocation_strategy": "Allocate the available parallel evaluation slots across multiple strategies so that the batch contains complementary points; selection can be based on ranking or ensemble combination of candidate lists from different strategies.",
            "computational_cost_metric": "Extra acquisition computation due to running multiple strategies (CPU/GPU time); experimental cost measured by number of parallel evaluations.",
            "information_gain_metric": "Not necessarily an explicit information-theoretic criterion; relies on ensemble of acquisition heuristics to cover both high-utility and high-uncertainty regions, indirectly increasing expected information gathered by the batch.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Diversity achieved by mixing exploitation-focused strategies (e.g., GP-UCB) with exploration-focused ones (pure exploration or PE) so the batch collectively balances exploitation and exploration.",
            "diversity_mechanism": "Explicit: combining outputs of multiple distinct batch-selection strategies to ensure the batch contains widely different candidate points and avoids redundancy.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Parallel batch size constraint (fixed number of concurrent evaluations).",
            "budget_constraint_handling": "Distributes the fixed batch slots across strategies to achieve coverage and diversity; selection may prioritise highest-scoring unique points from each strategy subject to batch size.",
            "breakthrough_discovery_metric": "Increased probability of finding high-performing points in one batch due to strategy diversity; no single numeric breakthrough metric provided in review.",
            "performance_metrics": "",
            "comparison_baseline": "Compared conceptually to single-strategy batch methods (e.g., local penalization, multi-point EI) and to sequential single-point BO.",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Trade-off between increased acquisition computation (running multiple strategies) and higher chance of diverse, informative, or breakthrough experiments per batch; recommended when parallel resources exist and exploration diversity is valuable.",
            "optimal_allocation_findings": "Promote strategy diversity in batches to cover multiple hypotheses and reduce risk of redundant parallel experiments; combine exploitation and exploration strategies within the same batch for robust allocation.",
            "uuid": "e2635.7",
            "source_info": {
                "paper_title": "Bayesian Optimization for Adaptive Experimental Design: A Review",
                "publication_date_yy_mm": "2020-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Predictive entropy search for Bayesian optimization with unknown constraints",
            "rating": 2
        },
        {
            "paper_title": "Parallel predictive entropy search for batch global optimization of expensive objective functions",
            "rating": 2
        },
        {
            "paper_title": "The parallel knowledge gradient method for batch Bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Multi-fidelity Bayesian optimisation with continuous approximations",
            "rating": 2
        },
        {
            "paper_title": "Multi-information source optimization",
            "rating": 2
        },
        {
            "paper_title": "Budgeted batch Bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Exploiting strategy-space diversity for batch Bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Bayesian optimization with unknown constraints",
            "rating": 1
        },
        {
            "paper_title": "Parallel Bayesian global optimization of expensive functions",
            "rating": 1
        },
        {
            "paper_title": "Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization",
            "rating": 1
        }
    ],
    "cost": 0.02219525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bayesian Optimization for Adaptive Experimental Design: A Review</h1>
<p>STEWART GREENHILL ${ }^{\ominus}$, SANTU RANA ${ }^{\ominus}$, SUNIL GUPTA ${ }^{\ominus}$, PRATIBHA VELLANKI ${ }^{\ominus}$, AND SVETHA VENKATESH ${ }^{\ominus}$<br>Applied Artificial Intelligence Institute, Deakin University, Waurn Ponds Campus, Geelong, VIC 3216, Australia<br>Corresponding author: Stewart Greenhill (s.greenhill@deakin.edu.au)</p>
<p>This work was supported by the Australian Government through the Australian Research Council (ARC). The work of Svetha Venkatesh was supported by the ARC Australian Laureate Fellowship under Grant FL170100006.</p>
<h4>Abstract</h4>
<p>Bayesian optimisation is a statistical method that efficiently models and optimises expensive "black-box" functions. This review considers the application of Bayesian optimisation to experimental design, in comparison to existing Design of Experiments (DOE) methods. Solutions are surveyed for a range of core issues in experimental design including: the incorporation of prior knowledge, high dimensional optimisation, constraints, batch evaluation, multiple objectives, multi-fidelity data, and mixed variable types.</p>
<p>INDEX TERMS Bayesian methods, design for experiments, design optimization, machine learning algorithms.</p>
<h2>I. INTRODUCTION</h2>
<p>Experiments are fundamental to scientific and engineering practice. A well-designed experiment yields an empirical model of a process, which facilitates understanding and prediction of its behaviour. Experiments are often costly, so formal Design of Experiments methods (or DOE) [1]-[3] optimise measurement of the design space to give the best model from the fewest observations.</p>
<p>Models are important decision tools for design engineers. Understanding of design problems is enhanced when the design space can be explored cheaply and rapidly, allowing adjustment of the number and range of design variables, identification of ineffective constraints, balancing multiple design objectives, and optimisation [4]. Industrial processes must be robust to environmental conditions, component variation, and variability around a target [3]. Robust Parameter Design (RPD) [5]-[7] systematically characterises the influence of uncontrollable variables and noise. The number of observations required to build a model increases rapidly with the number of variables, making it challenging to investigate systems with many variables. Screening experiments can identify subsets of important variables to be later investigated in more detail [8], [9]. Optimisation is important in most industrial applications, and there are often multiple objectives</p>
<p>The associate editor coordinating the review of this manuscript and approving it for publication was Bijoy Chand Chatterjee
which must balanced including yield, robustness, and cost. In classical experimental design, modelling and optimisation are separate processes, but newer model-based approaches can potentially sample more efficiently by adapting to the response surface, and can incorporate optimisation into the modelling process.</p>
<p>Machine learning has made great strides in the recent past, and we present here a machine learning approach to experimental design. Bayesian Optimisation (BO) [19], [20] is a powerful method for efficient global optimisation of expensive black-box functions. The experimental method introduces specific challenges: how to handle constraints, high dimensionality, mixed variable types, multiple objectives, parallel (batch) evaluation, and the transfer of prior knowledge. Several reviews have presented BO for a technical audience [20]-[22]. Our review surveys recent methods for systematically handling these challenges within a BO framework, with an emphasis on applications in science and engineering, and in the context of modern experimental design.</p>
<p>Bayesian optimisation is a sample efficient optimisation algorithm and thus suits optimisation of expensive, black-box systems. By "black-box" we mean that the objective function does not have a closed-form representation, does not provide function derivatives, and only allows point-wise evaluation. Several optimisation algorithms can handle optimisation of black-box functions such as</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h2>Two-level Factorial Design</h2>
<p>Factors are sampled at maxima and minima, which are coded -1 and 1. Two level designs estimate main effects and their products. Finer grids allow higher polynomial terms to be estimated. Fractional factorial designs estimate $k$ main effects in $k+1$ samples, with some confounding between main effects and their products.</p>
<h2>Central Composite Design</h2>
<p>The addition of $2 k+1$ axial samples to a factorial design allows the estimation of quadratic terms. These designs are used to estimate second order response surfaces for Response Surface Methodology (RSM)</p>
<h2>Space Filling Design</h2>
<p>Distributes samples evenly throughout the space, so that each level of each factor is sampled with equal probability. Methods include latin hypercube sampling, uniform designs, sphere packing, and low-discrepancy sequences (eg. Hammersley, Sobol)</p>
<h2>Adaptive Sampling</h2>
<p>Samples based on previous observations by balancing exploration (visiting unexplored areas) and exploitation (sampling in known interesting areas). May use model information (eg. Gaussian process variance), or be independent of the model type (eg. using local gradients or cross-validation). Figure colour shows mean of GP model of Camelback function.</p>
<p>Model: linear
Samples: $N=2^{k}$</p>
<p>Model: quadratic
Samples: $N=2^{k}+2 k+1$</p>
<p>Model: independent
Samples: $N \approx 10 k$ (heuristic)</p>
<p>Model: usually non-parametric, built incrementally and used to guide sampling</p>
<p>Samples: depends on model and required statistical significance of the estimate</p>
<p>FIGURE 1. Sampling methods used in experimental design. In classical Factorial designs samples are placed on a geometric grid. Space filling designs are used with a variety of non-linear models. Sample requirements are determined heuristically, but these designs are empirically much more efficient than grids.
multi-start derivative free local optimiser e.g. COBYLA [36], or evolutionary algorithms e.g. ISRES [37], or Lipschitzian methods such as DIRECT [34]. However, none of these are designed to be sample efficient, and all need to evaluate a function many times to perform optimisation. In contrast, Bayesian optimisation uses a model based approach with an adaptive sampling strategy to minimise the number of function evaluations.</p>
<p>Past approaches to experimental design have closely coupled sampling and modelling. Factorial designs assume a linear model and sample at orthogonal corners of the design space (see Figure 1). For more complex non-linear models, general purpose space-filling designs such as Latin hypercubes offer a more uniform coverage of the design space. For $N$ sample points in $k$ dimensions, there are $(N!)^{k-1}$ possible Latin hypercube designs, and finding a suitable design involves balancing space-filling (e.g. via entropy, or potential energy) with other desirable properties such as orthogonality. Much literature exists on the design of Latin hypercubes, and
many research issues remain open [10], [11] such as: mixing of discrete and continuous variables, incorporation of global sensitivity information, and sequential sampling.</p>
<p>Response Surface Methodology (RSM) [3], [12] is a sequential approach which has become the primary method for industrial experimentation. In its original form, response surfaces are second order polynomials which are determined using central composite factorial experiments, and a path of steepest ascent is used to seek an optimal point. For robust design, replication is used to estimate noise factors, and optimisation must consider dual responses for process mean and variance. Approaches for handling multiple objectives include "split-plot" techniques, "desirability functions" and Pareto fronts [13]. Non-parametric RSM can be more general than second-order polynomials, and uses techniques such as Gaussian processes, thin-plate splines, and neural networks. Alternative optimisation approaches include simulated annealing, branch-and-bound and genetic algorithms [14].</p>
<p>In many areas, experiments are performed with detailed computer simulations of physical systems. Aerospace designers frequently work with expensive CFD (computational fluid dynamic) and FEA (finite element analysis) simulations. Multi-agent simulations are used to model how actor behaviour determines the outcome of group interactions in areas such as defence, networking, transportation, and logistics. Design and Analysis of Computer Experiments (or DACE, after [15]) differs from DOE in several ways. Simulations are generally deterministic, without random effects and uncontrolled variables, so less emphasis is placed on dealing with measurement noise. Simulations often include many variables, so there is more need to handle high dimensionality and mixed variable types. Where the response is complex, non-parametric models are used, including Gaussian Processes, Multivariate Adaptive Regression Splines, and Support Vector Regression [4], [16], [17].</p>
<p>A problem with classical DOE and space-filling designs is that the sampling pattern is determined before measurements are made, and cannot adapt to features that appear during the experiment. In contrast, adaptive sampling [16], [18] is a sequential process that decides the location of the next sample by balancing two criteria. Firstly, it samples in areas that have not been previously explored (e.g. based on distance from previous samples). Secondly, it samples more densely in areas where interesting behaviour is observed, such as rapid change or non-linearity. This can be detected using local gradients, prediction variance (e.g. where uncertainty is modelled), by checking agreement between the model and data (cross-validation), or agreement between an ensemble of models. BO is a form of model-based global optimisation (MBGO [16]), which uses adaptive sampling to guide the experiment towards a global optimum. Unlike pure adaptive sampling, MBGO considers the optimum of the modelled objective when deciding where to sample.</p>
<p>Recently, there has been a surge in applying Bayesian optimisation to design problems involving physical products and processes. In [23], Bayesian optimisation is applied in combination with a density functional theory (DFT) based computational tool to design low thermal hysteresis NiTi-based shape memory alloys. Similarly, in [24] Bayesian optimisation is used to optimise both the alloy composition and the associated heat treatment schedule to improve the performance of Al-7xxx series alloys. In [25], Bayesian optimisation is applied for high-quality nano-fibre design meeting a required specification of fibre length and diameter within few tens of iterations, greatly accelerating the production process. It has also been applied in other diverse fields including optimisation of nano-structures for optimal phonon transport [26], optimisation for maximum power point tracking in photovoltaic power plants [27], optimisation for efficient determination of metal oxide grain boundary structures [28], and for optimisation of computer game design to maximise engagement [29]. It has also been used in a recent neuroscience study [30] in designing cognitive tasks that maximally segregate ventral and dorsal FPN activity.</p>
<p>The recent advances in both the theory and practice of Bayesian optimisation has led to a plethora of techniques. In most parts, each advance is applicable to a sub-set of experimental conditions. What is lacking is both an overview of these methods and a methodology to adapt these techniques to a particular experimental design context. We fill this gap and provide a comprehensive study of the state-of-the-art Bayesian optimisation algorithms in terms of their applicability in experimental optimisation. Further, we provide a template of how disparate algorithms can be connected to create a fit-for-purpose solution. This thus provides an overview of the capability and increases the reach of these powerful methods. We conclude by discussion where further research is needed.</p>
<h2>II. BAYESIAN OPTIMISATION</h2>
<p>Bayesian optimisation incorporates two main ideas:</p>
<ul>
<li>A Gaussian process (GP) is used to maintain a belief over the design space. This simultaneously models the predicted mean $\mu_{t}(\boldsymbol{x})$ and the epistemic uncertainty $\sigma_{t}(\boldsymbol{x})$ at any point $\boldsymbol{x}$ in the input space, given a set of observations $\mathcal{D}<em 1="1">{1: t}=\left{\left(\boldsymbol{x}</em>}, y_{1}\right),\left(\boldsymbol{x<em 2="2">{2}, y</em>}\right), \ldots\left(\boldsymbol{x<em t="t">{t}, y</em>}\right)\right}$, where $\boldsymbol{x<em t="t">{t}$ is the process input, and $y</em>$ is the corresponding output at time $t$.</li>
<li>An acquisition function expresses the most promising setting for the next experiment, based on the predicted mean $\mu_{t}(\boldsymbol{x})$ and the uncertainty $\sigma_{t}(\boldsymbol{x})$.
A GP is completely specified by its mean function $m(\boldsymbol{x})$ and covariance function $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ :</li>
</ul>
<p>$$
f(\boldsymbol{x}) \sim \mathcal{G} \mathcal{P}\left(m(\boldsymbol{x}), k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)
$$</p>
<p>The covariance function $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ is also called the "kernel", and expresses the "smoothness" of the process. We expect that if two points $\boldsymbol{x}$ and $\boldsymbol{x}^{\prime}$ are "close", then the corresponding process outputs $y$ and $y^{\prime}$ will also be "close", and that the closeness depends on the distance between the points, and not the absolute location or direction of separation. A popular choice for the covariance function is the squared exponential (SE) function, also known as radial basis function (RBF):</p>
<p>$$
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\exp \left(-\frac{1}{2 \theta^{2}}\left|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right|^{2}\right)
$$</p>
<p>Equation 2 says that the correlation decreases with the square of the distance between points, and includes a parameter $\theta$ to define the length scale over which this happens. Specialised kernel functions are sometimes used to express pre-existing knowledge about the function (e.g. if something is known about the shape of $f$ ).</p>
<p>In an experimental setting, observations include a term for normally distributed noise $\epsilon \sim \mathcal{N}\left(0, \sigma_{\text {noise }}^{2}\right)$, and the observation model is:</p>
<p>$$
y=f(\boldsymbol{x})+\epsilon
$$</p>
<p>Gaussian process regression (or "kriging") can predict the value of the objective function $f(\cdot)$ at time $t+1$ for any</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p><strong>FIGURE 2.</strong> Bayesian optimisation is an iterative process in which the unknown system response is modelled using a Gaussian process. An acquisition function expresses the most promising setting for the next experiment, and can be efficiently optimised. The model quality improves progressively over time as successive measurements are incorporated.</p>
<p>Location <em>x</em>. The result is a normal distribution with mean $$\mu_t(x)$$ and uncertainty $$\sigma_t(x)$$.</p>
<p>$$P(f_{t+1} \mid \mathcal{D}_{1:t}, x) = \mathcal{N}(\mu_t(x), \sigma_t^2(x)) \tag{3}$$</p>
<p>where</p>
<p>$$\begin{aligned}
\mu_t(x) &amp;= k^T [K + \sigma_{noise}^2 I]^-\frac{1}{2}y_{1:t} \
\sigma_t(x) &amp;= k(x, x) - k^T [K + \sigma_{noise}^2 I]^-\frac{1}{k} \
k &amp;= [k(x, x_1), k(x, x_2), \dots, k(x, x_t)] \
K &amp;= \begin{bmatrix}
k(x_1, x_1) &amp; \dots &amp; k(x_1, x_t) \
\vdots &amp; \ddots &amp; \vdots \
k(x_t, x_1) &amp; \dots &amp; k(x_t, x_t)
\end{bmatrix} \tag{5}
\end{aligned}$$</p>
<p>Using the Gaussian process model, an <strong>acquisition function</strong> is constructed to represent the most promising setting for the next experiment. Acquisition functions are mainly derived from the $$\mu(x)$$ and $$\sigma(x)$$ of the GP model, and are hence cheap to compute. The acquisition function allows a balance between <strong>exploitation</strong> (sampling where the objective mean $$\mu(\cdot)$$ is high) and <strong>exploration</strong> (sampling where the uncertainty $$\sigma(\cdot)$$ is high), and its global maximiser is used as the next experimental setting.</p>
<p>Acquisition functions are designed to be large near <em>potentially</em> high values of the objective function. Figure 3 shows commonly used acquisition functions: PI, EI, and GP-UCB. PI prefers areas where improvement over the current maximum $$f(x^+)$$ is most likely. EI considers not only probability of improvement, but also the expected magnitude of improvement. GP-UCB maximises $$f(\cdot)$$ while minimising regret, the difference between the average utility and the ideal utility. Regret bounds are important for theoretically proving convergence. Unlike the original function, the acquisition function can be cheaply sampled, and may be optimised using a derivative-free global optimisation method like DIRECT [34] or using multi-start method with a derivative based local optimiser such as L-BFGS [35]. Details can be found in [19], [21].</p>
<h3><strong>III. EXPERIMENTAL DESIGN WITH BAYESIAN OPTIMISATION</strong></h3>
<p>BO has been influential in computer science for hyperparameter tuning [38]–[42], combinatorial optimisation [43], [44], and reinforcement learning [21]. Recent years have seen new applications in areas such as robotics [45], [46], neuroscience [47], [48], and materials discovery [49]–[55].</p>
<p>Bayesian optimisation is an iterative process outlined in Figure 2, which can be applied to experiments where inputs are unconstrained and the objective is a scalarised function of measured outputs. Examples of this kind include material design using physical models [56], or laboratory experiments [25]. However, experiments often involve</p>
<table>
<thead>
<tr>
<th>Acquisition Function</th>
<th>Formulation</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td>Probability of Improvement</td>
<td>$\operatorname{PI}(\boldsymbol{x})=\Phi\left(\frac{\mu_{t}(\boldsymbol{x})-f\left(\boldsymbol{x}_{t}^{+}\right)-\xi}{\sigma(x)}\right)$</td>
<td>[31]</td>
</tr>
<tr>
<td>Expected Improvement</td>
<td>$\begin{aligned} &amp; \operatorname{EI}(\mathbf{x})=\left(\mu_{t}(\boldsymbol{x})-f\left(\boldsymbol{x}<em t="t">{t}^{+}\right)\right) \Phi(Z)+\sigma</em>}(\boldsymbol{x}) \phi(Z) \ &amp; \text { where } Z=\frac{\mu_{t}(\boldsymbol{x})-f\left(\boldsymbol{x<em t="t">{t}^{+}\right)}{\sigma</em>$}(\boldsymbol{x})} \end{aligned</td>
<td>[32]</td>
</tr>
<tr>
<td>GP Upper Confidence Bound</td>
<td>$\operatorname{GP}-\operatorname{UCB}(\boldsymbol{x})=\mu_{t}(\boldsymbol{x})+\kappa_{t} \sigma_{t}(\boldsymbol{x})$</td>
<td>[33]</td>
</tr>
</tbody>
</table>
<p>FIGURE 3. Acquisition functions expressed in terms of the mean $\mu(\boldsymbol{x})$, variance $\sigma(\boldsymbol{x})$, and current maximum $\boldsymbol{f}\left(\boldsymbol{x}^{+}\right)$. $\boldsymbol{\Phi}(\cdot)$ and $\phi(\cdot)$ are the cumulative distribution function and the probability distribution function of the standard normal distribution. Some functions include factors to balance between exploration and exploitation: $\xi$ in PI is constant, whereas $\kappa_{t}$ in GP-UCB usually increases with iteration, causing the search to maintain exploration even with many samples.</p>
<p>Step 1: Mixture Preparation
A batch of sample ingots is produced, each with a different mixture of elements
$\mathrm{Al} \div \mathrm{Zn} \div \mathrm{Mn} \div \mathrm{Cu} \div \mathrm{Si} \div \mathrm{Fe} \div \mathrm{Mn} \div \mathrm{Ti} \div \mathrm{Cr} \div 100 \%$
$5.6 \%&lt;\mathrm{Zn}&lt;6.1 \%$
$2.1 \%&lt;\mathrm{Mn}&lt;2.5 \%$
$1.2 \%&lt;\mathrm{Cu}&lt;1.6 \%$
$\mathrm{Si}, \mathrm{Fe}, \mathrm{Mn}, \mathrm{Ti}, \mathrm{Cr}&lt;0.5 \%$
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Step 2: Heat Treatment
The samples are strengthened by tempering, where they are exposed to a series of different temperatures for variable durations. Samples are placed in the same oven, so temperatures are the same, but they can be removed at different times so durations may vary</p>
<p>FIGURE 4. Case Study: Alloy design process. Alloy samples are cast with varying compositions according to a set of constraints. The samples are then tempered to improve hardness, which is subsequently measured. The physics of tempering of an alloy is based on nucleation and growth. During nucleation, new "phases" or precipitates are formed when clusters of atoms self-organise. These precipitates then diffuse together to achieve the requisite alloy characteristics in the growth step.
complicating factors such as constraints, batches, and multiple objectives. For example, in the alloy design process the composition of each sample follows a set of mixture constraints (see Figure 4). Batches of samples then undergo heat treatment for up to 70 hours, exposed to the same temperatures but with possible variation in duration between samples [24]. The optimiser must produce a batch of experimental settings, obeying inequality constraints, with some factors varying and others fixed within each batch. This impacts the design of the optimiser, through the formulation of the model, acquisition functions, and the search strategy. These are active areas of research, and recent developments are surveyed in the following discussion.</p>
<h2>A. INCORPORATING PRIOR KNOWLEDGE</h2>
<p>Where successive experiments are sufficiently similar to previous ones, it may be desirable to transfer knowledge from previous outcomes. Prior knowledge about the function or data can be used to reduce the search complexity and accelerate optimisation. Table 1 outlines some approaches. (1) Knowledge may be transferred from past (source) experiments to new (target) experiments where there are known or learnable similarities between the domains. For example, the source and target may be loosely similar, or have similar trends. (2) Where something is known about the influence of particular variables on the objective
function, this can be imposed on the GP model. This could include monotonicity, function shape, or the probable location of the optimum or other features. (3) Where dependency structures exist in the design space, these can exploited to constrain the GP, or to handle high dimensionality via embedding.</p>
<h2>B. HIGH DIMENSIONAL OPTIMISATION</h2>
<p>The acquisition function must be optimised to find the next best suggestion for evaluating the objective. In continuous domain the acquisition functions can be extremely sharp in high dimensions, having only a few peaks marooned in a large terrain of almost flat surface. Global optimisation algorithms such as DIRECT [34] are infeasible above about 10 dimensions, and gradient-dependent methods cannot move if initialised in the flat terrain.</p>
<p>General strategies for tackling high-dimensionality include [103]: reducing the design space, screening important variables, decomposing the design into simpler sub-problems, mapping into a lower-dimensional space, and visualisation. Table 1(4) outlines approaches that have been reported for high dimensional BO, including: using coarse-to-fine approximations, projection into a lower-dimensional space, and approximation through low-rank matrices or additive structures. Choice of a method depends on whether the objective function has an intrinsic low dimensional structure (4B) or not (4A).</p>
<p>Standard BO is known to perform well in low dimensions, but performance degrades above about 15-20 dimensions. High dimensional BO has been demonstrated for 25-34 intrinsic dimensions on "real world" data, and up to 50 dimensions for synthetic functions [73], [77]. Projection methods have been shown to work independently of the number of extrinsic dimensions [43], [79], [81], whereas special kernels are shown to work in hundreds of dimensions [75].</p>
<h2>C. MULTI-OBJECTIVE OPTIMISATION</h2>
<p>Design problems often include multiple objectives which can be challenging to optimise. For example [104] demonstrates multiple objectives for discovery of new materials. Scalarisation by weighted sum of objectives can be done, but may not work when objectives have strong conflicts. In that setting a Pareto set of optimal points can be found [105]. For a point in a Pareto set, any one of the objectives cannot be improved without penalising another objective.</p>
<p>TABLE 1. Methods for transferring prior knowledge from past experiments (source) to new experiments (target) (1-3). Methods marked (*) have only been demonstrated for Gaussian processes, but are also applicable to Bayesian optimisation. Methods for handling high dimensionality (4), constraints (5), and parallel optimisation (6).</p>
<table>
<thead>
<tr>
<th>1. Transfer Learning (Overall shape using source functions or data)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>A) Source \&amp; target functions are loosely similar</td>
<td>(a) Merge source \&amp; target data after standardisation [57]</td>
</tr>
<tr>
<td></td>
<td>(b) Merge source data as noisy observations into target [58, 59]</td>
</tr>
<tr>
<td>B) Difference between source \&amp; target can be learned as a simple function</td>
<td>Adjust source data through modelled difference function and then merge [59]</td>
</tr>
<tr>
<td>C) Source \&amp; target have similar overall trends</td>
<td>Source \&amp; target data are transformed and merged in latent space employing common rank [60]</td>
</tr>
<tr>
<td>D) Multiple similar sources, and similarity in target can be estimated</td>
<td>Similarity between source \&amp; target is incorporated in the kernel. GP jointly models all functions. Suitable for concurrent optimisation [41]</td>
</tr>
<tr>
<td>2. Transfer Learning (Trend per Variable)</td>
<td></td>
</tr>
<tr>
<td>A) Monotonicity or convexity in any variable</td>
<td>(monotonic) 1st derivative is imposed to be positive or negative in GP [61, 62, 63]</td>
</tr>
<tr>
<td></td>
<td>(convex/concave) 2nd order derivative is imposed to be positive or negative in GP [62, 64]</td>
</tr>
<tr>
<td>B) U or S shape in any variable</td>
<td>* Impose suitable constraints on first / second derivative. [64]</td>
</tr>
<tr>
<td>C) Unimodal in any variable</td>
<td>Estimating inflection point using a probabilistic model over inflection point [65]</td>
</tr>
<tr>
<td>D) Target optimum and source optimum are close</td>
<td>A prior belief on the optimum location is either estimated from source data or can be supplied by the user [66]</td>
</tr>
<tr>
<td>3. Transfer Learning (Dependency structure in design space)</td>
<td></td>
</tr>
<tr>
<td>A) Known tree structure dependency in input space</td>
<td>a) Use tree structure to transfer information across overlapping tree paths [67]</td>
</tr>
<tr>
<td></td>
<td>b) Use cylindrical embedding of input space [68]</td>
</tr>
<tr>
<td>B) Multiple sets of inputs independent of each other</td>
<td>a) *Additive structure imposed on kernel [69]</td>
</tr>
<tr>
<td></td>
<td>b) Use method (a) for BO with known structure [70]</td>
</tr>
<tr>
<td>C) Input space has hierarchical structures</td>
<td>* Structure embedded through kernel design [71]</td>
</tr>
<tr>
<td>D) Structure of a process is known</td>
<td>The underlying function is composite [72]</td>
</tr>
<tr>
<td>4. High Dimensional Optimisation</td>
<td></td>
</tr>
<tr>
<td>A) Function has no intrinsic low dimensional structure</td>
<td>a) Optimise acquisition function efficiently through "elastic" GP [73]</td>
</tr>
<tr>
<td></td>
<td>b) Select random subset of variables in any iteration [74],</td>
</tr>
<tr>
<td></td>
<td>c) Use cylindrical transformation of the high dimensional space [75]. However, it assumes that optimum is unlikely at the boundary.</td>
</tr>
<tr>
<td>B) Function has intrinsic low dimensional structure</td>
<td>a) Decompose into sub-spaces with smaller sub-sets.</td>
</tr>
<tr>
<td></td>
<td>Prior is known: See 3B (Transfer Learning with "additive structure") Prior is unknown:</td>
</tr>
<tr>
<td></td>
<td>Find latent space where model is decomposable [76, 77] Use method 3B(a) for BO with additive structure [78]</td>
</tr>
<tr>
<td></td>
<td>b) Reduce dimensions by random projection into latent space [43, 79, 80, 81]</td>
</tr>
<tr>
<td></td>
<td>c) Reduce dimensions through low rank matrix approximation [82]</td>
</tr>
<tr>
<td>5. Constraints</td>
<td></td>
</tr>
<tr>
<td>A) Inequality Constraints</td>
<td>a) Weighted EI [83, 84], ADMMBO [85]</td>
</tr>
<tr>
<td></td>
<td>b) Predictive Entropy Search [86, 87]</td>
</tr>
<tr>
<td></td>
<td>c) Lookahead strategy [88]</td>
</tr>
<tr>
<td>B) Equality Constraints</td>
<td>d) Slack variable augmented Lagrangian [89]</td>
</tr>
<tr>
<td>C) Preference Constraints</td>
<td>e) Multiobjective optimisation with preference constraints [90]</td>
</tr>
<tr>
<td>6. Parallel (Batch) Optimisation</td>
<td></td>
</tr>
<tr>
<td>A) No constraints within batch</td>
<td>a) Batch created through multi-step lookahead [91, 92, 93]</td>
</tr>
<tr>
<td></td>
<td>b) Using heuristics for batch selection [94, 95, 96]</td>
</tr>
<tr>
<td></td>
<td>c) Using multiple strategies</td>
</tr>
<tr>
<td></td>
<td>Use of GP-UCB (1st point) + PE (additional points) [97]</td>
</tr>
<tr>
<td></td>
<td>Array of GP-UCB strategies [98]</td>
</tr>
<tr>
<td></td>
<td>Array of different Gaussian process models [99]</td>
</tr>
<tr>
<td></td>
<td>d) Information theoretic approach [100]</td>
</tr>
<tr>
<td></td>
<td>e) Knowledge gradient [101]</td>
</tr>
<tr>
<td>B) Constraints within batch</td>
<td>Constraints are imposed on selected variables [102]</td>
</tr>
</tbody>
</table>
<p>Many methods have been proposed for using Bayesian optimisation for multi-objective optimisation [106]-[109], but these suffer from computational limitations because the acquisition function generally requires computation for all objective functions and as the number of objective functions grow the computational cost grows exponentially.</p>
<p>Moving away from EI, the method of [109] allows the optimisation of multiple objectives without rank modelling for conflicting objectives, while also remaining scale-invariant toward different objectives. The method performs better than [107], but suffers in high dimensions and can be computationally expensive. Predictive entropy search is used by [110], allowing the different objectives to be decoupled, computing acquisition for subsets of objectives when required. The computational cost increases linearly with the number of objectives. The method of [111] can be used for</p>
<p>single- or multiple-objective optimisation, including in multiple inequality constraints and has been shown to be robust in highly constrained settings where the feasible design space is small.</p>
<h2>D. CONSTRAINTS</h2>
<p>Table 1(5) outlines some approaches to handling constraints. If constraints are known, they can be handled during optimisation of the acquisition function by limiting the search. More difficult are "black box" constraints that can be evaluated but have unknown form. If the constraint is cheap to evaluate, this is not a problem. Methods for expensive constraint functions include a weighted EI function [83], [84], and weighted predictive entropy search [86]. A lookahead strategy for unknown constraints is described by [88]. A different formulation for the unknown is proposed by [85], handling expensive constraints using ADMM solver of [112].</p>
<p>The above methods deal with inequality constraints. In [89] both inequality and equality constraints are handled, using slack variables to convert inequality constraints to equality constraints, and Augmented Lagrangian (AL) to convert these inequality constraints into a sequence of simpler sub-problems.</p>
<p>The concept of weighted predictive entropy search has been extended for multi-objective problems [87] for inequality constraints which are both unknown and expensive to evaluate. A different type of constraint specifically for multiple objectives is investigated by [90] where between all the objectives, there exists a rank order preference on which objective is important. The algorithm developed therein can preferentially sample the Pareto set such that Pareto samples are more varied for the more important objectives.</p>
<h2>E. PARALLEL (BATCH) OPTIMISATION</h2>
<p>In some experiments it can be efficient to evaluate several settings in parallel. For example, during alloy design batches of different mixtures undergo similar heat treatment phases, so the optimiser must recommend multiple settings before receiving any new results. Sequential algorithms can be used to find the point that maximises the acquisition function, and then move on to find the next point in the batch after suppressing this point. Suppression can be achieved by temporarily updating the GP with a hypothetical value for the point (e.g. based on a recent posterior mean), or by applying a penalty in the acquisition function. Table 1(6) outlines some approaches that have been reported. Most methods are for unconstrained batches, though recent work has handled constraints on selected variables within a batch [102].</p>
<h2>F. MULTI-FIDELITY OPTIMISATION</h2>
<p>When function evaluations are prohibitively expensive, cheap approximations may be useful. In such situations high fidelity data obtained through experimentation might be augmented by low fidelity data obtained through running a simulation. For example, during alloy design, simulation software can predict the alloy strength but results may be less
accurate than measurements obtained from casting experiments. Multi-fidelity Bayesian optimisation has been demonstrated in [113], [114]. Recently, [115] proposed BO for an optimisation problem with multi-fidelity data. Although multi-fidelity approach has been applied in problem-specific context or non-optimisation related tasks [41], [116]-[120], the method of [115] generalises well for BO problems.</p>
<h2>G. MIXED-TYPE INPUT</h2>
<p>Experimental parameters are often combinations of different types: continuous, discrete, categorical, and binary. Incorporation of mixed type input is challenging across the domains, including simpler methods such as Latin hypercube sampling [11]. Non-continuous variables are problematic in BO because the objective function approximation with GP assumes continuous input space, with covariance functions defining the relationship between these continuous variables. One common way to deal with discrete variables is to round the value to a close integer [40], but this approach leads to sub-optimal optimisation [121].</p>
<p>Two options for handling mixed-type inputs are: (1) designing kernels that are suitable for different variables, and (2) subsampling of data for maximising the objective function, which is especially useful in higher dimensional space. For integer variables the problem can be solved through kernel transformation, by assuming the objective function to be flat for the region where two continuous variables would be rounded to the same integer [121]. In [67] categorical variables are included by one-hot-encoding alongside numerical variables. A specialised kernel for categorical variables is proposed in [122].</p>
<p>Random forest regression is a good alternative to GP for regression in a sequential model-based algorithm configuration (SMAC, [44]). Random forests are good at exploitation but don't perform well for exploration as they may not predict well at points that are distant from observations. Additionally, a non-differentiable response surface renders it unsuitable for gradient-based optimisation.</p>
<h2>IV. DISCUSSION</h2>
<p>Machine-learning methods through Bayesian optimisation offer a powerful way to deal with many problems of experimental optimization that have not been previously addressed. While techniques exist for different issues (high dimensionality, multi-objective, etc.), few works solve multiple issues in a general way. Methods are likely to be composable where no incompatible changes are required to the BO process. Figure 5 outlines composability based on the current repertoire of Bayesian optimisation algorithms. When a design problem is single objective, has single fidelity measurement, and all the variables are continuous then it offers the greatest flexibility in terms of adding specific capability such as transfer learning or high dimensional optimisation. Other cases require careful selection of algorithms to add desired capabilities. For example, the method of [111] handles multiple objectives with constraints, and the method</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIGURE 5. Current capability graph on the composability of various aspects of experimental design problems in Bayesian optimisation. It is possible to compose algorithms which lie on a path in the graph. It is possible to finish at any block and even skip multiple blocks on a path. Regular text denotes the capability achievable with standard Bayesian optimisation, whereas highlighted text denotes the existence of specialised algorithms.</p>
<p>of [43] handles parallel evaluation in high dimensions with mixed type inputs. Some combinations may not even be possible, for example, Random Forest based algorithm such as [44] would not admit many capabilities. Note that this graph does not portray any theoretical limitations, but merely presents a gist of the current capability through the lens of composability.</p>
<p>Several open-source libraries are available for incorporating BO into computer programs. Depending on the application, computation speed may be an issue. A common operation in most algorithms is Cholesky decomposition which is used to invert the kernel matrix and is generally $O\left(n^{3}\right)$ for $n$ data points, but with care this can be calculated incrementally as new points arrive, reducing the complexity to $O\left(n^{2}\right)$ [123]. Several algorithms gain speed-up by implementing part of the algorithm on a GPU, which can be up to 100 times faster than the equivalent single-threaded code [124].</p>
<ul>
<li>GPyOpt (https://github.com/SheffieldML/GPyOpt) is a Bayesian optimisation framework, written in Python and supporting parallel optimisation, mixed factor types (continuous, discrete, and categorical), and inequality constraints.</li>
<li>GPflowOpt (https://github.com/GPflow/GPflowOpt) is written in Python and uses TensorFlow (https://www.tensorflow.org) to accelerate computation on GPU hardware. It supports multi-objective acquisition functions, and black-box constraints [125].</li>
<li>DiceOptim (https://cran.r-project.org/web/packages/DiceOptim/index.html) is a BO package written in R. Mixed equality and inequality constraints are implemented using the method of [89], and parallel optimisation is via multipoint EI [91], however parallel and constraints cannot be mixed in a single optimisation.</li>
<li>MOE (https://github.com/Yelp/MOE) supports parallel optimisation via multi-point stochastic gradient ascent [124]. Interfaces are provided for Python and $\mathrm{C}++$, and optimisation can be accelerated on GPU hardware.</li>
<li>SigOpt (http://sigopt.com) offers Bayesian optimisation as a web service. The implementation is based on MOE, but includes some enhancements such as mixed factor types (continuous, discrete, categorical), and automatic hyperparameter tuning.</li>
<li>BayesOpt (https://github.com/rmcantin/bayesopt) is written in $\mathrm{C}++$, and includes common interfaces for C , C++, Python, Matlab, and Octave [123].</li>
</ul>
<h2>V. CONCLUSION</h2>
<p>This review has presented an overview of Bayesian optimisation (BO) with application to experimental design. BO was introduced in relation to existing Design of Experiments (DOE) methods such as factorial designs, response surface methodology, and adaptive sampling. A brief discussion of the theory highlighted the roles of the Gaussian process, kernel, and acquisition function. A set of seven core issues was identified as being important in practical experimental designs, and some detailed solutions were reviewed. These core issues are: (1) the incorporation of prior knowledge, (2) high dimensional optimisation, (3) constraints, (4) batch evaluation, (5) multiple objectives, (6) multi-fidelity data, and (7) mixed variable types.</p>
<p>Recent works have shown the potential of Bayesian optimisation in fields such as robotics, neuroscience, and materials discovery. As the range of potential applications expands, it is increasingly unlikely that "vanilla" optimisation approaches for small numbers of unconstrained, continuous variables will be appropriate. This is particularly true in DACE simulation applications where high dimensional mixed-type inputs are typical.</p>
<p>Bayesian optimisation offers a powerful and rigorous framework for exploring and optimising expensive "black box" functions. While solutions exist for the core issues in experimental design, each approach has strengths and weaknesses that could potentially be improved, and the combination of the individual solutions is not necessarily straightforward. Thus there is a need for ongoing work in this area to: (1) improve the efficiency, generality, and scalability of approaches to the core issues, (2) develop designs that allow easy combination of multiple approaches, and (3) develop theoretical guarantees on the performance of solutions.</p>
<h2>REFERENCES</h2>
<p>[1] R. A. Fisher, The Design of Experiments. Edinburgh, U.K.: Oliver \&amp; Boyd, 1935.
[2] D. C. Montgomery, Design and Analysis of Experiments. Hoboken, NJ, USA: Wiley, 2017.
[3] R. H. Myers, D. C. Montgomery, and C. M. Anderson-Cook, "Response surface methodology: Process and product optimization using designed experiments," in Applied Probability \&amp; Statistics (Wiley Series in Probability and Statistics), 2009.
[4] G. G. Wang and S. Shan, "Review of metamodeling techniques in support of engineering design optimization," J. Mech. Des., vol. 129, no. 4, pp. 370-380, Apr. 2007.
[5] W. C. Parr, "Introduction to quality engineering: Designing quality into products and processes," Technometrics, vol. 31, no. 2, pp. 255-256, May 1989.
[6] T. Hasenkamp, M. Arvidsson, and I. Gremyr, "A review of practices for robust design methodology," J. Eng. Design, vol. 20, no. 6, pp. 645-657, Dec. 2009.
[7] S. M. Göhler and T. J. Howard, "A framework for the application of robust design methods and tools," in Proc. 1st Int. Symp. Robust Design, T. J. Howard and T. Eifler, Eds. Lyngby, Denmark: Technical Univ. of Denmark, 2014, pp. 123-133.
[8] D. C. Woods and S. M. Lewis, "Design of experiments for screening," in Handbook Uncertainty Quantification. 2017, pp. 1143-1185.
[9] A. Dean and S. Lewis, Eds., Screening: Methods for Experimentation in Industry, Drug Discovery, and Genetics. New York, NY, USA: SpringerVerlag, 2006.
[10] F. A. Viana, "Things you wanted to know about the Latin hypercube design and were afraid to ask," in Proc. 10th World Congr. Struct. Multidisciplinary Optim., 2013, pp. 1-9.
[11] H. Vieira, S. M. Sanchez, K. H. Kienitz, and M. C. N. Belderrain, "Efficient, nearly orthogonal-and-balanced, mixed designs: An effective way to conduct trade-off analyses via simulation," J. Simul., vol. 7, no. 4, pp. 264-275, Nov. 2013.
[12] G. E. P. Box and K. B. Wilson, "On the experimental attainment of optimum conditions," J. Roy. Stat. Soc. B, Methodol., vol. 13, no. 1, pp. 1-45, 1951.
[13] J. L. Chapman, L. Lu, and C. M. Anderson-Cook, "Process optimization for multiple responses utilizing the Pareto front approach," Qual. Eng., vol. 26, no. 3, pp. 253-268, Jul. 2014.
[14] R. H. Myers, D. C. Montgomery, G. G. Vining, C. M. Borror, and S. M. Kowalski, "Response surface methodology: A retrospective and literature survey," J. Qual. Technol., vol. 36, no. 1, pp. 53-77, Jan. 2004.
[15] J. Sacks, W. J. Welch, T. J. Mitchell, and H. P. Wynn, "Design and analysis of computer experiments," Stat. Sci., vol. 4, no. 4, pp. 409-423, 1989.
[16] H. Liu, Y.-S. Ong, and J. Cai, "A survey of adaptive sampling for global metamodeling in support of simulation-based complex engineering design," Struct. Multidisciplinary Optim., vol. 57, no. 1, pp. 393-416, Jan. 2018.
[17] F. A. Viana, T. W. Simpson, V. Balabanov, and V. Toropov, "Special section on multidisciplinary design optimization: Metamodeling in multidisciplinary design optimization: How far have we really come?" AIAA J., vol. 52, no. 4, pp. 670-690, 2014.
[18] S. S. Garud, I. A. Karimi, and M. Kraft, "Design of computer experiments: A review," Comput. Chem. Eng., vol. 106, pp. 71-95, Nov. 2017.
[19] D. R. Jones, M. Schonlau, and W. J. Welch, "Efficient global optimization of expensive black-box functions," J. Global Optim., vol. 13, no. 4, pp. 455-492, 1998.
[20] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas, "Taking the human out of the loop: A review of Bayesian optimization," Proc. IEEE, vol. 104, no. 1, pp. 148-175, Jan. 2016.
[21] E. Brochu, V. M. Cora, and N. de Freitas, "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning," 2010, arXiv:1012.2599. [Online]. Available: https://arxiv.org/abs/1012.2599
[22] P. I. Frazier, "A tutorial on Bayesian optimization," 2018, arXiv:1807.02811. [Online]. Available: https://arxiv.org/abs/1807.02811
[23] D. Xue, P. V. Balachandran, J. Hogden, J. Theiler, D. Xue, and T. Lookman, "Accelerated search for materials with targeted properties by adaptive design," Nature Commun., vol. 7, p. 11241, Apr. 2016.
[24] A. Vahid, S. Rana, S. Gupta, P. Vellanki, S. Venkatesh, and T. Dorin, "New Bayesian-optimization-based design of high-strength 7xxx-series alloys from recycled aluminum," J. Minerals. Metals Mater. Soc., vol. 70, no. 11, pp. 2704-2709, Nov. 2018.
[25] C. Li, D. R. de Celis Leal, S. Rana, S. Gupta, A. Sutti, S. Greenhill, T. Slezak, M. Height, and S. Venkatesh, "Rapid Bayesian optimisation for synthesis of short polymer fiber materials," Sci. Rep., vol. 7, no. 1, p. 5683, 2017.
[26] S. Ju, T. Shiga, L. Feng, Z. Hou, K. Tsuda, and J. Shiomi, "Designing nanostructures for phonon transport via Bayesian optimization," Phys. Rev. X, vol. 7, no. 2, 2017, Art. no. 021024.
[27] H. Abdelrahman, F. Berkenkamp, J. Poland, and A. Krause, "Bayesian optimization for maximum power point tracking in photovoltaic power plants," in Proc. Eur. Control Conf. (ECC), Jun. 2016, pp. 2078-2083.
[28] S. Rikuchi, H. Oda, S. Kiyohara, and T. Mizoguchi, "Bayesian optimization for efficient determination of metal oxide grain boundary structures," Phys. B, Condens. Matter, vol. 532, pp. 24-28, Mar. 2018.
[29] M. M. Khajah, B. D. Roads, R. V. Lindsey, Y.-E. Liu, and M. C. Mozer, "Designing engaging games using Bayesian optimization," in Proc. CHI Conf. Hum. Factors Comput. Syst. (CHI), 2016, pp. 5571-5582.
[30] R. Lorenz, I. R. Violante, R. P. Monti, G. Montana, A. Hampshire, and R. Leech, "Dissociating frontoparietal brain networks with neuroadaptive Bayesian optimization," Nature Commun., vol. 9, no. 1, p. 1227, 2018.
[31] H. J. Kushner, "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise," J. Basic Eng., vol. 86, no. 1, pp. 97-106, Mar. 1964.
[32] J. Mockus, V. Tiesis, and A. Zilinskas, "The application of Bayesian methods for seeking the extremum," in Toward Global Optimization 2, L. C. W. Dixon and G. P Szego, Eds. Amsterdam, The Netherlands: North Holland, 1978.
[33] N. Srinivas, A. Krause, S. Kakade, and M. W. Seeger, "Gaussian process optimization in the bandit setting: No regret and experimental design," in Proc. Int. Conf. Mach. Learn., 2010, pp. 1015-1022.
[34] D. R. Jones, C. D. Portnunen, and B. E. Stuckman, "Lipschitzian optimization without the Lipschitz constant," J. Optim. Theory Appl., vol. 79, no. 1, pp. 157-181, 1993.
[35] D. C. Liu and J. Nocedal, "On the limited memory BFGS method for large scale optimization," Math. Program., vol. 45, nos. 1-3, pp. 503-528, 1989.
[36] M. J. Powell, "A view of algorithms for optimization without derivatives," Math. Today-Bull. Inst. Math. Appl., vol. 43, no. 5, pp. 170-174, 2007.
[37] T. P. Runarsson and X. Yao, "Stochastic ranking for constrained evolutionary optimization," IEEE Trans. Evol. Comput., vol. 4, no. 3, pp. 284-294, Sep. 2000.
[38] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, "Algorithms for hyperparameter optimization," in Proc. Adv. Neural Inf. Process. Syst., 2011, pp. 2546-2554.
[39] C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown, "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms," in Proc. Int. Conf. Knowl. Discovery Data Mining, 2013, pp. 847-855.
[40] J. Snoek, H. Larochelle, and R. P. Adams, "Practical Bayesian optimization of machine learning algorithms," in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 2960-2968.
[41] K. Swersky, J. Snoek, and R. P. Adams, "Multi-task Bayesian optimization," in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 2004-2012.
[42] P. Baldi, P. Sadowski, and D. Whiteson, "Enhanced Hggs boson to r+ r-search with deep learning," Phys. Rev. Lett., vol. 114, no. 11, 2015, Art. no. 111801.
[43] Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas, "Bayesian optimization in high dimensions via random embeddings," in Proc. Int. Joint Conf. Artif. Intell., Jul. 2013, pp. 1778-1784.
[44] F. Hutter, H. H. Hoos, and K. Leyton-Brown, "Sequential model-based optimization for general algorithm configuration," in Proc. Int. Conf. Learn. Intell. Optim. Berlin, Germany: Springer, 2011, pp. 507-523.
[45] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, "Robots that can adapt like animals," Nature, vol. 521, no. 7553, pp. 503-507, May 2015.
[46] R. Calandra, A. Seyfarth, J. Peters, and M. P. Deisenroth, "Bayesian optimization for learning gaits under uncertainty," Ann. Math. Artif. Intell., vol. 76, nos. 1-2, pp. 5-23, Feb. 2016.
[47] J. Lancaster, R. Lorenz, R. Leech, and J. H. Cole, "Bayesian optimisation for neuroimaging pre-processing in brain age classification and prediction," Frontiers Aging Neurosci., vol. 10, p. 28, Feb. 2018.
[48] R. Lorenz, R. P. Monti, I. R. Violante, C. Anagnostopoulos, A. A. Faisal, G. Montana, and R. Leech, "The automatic neuroscientist: A framework for optimizing experimental design with closed-loop real-time (MRI," NeuroImage, vol. 129, pp. 320-334, Apr. 2016.</p>
<p>[49] T. Ueno, T. D. Rhone, Z. Hou, T. Mizoguchi, and K. Tsuda, "COMBO: An efficient Bayesian optimization library for materials science," Mater. Discovery, vol. 4, pp. 18-21, Jun. 2016.
[50] T. Lookman, P. V. Balachandran, D. Xue, J. Hogden, and J. Theiler, "Statistical inference and adaptive design for materials discovery," Current Opinion Solid State Mater. Sci., vol. 21, no. 3, pp. 121-128, Jun. 2017.
[51] R. Gómez-Bombarelli et al., "Design of efficient molecular organic lightemitting diodes by a high-throughput virtual screening and experimental approach," Nature Mater., vol. 15, no. 10, pp. 1120-1127, Oct. 2016.
[52] P. I. Frazier and J. Wang, "Bayesian optimization for materials design," in Proc. Inf. Sci. Mater. Discovery Design. Cham, Switzerland: Springer, 2016, pp. 45-75.
[53] A. Seko, T. Maekawa, K. Tsuda, and I. Tanaka, "Machine learning with systematic density-functional theory calculations: Application to melting temperatures of single-and binary-component solids," Phys. Rev. B. Condens. Matter, vol. 89, no. 5, 2014, Art. no. 054303.
[54] A. Seko, A. Togo, H. Hayashi, K. Tsuda, L. Chaput, and I. Tanaka, "Discovery of low thermal conductivity compounds with firstprinciples anharmonic lattice dynamics calculations and Bayesian optimization," 2015, arXiv:1506.06439. [Online]. Available: https://arxiv.org/abs/1506.06439
[55] A. Seko, A. Togo, H. Hayashi, K. Tsuda, L. Chaput, and I. Tanaka, "Prediction of low-thermal-conductivity compounds with first-principles anharmonic lattice-dynamics calculations and Bayesian optimization," Phys. Rev. Lett., vol. 115, no. 20, 2015, Art. no. 205901.
[56] D. Packwood, Bayesian Optimization for Materials Science. Singapore: Springer, 2017.
[57] D. Yogatama and G. Mann, "Efficient transfer learning method for automatic hyperparameter tuning," in Proc. 17th Int. Conf. Artif. Intell. Statist. (ADJATS), Reykjavik, Iceland, Apr. 2014, pp. 1077-1085.
[58] T. T. Joy, S. Rana, S. K. Gupta, and S. Venkatesh, "Flexible transfer learning framework for Bayesian optimisation," in Proc. Pacific-Asia Conf. Knowl. Discovery Data Mining. Cham, Switzerland: Springer, 2016, pp. 102-114.
[59] A. Shilton, S. Gupta, S. Rana, and S. Venkatesh, "Regret bounds for transfer learning in Bayesian optimisation," in Proc. Artif. Intell. Statist., 2017, pp. 307-315.
[60] R. Bardenet, M. Brendel, B. Kégl, and M. Sebag, "Collaborative hyperparameter tuning," in Proc. 30th Int. Conf. Mach. Learn. (ICML), Atlanta, GA, USA, Jun. 2013, pp. 199-207.
[61] J. Riihimaki and A. Vehtari, "Gaussian processes with monotonicity information," in Proc. 13th Int. Conf. Artif. Intell. Statist., 2010, pp. 645-652.
[62] M. Jauch and V. Peña, "Bayesian optimization with shape constraints," 2016, arXiv:1612.08915. [Online]. Available: https://arxiv.org /abs/1612.08915
[63] C. Li, S. Rana, S. Gupta, V. Nguyen, and S. Venkatesh, "Bayesian optimization with monotonicity information," in Proc. 31st Conf. Neural Inf. Process. Syst. (NIPS), 2017.
[64] P. J. Lenk and T. Choi, "Bayesian analysis of shape-restricted functions using Gaussian process priors," Statistica Sinica, vol. 27, pp. 43-69, Jan. 2017.
[65] M. R. Andersen, E. Siivola, and A. Vehtari, "Bayesian optimization of unimodal functions," in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2017.
[66] A. Ramachandran, S. K. Gupta, R. Santu, and S. Venkatesh, "Information-theoretic transfer learning framework for Bayesian optimisation," in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases. Cham, Switzerland: Springer, 2018.
[67] R. Jenatton, C. Archambeau, J. González, and M. Seeger, "Bayesian optimization with tree-structured dependencies," in Proc. Int. Conf. Mach. Learn., 2017, pp. 1655-1664.
[68] K. Swersky, D. Duvenaud, J. Snoek, F. Hutter, and M. A. Osborne, "Raiders of the lost architecture: Kernels for Bayesian optimization in conditional parameter spaces," 2014, arXiv:1409.4011. [Online]. Available: https://arxiv.org/abs/1409.4011
[69] D. K. Duvenaud, H. Nickisch, and C. E. Rasmussen, "Additive Gaussian processes," in Proc. Adv. Neural Inf. Process. Syst., 2011, pp. 226-234.
[70] K. Kandasamy, J. G. Schneider, and B. Póczos, "High dimensional Bayesian optimisation and bandits via additive models," in Proc. 32nd Int. Conf. Mach. Learn. (ICML), Lille, France, Jul. 2015, pp. 295-304.
[71] F. Hutter and M. A. Osborne, "A kernel for hierarchical parameter spaces," 2013, arXiv:1310.5738. [Online]. Available: https://arxiv.org/abs/1310.5738
[72] T. Dai Nguyen, S. Gupta, S. Rana, V. Nguyen, S. Venkatesh, K. J. Deane, and P. G. Sanders, Cascade Bayesian Optimization. 2016, pp. 268-280.
[73] S. Rana, C. Li, S. Gupta, V. Nguyen, and S. Venkatesh, "High dimensional Bayesian optimization with elastic Gaussian process," in Proc. Int. Conf. Mach. Learn., 2017, pp. 2883-2891.
[74] C. Li, S. Gupta, S. Rana, V. Nguyen, S. Venkatesh, and A. Shilton, "High dimensional Bayesian optimization using dropout," in Proc. 26th Int. Joint Conf. Artif. Intell., 2017, pp. 2096-2102.
[75] C. Oh, E. Gavves, and M. Welling, "BOCK: Bayesian optimization with cylindrical kernels," in Proc. 35th Int. Conf. Mach. Learn., J. Dy and A. Krause, Eds. Stockholm, Sweden: Stockholmsmässan, 2018, pp. 3868-3877.
[76] C.-L. Li, K. Kandasamy, B. Póczos, and J. Schneider, "High dimensional Bayesian optimization via restricted projection pursuit models," in Proc. Artif. Intell. Statist., 2016, pp. 884-892.
[77] Z. Wang, C. Li, S. Jegelka, and P. Kohli, "Batched high-dimensional Bayesian optimization via structural kernel learning," in Proc. Int. Conf. Mach. Learn., 2017.
[78] J. Gardner, C. Guo, K. Weinberger, R. Garnett, and R. Grosse, "Discovering and exploiting additive structure for Bayesian optimization," in Proc. Artif. Intell. Statist., 2017, pp. 1311-1319.
[79] A. Nayebi, A. Munteanu, and M. Poloczek, "A framework for Bayesian optimization in embedded subspaces," in Proc. Int. Conf. Mach. Learn., 2019, pp. 4752-4761.
[80] M. Mutny and A. Krause, "Efficient high dimensional Bayesian optimization with additivity and quadrature Fourier features," in Proc. Adv. Neural Inf. Process. Syst., 2018, pp. 9005-9016.
[81] J. Kirschner, M. Mutny, N. Hiller, R. Ischebeck, and A. Krause, "Adaptive and safe Bayesian optimization in high dimensions via one-dimensional subspaces," in Proc. Int. Conf. Mach. Learn., 2019, pp. 3429-3438.
[82] J. Djolonga, A. Krause, and V. Cevher, "High-dimensional Gaussian process bandits," in Proc. Adv. Neural Inf. Process. Syst. 27th Annu. Conf. Neural Inf. Process. Syst., Lake Tahoe, NV, USA, 2013, pp. 1025-1033.
[83] M. A. Gelbart, J. Snoek, and R. P. Adams, "Bayesian optimization with unknown constraints," in Proc. Uncertainty Artif. Intell., 2014, pp. 250-259.
[84] J. R. Gardner, M. J. Kusner, Z. E. Xu, K. Q. Weinberger, and J. P. Cunningham, "Bayesian optimization with inequality constraints," in Proc. Int. Conf. Mach. Learn., 2014, pp. 937-945.
[85] S. Artafar, J. Coll-Font, D. Brooks, and J. Dy, "ADMMBO: Bayesian optimization with unknown constraints using ADMM," J. Mach. Learn. Res., vol. 20, no. 123, pp. 1-26, 2019.
[86] J. M. Hernández-Lobato, M. Gelbart, M. Hoffman, R. Adams, and Z. Ghahramani, "Predictive entropy search for Bayesian optimization with unknown constraints," in Proc. Int. Conf. Mach. Learn., 2015, pp. 1699-1707.
[87] E. C. Garrido-Merchán and D. Hernández-Lobato, "Predictive entropy search for multi-objective Bayesian optimization with constraints," Neurocomputing, vol. 361, pp. 50-68, Oct. 2019.
[88] R. Lam and K. Willcox, "Lookahead Bayesian optimization with inequality constraints," in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 1888-1898.
[89] V. Picheny, R. B. Gramacy, S. Wild, and S. Le Digabel, "Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian," in Proc. Adv. Neural Inf. Process. Syst., 2016, pp. 1435-1443.
[90] M. Abdolshah, A. Shilton, S. Rana, S. Gupta, and S. Venkatesh, "Multiobjective Bayesian optimisation with preferences over objectives," in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2019.
[91] D. Ginsbourger, R. Le Riche, and L. Carraro, "A multi-points criterion for deterministic parallel global optimization based on Gaussian processes," Département Méthodes et Modèles Mathématiques pour l'Industrie, 3MIENSMSE, Saint-Étienne, France, Tech. Rep. hal-00260579, 2008.
[92] J. Azimi, A. Fern, and X. Z. Fern, "Batch Bayesian optimization via simulation matching," in Proc. Adv. Neural Inf. Process. Syst., 2010, pp. 109-117.
[93] T. Desautels, A. Krause, and J. W. Burdick, "Parallelizing explorationexploitation tradeoffs in Gaussian process bandit optimization," J. Mach. Learn. Res., vol. 15, no. 1, pp. 3873-3923, 2014.
[94] J. González, Z. Dai, P. Hennig, and N. D. Lawrence, "Batch Bayesian optimization via local penalization," in Proc. Artif. Intell. Statist., 2015, pp. 648-657.
[95] V. Nguyen, S. Rana, S. K. Gupta, C. Li, and S. Venkatesh, "Budgeted batch Bayesian optimization," in Proc. IEEE 16th Int. Conf. Data Mining (ICDM), Dec. 2016, pp. 1107-1112.</p>
<p>[96] C. Gong, J. Peng, and Q. Liu, "Quantile stein variational gradient descent for batch Bayesian optimization," in Proc. Int. Conf. Mach. Learn., 2019, pp. 2347-2356.
[97] E. Contal, D. Buffoni, A. Robicquet, and N. Vayatis, "Parallel Gaussian process optimization with upper confidence bound and pure exploration," in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases. Berlin, Germany: Springer, 2013, pp. 225-240.
[98] S. Gupta, A. Shilton, S. Rana, and S. Venkatesh, "Exploiting strategyspace diversity for batch Bayesian optimization," in Proc. Int. Conf. Artif. Intell. Statist., 2018, pp. 538-547.
[99] T. T. Joy, S. Rana, S. Gupta, and S. Venkatesh, "Batch Bayesian optimization using multi-scale search," Knowl.-Based Syst., vol. 187, Jan. 2020, Art. no. 104818.
[100] A. Shah and Z. Ghahramani, "Parallel predictive entropy search for batch global optimization of expensive objective functions," in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 3330-3338.
[101] J. Wu and P. Frazier, "The parallel knowledge gradient method for batch Bayesian optimization," in Proc. Adv. Neural Inf. Process. Syst., 2016, pp. 3126-3134.
[102] P. Vellanki, S. Rana, S. Gupta, D. Rubin, A. Sutti, T. Dorin, M. Height, P. Sanders, and S. Venkatesh, "Process-constrained batch Bayesian optimization," in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3417-3426.
[103] S. Shan and G. G. Wang, "Survey of modeling and optimization strategies to solve high-dimensional design problems with computationallyexpensive black-box functions," Struct. Multidisciplinary Optim., vol. 41, no. 2, pp. 219-241, Mar. 2010.
[104] A. M. Gopakumar, P. V. Balachandran, D. Xue, J. E. Gubernatis, and T. Lookman, "Multi-objective optimization for materials discovery via adaptive design," Sci. Rep., vol. 8, no. 1, p. 3738, 2018.
[105] Y. Collette and P. Siarry, Multiobjective Optimization: Principles and Case Studies. Berlin, Germany: Springer-Verlag, 2013.
[106] J. Knowles, "ParEGO: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems," IEEE Trans. Evol. Comput., vol. 10, no. 1, pp. 50-66, Feb. 2006.
[107] W. Ponweiser, T. Wagner, D. Biermann, and M. Vincze, "Multiobjective optimization on a limited budget of evaluations using model-assisted $\mathcal{S}$-metric selection," in Proc. Int. Conf. Parallel Problem Solving Nature. Berlin, Germany: Springer, 2008, pp. 784-794.
[108] M. Emmerich and J.-W. Klinkenberg, "The computation of the expected improvement in dominated hypervolume of Pareto front approximations," Rapport Technique, Leiden Univ., Leiden, The Netherlands, Tech. Rep. LIACS-TR 9-2008, 2008.
[109] V. Picheny, "Multiobjective optimization using Gaussian process emulators via stepwise uncertainty reduction," Statist. Comput., vol. 25, no. 6, pp. 1265-1280, Nov. 2015.
[110] D. Hernández-Lobato, J. Hernandez-Lobato, A. Shah, and R. Adams, "Predictive entropy search for multi-objective Bayesian optimization," in Proc. Int. Conf. Mach. Learn., 2016, pp. 1492-1501.
[111] P. Feliot, J. Bect, and E. Vazquez, "A Bayesian approach to constrained single-and multi-objective optimization," J. Global Optim., vol. 67, nos. 1-2, pp. 97-133, 2017.
[112] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, "Distributed optimization and statistical learning via the alternating direction method of multipliers," Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1-122, 2011.
[113] P. Perdikaris and G. E. Karniadakis, "Model inversion via multi-fidelity Bayesian optimization: A new paradigm for parameter estimation in haemodynamics, and beyond," J. Roy. Soc. Interface, vol. 13, no. 118, p. 20151107, 2016.
[114] A. Marco, F. Berkenkamp, P. Hennig, A. P. Schoellig, A. Krause, S. Schaal, and S. Trimpe, "Virtual vs. real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization," in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2017, pp. 1557-1563.
[115] K. Kandasamy, G. Dasarathy, J. Schneider, and B. Poczos, "Multi-fidelity Bayesian optimisation with continuous approximations," in Proc. Int. Conf. Mach. Learn., 2017.
[116] A. Klein, S. Bartels, S. Falkner, P. Hennig, and F. Hutter, "Towards efficient Bayesian optimization for big data," in Proc. NIPS Workshop Bayesian Optim. (BayesOpt), vol. 134, 2015, p. 98.
[117] M. Poloczek, J. Wang, and P. Frazier, "Multi-information source optimization," in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 4291-4301.
[118] M. Cutler, T. J. Walsh, and J. P. How, "Reinforcement learning with multifidelity simulators," in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2014, pp. 3888-3895.
[119] A. Sabharwal, H. Samulowitz, and G. Tesauro, "Selecting nearoptimal learners via incremental data allocation," in Proc. AAAI, 2016, pp. 2007-2015.
[120] C. Zhang and K. Chaudhuri, "Active learning from weak and strong labelers," in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 703-711.
[121] E. C. Garrido-Merchán and D. Hernández-Lobato, "Dealing with Categorical and Integer-valued Variables in Bayesian optimization with Gaussian processes," 2017, arXiv:1706.03673. [Online]. Available: https://arxiv.org/abs/1805.03463
[122] M. A. Villegas García, "An investigation into new kernels for categorical variables," M.S. thesis, Departament de Llenguatges i Sistemes Informàtics, Universitat Politècnica de Catalunya, Barcelona, Spain, 2013.
[123] R. Martinez-Cantin, "BayesOpt: A Bayesian optimization library for nonlinear optimization, experimental design and bandits," J. Mach. Learn. Res., vol. 15, no. 1, pp. 3735-3739, 2014.
[124] J. Wang, S. C. Clark, E. Liu, and P. I. Frazier, "Parallel Bayesian global optimization of expensive functions," 2016, arXiv:1602.05149. [Online]. Available: https://arxiv.org/abs/1602.05149
[125] N. Knudde, J. van der Herten, T. Dhaene, and I. Couckuyt, "GPflow: A Gaussian process library using TensorFlow," 2017, arXiv:1711.03845. [Online]. Available: https://arxiv.org/abs/1610.08733
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>STEWART GREENHILL received the B.Sc. degree in computer science from the University of Western Australia, in 1987, and the Ph.D. degree in environmental science from Murdoch University, in 1992. He is currently a Research Fellow with the Applied Artificial Intelligence Institute, Deakin University, Australia. His research interests include machine learning, signal processing, embedded systems, software engineering, visualization, and interaction design.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>SANTU RANA is currently a Researcher in the field of machine learning and computer vision with the Applied Artificial Intelligence Institute, Deakin University, Australia. His research in high-dimensional Bayesian optimization has been applied to efficiently design alloys with large number of elements. He has been actively conducting research in Bayesian experimental design with applications in advanced manufacturing. In the last four years, he has published more than 40 research articles improving various aspects of Bayesian optimization algorithm. Altogether, he has published over 79 research articles, including 14 refereed journal articles, 58 fully refereed conference proceedings, and seven workshop articles, with over 515 citations and an H-index of 12. He is also a Co-Inventor of two patents. His broad research interests lie in devising practical machine learning algorithms for various tasks, such as object recognition, mathematical optimization, and healthcare data modeling.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>SUNIL GUPTA is currently a Researcher in the field of machine learning and data mining with the Applied Artificial Intelligence Institute, Deakin University, Australia. He has published over 100 research articles, including two book chapters, 25 refereed journal articles, 70 fully refereed conference proceedings, and nine workshop articles with over 1000 citations and an H-index of 17. His research interest lies in developing data-driven models for real world processes and phenomena covering both big-data and small-data problems. His recent research in optimization using small data (Bayesian optimization) has found applications in efficient experimental design of products and processes in advanced manufacturing, such as alloy design with certain target properties, design of short nanofibers with appropriate length and thickness, and optimal setting of parameters in 3d-printers. His research has won several best paper awards in the field of data mining and machine learning. He is also a Co-Inventor of a patent related to experimental design. He regularly serves at technical program committees of the prestigious machine learning conferences.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>PRATIBHA VELLANKI graduated from Pune University in electronics and telecommunication, in 2009, where she continued to pursue the master's degree in signal processing, in 2011. She received the Ph.D. degree from Deakin University, Australia. She worked as an Associate Research Fellow in the field of applied machine learning with Deakin University, from 2012 to 2018. She is interested in taking on a machine learning perspective on research problems that affect people and health. She currently works as a Data Scientist for the Office for National Statistics, U.K. This work was done during her time with Deakin University.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>SVETHA VENKATESH is currently an ARC Australian Laureate Fellow, the Alfred Deakin Professor, and the Co-Director of the Applied Artificial Intelligence Institute (A2I2), Deakin University, Australia. She and her team have tackled a wide range of problems of societal significance, including the critical areas of autism, security, and aged care. The outcomes have impacted the community and evolved into publications, patents, tools, and spin-off companies. This includes more than 600 publications, three full patents, start-up companies (iCetana), and a significant product (TOBY Playpad).</p>
<p>Prof. Venkatesh was elected as a Fellow of the International Association of Pattern Recognition, in 2004, for contributions to formulation and extraction of semantics in multimedia data, and the Australian Academy of Technological Sciences and Engineering, in 2006. In 2017, she was appointed as an Australian Laureate Fellow, the highest individual award the Australian Research Council can bestow.</p>            </div>
        </div>

    </div>
</body>
</html>
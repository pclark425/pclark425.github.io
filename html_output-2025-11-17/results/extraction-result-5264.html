<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5264 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5264</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5264</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-6f121e4188d1635568c0b73627c0acf6b251554a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6f121e4188d1635568c0b73627c0acf6b251554a" target="_blank">MatterGen: a generative model for inorganic materials design</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5264",
    "paper_id": "paper-6f121e4188d1635568c0b73627c0acf6b251554a",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00472125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MatterGen: a generative model for inorganic materials design</h1>
<p>Claudio Zeni ${ }^{1 \dagger}$, Robert Pinsler ${ }^{1 \dagger}$, Daniel Zügner ${ }^{1 \dagger}$,<br>Andrew Fowler ${ }^{1 \dagger}$, Matthew Horton ${ }^{1 \dagger}$, Xiang Fu ${ }^{1}$,<br>Aliaksandra Shysheya ${ }^{1}$, Jonathan Crabbé ${ }^{1}$, Lixin Sun ${ }^{1}$,<br>Jake Smith ${ }^{1}$, Bichlien Nguyen ${ }^{1}$, Hannes Schulz ${ }^{1}$, Sarah Lewis ${ }^{1}$,<br>Chin-Wei Huang ${ }^{1}$, Ziheng Lu ${ }^{1}$, Yichi Zhou ${ }^{1}$, Han Yang ${ }^{1}$,<br>Hongxia Hao ${ }^{1}$, Jielan $\mathrm{Li}^{1}$, Ryota Tomioka ${ }^{1 * \dagger}$, Tian Xie ${ }^{1 * \dagger}$<br>${ }^{1}$ Microsoft Research AI4Science.</p>
<p>*Corresponding author(s). E-mail(s): ryoto@microsoft.com; tianxie@microsoft.com;
${ }^{\dagger}$ Equal contribution; non-corresponding authors are listed in random order.</p>
<h4>Abstract</h4>
<p>The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture [1-3]. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints [4-13]. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties.</p>
<p>Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design.</p>
<h1>1 Introduction</h1>
<p>The rate at which we can discover better materials has a major impact on the pace of technological innovation in areas such as carbon capture, semiconductor design, and energy storage. Traditionally, most novel materials have been found through experimentation and human intuition, which require long iteration cycles and are limited by the number of candidates that can be tested. Thanks to the advance of high throughput screening [14], open material databases [15-19], machine-learning-based property predictors [20, 21], and machine learning force fields (MLFFs) [22, 23], it has become increasingly common to screen hundreds of thousands of materials to identify promising candidates [24-27]. However, screening-based methods are still fundamentally limited by the number of known materials. The largest explorations of previously unknown crystalline materials are in the orders of $10^{6}-10^{7}$ materials [23, 27-29], which is only a tiny fraction of the number of potential stable inorganic compounds ( $10^{10}$ quaternary compounds only considering stoichiometry [30]). Moreover, these methods cannot be efficiently steered towards finding materials with target properties.</p>
<p>Given these limitations, there has been an enormous interest in the inverse design of materials [31-34]. The aim of inverse design is to directly generate material structures that satisfy possibly rare or even conflicting target property constraints, e.g., via generative models [4, 9, 13], evolutionary algorithms [35], and reinforcement learning [36]. Generative models are particularly promising since they have the potential to efficiently explore entirely new structures, yet they can also be flexibly adapted to different downstream tasks. Despite recent progress, current generative models often fall short of producing stable materials according to density functional theory (DFT) calculations [4, 5, 37, 38], are constrained by a narrow subset of elements [8, 10, 11], and/or can only optimize a very limited set of properties, mainly formation energy $[4,5,9,13,38-40]$.</p>
<p>In this study, we present MatterGen, a diffusion-based generative model for designing stable inorganic materials across the periodic table. MatterGen can further be fine-tuned via adapter modules to steer the generation towards materials with desired chemical composition, symmetry, and scalar property (e.g., band gap, bulk modulus, magnetic density) constraints. Compared to the previous state-of-the-art generative model for materials [4], MatterGen more than doubles the percentage of generated stable, unique, and novel (S.U.N.) materials, and generates structures that are more than 15 times closer to their ground-truth structures at the DFT local energy minimum. When fine-tuned, MatterGen often generates more S.U.N. materials in target chemical systems than well-established methods like substitution and random structure search (RSS), is capable of generating highly symmetric structures given the desired</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Inorganic materials design with MatterGen. (a) MatterGen generates stable materials by reversing a corruption process through iteratively denoising an initially random structure. The forward diffusion process is designed to independently corrupt atom types $\boldsymbol{A}$, coordinates $\boldsymbol{X}$, and the lattice $\boldsymbol{L}$ to approach a physically motivated distribution of random materials. (b) An equivariant score network is pretrained on a large dataset of stable material structures to jointly denoise atom types, coordinates, and the lattice. The score network is then fine-tuned with a labeled dataset through an adapter module that alters the model using the encoded property c. (c) MatterGen can be fine-tuned to steer the generation towards materials with desired chemistry, symmetry, and scalar property constraints.
space groups, and directly generates S.U.N. materials that satisfy target mechanical, electronic, and magnetic property constraints. Finally, we showcase MatterGen's ability to design materials given multiple property constraints by generating promising materials that have both high magnetic density and a chemical composition with low supply-chain risk.</p>
<h1>2 Results</h1>
<h3>2.1 Diffusion process for crystalline material generation</h3>
<p>In MatterGen, we introduce a novel diffusion process tailored for crystalline materials (Fig. 1(a)). Diffusion models generate samples by learning a score network to reverse a fixed corruption process [41-43]. Corruption processes for images typically add Gaussian noise but crystalline materials have unique periodic structures and symmetries which demand a customized diffusion process. A crystalline material can be defined by its repeating unit, i.e., its unit cell, which encodes the atom types $\boldsymbol{A}$ (i.e., chemical elements), coordinates $\boldsymbol{X}$, and periodic lattice $\boldsymbol{L}$ (Appendices A. 1 and A.2). We define a corruption process for each component that suits their own geometry and has physically motivated limiting noise distributions. More concretely, the coordinate diffusion respects the periodic boundary by employing a wrapped Normal distribution and approaches a uniform distribution at the noisy limit (Appendix A.6). The lattice</p>
<p>diffusion takes a symmetric form and approaches a distribution whose mean is a cubic lattice with average atomic density from the training data (Appendix A.7). The atom diffusion is defined in categorical space where individual atoms are corrupted into a masked state (Appendix A.5). Given the corrupted structure, we learn a score network that outputs equivariant scores for atom type, coordinate, and lattice, respectively, which removes the need to learn the symmetries from data (Appendix A.8). We refer to this network as the base model.</p>
<p>To generate materials with desired property constraints, we introduce adapter modules that can be used for fine-tuning the base model on an additional dataset with property labels (Fig. 1(b), more details in Appendix B). Fine-tuning is particularly appealing as it still works well if the labeled dataset is small compared to unlabeled structure datasets, which is often the case due to the high computational cost of calculating properties. The adapter modules are tunable components injected into each layer of the base model to alter its output depending on the given property label [44]. The resulting fine-tuned model is used in combination with classifier-free guidance [45] to steer the generation towards target property constraints. We apply this approach to multiple types of properties, producing a set of fine-tuned models that can generate materials with target chemical composition, symmetry, or scalar properties such as magnetic density (Fig. 1(c)).</p>
<h1>2.2 Generating stable, diverse materials</h1>
<p>We formulate learning a generative model for inverse materials design as a two-step process, where we first pre-train a general base model for generating stable, diverse crystals across the periodic table, and then we fine-tune this base model towards different downstream tasks. In this section, we focus on the ability of MatterGen's base model to generate stable, diverse materials, which we argue is a prerequisite for addressing any inverse materials design task. Since diversity is difficult to measure directly, we resort to quantifying MatterGen's ability to generate S.U.N. materials, and provide an additional analysis of the quality and diversity of generated structures. To train the base model, we curate a large, diverse dataset comprising 607,684 stable structures with up to 20 atoms recomputed from the Materials Project (MP) [15] and Alexandria [29, 46] datasets, which we refer to as Alex-MP-20. We consider a structure to be stable if its energy per atom after relaxation via DFT is below the 0.1 eV /atom threshold of a reference dataset comprising 1,081,850 unique structures recomputed from the MP, Alexandria, and Inorganic Crystal Structure Database (ICSD) datasets. We refer to this dataset as Alex-MP-ICSD. We consider a structure to be novel if it is not contained in Alex-MP-ICSD. We adopt these definitions throughout unless stated otherwise. More details are in Appendices C and D.3.1.</p>
<p>Fig. 2(a) shows several random samples generated by MatterGen, featuring typical coordination environments of inorganic materials; see Appendix D.3.2 for a more detailed analysis. To assess stability, we perform DFT calculations on 1024 generated structures. Fig. 2(b) shows that $78 \%$ of generated structures fall below the 0.1 eV /atom threshold ( $13 \%$ below 0.0 eV /atom) of MP's convex hull, while $75 \%$ fall below the 0.1 eV /atom threshold ( $3 \%$ below 0.0 eV /atom) of the combined AlexMP-ICSD hull (Fig. 2(b)). Further, $95 \%$ of generated structures have an RMSD w.r.t.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Generating stable, unique and novel inorganic materials. (a) Visualization of four randomly selected crystals generated by MatterGen, with corresponding chemical formula and space group symbols. (b) Distribution of energy above the hull using MP and Alex-MP-ICSD dataset as energy references, respectively. (c) Distribution of root mean squared displacement (RMSD) between initial generated structures and DFT-relaxed structures. (d) Percentage of unique, novel structures as a function of number of generated structures. Novelty is defined with respect to Alex-MP-ICSD. (e-f) Percentage of S.U.N. structures (e) and average RMSD between initial and DFT-relaxed structures (f) for MatterGen, MatterGen-MP, and several baseline models, including CDVAE [4], P-G-SchNet, G-SchNet [47], and FTCP [38].</p>
<p>Their DFT-relaxed structures that is below 0.076 Å (Fig. 2(c)), which is almost one order of magnitude smaller than the atomic radius of the hydrogen atom (0.53 Å). These results indicate that the majority of structures generated by MatterGen are stable, and very close to the DFT local energy minimum. We further investigate whether MatterGen can generate a substantial amount of unique and novel materials. We showcase in Fig. 2(d) that the percentage of unique structures is 100 % when generating 1000 structures and only drops to 86 % after generating one million structures, while novelty remains stable around 68 %. This suggests that MatterGen is able to generate</p>
<p>diverse structures without significant saturation even at a large scale, and that the majority of those structures are novel with respect to Alex-MP-ICSD.</p>
<p>Moreover, we benchmark MatterGen against previous generative models for materials and show a significant improvement in performance. We focus on two key metrics: (1) the percentage of S.U.N. materials among generated samples, measuring the overall success rate of generating promising candidates, and (2) the average RMSD between generated samples and their DFT-relaxed structures, measuring the distance to equilibrium. We also compare to MatterGen-MP, which is a MatterGen model trained only on MP-20, i.e., the same, smaller, dataset used by the other baselines. In Fig. 2(e-f), MatterGen-MP shows a 1.8 times increase in the percentage of S.U.N. structures and a 3.1 times decrease in average RMSD compared with the previous state-of-the-art CDVAE [4]. When comparing MatterGen with MatterGen-MP, we observe a further 1.6 times increase in the percentage of S.U.N. structures and a 5.5 times decrease in RMSD as a result of scaling up the training dataset.</p>
<p>In summary, we have shown that MatterGen is able to generate S.U.N. materials at a substantially higher rate compared to previous generative models while the generated structures are orders of magnitudes closer to their local energy minimum. Next, we fine-tune the pre-trained base model of MatterGen towards different downstream applications, including target chemistry (Section 2.3), symmetry (Section 2.4), and scalar property constraints (Sections 2.5 and 2.6).</p>
<h1>2.3 Generating materials with target chemistry</h1>
<p>Finding the most stable material structures in a target chemical system (e.g., Li-Co-O) is crucial to define the true convex hull required for assessing stability, and indeed is one of the major challenges in materials design [48]. The most comprehensive approach for this task is ab initio RSS [49], which has been used to discover many novel materials that were later experimentally synthesized [48]. The biggest drawback of RSS is its computational cost, as the thorough exploration of even a ternary compound can require hundreds of thousands of DFT relaxations. In recent years, the combination of generating structures via RSS, substitution or evolutionary methods with MLFFs has proven successful in exploring chemical systems [23, 27, 50, 51]. Here, we evaluate the ability of MatterGen to explore target chemical systems by comparing it with substitution [23] and RSS [49, 52]. We equip all methods with the MatterSim[53] MLFF, which is used to pre-relax and filter the generated structures by their predicted stability before running more expensive DFT calculations. We fine-tune the MatterGen base model (Appendix B.1) and steer the generated structures towards different target chemical systems and an energy above hull of 0.0 eV /atom. We perform the benchmark evaluation for nine ternary, nine quaternary, and nine quinary chemical systems. For each of these three groups, we pick three chemical systems at random from the following categories: well explored, partially explored, and not explored. See Appendix D. 4 for additional details. In Fig. 3(a-b) we see that MatterGen generates the highest percentage of S.U.N. structures for every system type and every chemical complexity. As highlighted in Fig. 3(c), MatterGen also finds the highest number of unique structures on the combined convex hull in (1) 'partially explored' systems, where existing known</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Generating materials in target chemical system. (a-b) Mean percentage of S.U.N. structures generated by MatterGen and baselines for 27 chemical systems, grouped by system type (a) and number of elements (b). Vertical black lines indicate maximum and minimum values. (c-d) Number of structures on the combined convex hull found by each method and in the Alex-MP-ICSD dataset, grouped by system type (c) and number of elements (d). (e) Convex hull diagram for V-Sr-O, a well-explored ternary system. The dots represent structures on the hull, their coordinates represent the element ratio of their composition, and their color indicates by which method they were discovered. (f-i) Four of the five structures MatterGen discovered on the V-Sr-O hull depicted in (e), along with their composition and space group.
structures near the hull were provided during training, and in (2) 'well-explored systems', where structures near the hull are known but were not provided in training. While substitution offers a comparable or more efficient way to generate structures on the hull for ternary and quaternary systems, MatterGen achieves better performance on quinary systems, as shown in Fig. 3(d). Remarkably, the strong performance of MatterGen in quinary systems was achieved with only 10,240 generated samples,</p>
<p>compared to $\sim 70,000$ samples for substitution and 600,000 for RSS. This underscores the enormous efficiency gains that can be realized with generative models by proposing better initial candidates. Finally, in Fig. 3(e) we show that MatterGen finds five novel structures on the combined hull for V-Sr-O-an example of a well-explored ternary system-while substitution finds four, and RSS only two. A few of the structures discovered by MatterGen are shown in Fig. 3(f-i), and are analyzed in-depth in Appendix D.4.2.</p>
<h1>2.4 Designing materials with target symmetry</h1>
<p>The symmetry of a material directly affects its electronic and vibrational properties, and is a determining factor for its topological [54] and ferroelectric [55] characteristics. The generation of S.U.N. materials with a given symmetry is a challenging task, as the symmetric arrangement of atoms in space is hard to enforce without resorting to explicit constraints based on already known materials. Here, we assess MatterGen's ability to generate S.U.N. materials with a target symmetry by fine-tuning it on space group labels. We choose 14 space groups at random from the subset of space groups that had at least 1000 entries in the training dataset, two for each of the seven crystal systems, and generate 256 structures per space group. The results are shown in Fig. 4(a). On average, the fraction of generated S.U.N. structures that belong to the target space group is $20 \%$, and still surpassing $10 \%$ for some of the most highly symmetric space groups that were chosen, e.g., $\mathrm{P} 6_{3} / \mathrm{mmc}$ and $\mathrm{Im} 3$. This is a notable result given that most previous generative models struggled in generating highly symmetric crystals [4, 56]. In Fig. 4(b), we show four randomly generated S.U.N. structures from different space groups. Additional details and results are provided in Appendix D.5.</p>
<h3>2.5 Designing materials with target magnetic, electronic, and mechanical properties</h3>
<p>There is an enormous need for new materials with improved properties across a wide range of real-world applications, e.g., for designing carbon capture technologies, solar cells, or semiconductors [24-26]. The classical screening-based approach starts from a set of candidates and selects the ones with the best properties. However, screening methods are unable to explore structures beyond the set of known materials. Here, we demonstrate MatterGen's ability to directly generate S.U.N. materials with target constraints on three different single-property inverse design tasks. These feature a diverse set of properties-magnetic, electronic, and mechanical-with varying degrees of available labeled data for fine-tuning the model. In the first task, we aim to generate materials with high magnetic density, a prerequisite for permanent magnets. We finetune the model on 605,000 structures with DFT magnetic density labels (calculated assuming ferromagnetic ordering) and then generate structures with a target magnetic density value of $0.20 \AA^{-3}$. Second, we search for materials with a specific electronic property. We fine-tune the model on 42,000 structures with DFT band gap labels, then sample materials with a target calculated band gap value of 3.0 eV . Finally, we target structures with high bulk modulus-an important property for superhard materials. We fine-tune the model on only 5,000 labeled structures, and sample with a target</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Generating materials with target symmetry. (a) Fraction of generated S.U.N. structures that belong to the target space group for 14 randomly chosen space groups spanning the seven lattice types. (b) Four randomly selected S.U.N. structures generated by MatterGen, along with their chemical formula and space group.
value of 400 GPa . While the tasks above were chosen to evaluate the generality of the model, we note that additional follow-up investigations would be required to assess the suitability of these materials for specific applications, e.g., a superhard material needs to satisfy additional constraints such as a high shear modulus, or a permanent magnet needs a suitable magnetic order and critical temperature. See Appendix D. 6 for more details.</p>
<p>In Fig. 5(a-c), we highlight the significant shift in the distribution of property values among S.U.N. samples generated by MatterGen towards the desired targets, even when the targets are at the tail of the data distribution. In particular, this still holds true for properties where the number of DFT labels available for fine-tuning the model is substantially smaller than the size of the unlabeled training data. In Fig. 5(d-f) we showcase the S.U.N. structures with the best property values generated by MatterGen for each task. See Appendix D.6.2 for additional analysis.</p>
<p>Moreover, we assess how many S.U.N. structures satisfying extreme property constraints can be found by MatterGen when given a limited budget for DFT property calculations. As a baseline, we count the number of materials in the labeled fine-tuning dataset that satisfy the constraint. We also compare with a screening approach, which scans previously unlabeled materials for promising candidates. In contrast to the previous experiment, we fine-tune MatterGen with labels predicted by a machine learning property predictor - the same used for the screening baseline - when the dataset is not fully labeled. As shown in Fig. 5(g), MatterGen is able to find up to 47 S.U.N. structures with magnetic density above $0.2 \AA^{-3}$, much more than the 26 materials with such high property values in the fine-tuning dataset. Since the dataset is fully</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Generating materials with target magnetic, electronic, and mechanical properties. (a-c) Density of property values among (1) generated S.U.N. samples by MatterGen, and (2) structures in the labeled fine-tuning dataset for a magnetic, electronic, and mechanical property, respectively. The property target for MatterGen is shown as a black dashed line. Magnetic density values $&lt;10^{-3} \AA^{-3}$ in (a) are excluded from the labeled data to improve readability. (d-f) Visualization of S.U.N. structures with the best property values generated by MatterGen for magnetic density (d), band gap (e), and bulk modulus (f). Alongside each structure, the chemical formula, space group and property value is shown. (g-h) Number of S.U.N. structures that satisfy target constraints found MatterGen compared to number of structures found by baselines across a range of DFT property calculation budgets.
labeled, there is no screening baseline available. In Fig. 5(h), we see that MatterGen finds substantially more S.U.N. materials with high bulk modulus than screening. While the number of structures found by screening saturates with increasing budget, MatterGen keeps discovering S.U.N. structures at an almost constant rate. Given a budget of 500 DFT property calculations, we find 277 S.U.N. structures (with 126 distinct compositions), almost double the number found with a screening approach (149,</p>
<p>79 distinct compositions). In contrast, there are only two materials in the labeled finetuning dataset with such high bulk modulus values. Note that both MatterGen and screening produce multiple structures per composition that are unique according to our definition (Appendix D.3.1) but could potentially be alloys or solid solutions [57].</p>
<h1>2.6 Designing low-supply-chain-risk magnets</h1>
<p>Most materials design problems require finding structures satisfying multiple property constraints. MatterGen can be fine-tuned to generate materials given any combination of constraints. Here, we showcase its ability to tackle materials design problems with multiple constraints by searching for low-supply-chain-risk magnets. Since many existing high-performing permanent magnets contain rare earth elements that are subject to supply chain risks, there has been increasing interest in discovering rare-earth-free permanent magnets [58]. We simplify the problem of finding such a magnet to finding materials with high magnetic density and a low Herfindahl-Hirschman index (HHI). According to the U.S. Department of Justice and the Federal Trade Commission, a material with an HHI score below 1500 is considered to have low supply chain risk [59]. Thus, we ask the model to generate materials with a magnetic density of $0.2 \AA^{-3}$ and an HHI score of 1250 .</p>
<p>In Fig. 6(a), we observe that MatterGen generates S.U.N. structures that are narrowly distributed around the target values, despite the labeled fine-tuning data being extremely scarce in that region. Compared to a model that only targets high magnetic density values (single), targeting both properties (joint) shifts the distribution of HHI scores closer towards the desired target value while retaining high magnetic density values. Fig. 6(b) showcases the effect of jointly targeting both properties on the distribution of elements found in the generated materials. Due to the lower HHI scores, elements commonly employed for high-magnetic density materials that have supply chain issues, e.g., Cobalt (Co) and Gadolinium (Gd), are practically absent in the jointly generated structures. In contrast, these elements are still present in structures generated by the model only targeting materials with high magnetic density (single).</p>
<h2>3 Discussion</h2>
<p>Generative models are particularly promising for tackling inverse design tasks as they can potentially explore entirely novel structures with desired properties in an efficient way. However, generating the 3D structure of stable crystalline materials is challenging due to their periodicity and the interplay between atom types, coordinates, and lattice. MatterGen improves upon limitations of previous methods [4, 56] by introducing a joint diffusion process for atom types, coordinates, and lattice (Section 2.1 and Appendices A. 5 to A.7), which-combined with the substantially larger training dataset Alex-MP-20-drastically increase the stability, uniqueness, and novelty of generated materials. Thanks to the introduction of the adapter modules (Appendix B.1), MatterGen can further be fine-tuned to generate S.U.N. structures satisfying target constraints across a wide range of properties, with performance improvements over widely-employed methods such as MLFF-assisted RSS and substitution (Section 2.3), as well as ML-assisted screening (Section 2.5).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Designing low-supply-chain-risk magnets. (a) Distribution of S.U.N. structures generated by MatterGen when fine-tuned on the HHI score (single) and on both HHI score and magnetic density (joint), as well as structures from the labeled fine-tuning dataset. The property target of MatterGen is denoted as a black cross. (b) Occurrence of most frequent elements in S.U.N. structures for the two fine-tuned MatterGen models. (c) S.U.N. structures on the Pareto front for the joint property optimization task, along with their chemical composition, space group, magnetic density, and HHI score.</p>
<p>Despite these advances, MatterGen could still be improved in several ways. For example, we observe that the model disproportionately generates structures with P1 symmetry compared to the training data, indicating a tendency for generating less symmetric structures, especially for larger crystals (see discussion in Appendix D.2). We hypothesize that further improvements on the denoising process, the backbone architecture, and the expansion of the training dataset could enable the model to overcome such issues. We also acknowledge that our extensive evaluations only cover some of the criteria required for real-world applications, with experimental validation and characterization being the ultimate test [57]. We discuss the challenges in evaluating the quality of crystalline materials from generative models in Appendix D.2.</p>
<p>Overall, we believe that the breadth of MatterGen's capabilities and the quality of generated materials represent a major advancement towards creating a universal generative model for materials with high real-world impact. Given the transformative</p>
<p>effect of generative models in domains like image generation [60] and protein design [61], we envision that generative models like MatterGen will have a major impact in materials design in the coming years. As such, we are excited about the many directions in which MatterGen could be further extended. For instance, MatterGen could be expanded to cover a broader class of materials ranging from catalyst surfaces to metal organic frameworks, enabling us to tackle challenging problems like nitrogen fixation [62] and carbon capture [63]. The property constraints can be extended to non-scalar quantities like the band structure or X-ray diffraction (XRD) spectrum, which would further enable applications ranging from band engineering to the prediction of atomic structures of experimentally-measured XRD spectra of unknown samples.</p>
<p>Supplementary information Additional explanations and details regarding the MatterGen architecture, fine-tuning approach, datasets, and results can be found in the supplementary information.</p>
<p>Acknowledgments We are grateful for many insightful discussions with members from the Materials Project [15], and to Chris Pickard for providing helpful feedback. We would also like to thank our colleagues from Microsoft Research AI4Science for helpful discussions and support, including Andrew Foong, Karin Strauss, Keqiang Yan, Cristian Bodnar, Rianne van den Berg, Frank Noé, Marwin Segler, Elise van der Pol, and Max Welling. We are also grateful for useful feedback from the Microsoft Azure Quantum team, including Chi Chen, Leopold Talirz and Nathan Baker. Finally, we thank the AI on Xbox Team for providing part of the compute resources required for this work.</p>
<h1>Declarations</h1>
<h2>Author contributions</h2>
<p>AF, MH, RP, RT, TX, CZ and DZ (alphabetically ordered) conceived the study, implemented the methods, performed experiments, and wrote the manuscript. XF led the development of the adapter modules. SS implemented and ran the symmetry conditioned generation. JS implemented the band gap workflow. BN proposed the task of low-supply-chain risk magnets. ZL, YZ, HY, HH, and JL developed the machine learning force field. XF, SS, JC, LS, JS, BN, HS, SL, C-WH, ZL, YZ, HY, HH, and JL helped with implementing the methods, conducting experiments, and writing the manuscript. TX and RT led the research.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A Diffusion model for periodic materials ..... 15
A. 1 Representation of periodic materials ..... 15
A. 2 Invariance and equivariance in periodic materials ..... 16
A. 3 Diffusion model background and notation ..... 16
A. 4 Joint diffusion process ..... 18
A. 5 Atom type diffusion ..... 18
A. 6 Coordinate diffusion ..... 21
A. 7 Lattice diffusion ..... 23
A. 8 Architecture of the score network ..... 24
A. 9 Training loss ..... 27
B Fine-tuning the score network for property-guided generation ..... 29
B. 1 Fine-tuning the score network with adapter modules ..... 29
B. 2 Classifier-free guidance ..... 30
C Dataset generation ..... 31
C. 1 Data sources ..... 31
C. 2 DFT details ..... 32
D Results ..... 34
D. 1 Common experimental details ..... 34
D. 2 Qualitative analysis of generated structures ..... 35
D. 3 Generating stable and diverse materials ..... 37
D. 4 Generating materials with target chemistry ..... 38
D. 5 Designing materials with target symmetry ..... 42
D. 6 Designing materials with target magnetic, electronic and mechan- ical properties ..... 42
D. 7 Designing low-supply-chain risk magnets ..... 46</p>
<h1>A Diffusion model for periodic materials</h1>
<p>This section contains additional model details, following the notation listed in Table A1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">General notation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$n \in \mathbb{N}$</td>
<td style="text-align: center;">Number of atoms in a crystal</td>
</tr>
<tr>
<td style="text-align: center;">$M=(\boldsymbol{X}, \boldsymbol{A}, \boldsymbol{L})$</td>
<td style="text-align: center;">A crystal structure</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{X} \in[0,1)^{3 \times n}$</td>
<td style="text-align: center;">Fractional atomic coordinates</td>
</tr>
<tr>
<td style="text-align: center;">$\hat{\boldsymbol{X}} \in \mathbb{R}^{3 \times n}$</td>
<td style="text-align: center;">Cartesian atomic coordinates</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{A} \in \mathbb{A}^{n}$</td>
<td style="text-align: center;">Atomic species in a crystal</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{L}=\left(\boldsymbol{l}^{1}, \boldsymbol{l}^{2}, \boldsymbol{l}^{3}\right) \in \mathbb{R}^{3 \times 3}$</td>
<td style="text-align: center;">The unit cell lattice matrix</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{l}^{j} \in \mathbb{R}^{3}, j \in{1,2,3}$</td>
<td style="text-align: center;">The $j$-th lattice vector</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{V}=\left(\boldsymbol{v}<em 2="2">{1}, \boldsymbol{v}</em>$}, \ldots \boldsymbol{v}_{N}\right) \in \mathbb{R}^{d \times N</td>
<td style="text-align: center;">Concatenation of $N d$-dimensional column vectors into a matrix</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{E} \subset{1,2, \ldots, n}^{2} \times \mathbb{Z}^{3}$</td>
<td style="text-align: center;">Set of edges in a material</td>
</tr>
<tr>
<td style="text-align: center;">$i, j \in{1,2, \ldots, n}$</td>
<td style="text-align: center;">Index of an atom in a material</td>
</tr>
<tr>
<td style="text-align: center;">$d \in \mathbb{N}$</td>
<td style="text-align: center;">The number of hidden dimension in our GNN</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{1}_{n} \in \mathbb{R}^{n}$</td>
<td style="text-align: center;">$n$-dimensional column vector containing ones</td>
</tr>
<tr>
<td style="text-align: center;">Diffusion notation</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$t \in 1,2, \ldots, T$</td>
<td style="text-align: center;">Diffusion timestep</td>
</tr>
<tr>
<td style="text-align: center;">$T \in \mathbb{N}$</td>
<td style="text-align: center;">Number of time discretization steps for the diffusion process</td>
</tr>
<tr>
<td style="text-align: center;">$q\left(\boldsymbol{x}_{0}\right)$</td>
<td style="text-align: center;">The data distribution</td>
</tr>
<tr>
<td style="text-align: center;">$q\left(\boldsymbol{x}<em t-1="t-1">{t} \mid \boldsymbol{x}</em>\right)$</td>
<td style="text-align: center;">Single-step diffusion transition kernel</td>
</tr>
<tr>
<td style="text-align: center;">$q\left(\boldsymbol{x}<em 0="0">{t} \mid \boldsymbol{x}</em>\right)$</td>
<td style="text-align: center;">One-shot diffusion kernel</td>
</tr>
<tr>
<td style="text-align: center;">$q\left(\boldsymbol{x}_{T}\right)$</td>
<td style="text-align: center;">Prior (noise) distribution</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{s}_{\boldsymbol{\theta}}(\cdot, t)$</td>
<td style="text-align: center;">Score model</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{s}_{\boldsymbol{X}, \boldsymbol{\theta}}(\cdot, t)$</td>
<td style="text-align: center;">Score model for atomic coordinates</td>
</tr>
<tr>
<td style="text-align: center;">$\log \boldsymbol{s}<em 0="0">{\boldsymbol{\theta}}\left(\boldsymbol{A}</em>} \mid \boldsymbol{X<em t="t">{t}, \boldsymbol{L}</em>, t\right)$}, \boldsymbol{A}_{t</td>
<td style="text-align: center;">Predicted logits for atom types at $t=0$.</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{s}_{\boldsymbol{L}, \boldsymbol{\theta}}(\cdot, t)$</td>
<td style="text-align: center;">Score model for lattice</td>
</tr>
<tr>
<td style="text-align: center;">$\boldsymbol{z}$</td>
<td style="text-align: center;">Standard Gaussian noise $\boldsymbol{z} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$</td>
</tr>
</tbody>
</table>
<p>Table A1: Table of notations</p>
<h2>A. 1 Representation of periodic materials</h2>
<p>Any crystal structure can be represented by some repeating unit (called the unit cell) that tiles the entire 3D space. The unit cell itself contains a number of atoms that are arranged inside of it. Thus, we use the following universal representation for a material $\boldsymbol{M}$ :</p>
<p>$$
\boldsymbol{M}=(\boldsymbol{A}, \boldsymbol{X}, \boldsymbol{L})
$$</p>
<p>where $\boldsymbol{A}=\left(a^{1}, a^{2}, \ldots, a^{n}\right)^{\top} \in \mathbb{A}^{n}$ are the atomic species of the atoms inside the unit cell; $\boldsymbol{L}=\left(\boldsymbol{l}^{1}, \boldsymbol{l}^{2}, \boldsymbol{l}^{3}\right) \in \mathbb{R}^{3 \times 3}$ is the lattice, i.e., the shape of the repeating unit cell; and $\boldsymbol{X}=\left(\boldsymbol{x}^{1}, \boldsymbol{x}^{2}, \ldots, \boldsymbol{x}^{n}\right) \in[0,1)^{3 \times n}$ are the fractional coordinates of the atoms inside the unit cell.</p>
<p>The lattice $\boldsymbol{L}$ is a parallelepiped defined by the three lattice vectors $\boldsymbol{l}^{1}, \boldsymbol{l}^{2}$, and $\boldsymbol{l}^{3}$. It can thus be compactly represented as a single $3 \times 3$ matrix with the three lattice vectors as its columns. The volume of a lattice is given by $\operatorname{Vol}(\boldsymbol{L})=|\operatorname{det} \boldsymbol{L}|$. Any</p>
<p>physically sensible crystal must have a unit cell with nonzero volume, hence, we require any lattice matrix to be non-singular.</p>
<p>Fractional coordinates express the location of an atom using the lattice vectors as the basis vectors. For instance, an atom with fractional coordinates $\boldsymbol{x}=(0.2,0.3,0.5)^{\top}$ has Cartesian coordinates $\hat{\boldsymbol{x}}=0.2 \boldsymbol{l}^{1}+0.3 \boldsymbol{l}^{2}+0.5 \boldsymbol{l}^{3}$. The periodicity in fractional coordinates is defined by the (flat) unit hypertorus, i.e., we have the equivalence relation $\boldsymbol{x} \sim \boldsymbol{x}+\boldsymbol{k}, \boldsymbol{k} \in \mathbb{Z}^{3}$. We can convert between fractional coordinates $\boldsymbol{X}$ and Cartesian coordinates $\hat{\boldsymbol{X}}$ as follows:</p>
<p>$$
\begin{aligned}
&amp; \hat{\boldsymbol{X}}=\boldsymbol{L} \boldsymbol{X} \
&amp; \boldsymbol{X}=\boldsymbol{L}^{-1} \hat{\boldsymbol{X}}
\end{aligned}
$$</p>
<h1>A. 2 Invariance and equivariance in periodic materials</h1>
<p>The energy per atom $\epsilon(\boldsymbol{M})=E(\boldsymbol{M}) / n$ of a periodic material $\boldsymbol{M}=(\boldsymbol{X}, \boldsymbol{L}, \boldsymbol{A})$ has several invariances.</p>
<ul>
<li>Permutation invariance: $\epsilon(\boldsymbol{X}, \boldsymbol{L}, \boldsymbol{A})=\epsilon(\boldsymbol{P}(\boldsymbol{X}), \boldsymbol{L}, \boldsymbol{P}(\boldsymbol{A}))$ for every permutation matrix $\boldsymbol{P}$.</li>
<li>Translation invariance: $\epsilon(\boldsymbol{X}, \boldsymbol{L}, \boldsymbol{A})=\epsilon(\boldsymbol{X}+\boldsymbol{t}, \boldsymbol{L}, \boldsymbol{A})$ for every $\boldsymbol{t} \in \mathbb{R}^{3}$.</li>
<li>Rotation invariance: $\epsilon(\boldsymbol{X}, \boldsymbol{L}, \boldsymbol{A})=\epsilon(\boldsymbol{X}, \boldsymbol{R}(\boldsymbol{L}), \boldsymbol{A})$ for every rotation matrix $\boldsymbol{R} \in$ $O(3)$.</li>
<li>Periodic cell choice invariance: $\epsilon(\boldsymbol{X}, \boldsymbol{L}, \boldsymbol{A})=\epsilon\left(\boldsymbol{C}^{-1} \boldsymbol{X}, \boldsymbol{L} \boldsymbol{C}, \boldsymbol{A}\right)$, where $\boldsymbol{C}$ triangular with $\operatorname{det} \boldsymbol{C}=1$ and $\boldsymbol{C} \in \mathbb{Z}^{3 \times 3}$. See Fig. A1 for an example.</li>
<li>Supercell invariance: $\epsilon(\boldsymbol{X}, \boldsymbol{L}, \boldsymbol{A})=\epsilon\left(\bigoplus_{i=0}^{\operatorname{det}(\boldsymbol{C})} \boldsymbol{C}^{-1}\left(\boldsymbol{X}+\boldsymbol{k}<em a="a">{i} \mathbf{1}</em>$ indexes the cell repetitions in the three lattice components, and $\bigoplus$ indicates concatenation.}^{\top}\right), \boldsymbol{L} \boldsymbol{C}, \bigoplus_{i=0}^{\operatorname{det}(\boldsymbol{C})} \boldsymbol{A}\right)$, where $\boldsymbol{C}$ is a $3 \times 3$ diagonal matrix with positive integers on the diagonal, $\boldsymbol{k}_{i} \in$ $\mathbb{N}^{3</li>
</ul>
<p>Forces are instead equivariant to permutation and rotation, while being invariant to translation and periodic cell choice. Stress tensors are similarly invariant to permutation, translation, supercell choice, and periodic cell choice; while being equivariant to rotation (see Appendix A.8.1 for additional details).</p>
<h2>A. 3 Diffusion model background and notation</h2>
<p>Diffusion models [41, 42, 64, 65] are a class of generative models that learn to revert a diffusion process. The diffusion process (also called the forward process) gradually corrupts an input sample $\boldsymbol{x}<em t="t">{0}$ via transition kernels $q\left(\boldsymbol{x}</em>} \mid \boldsymbol{x<em 0="0">{t-1}\right)^{1}$, defining a Markov chain $\boldsymbol{x}</em>} \rightarrow \boldsymbol{x<em T="T">{1} \rightarrow \cdots \rightarrow \boldsymbol{x}</em>$ is the number of diffusion steps and $1 \leq t \leq T$. Here, we cover the typical case where the data is continuous-valued and the transition kernels are normal distributions. See Appendix A. 5 for details on discrete diffusion models.}$, where $T \in \mathbb{N</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The transition kernels are of the general form $q\left(\boldsymbol{x}<em t-1="t-1">{t} \mid \boldsymbol{x}</em>}\right)=\mathcal{N}\left(f\left(\boldsymbol{x<em t="t">{t-1}, t\right), \sigma</em>}^{2} \boldsymbol{I}\right)$, where $f\left(\boldsymbol{x<em t-1="t-1">{t-1}, t\right): \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ is affine in $\boldsymbol{x}</em>}$. This implies that the one-shot transition kernel $q\left(\boldsymbol{x<em 0="0">{t} \mid \boldsymbol{x}</em>$ at an arbitrary time step $t$ during training.}\right)$ is also Gaussian, and for popular choices $f(\cdot, t)$ the mean and variance are known in closed form. This enables us to efficiently obtain a noisy sample $\boldsymbol{x}_{t</p>
<p>Diffusion models are optimized to approximate the score function of the noise distributions $q\left(\boldsymbol{x}<em 0="0">{t} \mid \boldsymbol{x}</em>\right)$ for any $1 \leq t \leq T$ :</p>
<p>$$
\boldsymbol{\theta}^{*}=\underset{\boldsymbol{\theta}}{\arg \min } \sum_{t=1}^{T} \sigma_{t}^{2} \mathbb{E}<em 0="0">{q\left(\boldsymbol{x}</em>}\right)} \mathbb{E<em t="t">{q\left(\boldsymbol{x}</em>} \mid \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{0}\right)}\left[\left|\boldsymbol{s}</em>}}\left(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">{t}, t\right)-\nabla</em><em t="t">{t}} \log q\left(\boldsymbol{x}</em>} \mid \boldsymbol{x<em 2="2">{0}\right)\right|</em>\right]
$$}^{2</p>
<p>where $\boldsymbol{s}<em _="+">{\boldsymbol{\theta}}(\boldsymbol{x}, t): \mathbb{R}^{d} \times \mathbb{R}</em>} \rightarrow \mathbb{R}^{d}$ is called the score model. It is standard practice [41, 42] to parameterize the model to predict the noise $\epsilon_{t}=-\sigma_{t} \nabla_{\boldsymbol{x<em t="t">{t}} \log q\left(\boldsymbol{x}</em>} \mid \boldsymbol{x<em t="t">{0}\right)$ instead of the score, since the magnitude of $\epsilon</em>)$ is independent of the diffusion time step $t$.} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I</p>
<p>The forward diffusion process is designed such that $q\left(\boldsymbol{x}<em 0="0">{T} \mid \boldsymbol{x}</em>}\right) \approx q\left(\boldsymbol{x<em T="T">{T}\right)$, where $q\left(\boldsymbol{x}</em>\right)$ is a prior distribution that is easy to sample from (e.g., Gaussian).</p>
<p>In this work we leverage two popular diffusion processes for continuous data, i.e., the variance-exploding diffusion [41, 66] and the variance-preserving diffusion [42, 64] process, which we briefly explain in the following.</p>
<h1>Variance-exploding diffusion</h1>
<p>This is the diffusion process used in denoising score matching (DSM) [41]. We define a sequence of exponentially increasing standard deviations $\sigma_{\min }=\sigma_{1}, \ldots, \sigma_{T}=\sigma_{\max }$ that define the transition kernels:</p>
<p>$$
q\left(\boldsymbol{x}<em t-1="t-1">{t} \mid \boldsymbol{x}</em>}\right)=\mathcal{N}\left(\boldsymbol{x<em t="t">{t},\left(\sigma</em>}^{2}-\sigma_{t-1}^{2}\right) \boldsymbol{I}\right), \quad q\left(\boldsymbol{x<em 0="0">{t} \mid \boldsymbol{x}</em>}\right)=\mathcal{N}\left(\boldsymbol{x<em t="t">{0}, \sigma</em>\right)
$$}^{2} \boldsymbol{I</p>
<p>We can generate a sample using the learned model via annealed Langevin dynamics $[41,66]$ or ancestral sampling from the graphical model $\prod_{t=1}^{T} p_{\boldsymbol{\theta}}\left(\boldsymbol{x}<em t="t">{t-1} \mid \boldsymbol{x}</em>\right)$ [43]:</p>
<p>$$
\boldsymbol{x}<em t="t">{t-1}=\boldsymbol{x}</em>}+\left(\sigma_{t}^{2}-\sigma_{t-1}^{2}\right) \boldsymbol{s<em t="t">{\boldsymbol{\theta}^{*}}\left(\boldsymbol{x}</em>
$$}, t\right)+\boldsymbol{z} \sqrt{\sigma_{t}^{2}-\sigma_{t-1}^{2}</p>
<p>where $\boldsymbol{x}<em T="T">{T} \sim \mathcal{N}\left(\mathbf{0}, \sigma</em>)$ is standard Gaussian noise. In Appendix A. 6 we explain how we leverage variance-exploding diffusion in the diffusion process of the fractional coordinates.}^{2} \boldsymbol{I}\right)$, and $\boldsymbol{z} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I</p>
<h2>Variance-preserving diffusion</h2>
<p>This is the diffusion process used to train denoising diffusion probabilistic models (DDPMs) [42, 64]. In variance-preserving diffusion we define a sequence of positive noise scales $0&lt;\beta_{1}, \beta_{2}, \ldots \beta_{T}&lt;1$ to obtain transition kernels of the form</p>
<p>$$
q\left(\boldsymbol{x}<em t-1="t-1">{t} \mid \boldsymbol{x}</em>}\right)=\mathcal{N}\left(\sqrt{1-\beta_{t}} \boldsymbol{x<em t="t">{t-1}, \beta</em>} \boldsymbol{I}\right), \quad q\left(\boldsymbol{x<em 0="0">{t} \mid \boldsymbol{x}</em>}\right)=\mathcal{N}\left(\sqrt{\bar{\alpha<em 0="0">{t}} \boldsymbol{x}</em>\right)
$$},\left(1-\bar{\alpha}_{t}\right) \boldsymbol{I</p>
<p>where $\bar{\alpha}<em i="1">{t}=\prod</em>}^{t}\left(1-\beta_{t}\right)$. Sampling from a model trained to revert the variancepreserving diffusion process also works via ancestral sampling from the graphical model $\prod_{t=1}^{T} p_{\boldsymbol{\theta}}\left(\boldsymbol{x<em t="t">{t-1} \mid \boldsymbol{x}</em>\right)$ :</p>
<p>$$
\boldsymbol{x}<em t="t">{t-1}=\frac{1}{\sqrt{1-\beta</em>}}}\left(\boldsymbol{x<em t="t">{t}+\beta</em>} \boldsymbol{s<em t="t">{\boldsymbol{\theta}^{*}}\left(\boldsymbol{x}</em>
$$}, t\right)\right)+\sqrt{\beta_{t}} \boldsymbol{z</p>
<p>starting from $\boldsymbol{x}_{T} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})$, where $\boldsymbol{z} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})$ is standard Gaussian noise. See Appendix A. 7 for details about how we leverage variance-preserving diffusion in the diffusion process of the lattice.</p>
<h1>A. 4 Joint diffusion process</h1>
<p>To apply the construction of a diffusion process described in Appendix A. 3 to crystal structures described in Appendix A.1, we define the forward process through a Markov chain $\boldsymbol{M}<em 1="1">{0} \rightarrow \boldsymbol{M}</em>$ via a transition kernel that diffuses the atom coordinates, atom types, and the lattice independently as follows:} \rightarrow \cdots \rightarrow \boldsymbol{M}_{T</p>
<p>$$
\begin{aligned}
&amp; q\left(\boldsymbol{A}<em t_1="t+1">{t+1}, \boldsymbol{X}</em>}, \boldsymbol{L<em t="t">{t+1} \mid \boldsymbol{A}</em>}, \boldsymbol{X<em t="t">{t}, \boldsymbol{L}</em>\right) \
&amp; \quad=q\left(\boldsymbol{A}<em t="t">{t+1} \mid \boldsymbol{A}</em>}\right) q\left(\boldsymbol{X<em t="t">{t+1} \mid \boldsymbol{X}</em>}\right) q\left(\boldsymbol{L<em t="t">{t+1} \mid \boldsymbol{L}</em>\right) \quad(t=0,1, \ldots, T-1)
\end{aligned}
$$</p>
<p>In addition, the noise distributions of atom species $\boldsymbol{A}$ and the fractional coordinates $\boldsymbol{X}$ factorize into the diffusion of the individual atoms:</p>
<p>$$
q\left(\boldsymbol{A}<em t="t">{t+1} \mid \boldsymbol{A}</em>}\right)=\prod_{i=1}^{n} q\left(a_{t+1}^{i} \mid a_{t}^{i}\right), \quad q\left(\boldsymbol{X<em t="t">{t+1} \mid \boldsymbol{X}</em>}\right)=\prod_{i=1}^{n} q\left(\boldsymbol{x<em t="t">{t+1}^{i} \mid \boldsymbol{x}</em>\right)
$$}^{i</p>
<p>Note that the factorization of the forward diffusion process does not imply that the reverse diffusion process factorizes in the same way. Details of the atom type diffusion, coordinate diffusion, and lattice diffusion are described in Appendix A.5, Appendix A.6, Appendix A.7, respectively. The architecture of the score network $s_{\theta}\left(\boldsymbol{M}_{t}, t\right)$ is described in Appendix A.8. The combined objective function is presented in Appendix A.9.</p>
<h2>A. 5 Atom type diffusion</h2>
<p>For the diffusion of the (discrete) atom species $\boldsymbol{A}$, we use the discrete denoising diffusion probabilistic model (D3PM) approach [65], which is a generalization of DDPMs to discrete data problems. As in DDPM, the forward diffusion process is a Markov process that gradually corrupts an input sample $a_{0}$, which is a scalar discrete random variable with $K$ categories (e.g., atomic species):</p>
<p>$$
q\left(a_{1: T} \mid a_{0}\right)=\prod_{t=1}^{T} q\left(a_{t} \mid a_{t-1}\right)
$$</p>
<p>where $a_{0} \sim q\left(a_{0}\right)$ is an atomic species sampled from the data distribution and $a_{T} \sim$ $q\left(a_{T}\right)$, where $q\left(a_{T}\right)$ is a prior distribution that is easy to sample from.</p>
<p>Denoting the one-hot representation of $a$ as a row vector $\boldsymbol{a}$, we can express the transitions as:</p>
<p>$$
q\left(\boldsymbol{a}<em t-1="t-1">{t} \mid \boldsymbol{a}</em>}\right)=\operatorname{Cat}\left(\boldsymbol{a<em t-1="t-1">{t} ; \boldsymbol{p}=\boldsymbol{a}</em>\right)
$$} \boldsymbol{Q}_{t</p>
<p>where $\left[\boldsymbol{Q}<em i="i" j="j">{t}\right]</em>$. Hence, we only consider individual one-hot vectors in this section. D3PMs are trained by optimizing a variational lower bound:}=q\left(a_{t}=j \mid a_{t-1}=i\right)$ is the Markov transition matrix at time step $t$. $\operatorname{Cat}(\boldsymbol{a} ; \boldsymbol{p})$ is a categorical distribution over one-hot vectors whose probabilities are given by the row vector $\boldsymbol{p}$. Similar to DDPM, D3PM assumes that the forward diffusion factorizes over all discrete variables of a data point, i.e., all atomic species are diffused independently with the same transition matrices $\boldsymbol{Q}_{t</p>
<p>$$
\begin{aligned}
L_{\mathrm{vb}}=\mathbb{E}<em 0="0">{q\left(\boldsymbol{a}</em>}\right)}[ &amp; -\mathbb{E<em 1="1">{q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em _boldsymbol_theta="\boldsymbol{\theta">{0}\right)} \log p</em>}}\left(\boldsymbol{a<em 1="1">{0} \mid \boldsymbol{a}</em>}, 1\right)+D_{\mathrm{KL}}\left[q\left(\boldsymbol{a<em 0="0">{T} \mid \boldsymbol{a}</em>}\right) | q\left(\boldsymbol{a<em t="2">{T}\right)\right] \
&amp; +\sum</em>}^{T} \mathbb{E<em t="t">{q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em _mathrm_KL="\mathrm{KL">{0}\right)} D</em>}}\left[q\left(\boldsymbol{a<em t="t">{t-1} \mid \boldsymbol{a}</em>}, \boldsymbol{a<em _boldsymbol_theta="\boldsymbol{\theta">{0}\right) | p</em>}}\left(\boldsymbol{a<em t="t">{t-1} \mid \boldsymbol{a}</em>\right)\right]
\end{aligned}
$$</p>
<p>Moreover, Austin et al. [65] propose an additional cross-entropy loss on the model's prediction $p_{\boldsymbol{\theta}}\left(\boldsymbol{a}<em t="t">{0} \mid \boldsymbol{a}</em>, t\right)$ :</p>
<p>$$
L_{\mathrm{CE}}=-\mathbb{E}<em 0="0">{q\left(\boldsymbol{a}</em>}\right)}\left[\sum_{t=2}^{T} \mathbb{E<em t="t">{q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em _boldsymbol_theta="\boldsymbol{\theta">{0}\right)} \log p</em>}}\left(\boldsymbol{a<em t="t">{0} \mid \boldsymbol{a}</em>, t\right)\right]
$$</p>
<p>so that the overall loss becomes</p>
<p>$$
L=L_{\mathrm{vb}}+\lambda_{\mathrm{CE}} L_{\mathrm{CE}}
$$</p>
<p>Three important characteristics of DDPM and DSM are that (1) given $\boldsymbol{x}<em t="t">{0}$ we can sample noisy samples $\boldsymbol{x}</em>}$ for arbitrary $t$ in constant time; (2) after sufficiently many diffusion steps, $\boldsymbol{x<em t-1="t-1">{T}$ follows a prior distribution that is easy to sample from; and (3) the posterior $q\left(\boldsymbol{x}</em>} \mid \boldsymbol{x<em 0="0">{t}, \boldsymbol{x}</em>\right)$ in Eq. (A12) is tractable and can be computed efficiently. D3PM also has these properties, as we briefly outline in the following:
(1) Fast sampling of $\boldsymbol{a}<em t="t">{t} \sim q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em t="t">{0}\right)$. Since the forward diffusion in D3PM is governed by discrete transition matrices $\left{\boldsymbol{Q}</em>$, we can write}\right}_{t=1}^{T</p>
<p>$$
q\left(\boldsymbol{a}<em 0="0">{t} \mid \boldsymbol{a}</em>}\right)=\operatorname{Cat}\left(\boldsymbol{a<em t-1="t-1">{t} ; \boldsymbol{p}=\boldsymbol{a}</em>} \overline{\boldsymbol{Q}<em t="t">{t}\right), \quad \text { where } \overline{\boldsymbol{Q}}</em>}=\boldsymbol{Q<em 2="2">{1} \boldsymbol{Q}</em>
$$} \ldots \boldsymbol{Q}_{t</p>
<p>The cumulative transition matrices $\overline{\boldsymbol{Q}}_{t}$ can be pre-computed and for many diffusion processes even have a closed form.
(2) Tractable prior distribution. Two of the proposed diffusion processes are the absorbing (which we employ in MatterGen) and uniform diffusion processes. Both</p>
<p>gradually diffuse the data towards a limit distribution, which are the one-hot distribution on the absorbing state and the uniform distribution over all categories, respectively. For more details, we refer to Appendix A of Austin et al. [65].
(3) Tractable posterior $q\left(\boldsymbol{a}<em t="t">{t-1} \mid \boldsymbol{a}</em>}, \boldsymbol{a<em t="t">{0}\right)$. Using Bayes' rule and exploiting the Markov property $q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em 0="0">{t-1}, \boldsymbol{a}</em>}\right)=q\left(\boldsymbol{a<em t-1="t-1">{t} \mid \boldsymbol{a}</em>\right)$, we can write</p>
<p>$$
q\left(\boldsymbol{a}<em t="t">{t-1} \mid \boldsymbol{a}</em>}, \boldsymbol{a<em t="t">{0}\right)=\frac{q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em t-1="t-1">{t-1}\right) q\left(\boldsymbol{a}</em>} \mid \boldsymbol{a<em t="t">{0}\right)}{q\left(\boldsymbol{a}</em>
$$} \mid \boldsymbol{a}_{0}\right)</p>
<p>All terms in Eq. (A15) can be computed efficiently in closed form given the forward diffusion process.</p>
<h1>Reverse sampling process.</h1>
<p>We generate a sample $\boldsymbol{a}<em T="T">{0}$ by first sampling $\boldsymbol{a}</em>}$ and then gradually updating it to obtain $p_{\boldsymbol{\theta}}\left(\boldsymbol{a<em T="T">{0: T}\right)=q\left(\boldsymbol{a}</em>}\right) \prod_{t=1}^{T} p_{\boldsymbol{\theta}}\left(\boldsymbol{a<em t="t">{t-1} \mid \boldsymbol{a}</em>}\right)$. Austin et al. [65] propose to parameterize $p_{\boldsymbol{\theta}}\left(\boldsymbol{a<em t="t">{t-1} \mid \boldsymbol{a}</em>$ and then marginalizing it out:}\right)$ by predicting a distribution over $\boldsymbol{a}_{0</p>
<p>$$
p_{\boldsymbol{\theta}}\left(\boldsymbol{a}<em t="t">{t-1} \mid \boldsymbol{a}</em>}\right) \propto \sum_{\boldsymbol{a<em t-1="t-1">{0}} q\left(\boldsymbol{a}</em>}, \boldsymbol{a<em 0="0">{t} \mid \boldsymbol{a}</em>}\right) p_{\boldsymbol{\theta}}\left(\boldsymbol{a<em t="t">{0} \mid \boldsymbol{a}</em>, t\right)
$$</p>
<p>where we can use our tractable posterior computation again. Since we have a discrete state space, marginalizing out $\boldsymbol{a}<em t-1="t-1">{0}$ by explicit summation has complexity $\mathcal{O}(K)$. In the case of atomic species we have $K \simeq 100$; thus, this is relatively cheap. This parameterization has the advantage that potential sparsity in the diffusion process is efficiently enforced by using $q\left(\boldsymbol{a}</em>}, \boldsymbol{a<em 0="0">{t} \mid \boldsymbol{a}</em>\right)$ without having to be learned by the model.</p>
<h2>Forward diffusion process.</h2>
<p>As the specific flavor of D3PM forward diffusion we employ the masked diffusion process, which has shown best performance in the original study [65] as well as our initial experiments. Following Austin et al. [65], we introduce an extra atom species [MASK] at index $K-1$, which is the absorbing or masked state. At each timestep $t$, the transition matrices have the particularly simple form</p>
<p>$$
\left[\boldsymbol{Q}<em i="i" j="j">{t}^{\text {absorbing }}\right]</em>
$$}= \begin{cases}1 &amp; \text { if } i=j=m \ 1-\beta_{t} &amp; \text { if } i=j \neq m \ \beta_{t} &amp; \text { if } j=m \neq i \ 0 &amp; \text { if } m \neq i \neq j \neq m\end{cases</p>
<p>where $m$ corresponds to the absorbing state. Intuitively, each species has probability $1-\beta_{t}$ of staying unchanged, and probability $\beta_{t}$ of transitioning to the absorbing state. Once a species is absorbed, it can never leave that state, and there are no transitions between different non-masked atomic species. Thus, the limit distribution of this diffusion process is a point mass on the absorbing state.</p>
<h1>A. 6 Coordinate diffusion</h1>
<p>For our model we perform diffusion on the fractional coordinates and outline the approach in the following. See Appendix A.6.3 for a brief outline why we favor fractional coordinate diffusion over Cartesian. The fractional coordinates in a crystal structure live in a Riemannian manifold referred to as the flat torus $\mathbb{T}^{3}=\mathbb{S}^{1} \times \mathbb{S}^{1} \times \mathbb{S}^{1}$, i.e., it is the quotient space $\mathbb{R}^{3} / \mathbb{Z}^{3}$ with equivalence relation:</p>
<p>$$
\boldsymbol{x}+\boldsymbol{k} \sim \boldsymbol{x}, \quad \boldsymbol{k} \in \mathbb{Z}^{3}
$$</p>
<p>Thus, adding Gaussian noise to fractional coordinates naturally corresponds to sampling from a wrapped normal distribution, whose probability density is</p>
<p>$$
\mathcal{N}<em _boldsymbol_k="\boldsymbol{k">{\mathrm{W}}\left(\bar{x} ; \boldsymbol{x}, \sigma^{2} \boldsymbol{I}, \boldsymbol{I}\right)=\sum</em>\right)
$$} \in \mathbb{Z}^{3}} \mathcal{N}\left(\bar{x} ; \boldsymbol{x}-\boldsymbol{k}, \sigma^{2} \boldsymbol{I</p>
<p>where $\mathcal{N}<em _mathrm_W="\mathrm{W">{\mathrm{W}}(\boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{B})$ denotes a wrapped normal distribution with mean $\boldsymbol{\mu}$, covariance matrix $\boldsymbol{\Sigma}$, and periodic boundaries $\boldsymbol{B}$. If the periodic boundaries are $[0,1)^{3}$, i.e., $\boldsymbol{B}=\boldsymbol{I}$, we write $\mathcal{N}</em>)$ for brevity.}}(\boldsymbol{\mu}, \boldsymbol{\Sigma</p>
<p>For the diffusion of the atom coordinates we use variance-exploding diffusion, i.e., the variance of the diffusion process increases exponentially with diffusion time. This has the advantage that the prior distribution $q\left(\boldsymbol{x}_{T}\right)$ is particularly simple, i.e., the uniform distribution in the range $[0,1)^{3}$. Jing et al. [67] use this approach for torsional angles-which live in a 1D flat torus-in small molecule generation. The one-shot noising process of the fractional coordinates is therefore defined as</p>
<p>$$
q\left(\boldsymbol{x}<em 0="0">{t} \mid \boldsymbol{x}</em>}\right)=\mathcal{N<em t="t">{\mathrm{W}}\left(\boldsymbol{x}</em>} ; \boldsymbol{x<em t="t">{0}, \sigma</em>\right)
$$}^{2} \boldsymbol{I</p>
<h2>A.6.1 Variance adjustment for atomic density</h2>
<p>One limitation of using a constant variance for the fractional coordinate diffusion is that the diffusion in Cartesian space will have difference variance depending on the size of the unit cell. This limitation becomes clear if we express the distribution of the Cartesian coordinates $\tilde{\boldsymbol{x}}<em t="t">{t}$ using Eq. (A2) via linear transformation of a Gaussian random variable $\boldsymbol{x}</em>$ :</p>
<p>$$
q\left(\tilde{\boldsymbol{x}}<em 0="0">{t}, \mid \boldsymbol{x}</em>}, \boldsymbol{L<em _mathrm_W="\mathrm{W">{t}\right)=\mathcal{N}</em>}}\left(\tilde{\boldsymbol{x}<em t="t">{t} ; \boldsymbol{L}</em>} \boldsymbol{x<em t="t">{0}, \sigma</em>}^{2} \boldsymbol{L<em t="t">{t} \boldsymbol{L}</em>\right)
$$}^{\top}, \boldsymbol{L}_{t</p>
<p>Observe that the covariance matrix $\boldsymbol{\Sigma}<em t="t">{t}=\sigma</em>}^{2} \boldsymbol{L<em t="t">{t} \boldsymbol{L}</em>}^{\top}$ of the noisy Cartesian coordinates depends on the lattice. Thus, the (generalized) variance of the noise distribution also depends on the size of the unit cell, i.e., $\left|\operatorname{det}\left(\boldsymbol{\Sigma<em t="t">{t}\right)\right|=\left(\sigma</em>$.}^{3}\left|\operatorname{det} \boldsymbol{L}_{t}\right|\right)^{2</p>
<p>We mitigate this effect by scaling the variance in fractional coordinate diffusion based on the size of the unit cell. Assuming roughly constant atomic density $\mathrm{d}\left(\boldsymbol{L}<em t="t">{t}\right)=$ $\frac{n}{\operatorname{Vol}\left(\boldsymbol{L}</em>}\right)} \propto 1 \Leftrightarrow \operatorname{Vol}\left(\boldsymbol{L<em t="t">{t}\right)=\left|\operatorname{det} \boldsymbol{L}</em>$ accordingly, i.e.,}\right| \propto n$. We therefore choose to scale $\sigma_{t</p>
<p>$$
\sigma_{t}(n)=\frac{\sigma_{t}}{\sqrt[4]{n}}
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We follow the convention in machine learning literature that the functional forms of (conditional) probability density functions depend on the variables that appear as arguments. For example, $q\left(\boldsymbol{x}<em t-1="t-1">{t} \mid \boldsymbol{x}</em>}\right)$ could be written as $q_{\boldsymbol{X<em t-1="t-1">{t} \mid \boldsymbol{X}</em>}}\left(\boldsymbol{x<em t-1="t-1">{t} \mid \boldsymbol{x}</em>\right)$ to make the dependence of the functional form on $t$ explicit, but we avoid this to prevent clutter.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5636 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5636</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5636</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-3827029</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1803.04967v1.pdf" target="_blank">Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems, such as intrusion detection, denial of service attack detection, hardware and software system failures, and malware detection. In these contexts, model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models. Deep learning methods have been criticized as black box oracles which allow limited insight into decision factors. In this work we seek to bridge the gap between the impressive performance of deep learning models and the need for interpretable model introspection. To this end we present recurrent neural network (RNN) language models augmented with attention for anomaly detection in system logs. Our methods are generally applicable to any computer system and logging source. By incorporating attention variants into our RNN language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance. We demonstrate model performance and illustrate model interpretability on an intrusion detection task using the Los Alamos National Laboratory (LANL) cyber security dataset, reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day's worth of data.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5636.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5636.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN LM (EM / BEM / Tiered)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network Language Models: Event Model (EM), Bidirectional Event Model (BEM), and Tiered variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unsupervised LSTM-based language models that learn the normal distribution of token sequences in system log lines and assign anomaly scores as negative log-likelihoods of log-lines; includes single-line (EM), bidirectional (BEM), and tiered (per-user multi-event) variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM language models (EM, BEM, T-EM, T-BEM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-layer LSTM sequence models (Event Model EM and Bidirectional Event Model BEM); tiered variants use an upper-tier LSTM to model per-user sequences of log-lines. Models embed categorical tokens (embeddings size 128) and produce next-token probability distributions; anomaly score is the sum of negative log probabilities across tokens in a log-line. Training uses ADAM optimizer with online/daywise sync training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised language-model scoring: compute next-token probabilities with (bi-)directional LSTM; anomaly score = negative log-likelihood (sum over token negative log-probs) per log-line; online daily training (train on day N, evaluate on day N+1).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured sequential data: system log-lines tokenized as sequences (word-level CSV fields or character-level sequences); also per-user sequences (tiered model).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Cyber-security anomalies (red-team intrusions), rare/low-probability token sequences, syntactic/token-level mispredictions and semantic anomalies in event sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LANL (Los Alamos National Laboratory) authentication logs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Area under the ROC curve (AUC ROC). Paper reports high performance (abstract: "upward of 0.99" AUC ROC) for the intrusion detection task when training on a single day's data; experiments (train day 7, evaluate day 8) show BEM > EM, and attention-augmented EM matches BEM; results summarized as mean and variance across 5 random initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Benchmarked primarily against internal baselines: EM (forward LSTM) and BEM (bidirectional LSTM) and tiered versions. BEM outperforms EM; adding attention to EM improves EM to match BEM. The paper does not provide empirical comparisons to classical non-language-model anomaly detectors (e.g., statistical outlier methods) within the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Trained and evaluated in a constrained rapid-deployment setup (one day of pretraining); results reported only on LANL authentication logs (days 7→8). Word-tokenized models require OOV handling and per-user score centering to reduce inter-user bias. Shared vocabulary across fields can cause cross-field mispredictions. Some model variants (e.g., EM with fixed attention on character-tokenized data) show high variance or poor fit. No direct comparison to traditional anomaly-detection systems reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5636.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5636.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-augmented RNN LMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dot-product attention variants added to RNN language models (Fixed, Syntax, Semantic1, Semantic2, Tiered Attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dot-product attention mechanisms that compute weighted sums over previous LSTM hidden states to (1) give the predictor direct selective access to prior tokens and (2) provide interpretable attention weights indicating which prior tokens influenced predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dot-product attention + LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Attention uses keys K = tanh(V W_a), query q (varies by variant), and d = softmax(q K^T) to produce attention vector a = d V, concatenated with h(t-1) to predict next token. Implemented attention dimension = 128; variants differ in how q is computed: Fixed (learned global q), Syntax (position-dependent q), Semantic1 (q = tanh(h W_sem1)), Semantic2 (h split into value and query sub-vectors), and Tiered attention (upper-tier attention over lower-tier hidden states).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Augment the LSTM language model with attention shortcuts so predictions condition on an attention-weighted summary of prior hidden states; anomaly scoring remains negative log-likelihood of tokens. Attention weights are inspected for interpretability of anomaly decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured sequential data (log-line token sequences; both word- and character-tokenized).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Same as base LM: intrusion/red-team events, anomalous or low-probability token sequences; attention helps reveal which fields/tokens contribute to anomalous scores (syntactic or semantic anomalies).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LANL authentication logs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AUC ROC; attention-augmented EM models improved EM to match BEM performance. All attention variants had similar AUCs for word-tokenized models. For character-tokenized models, semantic variants improved performance whereas Fixed and Syntax variants did not; numeric AUCs reported in tables (statistics over 5 runs) and abstract cites >0.99 AUC in best cases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against non-attention LSTM baselines (EM, BEM) and tiered models. Attention improves EM to BEM-level performance and offers interpretability advantages; tiered attention provided little additional performance improvement over tiered non-attention models in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fixed and Syntax attention perform poorly for character-tokenized data with variable-length fields and can exhibit high variance. Semantic attention variants are more robust for variable-length inputs but still do not always exceed BEM. Attention sometimes concentrates on delimiters (e.g., commas) or field separator hidden states, which may indicate reliance on delimiters for positional cues. The attention-based approach does not replace the need for feature design in some cases (e.g., field-specific vocabularies could help).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5636.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5636.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog (Du et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work mentioned that uses customized parsing of raw system logs to generate sequences which are fed to LSTM models to detect Denial-of-Service attacks and diagnose anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (DeepLog pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as an LSTM-based approach applied to parsed log-event sequences (authors of the current paper cite Du et al. for parsing+LSTM approach to DoS detection), exact model hyperparameters and size not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Preprocess logs via customized parsing to produce event sequences then apply LSTM sequence model for anomaly detection (as described by Du et al. in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured/semi-structured system log sequences produced by parsing raw text.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Security anomalies (Denial of Service and related attacks) per the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned to contrast with the present work: DeepLog uses custom parsing while the current work operates directly on raw tokens with minimal preprocessing (only tokenization and OOV handling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5636.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5636.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhang et al. (system failure prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated IT system failure prediction: A deep learning approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that uses clustering on raw text from multiple log sources to generate feature sequences fed to LSTM models for hardware and software failure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated IT system failure prediction: A deep learning approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (clustering + LSTM pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as clustering raw text from multiple log sources to form feature sequences and inputting those to LSTM networks for failure prediction; specific model sizes/hyperparameters not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Preprocessing via clustering then LSTM-based sequence prediction for failure detection.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured/semi-structured log-derived sequences aggregated across multiple sources.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Hardware and software system failures (anomalous events leading to failures).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned to contrast preprocessing-heavy pipelines with the current approach that minimizes feature engineering by operating on raw tokenized fields.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5636.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5636.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tuor et al. 2017</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that introduced the base language modeling framework for cyber anomaly detection (EM and BEM) which serves as the starting point for the present attention-augmented models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN language models (EM/BEM) — prior formulation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Introduces EM (forward LSTM) and BEM (bidirectional LSTM) language models for open-vocabulary, event-level cyber anomaly detection; used as the foundation and extended here with attention and tiered variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Unsupervised next-token probability modeling with LSTMs and negative log-likelihood scoring at event level.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>System log event sequences (tokenized fields), open-vocabulary handling via embeddings and OOV token.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Event-level cyber anomalies (intrusions, rare events).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Serves as baseline framework; this paper extends Tuor et al. by adding attention mechanisms and detailed interpretability analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection <em>(Rating: 2)</em></li>
                <li>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning <em>(Rating: 2)</em></li>
                <li>Frustratingly short attention spans in neural language modeling <em>(Rating: 1)</em></li>
                <li>Memory Architectures in Recurrent Neural Network Language Models <em>(Rating: 1)</em></li>
                <li>Automated IT system failure prediction: A deep learning approach <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5636",
    "paper_id": "paper-3827029",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "RNN LM (EM / BEM / Tiered)",
            "name_full": "Recurrent Neural Network Language Models: Event Model (EM), Bidirectional Event Model (BEM), and Tiered variants",
            "brief_description": "Unsupervised LSTM-based language models that learn the normal distribution of token sequences in system log lines and assign anomaly scores as negative log-likelihoods of log-lines; includes single-line (EM), bidirectional (BEM), and tiered (per-user multi-event) variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM language models (EM, BEM, T-EM, T-BEM)",
            "model_description": "Single-layer LSTM sequence models (Event Model EM and Bidirectional Event Model BEM); tiered variants use an upper-tier LSTM to model per-user sequences of log-lines. Models embed categorical tokens (embeddings size 128) and produce next-token probability distributions; anomaly score is the sum of negative log probabilities across tokens in a log-line. Training uses ADAM optimizer with online/daywise sync training.",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised language-model scoring: compute next-token probabilities with (bi-)directional LSTM; anomaly score = negative log-likelihood (sum over token negative log-probs) per log-line; online daily training (train on day N, evaluate on day N+1).",
            "data_type": "Structured sequential data: system log-lines tokenized as sequences (word-level CSV fields or character-level sequences); also per-user sequences (tiered model).",
            "anomaly_type": "Cyber-security anomalies (red-team intrusions), rare/low-probability token sequences, syntactic/token-level mispredictions and semantic anomalies in event sequences.",
            "dataset_name": "LANL (Los Alamos National Laboratory) authentication logs",
            "performance_metrics": "Area under the ROC curve (AUC ROC). Paper reports high performance (abstract: \"upward of 0.99\" AUC ROC) for the intrusion detection task when training on a single day's data; experiments (train day 7, evaluate day 8) show BEM &gt; EM, and attention-augmented EM matches BEM; results summarized as mean and variance across 5 random initializations.",
            "baseline_comparison": "Benchmarked primarily against internal baselines: EM (forward LSTM) and BEM (bidirectional LSTM) and tiered versions. BEM outperforms EM; adding attention to EM improves EM to match BEM. The paper does not provide empirical comparisons to classical non-language-model anomaly detectors (e.g., statistical outlier methods) within the experiments.",
            "limitations_or_failure_cases": "Trained and evaluated in a constrained rapid-deployment setup (one day of pretraining); results reported only on LANL authentication logs (days 7→8). Word-tokenized models require OOV handling and per-user score centering to reduce inter-user bias. Shared vocabulary across fields can cause cross-field mispredictions. Some model variants (e.g., EM with fixed attention on character-tokenized data) show high variance or poor fit. No direct comparison to traditional anomaly-detection systems reported.",
            "uuid": "e5636.0",
            "source_info": {
                "paper_title": "Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Attention-augmented RNN LMs",
            "name_full": "Dot-product attention variants added to RNN language models (Fixed, Syntax, Semantic1, Semantic2, Tiered Attention)",
            "brief_description": "Dot-product attention mechanisms that compute weighted sums over previous LSTM hidden states to (1) give the predictor direct selective access to prior tokens and (2) provide interpretable attention weights indicating which prior tokens influenced predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dot-product attention + LSTM",
            "model_description": "Attention uses keys K = tanh(V W_a), query q (varies by variant), and d = softmax(q K^T) to produce attention vector a = d V, concatenated with h(t-1) to predict next token. Implemented attention dimension = 128; variants differ in how q is computed: Fixed (learned global q), Syntax (position-dependent q), Semantic1 (q = tanh(h W_sem1)), Semantic2 (h split into value and query sub-vectors), and Tiered attention (upper-tier attention over lower-tier hidden states).",
            "model_size": null,
            "anomaly_detection_method": "Augment the LSTM language model with attention shortcuts so predictions condition on an attention-weighted summary of prior hidden states; anomaly scoring remains negative log-likelihood of tokens. Attention weights are inspected for interpretability of anomaly decisions.",
            "data_type": "Structured sequential data (log-line token sequences; both word- and character-tokenized).",
            "anomaly_type": "Same as base LM: intrusion/red-team events, anomalous or low-probability token sequences; attention helps reveal which fields/tokens contribute to anomalous scores (syntactic or semantic anomalies).",
            "dataset_name": "LANL authentication logs",
            "performance_metrics": "AUC ROC; attention-augmented EM models improved EM to match BEM performance. All attention variants had similar AUCs for word-tokenized models. For character-tokenized models, semantic variants improved performance whereas Fixed and Syntax variants did not; numeric AUCs reported in tables (statistics over 5 runs) and abstract cites &gt;0.99 AUC in best cases.",
            "baseline_comparison": "Compared against non-attention LSTM baselines (EM, BEM) and tiered models. Attention improves EM to BEM-level performance and offers interpretability advantages; tiered attention provided little additional performance improvement over tiered non-attention models in these experiments.",
            "limitations_or_failure_cases": "Fixed and Syntax attention perform poorly for character-tokenized data with variable-length fields and can exhibit high variance. Semantic attention variants are more robust for variable-length inputs but still do not always exceed BEM. Attention sometimes concentrates on delimiters (e.g., commas) or field separator hidden states, which may indicate reliance on delimiters for positional cues. The attention-based approach does not replace the need for feature design in some cases (e.g., field-specific vocabularies could help).",
            "uuid": "e5636.1",
            "source_info": {
                "paper_title": "Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "DeepLog (Du et al.)",
            "name_full": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "brief_description": "Prior work mentioned that uses customized parsing of raw system logs to generate sequences which are fed to LSTM models to detect Denial-of-Service attacks and diagnose anomalies.",
            "citation_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "mention_or_use": "mention",
            "model_name": "LSTM (DeepLog pipeline)",
            "model_description": "Described in the paper as an LSTM-based approach applied to parsed log-event sequences (authors of the current paper cite Du et al. for parsing+LSTM approach to DoS detection), exact model hyperparameters and size not provided in this paper.",
            "model_size": null,
            "anomaly_detection_method": "Preprocess logs via customized parsing to produce event sequences then apply LSTM sequence model for anomaly detection (as described by Du et al. in related work).",
            "data_type": "Structured/semi-structured system log sequences produced by parsing raw text.",
            "anomaly_type": "Security anomalies (Denial of Service and related attacks) per the cited work.",
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "Mentioned to contrast with the present work: DeepLog uses custom parsing while the current work operates directly on raw tokens with minimal preprocessing (only tokenization and OOV handling).",
            "uuid": "e5636.2",
            "source_info": {
                "paper_title": "Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Zhang et al. (system failure prediction)",
            "name_full": "Automated IT system failure prediction: A deep learning approach",
            "brief_description": "Referenced prior work that uses clustering on raw text from multiple log sources to generate feature sequences fed to LSTM models for hardware and software failure prediction.",
            "citation_title": "Automated IT system failure prediction: A deep learning approach",
            "mention_or_use": "mention",
            "model_name": "LSTM (clustering + LSTM pipeline)",
            "model_description": "Described as clustering raw text from multiple log sources to form feature sequences and inputting those to LSTM networks for failure prediction; specific model sizes/hyperparameters not provided in this paper.",
            "model_size": null,
            "anomaly_detection_method": "Preprocessing via clustering then LSTM-based sequence prediction for failure detection.",
            "data_type": "Structured/semi-structured log-derived sequences aggregated across multiple sources.",
            "anomaly_type": "Hardware and software system failures (anomalous events leading to failures).",
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "Mentioned to contrast preprocessing-heavy pipelines with the current approach that minimizes feature engineering by operating on raw tokenized fields.",
            "uuid": "e5636.3",
            "source_info": {
                "paper_title": "Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Tuor et al. 2017",
            "name_full": "Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection",
            "brief_description": "Prior work that introduced the base language modeling framework for cyber anomaly detection (EM and BEM) which serves as the starting point for the present attention-augmented models.",
            "citation_title": "Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection",
            "mention_or_use": "use",
            "model_name": "RNN language models (EM/BEM) — prior formulation",
            "model_description": "Introduces EM (forward LSTM) and BEM (bidirectional LSTM) language models for open-vocabulary, event-level cyber anomaly detection; used as the foundation and extended here with attention and tiered variants.",
            "model_size": null,
            "anomaly_detection_method": "Unsupervised next-token probability modeling with LSTMs and negative log-likelihood scoring at event level.",
            "data_type": "System log event sequences (tokenized fields), open-vocabulary handling via embeddings and OOV token.",
            "anomaly_type": "Event-level cyber anomalies (intrusions, rare events).",
            "dataset_name": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "limitations_or_failure_cases": "Serves as baseline framework; this paper extends Tuor et al. by adding attention mechanisms and detailed interpretability analyses.",
            "uuid": "e5636.4",
            "source_info": {
                "paper_title": "Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection",
            "rating": 2,
            "sanitized_title": "recurrent_neural_network_language_models_for_open_vocabulary_eventlevel_cyber_anomaly_detection"
        },
        {
            "paper_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "rating": 2,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Frustratingly short attention spans in neural language modeling",
            "rating": 1,
            "sanitized_title": "frustratingly_short_attention_spans_in_neural_language_modeling"
        },
        {
            "paper_title": "Memory Architectures in Recurrent Neural Network Language Models",
            "rating": 1,
            "sanitized_title": "memory_architectures_in_recurrent_neural_network_language_models"
        },
        {
            "paper_title": "Automated IT system failure prediction: A deep learning approach",
            "rating": 1,
            "sanitized_title": "automated_it_system_failure_prediction_a_deep_learning_approach"
        }
    ],
    "cost": 0.012015749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection</p>
<p>Andy Brown browna52@wwu.edu 
Pacific Northwest National Laboratory
Pacific Northwest National Laboratory
Western Washington University
Western Washington University</p>
<p>Aaron Tuor aaron.tuor@pnnl.gov 
Pacific Northwest National Laboratory
Pacific Northwest National Laboratory
Western Washington University
Western Washington University</p>
<p>Brian Hutchinson brian.hutchinson@wwu.edu 
Pacific Northwest National Laboratory
Pacific Northwest National Laboratory
Western Washington University
Western Washington University</p>
<p>Nicole Nichols nicole.nichols@pnnl.gov 
Pacific Northwest National Laboratory
Pacific Northwest National Laboratory
Western Washington University
Western Washington University</p>
<p>Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection
CCS CONCEPTS • Computing methodologies → Anomaly detectionOnline learning settingsFeature selectionUnsupervised learningNeural networksMachine learning algorithmsKEYWORDS Anomaly detection, Attention, Recurrent Neural Networks, Inter- pretable Machine Learning, Online Training, System Log Analysis *
Deep learning has recently demonstrated state-of-the art performance on key tasks related to the maintenance of computer systems, such as intrusion detection, denial of service attack detection, hardware and software system failures, and malware detection. In these contexts, model interpretability is vital for administrator and analyst to trust and act on the automated analysis of machine learning models. Deep learning methods have been criticized as black box oracles which allow limited insight into decision factors. In this work we seek to "bridge the gap" between the impressive performance of deep learning models and the need for interpretable model introspection. To this end we present recurrent neural network (RNN) language models augmented with attention for anomaly detection in system logs. Our methods are generally applicable to any computer system and logging source. By incorporating attention variants into our RNN language models we create opportunities for model introspection and analysis without sacrificing state-of-the art performance. We demonstrate model performance and illustrate model interpretability on an intrusion detection task using the Los Alamos National Laboratory (LANL) cyber security dataset, reporting upward of 0.99 area under the receiver operator characteristic curve despite being trained only on a single day's worth of data.A. Brown et al.detection, and 2) we illustrate how model introspection in these systems is made possible by the attention mechanisms.</p>
<p>INTRODUCTION</p>
<p>System log analysis is critical for a wide range of tasks in maintaining large scale computer systems such as enterprise computer networks and high performance computing clusters. These include security tasks such as intrusion detection, insider threat detection, and malware detection, as well as more general maintenance tasks such as detecting hardware failure and modeling data or traffic flow patterns. Extracting knowledge from information rich system logs is complicated by several factors:</p>
<p>(1) Log sources can generate terabytes of data per day.</p>
<p>(2) Labeled data for application areas of interest is often scarce, unbalanced, or system specific. (3) Actionable information may be obscured by complex, undiscovered relationships across logging sources and system entities (e.g. users, PCs, processes, nodes).</p>
<p>Due to these factors, unaided human monitoring and assessment is impractical, so considerable research has been directed to automated methods for visualization and analysis of system logs. Furthermore, as administrative decisions may be of considerable consequence to organizations and associated persons, it is crucial to have some understanding of the factors involved in automated decision processes, even for highly effective algorithms.</p>
<p>Addressing these factors, we present unsupervised recurrent neural network (RNN) language models for system log anomaly detection. By modeling the normal distribution of events in system logs, the anomaly detection approach can discover complex relationships buried in these logs. Since the methods are unsupervised, the models do not depend on the time consuming and otherwise expensive procurement of labeled data. Our language modeling framework requires little to no feature engineering: it is applicable to any serializable logging source. Further, the models are trained online using bounded resources dictated by the daily volume of the log sources.</p>
<p>Our main contributions in this work are twofold: 1) we evaluate the effectiveness of augmenting RNN language models with several attention mechanisms specifically designed for system log anomaly</p>
<p>RELATED WORK</p>
<p>Recently, several researchers have used Long Short-Term Memory (LSTM) Networks [7] in system log analysis. Zhang et al. [23] use clustering techniques on the raw text from multiple log sources to generate feature sequences fed to an LSTM for hardware and software failure predictions. Du et al. [5] employ customized parsing methods on the raw text of system logs to generate sequences for LSTM Denial of Service attack detection. In contrast to these methods our approach works directly with raw text with no preprocessing beyond tokenization using known field delimiters. Others have incorporated LSTM networks to preprocess sequences of process API calls as components to malware detection systems [14] trained on labeled malware examples.</p>
<p>Attention-equipped LSTM models have been used to improve performance on complex sequence modeling tasks. Attention provides a dynamic weighted average of values from different points in a calculation during the processing of a sequence to provide long term context for downstream discriminative or generative prediction. In recent work [4,16,22], researchers have augmented LSTM language models with attention mechanisms in order to add capacity for modeling long term syntactic dependencies. Yogatama et al. [22] characterize attention as a differentiable random access memory. They compare attention language models with differentiable stack based memory [6] (which provides a bias for hierarchical structure), demonstrating the superiority of stack based memory on a verb agreement task with multiple attractors. Daniluk et al. [4] explore three additive attention [2] mechanisms with successive partitioning of the output of the LSTM; splitting the output into separate key, value, and prediction vectors performed best, likely due to removing the need for a single vector to encode information for multiple steps in the computation. In contrast we augment our language models with dot product attention [11,18], but also use separate vectors for the components of our attention mechanisms.</p>
<p>Many decision processes raise ethical dilemmas [12] or are applied in critical domains with high consequence. Such factors necessitate human interpretation of how a model is generating its predictions to ensure acceptable results. Vellido et al. [19] observe the gap between data modeling, knowledge extraction, and potential machine learning solutions, underscoring the need for interpretable automated decision processes. However, interpretability has multiple goals that are not always aligned with production of the most generalizable model architecture [10]. Hence, there is currently a large research focus on making interpretable deep learning algorithms for sensitive and critical application areas. Some proposed model introspection techniques include dimensionality reduction [20], analysis of intermediate layers [1] and saliency based methods [3,13]. In contrast to other deep learning components, attention mechanisms allow an immediate view into what factors are affecting model decisions. Xu et al. [21] examine attention weights to determine what convolutional neural networks are "looking" at while making predictions. Similarly, Rocktäschel et al. [15] analyze matrices of word-to-word attention weights for insight into how their LSTM entailment classifier reasons about sentences. We apply 1,C6@D1,U7@D2,C6,C6,Negotiate,Batch,LogOn,Success </p>
<p>METHODS</p>
<p>Here we describe the unsupervised language modeling framework and its extension via five variations of attention. In each case, the language models consume a sequence of log-line tokens and output log-line-level anomaly scores.</p>
<p>Preliminaries</p>
<p>3.1.1 Language Modeling. We assume that each log-line consists of a sequence of T tokens:
x (1:T ) = x (1) , x (2) , . . . , x (T ) . Each token x (t ) ∈ V,
where V denotes the vocabulary. A language model is a model that assigns probabilities to sequences: P(x (1:T ) ). A language model often evaluates the probability of a sequence using the chain rule of probability:
P(x (1:T ) ) = T t =1 P(x (t ) |x (&lt;t ) )(1)
where x (&lt;t ) denotes the (potentially empty) sequence of tokens from x (1) to x (t −1) . The conditional probabilities on the righthand side can be modeled with a recurrent neural network, as will be described in the Section 3.2.</p>
<p>Our data consist of a series of log-lines, each affiliated with a user. We denote user u's ith log-line with x (u,i) (1:T ) , but omit the superscript when it is non-essential. Our language models all output a single anomaly score, the negative log-likelihood, for each log-line. Figure 1 illustrates two methods to partition log lines into sequences of tokens: word and character tokenization. For word based language modeling, the tokens are the fields of the CSV format log file. The user fields are further split on the "@" character to generate user name and domain tokens. A frequency threshold is applied to replace infrequent words with an "out of vocabulary" (OOV) token; a value must occur in a field at least 40 times to be added to the vocabulary. The OOV token ensures that our models will have non-zero probabilities when encountering previously unseen words during evaluation.</p>
<p>Tokenization.</p>
<p>For character based language modeling we use a primitive vocabulary consisting of printable ASCII characters. This circumvents the out of vocabulary issues with the word model. Delimiters are left in the character inputs to give context of switching fields to the models. For both word and character tokenization, the time field is ignored and not tokenized. </p>
<p>Cyber Anomaly Language Models</p>
<p>We recently introduced a language modeling framework for cyber anomaly [17] that forms the starting point of this work. The first of four models presented in [17] is the "Event Model" (EM), which applies a standard LSTM [7] to the token sequences of individual events (log-lines). In order to feed the categorical tokens x (1:T ) into the model, we first perform an embedding lookup on each token to yield the sequence x (1:T ) (bold font), where each x (t ) ∈ R L emb for some embedding dimension hyperparameter, L emb . There are unique embedding vectors for each element in the vocabulary; these embedding vectors are parameters of the model, learned jointly with all other model parameters. An LSTM maps an embedding vector sequence to a sequence of hidden vectors h (1:T ) : 1
LSTM(x (1:T ) ) = h (1:T )(2)
Intuitively, h (t ) is a summary of the input sequence x (1:t ) defined by the same, standard LSTM equations used in [17]. Given the previous hidden state h (t −1) , weight matrix W and bias vector b, the probability distribution over the token at step t is:
p (t ) = softmax h (t −1) L h W L h × |V | + b |V | ∈ R |V |(3)
This conditions each prediction on all tokens that precede it in the log-line. The second model in [17] is the Bidirectional Event Model (BEM), which updates Eqn. 3 to also incorporate the hidden state from a backward-running LSTM, with hidden vector h b (t +1) and additional weight matrix W b as follows:
p (t ) = softmax h (t −1) W + h b (t +1) W b + b(4)
The BEM conditions each prediction on the all of the other tokens in the log-line (preceding or following), for richer context. The EM and BEM only condition predictions on other tokens in the same log line. However, Tuor et al. [17] also introduce tiered language model variants that employ an "upper tier" LSTM to model a user's sequence of log-lines (see Fig. 2). Each log-line is still For all language models (including the tiered models which incorporate inter-log-line context) we optimize the model parameters by minimizing the negative log-likelihood produced by EM or BEM predictions. The negative log-likelihood minimization objective also serves as the anomaly score for the log line (less probable events receiving higher anomaly scores).</p>
<p>Attention</p>
<p>In this work we use dot product attention (Figure 3), wherein an "attention vector" a is generated from three values: 1) a key matrix K, 2) a value matrix V, and 3) a query vector q. In this formulation, keys are a function of the value matrix:
K = tanh(VW a ),(5)
parameterized by W a . The importance of each timestep is determined by the magnitude of the dot product of each key vector with the query vector q ∈ R L a for some attention dimension hyperparameter, L a . These magnitudes determine the weights, d on the weighted sum of value vectors, a:
d = softmax(qK T ) (6) a = dV(7)
In an LSTM, the information relevant to a given prediction (e.g. of the next token, x (t +1) ) is accumulated and propagated via the LSTM's cell state, c (t ) . For any given prediction, however, certain tokens are likely to be more relevant than others. Attention provides a mechanism for predictions to be directly, selectively conditioned on a subset of the relevant tokens. In practice, this is accomplished by making p (t ) a function of the concatenation of h (t −1) with an attention vector a (t −1) that is a weighted sum over hidden states :
p (t ) = softmax h (t −1) a (t −1) W + b(8)
This attention mechanism not only introduces shortcuts in the flow of information over time, allowing the model to more readily access the relevant information for any given prediction, but the weights on the weighted sum also yield insights into the model's decision process, aiding interpretability. We first examine the case of adding attention to the standard EM. Each token-step t is associated with its own value matrix V (t ) , and query vector q (t ) . The value matrix V (t ) is the matrix of hidden states up to but excluding token-step t, where L h is the dimension of the LSTM hidden states. These are the values over which the weighted sum will be performed. (1) . . .
V (t ) =        hh (t −1)        ∈ R (t −1)×L h(9)
From the value matrix and weight matrix W a ∈ R L h ×L a , we compute a set of keys for each token/step:
K (t ) = tanh(V (t ) W a ) ∈ R (t −1)×L a(10)
Then,
d (t ) = softmax(q (t ) K T (t ) )(11)a (t ) = d (t ) V (t )(12)
Our EM attention variants differ primarily in the definition of the query vector q (t ) .</p>
<p>Fixed Attention.</p>
<p>In the fixed variation of attention [16] we let q (t ) = q for some fixed, learned vector q that is shared across all tokens/steps. This assumes some positions in the sequence are more important than others, but that importance does not depend on the token one is trying to predict.</p>
<p>Syntax</p>
<p>Attention. Syntax attention differs from fixed attention in that q (t ) is not shared across t. This assumes that some tokens are more important than others and this importance depends on the position in the sequence for the token to predict, but not on the actual values for tokens x 1 , . . . , x t −1 .</p>
<p>Semantic Attention 1.</p>
<p>For the first "semantic" variation, our query is a a function of the current hidden state and parameter matrix W sem1 ∈ R L h ×L a :
q (t ) = tanh(h (t ) W sem1 )(13)
3.3.4 Semantic Attention 2. Instead of making q (t ) a function of h (t ) , in this variant we interpret each h (t ) emitted from the LSTM as the concatenation of two vectors: h ′ (t ) and q (t ) . The query portion, q (t ) is used as before, but now the value
V (t ) defined in Eqn. 9 contains h ′ (1) through h ′ (t −1)
. Note that, per the LSTM equations, both h ′ (t ) and q (t ) will be fed back into the LSTM at time t + 1. Fig. 2, in original formulation of the tiered model, the lower tier LSTM hidden states are averaged in the process of passing information from the lower tier to the upper tier. Implementation of attention for the tiered language models replaces this mean with a weighted average via attention. Let V (u,i) be the lower tier hidden states for user u's ith log line:</p>
<p>Tiered Attention. As shown in
V (u,i) =          h (u,i)(1)
. . .  Let W t ier ∈ R L k ×L a and W a ∈ R L a ×L k be parameter matrices. We then define the following attention mechanisms:
h (u,i) (T )         (14)q = tanh(h (T ) W (t ier ) )(15)K = tanh(VW a )(16)d = softmax(qK T ) (17) a = dV(18)
We then replace the average of the hidden states in the tiered model with a. Note that each sequence shares weights W a and W t ier .</p>
<p>The BEM tiered attention model (TA-BEM) is depicted in Figure 5.</p>
<p>Online Training</p>
<p>We employ a syncopated online training algorithm, which both allows our model to continually adapt to changing distributions of activities on a network and to be deployed on high throughput streaming data sources. At the beginning of each day/cycle, the parameters of the current model are fixed for evaluation, thereby avoiding evolving anomaly score scale issues that could result from continuous online training. After anomaly scores have been calculated for the day's events we train on the current day's events. The days events are then discarded, bounding the storage demands of the algorithm to a day's worth of activity (plus model parameters).</p>
<p>On the first day we do not evaluate as the model has not had a training phase yet. At the cost of the additional space complexity of storing two copies of the model parameters, the training and evaluation phases can be run concurrently. The evaluation and training parameters are then synced daily so that the evaluation copy is updated with the parameters of the training copy at the beginning of each day.</p>
<p>EXPERIMENTS</p>
<p>This section discusses the data, experimental setup, evaluation metrics, and results assessing performance for the proposed methods.</p>
<p>Data</p>
<p>We evaluate our models on the publicly available LANL [8] dataset. LANL consists of over one billion log lines collected over 58 consecutive days. The logs contain anonymized process, network flow, DNS, and authentication information. Interleaved are attacks by a red team. Our experiments focus on modeling the authentication logs, which contain the following fields:</p>
<p>Source user, Destination user, Source pc, Destination pc, Authentication type, Logon type, Authentication orientation, Success/failure. These events are collected from desktop PCs, servers, and active directory servers using the Windows OS. We filter automated system events by discarding all log-lines that have a machine listed as the source user. Red team event log-lines are indicated in the dataset. As our models are fully unsupervised, we use the red team labels only for evaluation of model performance.</p>
<p>Experimental Setup</p>
<p>To assess our model's ability to spin up rapidly and detect anomalies with minimal burn-in time, we limit our scope to days 7 and 8, which contain 1 and 261 red team events respectively. Each of these days contains over seven million user log lines. We chose these particular days for evaluation because day 8 has the largest number of red events in the dataset. The entire experimental process is therefore 1) train on day 7, 2) evaluate on day 8. Further simulating a rapid deployment process, we performed no hyper-parameter tuning. Our learning rate is fixed to 0.01; we train using the ADAM [9] optimizer; the minibatch size is 64; our LSTMs have a single layer with 128 hidden units; our token embedding size is 128 and our attention size is 128. To estimate model variability, we trained each model five times with the fixed hyper-parameters but different random weight initializations. In our results section we report statistics over the five runs.</p>
<p>Metrics and Score Normalization</p>
<p>We evaluate our results using the area under the receiver operating characteristic curve (AUC ROC). ROC plots the true positive rate against the false positive rate as the detection threshold is swept. Perfect detection yields an AUC of 1 and random guessing yields 0.5. Recall that our anomaly scores, z (u,i) are given by the sum of the negative log probabilities of the tokens in line x  Table 2: AUC statistics for character tokenization models tokenization, we center each user's anomaly score:
z (u,i) ← z (u,i) − 1 N u i z (u,i) , ∀ u ,(19)
where N u is the number of events by user u in the day. This reduces inter-user anomaly bias, which can stem from the uneven distribution of user name tokens. This normalization is unnecessary for the character tokenization, as the user names are composed from a common character vocabulary.</p>
<p>Results</p>
<p>In this section we discuss performance of the different attention mechanisms. We note that variance of model performance across random parameter initializations is quite low for most models. This low variance given only a single day of pretraining suggests our method behaves predictably despite rapid deployment. word level LSTM baselines, the BEM outperforms the EM. However, adding attention to the EM improves performance to match the BEM. All variations of attention have very similar AUC scores. We hypothesize that the word model equally benefits from Syntax and Semantic attention, as it has a consistent syntax structure. Tiered word models with attention do not demonstrate as significant performance gains, however, both forward and bidirectional attention models trend slightly upwards in mean and maximum values from their non-attention counterparts. Table 2, the Fixed and Syntax attention models appear ill-suited for characterbased models with variable length fields; neither Fixed nor Syntax attention improve performance here, and the character EM model augmented with Fixed attention has a standard deviation 2-15 times that of other models. In contrast, semantic variants, where the attention weights are a function of the current input as opposed to sequence position, do improve performance but are not on par with the BEM. For the tiered models, we see little difference by incorporating attention, suggesting the shortcuts introduced by attention are unnecessary to propagate user context across loglines. One interesting outcome is that a tiered model with either attention or a bidirectional lower tier has reduced variance across random initializations by a large factor for the character models.</p>
<p>Word Tokenization Models.</p>
<p>Character Tokenization Models. As shown in</p>
<p>ANALYSIS</p>
<p>While attention performs comparably to bidirectionality, it offers substantial advantages in its interpretability. Investigating which fields the model is attending to (and when) offers clues to its decision-making. In this section we illustrate two approaches to analysis of attention-equipped LSTM language models: 1) Analysis of global model behavior from summary statistics of attention weights, and 2) analysis of particular model decisions from case studies of attention weights and language model predictions.</p>
<p>Global Behavior</p>
<p>We can gain insight into the global behavior of an attention-equipped LSTM from summary statistics such as the mean and standard deviation of attention weights over the course of a day's predictions. Figure 6 shows the average attention weights for each EM attention model when predicting the last meaningful token (Success/Fail). Error bars of one standard deviation are shown to illustrate the variability of these weights.</p>
<p>Heatmaps of average attention weights for the four EM attention models proposed in Section 3.3 are provided in Figures 7, 8, 9 and 10. Each time step in a sequence generates its own set of weights, d (t ) , over the previous hidden states. The larger the weight values are the more relevant the associated hidden state is to the current prediction. Note that the first input token is excluded from our figures as it has no previous hidden states to attend over. Figure 7 shows the mean weights for the Fixed attention which has a single fixed query that does not change with the context at the current time step. The source user, destination domain and source PC dominate the weight vectors, suggesting that they are the most important fields to this model. </p>
<p>Fixed.</p>
<p>Syntax.</p>
<p>With the syntax model ( Figure 8) each time step gets its own set of query weights. This makes sense for word tokenized models that have position dependent syntax. As an example of the model exhibiting intuitive behavior, when predicting the source PC, the model is attending heavily over the source user. </p>
<p>Semantic.</p>
<p>While the semantic attention mechanisms do not assume a fixed syntactic structure, Figures 9 and 10 show that both semantic attention variants learn reasonable attention strategies on this fixed syntax data. Overall they produce similar attention maps, attending heavily to source user and source PC. Semantic 1 also attends heavily to authentication type, while Semantic 2 also deems destination user and destination PC to be important.</p>
<p>Tiered attention models.</p>
<p>For the tiered model with a lower forward-directional LSTM, the attention weights were nearly all 1.0 for the second to last hidden state. This state is making the decision on success/fail, which conceptually makes sense with the goal of top tier LSTM to pass the most relevant information forward for the next event. Conversely, the tiered model with bidirectional LSTM cells attended fully on the very first hidden state. As Figure 5 shows, the backward LSTM ends with the first hidden state. Thus, the bidirectional tiered model is collecting both the final hidden state from the forward LSTM and the backward LSTM as its summary. This suggests that the shortcut connections attention provides are not needed for this model and task.  Figure 11: Red team word case study with semantic attention. See Figure 13 for description.  Figure 12: Low anomaly word case study with semantic attention. See Figure 13 for description.</p>
<p>Case Studies</p>
<p>We consider three case studies evaluated using semantic attention models. Figures 11 and 13 depict two randomly sampled red events evaluated with word and character semantic attention models, respectively. For contrast, Figure 12 is a random non-anomalous event evaluated with the semantic word model. Tokens where the predicted and true values diverge are of significant interest as they contribute heavily to the anomaly score. We can disregard the low probabilities when predicting the source user as it is impossible to foresee what user will be associated with a random input sequence.</p>
<p>Word Tokenization.</p>
<p>First consider the two word case studies. In both cases the source PC prediction is incorrect with low confidence. In the low-anomaly case the model is able to correctly predict the destination PC given the source PC token with very high probability. However, the red team event predicted a token associated with a different field for the destination PC. Examining the weights we see that the red team event was attending heavily over the hidden state taking the destination user domain as input and predicting the source user. We note that DOM1 is a very common domain in the LANL dataset and that the attention is likely considering the prediction that will be made from the embedding which relates to the current input token. This misclassification exposes a disadvantage in having a shared vocabulary for each field. Individual vocabularies for each field could improve performance, at the cost of minor feature engineering.</p>
<p>Character Tokenization.</p>
<p>Finally, we examine model function when processing a character tokenized red team event. When predicting the destination PC characters the hidden state associated with the comma character right before the prediction of the source PC has the largest associated weight. The second largest weight is the comma character right before the destination PC field begins. This may suggest that the model is learning positional information from the comma characters, or that it is accumulating summary vectors of the fields and "storing them" in the subsequent delimiter hidden state. Another point of interest is the attention weight   Figure 13: Red team case character study with Semantic attention. Coloring of the true token and predicted token rows is based on the probability of the given character during prediction. Green represents a near 100% probability while red is near 0%. Attention weights d(t) correspond to the top row of predictions. For example, when predicting character character 34, K, the model uses attention weights d(34). We provide a shifted copy of the predicted tokens at the bottom of the figure to align with the hidden states being attended to. Best viewed in color.</p>
<p>vector d(34). It will substantially impact our anomaly score as our model had almost 100% confidence that the next character would be 'K', while the true token, 'N', has near 0% probability. Again we see a heavy dependence on the delimiter hidden states.</p>
<p>CONCLUSIONS</p>
<p>In this paper we propose five attention mechanism implementations. The fixed and syntactic attention variants can be effective for modeling sequences with a fixed structure while semantic variants are more effective for input sequences that have varying lengths and looser structures. While maintaining state-of-the-art performance, the attention mechanisms provide information on both feature importance and relational mapping between features. Additionally, architectural insights can be gleaned from the attention applied, which may in the future lead to designing more effective models. Other future work includes evaluating the system on different tasks and domains (e.g. detection of hardware failures from computer logs). One could explore additional attention variants; e.g., bidirectional models with attention may lead to further gains in performance. Finally, equipping a lower tier model with the ability to attend over upper tier hidden states, may effectively weight the relevance of previous events in a user's log sequence.</p>
<p>Figure 1 :
1Top: Word tokens; Bottom: Character tokens the same concept to explore what factors our models attend over when predicting anomaly scores.</p>
<p>Figure 2 :
2Tiered language model (T-EM)[17].</p>
<p>Figure 3 :
3Dot Product Attention. modeled by an EM or BEM, but the input is the concatenation of embedding vectors x t along with a context vector produced by the upper tier LSTM. The upper tier LSTM takes as input a summary of the lower-tier hidden states (the average lower-tier hidden state concatenated with the final hidden state). The upper and lower tiers are trained jointly. For later reference, we name these models T-EM and T-BEM, respectively.</p>
<p>Figure 4 :
4Event Model (EM) with attention. Dotted lines indicate which hidden states are being attended over.</p>
<p>Figure 5 :
5Tiered attention with bidirectional lower tier</p>
<p>Figure 6 :Figure 7 :Figure 8 :
678Comparison of attention weights when predicting success/failure token. Average Fixed attention weights. Average Syntax attention weights.</p>
<p>Figure 9 :Figure 10 :
910Average Average Semantic 2 attention weights.</p>
<p>Table 1 shows
1AUC statistics for the word tokenization model experiments. Comparing the First Workshop On Machine Learning for Computer Systems, June 2018, Tempe, AZ USA A. Brown et al.
In this paper we assume all vectors are row vectors and adopt the notation convention of left multiplying matrices with row vectors (omitting the conventional transpose to avoid clutter).
ACKNOWLEDGMENTSThe research described in this paper is part of the Analysis in Motion Initiative at Pacific Northwest National Laboratory; conducted under the Laboratory Directed Research and Development Program at PNNL, a multi-program national laboratory operated by Battelle for the U.S. Department of Energy. The authors also thank Nvidia for their donations of Titan X GPU's used in this research.
Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, arXiv:1610.01644arXiv preprintGuillaume Alain and Yoshua Bengio. 2016. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644 (2016).</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma- chine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).</p>
<p>. Chun-Hao Chang, Elliot Creager, Anna Goldenberg, David Duvenaud, n. d.Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. [n. d.].</p>
<p>Interpreting Neural Network Classifications with Variational Dropout Saliency Maps. n. d.Interpreting Neural Network Classifications with Variational Dropout Saliency Maps. ([n. d.]).</p>
<p>Frustratingly short attention spans in neural language modeling. Michał Daniluk, Tim Rocktäschel, Johannes Welbl, Sebastian Riedel, arXiv:1702.04521arXiv preprintMichał Daniluk, Tim Rocktäschel, Johannes Welbl, and Sebastian Riedel. 2017. Frustratingly short attention spans in neural language modeling. arXiv preprint arXiv:1702.04521 (2017).</p>
<p>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. Min Du, Feifei Li, Guineng Zheng, Vivek Srikumar, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityACMMin Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 1285-1298.</p>
<p>Learning to transduce with unbounded memory. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom, Advances in Neural Information Processing Systems. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. 2015. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems. 1828-1836.</p>
<p>Long Short-Term Memory. Sepp Hochreiter, Jürgen Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 9Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (Nov. 1997), 1735-1780. https://doi.org/10.1162/neco.1997.9. 8.1735</p>
<p>Cyber security data sources for dynamic network research. D Alexander, Kent, Dynamic Networks and Cyber-Security. 137Alexander D Kent. 2016. Cyber security data sources for dynamic network research. Dynamic Networks and Cyber-Security 1 (2016), 37.</p>
<p>Adam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimiza- tion. CoRR abs/1412.6980 (2014). arXiv:1412.6980 http://arxiv.org/abs/1412.6980</p>
<p>The Mythos of Model Interpretability. Zachary Chase Lipton, arXiv:1606.03490Zachary Chase Lipton. 2016. The Mythos of Model Interpretability. CoRR abs/1606.03490 (2016). arXiv:1606.03490 http://arxiv.org/abs/1606.03490</p>
<p>Effective approaches to attention-based neural machine translation. Minh-Thang Luong, Hieu Pham, Christopher D Manning, arXiv:1508.04025arXiv preprintMinh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effec- tive approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025 (2015).</p>
<p>The ethics of algorithms: Mapping the debate. Patrick Brent Daniel Mittelstadt, Mariarosaria Allo, Sandra Taddeo, Luciano Wachter, Floridi, Big Data &amp; Society. 32053951716679679Brent Daniel Mittelstadt, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, and Luciano Floridi. 2016. The ethics of algorithms: Mapping the debate. Big Data &amp; Society 3, 2 (2016), 2053951716679679.</p>
<p>Explaining hyperspectral imaging based plant disease identification: 3D CNN and saliency maps. Koushik Nagasubramanian, Sarah Jones, K Asheesh, Arti Singh, Baskar Singh, Soumik Ganapathysubramanian, Sarkar, n. d.. n. d.Koushik Nagasubramanian, Sarah Jones, Asheesh K Singh, Arti Singh, Baskar Ganapathysubramanian, and Soumik Sarkar. [n. d.]. Explaining hyperspectral imaging based plant disease identification: 3D CNN and saliency maps. ([n. d.]).</p>
<p>Malware classification with recurrent networks. Razvan Pascanu, W Jack, Hermineh Stokes, Mady Sanossian, Anil Marinescu, Thomas, Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE. Razvan Pascanu, Jack W Stokes, Hermineh Sanossian, Mady Marinescu, and Anil Thomas. 2015. Malware classification with recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 1916-1920.</p>
<p>Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiskỳ, Phil Blunsom, arXiv:1509.06664Reasoning about entailment with neural attention. arXiv preprintTim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2015. Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664 (2015).</p>
<p>Attentive Language Models. Giancarlo Salton, Robert Ross, John Kelleher, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language Processing1Giancarlo Salton, Robert Ross, and John Kelleher. 2017. Attentive Language Models. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Vol. 1. 441-450.</p>
<p>Aaron Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, Rob Jasper, arXiv:1712.00557Recurrent Neural Network Language Models for Open Vocabulary Event-Level Cyber Anomaly Detection. arXiv preprintAaron Tuor, Ryan Baerwolf, Nicolas Knowles, Brian Hutchinson, Nicole Nichols, and Rob Jasper. 2017. Recurrent Neural Network Language Models for Open Vo- cabulary Event-Level Cyber Anomaly Detection. arXiv preprint arXiv:1712.00557 (2017).</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 6000-6010.</p>
<p>Making machine learning models interpretable. Alfredo Vellido, José David Martín-Guerrero, Paulo Jg Lisboa, ESANN. Citeseer12Alfredo Vellido, José David Martín-Guerrero, and Paulo JG Lisboa. 2012. Making machine learning models interpretable.. In ESANN, Vol. 12. Citeseer, 163-172.</p>
<p>Principal component analysis. Chemometrics and intelligent laboratory systems. Svante Wold, Kim Esbensen, Paul Geladi, 2Svante Wold, Kim Esbensen, and Paul Geladi. 1987. Principal component analysis. Chemometrics and intelligent laboratory systems 2, 1-3 (1987), 37-52.</p>
<p>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov, Richard S Zemel, Yoshua Bengio, arXiv:1502.03044Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. CoRR abs/1502.03044 (2015). arXiv:1502.03044 http://arxiv.org/abs/1502.03044</p>
<p>Memory Architectures in Recurrent Neural Network Language Models. Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom, International Conference on Learning Representations. Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom. 2018. Memory Architectures in Recurrent Neural Net- work Language Models. In International Conference on Learning Representations. https://openreview.net/forum?id=SkFqf0lAZ</p>
<p>Automated IT system failure prediction: A deep learning approach. Ke Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechrinis, Hui Zhang, Big Data (Big Data). Ke Zhang, Jianwu Xu, Martin Renqiang Min, Guofei Jiang, Konstantinos Pelechri- nis, and Hui Zhang. 2016. Automated IT system failure prediction: A deep learning approach. In Big Data (Big Data), 2016 IEEE International Conference on. IEEE, 1291-1300.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1665 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1665</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1665</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-258436994</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.01098v2.pdf" target="_blank">IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience</a></p>
                <p><strong>Paper Abstract:</strong> We present IndoorSim-to-OutdoorReal (I2O), an end-to-end learned visual navigation approach, trained solely in simulated short-range indoor environments, and demonstrates zero-shot sim-to-real transfer to the outdoors for long-range navigation on the Spot robot. Our method uses zero real-world experience (indoor or outdoor), and requires the simulator to model no predominantly-outdoor phenomenon (sloped grounds, sidewalks, etc). The key to I2O transfer is in providing the robot with additional context of the environment (i.e., a satellite map, a rough sketch of a map by a human, etc.) to guide the robot's navigation in the real-world. The provided context-maps do not need to be accurate or complete -- real-world obstacles (e.g., trees, bushes, pedestrians, etc.) are not drawn on the map, and openings are not aligned with where they are in the real-world. Crucially, these inaccurate context-maps provide a hint to the robot about a route to take to the goal. We find that our method that leverages Context-Maps is able to successfully navigate hundreds of meters in novel environments, avoiding novel obstacles on its path, to a distant goal without a single collision or human intervention. In comparison, policies without the additional context fail completely. Lastly, we test the robustness of the Context-Map policy by adding varying degrees of noise to the map in simulation. We find that the Context-Map policy is surprisingly robust to noise in the provided context-map. In the presence of significantly inaccurate maps (corrupted with 50% noise, or entirely blank maps), the policy gracefully regresses to the behavior of a policy with no context. Videos are available at https://www.joannetruong.com/projects/i2o.html</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1665.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1665.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>I2O</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IndoorSim-to-OutdoorReal (I2O) Context-Guided PointNav</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end learned visual navigation system trained only in simulated indoor environments (photorealistic 3D scans) that zero-shot transfers to long-range outdoor navigation on a Spot quadruped by combining egocentric depth sensing with rough allocentric context-maps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Boston Dynamics Spot (quadruped)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Quadrupedal robot using BD low-level controller; equipped with two front-facing Intel RealSense D430 depth cameras and BD localization/navigation API; used for long-range outdoor PointGoal navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>outdoor visual navigation / legged robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat (using HM3D and Gibson datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic 3D scanned indoor environments providing RGB-D observations, navigation meshes and occupancy/top-down maps; used to render depth inputs and to generate short-range navigation episodes for RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>photorealistic visual rendering combined with simplified kinematic motion (low-fidelity dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>high-fidelity photorealistic indoor geometry and textures, RGB-D rendering, allocentric top-down occupancy maps; kinematic position integration for agent movement</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>no full rigid‑body/dynamic simulation of robot locomotion or realistic outdoor terrain (no explicit slopes, sidewalks, vehicles modeled), kinematic control (Euler integration at 2 Hz) instead of dynamic contact physics, limited explicit sensor noise model (holes in depth addressed via augmentation rather than full sensor model)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Outdoor long-range routes hundreds of meters across with real-world obstacles (bushes, trees, cars, buildings, pedestrians) and varied weather (sunny, overcast, sunset); trials run on Spot with operator safety oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Long-range PointGoal navigation (reach a goal coordinate), including obstacle avoidance and slope traversal using egocentric depth and a high-level context-map.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning (Distributed Proximal Policy Optimization, DD-PPO) trained for 500M steps on indoor 3D scans</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success Rate (SR; reaching within 0.425 m of goal) and SPL in simulation; real-world trials reported SR and distance traveled (no SPL reported outdoors due to absent shortest-path oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Context-Map policy SR = 95.4% ± 0.2, SPL = 81.0 ± 2.4; No-Context SR = 78.7% ± 8.2, SPL = 56.4 ± 4.4 (reported on HM3D/Gibson test episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Context-Map policy: 100% success on the three long-range outdoor routes tested (three runs per route); No-Context policy: failed to complete routes (operator intervention to prevent collisions), effectively 0% success in those trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Camera pitch randomized during training (±30°) to emulate slopes; Random Erasing applied to depth images during training to increase robustness to missing pixels; optional training with noisy context-maps (shift and cutout) to improve robustness to map inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled outdoor phenomena (sloped/rocky terrain, sidewalks, dynamic obstacles like pedestrians and cars), simplified dynamics (no contact/dynamics), sensor differences and missing-depth holes, localization drift in real robot, incomplete/outdated maps.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of rough allocentric context-maps (Context-Map) to bias exploration, large-scale diverse indoor photorealistic training data, kinematic simulation (faster, lower-fidelity dynamics), sensor matching (depth completion / filtering), depth augmentation (Random Erasing), camera pitch randomization to expose agent to slope-like observations, and a robust real-world locomotion stack on Spot.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper argues full outdoor/dynamic fidelity is not required; accurate modeling of all outdoor phenomena is unnecessary when combining (1) robust locomotion, (2) sensor-domain matching (depth filtering/augmentation) and (3) high-level context hints — i.e., lower-fidelity dynamics plus good visual/sensor matching and context can suffice for zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Authors report and cite prior findings that kinematic (lower-fidelity) simulation yields better sim-to-real transfer for navigation than higher-fidelity dynamic simulation; they used kinematic control in this work because it yielded faster simulation and empirically better transfer (citing prior work). Training with noisy context-maps improved robustness to map corruption at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A policy trained solely in indoor photorealistic simulation can zero-shot transfer to long-range outdoor navigation on a legged robot when provided rough, potentially inaccurate top-down context-maps; context maps are critical (No-Context fails), and the Context-Map policy is robust to large map corruption (e.g., 50% cutout). Sensor preprocessing (depth completion), observation augmentation (Random Erasing, pitch randomization), and simplified kinematic simulation are practical enablers of successful sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1665.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1665.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kinematic Control (sim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kinematic control approximation for simulated robot movement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simplified simulation strategy that advances the agent via Euler integration (2 Hz) without running full rigid-body physics; collisions are handled by preventing motion to the next state. Used because it speeds simulation and empirically improves sim-to-real transfer for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Spot robot (simulated instance)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Simulated kinematic proxy of Spot used during RL training; does not model low-level gait or contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics navigation simulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat (kinematic motion model)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Photorealistic indoor scenes with agent moved by kinematic integration rather than dynamic simulators; produces depth observations and agent poses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>simplified dynamics / low-fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>agent position and orientation updates, collision detection at next pose (stop if collision), visual rendering of scenes and depth images</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>omits contact dynamics, actuator dynamics, slippage, inertial effects and fine-grained ground interaction that would be present in dynamic simulation</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Outdoor real-world deployment on Spot where true dynamics and contacts occur</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>High-level navigation policy (mapping observations+context to desired COM velocities)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DD-PPO reinforcement learning in kinematic simulation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Informal/empirical improvement in sim-to-real generalization reported and cited; overall transfer success of policies trained with kinematic control was high (see I2O results).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch between low-fidelity kinematic motion and true dynamics (contact, inertia) can contribute to gap, but authors argue kinematic choice reduced other simulation artifacts and improved generalization for navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Faster, lower-fidelity kinematic simulation allowed training at scale on photorealistic visuals and navigation episodes, which together with sensor matching and context-maps enabled transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper (and cited prior work) suggests that very high-fidelity dynamic modeling is not necessary for this class of navigation transfer, and that lower-fidelity kinematic simulation can even be advantageous.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Authors cite prior comparisons indicating kinematic (lower-fidelity) control leads to better sim-to-real transfer for navigation than detailed dynamics; this work adopts kinematic control and reports successful zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1665.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1665.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Depth Filtering & Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth completion, clipping and Random Erasing augmentation for sensor-domain matching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Depth images from Spot's real sensors were preprocessed with a depth completion algorithm and far ranges clipped to better match simulator depth; Random Erasing augmentation was applied during training to increase robustness to missing pixels in real depth sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Boston Dynamics Spot (sensing subsystem)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>RealSense D430 stereo depth cameras producing noisy/depth-hole-containing images in outdoor conditions; preprocessing used to align real sensor outputs with simulated depth.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>sim-to-real for vision-based navigation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat (simulated depth)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulated depth images (clean, hole-free within max range) rendered from 3D scans.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>photorealistic depth rendering with less sensor noise</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>depth range and per-pixel depth values; simulated depth images are relatively clean compared to raw real sensors</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>did not model full real sensor noise distributions and holes; instead used preprocessing and augmentation to bridge gap</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Raw noisy real-world depth frames with holes and far-range noise from Spot's D430 cameras captured outdoors</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based obstacle avoidance and navigation relying on depth inputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>RL with depth augmentation (Random Erasing) and matching preprocessing at test time</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Empirical success of transferred policies (Context-Map succeeded outdoors without collisions); augmentation reported as necessary to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Random Erasing applied to depth observations during training to simulate missing pixels/hole patterns and improve robustness to real depth noise.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Sensor noise, missing depth pixels/holes in real cameras, differing max-range behaviors; these contribute to failures unless addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Applying depth completion to real depth frames at test-time and using Random Erasing during training improved the match between sim and real observations and enabled reliable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate matching of sensor output statistics (or robust augmentation) is important; perfect modeling of every noise source is not required if preprocessing and augmentation are used.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1665.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1665.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context-Map Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-Guided PointNav (Context-Map)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A navigation policy that ingests a noisy allocentric top-down occupancy map (human sketch or auto-generated) as high-level context and attends to egocentric depth features to bias long-range exploration and enable zero-shot indoor-to-outdoor transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Boston Dynamics Spot (policy consumer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Policy outputs high-level linear and angular velocities to be executed by Spot's low-level controller; uses front-facing depth cameras and allocentric map input.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic navigation / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat (with allocentric top-down maps from HM3D/Gibson)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulated indoor environments with available top-down occupancy maps and shortest-path trajectories; context-maps represented as 2×N×N binary images and rotated into robot heading frame.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>photorealistic visuals + map context; context maps in sim are perfect unless noise injected for robustness experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>allocentric occupancy/top-down maps, egocentric depth, map rotation by heading, attention between map and depth features</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>map context in real-world is noisy/outdated (not modeled perfectly in base training), and real-world map acquisition errors/localization drift not fully simulated unless explicit noise is added</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Maps provided as human-sketched overlays on satellite images or auto-generated building/road segmentation; maps are incomplete/outdated relative to real obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Using coarse context hints to guide long-range navigation and avoid getting stuck in open outdoor spaces (bias high-level search, take shortcuts when visual freespace allows).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DD-PPO RL with ResNet18 encoder for maps, attention between depth and map features, GRU policy; trained on indoor datasets, optionally with map noise augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>SR and SPL in sim; SR and distance traveled in real trials. Robustness to map noise evaluated via SR and SPL under shift and cutout corruptions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Context-Map policy SR = 95.4% ± 0.2, SPL = 81.0 ± 2.4 (baseline sim results).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Context-Map policy achieved 100% SR on three tested outdoor routes (no collisions, no human interventions) even though maps were inaccurate/outdated.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>At test-time robustness tested to map 'shift' and 'cutout' corruptions; authors also trained variants with noisy maps (20% shift, 50% cutout) improving robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Map inaccuracies, localization shift errors, and dynamic/unseen obstacles; classical planners using the same maps fail because they cannot adapt to gross map inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Context-Maps provide high-level bias while egocentric vision enables local adaptation; attention architecture fusing map and depth; training on diverse indoor geometry; ability to gracefully ignore corrupted maps (policy regresses to No-Context behavior when map uninformative).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High-precision, up-to-date maps are not required — approximate, even noisy maps are sufficient and the policy is robust to significant corruption (e.g., 50% cutout yields ~81% SR); thus precise map fidelity is not a strict requirement for successful transfer in this approach.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Context-Map policies trained on perfect maps retained substantial performance when evaluated with noisy/outdated maps; training with noisy maps further improved robustness. Context-Waypoint and Context-Trajectory variants were far more sensitive to noise than Context-Map.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation <em>(Rating: 2)</em></li>
                <li>Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance? <em>(Rating: 2)</em></li>
                <li>ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1665",
    "paper_id": "paper-258436994",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "I2O",
            "name_full": "IndoorSim-to-OutdoorReal (I2O) Context-Guided PointNav",
            "brief_description": "An end-to-end learned visual navigation system trained only in simulated indoor environments (photorealistic 3D scans) that zero-shot transfers to long-range outdoor navigation on a Spot quadruped by combining egocentric depth sensing with rough allocentric context-maps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Boston Dynamics Spot (quadruped)",
            "agent_system_description": "Quadrupedal robot using BD low-level controller; equipped with two front-facing Intel RealSense D430 depth cameras and BD localization/navigation API; used for long-range outdoor PointGoal navigation experiments.",
            "domain": "outdoor visual navigation / legged robotics",
            "virtual_environment_name": "Habitat (using HM3D and Gibson datasets)",
            "virtual_environment_description": "Photorealistic 3D scanned indoor environments providing RGB-D observations, navigation meshes and occupancy/top-down maps; used to render depth inputs and to generate short-range navigation episodes for RL training.",
            "simulation_fidelity_level": "photorealistic visual rendering combined with simplified kinematic motion (low-fidelity dynamics)",
            "fidelity_aspects_modeled": "high-fidelity photorealistic indoor geometry and textures, RGB-D rendering, allocentric top-down occupancy maps; kinematic position integration for agent movement",
            "fidelity_aspects_simplified": "no full rigid‑body/dynamic simulation of robot locomotion or realistic outdoor terrain (no explicit slopes, sidewalks, vehicles modeled), kinematic control (Euler integration at 2 Hz) instead of dynamic contact physics, limited explicit sensor noise model (holes in depth addressed via augmentation rather than full sensor model)",
            "real_environment_description": "Outdoor long-range routes hundreds of meters across with real-world obstacles (bushes, trees, cars, buildings, pedestrians) and varied weather (sunny, overcast, sunset); trials run on Spot with operator safety oversight.",
            "task_or_skill_transferred": "Long-range PointGoal navigation (reach a goal coordinate), including obstacle avoidance and slope traversal using egocentric depth and a high-level context-map.",
            "training_method": "Deep reinforcement learning (Distributed Proximal Policy Optimization, DD-PPO) trained for 500M steps on indoor 3D scans",
            "transfer_success_metric": "Success Rate (SR; reaching within 0.425 m of goal) and SPL in simulation; real-world trials reported SR and distance traveled (no SPL reported outdoors due to absent shortest-path oracle).",
            "transfer_performance_sim": "Context-Map policy SR = 95.4% ± 0.2, SPL = 81.0 ± 2.4; No-Context SR = 78.7% ± 8.2, SPL = 56.4 ± 4.4 (reported on HM3D/Gibson test episodes).",
            "transfer_performance_real": "Context-Map policy: 100% success on the three long-range outdoor routes tested (three runs per route); No-Context policy: failed to complete routes (operator intervention to prevent collisions), effectively 0% success in those trials.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Camera pitch randomized during training (±30°) to emulate slopes; Random Erasing applied to depth images during training to increase robustness to missing pixels; optional training with noisy context-maps (shift and cutout) to improve robustness to map inaccuracies.",
            "sim_to_real_gap_factors": "Unmodeled outdoor phenomena (sloped/rocky terrain, sidewalks, dynamic obstacles like pedestrians and cars), simplified dynamics (no contact/dynamics), sensor differences and missing-depth holes, localization drift in real robot, incomplete/outdated maps.",
            "transfer_enabling_conditions": "Use of rough allocentric context-maps (Context-Map) to bias exploration, large-scale diverse indoor photorealistic training data, kinematic simulation (faster, lower-fidelity dynamics), sensor matching (depth completion / filtering), depth augmentation (Random Erasing), camera pitch randomization to expose agent to slope-like observations, and a robust real-world locomotion stack on Spot.",
            "fidelity_requirements_identified": "The paper argues full outdoor/dynamic fidelity is not required; accurate modeling of all outdoor phenomena is unnecessary when combining (1) robust locomotion, (2) sensor-domain matching (depth filtering/augmentation) and (3) high-level context hints — i.e., lower-fidelity dynamics plus good visual/sensor matching and context can suffice for zero-shot transfer.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Authors report and cite prior findings that kinematic (lower-fidelity) simulation yields better sim-to-real transfer for navigation than higher-fidelity dynamic simulation; they used kinematic control in this work because it yielded faster simulation and empirically better transfer (citing prior work). Training with noisy context-maps improved robustness to map corruption at test time.",
            "key_findings": "A policy trained solely in indoor photorealistic simulation can zero-shot transfer to long-range outdoor navigation on a legged robot when provided rough, potentially inaccurate top-down context-maps; context maps are critical (No-Context fails), and the Context-Map policy is robust to large map corruption (e.g., 50% cutout). Sensor preprocessing (depth completion), observation augmentation (Random Erasing, pitch randomization), and simplified kinematic simulation are practical enablers of successful sim-to-real transfer.",
            "uuid": "e1665.0",
            "source_info": {
                "paper_title": "IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Kinematic Control (sim)",
            "name_full": "Kinematic control approximation for simulated robot movement",
            "brief_description": "A simplified simulation strategy that advances the agent via Euler integration (2 Hz) without running full rigid-body physics; collisions are handled by preventing motion to the next state. Used because it speeds simulation and empirically improves sim-to-real transfer for navigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Spot robot (simulated instance)",
            "agent_system_description": "Simulated kinematic proxy of Spot used during RL training; does not model low-level gait or contact dynamics.",
            "domain": "robotics navigation simulation",
            "virtual_environment_name": "Habitat (kinematic motion model)",
            "virtual_environment_description": "Photorealistic indoor scenes with agent moved by kinematic integration rather than dynamic simulators; produces depth observations and agent poses.",
            "simulation_fidelity_level": "simplified dynamics / low-fidelity",
            "fidelity_aspects_modeled": "agent position and orientation updates, collision detection at next pose (stop if collision), visual rendering of scenes and depth images",
            "fidelity_aspects_simplified": "omits contact dynamics, actuator dynamics, slippage, inertial effects and fine-grained ground interaction that would be present in dynamic simulation",
            "real_environment_description": "Outdoor real-world deployment on Spot where true dynamics and contacts occur",
            "task_or_skill_transferred": "High-level navigation policy (mapping observations+context to desired COM velocities)",
            "training_method": "DD-PPO reinforcement learning in kinematic simulation",
            "transfer_success_metric": "Informal/empirical improvement in sim-to-real generalization reported and cited; overall transfer success of policies trained with kinematic control was high (see I2O results).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Mismatch between low-fidelity kinematic motion and true dynamics (contact, inertia) can contribute to gap, but authors argue kinematic choice reduced other simulation artifacts and improved generalization for navigation tasks.",
            "transfer_enabling_conditions": "Faster, lower-fidelity kinematic simulation allowed training at scale on photorealistic visuals and navigation episodes, which together with sensor matching and context-maps enabled transfer.",
            "fidelity_requirements_identified": "Paper (and cited prior work) suggests that very high-fidelity dynamic modeling is not necessary for this class of navigation transfer, and that lower-fidelity kinematic simulation can even be advantageous.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Authors cite prior comparisons indicating kinematic (lower-fidelity) control leads to better sim-to-real transfer for navigation than detailed dynamics; this work adopts kinematic control and reports successful zero-shot transfer.",
            "uuid": "e1665.1",
            "source_info": {
                "paper_title": "IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Depth Filtering & Augmentation",
            "name_full": "Depth completion, clipping and Random Erasing augmentation for sensor-domain matching",
            "brief_description": "Depth images from Spot's real sensors were preprocessed with a depth completion algorithm and far ranges clipped to better match simulator depth; Random Erasing augmentation was applied during training to increase robustness to missing pixels in real depth sensors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Boston Dynamics Spot (sensing subsystem)",
            "agent_system_description": "RealSense D430 stereo depth cameras producing noisy/depth-hole-containing images in outdoor conditions; preprocessing used to align real sensor outputs with simulated depth.",
            "domain": "sim-to-real for vision-based navigation",
            "virtual_environment_name": "Habitat (simulated depth)",
            "virtual_environment_description": "Simulated depth images (clean, hole-free within max range) rendered from 3D scans.",
            "simulation_fidelity_level": "photorealistic depth rendering with less sensor noise",
            "fidelity_aspects_modeled": "depth range and per-pixel depth values; simulated depth images are relatively clean compared to raw real sensors",
            "fidelity_aspects_simplified": "did not model full real sensor noise distributions and holes; instead used preprocessing and augmentation to bridge gap",
            "real_environment_description": "Raw noisy real-world depth frames with holes and far-range noise from Spot's D430 cameras captured outdoors",
            "task_or_skill_transferred": "Vision-based obstacle avoidance and navigation relying on depth inputs",
            "training_method": "RL with depth augmentation (Random Erasing) and matching preprocessing at test time",
            "transfer_success_metric": "Empirical success of transferred policies (Context-Map succeeded outdoors without collisions); augmentation reported as necessary to improve robustness.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Random Erasing applied to depth observations during training to simulate missing pixels/hole patterns and improve robustness to real depth noise.",
            "sim_to_real_gap_factors": "Sensor noise, missing depth pixels/holes in real cameras, differing max-range behaviors; these contribute to failures unless addressed.",
            "transfer_enabling_conditions": "Applying depth completion to real depth frames at test-time and using Random Erasing during training improved the match between sim and real observations and enabled reliable transfer.",
            "fidelity_requirements_identified": "Accurate matching of sensor output statistics (or robust augmentation) is important; perfect modeling of every noise source is not required if preprocessing and augmentation are used.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "uuid": "e1665.2",
            "source_info": {
                "paper_title": "IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Context-Map Policy",
            "name_full": "Context-Guided PointNav (Context-Map)",
            "brief_description": "A navigation policy that ingests a noisy allocentric top-down occupancy map (human sketch or auto-generated) as high-level context and attends to egocentric depth features to bias long-range exploration and enable zero-shot indoor-to-outdoor transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Boston Dynamics Spot (policy consumer)",
            "agent_system_description": "Policy outputs high-level linear and angular velocities to be executed by Spot's low-level controller; uses front-facing depth cameras and allocentric map input.",
            "domain": "robotic navigation / sim-to-real transfer",
            "virtual_environment_name": "Habitat (with allocentric top-down maps from HM3D/Gibson)",
            "virtual_environment_description": "Simulated indoor environments with available top-down occupancy maps and shortest-path trajectories; context-maps represented as 2×N×N binary images and rotated into robot heading frame.",
            "simulation_fidelity_level": "photorealistic visuals + map context; context maps in sim are perfect unless noise injected for robustness experiments",
            "fidelity_aspects_modeled": "allocentric occupancy/top-down maps, egocentric depth, map rotation by heading, attention between map and depth features",
            "fidelity_aspects_simplified": "map context in real-world is noisy/outdated (not modeled perfectly in base training), and real-world map acquisition errors/localization drift not fully simulated unless explicit noise is added",
            "real_environment_description": "Maps provided as human-sketched overlays on satellite images or auto-generated building/road segmentation; maps are incomplete/outdated relative to real obstacles.",
            "task_or_skill_transferred": "Using coarse context hints to guide long-range navigation and avoid getting stuck in open outdoor spaces (bias high-level search, take shortcuts when visual freespace allows).",
            "training_method": "DD-PPO RL with ResNet18 encoder for maps, attention between depth and map features, GRU policy; trained on indoor datasets, optionally with map noise augmentation.",
            "transfer_success_metric": "SR and SPL in sim; SR and distance traveled in real trials. Robustness to map noise evaluated via SR and SPL under shift and cutout corruptions.",
            "transfer_performance_sim": "Context-Map policy SR = 95.4% ± 0.2, SPL = 81.0 ± 2.4 (baseline sim results).",
            "transfer_performance_real": "Context-Map policy achieved 100% SR on three tested outdoor routes (no collisions, no human interventions) even though maps were inaccurate/outdated.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "At test-time robustness tested to map 'shift' and 'cutout' corruptions; authors also trained variants with noisy maps (20% shift, 50% cutout) improving robustness.",
            "sim_to_real_gap_factors": "Map inaccuracies, localization shift errors, and dynamic/unseen obstacles; classical planners using the same maps fail because they cannot adapt to gross map inaccuracies.",
            "transfer_enabling_conditions": "Context-Maps provide high-level bias while egocentric vision enables local adaptation; attention architecture fusing map and depth; training on diverse indoor geometry; ability to gracefully ignore corrupted maps (policy regresses to No-Context behavior when map uninformative).",
            "fidelity_requirements_identified": "High-precision, up-to-date maps are not required — approximate, even noisy maps are sufficient and the policy is robust to significant corruption (e.g., 50% cutout yields ~81% SR); thus precise map fidelity is not a strict requirement for successful transfer in this approach.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Context-Map policies trained on perfect maps retained substantial performance when evaluated with noisy/outdated maps; training with noisy maps further improved robustness. Context-Waypoint and Context-Trajectory variants were far more sensitive to noise than Context-Map.",
            "uuid": "e1665.3",
            "source_info": {
                "paper_title": "IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation",
            "rating": 2,
            "sanitized_title": "rethinking_sim2real_lower_fidelity_simulation_leads_to_higher_sim2real_transfer_in_navigation"
        },
        {
            "paper_title": "Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?",
            "rating": 2,
            "sanitized_title": "sim2real_predictivity_does_evaluation_in_simulation_predict_realworld_performance"
        },
        {
            "paper_title": "ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints",
            "rating": 1,
            "sanitized_title": "viking_visionbased_kilometerscale_navigation_with_geographic_hints"
        }
    ],
    "cost": 0.016758,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience
10 May 2023</p>
<p>Joanne Truong 
Georgia Institute of Technology</p>
<p>April Zitkovich 
Sonia Chernova 
Georgia Institute of Technology</p>
<p>Dhruv Batra 
Georgia Institute of Technology</p>
<p>MetaAI</p>
<p>Tingnan Zhang 
Jie Tan 
Wenhao Yu 
Robotics At Google 
IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience
10 May 2023C786AD910AA96D7907F7E9A7EDBC902FarXiv:2305.01098v2[cs.RO]</p>
<p>Figure 1: Left: We learn a visual navigation policy using large-scale reinforcement learning in simulated indoor environments.Right: We demonstrate zero-shot transfer of this policy for long-range outdoor navigation on the Spot robot.One key ingredient is to combine information from an inaccurate high-level map (titled Context-Map) and accurate-but-limited onboard sensing.The Context-Map is available trivially in indoor-sim and available from satellite imagery or human sketching in outdoor-real experiments.Notice how approximate and incorrect the map is (none of the cars or sidewalks show up on the map), but it does provide a crucial hint to the robot about an opening between the bushes.The robot leverages this hint to navigate up the slope to the opening in the bushes (orange), takes a shortcut between a tree and a wooden pole (blue) that is not marked on the map but visible in egocentric images, to successfully reach the goal position at the entrance of a distant building (red), utilizing the best of an outdated map and up-to-date (but partial) onboard sensing.</p>
<p>Abstract: We present IndoorSim-to-OutdoorReal (I2O), an end-to-end learned visual navigation approach, trained solely in simulated short-range indoor environments, and demonstrates zero-shot sim-to-real transfer to the outdoors for long-range navigation on the Spot robot.Our method uses zero real-world experience (indoor or outdoor), and requires the simulator to model no predominantlyoutdoor phenomenon (sloped grounds, sidewalks, etc).The key to I2O transfer is in providing the robot with additional context of the environment (i.e. a satellite map, a rough sketch of a map by a human, etc.) to guide the robot's navigation in the real-world.The provided context-maps do not need to be accurate or complete-real-world obstacles (e.g.trees, bushes, pedestrians, etc.) are not drawn on the map, and openings are not aligned with where they are in the real-world.Crucially, these inaccurate context-maps provide a hint to the robot about a route to take to the goal.We find that our method that leverages Context-Maps is able to successfully navigate hundreds of meters in novel environments, avoiding novel obstacles on its path, to a distant goal without a single collision or human intervention.In comparison, policies without the additional context fail completely.Lastly, we test the robustness of the Context-Map policy by adding varying degrees of noise to the map in simulation.We find that the Context-Map policy is surprisingly robust to noise in the provided context-map.In the presence of significantly inaccurate maps (corrupted with 50% noise, or entirely blank maps), the policy gracefully regresses to the behavior of a policy with no context.Videos are available on our project website.</p>
<p>Introduction</p>
<p>Much of Earth's landmass outdoors is occupied by challenging terrain that is inaccessible to wheeled robots.On the other hand, humans and legged animals are able to explore most of this landmass by finding stable footholds to navigate through these challenging terrains.Legged platforms provide robust locomotion, and have demonstrated successful sim2real transfer in challenging and diverse terrain such as the outdoors [1][2][3][4].Alongside these advancements in locomotion for legged robots, recent works in deep reinforcement learning have demonstrated success in training virtual robots to navigate efficiently in simulation before transferring the learned skills to real-world environments [5][6][7][8][9][10][11][12][13], on both wheeled and legged robots.These advances were made possible due to the development of fast, scalable simulators [10,[14][15][16][17][18][19][20][21][22][23] and the availability of large-scale datasets of photorealistic 3D scans of indoor environments [24][25][26].In this paper, we seek to bring the same advancements made in indoor visual navigation to the outdoors for legged robots.</p>
<p>Outdoor navigation with legged robots is different from indoor navigation, and is relatively understudied.While indoor navigation typically span tens of meters, outdoor navigation navigating hundreds of meters to travel from one building to another.At this scale of navigation, taking the wrong turn or backtracking can be costly, and simple exploration based methods can be inefficient.</p>
<p>We present IndoorSim-to-OutdoorReal (I2O), which enables a quadrupedal robot to successfully navigate hundreds of meters in novel outdoor environments, around previously unseen outdoor obstacles (trees, bushes, buildings, pedestrians, etc.), in different weather conditions (sunny, overcast, sunset) -despite being trained solely in simulated indoor environments.The robot has never seen any outdoor environments, and has only been trained using short-range trajectories (on average 8m).We show that by training on large-scale diverse photorealistic indoor environments, we enable a robot to learn navigational skills that are transferable to the outdoors.</p>
<p>We find that the key to enabling I2O is to provide the robot with additional context of its environment (i.e. via a satellite image, a rough sketch by a human, etc.), which allows the learned policy to bias its search, obviating the need for costly exhaustive search and backtracking in the real-world.We provide additional context to the robot through a rough human sketch over a satellite image from the environment to guide the robot's navigation.These context-maps do not need to be accurate, but serve as a hint in the general directions that the robot should explore.These sketches enable a human user to specify obscure paths that may be difficult for the robot to find otherwise, or paths that are not visible in satellite images (i.e. a small opening through bushes shown in Figure 1).It is difficult to apply traditional path planning methods on these inaccurate, incomplete, and outdated context-maps.Instead, the robot must learn to use them in conjunction with observations from its onboard cameras to adapt to on-the-ground reality -avoiding obstacles not drawn on the map (bushes, chairs, people).</p>
<p>In our experiments, we find that the robot with the Context-Map is able to navigate hundreds of meters to a distant goal without a single collision or human intervention.In contrast, without the additional context, the robot completely fails at navigating long-ranges outdoors, and is only able to make minor progress towards the goal before getting stuck around obstacles.We conduct a comprehensive quantitative analysis in simulation, and demonstrate that in indoor environments, the additional context can improve success rate by 17%, and improve path efficiency by 22%.Additionally, we find that the Context-Map policy is surprisingly robust to noise in the provided context-map.When the maps are inaccurate (corrupted with 50% noise, or an entirely blank map), the performance of the policy gracefully regresses back to the performance of policies trained without any context.</p>
<p>Related Work</p>
<p>Outdoor Navigation.Outdoor navigation has a long history in robotics for wheeled and legged robots, and autonomous vehicles.Classical navigation approaches decompose the problem into a sense-plan-act pipeline [27]: a map of the environment is built, the robot is localized within the map, and a planning algorithm is used to generate a path to the goal [28][29][30].While this classical pipeline has demonstrated successful long-range outdoor navigation [31,32], these approaches are reliant on accurate, pre-computed maps of the environment built using expensive LiDAR sensors, and are not adaptable to changes in the environment such as dynamic obstacles, or differences in weather.Our work removes the assumption of an accurate map of the environment.We use approximate and inaccurate maps to provide a hint to the robot, which are not accurate enough to support planning but contain enough information for a learned policy to improve performance using onboard sensing to adapt to the changes in the environment.</p>
<p>A variety of learning-based methods have been proposed for vision-based outdoor obstacle avoidance and navigation.One prominent method is to leverage offline-learning using large real-world datasets.Muller et al. [33] collect a large dataset using teleoperation, and trained a system endto-end to map short-range observations to steering wheel angles for outdoor obstacle avoidance.Hadsell et al. [34] use a offline dataset of stereo images to train a long-range vision traversability classifier, which is used build a hyperbolic polar map.The map is used with traditional path planning techniques for navigation.Sermanet et al. [35] accomplish long-range, off-road robot navigation by combining [33,34] to plan at at multiple ranges.Recent works investigated learning navigational affordances for long-range navigation from egocentric images by leveraging real-world datasets [36][37][38].The large, diverse, offline dataset consists of real-world navigation trajectories collected over months in a wide variety of environments (with the same robot embodiment) using a mix of teleoperation and random walk.Some works couple this learned module with planning on top of a pre-computed topological map [39,40].In contrast, our approach does not require any explicit topological map construction, and the robot can navigate immediately in a new environment without any physical pre-exploration.Shah et al. [41] is the most similar method to ours.They propose to use geographical hints for the task of image goal navigation outdoors.However, their work uses a large offline dataset to reason about traversability, and propose waypoints to reach long horizon goals.In contrast, our work is learned entirely in simulation and transfers to the real-world zero-shot; no real-world experience or outdoor data required and our approach is trivially adaptable to new robots because the new robot embodiment can be easily simulated.</p>
<p>An alternative to using real-world datasets is to leverage (outdoor) simulators.Sorokin et al. [42] use CARLA [43], an autonomous driving simulator, to train a quadrupedal robot to navigate on sidewalks by following waypoints generated by public map services (e.g.Google Maps).In our work, we remove the assumption of being provided pre-computed waypoints; instead we learn to extract navigational hints directly from the context-map and onboard sensing.This enables our robot to navigate using a high-level hint, and take shortcuts through rough terrains and openings when possible.</p>
<p>Indoor Visual Navigation &amp; Sim2real Transfer.Many works in indoor visual navigation leverage large-scale learning with photorealistic simulators, such as Habitat [14,15], iGibson [16,44], or AI2-THOR [18], and techniques for reducing the sim2real gap [45][46][47][48][49][50][51] to enable zero-shot sim2real transfer on wheeled and legged robots [5][6][7][8][9][10][11][12][13].We build upon these advances in indoor visual navigation to demonstrate long-range navigation outdoors by leveraging large-scale learning in short-range simulated indoor environments and outdated maps of the environment.</p>
<p>3 IndoorSim-to-OutdoorReal Transfer</p>
<p>Experimental Setup</p>
<p>Task: PointGoal Navigation.In PointGoal Navigation (PointNav) [52], a robot is initialized in a previously unseen environment and needs to navigate to a goal location (i.e."go to ∆x, ∆y").The robot has access to egocentric RGB-D observations, and an egomotion sensor which is used to calculate the goal location relative to the robot's current pose.The robot operates within constraints of maximum number of steps per episode (500 steps per 50m from the goal location) and velocity limits (±0.5 m /s for linear and ±0.3 rad /s for angular velocities).An episode is considered successful if the robot reaches within 0.425m (half the width of the Spot robot) of the goal location.For evaluation, we report the robot's success rate (SR), and Success inversely weighted by Path Length (SPL) [52], which measures the robot's path efficiency.</p>
<p>Dataset.</p>
<p>We use the Habitat-Matterport (HM3D) [24] and Gibson [44] 3D datasets, which consists of over 1000 scans of real-world indoor environments (homes, offices, etc.) consisting of realistic clutter (tables, chairs, etc.).We use the training and evaluation navigation episodes from [9], which were generated to result in complex paths (up to 30m) that are navigable by the Spot robot.The Context-Map input does not need to be very accurate, but should serve as a rough guide for general directions that the robot should explore (i.e. a satellite map showing roughly where buildings are).Consequently, the robot must use observations from its camera to adapt to novel obstacles or clutter present in the environment but absent on the map, and be willing to take shortcuts available in the world but shown as obstacles on the map.We represent the map as a top-down occupancy map, which can be obtained in the real-world by converting a satellite image to illustrate occupied and freespace through a human sketch, or through an automated process.Using a binary occupancy map provides a few benefits over directly using satellite images.An abstracted binary map allows the maps to be used for both indoor and outdoor navigation, while satellite images are only applicable to outdoor navigation.Additionally, by using human sketches for context maps, the human operator can give hints to the robot about paths that may not be easily visible in the satellite image, or from the robot's initial position (i.e.openings in bushes).In our experiments, we use human-sketched Context-Maps, however these maps can easily be generated automatically by postprocessing a digital map. Figure 4 shows an example of a human-sketched Context-Map (top row), and an automatically generated Context-Map (bottom row).Additional details on automatically generated Context-Maps are described in Section 7.6 in the Appendix.</p>
<p>Context-Guided PointGoal Navigation</p>
<p>Policy Architecture.We train a high-level visual navigation policy entirely in simulation using deep reinforcement learning.We first describe the architecture of a PointNav policy that operates without the additional context (No-Context) from [9], then we outline our extension to the policy architecture to incorporate additional environmental context.The policy takes as input an egocentric depth image, a goal vector, and the action at the previous timestep.The goal vector is represented as the distance and heading relative to the robot's current pose.The output of the policy is the desired center-of-mass linear and angular velocities (v x , ω) for the robot to follow.The depth observations are processed using a 3 layer CNN visual encoder, and the goal vector is encoded using a linear layer.These features are fed into a 1 layer GRU, followed by a single linear layer which parameterizes a Gaussian action distribution from which the action is sampled.The policy architecture is shown in Figure 5, left.Additional details are described in Section 7.3 of the Appendix.</p>
<p>Next, we describe how to incorporate additional context information for ContextNav.We first describe map-based context inputs (Context-Map and Context-Trajectory), followed by waypointbased context inputs (Context-Waypoint).In simulation, we use freely accessible top-down maps of indoor environments as additional context to aid navigation.The context-map is represented as a 2×N ×N matrix, where N ×N represents the size of the map (we use N = 100).For Context-Map, the first channel of the map is an allocentric occupancy map, in which the cells denote obstacles (0) or freespace (1).For Context-Trajectory, the first channel of the map is an allocentric shortest-path trajectory map obtained by running A* on the top-down map of the environment.In the trajectory map, the cells denote the shortest path trajectory (1), and 0 otherwise.For both map-based context inputs, the allocentric map is rotated according to the robot's current heading.The second channel of the map illustrates the agent's current location in the top-down map (1), and the location of the goal coordinate with a small disk (1), and 0 otherwise.An example of the second channel is overlaid in green and red in the Context-Map in Figure 3, left.We process the map context inputs using a ResNet18 visual encoder [53].To provide waypoints as context to the navigation policy, we calculate the next closest waypoint along the shortest path to the goal within 1m from the robot's current position.The waypoint is specified using polar coordinates (r, θ), representing the distance and heading from the robot's current position.The waypoint is encoded using a two-layer MLP, with a 512 hidden dimension.We compute the scaled dot-product attention [54] between the depth and context features, and pass the attended features into a second GRU.Additional details are described in Section 7.4 of the Appendix.</p>
<p>Training details.We train all our policies for 500M steps using DD-PPO [55], a distributed reinforcement learning method, in the Habitat simulator [14,15].The reward function is derived from [9], which includes a dense reward for following the shortest path to the goal, a terminal reward for successful episodes, a slack penalty to incentivize the robot to reach the goal quickly, a penalty for collisions, and a penalty for backward velocities, which can lead to collisions and hurts performance.Additional details on the reward function are described in Section 7.8 of the Appendix.</p>
<p>Techniques to Aid IndoorSim-to-OutdoorReal Transfer.</p>
<p>Empirically, we found that several additional key techniques were needed to enable IndoorSim-to-OutdoorReal transfer.We use these techniques for all policies tested in the real-world.</p>
<p>Indoor-to-Outdoor Transfer.Two of the main differences between indoor and outdoor navigation are 1) navigation length (short-range vs. long-range), and 2) terrain type (flat vs. rocky/sloped).First, we found that traditional PointNav policies were highly sensitive to the goal vector.Since the policies were only trained in simulation, and typically see trajectories ∼8m away, the policies failed to generalize to longer-range goals.We normalize the goal vector by using the log of the goal distance, which enabled longer-range navigation.Next, we found that naively trained PointNav policies had difficulty navigating up slopes.Since slopes are infrequent in indoor environments, when the robot sees a slope outdoors, the slope appears as a large obstacle in the robot's depth camera, and thus the robot avoids walking up the slope.To enable the robot to walk up and down slopes, we randomize the pitch of the camera during training by ±30 • .Adding pitch randomization artificially adds slopes to the robot's observation during training, which enables the robot to walk up slopes in the real-world.</p>
<p>Sim-to-Real Transfer.We use a few techniques to enable zero-shot sim2real transfer.In simulation, instead of modeling the robot's low-level locomotion control, we use kinematic control as an approximation for the robot's movement.In kinematic control, the robot is moved to its next state via Euler integration at 2Hz without running full rigid-body physics.If the robot were to collide at the next state, we simply keep the robot in place.Kinematic control was shown in [9] to lead to better sim-to-real transfer through faster simulation, as compared to dynamic control.Next, we filter the depth images from Spot's depth cameras in the real-world to better match observations from simulation.The raw (noisy) depth images from Spot are show in in Fig 6 (left).We use depth completion from [56] to fill in holes and smooth the image.Additionally, we set the pixels further away from the max depth range of the camera (3.5m) to white to match the depth images from simulation.The filtered depth image is shown in Figure 6 right.Lastly, improve the robustness of our policies to missing pixels in real-world depth images, we add add Random Erasing noise [57] to our depth observations during training.</p>
<p>Real-world Setup</p>
<p>We use the Boston Dynamics (BD) Spot robot in simulation and the real-world.Our navigation policy outputs high-level velocity commands, and we rely on BD's low-level controller for movement.We use Spot's two front-facing Intel Realsense D430 depth cameras for visual inputs to our policy.We disable the BD obstacle avoidance in order to isolate the performance of our policy from confounding factors; however, a human operator was vigilant to terminate the episode (and report it as a failure) if a collision were imminent.The BD API includes two modes for high-level navigation, but both methods have limitations and cannot be used directly for long-range outdoor navigation.We detail the two modes and their failure cases for long-range navigation in Section 7.7 of the Appendix.</p>
<p>Results</p>
<p>In this section, we address the following questions: 1) Can a robot trained to navigate solely in simulated short-range indoor environments zero-shot transfer to long-range outdoors in the realworld?2) How vital is the use of the additional context?3) How robust is the policy to noise?</p>
<p>Zero-shot IndoorSim-to-OutdoorReal Navigation</p>
<p>We investigate zero-shot sim2real indoor2outdoor navigation.We task the robot with navigating to 3 long-range goals outdoors (shown in Figure 7), with many real-world obstacles present (bushes, buildings, cars, tables, pedestrians, etc.) and different weather conditions (sunny, overcast, sunset).We report the average success rate (SR) and distance travelled across 3 runs for each method in Table 1.Videos of our experiments are available on our project website.</p>
<p>No-Context Real-world Outdoor Navigation.First, we test to see if policies with No-Context can directly transfer to long-range navigation outdoors.The robot is required to navigate around large obstacles to reach the goal successfully.However, we find that the No-Context policy immediately makes a beeline towards the goal using the goal vector for guidance.7: We test our learning-based navigation policies in novel environments for long-range outdoor navigation using the Spot robot.The three routes in the real-world contain many obstacles including bushes, buildings, cars and pedestrians that the robot has never seen before during training.We show the trajectories taken by our Context-Map policies in the outdoors (black); the policy is able to navigate hundreds of meters outdoors to successfully reach the goal 100% of the time.In comparison, the No-Context policies (blue) try to make a direct beeline towards the goal, and get stuck on obstacles, thereby failing to reach the goal.7).</p>
<p>Route</p>
<p>In indoor environments, there are more obstacles around the robot, such as walls, that guide the robot to avoid making a beeline to the goal.In the outdoors however, the robot makes a beeline presumably led by the depth sensors that indicate plenty of freespace around the robot.This leads the robot to wander into obstacles such as bushes, resulting in unsuccessful episodes.The No-Context policy completely fails to navigate in all three routes, each only making minor progress to the goal before an operator had to intervene to prevent a collision (Table 1).The trajectories taken by the No-Context policy is shown in blue in Figure 7.In the first route, we task the robot with navigating from one building to another (38m forward, 16m to the right).The shortest path to the next building requires passing through a small opening to the left of the robot between two bushes that is not visible to the robot from the start.Since the goal vector indicates that the goal is to the right, the robot starts walking to the right, and gets stuck searching by the bushes, resulting in an episode failure.Traditional Planning Real-world Outdoor Navigation.Next, we test to see if outdated context-maps can aid in navigating outdoors.We first investigate leveraging classical approaches to plan using the context-map.We run RRT* on the context map to generate a list of waypoints to the goal.These waypoints are then used with the Context-Waypoint policy, which was trained in simulation to follow waypoints along the shortest route to the goal.We find that the Context-Waypoint policy is able to successfully navigate to each successive waypoint.However, the robot ended up missing the actual opening in bushes because the waypoints were generated from an outdated map (shown in Figure 8).This confirms our conjecture that classical planning based approaches are highly sensitive to the map input, and are not able to adapt to gross inaccuracies present in the maps.In the real-world, creating a perfect, complete, and always-up-to-date map is not realistic.We show in the next section that the Context-Map policy can successfully reach the goal despite being given the same outdated, inaccurate map.</p>
<p>Context-Guided Real-world Outdoor Navigation.Finally, we test our Context-Map policy outdoors.We provide the robot with rough sketches indicating preferred paths for the robot to take for each route (Figure 7).Notice how rough and incomplete the provided maps are-obstacles such as cars, trees, or chairs are not shown on the map, and only rough hints for an opening to the goal is depicted.While classical planning methods that used these exact same outdated maps failed to reach the goal, we find that the Context-Map policy is able to leverage the context-maps to bias its search during outdoor navigation, and successfully reach the distant goal location 100% of the time (Table 1), without a single collision or human intervention.Shah et al. [41] report similar results using 42 hours of real-world robot navigation data.In contrast, our approach uses zero real-world experience.</p>
<p>We find that our policies are able to maintain a balance between the Context-Maps, and what it sees in its visual inputs.The robot leverages depth images to avoid clutter/dynamic obstacles, and Context-Maps are used for high-level guidance.While Context-Maps provide a means for the operator to specify a preferred route for the robot to take, the robot may take shortcuts along the way that are not present in the map when its vision senses freespace.However, we do not observe the robot finding a drastically 'better' solution than what is hinted on the map (i.e., a better path to the right, when the map indicates free space only on the left), due to the robot's limited vision (recall that the depth cameras have a range of only ∼3m).With our approach, the robot was able to navigate around dynamic obstacles such as pedestrians and real-world clutter despite these obstacles not being drawn directly on the context-map.Our approach is also not limited to 2D navigation; the robot navigates up slopes (Route 1), and can navigate in various terrains including dirt slopes and grass.</p>
<p>Simulation Results</p>
<p>Indoor Navigation.We conduct extensive experiments in simulation to systematically quantify the value of using the additional context input.We first evaluate using 1200 episodes of indoor environments from the HM3D and Gibson scenes [9].We report the average success rate (SR) and SPL across 3 seeds in Table 2. To get an upperbound for performance, we evaluate 2 policies that get different levels of oracle context-a policy that receives as input the next waypoint along the shortest path to the goal (row 3 in Table 2), and a policy that receives a top-down map with the shortest path trajectory drawn on the map (row 4 in Table 2).We find that the Context-Map policies achieve a similar high success rate and SPL as the policies that get as input oracle context (95.4 vs. 96.5 SR, and 81.0 vs. 86.3SPL; Table 2 rows 2-4), demonstrating that the policy is able to utilize its given context-map to navigate to the goal.In contrast, the No-Context policies achieve a lower success rate of 78.7% (-16.7%SR, rows 1 and 2 in Table 2), demonstrating that the use of the additional context is useful in guiding the robot in its navigation to successfully reach the goal.We also see a +24.6%SPL between the two policies, demonstrating that the robot can navigate to the goal more efficiently using the added context.These experiments in demonstrate that Context-Map policies can navigate through obstacles in cluttered environments, and is capable of complex navigating through multiple rooms.</p>
<h1>Method Context SR ↑ SPL ↑1</h1>
<p>Indoor Navigation using Outdated Maps.We test the robustness of our policy by adding varying degrees of noise to the context-map, shown in Figure 9. Specifically, we use:</p>
<ol>
<li>
<p>Shift noise, which randomly shifts the map provided to the robot in any direction.This experiment simulates localization errors that may occur on the robot.</p>
</li>
<li>
<p>Cutout noise, which randomly adds patches of free space or obstacle space to the map.This experiment simulates giving the robot a outdated map (i.e. a map with additional or missing obstacles in the environment).</p>
</li>
</ol>
<p>We find that our approach, trained entirely using perfect maps, is surprisingly robust to the noisy maps.At 50% cutout noise, the original top-down map is barely visible (Figure 9), and planning algorithms would be unable to find any feasible path to the goal.In contrast, our policy is still able to navigate to the goal successfully 81.4% of the time (row 7 in Table 4).When the Context-Map policy is given a map that is completely freespace (100% noise, row 8 in Table 4) the policy behavior regresses to the No-Context policy (79.8 vs. 78.7 SR, and 59.8 vs. 56.4SPL).Thus, our approach exhibit the best of both worlds -utilizing the information in the map when it's available but never underperforming a map-free approach.For completeness, we additionally train policies explicitly on noisy maps and report the results in Section 7.2 of the Appendix.We find that by training with noisy maps in simulation, the policy performance is even more robust when evaluated with noisy maps.</p>
<p>Discussion &amp; Limitations</p>
<p>Our results provide compelling evidence for rejecting the (admittedly reasonable) hypothesis that a new simulator must be designed for every new scenario we wish to study.This is especially important for environments that are challenging to design in simulation, such as the outdoors.Traditional approaches for 'virtualizing' a real-world environment to import into a simulator leverage infrared projection based 3D scanners.However, in the outdoors, infrared light interference from the sun prohibits scanning these environments accurately.Instead, our approach shows that simulators can be used for zero-shot real-world transfer without having to design and model the deployment scenario apriori.Our work uses zero real-world experience (indoor or outdoor) and requires the simulator to model no predominantly-outdoor phenomenon (terrain, ditches, sidewalks, cars, etc).The power in the approach comes from a combination of a robust locomotion system on Spot (which itself uses a well-designed classical robotics stack), low sim-vs-real gap in depth and map sensors, and the power of large-scale learning.Similar to literature in cognitive science [58] that find that humans use approximate cognitive maps (i.e. through landmarks) to guide navigation, our results indicate that providing robots with approximate, high-level maps can enable long-range navigation in novel outdoor environments.</p>
<p>Our approach suffers from a few limitations.First, there may still be certain instances in which the robot may fail such as when both the depth observation and Context-Map may suggest freespace in front of the robot despite an obstacle being present (narrow railings, glass walls, etc.).To avoid such failures, it is important to incorporate other sensing modalities such as RGB images or proprioception input [59], or to enable the robot with additional actions (look up, look down, etc.) to increase the robot's sensing.Additionally, our approach relies on the BD API for localization, which may drift over time as the robot navigates even longer distances.In the future, we would like to remove the assumption accurate localization sensors on the robot.</p>
<p>While our approach demonstrates that large-scale learning with simulated data alone can enable outdoor navigation, prior works have also demonstrated impressive results using real-world data.In the future we would like to explore combining learning with both real-world and simulated data to further improve our policy.Training the policy from scratch in the physical outdoor environment is intractable and may cause damage to the robot over time, but it may be beneficial to fine-tune a pretrained policy in the real-world.An alternative is to leverage offline trajectories from the real-world to augment the simulator [50,60].</p>
<p>Conclusion</p>
<p>In this work, we present a framework for IndoorSim-to-OutdoorReal transfer for navigation.Our navigation policy is trained solely in simulated short-range trajectories from indoor datasets, yet is able to navigate long ranges outdoors in the real-world.The navigation policy is robust to novel environments and obstacles such as sloped grounds, bushes, and trees never seen during training.Keys to the system's success are a set of sim-to-real techniques that enable the policy to handle real outdoor environments, as well as the addition of context in the form of a rough sketch provided by a human, which guides the robot's navigation.This work opens up navigation research to the less explored domain of the rich and diverse outdoors environments.</p>
<p>Appendix</p>
<p>The appendix is structured as follows: We evaluated the Context-Waypoint and Context-Trajectory policies (trained with perfect context) in simulation with varying noise levels.For the Context-Waypoint policies, we add shift noise to each of the waypoint inputs.Specifically, at the start of each episode, we obtain waypoints along the shortest path to the goal.We sample noise from a uniform distribution and add the noise to the waypoint.We use the perturbed waypoints as input to the policy.For the Context-Trajectory policy, we add shift noise following the same procedure used for Context-Maps, described in Section 4.2.We find that these context-variants are more susceptible to noisy inputs, furthering our conjecture that these methods are not suitable to run in the real-world where it's not realistic to have perfect trajectories and waypoints.In the presence of noise, the performance of the other context variants drop significantly.We see a maximum decrease of 59.In our experiments, we demonstrated that Context-Map policies trained with perfect maps in simulation do not exhibit overfitting.These policies achieve high performance in sim and the real-world when evaluated with inaccurate maps (shown in Table 1 and 4).To further improve the robustness of our Context-Map policies, we additionally train Context-Map policies with noisy maps in simulation (using 20% shift noise, and 50% cutout noise).We find that these Context-Map policies trained with noise perform just as well as Context-Map trained without noise (Table 4 vs.Table 5.We see that policies trained with noise are even more robust to varying degrees of noise during test-time (rows 2-4 and 6-8 in Table 5).In evaluation with varying noise levels, the Context-Map policies trained with noise maintain strong performance (93.3 ± 1.6 SR at 20% shift noise, and 92.2 ± 1.1 SR at 50% cutout noise).</p>
<p>Context-Maps in the Real-World</p>
<p>We show the Context-Maps used for each real-world route in Figure 10, middle.Starting from the satellite map (first column), a human sketches a map indicating a route for the robot to use to navigate (middle column).We show the context-map overlaid on the satellite map (last column).Notice how inaccurate, incomplete, and outdated the provided context-maps are.The maps hint at a route to the goal for the robot, yet do not show any real-world obstacles that exist on the way to the goal (e.g.bushes, cars, tables, chairs, pedestrians).The Context-Map obtained from segmenting out buildings gives the robot a high-level idea of the direction to travel to reach the goal.While both variants are useful for navigating on structured paths and large buildings, we note that small man-made openings that are not captured on digital or satellite maps (i.e.small opening between bushes that exists for Route 1) are not accurately shown on the map.In this case, the human operator can modify the segmented Context-Map to interject additional paths for the robot to use to navigate to the goal.Using these automatically generated context-maps for robot navigation outdoors is a promising direction for future work.</p>
<p>Satellite Map Context-Map Overlay</p>
<p>Boston Dynamics Navigation API</p>
<p>Boston Dynamics API has two modes for high-level navigation.The first method for navigation is a mapless high-level navigation API (i.e.go to position).However, this method cannot navigate around obstacles autonomously, and the robot will stop at the first obstacle it encounters.The second method is Spot's AutoWalk feature in which a user can record and replay navigation missions with Spot.However, this method requires a user to apriori teleoperate Spot through the desired route, and requires fiducials to be placed throughout the desired route to ensure that Spot is properly localized.More fiducuals are needed as the length of the route increases; without the fiducials to localize with, Spot will fail to replay the mission.In the outdoors, different lighting or weather conditions such as strong sunlight or the nightime, may cause the fiducials to not be properly detected by Spot.In contrast, our approach uses zero real-world outdoor experience, and is capable of navigating in different weather conditions.</p>
<p>Reward Function</p>
<p>Our reward function is derived from [9].The reward function is defined as: r t (a t , s t ) = R success + R geo + R slack + R backward + R coll (8)</p>
<p>Figure 2 :
2
Figure 2: Examples of indoor environments used for training and evaluation in simulation.</p>
<p>Figure 3 :
3
Figure 3: Three kinds of contextual input.We aim to leverage additional context information freely accessible through Google Maps satellite images for the task of long-range PointNav outdoors.We denote this variant of PointNav with additional environmental context, as Context-Guided PointNav (Con-textNav).The additional context input (Figure 3) can be in the form of an outdated map (Context-Map), a ground truth trajectory (Context-Trajectory), or ground truth waypoints for the robot to follow (Context-Waypoint).We train and compare the performance of all three forms of contextual input in simulation, with Context-Trajectory and Context-Waypoint policies serving as the upperbound for performance when provided with oracle context.However, we find in our simulation experiments that that Context-Trajectory and Context-Waypoint policies are susceptible to noise in the contextual input, while Context-Map policies are robust to noise.This points to Context-Maps being the best context modality for sim2real transfer.In the real-world, we present results using Context-Maps.Additional details are described in Section 7.1 in the Appendix.</p>
<p>Figure 4 :
4
Figure 4: Top Row: Using a satellite image of the area from Google Maps(left), a human operator sketches a rough map to serve as context input to the robot (middle).Notice how defective the map is (right): large parts or entire building are missing, no roads or sidewalks are shown; but crucially, the map contains a hint for an opening to get to the goal.Context-Maps for realworld routes are shown in Section 7.5 of the Appendix.Bottom Row: Context-Maps can also be automatically generated.Starting from the digital map (left), we automatically generate context-maps by segmenting out buildings (middle), or roads (right).</p>
<p>Figure 5 :
5
Figure5: Left: The No-Context PointNav policy architecture consists of a 3-layer CNN to process depth images from the robot's camera, and a MLP to process the goal vector.The policy is a GRU, and outputs linear and angular velocities for the Spot robot to follow.Right: The Context-Guided PointNav architecture includes an additional encoder to process the context-input (top-down map or waypoint).We compute the scaled dot product attention between the map and the depth image, and use a second GRU to process the attended features.</p>
<p>Figure 6 :
6
Figure 6: Left: Raw depth images from the Spot robot's front cameras.Right: We filter the depth images using depth completion from [56] to better match images from simulation.</p>
<p>Figure</p>
<p>Figure7: We test our learning-based navigation policies in novel environments for long-range outdoor navigation using the Spot robot.The three routes in the real-world contain many obstacles including bushes, buildings, cars and pedestrians that the robot has never seen before during training.We show the trajectories taken by our Context-Map policies in the outdoors (black); the policy is able to navigate hundreds of meters outdoors to successfully reach the goal 100% of the time.In comparison, the No-Context policies (blue) try to make a direct beeline towards the goal, and get stuck on obstacles, thereby failing to reach the goal.</p>
<p>Figure 8 :
8
Figure 8: Left: We run RRT* on an outdated contextmap, to find waypoints (blue) along the shortest path to the goal.We pass the waypoints into a Context-Waypoint policy to navigate to the goal (black).Right: Since the map is inaccurate, the waypoints lead the robot past the opening in the bushes (1), leading to an episode failure (2).</p>
<p>Figure 9 : 4
94
Figure 9: We use shift and cutout noise to degrade the top-down map obtained in simulation.# Eval Noise SR ↑ SPL ↑ Type Percent 1 --95.4±0.2 81.0 ±2.4</p>
<p>Figure 10 :
10
Figure 10: Left: Satellite images obtained via Google Maps showing the three real-world routes used to test our navigation policies.Middle: We provide outdated Context-Maps to our policy to navigate to the long-range goal.Right: We overlay the Context-Maps on the satellite map to show how outdated, and incomplete the maps are.Our navigation policies can leverage these imperfect maps to successfully navigate to the goal.</p>
<ol>
<li>6 Figure 11 :
611
Figure 11: We automatically generate Context-Maps for the three routes used in our real-world experiments (routes 1-3 by row).Starting from the digital map (left), we automatically generate context-maps by segmenting out roads (2nd column), or buildings (4th column).</li>
</ol>
<p>Table 1 :
1#GoalMethodSR ↑Distance Travelled (m) ↑138m Forward, 16m RightNo-Context Context-Map 100.0 0.016.6±0.1 63.4±2.5290m Forward, 30m LeftNo-Context Context-Map 100.0 0.09.7±3.4 112.2±1.8395m Forward, 45m LeftNo-Context Context-Map 100.0 0.05.1±0.3 129.8±2.8
We test the No-Context and Context-Map policies on 3 long-range outdoor routes (Figure</p>
<p>Table 2 :
2
The Context-Map policy navigates just as well as policies that are provided oracle context (waypoint, trajectory).Additionally, we see that the Context-Map policy achieves a 16.7% higher success rate and 24.6% higher SPL than the No-Context policy.
No-Context-78.7±8.2 56.4±4.42ContextMap95.4±0.2 81.0±2.43ContextWaypoint 96.4±0.4 86.3±0.64ContextTrajectory 96.5±0.1 84.5±1.2</p>
<p>Table 3 :
3
We evaluate Context-Map policies with varying degrees of noise to the map.</p>
<p>Table 4 :
4
We evaluate Context-Waypoint and Context-Trajectory policies with varying degrees of noise, and report the average across 3 seeds.We find that these methods are more susceptible to noisy inputs than Context-Map policies.
1. Indoor Navigation using Noisy Context Variants2. Improved Robustness for Indoor Navigation using Outdated Maps3. PointNav Policy Architecture4. ContextNav Policy Architecture5. Context-Maps in the Real-World6. Auto-Generated Context-Maps7. Boston Dynamics Navigation API8. Reward functions9. Additional Simulation Details7.1 Indoor Navigation using Noisy Context Variants# Method Eval NoiseSR ↑SPL ↑1No noise96.4±0.4 86.3±0.62Context-0.25m Shift 88.3±1.6 62.3±3.43Waypoint0.5m Shift 77.3±0.3 47.5±2.841.0m Shift 36.7±2.0 18.5±1.45No noise96.5±0.1 84.5±1.26Context-5% Shift68.5±1.4 42.7±1.37Trajectory10% Shift57.8±2.8 33.1±0.1820% Shift46.9±1.9 25.7±0.8</p>
<p>Table 5 :
5
7 SR and 67.8 SPL for Context-Waypoint (row 1 vs 4), and maximum decrease of 49.6 SR, 58.8 SPL for Context-Trajectory (row 5 vs 8).In contrast, Context-Map policies are robust to noise (Table4).This points to Context-Maps being the best context modality for sim2real transfer.We evaluate Context-Map policies with varying degrees of noise to the map, and report the average across 3 seeds.Policies trained with noisy maps maintain performance when evaluated with noisy maps.
7.2 Improved Robustness for Indoor Navigation using Outdated Maps# Train Noise Eval NoiseSR ↑SPL ↑1-94.0±1.8 73.6±5.82 320% Shift5% Shift 10% Shift94.0±1.7 73.7±5.6 93.8±1.6 73.9±6.0420% Shift93.3±1.6 73.6±6.25-91.6±1.3 72.4±3.16 750% Cutout10% Cutout 91.6±1.6 72.7±3.1 25% Cutout 92.5±1.1 73.1±3.5850% Cutout 92.2±1.1 72.6±3.1
AcknowledgmentsThe Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE.JT was supported by an Apple Scholars in AI/ML PhD Fellowship.The views and conclusions are those of the authors and should not be interpreted as representing the U.S. Government, or any sponsor.License for dataset used.Gibson Database of Spaces.License at http://svl.stanford.edu/gibson2/assets/GDS_agreement.pdfSatellite maps used are from Google Maps.Digital maps used are from OpenStreetMap.PointNav Policy ArchitectureWe outline the No-Context PointNav policy architecture from[9].Given depth observation d t , we encode the observation using a 3-layer CNN ψ to obtain depth features dt .dt = ψ(d t )(1) The goal vector g t is represented in polar coordinates [r, θ], representing the distance and heading relative to the robot's current position.Following[55], we transform this into [r, cos(θ), sin(θ)] to account for the discontinuity at the x-axis in polar coordinates.We then pass the goal vector into a full connected layer to get a 32-dimensional output ĝt .The previous action a t−1 is also encoded with a fully connected layer with a 32-dimensional output ât−1 .We pass the depth features, goal features, previous action features, and previous hidden state into a 1 layer GRU τ a with a 512-dimensional hidden size.We denote the hidden state from this GRU h a t .x,The output of the GRU x is fed into two parallel linear layers with a 2-dimensional output size (size of the action space).The outputs µ and σ parameterize a Gaussian action distribution from which the action is sampled.ContextNav Policy ArchitectureWe extend the original PointNav policy to incorporate additional environmental context.We use the same architecture for processing the depth obseravtion, goal vector, and previous action as above, and add a separate GRU to process the additional environmental context.We describe the process for encoding 3 types of context input (Context-Map, Context-Trajectory, and Context-Waypoint).Given a Context-Map or Context-Trajectory of size 2 × N × N (we use 2 × 100 × 100 as the size of our map), we encode the map using a ResNet18 visual encoder to obtain a 512-dimensional output (context features ĉt ).Alternatively, given a Context-Waypoint represented in polar coordinates [r, θ], we transform this into [r, cos(θ), sin(θ)].We then pass the waypoint vector into a two-layer MLP to get a 512-dimensional output (context features ĉt ).We compute the scaled dot-product attention[54]between the depth and context features to obtain context attention features ĉattn t and depth attention features dattn t .From a query Q of dimension d k , key K, and value V , we compute attention using:The query Q is obtained by passing features (h t or ĉt ) through a single linear layer to get Q h , or Q c .The key K is obtained by passing features (ĉ t or dt ) into a 1D convolution get K c , or K d .The value V is either ĉt or dt .We pass the context and depth attention features, goal features, previous action features, the hidden state from the first GRU (h a t ), and the hidden state from the previous timestep (h b t−1 ) into a second GRU τ b .x b , h b t = τ b (ĉ attn t , dattn t , ĝt , ât−1 , h a t , h b t−1 ).Following equation 3, the output of the GRU x b is fed into two parallel linear layers whose output parameterizes a Gaussian action distribution from which the action is sampled.
Learning quadrupedal locomotion over challenging terrain. J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abc5986Science Robotics. 54759862020</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, 10.1126/scirobotics.abk2822Science Robotics. 7622022</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, Robotics: Science and Systems. 2021</p>
<p>Legged locomotion in challenging terrains using egocentric vision. A Agarwal, A Kumar, J Malik, D Pathak, 2022CoRL</p>
<p>Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, IEEE Robotics and Automation Letters (RA-L). 2020</p>
<p>Learning navigation skills for legged robots with learned robot embeddings. J Truong, D Yarats, T Li, F Meier, S Chernova, D Batra, A Rai, International Conference on Intelligent Robots and Systems (IROS). 2020</p>
<p>Success weighted by completion time: A dynamics-aware evaluation criteria for embodied navigation. N Yokoyama, S Ha, D Batra, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021</p>
<p>Is mapping necessary for realistic pointgoal navigation?. R Partsey, E Wijmans, N Yokoyama, O Dobosevych, D Batra, O Maksymets, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2022</p>
<p>Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation. J Truong, M Rudolph, N Yokoyama, S Chernova, D Batra, A Rai, Conference on Robot Learning (CoRL). 2022</p>
<p>Robothor: An open simulation-to-real embodied AI platform. M Deitke, W Han, A Herrasti, A Kembhavi, E Kolve, R Mottaghi, J Salvador, D Schwenk, E Vanderbilt, M Wallingford, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Phone2proc: Bringing robust robots into our chaotic world. M Deitke, R Hendrix, L Weihs, A Farhadi, K Ehsani, A Kembhavi, ArXiv, abs/2212.048192022</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, International Conference on Learning Representations (ICLR). 2020</p>
<p>Combining optimal control and learning for visual navigation in novel environments. S Bansal, V Tolani, S Gupta, J Malik, C Tomlin, Conference on Robot Learning. PMLR2020</p>
<p>M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, Habitat: A Platform for Embodied AI Research. ICCV. 2019</p>
<p>Habitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D Chaplot, O Maksymets, A Gokaslan, V Vondrus, S Dharur, F Meier, W Galuba, A Chang, Z Kira, V Koltun, J Malik, M Savva, D Batra, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. B Shen, F Xia, C Li, R Martín-Martín, L Fan, G Wang, C Pérez-D'arpino, S Buch, S Srivastava, L P Tchapmi, M E Tchapmi, K Vainio, J Wong, L Fei-Fei, S Savarese, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021page accepted</p>
<p>. Isaac Nvidia, Sim, 2020</p>
<p>E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv. 2017</p>
<p>ThreeDWorld: A platform for interactive multi-modal physical simulation. C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J De Freitas, J Kubilius, A Bhandwaldar, N Haber, M Sano, NeurIPS Dataset. 2021</p>
<p>SAPIEN: A simulated part-based interactive environment. F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, L Yi, A X Chang, L J Guibas, H Su, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2020</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2012</p>
<p>Brax-a differentiable physics engine for large scale rigid body simulation. C D Freeman, E Frey, A Raichuk, S Girgin, I Mordatch, O Bachem, NeurIPS Dataset. 2021</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016</p>
<p>S K Ramakrishnan, A Gokaslan, E Wijmans, O Maksymets, A Clegg, J M Turner, E Undersander, W Galuba, A Westbury, A X Chang, M Savva, Y Zhao, D Batra, Habitatmatterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV). 2017</p>
<p>ShapeNet: An Information-Rich 3D Model Repository. A X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, J Xiao, L Yi, F Yu, arXiv:1512.03012[cs.GR]2015Stanford University -Princeton University -Toyota Technological Institute at ChicagoTechnical Report</p>
<p>Introduction to AI robotics. R R Murphy, 2019MIT press</p>
<p>Simultaneous localization and mapping. H Durrant-Whyte, T Bailey, IEEE robotics &amp; automation magazine. 2006</p>
<p>Probabilistic robotics. S Thrun, 2002ACM</p>
<p>Visual simultaneous localization and mapping: a survey. J Fuentes-Pacheco, J Ruiz-Ascencio, J M Rendón-Mancha, Artificial intelligence review. 4312015</p>
<p>Autonomous robot navigation in outdoor cluttered pedestrian walkways. Y Morales, A Carballo, E Takeuchi, A Aburadani, T Tsubouchi, Journal of Field Robotics. 2682009</p>
<p>Autonomous robot navigation in highly populated pedestrian zones. R Kümmerle, M Ruhnke, B Steder, C Stachniss, W Burgard, Journal of Field Robotics. 3242015</p>
<p>Off-road obstacle avoidance through end-to-end learning. U Muller, J Ben, E Cosatto, B Flepp, Y Cun, Advances in neural information processing systems. 182005</p>
<p>Learning long-range vision for autonomous off-road driving. R Hadsell, P Sermanet, J Ben, A Erkan, M Scoffier, K Kavukcuoglu, U Muller, Y Le-Cun, Journal of Field Robotics. 2622009</p>
<p>A multirange architecture for collision-free off-road robot navigation. P Sermanet, R Hadsell, M Scoffier, M Grimes, J Ben, A Erkan, C Crudele, U Miller, Y Lecun, Journal of Field Robotics. 2612009</p>
<p>Badgr: An autonomous self-supervised learning-based navigation system. G Kahn, P Abbeel, S Levine, IEEE Robotics and Automation Letters. 622021</p>
<p>Land: Learning to navigate from disengagements. G Kahn, P Abbeel, S Levine, IEEE Robotics and Automation Letters. 622021</p>
<p>Rapid Exploration for Open-World Navigation with Latent Goal Models. D Shah, B Eysenbach, N Rhinehart, S Levine, 5th Annual Conference on Robot Learning. 2021</p>
<p>Ving: Learning open-world navigation with visual goals. D Shah, B Eysenbach, G Kahn, N Rhinehart, S Levine, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>GNM: A General Navigation Model to Drive Any Robot. D Shah, A Sridhar, A Bhorkar, N Hirose, S Levine, International Conference on Robotics and Automation (ICRA). 2023</p>
<p>ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints. D Shah, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2022</p>
<p>Learning to navigate sidewalks in outdoor environments. M Sorokin, J Tan, K Liu, S Ha, IEEE Robotics and Automation Letters. 2022</p>
<p>CARLA: An open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A Lopez, V Koltun, Proceedings of the 1st Annual Conference on Robot Learning. the 1st Annual Conference on Robot Learning2017</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, 2018</p>
<p>Model identification via physics engines for improved policy search. S Zhu, A Kimmel, K E Bekris, A Boularias, CoRR, abs/1710.088932017</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, 2018</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2017</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, IEEE international conference on robotics and automation (ICRA). 2018. 2018</p>
<p>Dynamics randomization revisited: A case study for quadrupedal locomotion. Z Xie, X Da, M Van De Panne, B Babich, A Garg, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents. J Truong, S Chernova, D Batra, IEEE Robotics and Automation Letters (RA-L). IEEE20216</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, IEEE international conference on robotics and automation (ICRA). 2018. 2018IEEE</p>
<p>On Evaluation of Embodied Navigation Agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.067572018arXiv preprint</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I , 201730Polosukhin. Attention is all you need</p>
<p>. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, Dd-Ppo, 2020Learning near-perfect pointgoal navigators from 2.5 billion frames. ICLR</p>
<p>In defense of classical image processing: Fast depth completion on the cpu. J Ku, A Harakeh, S L Waslander, 2018 15th Conference on Computer and Robot Vision (CRV). IEEE2018</p>
<p>Random erasing data augmentation. Z Zhong, L Zheng, G Kang, S Li, Y Yang, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2020</p>
<p>Do humans integrate routes into a cognitive map? map-versus landmark-based navigation of novel shortcuts. P Foo, W H Warren, A Duchon, M J Tarr, Journal of Experimental Psychology: Learning, Memory, and Cognition. 3121952005</p>
<p>Coupling vision and proprioception for navigation of legged robots. Z Fu, A Kumar, A Agarwal, H Qi, J Malik, D Pathak, 2022CVPR</p>
<p>Sim-to-real transfer with neuralaugmented robot simulation. F Golemo, A A Taiga, A Courville, P.-Y Oudeyer, Conference on Robot Learning. 2018</p>
<p>R success is a terminal reward. If the robot successfully completes the episode. it receives a reward of 10.0, otherwise the robot receives 0 reward</p>
<p>R geo is a dense reward the robot receives at every step. The robot gets reward based on the change in geodesic distance to the goal from its previous timestep to the current timestep. </p>
<p>This reward encourages the robot to reach the goal using the minimum number of actions. We set the slack penalty to. -0.002R slack is a slack penalty</p>
<p>We set the backward penalty to -0.3. We found that using this high backwards penalty (10× higher than the value used in [6]) led to higher SPL, as the robot moved backwards less often. R backward is a penalty for backwards velocities. which leads to higher path efficiency</p>
<p>We set the collision penalty to -0.003. We use a small collision penalty in favor of using a higher backwards penalty since collisions were often due to moving backwards. R coll is a collision penalty</p>
<p>The fall penalty was used for training robots with dynamic control, as the low-level controller may cause the robot to fall over. With kinematic control, the robot is teleported to the next state and would never fall. We omit the fall penalty used. so the penalty is not needed</p>
<p>We do not report SPL for the outdoor experiments, since we do not have access to the shortest path. Instead, we report the success rate and the distance traveled. Context-Maps in Simulation. The context-maps in simulation are binary top-down maps that were generated by sampling a navigation mesh with a vertical slack. simulation, we obtain the shortest path in an environment by running A* on the top-down map. This shortest path is used to calculate SPL for our experiments in simulation. We use 0.5m, the default in Habitat</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5483 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5483</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5483</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-56caaf598c1bf36a24385f30ca775b94cf215b6b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/56caaf598c1bf36a24385f30ca775b94cf215b6b" target="_blank">Turning large language models into cognitive models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task, suggesting that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.</p>
                <p><strong>Paper Abstract:</strong> Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5483.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5483.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model Meta AI (LLaMA) - pre-trained baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Openly available family of foundation transformer language models; the paper used the pre-trained 65B-parameter LLaMA as a baseline by prompting it with task descriptions and reading out its choice probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (pre-trained baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer family trained on large publicly available corpora; the authors extracted final-layer embeddings and also read out the pre-trained model's log-probabilities for choices as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>choices13k (decisions from description)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>decision-making (risk/risk preference; description-based choice)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Large-scale choices-from-description dataset (choices13k) where participants choose between two gambles with fully specified outcome probabilities and values; used to probe decisions from description.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Negative log-likelihood (NLL) on choices13k: 96248.5 (reported as near chance-level). Regret (choices13k): mean = 1.85 (SE = 0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>choices13k: aggregated human choices from dataset (14,711 participants, >1M choices); human regret reported elsewhere in paper for simulated comparisons: mean regret = 1.24 (SE = 0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA (pre-trained) performed near chance on choices13k (NLL ~96248.5) and had higher regret (worse) than humans and the finetuned CENTaUR model; thus LLaMA underperformed relative to human behavior on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Pre-trained LLaMA without finetuning did not capture human choice structure in the description-based domain (near-chance NLL) and produced substantially higher regret than humans; the authors note it fails to model human patterns unless finetuned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Turning large language models into cognitive models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5483.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5483.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CENTaUR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CENTaUR (finetuned LLaMA linear head for human choice prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuned model consisting of a linear (logistic) readout layer trained on LLaMA final-layer embeddings to predict human choices across cognitive decision-making tasks; the finetuned model is evaluated as a cognitive model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CENTaUR (finetuned LLaMA readout)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Linear logistic regression layer (regularized) trained on embeddings extracted from LLaMA-65B prompts describing experimental trials; trained on behavioral datasets to predict human choices; also variants including random-effects for participant-level modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Built on LLaMA-65B embeddings (65B)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>choices13k (decisions from description)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>decision-making (risk/risk preference; description-based choice)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same choices-from-description dataset (choices13k). CENTaUR received the same text prompts as humans saw (i.e., full gamble specifications) and a linear layer was trained to map embeddings to choice probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>choices13k negative log-likelihood (NLL): 48002.3. Regret (choices13k): mean = 1.35 (SE = 0.01). CENTaUR outperformed the domain-specific BEAST model (BEAST NLL = 49448.1).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>choices13k human aggregated behavior: regret mean = 1.24 (SE = 0.01) (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CENTaUR achieved substantially better fit to human choices than pre-trained LLaMA and also beat the domain-specific cognitive model BEAST (CENTaUR NLL 48002.3 vs BEAST 49448.1); CENTaUR's regret (1.35) was closer to human regret (1.24) than pre-trained LLaMA (1.85).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>CENTaUR required finetuning on human behavioral data to match human behavior; the model was trained on aggregated choices (fixed-effect) and improved further when participant-level random effects were incorporated (NLL improved from 25968.6 to 23929.5 on the horizon task). The approach depends on availability of behavioral datasets for finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Turning large language models into cognitive models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5483.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5483.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA / CENTaUR (horizon task)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Horizon task (sequential decisions from experience) evaluated with pre-trained LLaMA and CENTaUR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential explore-exploit bandit paradigm where participants learn option values via sampling; the paper evaluated pre-trained LLaMA and the finetuned CENTaUR on this task (including individual-level fits).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (pre-trained) and CENTaUR (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See entries above: pre-trained LLaMA embeddings and a finetuned logistic readout (CENTaUR) trained on horizon-task choice data; comparisons to a hybrid domain-specific model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B (LLaMA backbone for both baseline and CENTaUR embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Horizon task (Wilson et al., 2014; replication Feng et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>decision-making, exploration vs. exploitation, sequential learning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A multi-trial bandit task where on each trial participants sample from two options to learn payoff distributions; task manipulates 'horizon' (remaining choices) and information conditions (equal vs unequal prior observations) to elicit directed and random exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Negative log-likelihoods: pre-trained LLaMA NLL = 46211.4; CENTaUR NLL = 25968.6. Regret: pre-trained LLaMA mean = 7.21 (SE = 0.02); CENTaUR mean = 2.38 (SE = 0.01). CENTaUR reproduced key human qualitative effects (increased randomness with longer horizon in equal-information condition; preferential selection of informative option in unequal-information condition) while pre-trained LLaMA did not show these effects.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human regret on horizon task: mean = 2.33 (SE = 0.05). Human behavioral signatures: (1) choices become more random with longer horizon in equal-information condition; (2) in unequal-information condition, people select the more informative option more when horizon is longer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Pre-trained LLaMA underperformed relative to humans (much higher regret and failure to show exploration patterns). CENTaUR matched human performance closely (regret 2.38 vs human 2.33) and reproduced qualitative human exploration behaviors; CENTaUR also outperformed a domain-specific hybrid model (hybrid NLL = 29042.5).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Pre-trained LLaMA relied on exploitative strategies and failed to capture human exploration (no horizon-dependent randomness or directed exploration), whereas CENTaUR required finetuning on behavioral data to exhibit human-like exploration. The authors simulated models by sampling predictions on test sets and noted that CENTaUR needed task-specific finetuning to align with human strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Turning large language models into cognitive models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5483.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5483.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experiential-symbolic (hold-out) task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiential-symbolic decision task (Garcia et al., 2023) - hold-out generalization test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task that contrasts an option described symbolically (S-option) versus an option represented by experienced outcomes (E-option) to measure overweighting/underweighting of symbolic vs experiential information; used as a hold-out test for CENTaUR generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Experiential values are underweighted in decisions involving symbolic options</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CENTaUR (finetuned on choices13k + horizon) and pre-trained LLaMA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CENTaUR was trained jointly on choices13k and horizon task data, then evaluated (without being trained on the experiential-symbolic dataset) to test zero-shot generalization; pre-trained LLaMA evaluated as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>CENTaUR uses LLaMA-65B embeddings (65B)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Experiential-symbolic task (Garcia et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>decision-making; integration of described vs experienced information</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Participants choose between an option presented symbolically with explicit probabilities (S-option) and an option for which they have previously observed experienced sample outcomes (E-option); measures whether participants overvalue S-options relative to E-options.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Negative log-likelihoods on hold-out task: CENTaUR NLL = 4521.1; pre-trained LLaMA NLL = 6307.9; random guessing NLL = 5977.7. Qualitatively, CENTaUR exhibited the human tendency to overweight symbolic (S) options, whereas pre-trained LLaMA weighed S and E options approximately equally.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans show a qualitative bias: they tend to overvalue symbolic/described options (S-options) relative to experiential options (E-options) in this paradigm (result reported from Garcia et al., 2023; exact numeric baselines not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CENTaUR generalized to the unseen task and captured human qualitative bias (overweighting S-options), achieving lower NLL than both pre-trained LLaMA and chance; pre-trained LLaMA failed to show the human bias and performed worse (higher NLL).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>CENTaUR's ability to generalize was enabled by joint finetuning on multiple tasks; the hold-out task evaluation used deterministic simulation based on median thresholds and simplified prompt histories. Exact demographic matching to human samples is not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Turning large language models into cognitive models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5483.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5483.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (prior work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Generative Pre-trained Transformer 3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Previously reported evaluation of GPT-3 in a sequential decision-making (explore-exploit) task showed it outperformed human subjects on reward but exhibited unhuman-like strategies (heavy exploitation and early plateauing of improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using cognitive psychology to understand gpt-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (originally described by Brown et al., 2020); in the referenced prior work GPT-3 was placed into a sequential decision-making experiment and analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (referenced prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sequential decision-making task (explore-exploit; Wilson et al., 2014 style)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>sequential decision-making, exploration vs exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A sequential decision-making paradigm requiring balance between exploitation of known good options and exploration of uncertain options; used to assess whether the model adopts human-like exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported qualitatively: GPT-3 outperformed human subjects in terms of task reward in the referenced study but relied heavily on exploitative strategies and stopped improving after a few trials.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans in the referenced study applied a combination of directed and random exploration strategies and continued improving as the task progressed (no numeric values provided in this paper for GPT-3 comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3 achieved higher reward than humans in the referenced experiment but differed qualitatively: it relied on exploitation rather than the mixture of exploration strategies humans used, and it plateaued earlier in learning.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper cites this prior result to illustrate that pre-trained LLMs can achieve high reward yet still behave in unhuman-like ways (e.g., lack of exploration); numeric performance and statistical details are not reported here and are in the cited prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Turning large language models into cognitive models', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 2)</em></li>
                <li>Experiential values are underweighted in decisions involving symbolic options <em>(Rating: 2)</em></li>
                <li>Using large-scale experiments and machine learning to discover theories of human decision-making <em>(Rating: 2)</em></li>
                <li>Humans use directed and random exploration to solve the explore-exploit dilemma <em>(Rating: 2)</em></li>
                <li>Language models show human-like content effects on reasoning <em>(Rating: 1)</em></li>
                <li>Inducing anxiety in large language models increases exploration and bias <em>(Rating: 1)</em></li>
                <li>Machine intuition: Uncovering human-like intuitive decision-making in gpt-3 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5483",
    "paper_id": "paper-56caaf598c1bf36a24385f30ca775b94cf215b6b",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "LLaMA (baseline)",
            "name_full": "Large Language Model Meta AI (LLaMA) - pre-trained baseline",
            "brief_description": "Openly available family of foundation transformer language models; the paper used the pre-trained 65B-parameter LLaMA as a baseline by prompting it with task descriptions and reading out its choice probabilities.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA (pre-trained baseline)",
            "model_description": "Autoregressive transformer family trained on large publicly available corpora; the authors extracted final-layer embeddings and also read out the pre-trained model's log-probabilities for choices as a baseline.",
            "model_size": "65B",
            "cognitive_test_name": "choices13k (decisions from description)",
            "cognitive_test_type": "decision-making (risk/risk preference; description-based choice)",
            "cognitive_test_description": "Large-scale choices-from-description dataset (choices13k) where participants choose between two gambles with fully specified outcome probabilities and values; used to probe decisions from description.",
            "llm_performance": "Negative log-likelihood (NLL) on choices13k: 96248.5 (reported as near chance-level). Regret (choices13k): mean = 1.85 (SE = 0.01).",
            "human_baseline_performance": "choices13k: aggregated human choices from dataset (14,711 participants, &gt;1M choices); human regret reported elsewhere in paper for simulated comparisons: mean regret = 1.24 (SE = 0.01).",
            "performance_comparison": "LLaMA (pre-trained) performed near chance on choices13k (NLL ~96248.5) and had higher regret (worse) than humans and the finetuned CENTaUR model; thus LLaMA underperformed relative to human behavior on this task.",
            "notable_differences_or_limitations": "Pre-trained LLaMA without finetuning did not capture human choice structure in the description-based domain (near-chance NLL) and produced substantially higher regret than humans; the authors note it fails to model human patterns unless finetuned.",
            "uuid": "e5483.0",
            "source_info": {
                "paper_title": "Turning large language models into cognitive models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CENTaUR",
            "name_full": "CENTaUR (finetuned LLaMA linear head for human choice prediction)",
            "brief_description": "A finetuned model consisting of a linear (logistic) readout layer trained on LLaMA final-layer embeddings to predict human choices across cognitive decision-making tasks; the finetuned model is evaluated as a cognitive model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CENTaUR (finetuned LLaMA readout)",
            "model_description": "Linear logistic regression layer (regularized) trained on embeddings extracted from LLaMA-65B prompts describing experimental trials; trained on behavioral datasets to predict human choices; also variants including random-effects for participant-level modeling.",
            "model_size": "Built on LLaMA-65B embeddings (65B)",
            "cognitive_test_name": "choices13k (decisions from description)",
            "cognitive_test_type": "decision-making (risk/risk preference; description-based choice)",
            "cognitive_test_description": "Same choices-from-description dataset (choices13k). CENTaUR received the same text prompts as humans saw (i.e., full gamble specifications) and a linear layer was trained to map embeddings to choice probabilities.",
            "llm_performance": "choices13k negative log-likelihood (NLL): 48002.3. Regret (choices13k): mean = 1.35 (SE = 0.01). CENTaUR outperformed the domain-specific BEAST model (BEAST NLL = 49448.1).",
            "human_baseline_performance": "choices13k human aggregated behavior: regret mean = 1.24 (SE = 0.01) (reported in paper).",
            "performance_comparison": "CENTaUR achieved substantially better fit to human choices than pre-trained LLaMA and also beat the domain-specific cognitive model BEAST (CENTaUR NLL 48002.3 vs BEAST 49448.1); CENTaUR's regret (1.35) was closer to human regret (1.24) than pre-trained LLaMA (1.85).",
            "notable_differences_or_limitations": "CENTaUR required finetuning on human behavioral data to match human behavior; the model was trained on aggregated choices (fixed-effect) and improved further when participant-level random effects were incorporated (NLL improved from 25968.6 to 23929.5 on the horizon task). The approach depends on availability of behavioral datasets for finetuning.",
            "uuid": "e5483.1",
            "source_info": {
                "paper_title": "Turning large language models into cognitive models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LLaMA / CENTaUR (horizon task)",
            "name_full": "Horizon task (sequential decisions from experience) evaluated with pre-trained LLaMA and CENTaUR",
            "brief_description": "Sequential explore-exploit bandit paradigm where participants learn option values via sampling; the paper evaluated pre-trained LLaMA and the finetuned CENTaUR on this task (including individual-level fits).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA (pre-trained) and CENTaUR (finetuned)",
            "model_description": "See entries above: pre-trained LLaMA embeddings and a finetuned logistic readout (CENTaUR) trained on horizon-task choice data; comparisons to a hybrid domain-specific model.",
            "model_size": "65B (LLaMA backbone for both baseline and CENTaUR embeddings)",
            "cognitive_test_name": "Horizon task (Wilson et al., 2014; replication Feng et al., 2021)",
            "cognitive_test_type": "decision-making, exploration vs. exploitation, sequential learning",
            "cognitive_test_description": "A multi-trial bandit task where on each trial participants sample from two options to learn payoff distributions; task manipulates 'horizon' (remaining choices) and information conditions (equal vs unequal prior observations) to elicit directed and random exploration strategies.",
            "llm_performance": "Negative log-likelihoods: pre-trained LLaMA NLL = 46211.4; CENTaUR NLL = 25968.6. Regret: pre-trained LLaMA mean = 7.21 (SE = 0.02); CENTaUR mean = 2.38 (SE = 0.01). CENTaUR reproduced key human qualitative effects (increased randomness with longer horizon in equal-information condition; preferential selection of informative option in unequal-information condition) while pre-trained LLaMA did not show these effects.",
            "human_baseline_performance": "Human regret on horizon task: mean = 2.33 (SE = 0.05). Human behavioral signatures: (1) choices become more random with longer horizon in equal-information condition; (2) in unequal-information condition, people select the more informative option more when horizon is longer.",
            "performance_comparison": "Pre-trained LLaMA underperformed relative to humans (much higher regret and failure to show exploration patterns). CENTaUR matched human performance closely (regret 2.38 vs human 2.33) and reproduced qualitative human exploration behaviors; CENTaUR also outperformed a domain-specific hybrid model (hybrid NLL = 29042.5).",
            "notable_differences_or_limitations": "Pre-trained LLaMA relied on exploitative strategies and failed to capture human exploration (no horizon-dependent randomness or directed exploration), whereas CENTaUR required finetuning on behavioral data to exhibit human-like exploration. The authors simulated models by sampling predictions on test sets and noted that CENTaUR needed task-specific finetuning to align with human strategies.",
            "uuid": "e5483.2",
            "source_info": {
                "paper_title": "Turning large language models into cognitive models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Experiential-symbolic (hold-out) task",
            "name_full": "Experiential-symbolic decision task (Garcia et al., 2023) - hold-out generalization test",
            "brief_description": "A task that contrasts an option described symbolically (S-option) versus an option represented by experienced outcomes (E-option) to measure overweighting/underweighting of symbolic vs experiential information; used as a hold-out test for CENTaUR generalization.",
            "citation_title": "Experiential values are underweighted in decisions involving symbolic options",
            "mention_or_use": "use",
            "model_name": "CENTaUR (finetuned on choices13k + horizon) and pre-trained LLaMA (baseline)",
            "model_description": "CENTaUR was trained jointly on choices13k and horizon task data, then evaluated (without being trained on the experiential-symbolic dataset) to test zero-shot generalization; pre-trained LLaMA evaluated as baseline.",
            "model_size": "CENTaUR uses LLaMA-65B embeddings (65B)",
            "cognitive_test_name": "Experiential-symbolic task (Garcia et al., 2023)",
            "cognitive_test_type": "decision-making; integration of described vs experienced information",
            "cognitive_test_description": "Participants choose between an option presented symbolically with explicit probabilities (S-option) and an option for which they have previously observed experienced sample outcomes (E-option); measures whether participants overvalue S-options relative to E-options.",
            "llm_performance": "Negative log-likelihoods on hold-out task: CENTaUR NLL = 4521.1; pre-trained LLaMA NLL = 6307.9; random guessing NLL = 5977.7. Qualitatively, CENTaUR exhibited the human tendency to overweight symbolic (S) options, whereas pre-trained LLaMA weighed S and E options approximately equally.",
            "human_baseline_performance": "Humans show a qualitative bias: they tend to overvalue symbolic/described options (S-options) relative to experiential options (E-options) in this paradigm (result reported from Garcia et al., 2023; exact numeric baselines not provided in this paper).",
            "performance_comparison": "CENTaUR generalized to the unseen task and captured human qualitative bias (overweighting S-options), achieving lower NLL than both pre-trained LLaMA and chance; pre-trained LLaMA failed to show the human bias and performed worse (higher NLL).",
            "notable_differences_or_limitations": "CENTaUR's ability to generalize was enabled by joint finetuning on multiple tasks; the hold-out task evaluation used deterministic simulation based on median thresholds and simplified prompt histories. Exact demographic matching to human samples is not reported here.",
            "uuid": "e5483.3",
            "source_info": {
                "paper_title": "Turning large language models into cognitive models",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-3 (prior work mention)",
            "name_full": "GPT-3 (Generative Pre-trained Transformer 3)",
            "brief_description": "Previously reported evaluation of GPT-3 in a sequential decision-making (explore-exploit) task showed it outperformed human subjects on reward but exhibited unhuman-like strategies (heavy exploitation and early plateauing of improvement).",
            "citation_title": "Using cognitive psychology to understand gpt-3",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive transformer language model (originally described by Brown et al., 2020); in the referenced prior work GPT-3 was placed into a sequential decision-making experiment and analyzed.",
            "model_size": "Not specified in this paper (referenced prior work)",
            "cognitive_test_name": "Sequential decision-making task (explore-exploit; Wilson et al., 2014 style)",
            "cognitive_test_type": "sequential decision-making, exploration vs exploitation",
            "cognitive_test_description": "A sequential decision-making paradigm requiring balance between exploitation of known good options and exploration of uncertain options; used to assess whether the model adopts human-like exploration strategies.",
            "llm_performance": "Reported qualitatively: GPT-3 outperformed human subjects in terms of task reward in the referenced study but relied heavily on exploitative strategies and stopped improving after a few trials.",
            "human_baseline_performance": "Humans in the referenced study applied a combination of directed and random exploration strategies and continued improving as the task progressed (no numeric values provided in this paper for GPT-3 comparison).",
            "performance_comparison": "GPT-3 achieved higher reward than humans in the referenced experiment but differed qualitatively: it relied on exploitation rather than the mixture of exploration strategies humans used, and it plateaued earlier in learning.",
            "notable_differences_or_limitations": "The paper cites this prior result to illustrate that pre-trained LLMs can achieve high reward yet still behave in unhuman-like ways (e.g., lack of exploration); numeric performance and statistical details are not reported here and are in the cited prior work.",
            "uuid": "e5483.4",
            "source_info": {
                "paper_title": "Turning large language models into cognitive models",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 2
        },
        {
            "paper_title": "Experiential values are underweighted in decisions involving symbolic options",
            "rating": 2
        },
        {
            "paper_title": "Using large-scale experiments and machine learning to discover theories of human decision-making",
            "rating": 2
        },
        {
            "paper_title": "Humans use directed and random exploration to solve the explore-exploit dilemma",
            "rating": 2
        },
        {
            "paper_title": "Language models show human-like content effects on reasoning",
            "rating": 1
        },
        {
            "paper_title": "Inducing anxiety in large language models increases exploration and bias",
            "rating": 1
        },
        {
            "paper_title": "Machine intuition: Uncovering human-like intuitive decision-making in gpt-3",
            "rating": 1
        }
    ],
    "cost": 0.012261,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Turning large language models into cognitive models</h1>
<p>Marcel Binz<br>MPRG Computational Principles of Intelligence<br>Max Planck Institute for Biological Cybernetics, Tübingen, Germany<br>marcel.binz@tue.mpg.de</p>
<p>Eric Schulz<br>MPRG Computational Principles of Intelligence<br>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</p>
<h4>Abstract</h4>
<p>Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that - after finetuning them on data from psychological experiments - these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.</p>
<h2>1 Introduction</h2>
<p>Large language models are neural networks trained on vast amounts of data to predict the next token for a given text sequence [Brown et al., 2020]. These models display many emergent abilities that were not anticipated by extrapolating the performance of smaller models [Wei et al., 2022]. Their abilities are so impressive and far-reaching that some have argued that they show sparks of general intelligence [Bubeck et al., 2023]. We may currently witness one of the biggest revolutions in artificial intelligence, but the impact of modern language models is felt far beyond, permeating into education [Kasneci et al., 2023], medicine [Li et al., 2023], and the labor market [Eloundou et al., 2023].</p>
<p>In-context learning - the ability to extract information from a context and to use that information to improve the production of subsequent outputs - is one of the defining features of such models. It is through this mechanism that large language models are able to solve a variety of tasks, ranging from translation [Brown et al., 2020] to analogical reasoning [Webb et al., 2022]. Previous work has shown that these models can even successfully navigate when they are placed into classic psychological experiments [Binz and Schulz, 2023, Coda-Forno et al., 2023, Dasgupta et al., 2022, Hagendorff et al., 2022]. To provide just one example, GPT-3 - an autoregressive language model designed by OpenAI [Brown et al., 2020] - outperformed human subjects in a sequential decision-making task that required to balance between exploitative and exploratory actions [Binz and Schulz, 2023].</p>
<p>Even though these models show human-like behavioral characteristics in some situations, this is not always the case. In the sequential decision-making task mentioned above, for instance, GPT-3</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of our approach and main results. (a) We provided text-based descriptions of psychological experiments to a large language model and extracted the resulting embeddings. We then finetuned a linear layer on top of these embeddings to predict human choices. We refer to the resulting model as CENTaUR. (b) Example prompt for the choices13k data set. (c) Negative log-likelihoods for the choices13k data set. (d) Example prompt for the horizon task. (e) Negative log-likelihoods for the horizon task. Prompts shown in this figure are stylized for readability. Exact prompts can be found in the Supplementary Materials.</p>
<p>Replied heavily on exploitative strategies, while people applied a combination of elaborate exploration strategies [Wilson et al., 2014]. Moreover, GPT-3 stopped improving after only a few trials, while people continued learning as the task progressed.</p>
<p>In the present paper, we investigate whether it is possible to fix the behavioral discrepancy between large language models and humans. To do so, we rely on the idea of finetuning on domain-specific data. This approach has been fruitful across a number of areas [Sanh et al., 2019, Ouyang et al., 2022] and eventually led to the creation of the term <em>foundation models</em> [Bommasani et al., 2021] – models trained on broad data at scale and adapted to a wide range of downstream tasks. In the context of human cognition, such domain-specific data can be readily accessed by tapping the vast amount of behavioral studies that psychologists have conducted over the last century. We made use of this and extracted data sets for several behavioral paradigms which we then used to finetune a large language model.</p>
<p>We show that this approach can be used to create models that describe human behavior better than traditional cognitive models. We verify this result through extensive model simulations, which confirm that finetuned language models indeed show human-like behavioral characteristics. Furthermore, we find that the embeddings obtained from such models contain the information necessary to capture individual differences. Finally, we highlight that a model finetuned on two tasks is capable of predicting human behavior on a third, hold-out task. Taken together, our work demonstrates that it is possible to turn large language models into cognitive models, thereby opening up completely new opportunities to harvest the power of large language models for building domain-general models of human learning and decision-making.</p>
<p>2 Finetuned language models beat domain-specific models</p>
<p>We started our investigations by testing whether it is possible to capture how people make decisions through finetuning a large language model. For our analyses, we relied on the Large Language Model Meta AI, or in short: LLaMA [Touvron et al., 2023]. LLaMA is a family of state-of-the-art foundational large language models (with either 7B, 13B, 33B, or 65B parameters) that were trained on trillions of tokens coming from exclusively publicly available data sets. We focused on the largest of these models - the 65B parameter version - for the analyses in the main text. LLaMA is publicly available, meaning that researchers are provided with complete access to the network architecture including its pre-trained weights. We utilized this feature to extract embeddings for several cognitive tasks and then finetuned a linear layer on top of these embeddings to predict human choices (see Figure 1a for a visualization). We call the resulting class of models CENTaUR, in analogy to the mythical creature that is half human and half ungulate.</p>
<p>We considered two paradigms that have been extensively studied in the human decision-making literature for our initial analyses: decisions from descriptions [Kahneman and Tversky, 1972] and decisions from experience [Hertwig et al., 2004]. In the former, a decision-maker is asked to choose between one of two hypothetical gambles like the ones shown in Figure 1b. Thus, for both options, there is complete information about outcome probabilities and their respective values. In contrast, the decisions from experience paradigm does not provide such explicit information. Instead, the decision-maker has to learn about outcome probabilities and their respective values from repeated interactions with the task as shown in Figure 1d. Importantly, this modification calls for a change in how an ideal decision-maker should approach such problems: it is not enough to merely exploit currently available knowledge anymore but also crucial to explore options that are unfamiliar [Schulz and Gershman, 2019].</p>
<p>For both these paradigms, we created a data set consisting of embeddings and the corresponding human choices. We obtained embeddings by passing prompts that included all the information that people had access to on a given trial through LLaMA and then extracting the hidden activations of the final layer (see Figure 1b and d for example prompts, and the Supplementary Materials for a more detailed description about the prompt generation procedure). We relied on publicly available data from earlier studies in this process. In the decisions from descriptions setting, we used the choices13k data set [Peterson et al., 2021], which is a large-scale data set consisting of over 13,000 choice problems (all in all, 14,711 participants made over one million choices on these problems). In the decisions from experience setting, we used data from the horizon task [Wilson et al., 2014] and a replication study [Feng et al., 2021], which combined include 60 participants making a total of 67,200 choices.</p>
<p>With these two data sets at hand, we fitted a regularized logistic regression model from the extracted embeddings to human choices. In this section, we restricted ourselves to a joint model for all participants, thereby neglecting potential individual differences (but see one of the following sections for an analysis that allows for individual differences). Model performance was measured through the predictive log-likelihood on hold-out data obtained using a 100-fold cross-validation procedure. We standardized all input features and furthermore applied a nested cross-validation for tuning the hyperparameter that controls the regularization strength. Further details are provided in the Materials and Methods section.</p>
<p>We compared the goodness-of-fit of the resulting models against three baselines: a random guessing model, LLaMA without finetuning (obtained by reading out log-probabilities of the pre-trained model), and a domain-specific model (Best Estimate and Sampling Tools, or BEAST, for the choices13k data set [Erev et al., 2017] and a hybrid model [Gershman, 2018] that involves a combination of different exploitation and exploration strategies for the horizon task). We found that LLaMA did not capture human behavior well, obtaining a negative log-likelihood (NLL) close to chance-level for the choices13k data set (NLL $=96248.5$ ) and the horizon task (NLL $=46211.4$ ). However, finetuning led to models that captured human behavior better than the domain-specific models under consideration. In the choices13k data set, CENTaUR achieved a negative log-likelihood of 48002.3 while BEAST only achieved a negative log-likelihood of 49448.1 (see Figure 1c). In the horizon task, CENTaUR achieved a negative log-likelihood of 25968.6 while the hybrid model only achieved a negative log-likelihood of 29042.5 (see Figure 1e). Together, these results suggest that the representations extracted from large language models are rich enough to attain state-of-the-art results for modeling human decision-making.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model simulations. (a) Performance for different models and human participants on the choices13k data set. (b) Performance for different models and human participants on the horizon task. (c) Human choice curves in the equal information condition of the horizon task. (d) Human choice curves in the unequal information condition of the horizon task. (e) LLaMA choice curves in the equal information condition of the horizon task. (f) LLaMA choice curves in the unequal information condition of the horizon task. (g) CENTaUR choice curves in the equal information condition of the horizon task. (h) CENTaUR choice curves in the unequal information condition of the horizon task.</p>
<h1>3 Model simulations reveal human-like behavior</h1>
<p>We next verified that CENTaUR shows human-like behavioral characteristics. To do so, we simulated the model on the experimental data. Looking at performance, we found that finetuning led to models that closely resemble human performance as shown in Figure 2a and b. For the choices-13k data set, CENTaUR obtained a regret (defined as the difference between the highest possible reward and the reward for the action selected by the model) of $1.35(\mathrm{SE}=0.01)$, which was much closer to the human regret ( $\mathrm{M}=1.24, \mathrm{SE}=0.01$ ) than the regret of LLaMA ( $\mathrm{M}=1.85, \mathrm{SE}=0.01$ ). The results for the horizon task showed an identical pattern, with CENTaUR ( $\mathrm{M}=2.38, \mathrm{SE}=0.01$ ) matching human regret ( $\mathrm{M}=2.33, \mathrm{SE}=0.05$ ) more closely than LLaMA ( $\mathrm{M}=7.21, \mathrm{SE}=0.02$ ).</p>
<p>In addition to looking at performance, we also inspected choice curves. For this analysis, we took the data from the first free-choice trial in the horizon task and divided it into two conditions: (1) an equal information condition that includes trials where the decision-maker had access to an equal number of observations for both options and (2) an unequal information condition that includes trials where the decision-maker previously observed one option fewer times than the other. We then fitted a separate logistic regression model for each condition with reward difference, horizon, and their interaction as independent variables onto the simulated choices. Earlier studies with human subjects [Wilson et al., 2014] identified the following two main results regarding their exploratory behavior: (1) people's choices become more random with a longer horizon in the equal information condition (as shown in Figure 2c) and (2) people in the unequal information condition select the more informative option more frequently when the task horizon is longer (as shown in Figure 2d). While LLaMA did not show any of the two effects (see Figure 2e and f), CENTaUR exhibited both of them (see Figure 2g and h), thereby further corroborating that it accurately captures human behavior.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Individual differences. (a) Negative log-likelihood difference to the best-fitting model for each participant. Black highlights the best-fitting model, while white corresponds to a difference larger than ten. (b) Negative log-likelihoods for models that were finetuned using the random-effects structure described in the main text.</p>
<h2>4 Language model embeddings capture individual differences</h2>
<p>We also investigated how well CENTaUR describes the behavior of each individual participant. Note that this form of analysis is only possible for the horizon task as choice information on the participant level is not available for the choices13k data set. In total, the majority of participants (N = 52 out of 60) was best modeled by CENTaUR (see Figure 3a for a detailed visualization). We furthermore entered the negative log-likelihoods into a random-effects model selection procedure which estimates the probability that a particular model is the most frequent explanation within a set of candidate models [Rigoux et al., 2014]. This procedure favored CENTaUR decisively, assigning a probability that it is the most frequent explanation of close to one.</p>
<p>Thus far, we have finetuned LLaMA jointly for all participants. However, people may exhibit individual differences that are not captured by this analysis. To close this gap and test whether LLaMA embeddings can account for individual differences, we incorporated random effects in the finetuned layer. We added a random effect for each participant and embedding dimension while keeping the remaining evaluation procedure the same. Figure 3b illustrates the resulting negative log-likelihoods. Including the random-effect structure improved goodness-of-fit considerably (NLL = 23929.5) compared to the fixed-effect-only model (NLL = 25968.6). Furthermore, CENTaUR remained superior to the hybrid model with an identical random-effect structure (NLL = 24166.0). Taken together, the findings reported in this section highlight that embeddings of large language models contain the information necessary to model behavior on the participant level.</p>
<h2>5 Evaluating goodness-of-fit on hold-out tasks</h2>
<p>Finally, we examined whether CENTaUR – after being finetuned on multiple tasks – is able to predict human behavior in an entirely different task. This evaluation protocol provides a much stronger test for the generalization abilities of our approach. Following our initial analyses, we finetuned a linear layer on top of LLaMA embeddings. However, this time, we fitted a joint model using both the data from the choices13k data set and the horizon task, and then evaluated how well the finetuned model captures human choices on a third task. Further details about the fitting procedure are provided in the Materials and Methods section. For the hold-out task, we considered data from a recent study that provided participants with a choice between one option whose information is provided via a description and another option for which information is provided via a list of experienced outcomes [Garcia et al., 2023]. Figure 4a shows an example prompt for this experimental paradigm.</p>
<p>Finetuning was generally beneficial for modeling human behavior on the hold-out task: negative log-likelihoods for CENTaUR (NLL = 4521.1) decreased both in comparison to a random guessing model (NLL = 5977.7) and LLaMA (NLL = 6307.9). We were thus curious whether CENTaUR also captures human behavior on a qualitative level. To test this, we took a look at the key insight from the original study: people tend to overvalue options that are provided through a description</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Hold-out task evaluations. (a) Example prompt for the experiential-symbolic task of Garcia et al. [2023]. (b) Human choice curves as a function of win probabilities for both options. (c) Human indifference points as a function of win probability for the E-option. Indifferent points express the win probabilities at which a decision-maker is equally likely to select both options. (d) LLaMA choice curves as a function of win probabilities for both options. (e) LLaMA indifference points as a function of win probability for the E-option. (f) CENTaUR choice curves as a function of win probabilities for both options. (g) CENTaUR indifference points as a function of win probability for the E-option.</p>
<p>(symbolic or S-options) over the options that come with a list of experienced outcomes (experiential or E-options) as illustrated in Figure 4b and c. LLaMA does not show this characteristic and instead weighs both option types equally (Figure 4d and e). In contrast to this, CENTaUR shows human-like behavior, taking mostly the S-option into account (Figure 4f and g). This is remarkable because we never presented data from the experiment under consideration during finetuning.</p>
<h2>6 Discussion</h2>
<p>We have demonstrated that large language models can be turned into cognitive models by finetuning their final layer. This process led to models that achieved state-of-the-art performance in two domains. Furthermore, these models were able to capture behavioral differences at the individual participant level. Finally, we have shown that our approach generalizes to previously unseen tasks. In particular, a model that was finetuned on two tasks also exhibited human-like behavior on a third, hold-out task.</p>
<p>These results complement earlier work showing that large language model embeddings allow us to predict behavior and neural activations in linguistic settings [Schrimpf et al., 2021, Kumar et al., 2022, Tuckute et al., 2023, Antonello et al., 2023]. For example, Schrimpf et al. [2021] showed that large language models can predict neural and behavioral responses in tasks that involved reading short passages with an accuracy that was close to noise ceiling. While it may be expected that large language models explain human behavior in linguistic domains (after all these models are trained to predict future word occurrences), the observation that these results also transfer to more cognition domains like the ones studied here is highly non-trivial.</p>
<p>We are particularly excited about one feature of CENTaUR: embeddings extracted for different tasks all lie in a common space. This property allows finetuned large language models to solve multiple tasks in a unified architecture. We have presented preliminary results in this direction, showing that a model finetuned on two tasks can predict human behavior on a third. However, we believe that our current results only hint at the potential of this approach. Ideally, we would like to scale up our approach to finetuning on a larger number of tasks from the psychology literature. If one would include enough tasks in the training set, the resulting system should – in principle – generalize to <em>any</em></p>
<p>hold-out task. Therefore, our approach provides a path towards a domain-general model of human cognition, which has been the goal of theoreticians for decades [Newell, 1992, Yang et al., 2019, Riveland and Pouget, 2022, Binz et al., 2023]. We believe that having access to such a model would transform psychology and the behavioral sciences more generally. It could, among other applications, be used to rapidly prototype the outcomes of projected experiments, thereby easing the trial-and-error process of experimental design, or to provide behavioral policy recommendations while avoiding expensive data collection procedures.</p>
<p>Finally, we have to ask ourselves what we can learn about human cognition when finetuning large language models. For now, our insights are limited to the observation that large language model embeddings are rich enough to explain human decision-making. While this is interesting in its own right, it is certainly not the end of the story. Looking beyond the current work, having access to an accurate neural network model of human behavior provides the opportunity to apply a wide range of explainability techniques from the machine learning literature. For instance, we could pick a particular neuron in the embedding and trace back what parts of a particular input sequence excite that neuron using methods such as layer-wise relevance propagation [Bach et al., 2015, Chefer et al., 2021]. Thus, our work also opens up a new spectrum of analyses that are not possible when working with human subjects.</p>
<p>To summarize, large language models are an immensely powerful tool for studying human behavior. We believe that our work has only scratched the surface of this potential and there is certainly much more to come.</p>
<p>Acknowledgements: We like to thank Robert Wilson and Basile Garcia for their help on the horizon task and the experiential-symbolic task respectively, Ido Erev and Eyal Ert for their help with the BEAST model, and Meta AI for making LLaMA accessible to the research community.</p>
<p>Funding: This work was funded by the Max Planck Society, the Volkswagen Foundation, as well as the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy-EXC2064/1-390727645.</p>
<p>Data and materials availability: Data and code for the current study are available through the GitHub repository https://github.com/marcelbinz/CENTaUR.</p>
<h1>References</h1>
<p>Richard Antonello, Aditya Vaidya, and Alexander G Huth. Scaling laws for language encoding models in fmri. arXiv preprint arXiv:2305.11863, 2023.</p>
<p>Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.</p>
<p>Marcel Binz and Eric Schulz. Using cognitive psychology to understand gpt-3. Proceedings of the National Academy of Sciences, 120(6):e2218523120, 2023.</p>
<p>Marcel Binz, Ishita Dasgupta, Akshay Jagadish, Matthew Botvinick, Jane X Wang, and Eric Schulz. Meta-learned models of cognition. arXiv preprint arXiv:2304.06729, 2023.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 782-791, 2021.</p>
<p>Julian Coda-Forno, Kristin Witte, Akshay K Jagadish, Marcel Binz, Zeynep Akata, and Eric Schulz. Inducing anxiety in large language models increases exploration and bias. arXiv:2304.11111, 2023.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022.</p>
<p>Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023.</p>
<p>Ido Erev, Eyal Ert, Ori Plonsky, Doron Cohen, and Oded Cohen. From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience. Psychological review, 124(4):369, 2017.</p>
<p>Samuel F Feng, Siyu Wang, Sylvia Zarnescu, and Robert C Wilson. The dynamics of explore-exploit decisions reveal a signal-to-noise mechanism for random exploration. Scientific reports, 11(1): $1-15,2021$.</p>
<p>Basile Garcia, Maël Lebreton, Sacha Bourgeois-Gironde, and Stefano Palminteri. Experiential values are underweighted in decisions involving symbolic options. Nature human behaviour, pages 1-16, 2023.</p>
<p>Samuel J Gershman. Deconstructing the human algorithms for exploration. Cognition, 173:34-42, 2018.</p>
<p>Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. Machine intuition: Uncovering human-like intuitive decision-making in gpt-3.5. arXiv preprint arXiv:2212.05206, 2022.</p>
<p>Ralph Hertwig, Greg Barron, Elke U Weber, and Ido Erev. Decisions from experience and the effect of rare events in risky choice. Psychological science, 15(8):534-539, 2004.</p>
<p>Daniel Kahneman and Amos Tversky. Subjective probability: A judgment of representativeness. Cognitive psychology, 3(3):430-454, 1972.</p>
<p>Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274, 2023.</p>
<p>Sreejan Kumar, Theodore R Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A Norman, Thomas L Griffiths, Robert D Hawkins, and Samuel A Nastase. Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model. BioRxiv, pages 2022-06, 2022.</p>
<p>Hanzhou Li, John T Moon, Saptarshi Purkayastha, Leo Anthony Celi, Hari Trivedi, and Judy W Gichoya. Ethics of large language models in medicine and medical research. The Lancet Digital Health, 5(6):e333-e335, 2023.</p>
<p>Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503-528, 1989.</p>
<p>Allen Newell. Unified theories of cognition and the role of soar. Soar: A cognitive architecture in perspective: A tribute to Allen Newell, pages 25-79, 1992.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.</p>
<p>Joshua C Peterson, David D Bourgin, Mayank Agrawal, Daniel Reichman, and Thomas L Griffiths. Using large-scale experiments and machine learning to discover theories of human decision-making. Science, 372(6547):1209-1214, 2021.</p>
<p>Ori Plonsky, Reut Apel, Ido Erev, Eyal Ert, and Moshe Tennenholtz. When and how can social scientists add value to data scientists? a choice prediction competition for human decision making. Unpublished Manuscript, 2018.</p>
<p>Lionel Rigoux, Klaas Enno Stephan, Karl J Friston, and Jean Daunizeau. Bayesian model selection for group studies—revisited. Neuroimage, 84:971-985, 2014.</p>
<p>Reidar Riveland and Alexandre Pouget. A neural model of task compositionality with natural language instructions. bioRxiv, 2022.</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p>
<p>Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sciences, 118(45):e2105646118, 2021.</p>
<p>Eric Schulz and Samuel J Gershman. The algorithmic architecture of exploration in the human brain. Current opinion in neurobiology, 55:7-14, 2019.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023.</p>
<p>Greta Tuckute, Aalok Sathe, Shashank Srikant, Maya Taliaferro, Mingye Wang, Martin Schrimpf, Kendrick Kay, and Evelina Fedorenko. Driving and suppressing the human language network using large language models. bioRxiv, 2023.</p>
<p>Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. arXiv preprint arXiv:2212.09196, 2022.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. TMLR, 2022. ISSN 2835-8856.</p>
<p>Robert C Wilson, Andra Geana, John M White, Elliot A Ludvig, and Jonathan D Cohen. Humans use directed and random exploration to solve the explore-exploit dilemma. Journal of Experimental Psychology: General, 143(6):2074, 2014.</p>
<p>Guangyu Robert Yang, Madhura R Joglekar, H Francis Song, William T Newsome, and Xiao-Jing Wang. Task representations in neural networks trained to perform many cognitive tasks. Nature neuroscience, 22(2):297-306, 2019.</p>
<h1>Supplementary Materials</h1>
<h2>Materials and Methods</h2>
<p>Fitting procedure: For the main analyses, we fitted a separate regularized logistic regression model for each data set via a maximum likelihood estimation. Final model performance was measured through the predictive log-likelihood on hold-out data obtained using a 100 -fold cross-validation procedure. In each fold, we split the data into a training set $(90 \%)$, a validation set $(9 \%)$, and a test set $(1 \%)$. The validation set was used to identify the parameter $\alpha$ that controls the strength of the $\ell_{2}$-regularization term. We considered discrete $\alpha$-values of $[0,0.0001,0.0003,0.001,0.003$, $0.01,0.03,0.1,0.3,1.0]$. The optimization procedure was implemented in PyTorCH [Paszke et al., 2019] and used the default LBFGS optimizer [Liu and Nocedal, 1989]. For the individual difference analyses, the procedure was identical except that we added a random effect for each participant and embedding dimension.
For the hold-out task analyses, the training set consisted of the concatenated choices 13 k and horizon task data. To obtain a validation and test set, we split the data of the experiential-symbolic task into eight folds and repeated the previously described fitting procedure for each fold. The validation set was used to identify the parameter $\alpha$ that controls the strength of the $\ell_{2}$-regularization term and an inverse temperature parameter $\tau^{-1}$. We considered discrete inverse temperature values of $[0.05,0.1$, $0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]$ and $\alpha$-values as described above.</p>
<p>Model simulations: For the main analyses, we simulated model behavior by sampling from the predictions on the test set. For the hold-out task analyses, we simulated data deterministically based on a median threshold (again using the predictions on the test set). The resulting choice curves were generated by fitting a separate logistic regression model for each possible win probability of the E-option. Each model used the win probability of the S-option and an intercept term as independent variables and the probability of choosing the E-option as the dependent variable.</p>
<p>Baseline models: For the LLaMA baseline, we fitted an inverse temperature parameter to human choices using the procedure described above. For the BEAST baseline, we relied on the version provided for the choice prediction competition 2018 [Plonsky et al., 2018]. We additionally included an error model that selects a random choice with a particular probability. We treated this probability as a free parameter and fitted it using the procedure described above. The hybrid model closely followed the implementation of Gershman [2018]. We replaced the probit link function with a logit link function to ensure comparability to CENTaUR.</p>
<h2>Supplementary Text</h2>
<p>For the choices 13 k data set, we prompted each decision independently, thereby ignoring the potential effect of feedback. We used the following template:</p>
<p>Machine 1 delivers 90 dollars with $10.0 \%$ chance and -12 dollars with $90.0 \%$ chance.
Machine 2 delivers -13 dollars with $40.0 \%$ chance and 22 dollars with $60.0 \%$ chance.</p>
<p>Your goal is to maximize the amount of received dollars.</p>
<p>Q: Which machine do you choose?
A: Machine [insert]</p>
<p>For the horizon task, we prompted each task independently, thereby ignoring potential learning effects across the experiment. We used the following template:</p>
<p>You made the following observations in the past:</p>
<ul>
<li>Machine 1 delivered 34 dollars.</li>
<li>
<p>Machine 1 delivered 41 dollars.</p>
</li>
<li>
<p>Machine 2 delivered 57 dollars.</p>
</li>
<li>Machine 1 delivered 37 dollars.</li>
</ul>
<p>Your goal is to maximize the sum of received dollars within six additional choices.</p>
<p>Q: Which machine do you choose?
A: Machine [insert]</p>
<p>For the experiential-symbolic task, we prompted each decision independently and only considered the post-learning phase. We furthermore simplified the observation history by only including the option that is relevant to the current decision. We used the following template:</p>
<p>You made the following observations in the past:</p>
<ul>
<li>Machine 1 delivered 1 dollars.</li>
<li>Machine 1 delivered 1 dollars.</li>
<li>Machine 1 delivered -1 dollars.</li>
<li>Machine 1 delivered 1 dollars.</li>
<li>Machine 1 delivered 1 dollars.</li>
<li>Machine 1 delivered -1 dollars.
$[\cdots]$</li>
<li>Machine 1 delivered 1 dollars.</li>
</ul>
<p>Machine 2 delivers -1 dollars with $30.0 \%$ chance and 1 dollars with $70.0 \%$ chance.</p>
<p>Your goal is to maximize the amount of received dollars.</p>
<p>Q: Which machine do you choose?
A: Machine [insert]</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3290 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3290</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3290</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-5437e8adab596d7294124c0e798708e050e25321</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321" target="_blank">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts.</p>
                <p><strong>Paper Abstract:</strong> Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3290.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3290.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Last-letter L2M vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Last-letter-concatenation task: Least-to-most prompting versus Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison on a symbolic manipulation task where the model must return the concatenation of the last letters of N input words; tests easy-to-hard (length) generalization for different prompting methods on GPT-3 family models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (primary); also evaluated: text-davinci-002, code-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants of OpenAI GPT-3 / Codex family accessed via prompting in experiments. The paper does not provide internal architecture or parameter counts; models are large pretrained transformer LMs specialized for code/text (codex/text variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['least-to-most prompting', 'chain-of-thought prompting', 'standard few-shot prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>least-to-most: two-stage few-shot prompting — (1) decompose an input list into a sequence of simpler subproblems (sequential sublists), (2) sequentially solve each subproblem where each solution is appended to the prompt for the next step; exemplars include a base case and a recursive/extension step. chain-of-thought (CoT): include intermediate natural-language rationales that solve examples end-to-end (independent exemplars) without explicit decomposition into previously-answered subquestions. Standard few-shot: example input→output pairs without intermediate rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The paper contrasts a distinct/diverse method (least-to-most: explicit decomposition + sequential subproblem solving) against a similar-style reasoning approach (chain-of-thought: step-by-step rationales built from scratch). L2M is a different prompting strategy (decomposition + reuse of intermediate answers) rather than just a stylistic variant of CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Last-letter-concatenation (symbolic manipulation, length generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a comma-separated list of words, output the concatenation of the last letters of the words. Evaluation focuses on generalization to longer lists than those in exemplars (lengths from 4 to 12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Using code-davinci-002, accuracies reported for list lengths L=4,6,8,10,12: Standard prompting 0.0% across lengths. Chain-of-Thought (example in paper) 84.2%, 69.2%, 50.2%, 39.8%, 31.8% respectively. Least-to-Most 94.0%, 88.4%, 83.0%, 76.4%, 74.0% respectively. Table 13 additionally reports other shot counts (e.g., L2M 4-shot up to 96.0% at L=4 and 76.6% at L=12) and results on other model variants (text-davinci-002 and code-davinci-001) showing lower absolute accuracies but the same relative trend (L2M > CoT > standard).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Explicit experiments compare L2M, CoT, and standard few-shot across multiple list lengths and prompt sizes; L2M substantially outperforms CoT and standard prompting on length generalization. The authors also ran ablations with more independent CoT exemplars (4-shot, 8-shot) and with CoT using the same examples as L2M; even when CoT had more or different examples, L2M remained substantially better, especially as list length increases. Error analysis attributes remaining L2M failures mostly to concatenation/copying errors rather than decomposition or letter-identification errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most prompting dramatically improves easy-to-hard (length) generalization on this symbolic task compared to chain-of-thought and standard prompting; L2M is more data-efficient and more robust as sequence length increases. The dominant class of L2M errors are concatenation/copy errors, not decomposition failures.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although L2M outperforms CoT, it is not perfect: accuracy for long lists (L=12) remains well below 100% (e.g., 74.0% for 2-shot L2M, up to ~76.6% for 4-shot L2M on code-davinci-002). Error analysis shows concatenation mistakes (dropping/adding/duplicating letters) and occasional template-application or copying errors. Some model variants (code-davinci-001) perform poorly even with L2M, indicating dependence on model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3290.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3290.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCAN L2M vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCAN compositional generalization benchmark: Least-to-most prompting versus Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison on SCAN length-split compositional generalization: mapping natural language commands to action sequences; evaluates whether L2M enables generalization to longer/novel compositions using few exemplars without model training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (primary); also evaluated: text-davinci-002, code-davinci-001</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 / Codex family transformer LMs used with prompting. The paper uses Python-style compact intermediate representation in prompts and postprocesses outputs into final action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['least-to-most prompting (decomposition + mapping)', 'chain-of-thought prompting', 'standard few-shot prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Least-to-most: two prompt contexts — (1) command decomposition prompt (8 exemplars) demonstrating how to break long commands into short commands, (2) command mapping prompt (14 exemplars) demonstrating how to map short commands to action sequences; decomposition output used sequentially to solve full command. Chain-of-thought: same mapping exemplars but without explicit decomposition stage. Standard few-shot: mapping exemplars without intermediate rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The paper contrasts L2M (a decomposition + sequential solving method that produces dependent subquestions) against CoT (similar style but without separate decomposition); thus L2M represents a qualitatively different method rather than a minor stylistic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SCAN (compositional generalization, length split)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmark mapping English-like commands to action sequences; the length split requires generalizing to test sequences longer than training sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On SCAN length split using code-davinci-002: Standard prompting 16.7%, Chain-of-Thought 16.2%, Least-to-Most 99.7%. Using text-davinci-002: Standard 6.0%, Chain-of-Thought 0.0%, Least-to-Most 76.0%. Using code-davinci-001: Standard 0.4%, Chain-of-Thought 0.0%, Least-to-Most 60.7%. (All percentages are accuracy on test set under length split; L2M used 8 decomposition exemplars and 14 mapping exemplars.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct head-to-head comparison shows L2M vastly outperforms CoT and standard prompting on SCAN length split for the stronger model (code-davinci-002). The paper notes that neural-symbolic models trained on the entire SCAN training set have achieved high performance in prior work, but L2M achieves ~99.7% with only a handful of exemplars and no training. Error analysis for the few L2M failures highlights specific linguistic operator misapplications (e.g., interpreting 'around' with 'twice'/'thrice' and mixing 'after' vs 'and').</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most prompting enables near-perfect compositional generalization on SCAN length split with very few exemplars for a capable LM (code-davinci-002), while chain-of-thought and standard prompting largely fail. L2M's explicit decomposition is crucial to solving longer compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>L2M isn't flawless: for code-davinci-002 on SCAN length split there were 13 failures; issues include misapplication of multiplicative modifiers ('thrice'/'twice') after 'around' and incorrectly combining clauses connected by 'after' (treated like 'and'). Also, performance varies by LM variant (text-davinci-002 and code-davinci-001 show lower absolute L2M accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3290.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3290.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K L2M vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8K math reasoning benchmark: Least-to-most prompting versus Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation on grade-school math word problems (GSM8K) to see how L2M generalizes from few-step exemplars to multi-step problems, broken down by number of required reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 / Codex family LM (code-davinci-002) used via few-shot prompting; the paper does not provide parameter counts or training details for the model within the manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['least-to-most prompting (single-pass or two-stage decomposition + solving)', 'chain-of-thought prompting', 'standard few-shot prompting', 'zero-shot prompting (baseline)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>L2M prompt includes an exemplar where the problem is explicitly decomposed into subquestions and each subquestion is solved sequentially (single-pass combination of decomposition and solving for some experiments). Chain-of-thought removes the decomposition step and provides an end-to-end worked solution. Standard and zero-shot baselines are also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>L2M introduces explicit decomposition (different method) compared to CoT (a similar step-by-step rationale style); both are natural-language reasoning prompts but L2M enforces decomposition into subproblems that are then sequentially solved.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K (grade-school math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A dataset of multi-step math word problems where solutions typically require multi-step arithmetic reasoning; the paper stratifies results by number of reasoning steps required (2,3,4,>=5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On GSM8K with code-davinci-002: Zero-shot 16.38%, Standard prompting 17.06%, Chain-of-Thought 60.87%, Least-to-Most 62.39% (overall). By step count: Least-to-Most: All 62.39%; 2-step 74.53%, 3-step 68.91%, 4-step 59.73%, >=5-step 45.23%. Chain-of-Thought: All 60.87%; 2-step 76.68%, 3-step 67.29%, 4-step 59.39%, >=5-step 39.07%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Overall improvement of L2M over CoT is modest (62.39% vs 60.87%), but L2M shows a clearer advantage on problems requiring many reasoning steps (>=5) where L2M achieves 45.23% vs CoT 39.07%. Conversely, CoT slightly outperforms L2M on the easiest (2-step) problems. The authors note that providing correct manual decompositions often allows models to solve nearly all GSM8K problems, indicating decomposition is a key lever.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most prompting yields small overall gains on GSM8K relative to chain-of-thought, but provides substantial improvements on long multi-step problems (>=5 steps). The result suggests decomposition helps with easy-to-hard generalization in math reasoning, particularly for problems requiring many sequential steps.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>L2M does not uniformly outperform CoT: on 2-step problems CoT slightly outperforms L2M (76.68% vs 74.53%). The overall gain across GSM8K is small, indicating that not all reasoning problems benefit equally from automated decomposition; human-crafted decompositions often do better than the model-generated decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3290.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3290.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DROP L2M vs CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DROP reading-comprehension benchmark (numerical subset): Least-to-most prompting versus Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation on the numerical subset of DROP (discrete reasoning over paragraphs) comparing prompting strategies. The focus is on whether L2M's decomposition helps multi-step discrete reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (primary); also reported text-davinci-002, LM-540B in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer language models (GPT-3 / Codex family) accessed via prompting. The paper evaluates on a subset of DROP containing numerical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['least-to-most prompting', 'chain-of-thought prompting', 'standard few-shot prompting', 'zero-shot prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Least-to-most: separate decomposition and subproblem-solving prompt contexts (e.g., 5 decomposition examples and 3 solving examples for non-football subset) used to iteratively decompose paragraph-based questions and solve subquestions. Chain-of-thought: few-shot rationale-based solutions without explicit decomposition. Baselines include standard few-shot and zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>L2M applies decomposition-specific prompting (a distinct approach) as opposed to CoT which provides end-to-end internal rationales; the two represent different prompting strategies rather than identical styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (numerical subset) and a football subset (both require discrete, multi-step reasoning over paragraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>DROP requires discrete reasoning over passages (counting, addition/subtraction, selecting spans, arithmetic) — many problems are naturally decomposable into simpler subquestions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported with code-davinci-002 on two DROP subsets: Non-football subset: Zero-shot 43.86%, Standard prompting 58.78%, Chain-of-Thought 74.77%, Least-to-Most 82.45%. Football subset: Zero-shot 51.77%, Standard 62.73%, Chain-of-Thought 59.56%, Least-to-Most 73.42%. (Percent accuracy as reported in Table 11; L2M substantially outperforms CoT on the non-football subset and outperforms CoT on the football subset as well.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>L2M outperforms CoT and the other baselines by substantial margins on DROP's numerical problems, consistent with the observation that many DROP problems are trivially decomposable and benefit from explicit decomposition. The paper provides examples where L2M succeeds and CoT fails and includes error analyses for L2M.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most prompting yields large gains on DROP numerical problems relative to chain-of-thought and standard prompting; decomposition is especially effective when the task naturally decomposes into independent or sequential subquestions.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some DROP problems still fail with L2M; appendix error analysis shows failures due to incorrect decomposition or incorrect subproblem solving. For the football subset CoT performed relatively poorly vs L2M, indicating task-dependent method efficacy; success depends on designing decomposition exemplars appropriate to the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks <em>(Rating: 2)</em></li>
                <li>Compositional generalization via neural-symbolic stack machines <em>(Rating: 2)</em></li>
                <li>Neural Logic Machines <em>(Rating: 1)</em></li>
                <li>Learning compositional rules via neural program synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3290",
    "paper_id": "paper-5437e8adab596d7294124c0e798708e050e25321",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "Last-letter L2M vs CoT",
            "name_full": "Last-letter-concatenation task: Least-to-most prompting versus Chain-of-thought prompting",
            "brief_description": "Comparison on a symbolic manipulation task where the model must return the concatenation of the last letters of N input words; tests easy-to-hard (length) generalization for different prompting methods on GPT-3 family models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (primary); also evaluated: text-davinci-002, code-davinci-001",
            "model_description": "Variants of OpenAI GPT-3 / Codex family accessed via prompting in experiments. The paper does not provide internal architecture or parameter counts; models are large pretrained transformer LMs specialized for code/text (codex/text variants).",
            "model_size": null,
            "reasoning_methods": [
                "least-to-most prompting",
                "chain-of-thought prompting",
                "standard few-shot prompting"
            ],
            "reasoning_methods_description": "least-to-most: two-stage few-shot prompting — (1) decompose an input list into a sequence of simpler subproblems (sequential sublists), (2) sequentially solve each subproblem where each solution is appended to the prompt for the next step; exemplars include a base case and a recursive/extension step. chain-of-thought (CoT): include intermediate natural-language rationales that solve examples end-to-end (independent exemplars) without explicit decomposition into previously-answered subquestions. Standard few-shot: example input→output pairs without intermediate rationales.",
            "diversity_of_methods": "The paper contrasts a distinct/diverse method (least-to-most: explicit decomposition + sequential subproblem solving) against a similar-style reasoning approach (chain-of-thought: step-by-step rationales built from scratch). L2M is a different prompting strategy (decomposition + reuse of intermediate answers) rather than just a stylistic variant of CoT.",
            "reasoning_task_name": "Last-letter-concatenation (symbolic manipulation, length generalization)",
            "reasoning_task_description": "Given a comma-separated list of words, output the concatenation of the last letters of the words. Evaluation focuses on generalization to longer lists than those in exemplars (lengths from 4 to 12).",
            "performance_by_method": "Using code-davinci-002, accuracies reported for list lengths L=4,6,8,10,12: Standard prompting 0.0% across lengths. Chain-of-Thought (example in paper) 84.2%, 69.2%, 50.2%, 39.8%, 31.8% respectively. Least-to-Most 94.0%, 88.4%, 83.0%, 76.4%, 74.0% respectively. Table 13 additionally reports other shot counts (e.g., L2M 4-shot up to 96.0% at L=4 and 76.6% at L=12) and results on other model variants (text-davinci-002 and code-davinci-001) showing lower absolute accuracies but the same relative trend (L2M &gt; CoT &gt; standard).",
            "comparison_of_methods": "Explicit experiments compare L2M, CoT, and standard few-shot across multiple list lengths and prompt sizes; L2M substantially outperforms CoT and standard prompting on length generalization. The authors also ran ablations with more independent CoT exemplars (4-shot, 8-shot) and with CoT using the same examples as L2M; even when CoT had more or different examples, L2M remained substantially better, especially as list length increases. Error analysis attributes remaining L2M failures mostly to concatenation/copying errors rather than decomposition or letter-identification errors.",
            "key_findings": "Least-to-most prompting dramatically improves easy-to-hard (length) generalization on this symbolic task compared to chain-of-thought and standard prompting; L2M is more data-efficient and more robust as sequence length increases. The dominant class of L2M errors are concatenation/copy errors, not decomposition failures.",
            "counter_examples_or_negative_results": "Although L2M outperforms CoT, it is not perfect: accuracy for long lists (L=12) remains well below 100% (e.g., 74.0% for 2-shot L2M, up to ~76.6% for 4-shot L2M on code-davinci-002). Error analysis shows concatenation mistakes (dropping/adding/duplicating letters) and occasional template-application or copying errors. Some model variants (code-davinci-001) perform poorly even with L2M, indicating dependence on model capabilities.",
            "uuid": "e3290.0",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "SCAN L2M vs CoT",
            "name_full": "SCAN compositional generalization benchmark: Least-to-most prompting versus Chain-of-thought prompting",
            "brief_description": "Comparison on SCAN length-split compositional generalization: mapping natural language commands to action sequences; evaluates whether L2M enables generalization to longer/novel compositions using few exemplars without model training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (primary); also evaluated: text-davinci-002, code-davinci-001",
            "model_description": "OpenAI GPT-3 / Codex family transformer LMs used with prompting. The paper uses Python-style compact intermediate representation in prompts and postprocesses outputs into final action sequences.",
            "model_size": null,
            "reasoning_methods": [
                "least-to-most prompting (decomposition + mapping)",
                "chain-of-thought prompting",
                "standard few-shot prompting"
            ],
            "reasoning_methods_description": "Least-to-most: two prompt contexts — (1) command decomposition prompt (8 exemplars) demonstrating how to break long commands into short commands, (2) command mapping prompt (14 exemplars) demonstrating how to map short commands to action sequences; decomposition output used sequentially to solve full command. Chain-of-thought: same mapping exemplars but without explicit decomposition stage. Standard few-shot: mapping exemplars without intermediate rationales.",
            "diversity_of_methods": "The paper contrasts L2M (a decomposition + sequential solving method that produces dependent subquestions) against CoT (similar style but without separate decomposition); thus L2M represents a qualitatively different method rather than a minor stylistic variation.",
            "reasoning_task_name": "SCAN (compositional generalization, length split)",
            "reasoning_task_description": "Benchmark mapping English-like commands to action sequences; the length split requires generalizing to test sequences longer than training sequences.",
            "performance_by_method": "On SCAN length split using code-davinci-002: Standard prompting 16.7%, Chain-of-Thought 16.2%, Least-to-Most 99.7%. Using text-davinci-002: Standard 6.0%, Chain-of-Thought 0.0%, Least-to-Most 76.0%. Using code-davinci-001: Standard 0.4%, Chain-of-Thought 0.0%, Least-to-Most 60.7%. (All percentages are accuracy on test set under length split; L2M used 8 decomposition exemplars and 14 mapping exemplars.)",
            "comparison_of_methods": "Direct head-to-head comparison shows L2M vastly outperforms CoT and standard prompting on SCAN length split for the stronger model (code-davinci-002). The paper notes that neural-symbolic models trained on the entire SCAN training set have achieved high performance in prior work, but L2M achieves ~99.7% with only a handful of exemplars and no training. Error analysis for the few L2M failures highlights specific linguistic operator misapplications (e.g., interpreting 'around' with 'twice'/'thrice' and mixing 'after' vs 'and').",
            "key_findings": "Least-to-most prompting enables near-perfect compositional generalization on SCAN length split with very few exemplars for a capable LM (code-davinci-002), while chain-of-thought and standard prompting largely fail. L2M's explicit decomposition is crucial to solving longer compositions.",
            "counter_examples_or_negative_results": "L2M isn't flawless: for code-davinci-002 on SCAN length split there were 13 failures; issues include misapplication of multiplicative modifiers ('thrice'/'twice') after 'around' and incorrectly combining clauses connected by 'after' (treated like 'and'). Also, performance varies by LM variant (text-davinci-002 and code-davinci-001 show lower absolute L2M accuracy).",
            "uuid": "e3290.1",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "GSM8K L2M vs CoT",
            "name_full": "GSM8K math reasoning benchmark: Least-to-most prompting versus Chain-of-thought prompting",
            "brief_description": "Evaluation on grade-school math word problems (GSM8K) to see how L2M generalizes from few-step exemplars to multi-step problems, broken down by number of required reasoning steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (primary)",
            "model_description": "GPT-3 / Codex family LM (code-davinci-002) used via few-shot prompting; the paper does not provide parameter counts or training details for the model within the manuscript.",
            "model_size": null,
            "reasoning_methods": [
                "least-to-most prompting (single-pass or two-stage decomposition + solving)",
                "chain-of-thought prompting",
                "standard few-shot prompting",
                "zero-shot prompting (baseline)"
            ],
            "reasoning_methods_description": "L2M prompt includes an exemplar where the problem is explicitly decomposed into subquestions and each subquestion is solved sequentially (single-pass combination of decomposition and solving for some experiments). Chain-of-thought removes the decomposition step and provides an end-to-end worked solution. Standard and zero-shot baselines are also reported.",
            "diversity_of_methods": "L2M introduces explicit decomposition (different method) compared to CoT (a similar step-by-step rationale style); both are natural-language reasoning prompts but L2M enforces decomposition into subproblems that are then sequentially solved.",
            "reasoning_task_name": "GSM8K (grade-school math word problems)",
            "reasoning_task_description": "A dataset of multi-step math word problems where solutions typically require multi-step arithmetic reasoning; the paper stratifies results by number of reasoning steps required (2,3,4,&gt;=5).",
            "performance_by_method": "On GSM8K with code-davinci-002: Zero-shot 16.38%, Standard prompting 17.06%, Chain-of-Thought 60.87%, Least-to-Most 62.39% (overall). By step count: Least-to-Most: All 62.39%; 2-step 74.53%, 3-step 68.91%, 4-step 59.73%, &gt;=5-step 45.23%. Chain-of-Thought: All 60.87%; 2-step 76.68%, 3-step 67.29%, 4-step 59.39%, &gt;=5-step 39.07%.",
            "comparison_of_methods": "Overall improvement of L2M over CoT is modest (62.39% vs 60.87%), but L2M shows a clearer advantage on problems requiring many reasoning steps (&gt;=5) where L2M achieves 45.23% vs CoT 39.07%. Conversely, CoT slightly outperforms L2M on the easiest (2-step) problems. The authors note that providing correct manual decompositions often allows models to solve nearly all GSM8K problems, indicating decomposition is a key lever.",
            "key_findings": "Least-to-most prompting yields small overall gains on GSM8K relative to chain-of-thought, but provides substantial improvements on long multi-step problems (&gt;=5 steps). The result suggests decomposition helps with easy-to-hard generalization in math reasoning, particularly for problems requiring many sequential steps.",
            "counter_examples_or_negative_results": "L2M does not uniformly outperform CoT: on 2-step problems CoT slightly outperforms L2M (76.68% vs 74.53%). The overall gain across GSM8K is small, indicating that not all reasoning problems benefit equally from automated decomposition; human-crafted decompositions often do better than the model-generated decompositions.",
            "uuid": "e3290.2",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "DROP L2M vs CoT",
            "name_full": "DROP reading-comprehension benchmark (numerical subset): Least-to-most prompting versus Chain-of-thought prompting",
            "brief_description": "Evaluation on the numerical subset of DROP (discrete reasoning over paragraphs) comparing prompting strategies. The focus is on whether L2M's decomposition helps multi-step discrete reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (primary); also reported text-davinci-002, LM-540B in appendix",
            "model_description": "Large pretrained transformer language models (GPT-3 / Codex family) accessed via prompting. The paper evaluates on a subset of DROP containing numerical problems.",
            "model_size": null,
            "reasoning_methods": [
                "least-to-most prompting",
                "chain-of-thought prompting",
                "standard few-shot prompting",
                "zero-shot prompting"
            ],
            "reasoning_methods_description": "Least-to-most: separate decomposition and subproblem-solving prompt contexts (e.g., 5 decomposition examples and 3 solving examples for non-football subset) used to iteratively decompose paragraph-based questions and solve subquestions. Chain-of-thought: few-shot rationale-based solutions without explicit decomposition. Baselines include standard few-shot and zero-shot.",
            "diversity_of_methods": "L2M applies decomposition-specific prompting (a distinct approach) as opposed to CoT which provides end-to-end internal rationales; the two represent different prompting strategies rather than identical styles.",
            "reasoning_task_name": "DROP (numerical subset) and a football subset (both require discrete, multi-step reasoning over paragraphs)",
            "reasoning_task_description": "DROP requires discrete reasoning over passages (counting, addition/subtraction, selecting spans, arithmetic) — many problems are naturally decomposable into simpler subquestions.",
            "performance_by_method": "Reported with code-davinci-002 on two DROP subsets: Non-football subset: Zero-shot 43.86%, Standard prompting 58.78%, Chain-of-Thought 74.77%, Least-to-Most 82.45%. Football subset: Zero-shot 51.77%, Standard 62.73%, Chain-of-Thought 59.56%, Least-to-Most 73.42%. (Percent accuracy as reported in Table 11; L2M substantially outperforms CoT on the non-football subset and outperforms CoT on the football subset as well.)",
            "comparison_of_methods": "L2M outperforms CoT and the other baselines by substantial margins on DROP's numerical problems, consistent with the observation that many DROP problems are trivially decomposable and benefit from explicit decomposition. The paper provides examples where L2M succeeds and CoT fails and includes error analyses for L2M.",
            "key_findings": "Least-to-most prompting yields large gains on DROP numerical problems relative to chain-of-thought and standard prompting; decomposition is especially effective when the task naturally decomposes into independent or sequential subquestions.",
            "counter_examples_or_negative_results": "Some DROP problems still fail with L2M; appendix error analysis shows failures due to incorrect decomposition or incorrect subproblem solving. For the football subset CoT performed relatively poorly vs L2M, indicating task-dependent method efficacy; success depends on designing decomposition exemplars appropriate to the domain.",
            "uuid": "e3290.3",
            "source_info": {
                "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "rating": 2
        },
        {
            "paper_title": "Compositional generalization via neural-symbolic stack machines",
            "rating": 2
        },
        {
            "paper_title": "Neural Logic Machines",
            "rating": 1
        },
        {
            "paper_title": "Learning compositional rules via neural program synthesis",
            "rating": 1
        }
    ],
    "cost": 0.015124499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LeAST-TO-MOST PROMPTING EnAbleS COMPLEX REASONING IN LARGE LANGUAGE MODELS</h1>
<p>Denny Zhou ${ }^{\dagger * *}$ Nathanael Schärli ${ }^{\dagger}$ Le Hou ${ }^{\dagger}$ Jason Wei ${ }^{\dagger}$ Nathan Scales ${ }^{\dagger}$ Xuezhi Wang ${ }^{\dagger}$ Dale Schuurmans ${ }^{\dagger}$ Claire Cui ${ }^{\dagger}$ Olivier Bousquet ${ }^{\dagger}$ Quoc Le ${ }^{\dagger}$ Ed Chi ${ }^{\dagger}$<br>${ }^{\dagger}$ Google Research, Brain Team</p>
<h4>Abstract</h4>
<p>Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least $99 \%$ using just 14 exemplars, compared to only $16 \%$ accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.</p>
<h2>1 INTRODUCTION</h2>
<p>Despite the great success of deep learning in the past decade, there still remain huge differences between human intelligence and machine learning: (1) Given a new task, humans usually can learn to accomplish it from only a few demonstration examples, while machine learning requires a large amount of labeled data for model training; (2) Humans can clearly explain the underlying rationale for their predictions or decisions, while machine learning is essentially a black box; (3) Humans can solve problems more difficult than any they have seen before, while for machine learning, examples in training and testing are typically at the same level of difficulty.
The recently proposed chain-of-thought prompting approach (Wei et al., 2022; Chowdhery et al., 2022) has taken a significant step for narrowing the gap between human intelligence and machine intelligence. It combines the idea of natural language rationales (Ling et al., 2017; Cobbe et al., 2021) with few-shot prompting (Brown et al., 2020). When further integrated with self-consistency decoding (Wang et al., 2022b) rather than using the typical greedy decoding, few-shot chain-of-thought prompting largely outperforms the state-of-the-art results in the literature on many challenging natural language processing tasks obtained from specially designed neural models trained with hundreds of times more annotated examples, while being fully interpretable.</p>
<p>However, chain-of-thought prompting has a key limitation-it often performs poorly on tasks that require generalization of solving problems harder than the demonstration examples, such as compositional generalization (Lake \&amp; Baroni, 2018; Keysers et al., 2020). To tackle such easy-to-hard generalization issues, we propose least-to-most prompting. It consists of two stages: first decomposing a complex problem into a list of easier subproblems, and then sequentially solving these subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>subproblems. Both stages are implemented by few-shot prompting, so that there is no training or finetuning in either stage. An example usage of least-to-most prompting is illustrated in Figure 1.</p>
<p>The term least-to-most prompting is borrowed from educational psychology (Libby et al., 2008), where it is used to denote the technique of using a progressive sequence of prompts to help a student to learn a new skill. Here we apply this technique for teaching humans to teach language models. Empirical results on symbolic manipulation, compositional generalization, and math reasoning show that least-to-most prompting can indeed generalize to problems harder than those demonstrated.</p>
<h1>Stage 1: Decompose Question into Subquestions</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Least-to-most prompting solving a math word problem in two stages: (1) query the language model to decompose the problem into subproblems; (2) query the language model to sequentially solve the subproblems. The answer to the second subproblem is built on the answer to the first subproblem. The demonstration examples for each stage's prompt are omitted in this illustration.</p>
<h2>2 LeAST-TO-MOST PROMPTING</h2>
<p>Least-to-most prompting teaches language models how to solve a complex problem by decomposing it to a series of simpler subproblems. It consists of two sequential stages:</p>
<ol>
<li>Decomposition. The prompt in this stage contains constant examples that demonstrate the decomposition, followed by the specific question to be decomposed.</li>
<li>Subproblem solving. The prompt in this stage consists of three parts: (1) constant examples demonstrating how subproblems are solved; (2) a potentially empty list of previously answered subquestions and generated solutions, and (3) the question to be answered next.</li>
</ol>
<p>In the example shown in Figure 1, the language model is first asked to decompose the original problem into subproblems. The prompt that is passed to the model consists of examples that illustrate how to decompose complex problems (which are not shown in the figure), followed by the specific problem to be decomposed (as shown in the figure). The language model figures out that the original problem can be solved via solving an intermediate problem "How long does each trip take?".</p>
<p>In the next phase, we ask the language model to sequentially solve the subproblems from the problem decomposition stage. The original problem is appended as the final subproblem. The solving starts from passing to the language model a prompt that consists of examples that illustrate how problems are solved (not shown in the figure), followed by the first subproblem "How long does each trip take?". We then take the answer generated by the language model ("... each trip takes 5 minutes.") and construct the next prompt by appending the generated answer to the previous prompt, followed by the next subproblem, which happens to be the original problem in this example. The new prompt is then passed back to the language model, which returns the final answer.</p>
<p>Least-to-most prompting can be combined with other prompting techniques like chain-of-thought (Wei et al., 2022) and self-consistency (Wang et al., 2022b), but does not need to be. Also, for some tasks, the two stages in least-to-most prompting can be merged to form a single-pass prompt.</p>
<h1>3 ReSults</h1>
<p>We present least-to-most prompting results for symbolic manipulation, compositional generalization, and math reasoning tasks, and compare it with chain-of-thought prompting.</p>
<h3>3.1 Symbolic Manipulation</h3>
<p>We take the last-letter-concatenation task (Wei et al., 2022). In this task, each input is a list of words, and the corresponding output is the concatenation of the last letters of the words in the list. For example, "thinking, machine" outputs "ge", since the last letter of "thinking" is " g " and the last letter of "machine" is " e ". Chain-of-thought prompting does a perfect job when the testing lists have the same length as the lists in the prompt exemplars. However, it performs poorly when the testing lists are much longer than the lists in the prompt exemplars. We show that least-to-most prompting overcomes this limitation and significantly outperforms chain-of-thought prompting on length generalization.</p>
<p>Q: "think, machine, learning"
A: "think", "think, machine", "think, machine, learning"</p>
<p>Table 1: Least-to-most prompt context (decomposition) for the last-letter-concatenation task. It can decompose arbitrary long lists into sequential subsists with an accuracy of $100 \%$.</p>
<p>Q: "think, machine"
A: The last letter of "think" is " k ". The last letter of "machine" is " e ". Concatenating " k ", " e " leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "think, machine, learning"
A: "think, machine" outputs "ke". The last letter of "learning" is " g ". Concatenating "ke", " g " leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<p>Table 2: Least-to-most prompt context (solution) for the last-letter-concatenation task. The two exemplars in this prompt actually demonstrate a base case and a recursive step.</p>
<p>Least-to-most prompting. The least-to-most prompt contexts for the last-letter-concatenation task are shown in Tables 1 and 2. The exemplar in Table 1 demonstrates how to decompose a list into a sequence of sublists. The exemplar in Table 2 demonstrates how to map an input to the desired output. Given a new list, we first append it to the exemplar in Table 1 to construct the decomposition prompt, which is sent to the language model to obtain the list's decomposition. Then, we construct for each sublist $S$ a solution prompt, which consists of the exemplars in Table 2, followed by the previous sublist/response pairs (if any), followed by $S$. We sequentially issue these prompts to the language model and use the last response as the final solution.</p>
<p>It is worth a closer look at the exemplars in Table 2. Essentially, they teach language models how to build answers to new problems using the answers to previously solved problems: (1) the list in the</p>
<p>second exemplar ("think, machine, learning") is an extension of the list in the first exemplar ("think, machine") rather than an entirely independent one; (2) the response to "think, machine, learning" is built on the output of "think, machine" by starting with a sentence saying that "think, machine" outputs "ke". The two exemplars together illustrate a base case and a recursive step.
Chain-of-thought prompting. The chain-of-thought prompt context for the last-letterconcatenation task is listed in Table 3. It uses the same lists as the least-to-most prompt in Table 2. The only difference is that, in the chain-of-thought prompt, the response to the second list ("think, machine, learning") is built from scratch, instead of using the output of the first list ("think, machine").</p>
<p>Q: "think, machine"
A: The last letter of "think" is " k ". The last letter of "machine" is " e ". Concatenating " k ", " e " leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "think, machine, learning"
A: The last letter of "think" is " k ". The last letter of "machine" is " e ". The last letter of "learning" is " g ". Concatenating " k ", " e ", " g " leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<p>Table 3: Chain-of-thought prompt context for the last-letter-concatenation task. Unlike the least-to-most prompt in Table 2, the exemplars in the chain-of-thought prompt are independent of each other.</p>
<p>We compare least-to-most prompting (Table $1 \&amp; 2$ ) with chain-of-thought prompting (Table 3) and the standard few-shot prompting. The prompt for the standard few-shot prompting is constructed by removing the intermediate explanations in the chain-of-thought prompt. That is, it just consists of these two exemplars: (1) "think, machine" outputs "ke"; and (2) "think, machine, learning" outputs "keg". We do not consider a training or finetuning baseline because a machine learning model based on two examples would generalize very poorly.
Results. We randomly sample words in Wiktionary ${ }^{1}$ to construct testing lists with lengths varying from 4 to 12 . For each given length, 500 lists are constructed. The accuracies of different methods with code-davinci-002 in GPT-3 are shown in Table 4. Standard prompting completely fails all test cases with an accuracy of 0 . Chain-of-thought prompting significantly boosts the performance over standard prompting, but it still falls well behind least-to-most prompting, particularly when the lists are long. Moreover, the performance of chain-of-thought prompting drops much faster than least-to-most prompting as the length increases.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$L=4$</th>
<th style="text-align: center;">$L=6$</th>
<th style="text-align: center;">$L=8$</th>
<th style="text-align: center;">$L=10$</th>
<th style="text-align: center;">$L=12$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard prompting</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Chain-of-Thought</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: left;">Least-to-Most</td>
<td style="text-align: center;">$\mathbf{9 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 4}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracies of different prompting methods on the last-letter-concatenation task. The length of testing lists increases from 4 to 12 .</p>
<p>In Appendices 7.2 and 7.3, we present additional experiments with different chain-of-thought prompts and different language models. Note that in contrast to least-to-most prompting, the exemplars in a chain-of-thought prompt can be independent of each other. For the last-letter concatenation task, this means that we do not need to present exemplars that are sublists of other exemplars. In fact, a chain-of-thought prompt with independent lists tends to outperform one with dependent lists, as the former conveys more information. Furthermore, we can enhance chain-of-thought prompting by incorporating additional exemplars. This seems to be fair, as the least-to-most prompt contains more words due to its extra decomposition. As shown in Table 13 (Appendix 7.3), for lists with length 12, chain-of-thought prompting achieves an accuracy of $37.4 \%$ with 4 independent exemplars (Appendix 7.2.2), and $38.4 \%$ with 8 independent exemplars (Appendix 7.2.3). Although there</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>have been notable advancements compared to an accuracy of $31.8 \%$ by the original prompt in Table 3 , chain-of-thought prompting still lags behind least-to-most prompting, which boasts an accuracy of $74.0 \%$.</p>
<p>Error analysis. While least-to-most prompting significantly outperforms chain-of-thought prompting, it is still far from achieving $100 \%$ accuracy for long lists. In Appendix 7.4, we present a detailed error analysis. We find that only very few of them are due to incorrect last letters, while most of them are concatenation errors (dropping or adding a letter). For example, given the list "gratified, contract, fortitude, blew", the model drops the last letter in the concatenation of "dte" and "w", and thus predicts the outcome to be "dte" instead of "dtew". In another example "hollow, supplies, function, gorgeous", the model somehow duplicates the last letter " s " in the concatenation of "wsn" and "s", and thus the prediction becomes "wsnss" instead of "wsns".</p>
<h1>3.2 COMPOSITIONAL GENERALIZATION</h1>
<p>SCAN (Lake \&amp; Baroni, 2018) is probably the most popular benchmark for evaluating compositional generalization. It requires mapping natural language commands to action sequences (Table 5). Sequence-to-sequence models perform poorly under length split where the action sequences in the training set (about $80 \%$ of the full set with over 20,000 examples) are shorter than the action sequences in the testing set. Many specialized neural-symbolic models have been proposed to solve SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021; Kim, 2021). We show that large language models with least-to-most prompting can solve SCAN using only a few demonstration examples. No training or finetuning is needed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Action Sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">"look thrice after jump"</td>
<td style="text-align: left;">JUMP LOOK LOOK LOOK</td>
</tr>
<tr>
<td style="text-align: left;">"run left and walk"</td>
<td style="text-align: left;">TURN_LEFT RUN WALK</td>
</tr>
<tr>
<td style="text-align: left;">"look opposite right"</td>
<td style="text-align: left;">TURN_RIGHT TURN_RIGHT LOOK</td>
</tr>
</tbody>
</table>
<p>Table 5: Example commands in SCAN and their corresponding action sequences. An agent successfully executes a natural language command by performing its corresponding action sequence.</p>
<p>Least-to-most prompting. Like the last-letter-concatenation task in Section 3.1, least-to-most prompting for SCAN is based on two kinds of prompts: (1) a command decomposition prompt containing 8 exemplars to demonstrate how to decompose a long command to a list of short commands (see Table 6 for some of these exemplars); and (2) a command mapping prompt containing 14 exemplars to demonstrate how to map natural language commands to action sequences (see Table 7 for some of these exemplars). The full prompt contexts for both decomposition and mapping are listed in Appendix 8. The exemplars used in the command-mapping prompt are intended to fully cover the semantics of the SCAN commands.</p>
<div class="codehilite"><pre><span></span><code>Q: &quot;look opposite right thrice after walk&quot;
A: &quot;look opposite right thrice&quot; can be solved by: &quot;look opposite right&quot;, &quot;look opposite right
thrice&quot;. &quot;walk&quot; can be solved by &quot;walk&quot;. So, &quot;look opposite right thrice after walk&quot; can be
solved by: &quot;look opposite right&quot;, &quot;look opposite right thrice&quot;, &quot;walk&quot;.
Q: &quot;look around right thrice and walk&quot;
A: &quot;look around right thrice&quot; can be solved by: &quot;look right&quot;, &quot;look around right&quot;, &quot;look around
right thrice&quot;. &quot;walk&quot; can be solved by &quot;walk&quot;. So, &quot;look around right thrice and walk&quot; can be
solved by: &quot;look right&quot;, &quot;look around right&quot;, &quot;look around right thrice&quot;, &quot;walk&quot;.
</code></pre></div>

<p>Table 6: Two exemplars taken from the least-to-most prompt for decomposing a long command to a sequence of short commands.</p>
<p>We use Python notation to make our prompts in least-to-most prompting and the baselines (stand few-shot prompting and chain-of-thought prompting) concise and meet the input size limit of language models (usually up to 2048 tokens). For example, in our prompt design, we map "look twice"</p>
<p>to "LOOK" * 2 rather than "LOOK LOOK". We may have to point out that these Python expressions are just intermediate representations. After obtaining responses from language models, we run a postprocessing script to expand the Python expressions to generate the final results. However, it should not be surprising that we can also further teach language models to correctly process expressions. In Appendix 8.4, we show that with several demonstration exemplars, language models are able to expand Python expressions with a near-perfect accuracy of $99.7 \%$.</p>
<div class="codehilite"><pre><span></span><code>Q: &quot;jump left&quot;
A: The output of &quot;jump left&quot; concatenates: the output of &quot;turn left&quot;, the output of &quot;jump&quot;. &quot;turn
left&quot; outputs &quot;TURN_LEFT&quot;. &quot;jump&quot; outputs &quot;JUMP&quot;. So concatenating the output of &quot;turn
left&quot; and the output of &quot;jump&quot; leads to &quot;TURN_LEFT&quot; + &quot;JUMP&quot;. So the output of &quot;jump left&quot;
is &quot;TURN_LEFT&quot; + &quot;JUMP&quot;.
Q: &quot;run and look twice&quot;
A: The output of &quot;run and look twice&quot; concatenates: the output of &quot;run&quot;, the output of &quot;look
twice&quot;. &quot;run&quot; outputs &quot;RUN&quot;. &quot;look twice&quot; outputs &quot;LOOK&quot; <span class="gs">* 2. So concatenating the output of</span>
<span class="gs">&quot;run&quot; and the output of &quot;look twice&quot; leads to &quot;RUN&quot; + &quot;LOOK&quot; *</span> 2. So the output of &quot;run and
look twice&quot; is &quot;RUN&quot; + &quot;LOOK&quot; * 2.
</code></pre></div>

<p>Table 7: Two exemplars taken from the least-to-most / chain-of-thought prompt for mapping commands to action sequences. Python expressions are used as intermediate representations.</p>
<p>Chain-of-thought prompting. The chain-of-thought prompt for SCAN uses the same commandmapping context as least-to-most prompting (see Table 7) but it does not use command decomposition, which is exclusively used for least-to-most prompting.</p>
<p>Results. We compare least-to-most prompting with chain-of-thought prompting and standard fewshot prompting. The exemplars for standard few-shot prompting are derived from the chain-ofthought prompt by removing the intermediate explanations. The accuracies of different prompting methods with different language models are presented in Table 8. Example outputs can be found in Appendix 8.3. Using code-davinci-002, least-to-most prompting achieves an accuracy of $99.7 \%$ under length split. We also test least-to-most prompting on all other splits and even the full SCAN dataset. We find that its solving rate remains the same. In addition, it may be interesting to note that code-davinci-002 consistently outperforms text-davinci-002, regardless of the prompting method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Standard prompting</th>
<th style="text-align: center;">Chain-of-Thought</th>
<th style="text-align: center;">Least-to-Most</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">code-davinci-002</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">$\mathbf{9 9 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">text-davinci-002</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{7 6 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">code-davinci-001</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$\mathbf{6 0 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracies (\%) of different prompting methods on the test set of SCAN under length split. The results of text-davinci-002 are based on a random subset of 100 commands.</p>
<p>Error analysis. In the test set of the length split, there are 13 failures in total from least-to-most prompting: 6 of them incorrectly interpret "twice" and "thrice" following "around", and the rest incorrectly interpret "after" as "and". Let us show a failed example for each category. In the example "walk opposite right twice after run around right thrice", code-davinci-002 correctly translates the expression "run around right" to ("TURN_RIGHT" + "RUN") * 4. Then it makes a mistake when applying "thrice" to this expression and produces ("TURN_RIGHT" + "RUN") * 9 instead of ("TURN_RIGHT" + "RUN") * 4 * 3 or ("TURN_RIGHT" + "RUN") * 12. In the example "run opposite left thrice after run around left twice", code-davinci-002 produces the correct translations for both sub-expressions that are connected by "after" but it combines them as if they were connected by "and". This means that the model produces ("TURN_LEFT" * 2 + "RUN") * 3 + ("TURN_LEFT" + "RUN") * 4 * 2 instead of ("TURN_LEFT" + "RUN") * 4 * 2 + ("TURN_LEFT" * 2 + "RUN") * 3. A detailed error analysis can be found in Appendix 8.2.</p>
<h1>3.3 Math ReAsoning</h1>
<p>In this section, we apply least-to-most prompting to solve math word problems in GSM8K (Cobbe et al., 2021) and DROP (Dua et al., 2019). We are particularly interested at seeing if large language models combined with least-to-most prompting can solve problems more difficult than those seen in the prompts. Here, we simply measure the difficulty by the number of solving steps.</p>
<p>Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Let's break down this problem: 1. How many apples does Anna have? 2. How many apples do they have together?</p>
<ol>
<li>Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples.</li>
<li>Elsa and Anna have $5+7=12$ apples together.</li>
</ol>
<p>The answer is: 12 .
Table 9: The least-to-most prompt for solving GSM8K. The demonstration problem is solved in only two steps, but the prompt can handle problems that require multiple steps to be solved.</p>
<p>Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples. So Elsa and Anna have $5+7$ $=12$ apples together.</p>
<p>The answer is: 12 .
Table 10: The chain-of-thought prompt for solving GSM8K. It is derived from the least-to-most prompt in Table 9 by removing the decomposition part.</p>
<p>The prompt that we design to solve GSM8K is shown in Table 9. The demonstration exemplar consists of two parts. The first part (starting from "Let's break down this problem ...") shows how the original problem can be decomposed into simpler subproblems, and the the second part shows how the subproblems are solved in sequence. Note that this prompt combines decomposition and subproblem solving into a single pass. One may instead design two different prompts respectively for decomposition and subproblem solving, as the least-to-most prompts in the previous sections, to further improve performance. Here, we focus on investigating how this simple least-to-most prompt generalizes from a simple 2-step problem to more complex multi-step problems.
We also construct a chain-of-thought prompt (Table 10) as our baseline. It is derived from the least-to-most prompt (Table 9) by removing the decomposition part. The results are shown in Table 11. Overall, least-to-most prompting only slightly improves chain-of-thought prompting: from $60.97 \%$ to $62.39 \%$. However, least-to-most prompting essentially improves chain-of-thought prompting in solving problems which need at least 5 steps to be solved: from $39.07 \%$ to $45.23 \%$ (Table 12). We find that almost every problem in GSM8K that least-to-most prompting fails to solve can be eventually solved by using a manually crafted decomposition. This should not be surprising. For our humans, as long as we know how to decompose a complex problem into simpler subproblems, we actually have solved it. For the DROP benchmark, least-to-most prompting outperforms chain-of-thought prompting by a large margin (Table 11). That is probably because most problems in DROP can be trivially decomposed.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Non-football (DROP)</th>
<th style="text-align: center;">Football (DROP)</th>
<th style="text-align: center;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">43.86</td>
<td style="text-align: center;">51.77</td>
<td style="text-align: center;">16.38</td>
</tr>
<tr>
<td style="text-align: center;">Standard prompting</td>
<td style="text-align: center;">58.78</td>
<td style="text-align: center;">62.73</td>
<td style="text-align: center;">17.06</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">74.77</td>
<td style="text-align: center;">59.56</td>
<td style="text-align: center;">60.87</td>
</tr>
<tr>
<td style="text-align: center;">Least-to-Most</td>
<td style="text-align: center;">$\mathbf{8 2 . 4 5}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4 2}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 3 9}$</td>
</tr>
</tbody>
</table>
<p>Table 11: Accuracies (\%) of different prompting methods on GSM8K and DROP (only the subset containing numerical problems). The base language model is code-davinci-002.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy by Steps (GSM8K)</th>
<th style="text-align: center;">All</th>
<th style="text-align: center;">2 Steps</th>
<th style="text-align: center;">3 Steps</th>
<th style="text-align: center;">4 steps</th>
<th style="text-align: center;">$\geq 5$ steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Least-to-Most</td>
<td style="text-align: center;">$\mathbf{6 2 . 3 9}$</td>
<td style="text-align: center;">74.53</td>
<td style="text-align: center;">$\mathbf{6 8 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 7 3}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 2 3}$</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">60.87</td>
<td style="text-align: center;">$\mathbf{7 6 . 6 8}$</td>
<td style="text-align: center;">67.29</td>
<td style="text-align: center;">59.39</td>
<td style="text-align: center;">39.07</td>
</tr>
</tbody>
</table>
<p>Table 12: Accuracies (\%) of least-to-most prompting and chain-of-thought prompting, broken down by the number of reasoning steps required in the expected solution.</p>
<h1>4 Related Work</h1>
<p>Compositional generalization. SCAN (Lake \&amp; Baroni, 2018) is a widely used benchmark to evaluate compositional generalization. Among all of its splits, the most challenging is the length split, which requires a model to generalize to test sequences longer than training ones. Prior work with good performance on SCAN mostly proposed neural-symbolic architectures (Chen et al., 2020; Liu et al., 2020) and grammar induction techniques (Nye et al., 2020; Shaw et al., 2021; Kim, 2021). Chen et al. (2020) proposed the neural-symbolic stack machine, which contains a neural network as the controller to generate an execution trace for a given input, and a symbolic stack machine to execute the trace and produce the output. The execution trace consists of domain-specific primitives for sequence manipulation, which allows the machine to break down the input sentence into different components, translate them separately, and compose them together. Liu et al. (2020) proposed a framework that cooperatively learns two neural modules, a composer and a solver, to jointly learn the input structure and the symbolic grammar rules. Both Nye et al. (2020) and Shaw et al. (2021) inferred the symbolic grammar rules of SCAN, while Kim (2021) proposed to learn a latent neural grammar. While approaches with symbolic components are able to achieve $100 \%$ accuracy on SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021), they require complicated model training and grammar inference algorithms to search in a large grammar space. Another line of work on SCAN designs data augmentation schemes (Andreas, 2020; Akyürek et al., 2021; Lake, 2019). Both Andreas (2020) and Akyürek et al. (2021) construct synthetic training samples by recombining fragments occurring in different training samples, and Akyürek et al. (2021) further designs a sampling scheme that encourages the recombination model to produce rare samples. On the other hand, Lake (2019) proposed a meta training algorithm, which requires a meta-grammar space to construct training data, and the format of sampled grammars is similar to the SCAN grammar. While these data augmentation techniques improve the performance on several compositional generalization benchmarks, they fail to solve the length split of SCAN. Other prior works propose neural network architectures to improve compositional generalization, where they encourage the model to learn the word and span mapping (Russin et al., 2019; Li et al., 2019), the alignment of input and output as span trees (Herzig \&amp; Berant, 2021), and the permutation equivariance of input and output words (Gordon et al., 2020). Still, these end-to-end neural networks without symbolic components do not generalize to longer test inputs. Unlike the existing work, we demonstrate that without model architectures and symbolic components specially designed to improve compositional generalization, least-to-most prompting achieves $99.7 \%$ accuracy on any split (including length split) with only a handful of demonstration examples, and it does not require any training or finetuning.
Easy-to-hard generalization. In addition to compositional generalization, there are many other tasks where the test cases require more reasoning steps to solve than the training examples, for example, the last-letter-concatenation task where the test lists are longer than the demonstration examples. Dong et al. (2019) propose Neural Logic Machines (NLMs) for both inductive learning and logic reasoning. NLMs trained on small-scale tasks (such as small size block worlds) can perfectly generalize to large-scale tasks (such as larger size block worlds). Schwarzschild et al. (2021) show that recurrent networks trained to solve simple problems with few recurrent steps (such as small size mazes or chess puzzles) can solve more complex problems (such as larger size mazes or chess puzzles) by performing additional recurrences during inference. In our method, we achieve easy-to-hard generalization by decomposing a complex problem into a series of easier problems.
Task decomposition. Perez et al. (2020) decompose a multi-hop question into a number of independent single-hop subquestions, which are answered by an off-the-shelf question answering (QA) model. Then those answers are aggregated to form the final answer. Both question decomposition and answer aggregation are implemented by trained models. Wang et al. (2022a) conducts multi-hop QA by modeling prompts as continuous virtual tokens and progressively eliciting relevant knowl-</p>
<p>edge from language models via iterative prompting. Unlike these methods, our approach does not involve any training or finetuning. Moreover, the subquestions generated in least-to-most prompting are usually dependent and have to be sequentially solved in a specific order so that answers to some subquestions can be used as building blocks to solve other subquestions. Yang et al. (2022) translate natural language questions to SQL queries by decomposing a question into a sequence of slot-filling natural language prompts corresponding to SQL clauses via a rule-based system. Wu et al. (2022) propose chaining large language model steps such that the output of one step becomes the input for the next and develop an interactive system for users to construct and modify chains. Least-to-most prompting chains the processes of problem decomposition and subproblem solving.</p>
<h1>5 LIMITATIONS</h1>
<p>Decomposition prompts typically don't generalize well across different domains. For instance, a prompt that demonstrates decomposing math word problems (as seen in Table 9) isn't effective for teaching large language models to break down common sense reasoning problems, such as "Did Aristotle use a laptop?" (Geva et al., 2021). A new prompt must be designed to demonstrate decomposition for these types of problems in order to achieve optimal performance.</p>
<p>Generalizing decomposition can even be difficult within the same domain. We've observed that nearly all problems in GSM8K can be accurately solved if the large language models are provided with the correct decomposition of those challenging problems. This finding isn't surprising and aligns with our experiences in solving math problems. Whenever we successfully break down a math problem into simpler subproblems we can solve, we've essentially solved the original problem. Exceptional results are achieved on the last-letter-concatenation task and the SCAN benchmark because decomposition in these tasks is relatively straightforward.</p>
<h2>6 CONCLUSION AND DISCUSSION</h2>
<p>We introduced least-to-most prompting to enable language models to solve problems that are harder than those in the prompt. This approach entails a two-fold process: a top-down decomposition of the problem and a bottom-up resolution generation. Our empirical findings, which encompass symbolic manipulation, compositional generalization, and mathematical reasoning, reveal that least-to-most prompting significantly surpasses standard prompting and chain-of-thought prompting.
In general, prompting might not be the optimal method for teaching reasoning skills to large language models. Prompting can be viewed as a unidirectional communication form in which we instruct a language model without considering its feedback. A natural progression would be to evolve prompting into fully bidirectional conversations, enabling immediate feedback to language models, thereby facilitating more efficient and effective learning. The least-to-most prompting technique represents a stride towards instructing language models through such bidirectional interactions.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>We sincerely thank Xinyun Chen, Xinying Song, Jeff Dean, Zoubin Ghahramani, Fernando Pereira, Jacob Devlin, and Pete Shaw for sharing their valuable knowledge and advice during our discussions. Their expertise greatly improved the quality of our work. Additionally, we are grateful to the anonymous reviewers for their careful review and helpful suggestions, which helped shape our manuscript into its final form.</p>
<h2>REFERENCES</h2>
<p>Ekin Akyürek, Afra Feyza Akyürek, and Jacob Andreas. Learning to recombine and resample data for compositional generalization. In International Conference on Learning Representations, 2021.</p>
<p>Jacob Andreas. Good-enough compositional data augmentation. In Annual Meeting of the Association for Computational Linguistics, 2020.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization via neural-symbolic stack machines. Advances in Neural Information Processing Systems, 33:1690-1701, 2020.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In International Conference on Learning Representations, 2019.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.</p>
<p>Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations, 2020.</p>
<p>Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Annual Meeting of the Association for Computational Linguistics, 2021.</p>
<p>Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generalization: A comprehensive method on realistic data. International Conference on Learning Representations, 2020.</p>
<p>Yoon Kim. Sequence-to-sequence learning with latent neural grammars. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pp. 2873-2882. PMLR, 2018.</p>
<p>Brenden M Lake. Compositional generalization through meta sequence-to-sequence learning. Advances in neural information processing systems, 32, 2019.</p>
<p>Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. Compositional generalization for primitive substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4284-4293, 2019.</p>
<p>Myrna E Libby, Julie S Weiss, Stacie Bancroft, and William H Ahearn. A comparison of most-toleast and least-to-most prompting on the acquisition of solitary play skills. Behavior analysis in practice, 1(1):37-43, 2008.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017.</p>
<p>Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. Advances in Neural Information Processing Systems, 33:11416-11427, 2020.</p>
<p>Maxwell Nye, Armando Solar-Lezama, Josh Tenenbaum, and Brenden M Lake. Learning compositional rules via neural program synthesis. Advances in Neural Information Processing Systems, 33:10832-10842, 2020.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8864-8880, 2020.</p>
<p>Jake Russin, Jason Jo, Randall C O'Reilly, and Yoshua Bengio. Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708, 2019.</p>
<p>Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 922-938, 2021.</p>
<p>Boshi Wang, Xiang Deng, and Huan Sun. Shepherd pre-trained language models to develop a train of thought: An iterative prompting approach. arXiv preprint arXiv:2203.08383, 2022a.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35, 2022.</p>
<p>Tongshuang Wu, Michael Terry, and Carrie Jun Cai. AI chains: Transparent and controllable humanAI interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pp. 1-22, 2022.</p>
<p>Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. Seqzero: Few-shot compositional semantic parsing with sequential prompts and zero-shot models. arXiv preprint arXiv:2205.07381, 2022.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>7 Last-letter-concatenation ..... 14
7.1 Prompt context for decomposing a word list into subproblems ..... 14
7.2 Prompt contexts with more and different examples ..... 14
7.2.1 Standard prompting, 4 -shot ..... 14
7.2.2 Chain-of-thought prompting, 4 -shot ..... 14
7.2.3 Chain-of-thought prompting, 8 -shot ..... 15
7.2.4 Chain-of-thought prompting, 2-shot, same examples as for least-to-most ..... 15
7.2.5 Least-to-most prompting, 4-shot ..... 15
7.3 Data Generation and additional results ..... 16
7.4 Error analysis: Least-to-most prompting ..... 17
7.5 Example outputs from code-davinci-002 ..... 18
7.5.1 Standard prompting: Failure ..... 18
7.5.2 Chain-of-thought prompting: Success ..... 19
7.5.3 Chain-of-thought prompting: Failure ..... 21
7.5.4 Least-to-most prompting: Success ..... 22
7.5.5 Least-to-most prompting: Failure ..... 25
8 SCAN ..... 28
8.1 Prompt contexts ..... 28
8.1.1 Standard prompting ..... 29
8.1.2 Least-to-most prompting ..... 29
8.1.3 Chain-of-thought prompting ..... 31
8.2 Error analysis: Least-to-most prompting ..... 31
8.3 Example outputs from code-davinci-002 ..... 33
8.3.1 Chain-of-thought prompting: Success ..... 33
8.3.2 Chain-of-thought prompting: Failure ..... 35
8.3.3 Least-to-most prompting: Success ..... 37
8.3.4 Least-to-most prompting: Failure ..... 40
8.4 Expanding Python expressions using prompting ..... 45
9 DROP ..... 46
9.1 Results with text-davinci-002 and LM-540B ..... 46
9.2 Non-football Subset ..... 46
9.2.1 Zero-shot prompting ..... 46
9.2.2 Standard prompting with 3 examples ..... 47
9.2.3 Chain-of-thought prompting with 3 examples ..... 47
9.2.4 Least-to-most prompting I: problem decomposition (5 examples) ..... 48
9.2.5 Least-to-most prompting II: problem solving (3 examples) ..... 48
9.3 Football subset ..... 49
9.3.1 Zero-shot prompting ..... 49
9.3.2 Standard prompting with 3 examples ..... 49
9.3.3 Chain-of-thought prompting with 3 examples ..... 49
9.3.4 Least-to-most prompting I: problem decomposition (6 examples) ..... 50
9.3.5 Least-to-most prompting II: problem solving (3 examples) ..... 51
9.4 Examples where least-to-most succeeded but chain-of-thought failed ..... 52
9.4.1 Case 1 ..... 52
9.4.2 Case 2 ..... 52
9.4.3 Case 3 ..... 53</p>
<p>9.4.4 Case 4 ..... 54
9.4.5 Case 5 ..... 54
9.5 Error analysis: Least-to-most prompting ..... 54
9.5.1 Example of wrong problem decomposition ..... 55
9.5.2 Example of wrong problem solving ..... 55
9.5.3 Example of wrong given label ..... 55
10 GSM8K ..... 56
10.1 Experiment results: One-shot prompts ..... 56
10.2 Experiment results: Engineered prompts ..... 56
10.3 Prompt contexts: One-shot prompts ..... 57
10.3.1 Chain-of-Thought (1-shot) ..... 58
10.3.2 Least-to-Most (1-shot) ..... 58
10.4 Prompt contexts: Engineered prompts ..... 58
10.4.1 Zero-Shot ..... 58
10.4.2 Standard prompting: 4 examples ..... 58
10.4.3 Chain-of-Thought (best): 4 examples ..... 59
10.4.4 Least-to-Most (best) I - problem decomposition: 7 examples ..... 59
10.4.5 Least-to-Most (best) II - problem solving: 4 examples ..... 60</p>
<h1>7 LAST-LETTER-CONCATENATION</h1>
<h3>7.1 PROMPT CONTEXT FOR DECOMPOSING A WORD LIST INTO SUBPROBLEMS</h3>
<p>In Section 3.1 we mentioned that language model prompting can be used to decompose a word list such as "think, machine, learning, reasoning" into a sequence of subproblems "think, machine", "think, machine, learning", and "think, machine, learning, reasoning".</p>
<p>The following prompt context achieves $100 \%$ accuracy on this task when using the text-davinci-002 model. Note that it achieves perfect accuracy on lists up to size 12 (which is the maximum that we tested for our experiment) even though it only contains one exemplar each for lists of sizes 2 and 3 .</p>
<p>Q: "machine, learning"
A: creating sequential sublists of the list "machine, learning":
"machine"
"machine, learning"
Q: "machine, learning, artificial"
A: creating sequential sublists of the list "machine, learning, artificial":
"machine"
"machine, learning"
"machine, learning, artificial"</p>
<h3>7.2 PROMPT CONTEXTS WITH MORE AND DIFFERENT EXAMPLES</h3>
<p>The last-letter-concatenation experiments presented in Section 3.1 are based on prompt contexts that consists of 2 demonstration examples. To make sure that the accuracy gain achieved by least-to-most prompting is not caused by the slight increase in example length when compared to chain-of-thought, we also performed experiments with more context examples so that we can compare least-to-most vs. chain-of-thought for different prompt sizes. Also, we perform experiments where we use for chain-of-thought prompting the same prompt examples that we use for least-to-most prompting (unlike the situation in Table ?? where we use different examples). All these prompts are shown below, and we present and discuss the corresponding accuracies in Section 7.3.</p>
<h3>7.2.1 StANDARD PROMPTING, 4-SHOT</h3>
<p>Q: "think, machine"
A: "ke"
Q: "learning, reasoning, generalization"
A: "ggn"
Q: "artificial, intelligence"
A: "le"
Q: "transformer, language, vision"
A: "ren"</p>
<h3>7.2.2 CHAIN-OF-THOUGHT PROMPTING, 4-SHOT</h3>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "learning, reasoning, generalization"
A: The last letter of "learning" is "g". The last letter of "reasoning" is "g". The last letter of "generalization" is " $n$ ". Concatenating " $g$ ", " $g$ ", " $n$ " leads to "ggn". So, "learning, reasoning, generalization" outputs "ggn".</p>
<p>Q: "artificial, intelligence"
A: The last letter of "artificial" is "l". The last letter of "intelligence" is "e". Concatenating "l", "e" leads to "le". So, "artificial, intelligence" outputs "le".</p>
<p>Q: "transformer, language, vision"
A: The last letter of "transformer" is "r". The last letter of "language" is "e". The last letter of "vision" is " $n$ ". Concatenating " $r$ ", " $e$ ", " $n$ " leads to "ren". So, "transformer, language, vision" outputs "ren".</p>
<h1>7.2.3 CHAIN-OF-THOUGHT PROMPTING, 8-SHOT</h1>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "learning, reasoning, generalization"
A: The last letter of "learning" is " $g$ ". The last letter of "reasoning" is " $g$ ". The last letter of "generalization" is " $n$ ". Concatenating " $g$ ", " $g$ ", " $n$ " leads to " $g g n$ ". So, "learning, reasoning, generalization" outputs "ggn".</p>
<p>Q: "artificial, intelligence"
A: The last letter of "artificial" is "l". The last letter of "intelligence" is "e". Concatenating "l", "e" leads to "le". So, "artificial, intelligence" outputs "le".</p>
<p>Q: "transformer, language, vision"
A: The last letter of "transformer" is "r". The last letter of "language" is "e". The last letter of "vision" is " $n$ ". Concatenating " $r$ ", "e", " $n$ " leads to "ren". So, "transformer, language, vision" outputs "ren".</p>
<p>Q: "school, teacher"
A: The last letter of "school" is "l". The last letter of "teacher" is "r". Concatenating "l", "r" leads to "lr". So, "school, teacher" outputs "lr".</p>
<p>Q: "art, science, department"
A: The last letter of "art" is "t". The last letter of "science" is "e". The last letter of "department" is "t". Concatenating "t", "e", "t" leads to "tet". So, "art, science, department" outputs "tet".</p>
<p>Q: "pen, laptop"
A: The last letter of "pen" is " $n$ ". The last letter of "laptop" is " $p$ ". Concatenating: " $n$ ", " $p$ " leads to " $n p$ ". So, "pen, laptop" outputs "np".</p>
<p>Q: "book, campus, university"
A: The last letter of "book" is " $k$ ". The last letter of "campus" is " $s$ ". The last letter of "university" is " $y$ ". Concatenating: " $k$ ", " $s$ ", " $y$ " leads to "ksy". So, "book, campus, university" outputs "ksy".</p>
<h3>7.2.4 CHAIN-OF-THOUGHT PROMPTING, 2-SHOT, SAME EXAMPLES AS FOR LEAST-TO-MOST</h3>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to "ke". So, "think, machine" outputs "ke".</p>
<p>Q: "think, machine, learning" A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". The last letter of "learning" is " $g$ ". Concatenating " $k$ ", "e", " $g$ " leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<h3>7.2.5 LeAST-TO-MOST PROMPTING, 4-SHOT</h3>
<p>Q: "think, machine"
A: The last letter of "think" is " $k$ ". The last letter of "machine" is "e". Concatenating " $k$ ", "e" leads to</p>
<p>"ke". So, "think, machine" outputs "ke".
Q: "think, machine, learning"
A: "think, machine" outputs "ke". The last letter of "learning" is "g". Concatenating "ke", "g" leads to "keg". So, "think, machine, learning" outputs "keg".</p>
<p>Q: "transformer, language"
A: The last letter of "transformer" is "r". The last letter of "language" is "e". Concatenating: "r", "e" leads to "re". So, "transformer, language" outputs "re".</p>
<p>Q: "transformer, language, vision"
A: "transformer, language" outputs "re". The last letter of "vision" is "n". Concatenating: "re", "n" leads to "ren". So, "transformer, language, vision" outputs "ren".</p>
<h1>7.3 Data Generation and additional ReSults</h1>
<p>Data generation. The last-letter-concatenation dataset is based on a list of the 10k most common English words (including proper nouns) used in books that are part of project Gutenberg, as collected in Wiktionary ${ }^{2}$. After eliminating profane words, we ended up with a list of 9694 words (all lowercase). For each of the desired list sizes $2,4,6,8,10,12$, we then generated 500 examples, each of which consists of a random sequence of these words (input) and the corresponding sequence of last letters (output). We will release the full dataset upon publication of this paper. Below are 10 random examples of list size 6:</p>
<ul>
<li>IN: "narrative, celebrate, neighbouring, indebted, stove, calling" OUT: "eegdeg"</li>
<li>IN: "barley, silk, thankful, kiss, logs, silent" OUT: "yklsst"</li>
<li>IN: "knitting, conveyance, receives, represent, cow, shut" OUT: "gestwt"</li>
<li>IN: "olive, dark, limitation, airy, pocket, wondered" OUT: "eknytd"</li>
<li>IN: "apprehensive, exclamation, perspiration, trusting, destiny, tactics" OUT: "enngys"</li>
<li>IN: "qualified, envoy, disciple, exert, witnesses, plane" OUT: "dyetse"</li>
<li>IN: "decidedly, dome, france, chris, knowing, peaceful" OUT: "yeesgl"</li>
<li>IN: "deceit, refinement, tips, cord, princes, discovery" OUT: "ttsdsy"</li>
<li>IN: "drops, paste, defective, bohemia, requested, convenient" OUT: "seeadt"</li>
<li>IN: "diverse, christopher, homely, agreeable, fright, suspended" OUT: "eryetd"</li>
</ul>
<p>Complete results. Table 13 summarizes all the experiments we performed for the last-letterconcatenation task. In addition to the experiments where prompt contexts contain 2 demonstration examples presented in Section 3.1, this includes experiments where the prompts contain 4 and 8 demonstration examples (see above).
While more prompt examples have no effect for standard prompting (the accuracy remains at 0 ), they increase the accuracy across the board for chain-of-thought and least-to-most prompting. However, least-to-most prompting consistently outperforms chain-of-thought prompting. In fact, even if we compare 2-shot least-to-most (prompt size 123 GPT3 tokens) to 8 -shot chain-of-thought (prompt size 573 GPT3 tokens), the accuracy for least-to-most prompting is much higher than for chain-of-thought prompting. The difference is especially pronounced for long sequences (e.g., for $\mathrm{L}=$ 12, we have least-to-most at $74.0 \%$ vs. chain-of-thought at $38.4 \%$ ). This shows that least-to-most prompting is much more data-efficient than chain-of-thought prompting for this problem.
Comparing the first two rows for chain-of-thought prompting shows that chain-of-thought achieves higher accuracy if we use two independent examples (see prompt in Table ??) instead of the two dependent examples that we use for least-to-most prompting. This demonstrates that the accuracy advantage of least-to-most prompting over chain-of-thought prompting remains even if the use the same examples for both of them.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting method</th>
<th style="text-align: center;"># Examples</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{4}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{6}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{8}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{1 0}$</th>
<th style="text-align: center;">$\mathbf{L}=\mathbf{1 2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">Any</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Chain-of-Thought</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2 (L2M)</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">31.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">text-002*</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-001</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Least-to-Most</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-002</td>
<td style="text-align: center;">$\mathbf{9 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">text-002*</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">code-001</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 13: Accuracy of different prompting methods, prompt sizes, and GPT3 models on the last-letter-concatenation task with the length of lists increasing from 4 to 12 . We use code-002 to denote the model code-davinci-002, text-002 to denote the model text-davinci-002, and code-001 to denote the model code-davinci-001. The results in the second row for chain-of-thought prompting correspond to the experiment where we use for chain-of-thought the same prompt examples that we use for least-to-most. The results of text-davinci-002 are based on a subset of 100 random examples (rather than the full set of 500 exammples).</p>
<p>The table also contains the results from running against two additional GPT-3 models: text-davinci-002 and codex-davinci-001. While text-davinci-002 shows similar accuracy to code-davinci-002 on small list sizes, the accuracy drops off much faster when moving to larger list sizes, both for chain-of-thought prompting as well as for least-to-most prompting. This indicates that the code-davinci-002 model has an advantage when it comes to dealing with iteration and recursion.</p>
<p>The code-davinci-001 model performs much worse than code-davinci-002 across all dimensions. Even for the shortest list size $(\mathrm{L}=4)$, the accuracy for least-to-most prompting is only $19.6 \%$ compared to $96 \%$ for code-davinci-002. This indicates that there is a large potential for improvement when using the exact same configuration with new model generations.</p>
<h1>7.4 ERROR ANALYSIS: LEAST-TO-MOST PROMPTING</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Error type</th>
<th style="text-align: center;">2 examples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">4 examples</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{4}$</td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{1 2}$</td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{4}$</td>
<td style="text-align: center;">$\mathbf{L}=\mathbf{1 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Concatenation error</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">- Dropping a letter</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">- Adding a letter</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">- Wrong order</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Wrong template</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect last letter</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Copy error</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 14: Least-to-most prompting error analysis of 20 random failures of the code-davinci-002 model on list lengths 4 and 12 for prompt contexts consisting of 2 and 4 examples. Note that for some examples, the model made more than one type of error (e.g., dropping and adding a letter during concatenation).</p>
<p>For least-to-most prompting, we analyzed 20 random failures of the code-davinci-002 model on list lengths 4 and 12 for prompt contexts consisting of 2 and 4 examples. The results are shown in Table 14. Concatenation errors may either be due to dropping a letter, adding a letter or outputting the letters in the wrong order. Wrong template means that the model used the extension template instead of the base template to concatenate the last letter of the first two words of the list. Incorrect last letter means that the model got the last letter of a word wrong, and copy error means that the error was due to making a mistake when copying an intermediate result.</p>
<p>We observe that for the prompt consisting of 2 examples, the fraction of concatenation errors increases as we go from length 4 to length 12 while the fraction of wrong template errors go down. This makes sense because the number of concatenations grows with the length of the list, while the number of times the model needs to use the base template stays constant. Note that the template errors disappear when we move to the double prompt, which means that adding two more examples helps the model recognize which template to use. As a consequence, the double prompt has a similar distribution of errors for both list lengths.</p>
<p>Examples of concatenation errors. In the example "gratified, contract, fortitude, blew", the model drops the last letter in the concatenation of "dte" and "w", which means that it predicts the last letter sequence to be "dte" instead of "dtew".</p>
<p>In the example "hollow, supplies, function, gorgeous", the model duplicates the last letter "s" in the concatenation of "wsn" and "s", which means that it predicts the last letter sequence "wsnss" instead of "wsns".</p>
<p>In the example "madly, vengeance, cowardice, monk", the model drops the last letter " k " in the concatenation of "yee" and "k" and instead adds the letter "g". Consequently, the model predicts "yeeg" instead of "yeek".</p>
<p>In the example "slender, lash, throng, scheme", the model breaks the order of the letters "h" and "g" in the concatenation of "rh" and "g", which means that it predicts the last letter sequence "rghe" instead of "rhge".</p>
<p>Example of incorrect last letter. In the example "modification, introducing, schools, lunch", the model determines the last letter of the word "modification" to be "g". Consequently, the predicted last letter sequence is "ggsh" instead of "ngsh".</p>
<p>Example of wrong template application. In the example "upper, unexpectedly, specifically, connection", the model uses the extension template to determine the output of the first two words "upper, unexpectedly". I.e., it produces:</p>
<ul>
<li>"upper" outputs "er". The last letter of "unexpectedly" is "y". Concatenating "er", "y" leads to "ery". So, "upper, unexpectedly" outputs "ery".
when it should have produced:</li>
<li>The last letter of "upper" is "r". The last letter of "unexpectedly" is "y". Concatenating "r", "y" leads to "ry".</li>
</ul>
<p>As a consequence, the model predicts the sequence "eryyn" instead of "ryyn".
Example of copy error. In the example "displeased, hawk, healing, anchor", the model correctly computes the final concatenation "dkgr" but then makes a mistake when copying the result to the final output and drops the final "r", which leads to the prediction "dkg" instead of "dkgr":</p>
<ul>
<li>"displeased, hawk, healing" outputs "dkg". The last letter of "anchor" is "r". Concatenating: "dkg", "r" leads to "dkgr". So, "displeased, hawk, healing, anchor" outputs "dkg".</li>
</ul>
<h1>7.5 EXAMPLE OUTPUTS FROM CODE-DAVINCI-002</h1>
<p>In this section, we present 5 examples (one example for lengths $4,6,8,10,12$ ) of successes and failures of the code-davinci-002 model for standard prompting, chain-of-thought prompting, and least-to-most prompting. They are all based on the prompt contexts containing 4 demonstration examples.</p>
<h3>7.5.1 Standard Prompting: Failure</h3>
<p>"witnesses, arkansas, remaining, privileged"</p>
<h2>Golden:</h2>
<p>"ssgd"</p>
<h1>Solution:</h1>
<p>Q: "witnesses, arkansas, remaining, privileged"
A: " $n$ "
"clan, noun, roy, nicholas, relish, liking"
Golden:
"nnyshg"</p>
<h2>Solution:</h2>
<p>Q: "clan, noun, roy, nicholas, relish, liking"
A: "nicholas"
"franklin, temporarily, high, origin, rapidly, diversion, firmament, promptly"</p>
<h2>Golden:</h2>
<p>"nyhnynty"</p>
<h2>Solution:</h2>
<p>Q: "franklin, temporarily, high, origin, rapidly, diversion, firmament, promptly"
A: " $n$ "
"conquered, owed, enclosure, indeed, shattered, beads, increased, provided, weak, grind"
Golden:
"ddeddsddkd"</p>
<h2>Solution:</h2>
<p>Q: "conquered, owed, enclosure, indeed, shattered, beads, increased, provided, weak, grind"
A: "d"
"hope, alison, mirrors, benefits, altered, curve, keen, portuguese, distance, interfaces, proof, seamus"</p>
<h2>Golden:</h2>
<p>"enssdeneesfs"</p>
<h2>Solution:</h2>
<p>Q: "hope, alison, mirrors, benefits, altered, curve, keen, portuguese, distance, interfaces, proof, seamus"
A: "hope"</p>
<h3>7.5.2 CHAIN-OF-THOUGHT PROMPTING: SUCCESS</h3>
<p>"geese, rogers, benefactor, version"</p>
<h2>Golden:</h2>
<p>"esrn"</p>
<h2>Solution:</h2>
<p>Q: "geese, rogers, benefactor, version"
A: The last letter of "geese" is "e". The last letter of "rogers" is "s". The last letter of "benefactor" is "r". The last letter of "version" is " $n$ ". Concatenating "e", "s", "r", "n" leads to "esrn". So, "geese, rogers,</p>
<p>benefactor, version" outputs "esrn".</p>
<h1>"amply, bestow, maryland, edited, blowing, prosperous"</h1>
<h2>Golden:</h2>
<p>"ywddgs"</p>
<h2>Solution:</h2>
<p>Q: "amply, bestow, maryland, edited, blowing, prosperous"
A: The last letter of "amply" is "y". The last letter of "bestow" is "w". The last letter of "maryland" is "d". The last letter of "edited" is "d". The last letter of "blowing" is "g". The last letter of "prosperous" is "s". Concatenating "y", "w", "d", "d", "g", "s" leads to "ywddgs". So, "amply, bestow, maryland, edited, blowing, prosperous" outputs "ywddgs".</p>
<h2>"needless, courts, reflected, curly, glowing, asp, chronicles, anthony"</h2>
<h2>Golden:</h2>
<p>"ssdygpsy"</p>
<h2>Solution:</h2>
<p>Q: "needless, courts, reflected, curly, glowing, asp, chronicles, anthony"
A: The last letter of "needless" is "s". The last letter of "courts" is "s". The last letter of "reflected" is "d". The last letter of "curly" is "y". The last letter of "glowing" is "g". The last letter of "asp" is "p". The last letter of "chronicles" is "s". The last letter of "anthony" is "y". Concatenating "s", "s", "d", "y", "g", "p", "s", "y" leads to "ssdygpsy". So, "needless, courts, reflected, curly, glowing, asp, chronicles, anthony" outputs "ssdygpsy".</p>
<h2>"nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour"</h2>
<h2>Golden:</h2>
<p>"latlrlnedr"</p>
<h2>Solution:</h2>
<p>Q: "nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour"
A: The last letter of "nigel" is "l". The last letter of "alaska" is "a". The last letter of "rent" is "t". The last letter of "hospital" is "l". The last letter of "newspaper" is "r". The last letter of "medical" is "l". The last letter of "nation" is " $n$ ". The last letter of "dave" is " $e$ ". The last letter of "associated" is " $d$ ". The last letter of "rumour" is "r". Concatenating "l", "a", "t", "l", "r", "l", "n", "e", "d", "r" leads to "latlrlnedr". So, "nigel, alaska, rent, hospital, newspaper, medical, nation, dave, associated, rumour" outputs "latlrlnedr".</p>
<h2>"brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert"</h2>
<h2>Golden:</h2>
<p>"syedysspnytt"</p>
<h2>Solution:</h2>
<p>Q: "brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert"
A: The last letter of "brightness" is "s". The last letter of "preparatory" is "y". The last letter of "intense" is "e". The last letter of "accumulated" is "d". The last letter of "reluctantly" is "y". The last letter of "nets" is "s". The last letter of "inquiries" is "s". The last letter of "grip" is "p". The last letter of "inquisition" is " $n$ ". The last letter of "alley" is " $y$ ". The last letter of "innocent" is " $t$ ". The last letter of</p>
<p>"herbert" is "t". Concatenating "s", "y", "e", "d", "y", "s", "s", "p", "n", "y", "t", "t" leads to "syedysspnytt". So, "brightness, preparatory, intense, accumulated, reluctantly, nets, inquiries, grip, inquisition, alley, innocent, herbert" outputs "syedysspnytt".</p>
<h1>7.5.3 Chain-of-thought prompting: Failure</h1>
<p>"pattern, vote, afterwards, solemn"</p>
<h2>Golden:</h2>
<p>"nesn"</p>
<h2>Solution:</h2>
<p>Q: "pattern, vote, afterwards, solemn"
A: The last letter of "pattern" is " $n$ ". The last letter of "vote" is "e". The last letter of "afterwards" is "s". The last letter of "solemn" is " $n$ ". Concatenating " $n$ ", " $e$ ", " $s$ ", " $n$ " leads to "ness". So, "pattern, vote, afterwards, solemn" outputs "ness".</p>
<h2>"disappointment, troublesome, respectfully, monstrous, worry, politics"</h2>
<h2>Golden:</h2>
<p>"teysys"</p>
<h2>Solution:</h2>
<p>Q: "disappointment, troublesome, respectfully, monstrous, worry, politics"
A: The last letter of "disappointment" is " $t$ ". The last letter of "troublesome" is " $e$ ". The last letter of "respectfully" is " $y$ ". The last letter of "monstrous" is " $s$ ". The last letter of "worry" is " $y$ ". The last letter of "politics" is "s". Concatenating "t", "e", "y", "s", "y", "s" leads to "teyssy". So, "disappointment, troublesome, respectfully, monstrous, worry, politics" outputs "teyssy".</p>
<h2>"drawing, changes, purified, basic, others, send, chimney, vice"</h2>
<h2>Golden:</h2>
<p>"gsdcsdye"</p>
<h2>Solution:</h2>
<p>Q: "drawing, changes, purified, basic, others, send, chimney, vice"
A: The last letter of "drawing" is " $g$ ". The last letter of "changes" is " $s$ ". The last letter of "purified" is "d". The last letter of "basic" is "c". The last letter of "others" is "s". The last letter of "send" is "d". The last letter of "chimney" is " $y$ ". The last letter of "vice" is "e". Concatenating "g", "s", "d", "c", "s", "d", "y", "e" leads to "gsdcsyde". So, "drawing, changes, purified, basic, others, send, chimney, vice" outputs "gsdcsyde".</p>
<h2>"sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch"</h2>
<h2>Golden:</h2>
<p>"eyhtnsylgh"</p>
<h2>Solution:</h2>
<p>Q: "sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch"
A: The last letter of "sensitive" is "e". The last letter of "pray" is "y". The last letter of "wreath" is "h". The last letter of "betwixt" is "t". The last letter of "golden" is " $n$ ". The last letter of "regions" is "s". The last letter of "loyalty" is " $y$ ". The last letter of "interval" is " 1 ". The last letter of "hunting" is " $g$ ". The last letter of "ranch" is " $h$ ". Concatenating "e", " $y$ ", " $h$ ", " $t$ ", " $n$ ", " $s$ ", " $y$ ", " $l$ ", " $g$ ", " $h$ " leads to "eyhtsynslgh". So, "sensitive, pray, wreath, betwixt, golden, regions, loyalty, interval, hunting, ranch" outputs "eyhtsynslgh".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/ $1-10000$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-385 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-385</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-385</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-273186345</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.03878v2.pdf" target="_blank">SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Integrating the 3D world into large language models (3D-based LLMs) has been a promising research direction for 3D scene understanding. However, current 3D-based LLMs fall short in situated understanding due to two key limitations: 1) existing 3D datasets are constructed from a global perspective of the 3D scenes and lack situated context. 2) the architectures of existing 3D-based LLMs lack explicit alignment between the spatial representations of 3D scenes and natural language, limiting their performance in tasks requiring precise spatial reasoning. We address these issues by introducing a scalable situated 3D dataset, named Spartun3D, that incorporates various situated spatial reasoning tasks. Furthermore, we propose Spartun3D-LLM, built on an existing 3D-based LLM but integrated with a novel situated spatial alignment module, aiming to enhance the alignment between 3D visual representations and their corresponding textual descriptions. Experimental results demonstrate that both our proposed dataset and alignment module significantly enhance the situated spatial understanding of 3D-based LLMs.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e385.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e385.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spartun3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spartun3D (Situated 3D Dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, LLM-generated situated 3D dataset (~133k examples) that encodes an agent's standing point and orientation and provides situated scene graphs, situated captioning, and situated QA (including multi-step planning) to teach LLMs situated spatial, procedural, and object-relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (data-generation LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o was prompted (Cord-prompt and Spa-prompt) to synthesize situations, situated scene graphs, situated captions, and QA pairs from JSON scene graphs derived from 3RScan; Spa-prompt (qualitative spatial relations like distance/direction) produced higher-quality situated outputs than raw coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spartun3D dataset construction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Automated pipeline: (1) create diverse agent situations (standing point + orientation), (2) compute situated scene graphs with rotation angles, direction bins, distances, and passby objects, (3) prompt GPT-4o to produce situated captions and QA (object attributes & relations, affordances, and 2-hop situated planning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dataset generation / situated captioning and QA (supports navigation/planning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM-generation (GPT-4o) from 3D scene graphs + human-annotated 3RScan object labels</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>LLM prompting (Cord-prompt and Spa-prompt), scripted scene-graph generation from 3D coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Situated scene graphs (JSON) encoding agent standing point/orientation, per-object distances, direction bins (front/right/back/left), rotation angles, passby objects, and object attributes; textual situational templates used to create descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human validation scores (language naturalness and spatial fidelity), dataset scale and diversity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Collected ~10k situated captions and ~123k QA pairs; human validation showed Spa-prompt produced substantially more high-quality examples (higher % with score ≥4) than Cord-prompt; overall human scores vary by task (captioning & planning lower due to complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Spa-prompt (qualitative spatial relations) enabled GPT-4o to produce coherent situated textual data; inclusion of passby objects and direction/distance bins allowed generation of planning and affordance QAs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Cord-prompt (raw coordinates) produced many spatial errors — LLMs struggle to reason from raw 3D coordinates; some semantic/common-sense violations and occlusion/misalignment errors (especially in multi-step planning answers).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared prompting strategies: Spa-prompt > Cord-prompt (higher human quality); no prior large-scale automated situated 3D dataset at this scale reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Prompt ablation: Spa-prompt improved quality metrics vs Cord-prompt; sampling fewer situations for planning/captioning due to token cost; human-eval identified majority of errors in planning task generation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Constructing qualitative, situated scene graphs (directions, passby, distances) and prompting an LLM (GPT-4o) with these qualitative relations produces scalable, high-quality situated datasets; raw coordinates are poorly interpreted by LLMs, so higher-level spatial descriptors are essential for LLM-mediated dataset generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e385.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spartun3D-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spartun3D-LLM (3D-based LLM with Situated Spatial Alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D-LLM built on LEO that integrates an explicit Situated Spatial Alignment module which aligns object-centric 3D visual representations with situated textual descriptions (via a text encoder like CLIP) using an MSE alignment loss and spatial self-attention with pairwise spatial features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spartun3D-LLM (built on LEO backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses object-centric PointNet++ encodings for each segmented object, spatial self-attention that injects pairwise spatial features f_ij = [d_ij, sin(theta_h), cos(theta_h), sin(theta_v), cos(theta_v)], and an alignment loss (MSE) between learned object representations and text embeddings from a pre-trained text encoder (CLIP preferred). Fine-tuned via LoRA on Spartun3D; optionally initialized from instruction-tuned LEO weights.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Situated captioning, Situated QA (attributes/relations, affordance, 2-hop planning), and zero-shot MP3D ObjNav evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate textual answers/captions or action tokens from combined inputs <C,S,Q> where C is 3D scene context (object point clouds), S is situation (text + standing point/orientation during training), Q is question. For navigation, produce stepwise navigation actions (turn left/right, move forward, stop).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation, multi-step planning, situated QA, captioning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on Spartun3D (LLM + 3D object encoders) and alignment to pre-trained text encoder embeddings (CLIP); optionally initialized from pre-trained LEO weights</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning (LoRA) on Spartun3D; zero-shot evaluation on external datasets (SQA3D, MP3D ObjNav); generation (autoregressive) conditioned on text + object tokens; alignment loss jointly optimized with language modeling loss.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Object-centric 3D embeddings (PointNet++), spatial-aware object representations O' via spatial self-attention augmented by pairwise spatial matrix F, situated textual embeddings W (CLIP), and MSE alignment (L_align) between O' and W; situations represented as textual templates and/or standing point/orientation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>generation metrics for captioning (CIDEr, BLEU-4, METEOR, ROUGE-L, sentence similarity), exact-match (EM) for QA, and stepwise action accuracy for navigation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Spartun3D-LLM outperforms LEO+Spartun3D by ~2–3% across many captioning/QA metrics; zero-shot navigation accuracy: Spartun3D-LLM 20.3% vs LEO 0% (Table 5). Alignment yields modest gains on Scan2Cap sentence-similarity 54.2%→55.7% and ScanQA 46.3%→48.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved fine-grained, context-aware spatial descriptions (e.g., 'turn slightly right' vs generic 'turn around'), better object attribute and relation QA zero-shot generalization, reduction of directional bias (LEO favoured 'left' 97% — bias reduced with Spartun3D training and further with alignment), and transferability to generate navigation actions without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Remaining errors in multi-step (2-hop) situated planning (inaccuracies in the second action), occasional semantic/common-sense violations from generated dataset artifacts, and vulnerabilities when viewpoints are occluded or when textual descriptions mismatch visual layout.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to LEO and LEO+Spartun3D: Spartun3D-LLM shows consistent +2–3% improvements across tasks; navigation: LEO baseline ~0% vs Spartun3D-LLM 20.3% (zero-shot, stepwise action accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing the situated spatial alignment module (i.e., L_align) reduces fine-grained spatial detail and lowers metrics by ~2–3%; initializing from instruction-tuned LEO weights improves absolute performance vs training from scratch; CLIP as text encoder outperforms BERT/BLIP for the alignment objective (CLIP EM 56.9% vs BLIP 54.2% vs BERT 53.5% in ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit alignment between object-level 3D embeddings and situated textual descriptions, combined with spatial self-attention that injects pairwise geometric features, enables LLMs to better encode and utilize spatial, procedural, and object-relational knowledge — improving situated reasoning, fine-grained spatial language generation, and zero-shot transfer to navigation tasks. LLMs alone poorly interpret raw coordinates; representing spatial relations qualitatively (directions, passby, distances) and aligning visual and textual modalities is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e385.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Situated Spatial Alignment Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Situated Spatial Alignment Module (object-text alignment with spatial self-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module that generates per-object situated textual descriptions and enforces alignment between spatial-aware object visual representations and textual embeddings (from CLIP) using an MSE loss, with spatial self-attention injecting pairwise geometric features into object attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spartun3D-LLM (module integrated into LEO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Creates textual templates per object describing name, attributes, and directional neighbors (up to five per direction). Computes text embeddings W via a pre-trained text encoder (CLIP) and minimizes MSE between W and spatial-aware object representations O' computed by spatial self-attention that receives pairwise spatial matrix F.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D object-text alignment for situated understanding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>During training, for each object obtain a situated textual description and its text embedding; compute object visual representation O' via PointNet++ + spatial self-attention that incorporates f_ij pairwise features; minimize L_align = MSE(O', W) jointly with language modeling loss.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation learning / multimodal alignment / enables downstream navigation & planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained text encoder embeddings (CLIP) and 3D object encoders (PointNet++) trained/fine-tuned on Spartun3D</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised alignment loss (MSE) between object visual features and text embeddings; spatial self-attention computes O' using pairwise geometric features.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit object-text alignment: textual templates -> CLIP embeddings W; object visual O' via spatial self-attention augmented with pairwise spatial features F (d_ij, sin/cos angles); alignment loss aligns geometric-aware visual vectors to textual semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvements in downstream captioning/QA metrics and sentence-similarity on external benchmarks (Scan2Cap, ScanQA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Adding alignment improved downstream metrics by ~2–3% relative to LEO+Spartun3D; sentence-similarity increased on Scan2Cap from 54.2% → 55.7% and ScanQA 46.3% → 48.6% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Produces finer-grained spatial phrasing, better object naming and directional references, and improved generalization across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Alignment depends on the quality of textual descriptions and text encoder; mis-specified affordances or erroneous scene labels (from 3RScan) can propagate incorrect textual supervision (common-sense violations).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LEO+Spartun3D (no explicit alignment) vs Spartun3D-LLM (with alignment): alignment yields consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Text-encoder ablation: CLIP > BLIP > BERT for alignment effectiveness; removing alignment reduces fine-grained spatial language generation and lowers QA/captioning metrics by a few percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly supervising 3D object embeddings with situated textual embeddings (via CLIP) and injecting pairwise geometric features into attention provides an effective mechanism for encoding and utilizing spatial and object-relational knowledge for situated planning and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e385.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Situated Scene Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Situated Scene Graph (agent-centric adaptation of global scene graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-centered JSON scene graph that encodes per-object attributes and relative spatial relations from the agent's standing point and orientation (rotation angles normalized and binned into front/right/back/left, Euclidean distances, and passby objects intersecting straight-line paths).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spartun3D pipeline (used to construct dataset and condition prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructed by projecting 3D objects to bird's-eye view, selecting an agent standing point beside a pivot object and orientation facing object center, computing rotation angles to other objects, binning directions, computing distances and passby objects by line intersection, and outputting JSON entries used to prompt GPT-4o and to condition model inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Situated scene graph construction for dataset generation and model conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Represents the immediate egocentric surroundings of an agent: for each object provide direction bin (front/right/back/left), normalized rotation angle, distance (Euclidean), passby objects if a straight path intersects other objects, and object attributes/affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial representation / grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>computed from 3RScan 3D coordinates and segmentation masks</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>explicit geometric computation from 3D coordinates (used to generate textual prompts and to build alignment supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>JSON-structured agent-centric scene graph encoding directional bins, rotation angles, distances, passby lists, and attributes; used both for prompting GPT-4o and creating situated textual descriptions for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human evaluation of dataset spatial fidelity and downstream model performance gains when trained with dataset conditioned on these graphs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using situated scene graphs as input to LLM prompts (Spa-prompt) yielded significantly higher-quality generated situated data than raw coordinate prompts; models trained on this data show improved situated reasoning and lower directional bias.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides a concise, qualitative representation that LLMs can successfully use to reason about agent-centric spatial relations and plan actions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If the scene graph is generated from noisy annotations (e.g., erroneous affordances), the subsequent textual data and alignment can propagate mistakes; occlusion issues (object not visible from standing point) may still cause misalignment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Spa-prompt (derived from scene graph) vs Cord-prompt (raw coordinates): Spa-prompt produces better LLM outputs and dataset quality.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Prompt ablation showed Spa-prompt substantially outperforms Cord-prompt in human-evaluated spatial fidelity and language naturalness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing egocentric spatial relations qualitatively (direction bins, passby) in a structured scene graph is far more usable by LLMs than raw coordinates and is critical for generating situated datasets and for supervising 3D-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e385.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spatial Self-Attention & Pairwise Features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Self-Attention with Pairwise Spatial Feature Matrix (F)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism injecting pairwise geometric information (distance and sine/cosine of horizontal/vertical angles) into self-attention over object tokens to produce spatial-aware object representations O'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Spartun3D-LLM (attention module inside object encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compute pairwise feature f_ij = [d_ij, sin(theta_h), cos(theta_h), sin(theta_v), cos(theta_v)] for object pairs, pass F through an MLP and add to the attention logits: O' = softmax((QK^T)/sqrt(d_h) + MLP(F)) V, producing spatial-aware object embeddings used for alignment with textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>spatially-aware object representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Enhance object-level visual embeddings with explicit geometric relations so that downstream language generation and planning can leverage encoded pairwise spatial geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation learning / multimodal grounding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>3D geometry (pairwise distances and angles between object bounding-box centers)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>architectural injection of geometric features into attention layers (during training/fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Dense pairwise spatial feature matrix F combined with standard attention (QKV) to yield O' vectors that encode relative positions and orientations among objects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvements in downstream metrics when combined with alignment (captioning/QA accuracy, sentence similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Incorporating spatial self-attention + alignment improved QA/captioning by a few percentage points; enabled more precise directional language (e.g., 'turn slightly right').</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Made object embeddings sensitive to relative geometry and improved alignment to textual direction/distance descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Relies on accurate pairwise geometry; noisy segmentation or inaccurate object centers degrade effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Standard concatenation-based multimodal token fusion (used in prior 3D-LLMs) vs spatial self-attention with F: the latter yields better spatial-aware outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing pairwise feature injection reduced fine-grained spatial phrasing and lowered downstream metrics (consistent with alignment ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly injecting pairwise geometric signals into attention is an effective way to encode spatial relations into object-level embeddings that LLM decoders can exploit for situated planning and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e385.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEO (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEO (Existing 3D-based LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art 3D-based LLM that injects 3D point-cloud object tokens into an LLM backbone and was used as the base architecture for Spartun3D-LLM and as the experimental baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LEO</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that integrates object-level point cloud encoders (PointNet++), converts multi-modal inputs into token sequences, and uses autoregressive LLM decoding for 3D grounding, reasoning, and action planning; in this paper LEO is fine-tuned and used with OPT-1.3B and Vicuna-7B backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>situated QA, captioning, and navigation baselines</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LEO was evaluated zero-shot and fine-tuned on Spartun3D and SQA3D for attribute/relation QA, affordance, planning, captioning, and MP3D ObjNav; it typically concatenates modality tokens without explicit object-text alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation, situated QA, captioning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (but limited in situated alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on multi-modal datasets and prior LEO training; fine-tuning on Spartun3D for comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting and fine-tuning (LoRA) on Spartun3D; model initialized from LEO pre-trained weights in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Object-centric point cloud embeddings concatenated with text tokens and fed to LLM (no explicit object-text alignment or pairwise spatial feature injection in baseline LEO used here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>same as Spartun3D-LLM (CIDEr, BLEU-4, METEOR, ROUGE, sentence-similarity, exact-match, navigation accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>LEO showed reasonable zero-shot performance on attribute/relation QA but struggled on other situated tasks; direction bias (predicted 'left' 97% in SQA3D questions) and near-zero navigation action generation without task-specific fine-tuning (LEO ~0% navigation accuracy in zero-shot, vs Spartun3D-LLM 20.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good on attribute/relation QA zero-shot where global object knowledge suffices.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Poor situated understanding from an egocentric perspective, inability to properly ground directions and plan multi-step actions without situated dataset and explicit alignment, strong directional bias.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as baseline; Spartun3D-LLM outperforms LEO and LEO+Spartun3D across nearly all metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>LEO trained on Spartun3D (LEO+Spartun3D) reduces bias and improves performance; adding explicit alignment (Spartun3D-LLM) further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla concatenation of modality tokens is insufficient for precise situated spatial reasoning; explicit situated datasets and alignment modules are necessary to overcome egocentric biases and to enable planning/navigation language outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e385.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (in pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI) used to generate Spartun3D annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned LLM used to synthesize situated captions and QA pairs from structured situated scene graphs; Spa-prompt (qualitative relations) yielded higher-quality data than direct coordinate prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted with the JSON situated scene graph (Spa-prompt or Cord-prompt) to generate situated captions and QA pairs; Spa-prompt provided pre-computed qualitative relations (direction/distance) which LLMs leverage more effectively than raw coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>dataset generation / data augmentation for situated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce natural-language situated descriptions, question templates, and multi-hop planning QAs conditioned on agent-centric scene graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>data generation for situated reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (encoded in generated text)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretrained language model world knowledge + structured scene graph prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting (Cord-prompt vs Spa-prompt) with structured JSON scene graphs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Outputs natural language datasets (captions, QA, planning sequences) representing spatial relations qualitatively; not directly trained to map raw coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human evaluation on language naturalness and spatial fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Spa-prompt produced significantly more high-quality examples (higher % with human score ≥4) than Cord-prompt; overall dataset quality adequate for large-scale training though planning examples had more errors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Effective at producing diverse situated QA and captions when given qualitative spatial prompts; generated affordance and planning QAs usable for training 3D-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggled with raw coordinate interpretation (Cord-prompt), produced some semantic/common-sense errors when source annotations were noisy, and generated planning mistakes in second-step actions.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Spa-prompt > Cord-prompt in generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Prompt ablation confirmed superior performance of Spa-prompt; human-eval identified ~26/200 errors across tasks due to semantics, common-sense, spatial inconsistencies, and misalignments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be used to scale situated dataset generation but must be given qualitative, human-friendly spatial encodings (directions/distances/passby) rather than raw coordinates to avoid spatial reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e385.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointNet++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PointNet++ (object-centric point cloud encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical point-set encoder used to extract object-level 3D visual representations from segmented object point clouds as input to the 3D-LLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PointNet++</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained on ScanNet and used to encode sampled points (1024 per object) into object-level visual vectors O that are then passed into spatial self-attention and LLM fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>object-level 3D feature extraction for 3D-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Segment scene into objects, sample points per object, encode with PointNet++ to produce object-centric embeddings used in sequence input to the LLM and for alignment with textual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>perception / representation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object attributes</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on 3D datasets (ScanNet) and frozen during much of Spartun3D training (paper states point cloud encoder and LLM are frozen during training with LoRA applied).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pre-trained encoder applied to object point clouds before attention/alignment steps</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>learned geometric feature vectors per object capturing shape/geometry, which are further processed by spatial self-attention for relational encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream task improvements when combined with spatial alignment and Spartun3D training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Used as standard object encoder; no isolated numeric ablation provided, but pipeline leveraging PointNet++ + alignment improved downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides robust object-level geometric features that, when aligned to textual embeddings and enriched with pairwise spatial features, support situated reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If frozen, cannot adapt to corrected affordance/textual supervision; errors in segmentation/point sampling propagate.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Standard in 3D pipelines; used as the visual encoder for LEO and Spartun3D-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper freezes encoder and LLM and trains LoRA; ablations focus on alignment and prompt types rather than replacing PointNet++.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-centric point-cloud encoders are an effective substrate for aligning spatial geometry with textual descriptions, but the downstream benefit requires explicit alignment and relational modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e385.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e385.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (text encoder used for alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining) used for text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal encoder used to produce textual embeddings of situated object descriptions; empirically superior in the paper's ablation for aligning 3D object representations to text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained multimodal encoder producing text embeddings used as alignment targets (W) for object visual representations; chosen after ablation (CLIP EM 56.9% vs BLIP 54.2% vs BERT 53.5% on object attribute & relation tasks with alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>text embedding provider for object-text alignment</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Encode situated textual descriptions (templates describing object name, attributes, and surrounding objects per direction) into vector targets used by the alignment MSE loss.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multimodal grounding / supervision</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (semantic) + spatial (encoded via textual phrasing)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large image-text corpora (CLIP's original training)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pre-compute embeddings of situated textual templates and minimize MSE with object visual embeddings O'</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>dense text embedding vectors representing situated descriptions of objects (attributes + directional neighbors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match (EM) gains and downstream metric improvements when used in alignment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CLIP outperformed BLIP and BERT in alignment ablation (EM: CLIP 56.9% vs BLIP 54.2% vs BERT 53.5%). Using CLIP as alignment target improved QA/captioning metrics by a few percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Produced embeddings that reliably supervised geometric-aware object embeddings to align semantics with spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If textual templates include incorrect affordances or wrong relations from source annotations, CLIP embeddings will faithfully encode those errors and misguide alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Other text encoders (BERT, BLIP) were worse in alignment ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>CLIP chosen as best-performing text encoder for the alignment loss in the ablation study (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a multimodal text encoder (CLIP) as the target for object-text alignment is more effective than pure-text encoders for teaching 3D visual features to encode semantics and situated relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SQA3D: Situated question answering in 3d scenes <em>(Rating: 2)</em></li>
                <li>3d-llm: Injecting the 3d world into large language models <em>(Rating: 2)</em></li>
                <li>3d-vista: Pretrained transformer for 3d vision and text alignment <em>(Rating: 2)</em></li>
                <li>Scan2Cap: Context-aware dense captioning in rgb-d scans <em>(Rating: 1)</em></li>
                <li>Pointnet++: Deep hierarchical feature learning on point sets in a metric space <em>(Rating: 1)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-385",
    "paper_id": "paper-273186345",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Spartun3D",
            "name_full": "Spartun3D (Situated 3D Dataset)",
            "brief_description": "A large-scale, LLM-generated situated 3D dataset (~133k examples) that encodes an agent's standing point and orientation and provides situated scene graphs, situated captioning, and situated QA (including multi-step planning) to teach LLMs situated spatial, procedural, and object-relational reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (data-generation LLM)",
            "model_size": null,
            "model_description": "GPT-4o was prompted (Cord-prompt and Spa-prompt) to synthesize situations, situated scene graphs, situated captions, and QA pairs from JSON scene graphs derived from 3RScan; Spa-prompt (qualitative spatial relations like distance/direction) produced higher-quality situated outputs than raw coordinates.",
            "task_name": "Spartun3D dataset construction tasks",
            "task_description": "Automated pipeline: (1) create diverse agent situations (standing point + orientation), (2) compute situated scene graphs with rotation angles, direction bins, distances, and passby objects, (3) prompt GPT-4o to produce situated captions and QA (object attributes & relations, affordances, and 2-hop situated planning).",
            "task_type": "dataset generation / situated captioning and QA (supports navigation/planning)",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "LLM-generation (GPT-4o) from 3D scene graphs + human-annotated 3RScan object labels",
            "has_direct_sensory_input": false,
            "elicitation_method": "LLM prompting (Cord-prompt and Spa-prompt), scripted scene-graph generation from 3D coordinates",
            "knowledge_representation": "Situated scene graphs (JSON) encoding agent standing point/orientation, per-object distances, direction bins (front/right/back/left), rotation angles, passby objects, and object attributes; textual situational templates used to create descriptions.",
            "performance_metric": "human validation scores (language naturalness and spatial fidelity), dataset scale and diversity metrics",
            "performance_result": "Collected ~10k situated captions and ~123k QA pairs; human validation showed Spa-prompt produced substantially more high-quality examples (higher % with score ≥4) than Cord-prompt; overall human scores vary by task (captioning & planning lower due to complexity).",
            "success_patterns": "Spa-prompt (qualitative spatial relations) enabled GPT-4o to produce coherent situated textual data; inclusion of passby objects and direction/distance bins allowed generation of planning and affordance QAs.",
            "failure_patterns": "Cord-prompt (raw coordinates) produced many spatial errors — LLMs struggle to reason from raw 3D coordinates; some semantic/common-sense violations and occlusion/misalignment errors (especially in multi-step planning answers).",
            "baseline_comparison": "Compared prompting strategies: Spa-prompt &gt; Cord-prompt (higher human quality); no prior large-scale automated situated 3D dataset at this scale reported in paper.",
            "ablation_results": "Prompt ablation: Spa-prompt improved quality metrics vs Cord-prompt; sampling fewer situations for planning/captioning due to token cost; human-eval identified majority of errors in planning task generation.",
            "key_findings": "Constructing qualitative, situated scene graphs (directions, passby, distances) and prompting an LLM (GPT-4o) with these qualitative relations produces scalable, high-quality situated datasets; raw coordinates are poorly interpreted by LLMs, so higher-level spatial descriptors are essential for LLM-mediated dataset generation.",
            "uuid": "e385.0",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Spartun3D-LLM",
            "name_full": "Spartun3D-LLM (3D-based LLM with Situated Spatial Alignment)",
            "brief_description": "A 3D-LLM built on LEO that integrates an explicit Situated Spatial Alignment module which aligns object-centric 3D visual representations with situated textual descriptions (via a text encoder like CLIP) using an MSE alignment loss and spatial self-attention with pairwise spatial features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spartun3D-LLM (built on LEO backbone)",
            "model_size": null,
            "model_description": "Uses object-centric PointNet++ encodings for each segmented object, spatial self-attention that injects pairwise spatial features f_ij = [d_ij, sin(theta_h), cos(theta_h), sin(theta_v), cos(theta_v)], and an alignment loss (MSE) between learned object representations and text embeddings from a pre-trained text encoder (CLIP preferred). Fine-tuned via LoRA on Spartun3D; optionally initialized from instruction-tuned LEO weights.",
            "task_name": "Situated captioning, Situated QA (attributes/relations, affordance, 2-hop planning), and zero-shot MP3D ObjNav evaluation",
            "task_description": "Generate textual answers/captions or action tokens from combined inputs &lt;C,S,Q&gt; where C is 3D scene context (object point clouds), S is situation (text + standing point/orientation during training), Q is question. For navigation, produce stepwise navigation actions (turn left/right, move forward, stop).",
            "task_type": "navigation, multi-step planning, situated QA, captioning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "fine-tuning on Spartun3D (LLM + 3D object encoders) and alignment to pre-trained text encoder embeddings (CLIP); optionally initialized from pre-trained LEO weights",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning (LoRA) on Spartun3D; zero-shot evaluation on external datasets (SQA3D, MP3D ObjNav); generation (autoregressive) conditioned on text + object tokens; alignment loss jointly optimized with language modeling loss.",
            "knowledge_representation": "Object-centric 3D embeddings (PointNet++), spatial-aware object representations O' via spatial self-attention augmented by pairwise spatial matrix F, situated textual embeddings W (CLIP), and MSE alignment (L_align) between O' and W; situations represented as textual templates and/or standing point/orientation during training.",
            "performance_metric": "generation metrics for captioning (CIDEr, BLEU-4, METEOR, ROUGE-L, sentence similarity), exact-match (EM) for QA, and stepwise action accuracy for navigation",
            "performance_result": "Spartun3D-LLM outperforms LEO+Spartun3D by ~2–3% across many captioning/QA metrics; zero-shot navigation accuracy: Spartun3D-LLM 20.3% vs LEO 0% (Table 5). Alignment yields modest gains on Scan2Cap sentence-similarity 54.2%→55.7% and ScanQA 46.3%→48.6%.",
            "success_patterns": "Improved fine-grained, context-aware spatial descriptions (e.g., 'turn slightly right' vs generic 'turn around'), better object attribute and relation QA zero-shot generalization, reduction of directional bias (LEO favoured 'left' 97% — bias reduced with Spartun3D training and further with alignment), and transferability to generate navigation actions without task-specific fine-tuning.",
            "failure_patterns": "Remaining errors in multi-step (2-hop) situated planning (inaccuracies in the second action), occasional semantic/common-sense violations from generated dataset artifacts, and vulnerabilities when viewpoints are occluded or when textual descriptions mismatch visual layout.",
            "baseline_comparison": "Compared to LEO and LEO+Spartun3D: Spartun3D-LLM shows consistent +2–3% improvements across tasks; navigation: LEO baseline ~0% vs Spartun3D-LLM 20.3% (zero-shot, stepwise action accuracy).",
            "ablation_results": "Removing the situated spatial alignment module (i.e., L_align) reduces fine-grained spatial detail and lowers metrics by ~2–3%; initializing from instruction-tuned LEO weights improves absolute performance vs training from scratch; CLIP as text encoder outperforms BERT/BLIP for the alignment objective (CLIP EM 56.9% vs BLIP 54.2% vs BERT 53.5% in ablation).",
            "key_findings": "Explicit alignment between object-level 3D embeddings and situated textual descriptions, combined with spatial self-attention that injects pairwise geometric features, enables LLMs to better encode and utilize spatial, procedural, and object-relational knowledge — improving situated reasoning, fine-grained spatial language generation, and zero-shot transfer to navigation tasks. LLMs alone poorly interpret raw coordinates; representing spatial relations qualitatively (directions, passby, distances) and aligning visual and textual modalities is critical.",
            "uuid": "e385.1",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Situated Spatial Alignment Module",
            "name_full": "Situated Spatial Alignment Module (object-text alignment with spatial self-attention)",
            "brief_description": "A module that generates per-object situated textual descriptions and enforces alignment between spatial-aware object visual representations and textual embeddings (from CLIP) using an MSE loss, with spatial self-attention injecting pairwise geometric features into object attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spartun3D-LLM (module integrated into LEO)",
            "model_size": null,
            "model_description": "Creates textual templates per object describing name, attributes, and directional neighbors (up to five per direction). Computes text embeddings W via a pre-trained text encoder (CLIP) and minimizes MSE between W and spatial-aware object representations O' computed by spatial self-attention that receives pairwise spatial matrix F.",
            "task_name": "3D object-text alignment for situated understanding",
            "task_description": "During training, for each object obtain a situated textual description and its text embedding; compute object visual representation O' via PointNet++ + spatial self-attention that incorporates f_ij pairwise features; minimize L_align = MSE(O', W) jointly with language modeling loss.",
            "task_type": "representation learning / multimodal alignment / enables downstream navigation & planning",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "pre-trained text encoder embeddings (CLIP) and 3D object encoders (PointNet++) trained/fine-tuned on Spartun3D",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised alignment loss (MSE) between object visual features and text embeddings; spatial self-attention computes O' using pairwise geometric features.",
            "knowledge_representation": "Explicit object-text alignment: textual templates -&gt; CLIP embeddings W; object visual O' via spatial self-attention augmented with pairwise spatial features F (d_ij, sin/cos angles); alignment loss aligns geometric-aware visual vectors to textual semantics.",
            "performance_metric": "improvements in downstream captioning/QA metrics and sentence-similarity on external benchmarks (Scan2Cap, ScanQA)",
            "performance_result": "Adding alignment improved downstream metrics by ~2–3% relative to LEO+Spartun3D; sentence-similarity increased on Scan2Cap from 54.2% → 55.7% and ScanQA 46.3% → 48.6% (Table 6).",
            "success_patterns": "Produces finer-grained spatial phrasing, better object naming and directional references, and improved generalization across datasets.",
            "failure_patterns": "Alignment depends on the quality of textual descriptions and text encoder; mis-specified affordances or erroneous scene labels (from 3RScan) can propagate incorrect textual supervision (common-sense violations).",
            "baseline_comparison": "LEO+Spartun3D (no explicit alignment) vs Spartun3D-LLM (with alignment): alignment yields consistent gains.",
            "ablation_results": "Text-encoder ablation: CLIP &gt; BLIP &gt; BERT for alignment effectiveness; removing alignment reduces fine-grained spatial language generation and lowers QA/captioning metrics by a few percentage points.",
            "key_findings": "Directly supervising 3D object embeddings with situated textual embeddings (via CLIP) and injecting pairwise geometric features into attention provides an effective mechanism for encoding and utilizing spatial and object-relational knowledge for situated planning and navigation.",
            "uuid": "e385.2",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Situated Scene Graph",
            "name_full": "Situated Scene Graph (agent-centric adaptation of global scene graphs)",
            "brief_description": "An agent-centered JSON scene graph that encodes per-object attributes and relative spatial relations from the agent's standing point and orientation (rotation angles normalized and binned into front/right/back/left, Euclidean distances, and passby objects intersecting straight-line paths).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spartun3D pipeline (used to construct dataset and condition prompts)",
            "model_size": null,
            "model_description": "Constructed by projecting 3D objects to bird's-eye view, selecting an agent standing point beside a pivot object and orientation facing object center, computing rotation angles to other objects, binning directions, computing distances and passby objects by line intersection, and outputting JSON entries used to prompt GPT-4o and to condition model inputs.",
            "task_name": "Situated scene graph construction for dataset generation and model conditioning",
            "task_description": "Represents the immediate egocentric surroundings of an agent: for each object provide direction bin (front/right/back/left), normalized rotation angle, distance (Euclidean), passby objects if a straight path intersects other objects, and object attributes/affordances.",
            "task_type": "spatial representation / grounding",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "computed from 3RScan 3D coordinates and segmentation masks",
            "has_direct_sensory_input": false,
            "elicitation_method": "explicit geometric computation from 3D coordinates (used to generate textual prompts and to build alignment supervision)",
            "knowledge_representation": "JSON-structured agent-centric scene graph encoding directional bins, rotation angles, distances, passby lists, and attributes; used both for prompting GPT-4o and creating situated textual descriptions for alignment.",
            "performance_metric": "human evaluation of dataset spatial fidelity and downstream model performance gains when trained with dataset conditioned on these graphs",
            "performance_result": "Using situated scene graphs as input to LLM prompts (Spa-prompt) yielded significantly higher-quality generated situated data than raw coordinate prompts; models trained on this data show improved situated reasoning and lower directional bias.",
            "success_patterns": "Provides a concise, qualitative representation that LLMs can successfully use to reason about agent-centric spatial relations and plan actions.",
            "failure_patterns": "If the scene graph is generated from noisy annotations (e.g., erroneous affordances), the subsequent textual data and alignment can propagate mistakes; occlusion issues (object not visible from standing point) may still cause misalignment.",
            "baseline_comparison": "Spa-prompt (derived from scene graph) vs Cord-prompt (raw coordinates): Spa-prompt produces better LLM outputs and dataset quality.",
            "ablation_results": "Prompt ablation showed Spa-prompt substantially outperforms Cord-prompt in human-evaluated spatial fidelity and language naturalness.",
            "key_findings": "Representing egocentric spatial relations qualitatively (direction bins, passby) in a structured scene graph is far more usable by LLMs than raw coordinates and is critical for generating situated datasets and for supervising 3D-LLMs.",
            "uuid": "e385.3",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Spatial Self-Attention & Pairwise Features",
            "name_full": "Spatial Self-Attention with Pairwise Spatial Feature Matrix (F)",
            "brief_description": "A mechanism injecting pairwise geometric information (distance and sine/cosine of horizontal/vertical angles) into self-attention over object tokens to produce spatial-aware object representations O'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Spartun3D-LLM (attention module inside object encoder)",
            "model_size": null,
            "model_description": "Compute pairwise feature f_ij = [d_ij, sin(theta_h), cos(theta_h), sin(theta_v), cos(theta_v)] for object pairs, pass F through an MLP and add to the attention logits: O' = softmax((QK^T)/sqrt(d_h) + MLP(F)) V, producing spatial-aware object embeddings used for alignment with textual descriptions.",
            "task_name": "spatially-aware object representation learning",
            "task_description": "Enhance object-level visual embeddings with explicit geometric relations so that downstream language generation and planning can leverage encoded pairwise spatial geometry.",
            "task_type": "representation learning / multimodal grounding",
            "knowledge_type": "spatial+object-relational",
            "knowledge_source": "3D geometry (pairwise distances and angles between object bounding-box centers)",
            "has_direct_sensory_input": true,
            "elicitation_method": "architectural injection of geometric features into attention layers (during training/fine-tuning)",
            "knowledge_representation": "Dense pairwise spatial feature matrix F combined with standard attention (QKV) to yield O' vectors that encode relative positions and orientations among objects.",
            "performance_metric": "improvements in downstream metrics when combined with alignment (captioning/QA accuracy, sentence similarity)",
            "performance_result": "Incorporating spatial self-attention + alignment improved QA/captioning by a few percentage points; enabled more precise directional language (e.g., 'turn slightly right').",
            "success_patterns": "Made object embeddings sensitive to relative geometry and improved alignment to textual direction/distance descriptions.",
            "failure_patterns": "Relies on accurate pairwise geometry; noisy segmentation or inaccurate object centers degrade effectiveness.",
            "baseline_comparison": "Standard concatenation-based multimodal token fusion (used in prior 3D-LLMs) vs spatial self-attention with F: the latter yields better spatial-aware outputs.",
            "ablation_results": "Removing pairwise feature injection reduced fine-grained spatial phrasing and lowered downstream metrics (consistent with alignment ablation).",
            "key_findings": "Explicitly injecting pairwise geometric signals into attention is an effective way to encode spatial relations into object-level embeddings that LLM decoders can exploit for situated planning and navigation.",
            "uuid": "e385.4",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LEO (baseline)",
            "name_full": "LEO (Existing 3D-based LLM baseline)",
            "brief_description": "A state-of-the-art 3D-based LLM that injects 3D point-cloud object tokens into an LLM backbone and was used as the base architecture for Spartun3D-LLM and as the experimental baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LEO",
            "model_size": null,
            "model_description": "Framework that integrates object-level point cloud encoders (PointNet++), converts multi-modal inputs into token sequences, and uses autoregressive LLM decoding for 3D grounding, reasoning, and action planning; in this paper LEO is fine-tuned and used with OPT-1.3B and Vicuna-7B backbones.",
            "task_name": "situated QA, captioning, and navigation baselines",
            "task_description": "LEO was evaluated zero-shot and fine-tuned on Spartun3D and SQA3D for attribute/relation QA, affordance, planning, captioning, and MP3D ObjNav; it typically concatenates modality tokens without explicit object-text alignment.",
            "task_type": "navigation, situated QA, captioning",
            "knowledge_type": "spatial+object-relational (but limited in situated alignment)",
            "knowledge_source": "pre-training on multi-modal datasets and prior LEO training; fine-tuning on Spartun3D for comparisons",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot prompting and fine-tuning (LoRA) on Spartun3D; model initialized from LEO pre-trained weights in some experiments",
            "knowledge_representation": "Object-centric point cloud embeddings concatenated with text tokens and fed to LLM (no explicit object-text alignment or pairwise spatial feature injection in baseline LEO used here).",
            "performance_metric": "same as Spartun3D-LLM (CIDEr, BLEU-4, METEOR, ROUGE, sentence-similarity, exact-match, navigation accuracy)",
            "performance_result": "LEO showed reasonable zero-shot performance on attribute/relation QA but struggled on other situated tasks; direction bias (predicted 'left' 97% in SQA3D questions) and near-zero navigation action generation without task-specific fine-tuning (LEO ~0% navigation accuracy in zero-shot, vs Spartun3D-LLM 20.3%).",
            "success_patterns": "Good on attribute/relation QA zero-shot where global object knowledge suffices.",
            "failure_patterns": "Poor situated understanding from an egocentric perspective, inability to properly ground directions and plan multi-step actions without situated dataset and explicit alignment, strong directional bias.",
            "baseline_comparison": "Serves as baseline; Spartun3D-LLM outperforms LEO and LEO+Spartun3D across nearly all metrics.",
            "ablation_results": "LEO trained on Spartun3D (LEO+Spartun3D) reduces bias and improves performance; adding explicit alignment (Spartun3D-LLM) further improves results.",
            "key_findings": "Vanilla concatenation of modality tokens is insufficient for precise situated spatial reasoning; explicit situated datasets and alignment modules are necessary to overcome egocentric biases and to enable planning/navigation language outputs.",
            "uuid": "e385.5",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o (in pipeline)",
            "name_full": "GPT-4o (OpenAI) used to generate Spartun3D annotations",
            "brief_description": "A large instruction-tuned LLM used to synthesize situated captions and QA pairs from structured situated scene graphs; Spa-prompt (qualitative relations) yielded higher-quality data than direct coordinate prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "model_description": "Prompted with the JSON situated scene graph (Spa-prompt or Cord-prompt) to generate situated captions and QA pairs; Spa-prompt provided pre-computed qualitative relations (direction/distance) which LLMs leverage more effectively than raw coordinates.",
            "task_name": "dataset generation / data augmentation for situated tasks",
            "task_description": "Produce natural-language situated descriptions, question templates, and multi-hop planning QAs conditioned on agent-centric scene graphs.",
            "task_type": "data generation for situated reasoning tasks",
            "knowledge_type": "spatial+procedural+object-relational (encoded in generated text)",
            "knowledge_source": "pretrained language model world knowledge + structured scene graph prompts",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompting (Cord-prompt vs Spa-prompt) with structured JSON scene graphs",
            "knowledge_representation": "Outputs natural language datasets (captions, QA, planning sequences) representing spatial relations qualitatively; not directly trained to map raw coordinates.",
            "performance_metric": "human evaluation on language naturalness and spatial fidelity",
            "performance_result": "Spa-prompt produced significantly more high-quality examples (higher % with human score ≥4) than Cord-prompt; overall dataset quality adequate for large-scale training though planning examples had more errors.",
            "success_patterns": "Effective at producing diverse situated QA and captions when given qualitative spatial prompts; generated affordance and planning QAs usable for training 3D-LLMs.",
            "failure_patterns": "Struggled with raw coordinate interpretation (Cord-prompt), produced some semantic/common-sense errors when source annotations were noisy, and generated planning mistakes in second-step actions.",
            "baseline_comparison": "Spa-prompt &gt; Cord-prompt in generation quality.",
            "ablation_results": "Prompt ablation confirmed superior performance of Spa-prompt; human-eval identified ~26/200 errors across tasks due to semantics, common-sense, spatial inconsistencies, and misalignments.",
            "key_findings": "LLMs can be used to scale situated dataset generation but must be given qualitative, human-friendly spatial encodings (directions/distances/passby) rather than raw coordinates to avoid spatial reasoning failures.",
            "uuid": "e385.6",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "PointNet++",
            "name_full": "PointNet++ (object-centric point cloud encoder)",
            "brief_description": "A hierarchical point-set encoder used to extract object-level 3D visual representations from segmented object point clouds as input to the 3D-LLM pipeline.",
            "citation_title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "mention_or_use": "use",
            "model_name": "PointNet++",
            "model_size": null,
            "model_description": "Pre-trained on ScanNet and used to encode sampled points (1024 per object) into object-level visual vectors O that are then passed into spatial self-attention and LLM fusion.",
            "task_name": "object-level 3D feature extraction for 3D-LLMs",
            "task_description": "Segment scene into objects, sample points per object, encode with PointNet++ to produce object-centric embeddings used in sequence input to the LLM and for alignment with textual embeddings.",
            "task_type": "perception / representation extraction",
            "knowledge_type": "spatial+object attributes",
            "knowledge_source": "pre-training on 3D datasets (ScanNet) and frozen during much of Spartun3D training (paper states point cloud encoder and LLM are frozen during training with LoRA applied).",
            "has_direct_sensory_input": true,
            "elicitation_method": "pre-trained encoder applied to object point clouds before attention/alignment steps",
            "knowledge_representation": "learned geometric feature vectors per object capturing shape/geometry, which are further processed by spatial self-attention for relational encoding.",
            "performance_metric": "downstream task improvements when combined with spatial alignment and Spartun3D training",
            "performance_result": "Used as standard object encoder; no isolated numeric ablation provided, but pipeline leveraging PointNet++ + alignment improved downstream metrics.",
            "success_patterns": "Provides robust object-level geometric features that, when aligned to textual embeddings and enriched with pairwise spatial features, support situated reasoning.",
            "failure_patterns": "If frozen, cannot adapt to corrected affordance/textual supervision; errors in segmentation/point sampling propagate.",
            "baseline_comparison": "Standard in 3D pipelines; used as the visual encoder for LEO and Spartun3D-LLM.",
            "ablation_results": "Paper freezes encoder and LLM and trains LoRA; ablations focus on alignment and prompt types rather than replacing PointNet++.",
            "key_findings": "Object-centric point-cloud encoders are an effective substrate for aligning spatial geometry with textual descriptions, but the downstream benefit requires explicit alignment and relational modeling.",
            "uuid": "e385.7",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CLIP (text encoder used for alignment)",
            "name_full": "CLIP (Contrastive Language–Image Pretraining) used for text embeddings",
            "brief_description": "A multimodal encoder used to produce textual embeddings of situated object descriptions; empirically superior in the paper's ablation for aligning 3D object representations to text.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP",
            "model_size": null,
            "model_description": "Pre-trained multimodal encoder producing text embeddings used as alignment targets (W) for object visual representations; chosen after ablation (CLIP EM 56.9% vs BLIP 54.2% vs BERT 53.5% on object attribute & relation tasks with alignment).",
            "task_name": "text embedding provider for object-text alignment",
            "task_description": "Encode situated textual descriptions (templates describing object name, attributes, and surrounding objects per direction) into vector targets used by the alignment MSE loss.",
            "task_type": "multimodal grounding / supervision",
            "knowledge_type": "object-relational (semantic) + spatial (encoded via textual phrasing)",
            "knowledge_source": "pre-training on large image-text corpora (CLIP's original training)",
            "has_direct_sensory_input": false,
            "elicitation_method": "pre-compute embeddings of situated textual templates and minimize MSE with object visual embeddings O'",
            "knowledge_representation": "dense text embedding vectors representing situated descriptions of objects (attributes + directional neighbors).",
            "performance_metric": "exact-match (EM) gains and downstream metric improvements when used in alignment",
            "performance_result": "CLIP outperformed BLIP and BERT in alignment ablation (EM: CLIP 56.9% vs BLIP 54.2% vs BERT 53.5%). Using CLIP as alignment target improved QA/captioning metrics by a few percentage points.",
            "success_patterns": "Produced embeddings that reliably supervised geometric-aware object embeddings to align semantics with spatial relations.",
            "failure_patterns": "If textual templates include incorrect affordances or wrong relations from source annotations, CLIP embeddings will faithfully encode those errors and misguide alignment.",
            "baseline_comparison": "Other text encoders (BERT, BLIP) were worse in alignment ablation.",
            "ablation_results": "CLIP chosen as best-performing text encoder for the alignment loss in the ablation study (Table 9).",
            "key_findings": "Using a multimodal text encoder (CLIP) as the target for object-text alignment is more effective than pure-text encoders for teaching 3D visual features to encode semantics and situated relations.",
            "uuid": "e385.8",
            "source_info": {
                "paper_title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SQA3D: Situated question answering in 3d scenes",
            "rating": 2,
            "sanitized_title": "sqa3d_situated_question_answering_in_3d_scenes"
        },
        {
            "paper_title": "3d-llm: Injecting the 3d world into large language models",
            "rating": 2,
            "sanitized_title": "3dllm_injecting_the_3d_world_into_large_language_models"
        },
        {
            "paper_title": "3d-vista: Pretrained transformer for 3d vision and text alignment",
            "rating": 2,
            "sanitized_title": "3dvista_pretrained_transformer_for_3d_vision_and_text_alignment"
        },
        {
            "paper_title": "Scan2Cap: Context-aware dense captioning in rgb-d scans",
            "rating": 1,
            "sanitized_title": "scan2cap_contextaware_dense_captioning_in_rgbd_scans"
        },
        {
            "paper_title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "rating": 1,
            "sanitized_title": "pointnet_deep_hierarchical_feature_learning_on_point_sets_in_a_metric_space"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 1,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        }
    ],
    "cost": 0.023832749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SPARTUN3D: SITUATED SPATIAL UNDERSTANDING OF 3D WORLD IN LARGE LANGUAGE MODELS
2 Mar 2025</p>
<p>Yue Zhang 
Michigan State University</p>
<p>Zhiyang Xu zhiyangx@vt.edu 
Virginia Tech</p>
<p>Ying Shen ying22@illinois.edu 
University of Illinois at Urbana
Champaign 4 UC Davis</p>
<p>Parisa Kordjamshidi 
Michigan State University</p>
<p>Lifu Huang 
SPARTUN3D: SITUATED SPATIAL UNDERSTANDING OF 3D WORLD IN LARGE LANGUAGE MODELS
2 Mar 2025C8B9102CFF38F75F59B0C49E5DECC02FarXiv:2410.03878v2[cs.CV]
Integrating the 3D world into large language models (3D-based LLMs) has been a promising research direction for 3D scene understanding.However, current 3Dbased LLMs fall short in situated understanding due to two key limitations: 1) existing 3D datasets are constructed from a global perspective of the 3D scenes and lack situated context.2) the architectures of existing 3D-based LLMs lack explicit alignment between the spatial representations of 3D scenes and natural language, limiting their performance in tasks requiring precise spatial reasoning.We address these issues by introducing a scalable situated 3D dataset, named Spartun3D, that incorporates various situated spatial reasoning tasks.Furthermore, we propose Spartun3D-LLM, built on an existing 3D-based LLM but integrated with a novel situated spatial alignment module, aiming to enhance the alignment between 3D visual representations and their corresponding textual descriptions.Experimental results demonstrate that both our proposed dataset and alignment module significantly enhance the situated spatial understanding of 3D-based LLMs.</p>
<p>INTRODUCTION</p>
<p>Situation1: You are standing beside a trash bin while there is a toilet in front of you.</p>
<p>Question：what should you do to wash hands?Recent advances in large language models (LLMs) have demonstrated their remarkable reasoning and communication capabilities across various tasks and modalities (Achiam et al., 2023;Alayrac et al., 2022;Zhang et al., 2023a;Rubenstein et al., 2023).Building on these breakthroughs, there has been a growing interest in extending LLMs to the 3D world (3Dbased LLMs) (Hong et al., 2023;Huang et al., 2023;Chen et al., 2024;Zhen et al., 2024;Wang et al., 2023).Existing studies mainly focus on integrating various 3D scene representations into LLMs, enabling the models to perform 3D grounding and spatial reasoning through natural language.For example, 3D-LLM (Hong et al., 2023) utilizes multi-view images to represent 3D scenes, pioneering a new direction in this field, while LEO (Huang et al., 2023) further pushes the boundary by directly injecting 3D point clouds into LLMs, aiming to develop a generalist embodied agent capable of 3D grounding, embodied reasoning, and action planning.</p>
<p>Despite the promising progress, current 3D-based LLMs still fall short in situated understanding, a fundamental capability for completing embodied tasks, such as Embodied Question Answering (Das et al., 2018), Vision and Language Navigation (Anderson et al., 2018;Zhang et al., 2024c), robotic manipulation (Shridhar et al., 2022), and many others.Situated understanding refers to the ability to A: In front, there's a rectangular box and a big picture that can be hung or moved.Nearby, there's a tall nightstand beside the bed.To the right, there's a tall pillow close to another pillow and a blanket.</p>
<p>Behind, a polished mirror and a flexible bag are near a closed door with hanging clothes.On the left, there's an artificial lamp and a desk close to a sofa chair and trash bin, a curtain, another lamp, and a tall wardrobe near a cabinet.</p>
<p>Situated QA</p>
<p>Other Views</p>
<p>Figure 2: Examples of Spartun3D.Green box and arrow show the standing point and orientation.interpret and reason about a 3D scene from a dynamic egocentric perspective, where the agent must continuously adjust understanding based on its changing position and evolving environment around it.This capability is crucial because an agent's reasoning and response to the same question can vary depending on its current situation.For example, as shown in Fig 1 , given the same question "What should you do to wash hands?", the agent might need to answer "use the sink on the left/right" based on the agent's current perspective and location relative to the "sink".</p>
<p>However, achieving such situated understanding remains challenging for current 3D-based LLMs, and we identify two primary reasons.First, most existing 3D datasets (Chen et al., 2021;Azuma et al., 2022;Zhu et al., 2023;Huang et al., 2023) are constructed from a global perspective of 3D scenes, lacking the situated information necessary for training models to reason from an agent's perspective.As a result, models fine-tuned on these datasets struggle to develop situated reasoning ability.While the introduction of SQA3D (Ma et al., 2022) has made progress by providing a situated 3D dataset, the dataset is mainly human-annotated, making it expensive and difficult to scale for the large-scale training needed by 3D-based LLMs.Second, existing 3D-based LLMs inject 3D representations into LLMs by simply concatenating tokens from different modalities (e.g., text, images, 3D point clouds).While this allows for basic cross-modal interaction, it lacks an explicit mechanism to align the situated spatial information from the 3D scene with natural language.Therefore, the models struggle to capture the critical spatial relationships for situated understanding.</p>
<p>To address the aforementioned issues, we propose two key innovations: we first introduce a scalable, LLM-generated dataset named Spartun3D, consisting of approximately 133k examples.Different from datasets used by previous 3D-based LLMs (Zhu et al., 2023;Huang et al., 2023;Hong et al., 2023), Spartun3D incorporates various situated spatial information conditioned on the agent's standing point and orientation within the environment, consisting of two situated tasks: situated captioning and situated QA.Situated captioning is our newly proposed task that requires generating descriptions of the surrounding objects and their spatial direction based on the agent's situation.Situated QA is designed with different types of questions targeting various levels of spatial reasoning ability for embodied agents.Furthermore, based on Spartun3D, we propose a new 3D-based LLM, Spartun3D-LLM, which is built on the most recent state-of-the-art 3D-based LLM, LEO (Huang et al., 2023), but integrated with a novel situated spatial alignment module that explicitly aligns 3D visual objects, their attributes and spatial relationship to surrounding objects with corresponding textual descriptions, with the goal of better bridging the gap between the 3D and text spaces.</p>
<p>We conduct extensive experiments across a variety of tasks, including Spartun3D, SQA3D, and MP3D Nav (Savva et al., 2019).Our results demonstrate that our model, trained on Spartun3D, exhibits strong generalization to other tasks, specifically in zero-shot settings, highlighting the effectiveness of our proposed dataset.Additionally, Spartun3D-LLM outperforms the baseline on nearly all tasks, indicating that incorporating direct text supervision improves the model's spatial understanding ability.This is further supported by our observation that the explicit alignment module improves the generation of more fine-grained, context-aware spatial information.</p>
<p>RELATED WORK</p>
<p>Situated Scene Understanding is essential for various embodied tasks, including embodied QA (Das et al., 2018;Wijmans et al., 2019), vision-and-language navigation (Anderson et al., 2018 Zhang et al., 2021;Zhang &amp; Kordjamshidi, 2022a;b;2024) and robotic manipulation (Shridhar et al., 2022;Jiang et al., 2022;Driess et al., 2023).Recently, SQA3D (Ma et al., 2022) introduces a human-annotated dataset where the model generates answers based on questions and given situations.SIG3D (Man et al., 2024) highlights the situated awareness via a situation-grounded 3D VL reasoning architecture.While SQA3D is crucial in 3D vision-language learning (Zhu et al., 2023;Huang et al., 2023), its reliance on human annotations makes it costly and difficult to scale for the large-scale training required by 3D-based LLMs.In contrast, we use the LLM to design an automated pipeline to generate situated questions and answers, enhancing both the scalability and diversity in data collection procedure.</p>
<p>Grounded 3D Scene Understanding.Compared to 2D vision-language tasks (Song et al., 2024;Antol et al., 2015;Zhang et al., 2024a;Guo et al., 2024a;b;Xu et al., 2024;Wang et al., 2024;Qi et al., 2024;Guo et al., 2025), 3D scenes introduce additional dimensions of knowledge that is more challenging to align with text modalities.Early studies in this area primarily focused on grounding language to individual objects within 3D environments (Achlioptas et al., 2019;2020;Chen et al., 2020;2019;2021).Recently, 3D Vista (Zhu et al., 2023) proposes a pre-trained VL Transformer for 3D vision and text alignment, and a few works utilize LLMs in understanding 3D scenes (Hong et al., 2023;Huang et al., 2023;Chen et al., 2024;Zhen et al., 2024;Yang et al., 2024).However, these works are less effective in situated understanding tasks that require generating answers from the agent's dynamic perspectives.MSQA (Linghu et al., 2025) is a concurrent work that also explore situated understanding of 3D-based LLMs.However, our approach differs in both dataset construction and model design.While MSQA focuses on leveraging images to assist in describing a situation, our method emphasizes directly understanding the situation from textual descriptions, leading to distinct contributions in the field.</p>
<p>SPARTUN3D DATASET CONSTRUCTION</p>
<p>To better equip 3D-based LLMs with the capability of understanding situated 3D scenes, we introduce Spartun3D, a diverse and scalable situated 3D dataset.To ensure the scalability of Spartun3D, we carefully design an automatic pipeline that leverages the strong capabilities of GPT-4o (OpenAI, 2024), with three key stages as shown in Fig. 3: (1) Designing diverse situations that specify the agent's standing point and orientation given a 3D scene as input (Sec.3.1); (2) Constructing situated scene graphs to describe the spatial relationships between the agent and objects in the environment conditioned on the agent's situations (Sec.3.2) ; and (3) Prompting LLMs to generate dataset based on situated scene graphs (Sec.3.3).</p>
<p>SITUATION DESIGN</p>
<p>The 3D scenes in Spartun3D are taken from 3RScan (Wu et al., 2021), which provides a diverse set of realistic 3D environments.Given a particular 3D scene with all the objects labeled by humans from 3RScan, such as the example shown in Fig. 3, our first step is to generate diverse situations for the agent.To construct the situation, we begin by identifying the standing point and orientation and then complete a situation description accordingly using the following template: "You are standing beside {pivot object name}, and there is {referent object name} on the {left/right/front/backward}."</p>
<p>The elements within {} specify the key components that together define the situation.Below, we define the agent's standing point and orientation and explain how these elements are obtained to construct diverse and reliable situations.Standing Point and Orientation.We begin with determining the agent's standing point and orientation within the 3D scene.Our approach is to place the agent beside an object, ensuring a clear reference for orientation when interacting with the environment.Specifically, we project all objects from 3D space onto a 2D plane, focusing only on the x and y coordinates to construct a bird-eyeview of the scene.From this projected 2D scene, we randomly select an object from the set of segmented objects within the 3D scene.To ensure the constructed situation remains realistic, we exclude objects that are positioned too high to avoid unnatural situations like "standing beside the lamp on the ceiling".As a result, we limit the selection to objects whose z-axis is below the average height of all objects in the scene.Then, we choose a midpoint from two sides of the selected object's bounding box that are closest to the center of the scene, as shown in Fig. 4. By prioritizing the side closest to the center, we minimize this risk and keep the agent within the scene's boundaries.Finally, the selected midpoint will be used as the agent's standing point.In addition, we need to determine the agent's orientation.We assume the agent's orientation is always facing forward to the center of the selected object.This guarantees that the selected object remains within the agent's field of view.</p>
<p>Pivot and Referent Object.Once the agent's standing point and orientation are determined, we refer to the object that the agent stands beside as the "pivot object", and other objects surrounding the pivot object are potential referent objects.A referent object is then randomly selected, and its relative position (left/right/front/backward), with respect to the agent's standing point and orientation, is used to generate the description of the situation.</p>
<p>SITUATED SCENE GRAPH CONSTRUCTION</p>
<p>Building on the agent's situation, we further construct a situated scene graph that captures the comprehensive spatial relationships between the agent and its surrounding objects.Existing 3D-based LLMs (Zhu et al., 2023;Huang et al., 2023) represent scenes in a structured manner using JSONformatted scene graphs, including detailed scene context of object attributes and relative spatial relationships between objects.However, their spatial relations are based on a global view, such as a bird-view-eye perspective (as shown in Fig. 5).To enable situated understanding, we introduce a situated-scene-graph adapted from the original global scene graph to capture all relative spatial relationships between the agent's standing point and surrounding objects as follows:</p>
<p>• Rotation Angles.We calculate rotation angles that reorient the agent from its orientation to the surrounding objects.Specifically, we first calculate the horizontal angle between the standing point and the center of the pivot object.Next, we calculate the horizontal angle between the standing point and a surrounding object.The rotation angle is determined by the difference between these two angles.We further normalize the rotation angles such that larger values correspond to a greater degree of rightward rotation.• Direction.We classify the object's rotation angles to the agent into four directional categories according to a predefined standard: [front, right, backward, left] (see Fig. 5 (b)).For instance, an object is categorized as "right" if the turn angle falls within the range of  degrees relative to the agent's forward-facing orientation.• Distance.We compute the Euclidean distance between the agent's standing point and the center of the bounding boxes of surrounding objects.• Passby Objects.We assess whether the agent can move freely from its standing point to other objects.We draw a straight line from the agent's standing point to the center of the referenced object.If this line intersects any other objects in the scene, those objects are considered "passby objects".For example, as illustrated in Fig 5 (d), the "table" is a passby object between the agent and the "kitchen cabinet".We explictly include the information of passby objects to help the agent build awareness of objects that might influence its path while navigating.In this example, the pivot object is the "sofa", the referent object is the "TV", and the surrounding objects include the "table" and "cabinet".</p>
<p>After gathering the spatial information described above, we organize it into a JSON format (shown in Fig. 5 (e)), which is then used as input to prompt LLMs to generate our datasets.</p>
<p>LLMS PROMPTING</p>
<p>We design specific instructions to prompt GPT-4o (OpenAI, 2024) for two situated tasks: Situated Captioning and Situated QA.For both tasks, we ask GPT-4o to provide responses considering situated spatial information.Detailed prompts for each task are provided in Tab. 10 in the Appendix, and examples of the generated dataset are shown in Fig. 2.</p>
<p>Situated Captioning is our newly introduced task, aiming to generate brief situated descriptions of the surrounding objects as the agent performs a 360 • clockwise rotation starting from its standing point and orientation.The motivation for introducing this task stems from its crucial role in embodied tasks, such as navigation, where the agent must interpret and reason about its environment from 360 • panoramic views to make decisions about movement and interaction (Zhou et al., 2024;Zhang &amp; Kordjamshidi, 2023;Zhang et al., 2024b).Therefore, we guide GPT-4o to generate descriptions progressively, starting from lower rotation angles and moving toward higher angles in each direction.</p>
<p>Situated QA.We design three types of questions for the Situated QA task, each targeting a different aspect of spatial reasoning for embodied agents.Unlike previous works that rely on a single generic prompt for all question types, we develop tailored prompting strategies for each question type, encouraging LLM to generate QA pairs focusing on different levels of reasoning.</p>
<p>• Object Attribute and Relations include questions about objects attributes, such as color, shape, and size, while also incorporating situated spatial information.For instance, the questions to identify "the color of the table positioned to the left", and determine "how many pictures are hanging on the wall to the right".</p>
<p>• Object Affordance focuses on the function utility of the objects, often based on common sense knowledge about how objects are used.Similarly, we require situated spatial information to be part of the answer.For example, when asked "Where can you check your appearance?", the correct answer should be "mirror on your left", specifying both the object name (mirror) and its spatial location from the agent.</p>
<p>• Situated Planning is the most challenging task, as it requires the agent to perform multi-hop situated spatial reasoning.The agent must not only recognize its surroundings but also plan and execute a series of actions across multiple steps, where each subsequent action depends on the outcome of the previous one.In our dataset, we implement 2-hop reasoning, which requires the agent to perform a sequence of two continuous actions.For example, given the example in Fig. 2, "make the room brighter and then sit down to relax.", the agent needs to first turn left from its orientation to face and move toward the window, open it to brighten the room, then based on its new position, the agent continues turning left toward the sofa chairs and sits down.In total, we collect approximately 10k situated captions and 123k QA pairs.For the tasks of object attribute and relation and the tasks of affordance, we sampled around 10 situations per scene.For captioning and planning tasks, we sample around 5 situations per scene due to the increasing cost of longer token sequences required for these tasks.For each task, we split the data instances into a training and test set.Table 1 shows the statistics of our dataset.We conduct human evaluation to assess the quality of Spartun3D, introducing scores based on two key criteria: language naturalness, which evaluates whether the text reads as if it were naturally written by a human, and spatial fidelity, which ensures that the data accurately reflects the 3D scene with correct spatial relationships.Detailed explanations of the criteria are in Sec.A.1.Each criterion is rated on a scale from 1 to 5, and the average of these two scores is the overall human score.We randomly select 50 examples from each task and compute human scores of situation, question, and answer, respectively.As shown in Fig. 6 (a), the average scores align with the complexity of each task, with relatively lower scores for captioning and planning tasks.To evaluate how our generated data compares to human-annotated data, we sampled 50 examples from SQA3D and combined them with our dataset.Our data shows a similar trend in human evaluation results across different question types as observed in SQA3D (shown in Fig. 6 (b)).</p>
<p>We finally evaluate how different prompting strategies influence the quality of the data.We experiment with two types of prompts for representing spatial information to prompt GPT-4o: Cordprompt, which consists of object center coordinates, standing point, orientation, and instructions for calculating distances and rotation angles, and Spa-prompt, consisting of the calculated angles and distance based on the approaches we described in Sec.3.3.An example of each type of prompt can be found in Tab. 11. Fig.6 (c) shows the percentage of examples with high human scores (≥ 4) for each prompt across tasks.The results indicate that Cord-prompt yields unsatisfactory results, revealing that LLMs lack strong 3D spatial reasoning when interpreting raw spatial coordinates, which is consistent with their struggles in spatial reasoning across text and 2D images (Zhang et al., 2024d;Premsri &amp; Kordjamshidi, 2024;2025).Our Spa-prompt significantly improves the quality of the generated dataset by providing qualitative spatial relations (e.g.distance, direction).</p>
<p>MODEL ARCHITECTURE</p>
<p>In addition to enhancing the situated understanding of 3D-based LLMs with Spartun3D, we also propose a new 3D-based LLM, named Spartun3D-LLM, which integrates a novel Situated Spatial Alignment module to strengthen the alignment between the situated 3D visual features and their corresponding textual descriptions.Spartun3D-LLM is built upon LEO (Huang et al., 2023), which represents the most recent and state-of-the-art 3D-based LLM, and directly takes 3D point cloud data as input, making it well-suited for spatial reasoning tasks in 3D environments.Fig. 7 illustrates the overview architecture of Spartun3D-LLM.</p>
<p>BACKGROUND</p>
<p>Problem Formulation.We formally define the input as a triple &lt; C, S, Q &gt;, where C is the 3D scene context, S is the situation, and Q is a question.The situation S can be further denoted as S =&lt; S t , S p , S r &gt;, where S t is a textual situation description, and S p and S r are the standing points and orientation, respectively.Specifically, S p is a 3D coordinate in the form &lt; x, y, z &gt; and S r is the quaternion &lt; qx, qy, qz, w &gt;, where &lt; qx, qy, qz &gt; is the rotation axis and w is the rotation angle.For simplicity, we define z = 0 to calculate the rotation angle on a 2D plane.The task is to generate a textual answer, denoted as A, given scene context C, situation S, and question Q.During training, S p and S r are provided to the agent to rotate and translate the environment, while during testing, only questions and situations are provided.where N is the number of tokens in the response.The model's objective is to generate the answer given these combined inputs.The loss for generating the i-th token of the output answer is formulated as follows:
! # ! " ! ! ! # ! ! ! " " # " ! " "</p>
<p>Situated
L LM (θ) = i logp θ (a i |a i−1 , W S , o).(1)
LEO can integrate various LLM backbones, including OPT1.3B (Zhang et al., 2023b) and Vicuna-7B (Chiang et al., 2023).In our experiments, we fine-tune LEO with different LLM backbones on our proposed dataset via LoRA (Hu et al., 2021).</p>
<p>SITUATED SPATIAL ALIGNMENT MODULE</p>
<p>Situated tasks require robust spatial reasoning abilities to comprehend the position, orientation, and spatial relationships of objects within a 3D environment.Existing 3D-based LLMs typically process inputs by concatenating output representations from various modality encoders.While this method facilitates the integration of data across different modalities, it does not inherently ensure that the 3D visual representations encode situated spatial information or effectively align with textual descriptions, which potentially limits the model's ability to perform tasks that require precise spatial understanding.To tackle this challenge, we introduce a novel Situated Spatial Alignment Module to improve the alignment between the object-centric 3D visual representations and their situated textual descriptions.The process begins by generating detailed situated textual descriptions for each object.Subsequently, an alignment loss is introduced, which directs the model in effectively learning the 3D visual representations based on these situated textual descriptions.</p>
<p>Situated Textual Descriptions.</p>
<p>For each object, we construct a comprehensive situated textual description based on a template that captures the object's name, attributes, and spatial relations with nearby objects, as "Stand besides {object name} and facing the center of the {object name}, in front, there are {a list of nearby objects}; on the right, ...; behind ...; and on the left...".The object's attributes are also considered (e.g., "white chair").We consider up to five objects per direction.If no object is present in a specific direction, the description explicitly states this, ensuring to provide complete information about the 3D environment.3D Object-Text Alignment.Inspired by the success of 2D Visual-Language models, which effectively leverage semantically aligned text and visual features to excel in downstream tasks (Radford et al., 2021;Li et al., 2022;2023), we aim to enhance the 3D visual representations so that they can better encode the situated spatial information and effectively align with the textual descriptions.Specifically, we introduce a 3D object-text alignment loss to guide the learning process of point cloud encoders within 3D-based LLMs, leveraging the robust language representations captured by pre-trained text encoders.We experiment with various text encoders, and CLIP achieved the best performance.For more details, please refer to the Sec.A.4 in the Appendix.</p>
<p>More formally, we obtain the text representations of situated textual description for each object from pre-trained text encoders, denoted as W = [w 1 , w 2 , ..., w k ] ∈ R K×D .For object visual representations O, we employ spatial self-attention layers (Chen et al., 2022) to learn spatial-aware object representations.Specifically, a pairwise spatial feature matrix F ∈ R K×K×5 is introduced to represent relative spatial relations between objects.For example, for each object pairs o i and o j , we construct pairwise spatial feature as
f ij = [d ij , sin(θ h ), cos(θ h ), sin(θ v ), cos(θ v )] ∈ R 1×5
, where d ij is Euclidean distance between two objects, and θ h and θ v are horizontal and vertical angles connecting bounding box centers of o i and o j , respectively.Then, we inject F into the self-attention of the object as,
O ′ = softmax( QK T √ d h + MLP(F))V, where Q = W Q O; K = W K O, V = W V O,(2)
where O ′ ∈ R K×D denotes the spatial-aware object representation; Then we use a Mean Squared Error (i.e., MSE) as the objective function to minimize the distance between the object representation O ′ and the corresponding situated textual embedding W, denoted as L align = MSE(o ′ , w t ).The model is trained to jointly optimize both the alignment loss and the language modeling loss (Eq. 1) as L = L LM + L align .</p>
<p>EXPERIMENT</p>
<p>EXPERIMENTAL SETUP</p>
<p>To demonstrate the effectiveness of our proposed Spartun3D-LLM, we conduct experiments on two situated understanding datasets: Spartun3D1 and SQA3D.For SQA3D, we evaluate under two conditions: object proposals in 3D are derived either from Mask3D (Schult et al., 2023) or groundtruth annotations.Also, we assess the transferability of our method on the navigation task using MP3D ObjNav (Savva et al., 2019).Following LEO (Huang et al., 2023), we report the performance using standard generation metrics, including CIDEr, METEOR, BLEU-4, and ROUGE L, sentence similarity (Reimers, 2019) for captioning task.For SQA3D and situated QA tasks of questions about attributes and relations, we also report an additional metric of exact-match accuracy.More details about metrics are introduced in Appendix A.2.1.We also provide implementation details in Sec.A.3.We leverage LEO as baseline.Since the training stage in LEO has covered most of the evaluation tasks, we experiment with models initialized from scratch to ensure a fair comparison in the zero-shot setting.For other settings, we report the performance of models initialized both from scratch and from the instruction-tuned LEO.To distinguish between the two, models initialized from the instruction-tuned LEO are marked with an asterisk ( * ).</p>
<p>EXPERIMENTAL RESULTS</p>
<p>Spartun3D</p>
<p>Benchmark.We evaluate the performance of both the LEO model and Spartun3D-LLM after fine-tuning them on our proposed Spartun3D dataset.The fine-tuned LEO model is referred to as LEO+Spartun3D.Table 2 and Table 4 show the experimental results on captioning and QA tasks, respectively.We experiment with two different LLM backbones: Opt1.3B and Vicuna7B.</p>
<p>Our experiments show that Spartun3D-LLM consistently outperforms LEO+Spartun3D across all question types (around 2%−3% across all metrics), regardless of the LLM backbone used, indicating the effectiveness of our explicit alignment module.We observe that initializing our model with LEO pre-trained weights improves performance.Notably, without fine-tuning, LEO performs reasonably well on attribute and relation questions in a zero-shot setting but struggles with other situated tasks.SQA3D Performance.We evaluate our method on the SQA3D dataset, whose scenes are derived from ScanNet (Dai et al., 2017).Their scenes differ from those in Spartun3D, which are sourced from 3RScan.We experiment with two settings: zero-shot and fine-tuning.In the zero-shot setting, we re-trained LEO on their dataset (row#1) only constructed from 3RScan excluding all dataset constructed from ScanNet to ensure a fair comparison with our method.As shown in Table 3, LEO performs poorly on SQA3D in the zero-shot setting, suggesting its limitations in learning situated understanding from its dataset.In contrast, LEO trained on Spartun3D (row#2) shows significant improvement, demonstrating the effectiveness of our dataset.Further comparisons of Spartun3D-LLM with LEO+Spartun3D demonstrate a better zero-shot learning (i.e., generalization) capability of our model.In the fine-tuning setting, Spartun3D-LLM continues to outperform LEO across all metrics.Table 8 in the Appendix provides a breakdown of the fine-tuned performance across various question types, where Spartun3D-LLM shows consistent improvement.Navigation Performance.To demonstrate the effectiveness of our approach on downstream embodied tasks, we evaluate it on the object navigation tasks.Specifically, we randomly select 5 scenes that contain around 1000 examples from the MP3D ObjNav dataset.In this task, we additionally input 2D ego-centric images to both LEO and Spartun3D-LLM for comparison.There are four types of navigation actions: turn left, turn right, move forward, and stop.We evaluate whether the model generates correct action at each step.We conduct the experiment in a zero-shot setting, and Table 5 shows the accuracy of the model's performance.The baseline model, LEO, struggles to generate the required action-related text to guide navigation steps without fine-tuning specifically for navigation tasks.In contrast, our model demonstrates strong transferability to generate correct actions.</p>
<p>ABLATION STUDY AND EXTRA ANALYSIS</p>
<p>Explicit Alignment Enhances General Spatial Understanding.We evaluate the effectiveness of our proposed situated spatial alignment module on general scene understanding tasks, such as Scan2Cap (Chen et al., 2021) and ScanQA (Azuma et al., 2022).In line with our approach for  Improved Situated Understanding.To analyze the model's situated understanding ability further, we visualize the distribution of model responses generated for questions requiring strong spatial understanding from SQA3D.Specifically, we extract questions starting with "which direction".Fig. 10 illustrates the distribution of generated "directions", including "left", "right", "forward" and "backward".We observe that LEO is biased towards generating "left" 97% of the time.However, the ground-truth distribution of "left" and "right" should be balanced, suggesting that LEO may have a limited understanding of situated spatial relationships.The bias is significantly mitigated when LEO is trained on our dataset (LEO+Spartun3D).While adding our alignment loss (Spartun3D-LLM) helps futher, our dataset is the primary factor in addressing the bias.</p>
<p>Scaling Performance.We conduct scaling experiments to demonstrate how model performance improves with the addition of Spartun3D datasets.As shown in Fig. 9, we evaluate performance on SQA3D and observe consistent improvement as the dataset scales, highlighting the potential for dataset expansion using our proposed method.</p>
<p>Qualitative Examples.In Fig. 8, we showcase several successful examples to demonstrate the effectiveness of Spartun3D-LLM across various situated tasks.Notably,in Fig 8 (c), the model without an explicit alignment module tends to generate more general or vague spatial descriptions, such as "turn around".In contrast, with the alignment module, the model produces more specific details, including terms like "turn slightly right".To verify this, we examine 30 examples from both situated planning and situated captioning tasks and observe this phenomenon in 17 of them.This highlights how the proposed spatial alignment module enhances the generation of fine-grained spatial information, leading to more precise and contextually accurate outputs.</p>
<p>DISCUSSION AND CONCLUSION</p>
<p>In this work, our goal is to address the limitation of situated understanding of the 3D-based LLMs from two perspectives.First, we propose a method to construct an LLM-generated dataset based on our designed situated scene graph.Then, we propose an explicit situated spatial alignment on the 3D-LLM to encourage the model to learn alignment between 3D object and their textual representations directly.Finally, we provide comprehensive experiments to show our own benchmark improve situated understanding of SQA3D and navigation.We also provide analysis to show our proposed explicit alignment module helps spatial understanding.</p>
<p>A APPENDIX SECTION</p>
<p>A.1 HUMAN EVALUATION Human Scores.To evaluate the quality of Spartun3D based on human scores, we mainly consider the following two aspects.</p>
<p>• Language Naturalness evaluates the flow and syntax of the language, ensuring it reads naturally as if written by a human and adheres to common-sense knowledge in practical scenarios.Examples of different scoring levels for situations, questions, and answers are provided in Table 7.For instance, a score of 1 might be assigned to a scenario like "Standing beside a blanket while there is a toilet in the background," which is uncommon in a typical setting.Similarly, questions such as "Which is cleaner, the trash bin or the bed?" are unlikely to be asked by humans, reflecting low language naturalness.</p>
<p>• Spatial Fidelity is critical to ensure that the generated data accurately represents the information in 3D scenes and adheres to factual details.This includes verifying the presence of objects within the scene and ensuring that spatial relationships between objects are correctly represented.Additionally, common sense knowledge must be considered, especially when unusual objects or scenarios are mentioned in the questions or answers.For example, as shown in Fig. 12, a score of 1 is assigned to the instance where "clothes are hung on a mirror."This error arises because the human-annotated data from 3RScan labeled the mirror's affordance as "hanging," which misled the GPT model into generating an incorrect dataset.</p>
<p>Error Analysis.We randomly sampled 50 examples from each task (200 in total) to validate the quality of our automatically generated data and manually assess the quality from the aspects of language naturalness and spatial fidelity.Language naturalness evaluates whether the generated texts are natural or written by a human, while spatial fidelity ensures that data accurately reflects the 3D scene.We observe 26 errors in total and summarize them into the following categories.</p>
<p>• Semantic Errors: The generated sentences may contain semantic mistakes.For instance, the answer " You should go in front of you to water the potted plant.".</p>
<p>• Common Sense Violations: The generated content may occasionally conflict with basic common sense knowledge, such as producing unusual questions and answers.For example, it might generate a question like "If I want to store items, which object is most accessible?" with the answer being "trash bin."This issue arises because the human-annotated data includes an affordance for trash bins as objects for storing items.Such annotations inadvertently influence GPT-4o to generate QA pairs that conflict with common sense knowledge.</p>
<p>• Spatial Inconsistencies: Errors in capturing or reasoning about spatial relationships in the 3D environment primarily occur in Situated Planning tasks, which demand complex twostep spatial reasoning.These errors often arise because the second action depends on the outcome of the first action, and inaccuracies sometimes occur during the generation of the second action.</p>
<p>• Misalignment between visual context and textual descriptions.In some cases, the agent's view is obstructed due to the room layout or object size.For example, consider the situation: "Standing beside the sofa, there is a closet on your left."However, the closet is actually located in another room and cannot be seen from the agent's current standpoint.</p>
<p>To address this issue, we designed a scenario where the agent stands beside a pivot object and consistently faces the center of the pivot object, rather than facing a random object that could potentially be obstructed.Additionally, we incorporated pass-by spatial information to enhance the agent's awareness of surrounding objects, providing a more comprehensive sense of the environment.You are standing beside the nightstand while there is a picture on your left.</p>
<p>How many sofa chairs are to my left?</p>
<p>You can use the lamp behind you, but be careful of the stools you will pass by. 4</p>
<p>You are standing beside a lamp while there is a box on your left.</p>
<p>Is the bag closer to me than the trash bin behind me?</p>
<p>You can place items on the cabinet to your left, which is the closest option.Alternatively, you can place them on the bed to your left.3</p>
<p>You are standing beside the radiator while there is a curtain rod on your back.</p>
<p>I want to read a book.Should I walk to the window or sit in the chair?</p>
<p>You can go to the clothing hanging on the door behind you.</p>
<p>2 You are standing beside salt while there is a kitchen cabinet on your left.</p>
<p>What are the characteristics of the window to my left?</p>
<p>Turn slightly back to your right and head towards the lamp to move it closer to your bed.You may pass by the curtain rod. 1</p>
<p>You are standing beside a blanket while there is a toilet on your back.</p>
<p>Which is cleaner, the trash bin behind me or the bed to my left?</p>
<p>You should go in front of you to water the potted plant.</p>
<p>A.2 EXPERIMENTAL RESULTS</p>
<p>A.2.1 EVALUATION METRICS</p>
<p>• CIDEr (Consensus-based Image Description Evaluation): A metric that measures the similarity between generated and reference descriptions by assessing n-gram overlap, tailored for image captioning but applicable in NLP for structured comparisons.</p>
<p>• METEOR (Metric for Evaluation of Translation with Explicit ORdering): A metric that evaluates machine translation and other language generation tasks, considering precision, recall, and harmonic mean, while accounting for synonyms and stemming.</p>
<p>• ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation, Longest Common Subsequence): This metric focuses on the longest common subsequence between generated and reference texts, providing insight into recall and precision in longer sequences.</p>
<p>• EM: An evaluation metric that checks whether the predicted output matches the reference exactly, often used in tasks like question answering.</p>
<p>A.3 IMPLEMENTATION DETAILS.A.4 EXTRA EXPERIMENTAL RESULTS</p>
<p>Text Encoder EM</p>
<p>BERT (Reimers, 2019) 53.5 BLIP (Li et al., 2022) 54.2 CLIP (Radford et al., 2021) 56.9</p>
<p>Table 9: Different Text Encoder Evaluation.</p>
<p>Different Text Encoder.We conducted an ablation study to evaluate the impact of different text encoders on enhancing 3D visual representations.Table 9 shows the results on object attribute and relation tasks.Our results indicate that text encoders from VL models outperform pure text encoders, highlighting the importance of multimodal learning for 3D object understanding.Among the tested encoders, the CLIP encoder yields the best results.</p>
<p>3D LLMs:Figure 1 :
1
Figure 1: Illustration of situated scene understanding of Spartun3D-LLM compared to other 3D-based LLMs.</p>
<p>Situation:</p>
<p>Standing beside blue octagon wide bed that is messy while there is a window on the leftDescribe the scene from your current position.</p>
<p>Figure 3 :
3
Figure 3: Illustration of Spartun3D Dataset Construction Process.Given a 3D scene: a) we first select a pivot object (e.g., sofa) and a referent object (e.g., table) to define the situation; b) we create situated scene graph based on situation, incorporating various spatial relationships (See Fig. 5); c) we use the scene graph to prompt GPT-4o (shown in gray box) to generate data; e) we utilize different prompting strategies for generating situated captions and QA pairs.</p>
<p>Front</p>
<p>Figure 4: Standing Point and Orientation Selection.</p>
<p>Figure 5 :
5
Figure5: Spatial information in Situated Scene Graph.The red dot and green arrow show the standing point and orientation, respectively.In this example, the pivot object is the "sofa", the referent object is the "TV", and the surrounding objects include the "table" and "cabinet".</p>
<p>Figure 6 :
6
Figure 6: Human Evaluation of Spartun3D.3.4 DATASET STATISTICS AND QUALITY EVALUATION</p>
<p>Figure 7 :
7
Figure 7: Spartun3D-LLM Model Architecture.Backbone.LEO takes text, 2D image (optional), and 3D point clouds as input and formulate comprehensive 3D tasks as autoregressive sequence generation.Specifically, data from different modalities are converted into a sequence of tokens as input to the LLM.The text tokens include system messages (e.g., "You are an AI visual assistant situated in a 3D scene."),situations, and questions.These tokens are then embedded into vector representations using an embedding look-up table.For 3D point clouds, LEO first applies segmentation masks to extract the point clouds of individual objects in the 3D scenes.Then, the sampled points of each object are input into a object-centric point cloud encoder, PointNet++(Qi et al., 2017) pre-trained on ScanNet(Dai et al., 2017), to obtain the object-level representations.Formally, we denote the representation of input text tokens as W = [w 1 , w 2 , ..., w M ] ∈ R M ×D , where M denotes the number of input tokens, and D represents the dimensionality of each token's embedding.Additionally, the input object visual representations are expressed as O = [o 1 , o 2 , ..., o K ] ∈ R K×D , where K is the number of extracted objects from the scene.Finally, the output answer are represented as A = [a 1 , a 2 , ..., a N ], where N is the number of tokens in the response.The model's objective is to generate the answer given these combined inputs.The loss for generating the i-th token of the output answer is formulated as follows:</p>
<p>Fig 8 (e) showcases a qualitative example, illustrating how our model effectively generates accurate navigation actions without task-specific fine-tuning.</p>
<p>LEO:Figure 8 :
8
Figure 8: Qualitative Examples.(a), (c), (c) situated qa examples of object attribute and relation, affordance, and planning, respectively.(d) situated captioning.(e) navigation in a zero-shot setting.Our model generates both actions and descriptions of surrounding objects while navigating.situatedtasks, we construct textual descriptions for each object based on its attributes and spatial relations to others from a top-view perspective.As shown in Tab.6, by incorporating the explicit spatial alignment module, our model shows better results, indicating that our proposed alignment module not only improves situated understanding but also enhances general 3D scene understanding.</p>
<p>Figure 9 :
9
Figure 9: Scaling Effect on SQA3D.</p>
<p>Figure 11: Spartun3D Word Distribution.</p>
<p>Figure 12 :
12
Figure 12: Examples to Evaluate Spatial Fidelity.</p>
<p>Describe the objects on your [left/right/front/Backward] from the lowest to highest angle.
3D Scenea) Situation DesignSofa</p>
<p>Table Cabinet
Cabinet
b) Situated Scene Graph e) Situated Tasks Sofa: { Attribute:{'color':'yellow'} Situation relation':[ 'Backwards':{'cabinet': 'Affordance': store items.'Attribute':{'size':wide} 'Angle': 165 degree 'Distance': 1.5 'Passby': table}}</p>
<p>Table Table CabinetTable Passby
TablePassby</p>
<p>Table e )
e
Situated Scene Graph</p>
<p>Table 1 :
1
Dataset statistics of Spartun3D and human validation results.
Tasks# of ExamplesTrain/TestCaptioning∼ 10K8, 367/1, 350Attr. &amp; Rel.∼ 62K61, 254/8, 168Affordance∼ 40K35, 070/5, 017Planning∼ 21K19, 434/2, 819</p>
<p>Standing beside the large window and facing the center of the window…
EncoderTextPointNet++SpatialSelf-attention! " ! !TokenizerCVL Computer Vision Lab
2 Situation: I stand behind a chair facing the table and don't have a monitor on my table.Tokenizer Question: What is the first object in my left?T1:</p>
<p>Table 2 :
2
Experimental Results on Spartun3D Situated QA Tasks.<em> represents the model initialized with LEO instruction-tuned weights.[Keys: C: CIDER; B-4: BLEU-4; M: METEOR; R: ROUGE; Sim: Sentence Similarity; EM: Exact Match; Bold: best results].
ModelsLLMsAttributes and RelationsAffordanceSituated PlanningCB-4MREMCB-4MRSCB-4MRSLEOzero-shot 100.3 0.00 17.4 39.1 42.713.30.00 3.00 5.00 32.30.000.00 7.00 15.3 59.2LEO+Spartun3DOPT1.3B 121.3 Vicuna7B 125.4 10.1 22.1 46.7 52.1 238.9 32.1 24.4 55.0 68.3 242.1 46.5 35.2 63.1 84.3 7.0 20.1 45.3 47.7 224.6 30.6 24.9 53.2 66.9 229.7 44.8 32.1 60.9 83.8LEO</em>+Spartun3D Vicuna7B 129.2 10.4 23.0 48.1 53.2 211.3 32.1 24.6 55.0 67.8 247.1 47.5 36.2 65.1 85.8Spartune3D-LLMOPT1.3B 124.1 Vicuna7B 131.2 10.3 24.3 48.8 53.7 240.4 32.1 25.0 55.3 68.7 244.0 47.1 36.4 64.0 86.8 9.2 21.0 47.3 49.4 227.2 31.4 26.3 54.1 68.2 232.3 45.2 33.2 62.1 85.4Spartune3D-LLM* Vicuna7B 135.4 10.7 24.9 51.3 56.9 254.7 32.9 26.7 57.3 69.7 252.1 47.6 36.2 65.4 88.7</p>
<p>Table 3 :
3
Experimental Results on SQA3D given the 3D objects from Mask3D and Ground-truth.</p>
<h1>MethodsMask3D (Schult et al., 2023)GTCMREMCMREMZero-shot1 2LEO (Huang et al., 2023) LEO+Spartun3D14.2 82.36.4 14.28.2 32.812.4 34.715.3 83.16.7 15.28.6 33.713.9 35.93Spartun3D-LLM83.515.734.736.285.616.635.837.143D-Vista (Zhu et al., 2023)---48.5----5 3D-LLM (Hong et al., 2023)---50.2----Fine-tune6LEO (Huang et al., 2023)132.033.049.252.4132.334.351.452.57LEO<em>+Spartun3D134.034.652.253.5135.334.252.154.28Spartun3D-LLM</em>138.2 35.3 53.4 54.8 138.3 35.4 53.7 55.0</h1>
<p>Table 4 :
4
Experimental Results on Spartun3D Situated Captioning Task.
ModelsLLMsCB-4MRSLEOzero-shot 0.00 0.00 9.00 15.3 51.9LEO+Spartun3DOPT1.3B Vicuna7B5.9 6.715.3 17.7 31.2 67.3 15.8 18.7 32.3 70.4LEO+Spartun3D<em> Vicuna7B 14.1 17.2 22.6 32.1 76.3Spartun3D-LLMOPT1.3B Vicuna7B6.4 8.515.7 18.5 31.2 68.6 16.4 19.6 32.5 72.5Spartun3D-LLM</em> Vicuna7B 14.6 19.3 23.3 33.4 78.1</p>
<p>Table 5
5: Performance on Nav-igation. (Accuracy %)LEO Spartun3D-LLMZero-shot020.3</p>
<p>Table 6 :
6
Spatial Alignment Evaluation on Other Benchmarks.The metric is Sentence Similarity (%).
MethodsScan2Cap ScanQALEO + Spartun3D54.246.3Spartun3D-LLM55.748.6</p>
<p>Table 7 :
7
Examples to Evaluate Language Naturalness.</p>
<p>Table 8 :
8
(Zhu et al., 2023)) length and output length of LLM are both set to 256.For each 3D scene, we sample up to 60 objects with 1024 points per object.During training, the pre-trained 3D point cloud encoder and the LLM are frozen.We set rank and α in LoRA to be 16 and dropout rate to 3D-LLM(Hong et al., 2023)35.066.0 47.0 69.0 48.0 46.0 3D-Vista(Zhu et al., 2023)Exact Match Performance on SQA3D Across Various Question Types
WhatIsHow Can Which OthersCLIPBERT (Ma et al., 2022) 39.746.0 40.5 45.636.138.4SQA3D (Ma et al., 2022)31.663.8 46.0 69.543.945.334.863.3 45.4 69.847.248.5LEO (Huang et al., 2023)46.864.1 47.0 60.844.254.3Spartun3D-LLM49.4 67.3 47.1 63.445.456.6
https://github.com/zhangyuejoslin/Spartun3D
This project is partially supported by the Office of Naval Research (ONR) grant N00014-23-1-2417, the award No. 2238940 from the Faculty Early Career Development Program (CAREER) of the National Science Foundation (NSF) and the U.S. DARPA FoundSci Program #HR00112490370.. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Office of Naval Research.Published as a conference paper at ICLR 2025 Table10: Prompts for Different Tasks.Prompts for Situated Captioning: Provide a summary of a scene focusing on object types and attributes ON MY LEFT/RIGHT/FRONT/BACK.Describe the scene also considering common sense, such as how objects can be used by humans and human activities in the left part of the scene.The description should conform to the given scene information.You don't need to describe each object in the scene, pick some objects of the scene for summary.You can also summarize the room's function, style, and comfort level based on the arrangement and color of objects within the room.Your summary must be one paragraph, not exceeding 300 words.Don't use IDs of the objects in the summary.Don't use turn degrees or distance meters in the summary.Prompts for Object Attributes and Relations: Ask questions about object types, and counting.The questions related to attributes are better be asked when multiple objects contain the same attributes and the answer can be specified based on spatial relations.You can also ask about spatial positions between me and other objects or spatial relations between objects by comparing the angles.Based on 'relations' in the scene graph, you can ask about other relations between objects.Prompts for Object Affordance: Ask questions about object affordance and object utility based on common sense.The answer should consider the best option that follows common sense knowledge and is closer to me.If I plan to go to some objects, other objects are blocking my way; please specify them.There are several examples: Q: Where should I go to quickly put something down?A: You can use the chair in front of you.Q: I want to read a book.Should I walk to the window or sit in the chair?A: Sit in the chair, which is closer.Q: If I want to reach the kitchen counter, what object will be passed by? Answer: tables with chairs.Q: What should I do if I want to cook?A: You can go to the kitchen area, but be careful about the tables and chairs you will pass by.Prompts for Situated Planning: You need to generate 6 meaningful question-answer pairs that require multi-hop reasoning and planning based on the scene information.Ask questions about object affordance and object utility based on common sense and path planning.The question must be answered based on my position.If I plan to go to some objects, other objects are blocking my way, please specify them.The turn action should be considered based on angles if I plan to go to multiple places.Do not use the number of turn degrees or distance meters in the question and answer.There are several examples: 1. Question: I want to dim the lights and take a nap; What should I do?Answer: Turn to your right and head towards the lamp.Dim the lights, then turn slightly to your left and head towards the sofa to lie down.2. Question: I want to light up the area near the kitchen counter to prepre some food.How should I proceed?Answer: Turn slightly to your left and head towards the blinds on your left to adjust them.Then, turn slightly back to your right and head towards the kitchen cabinet in front of you.3. Question: I need to adjust the lighting to make the room brighter.What should I do?Answer: Turn to your left and head towards the lamp.Adjust the lighting.Then, turn slightly back to your right and ensure the curtains or blinds are open to let in more light.4. Question: I need to adjust the lighting to make the room brighter and then prepare a snack on the kitchen counter.How should I proceed?Answer: Turn to your left and head towards the lamp to adjust the lighting.After adjusting the lighting, turn slightly back to your right and head towards the kitchen counter.You may pass tables and chairs on your way.Published as a conference paper at ICLR 2025 General Prompts for Object Attribute and Relation: You need to generate at least 10 meaningful question-answer pairs based on the scene information.Ask questions about object types, and counting.The questions related to attributes are better be asked when multiple objects contain the same attributes and the answer can be specified based on spatial relations.You can also ask about spatial positions between me and other objects or spatial relations between objects by comparing the angles.Based on 'relations' in the scene graph, you can ask about other relations between objects.You need to provide the queried object.Do not consider the object's utility and affordance.Do not use the number of turn degrees or distance meters in the question and answer.Do not use the IDs of the objects in the question and answer.The question-answer pair should be following format: Q: <question>T: <queried object id(s)>A: <Answer>.You can answer the question according to the queried object(s).If there is no information about the question, the <Answer>should be "unkown".There are several examples: Q: What is the object closest to the left of me?T:lamp 1 Answer: a lamp.Q: How many stools are on my left?T:stool 3 A: One.Q: There are multiple chairs, what is the size of the chair left of me?T:chair 1, chair 2 A: low chair.Q: Is the cabinet far from me or the sofa far from me? A: sofa.Q: How many black objects are to my right?A: Two, a towel and a toilet brush.Q: Where is the trash bin?A: Behind you.Q: What color is the trash bin in front of me?A: black (one white left of me and one black in front of me) Q: Is the mirror right of the shower curtain based on my standing position?A: Yes.Q: Is the light on my right on or off?Q: What is the object to the left of the white heater to my right?Q: Is there a picture to my right?Q: Is the door in front of me the same color as the cabinet to my right?Q: The tv to your 11 o ' clock direction on; true or false ?Q: Can black objects are to my right be divided by three?Coordinate prompt: You are standing beside the white toilet 6, and the initial 3d coordinate is [-1.15,0.29, 0.48] toilet 6.You are facing the center of the toilet 6, and the center coordinate of toilet 6 is[-1.36, 0.28, 0.48].The scene contains some objects, which compose a scene graph in JSON format describing objects, such as object coordinate, color, size, shape, and state.You can calculate object distance and rotation angle related to your standing point and orientation using coordinates.If the rotation angle is in the range  is defined as Front,  isRIGHT,isBACK,is right.For example, from the scene graph "table 8": "coordinate": [1,1,1], "affordances": ["placing items on"], "attributes": "color":"red", "relations": ["close by chair 36"].We know that the coordinate of table 8 is [1,1,1], there is a table 8 that is close by chair 36.You can place items on this table 8.Spatial Prompt:You are standing beside a white toilet 6.The scene contains some objects, which compose a scene graph in JSON format with four keys: "left", "right", "front", "backwards", indicating objects in the corresponding direction.Each entity in the scene graph denotes an object instance with a class label and an object ID.The "distance" indicates the meters between you and the object.The "angle" represents the degrees compared to your current direction, where your direction in front is 0 degrees.The larger angles are means further right.The "affordance" is the motion activity related to this object.The "attributes" describe the object's characteristics, such as 'color' and "size".The "relations" describe the spatial relationships with other objects.The "passby" indicates other objects in your path if you walk toward it from your current position.For example, from the scene graph "Left":"table 8": "distance": 2.6, "passby": ["chair 21"], "affordances": ["placing items on"], "attributes": "color":"red", "angle": 257.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Shapeglot: Learning language for shape differentiation. Panos Achlioptas, Judy Fan, Robert Hawkins, Noah Goodman, Leonidas J Guibas, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part I 16</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Scanqa: 3d question answering for spatial scene understanding. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe, proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu, Chen , Angel X Chang, Matthias Nießner, European conference on computer vision. Springer2020</p>
<p>Text2shape: Generating shapes from natural language by learning joint embeddings. Kevin Chen, Christopher B Choy, Manolis Savva, X Angel, Thomas Chang, Silvio Funkhouser, Savarese, Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision. Revised Selected Papers, Part III. Perth, AustraliaSpringerDecember 2-6, 2018. 201914</p>
<p>Language conditioned spatial relation reasoning for 3d object grounding. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev, Advances in neural information processing systems. 202235</p>
<p>Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, Tao Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Scan2cap: Context-aware dense captioning in rgb-d scans. Zhenyu Chen, Ali Gholami, Matthias Nießner, Angel X Chang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Language-guided hierarchical finegrained image forgery detection and localization. Xiao Guo, Xiaohong Liu, Iacopo Masi, Xiaoming Liu, International Journal of Computer Vision. 2024a</p>
<p>Dense-face: Personalized face generation model via dense annotation prediction. Xiao Guo, Manh Tran, Jiaxin Cheng, Xiaoming Liu, arXiv:2412.181492024barXiv preprint</p>
<p>Rethinking vision-language model for deepfake detection: Multi-modal interpretable forged face detection. Xiao Guo, Xiufeng Song, Yue Zhang, Xiaohong Liu, Xiaoming Liu, CVPR. 2025</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems. 202336</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, arXiv:2311.12871Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. 2023arXiv preprint</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. 202226arXiv preprint</p>
<p>Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR2022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Multi-modal situated reasoning in 3d scenes. Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian , Shawn Ma, Baoxiong Jia, Siyuan Huang, Advances in Neural Information Processing Systems. 202537</p>
<p>Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, Siyuan Huang, arXiv:2210.07474Sqa3d: Situated question answering in 3d scenes. 2022arXiv preprint</p>
<p>Situational awareness matters in 3d vision language reasoning. Yunze Man, Gui Liang-Yan, Yu-Xiong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Neuro-symbolic training for reasoning over spatial language. Tanawan Premsri, Parisa Kordjamshidi, 2024</p>
<p>Forest: Frame of reference evaluation in spatial reasoning tasks. Tanawan Premsri, Parisa Kordjamshidi, 2025</p>
<p>Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Charles Ruizhongtai, Qi , Li Yi, Hao Su, Leonidas J Guibas, Advances in neural information processing systems. 302017</p>
<p>Rora-vlm: Robust retrieval-augmented vision language models. Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, Jin Di, Yu Cheng, Qifan Wang, Lifu Huang, 2024</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Reimers, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Habitat: A platform for embodied ai research. Chulayuth Paul K Rubenstein, Asawaroengchai, Dung Duc, Ankur Nguyen, Zalán Bapna, Félix Borsos, De Chaumont, Peter Quitry, Dalia El Chen, Wei Badawy, Eugene Han, Kharitonov, arXiv:2306.12925Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023. 2019arXiv preprintA large language model that can speak and listen</p>
<p>Mask3d: Mask transformer for 3d semantic instance segmentation. Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, Bastian Leibe, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on robot learning. PMLR2022</p>
<p>On learning multi-modal forgery representation for diffusion generated video detection. Xiufeng Song, Xiao Guo, Jiache Zhang, Qirui Li, Lei Bai, Xiaoming Liu, Guangtao Zhai, Xiaohong Liu, Proceeding of Thirty-eighth Conference on Neural Information Processing Systems. eeding of Thirty-eighth Conference on Neural Information essing Systems2024</p>
<p>Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, Lifu Huang, 2024</p>
<p>Chat-3d: Dataefficiently tuning large language model for universal dialogue of 3d scenes. Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, Zhou Zhao, arXiv:2308.087692023arXiv preprint</p>
<p>Embodied question answering in photorealistic environments with point cloud perception. Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences. Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, Federico Tombari, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Lateralization lora: Interleaved instruction tuning with modality-specialized adaptations. Zhiyang Xu, Minqian Liu, Ying Shen, Joy Rimchala, Jiaxin Zhang, Qifan Wang, Yu Cheng, Lifu Huang, 2024</p>
<p>Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent. Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F Fouhey, Joyce Chai, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu, arXiv:2305.110002023aarXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, 2022. 2023b3</p>
<p>Explicit object relation alignment for vision and language navigation. Yue Zhang, Parisa Kordjamshidi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop2022a</p>
<p>Lovis: Learning orientation and visual signals for vision and language navigation. Yue Zhang, Parisa Kordjamshidi, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022b</p>
<p>Vln-trans: Translator for the vision and language navigation agent. Yue Zhang, Parisa Kordjamshidi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Narrowing the gap between vision and action in navigation. Yue Zhang, Parisa Kordjamshidi, arXiv:2408.103882024arXiv preprint</p>
<p>Towards navigation by reasoning over spatial configurations. Yue Zhang, Quan Guo, Parisa Kordjamshidi, arXiv:2105.068392021arXiv preprint</p>
<p>Common sense reasoning for deep fake detection. Yue Zhang, Ben Colman, Ali Shahriyari, Gaurav Bharaj, arXiv:2402.001262024aarXiv preprint</p>
<p>Navhint: Vision and language navigation agent with a hint generator. Yue Zhang, Quan Guo, Parisa Kordjamshidi, 2024bAssociation for Computational Linguistics</p>
<p>Vision-and-language navigation today and tomorrow: A survey in the era of foundation models. Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, Parisa Kordjamshidi, arXiv:2407.070352024carXiv preprint</p>
<p>Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities. Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, Ziqiao Ma, ArXiv, abs/2410.173852024d</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Navgpt: Explicit reasoning in vision-and-language navigation with large language models. Gengze Zhou, Yicong Hong, Qi Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>3d-vista: Pretrained transformer for 3d vision and text alignment. Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>            </div>
        </div>

    </div>
</body>
</html>
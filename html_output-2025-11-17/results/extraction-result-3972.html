<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3972 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3972</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3972</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-6c5a1079d9705c0ee022cef77207daa20ce2cde5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c5a1079d9705c0ee022cef77207daa20ce2cde5" target="_blank">Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models from the GPT series, and their prospective applications across diverse domains is presented.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3972.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3972.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA (GPT-based Evaluation Metric for translatioN/BleA?)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-based translation quality assessment metric that scores translation fragments with LLM-generated judgments and averages them to a system-level score; reported to correlate well with human judgments and outperform many automatic metrics at the system level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are state-of-the-art evaluators of translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Translation adequacy and quality at fragment and system level (accuracy-like correctness per fragment aggregated to system-level score).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Use GPT-family models with designed cue templates to score each translation fragment; average fragment scores to obtain system-level score; compare with human judgments and other automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MQM2022 test set (English-German, English-Russian, Chinese-English) used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>System-level accuracy (e.g., GEMBA reported 88.0% system-level accuracy), comparison to automatic metrics (BLEU etc.), model-level accuracies (ChatGPT >80% in some templates).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human judgments are used as a gold standard for comparison; GEMBA is evaluated by computing agreement with human MQM annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation currently applicable at system level (not necessarily segment-level); performance depends on prompt templates and degree of constraint; further improvements needed for finer-grained evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GEMBA achieved highest system-level accuracy (~88%) on MQM2022 compared to >10 automatic metrics and ChatGPT achieved >80% under some templates, showing LLMs' promise as translation evaluators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3972.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-as-NLG-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT used as a Natural Language Generation (NLG) evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using ChatGPT to evaluate generated text across NLG tasks by prompting it with task- and aspect-specific cues and computing correlations between ChatGPT scores and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human-judged aspects of NLG such as relevance, fluency, coherence, factuality depending on the task (summary quality, story quality, data-to-text correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Design task- and aspect-specific prompts/cues for ChatGPT to score outputs; compute statistical correlations (Spearman, Pearson, Kendall's Tau) between ChatGPT scores and human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CNN/DM (summarization), OpenMEVA-ROC (story generation), BAGEL (data-to-text) — used to compare ChatGPT judgments with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Correlation coefficients (Spearman, Pearson, Kendall's Tau); reported correlations >= 0.4 in many aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotations used as reference judgments to compute correlations; human raters supplied the ground truth evaluation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Correlation is task- and cue-dependent; requires careful prompt engineering; correlations are moderate (not perfect), and LLM evaluator may still disagree with humans on nuanced aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT shows substantial correlation (Spearman/Pearson/Kendall) with human judgments across multiple NLG tasks (coefficients >= 0.4), indicating potential as an automatic evaluator but not full replacement for humans.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3972.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human manual evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human/manual assessment (expert panels, crowdworkers, domain experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional evaluation where humans (experts or crowdworkers) read and judge model outputs according to task-specific rubrics; considered effective but subjective and time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness, completeness, safety, trustworthiness, clinical relevance (for medical), comprehensibility, logical coherence, pedagogy utility (for education), etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Expert review, blind A/B tests, annotator labeling tasks, user studies (e.g., patients judging whether replies were from doctor or ChatGPT), teacher grading of student essays assisted by ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Task-specific human-evaluation setups (e.g., 10 patient-doctor interactions from EHR in Nov et al.; teacher evaluations in student essay studies).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Percent correct identification (e.g., 65.5% identification of ChatGPT responses), Likert trust scores (e.g., average 3.4), inter-rater agreement implied but not always reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — domain experts, clinicians, teachers, Mturk crowdworkers used for labeling and judgment in many studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subjectivity, time and cost, potential annotator bias, limited scalability, and variable inter-rater reliability; also humans may be poor detectors of subtle factual errors or hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human assessments reveal that ChatGPT can be persuasive but may be identified correctly only ~65% of the time in some patient-doctor tasks; experts flag omissions and risky errors in medical simplifications.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3972.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard automatic metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic evaluation metrics (BLEU, ROUGE, BERTScore, Accuracy, Precision/Recall/F1, UAR, Micro/Macro F1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common automated numeric metrics used across tasks: BLEU and ROUGE for generation/translation, BERTScore for semantic similarity, and precision/recall/F1/UAR for classification/information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N-gram overlap (BLEU/ROUGE), semantic similarity (BERTScore), classification correctness (precision/recall/F1), balanced accuracy (UAR), micro/macro aggregated performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute metric scores between model output and reference/gold annotations (single or multiple references) for tasks such as translation, summarization, classification, and IE.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>WMT (translation), WMT19 biomedical, Flores101, CNN/DM (summarization), SemEval datasets (stance, intimacy), QuixBugs (program repair), DRAW-1K (math word problems), and many IE datasets (NYT11-HRL, ACE05, DuIE2.0, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>BLEU, ROUGE-1/2/L, BERTScore (B_S in paper), Accuracy (%), Recall, Precision, F1, UAR, Micro F1, Macro F1.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Minimal in automated scoring but typically compared against human-annotated gold references; human annotations required to create ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Overlap-based metrics can fail for semantically-correct but lexically-different outputs; single-reference issues; do not capture factuality/hallucination or deeper explanatory power; may not align with human judgments for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT competitive on high-resource language translation per BLEU/BERTScore in some settings but weaker on low-resource/distant languages and biomedical domains; overall performance typically lower than SOTA fine-tuned models on many metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3972.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks & datasets (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative benchmarks and datasets used to evaluate LLM capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of public datasets and task suites used across studies to measure ChatGPT/LLM performance on math, reasoning, translation, IE, code, and domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific correctness and metrics (e.g., answer correctness for math, F1 for IE, BLEU/ROUGE for translation/summarization, accuracy/F1 for classification).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Zero-shot/one-shot prompting and direct evaluation on held-out test sets; comparisons to SOTA and task-specific models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Examples: GHOSTS (graduate math questions), MATH, Grad Text, Holes-in-Proofs, DRAW-1K (math word problems), Force Concept Inventory (FCI) for physics concepts, QuixBugs (program repair), MQM2022, WMT19 biomedical, Flores101, CNN/DM, SemEval-2016, P-Stance, EN-GINCO/GINCO, NYT11-HRL, DuIE2.0, ACE05, ACE2005, FUNSD, CORD, SROIE, MIMIC-CXR, OpenI.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Task-specific metrics (accuracy %, percent correct, F1, recall, BLEU, ROUGE, Spearman/Pearson correlations to human scores, success/failure rates).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-annotated gold labels used to form benchmarks; human evaluation often used for difficult/ambiguous items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmarks may not capture creativity or genuine theory-formation; many are text-level tasks rather than evaluation of novel scientific theories; domain and dataset biases can misrepresent real-world competency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Results vary by dataset: ChatGPT sometimes matches or exceeds zero-shot baselines and even some fine-tuned models on specific tasks, but underperforms SOTA on average and shows instability across datasets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3972.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination / factuality probing (physics example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hallucination and conceptual-connection evaluation (example: Lehnert on swampland conjecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Probing LLMs on obscure/specialized scientific domains via targeted dialogues to evaluate ability to define concepts, connect ideas, and avoid fabricating facts; expert inspection reveals confidence in false statements and inability to truly create new knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Factual correctness, conceptual coherence, ability to connect concepts accurately, avoidance of fabrications, and explanatory power.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Iterative dialogue starting from general to specific questions on specialized topics; human expert assessment of answers and detection of fabricated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Qualitative, expert-curated dialogues probing the 'swampland conjecture' and other obscure physics topics (no standard dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Qualitative judgments (presence of false statements, fabrications, inability to form genuine novel connections); no numeric metrics reported in that example.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Domain experts (physicists) performed evaluation and judged correctness and meaningfulness of model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLMs may produce confident but false claims, cannot reliably synthesize genuinely new theories, and can fail to establish valid conceptual links; evaluation is qualitative and requires domain expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>ChatGPT-generated explanations of swampland conjectures that included invented statements and incorrect connections.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT could define and stylistically explain concepts but failed to truly connect them, sometimes fabricating statements, indicating it cannot be relied on to generate correct novel scientific theories without expert oversight.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3972.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plagiarism / originality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plagiarism and originality assessment (Turnitin, iThenticate study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Testing originality of ChatGPT-generated manuscripts using commercial plagiarism detectors to evaluate whether generated scientific-like text is flagged as plagiarized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Will chatgpt get you caught? rethinking of plagiarism detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Degree of overlap or matches detected by plagiarism-detection engines; ability of detectors to flag AI-generated content.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Generate multiple full papers with ChatGPT and submit them to Turnitin and iThenticate to measure detection/overlap scores.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>A set of 50 ChatGPT-generated papers on various topics used as input to plagiarism detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Plagiarism / similarity scores produced by the detection tools; qualitative assessment of detectors' ability to catch AI-generated prose.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Researchers evaluating detector outputs and interpreting results; no external human grading of originality beyond tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Commercial detectors may fail to detect AI-generated text reliably; detection engines need updates to capture characteristics of LLM-generated content.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT-generated long-form content was often not flagged reliably by Turnitin/iThenticate, showing that existing plagiarism detectors may need updates to detect AI-generated material.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3972.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3972.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Information extraction evaluation frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatIE / ICL-D3IE / ChatExtract (LLM-based IE evaluation frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frameworks that decompose IE tasks into multi-round or in-context prompting strategies for ChatGPT and then evaluate on standard IE datasets using precision/recall/F1; aim to bridge gap between generic LLMs and task-specific IE models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Precision, recall, F1 score on entity/relation/event extraction tasks; robustness in long-tail and multi-event texts; sensitivity to prompt style.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Multi-round Q&A decomposition (ChatIE), in-context learning with formatted/iteratively updated demonstrations (ICL-D3IE), prompt-engineering pipelines (ChatExtract); compare LLM outputs to gold-labeled datasets and to task-specific supervised models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>NYT11-HRL, DuIE2.0, conllpp, MSR, DuEE1.0, ACE05, ACE2005, FUNSD, CORD, SROIE and others used for evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Precision, recall, F1; reported relative improvements (e.g., ChatIE improved average performance by ~18.98% over baseline ChatGPT; ICL-D3IE reported F1 scores like 90.32% on FUNSD, 97.88% on SROIE in some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-curated gold annotations in datasets; some approaches are designed to reduce annotation needs but still benchmark against human-labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Original ChatGPT struggles on complex IE tasks without specialized prompting; high sensitivity to prompt style; performance often below task-specific supervised models in long-tail/complex scenarios; consistency/stability issues across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>With specialized prompting frameworks, ChatGPT-based IE methods can approach or exceed some supervised baselines on particular datasets (not uniformly), but remain sensitive to prompt design and underperform on long-tail or complex multi-event texts compared to task-specific models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality. <em>(Rating: 2)</em></li>
                <li>Mathematical capabilities of chatgpt. <em>(Rating: 2)</em></li>
                <li>Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt. <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback. <em>(Rating: 1)</em></li>
                <li>Will chatgpt get you caught? rethinking of plagiarism detection. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3972",
    "paper_id": "paper-6c5a1079d9705c0ee022cef77207daa20ce2cde5",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "GEMBA",
            "name_full": "GEMBA (GPT-based Evaluation Metric for translatioN/BleA?)",
            "brief_description": "A GPT-based translation quality assessment metric that scores translation fragments with LLM-generated judgments and averages them to a system-level score; reported to correlate well with human judgments and outperform many automatic metrics at the system level.",
            "citation_title": "Large language models are state-of-the-art evaluators of translation quality.",
            "mention_or_use": "mention",
            "evaluation_criteria": "Translation adequacy and quality at fragment and system level (accuracy-like correctness per fragment aggregated to system-level score).",
            "evaluation_methods": "Use GPT-family models with designed cue templates to score each translation fragment; average fragment scores to obtain system-level score; compare with human judgments and other automatic metrics.",
            "benchmark_or_dataset": "MQM2022 test set (English-German, English-Russian, Chinese-English) used for evaluation.",
            "metrics_reported": "System-level accuracy (e.g., GEMBA reported 88.0% system-level accuracy), comparison to automatic metrics (BLEU etc.), model-level accuracies (ChatGPT &gt;80% in some templates).",
            "human_involvement": "Human judgments are used as a gold standard for comparison; GEMBA is evaluated by computing agreement with human MQM annotations.",
            "limitations_or_challenges": "Evaluation currently applicable at system level (not necessarily segment-level); performance depends on prompt templates and degree of constraint; further improvements needed for finer-grained evaluation.",
            "llm_theory_example": null,
            "evaluation_results": "GEMBA achieved highest system-level accuracy (~88%) on MQM2022 compared to &gt;10 automatic metrics and ChatGPT achieved &gt;80% under some templates, showing LLMs' promise as translation evaluators.",
            "uuid": "e3972.0"
        },
        {
            "name_short": "ChatGPT-as-NLG-evaluator",
            "name_full": "ChatGPT used as a Natural Language Generation (NLG) evaluator",
            "brief_description": "Using ChatGPT to evaluate generated text across NLG tasks by prompting it with task- and aspect-specific cues and computing correlations between ChatGPT scores and human judgments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Human-judged aspects of NLG such as relevance, fluency, coherence, factuality depending on the task (summary quality, story quality, data-to-text correctness).",
            "evaluation_methods": "Design task- and aspect-specific prompts/cues for ChatGPT to score outputs; compute statistical correlations (Spearman, Pearson, Kendall's Tau) between ChatGPT scores and human ratings.",
            "benchmark_or_dataset": "CNN/DM (summarization), OpenMEVA-ROC (story generation), BAGEL (data-to-text) — used to compare ChatGPT judgments with human judgments.",
            "metrics_reported": "Correlation coefficients (Spearman, Pearson, Kendall's Tau); reported correlations &gt;= 0.4 in many aspects.",
            "human_involvement": "Human annotations used as reference judgments to compute correlations; human raters supplied the ground truth evaluation scores.",
            "limitations_or_challenges": "Correlation is task- and cue-dependent; requires careful prompt engineering; correlations are moderate (not perfect), and LLM evaluator may still disagree with humans on nuanced aspects.",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT shows substantial correlation (Spearman/Pearson/Kendall) with human judgments across multiple NLG tasks (coefficients &gt;= 0.4), indicating potential as an automatic evaluator but not full replacement for humans.",
            "uuid": "e3972.1"
        },
        {
            "name_short": "Human manual evaluation",
            "name_full": "Human/manual assessment (expert panels, crowdworkers, domain experts)",
            "brief_description": "Traditional evaluation where humans (experts or crowdworkers) read and judge model outputs according to task-specific rubrics; considered effective but subjective and time-consuming.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Correctness, completeness, safety, trustworthiness, clinical relevance (for medical), comprehensibility, logical coherence, pedagogy utility (for education), etc.",
            "evaluation_methods": "Expert review, blind A/B tests, annotator labeling tasks, user studies (e.g., patients judging whether replies were from doctor or ChatGPT), teacher grading of student essays assisted by ChatGPT.",
            "benchmark_or_dataset": "Task-specific human-evaluation setups (e.g., 10 patient-doctor interactions from EHR in Nov et al.; teacher evaluations in student essay studies).",
            "metrics_reported": "Percent correct identification (e.g., 65.5% identification of ChatGPT responses), Likert trust scores (e.g., average 3.4), inter-rater agreement implied but not always reported explicitly.",
            "human_involvement": "High — domain experts, clinicians, teachers, Mturk crowdworkers used for labeling and judgment in many studies.",
            "limitations_or_challenges": "Subjectivity, time and cost, potential annotator bias, limited scalability, and variable inter-rater reliability; also humans may be poor detectors of subtle factual errors or hallucinations.",
            "llm_theory_example": null,
            "evaluation_results": "Human assessments reveal that ChatGPT can be persuasive but may be identified correctly only ~65% of the time in some patient-doctor tasks; experts flag omissions and risky errors in medical simplifications.",
            "uuid": "e3972.2"
        },
        {
            "name_short": "Standard automatic metrics",
            "name_full": "Automatic evaluation metrics (BLEU, ROUGE, BERTScore, Accuracy, Precision/Recall/F1, UAR, Micro/Macro F1)",
            "brief_description": "Common automated numeric metrics used across tasks: BLEU and ROUGE for generation/translation, BERTScore for semantic similarity, and precision/recall/F1/UAR for classification/information extraction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "N-gram overlap (BLEU/ROUGE), semantic similarity (BERTScore), classification correctness (precision/recall/F1), balanced accuracy (UAR), micro/macro aggregated performance.",
            "evaluation_methods": "Compute metric scores between model output and reference/gold annotations (single or multiple references) for tasks such as translation, summarization, classification, and IE.",
            "benchmark_or_dataset": "WMT (translation), WMT19 biomedical, Flores101, CNN/DM (summarization), SemEval datasets (stance, intimacy), QuixBugs (program repair), DRAW-1K (math word problems), and many IE datasets (NYT11-HRL, ACE05, DuIE2.0, etc.).",
            "metrics_reported": "BLEU, ROUGE-1/2/L, BERTScore (B_S in paper), Accuracy (%), Recall, Precision, F1, UAR, Micro F1, Macro F1.",
            "human_involvement": "Minimal in automated scoring but typically compared against human-annotated gold references; human annotations required to create ground truth.",
            "limitations_or_challenges": "Overlap-based metrics can fail for semantically-correct but lexically-different outputs; single-reference issues; do not capture factuality/hallucination or deeper explanatory power; may not align with human judgments for some tasks.",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT competitive on high-resource language translation per BLEU/BERTScore in some settings but weaker on low-resource/distant languages and biomedical domains; overall performance typically lower than SOTA fine-tuned models on many metrics.",
            "uuid": "e3972.3"
        },
        {
            "name_short": "Benchmarks & datasets (examples)",
            "name_full": "Representative benchmarks and datasets used to evaluate LLM capabilities",
            "brief_description": "A collection of public datasets and task suites used across studies to measure ChatGPT/LLM performance on math, reasoning, translation, IE, code, and domain tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Task-specific correctness and metrics (e.g., answer correctness for math, F1 for IE, BLEU/ROUGE for translation/summarization, accuracy/F1 for classification).",
            "evaluation_methods": "Zero-shot/one-shot prompting and direct evaluation on held-out test sets; comparisons to SOTA and task-specific models.",
            "benchmark_or_dataset": "Examples: GHOSTS (graduate math questions), MATH, Grad Text, Holes-in-Proofs, DRAW-1K (math word problems), Force Concept Inventory (FCI) for physics concepts, QuixBugs (program repair), MQM2022, WMT19 biomedical, Flores101, CNN/DM, SemEval-2016, P-Stance, EN-GINCO/GINCO, NYT11-HRL, DuIE2.0, ACE05, ACE2005, FUNSD, CORD, SROIE, MIMIC-CXR, OpenI.",
            "metrics_reported": "Task-specific metrics (accuracy %, percent correct, F1, recall, BLEU, ROUGE, Spearman/Pearson correlations to human scores, success/failure rates).",
            "human_involvement": "Human-annotated gold labels used to form benchmarks; human evaluation often used for difficult/ambiguous items.",
            "limitations_or_challenges": "Benchmarks may not capture creativity or genuine theory-formation; many are text-level tasks rather than evaluation of novel scientific theories; domain and dataset biases can misrepresent real-world competency.",
            "llm_theory_example": null,
            "evaluation_results": "Results vary by dataset: ChatGPT sometimes matches or exceeds zero-shot baselines and even some fine-tuned models on specific tasks, but underperforms SOTA on average and shows instability across datasets.",
            "uuid": "e3972.4"
        },
        {
            "name_short": "Hallucination / factuality probing (physics example)",
            "name_full": "Hallucination and conceptual-connection evaluation (example: Lehnert on swampland conjecture)",
            "brief_description": "Probing LLMs on obscure/specialized scientific domains via targeted dialogues to evaluate ability to define concepts, connect ideas, and avoid fabricating facts; expert inspection reveals confidence in false statements and inability to truly create new knowledge.",
            "citation_title": "Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt.",
            "mention_or_use": "mention",
            "evaluation_criteria": "Factual correctness, conceptual coherence, ability to connect concepts accurately, avoidance of fabrications, and explanatory power.",
            "evaluation_methods": "Iterative dialogue starting from general to specific questions on specialized topics; human expert assessment of answers and detection of fabricated claims.",
            "benchmark_or_dataset": "Qualitative, expert-curated dialogues probing the 'swampland conjecture' and other obscure physics topics (no standard dataset).",
            "metrics_reported": "Qualitative judgments (presence of false statements, fabrications, inability to form genuine novel connections); no numeric metrics reported in that example.",
            "human_involvement": "Domain experts (physicists) performed evaluation and judged correctness and meaningfulness of model responses.",
            "limitations_or_challenges": "LLMs may produce confident but false claims, cannot reliably synthesize genuinely new theories, and can fail to establish valid conceptual links; evaluation is qualitative and requires domain expertise.",
            "llm_theory_example": "ChatGPT-generated explanations of swampland conjectures that included invented statements and incorrect connections.",
            "evaluation_results": "ChatGPT could define and stylistically explain concepts but failed to truly connect them, sometimes fabricating statements, indicating it cannot be relied on to generate correct novel scientific theories without expert oversight.",
            "uuid": "e3972.5"
        },
        {
            "name_short": "Plagiarism / originality evaluation",
            "name_full": "Plagiarism and originality assessment (Turnitin, iThenticate study)",
            "brief_description": "Testing originality of ChatGPT-generated manuscripts using commercial plagiarism detectors to evaluate whether generated scientific-like text is flagged as plagiarized.",
            "citation_title": "Will chatgpt get you caught? rethinking of plagiarism detection.",
            "mention_or_use": "mention",
            "evaluation_criteria": "Degree of overlap or matches detected by plagiarism-detection engines; ability of detectors to flag AI-generated content.",
            "evaluation_methods": "Generate multiple full papers with ChatGPT and submit them to Turnitin and iThenticate to measure detection/overlap scores.",
            "benchmark_or_dataset": "A set of 50 ChatGPT-generated papers on various topics used as input to plagiarism detectors.",
            "metrics_reported": "Plagiarism / similarity scores produced by the detection tools; qualitative assessment of detectors' ability to catch AI-generated prose.",
            "human_involvement": "Researchers evaluating detector outputs and interpreting results; no external human grading of originality beyond tool outputs.",
            "limitations_or_challenges": "Commercial detectors may fail to detect AI-generated text reliably; detection engines need updates to capture characteristics of LLM-generated content.",
            "llm_theory_example": null,
            "evaluation_results": "ChatGPT-generated long-form content was often not flagged reliably by Turnitin/iThenticate, showing that existing plagiarism detectors may need updates to detect AI-generated material.",
            "uuid": "e3972.6"
        },
        {
            "name_short": "Information extraction evaluation frameworks",
            "name_full": "ChatIE / ICL-D3IE / ChatExtract (LLM-based IE evaluation frameworks)",
            "brief_description": "Frameworks that decompose IE tasks into multi-round or in-context prompting strategies for ChatGPT and then evaluate on standard IE datasets using precision/recall/F1; aim to bridge gap between generic LLMs and task-specific IE models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_criteria": "Precision, recall, F1 score on entity/relation/event extraction tasks; robustness in long-tail and multi-event texts; sensitivity to prompt style.",
            "evaluation_methods": "Multi-round Q&A decomposition (ChatIE), in-context learning with formatted/iteratively updated demonstrations (ICL-D3IE), prompt-engineering pipelines (ChatExtract); compare LLM outputs to gold-labeled datasets and to task-specific supervised models.",
            "benchmark_or_dataset": "NYT11-HRL, DuIE2.0, conllpp, MSR, DuEE1.0, ACE05, ACE2005, FUNSD, CORD, SROIE and others used for evaluations.",
            "metrics_reported": "Precision, recall, F1; reported relative improvements (e.g., ChatIE improved average performance by ~18.98% over baseline ChatGPT; ICL-D3IE reported F1 scores like 90.32% on FUNSD, 97.88% on SROIE in some settings).",
            "human_involvement": "Human-curated gold annotations in datasets; some approaches are designed to reduce annotation needs but still benchmark against human-labeled data.",
            "limitations_or_challenges": "Original ChatGPT struggles on complex IE tasks without specialized prompting; high sensitivity to prompt style; performance often below task-specific supervised models in long-tail/complex scenarios; consistency/stability issues across prompt variants.",
            "llm_theory_example": null,
            "evaluation_results": "With specialized prompting frameworks, ChatGPT-based IE methods can approach or exceed some supervised baselines on particular datasets (not uniformly), but remain sensitive to prompt design and underperform on long-tail or complex multi-event texts compared to task-specific models.",
            "uuid": "e3972.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality.",
            "rating": 2
        },
        {
            "paper_title": "Mathematical capabilities of chatgpt.",
            "rating": 2
        },
        {
            "paper_title": "Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt.",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback.",
            "rating": 1
        },
        {
            "paper_title": "Will chatgpt get you caught? rethinking of plagiarism detection.",
            "rating": 2
        }
    ],
    "cost": 0.01728075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Highlights</h1>
<h2>Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models</h2>
<p>Yiheng Liu <em>,Tianle Han </em>,Siyuan Ma,Jiayue Zhang,Yuanyuan Yang,Jiaming Tian,Hao He,Antong Li,Mengshen He,Zhengliang Liu,Zihao Wu,Lin Zhao,Dajiang Zhu,Xiang Li,Ning Qiang,Dingang Shen,Tianming Liu,Bao Ge</p>
<ul>
<li>A comprehensive survey of ChatGPT-related research.</li>
<li>Analysis of 194 research papers.</li>
<li>Paving the way for further research and exploration in leveraging large language models for various applications.</li>
</ul>
<h1>Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models</h1>
<p>Yiheng Liu ${ }^{<em> a}$, Tianle Han ${ }^{</em> a}$, Siyuan $\mathrm{Ma}^{a}$, Jiayue Zhang ${ }^{a}$, Yuanyuan Yang ${ }^{a}$, Jiaming Tian ${ }^{a}$, Hao $\mathrm{He}^{a}$, Antong $\mathrm{Li}^{b}$, Mengshen $\mathrm{He}^{a}$, Zhengliang Liu ${ }^{c}$, Zihao $\mathrm{Wu}^{c}$, Lin Zhao ${ }^{c}$, Dajiang $\mathrm{Zhu}^{d}$, Xiang $\mathrm{Li}^{e}$, Ning Qiang ${ }^{a}$, Dingang Shen ${ }^{f, g, h}$, Tianming Liu ${ }^{c}$ and Bao $\mathrm{Ge}^{a}$<br>${ }^{a}$ School of Physics and Information Technology, Shaanxi Normal University, Xi'an, 710119, Shaanxi, China<br>${ }^{b}$ School of Life and Technology Biomedical-Engineering, Xi'an Jiaotong University, Xi'an, 710119, Shaanxi, China<br>${ }^{c}$ School of Computing, The University of Georgia, Athens, 30602, USA<br>${ }^{d}$ Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, 76019, USA<br>${ }^{e}$ Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, 02115, USA<br>${ }^{f}$ School of Biomedical Engineering, ShanghaiTech University, Shanghai, 201210, China<br>${ }^{g}$ Shanghai United Imaging Intelligence Co., Ltd., Shanghai, 200230, China<br>${ }^{h}$ Shanghai Clinical Research and Trial Center, Shanghai, 201210, China</p>
<h2>ABSTRACT</h2>
<p>This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.</p>
<h2>1. Introduction</h2>
<p>Recent advances in natural language processing (NLP) have led to the development of powerful language models such as the GPT (Generative Pre-trained Transformer) series [79; 81; 80; 8; 73], including large language models (LLM) such as ChatGPT (GPT-3.5 and GPT-4) [71]. These models are pre-trained on vast amounts of text data and have demonstrated exceptional performance in a wide range of NLP tasks, including language translation, text summarization, and question-answering. In particular, the ChatGPT model has demonstrated its potential in various fields, including education, healthcare, reasoning, text generation, human-machine interaction, and scientific research.</p>
<p>A key milestone of LLM development is InstructGPT [73], a framework that allows for instruction fine-tuning of a pre-trained language model based on Reinforcement Learning from Human Feedback (RLHF) [11; 73]. This framework enables an LLM to adapt to a wide range of NLP tasks, making it highly versatile and flexible by leveraging human feedback. RLHF enables the model to align with human preferences and human values, which significantly improves from large language models that are solely trained text corpora through unsupervised pretraining. ChatGPT is a successor to InstructGPT. Since its release in December 2022, ChatGPT has been equipped with these advanced developments, leading to impressive performances in various downstream NLP tasks such as reasoning and generalized text generation. These unprecedented NLP capabilities spur applications in diverse domains such as education, healthcare, human-machine interaction, medicine and scientific research. ChatGPT has received widespread attention and interest, leading to an increasing number of applications and research that harness its exceeding potential.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The graphical representation is utilized to depict the number of research articles related to ChatGPT published from 2022 to April, 2023, revealing the trend and growth of ChatGPT-related research over time. The graph showcases the monthly count of submissions and cumulative daily submitted count in arXiv. Over time, there has been an increasing amount of research related to ChatGPT.</p>
<p>The open release of the multi-modal GPT-4 model further expands the horizon of large language models and empowers exciting developments that involve diverse data beyond text.</p>
<p>The purpose of this paper is to provide a comprehensive survey of the existing research on ChatGPT and its potential applications in various fields. To achieve this goal, we conducted a thorough analysis of papers related to ChatGPT in the arXiv repository. As of April 1st, 2023, there are a total of 194 papers mentioning ChatGPT on arXiv. In this study, we conducted a trend analysis of these papers and generated a word cloud to visualize the commonly used terms. Additionally, we also examined the distribution of the papers across various fields and presented the corresponding statistics. Figure 1 displays the submission trend of papers related to ChatGPT, indicating a growing interest in this field. Figure 2 illustrates the word cloud analysis of all the papers. We can observe that the current research is primarily focused on natural language processing, but there is still significant potential for research in other fields such as education, medical and history. This is further supported by Figure 3, which displays the distribution of submitted papers across various fields, highlighting the need for more research and development in these areas. Due to the rapid advancement in research related to ChatGPT, we have also introduced a dynamic webpage that provides real-time updates on the latest trends in these areas. Interested readers can access the webpage and stay informed about the evolving research directions by following this link ${ }^{1}$.</p>
<p>This paper aims to shed light on the promising capabilities of ChatGPT and provide insight into its potential impact in the future, including ethical considerations. Through this survey, we hope to provide insights into how these models can be improved and extended in the future. In section 2, we will review the existing work related to ChatGPT, including its applications and ethical considerations. In section 3, we conducted a review of existing literature that assesses the capabilities of ChatGPT. We comprehensively evaluated the performance of ChatGPT based on these studies. In addition to discussing the current state of research related to ChatGPT, we will also explore its limitations in section 4. Furthermore, we will provide guidance on future directions for language model development.</p>
<h1>2. Related work of ChatGPT</h1>
<p>In this section, we review the latest research related to the application and ethics of ChatGPT. Figure 4 shows the overall framework of this part.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Word cloud analysis of all the 194 papers.</p>
<h1>2.1. Application of ChatGPT</h1>
<h3>2.1.1. Question And Answering</h3>
<h2>In the field of education</h2>
<p>ChatGPT is commonly used for question and answers testing in the education sector. Users can use ChatGPT to learn, compare and verify answers for different academic subjects such as physics, mathematics, and chemistry, and/or conceptual subjects such as philosophy and religion. Additionally, users can ask open-ended and analytical questions to understand the capabilities of ChatGPT.</p>
<p>In the field of mathematics, Frieder et al. [17] constructed the GHOSTS natural language dataset, which consists of graduate-level math test questions. The authors tested ChatGPT's math abilities on the GHOSTS dataset using a question-and-answer format and evaluated it according to fine-grained standards.In the Grad Text dataset, which covers simple set theory and logic problems, ChatGPT performed the best. However, in the Olympiad-Problem-Solving dataset, ChatGPT performed poorly, receiving only two 4-point scores (out of a total of 5), with the majority of scores being 2 points. In the Holes-in-Proofs dataset, ChatGPT received the lowest score of 1 point. In the MATH dataset, ChatGPT only scored impressively in $26 \%$ of cases. These results suggest that ChatGPT's math abilities are clearly lower than those of ordinary math graduate students. Although ChatGPT can generally understand math problems, it fails to provide the correct solutions. Pardos et al. [74] used the Open Adaptive Tutoring system (OATutor) to investigate whether prompts generated by ChatGPT were helpful for learning algebra, with 77 participants from Mechanical Turk taking part in the experiment. The experiment used questions from OpenStax's Elementary and Intermediate Algebra textbooks. These participants were randomly assigned to either a control group (with manual prompts) or an experimental group (with ChatGPT prompts). For each question in both courses, the authors obtained answers from ChatGPT through a question-and-answer format and evaluated scores according to three criteria: ChatGPT provided an answer, the answer was correct, and inappropriate language was not used in the answer. The study found that $70 \%$ of prompts generated by ChatGPT passed manual quality checks, and both humans and ChatGPT produced positive learning gains. However, the scores of human prompts ranged from $74.59 \%$ to $84.32 \%$, significantly higher than those of ChatGPT prompts. Shakarian et al. [82] studied the performance of ChatGPT on math word problems (MWPs), using the DRAW-1K dataset for experimentation. The dataset consists of 1000 MWPs and their answers, along with algebraic equation templates for solving such problems. The authors used the idea of machine learning introspection and built performance prediction models using random forests and XGBoost, and evaluated them on the dataset using</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The distribution of submitted papers across various fields.
five-fold cross-validation. ChatGPT's accuracy increased from an initial $34 \%$ to a final $69 \%$, while its recall increased from an initial $41 \%$ to a final $83 \%$. The authors also found that ChatGPT's failure rate decreased from an initial $84 \%$ to a final $20 \%$, indicating that performance can vary greatly depending on specific job requirements.</p>
<p>In the field of physics, Lehnert et al. [48] explored the capabilities and limitations of ChatGPT by studying how it handles obscure physics topics such as the swamp land conjecture in string theory. The experimental dialogue began with broader and more general questions in the field of string theory before narrowing down to specific swamp land conjectures and examining ChatGPT's understanding of them. The study found that ChatGPT could define and explain different concepts in various styles, but was not effective in truly connecting various concepts. It would confidently provide false information and fabricate statements when necessary, indicating that ChatGPT cannot truly create new knowledge or establish new connections. However, in terms of identifying analogies and describing abstract concepts of visual representation, ChatGPT can cleverly use language. Kortemeyer et al. [44] evaluated ChatGPT's ability to answer calculus-based physics questions through a question-and-answer test. The tests included online homework, clicker questions, programming exercises, and exams covering classical mechanics, thermodynamics, electricity and magnetism, and modern physics. While ChatGPT was able to pass the course, it also demonstrated many misconceptions and errors commonly held by beginners. West et al. [98] used the Force Concept Inventory</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Structure Diagram of Chapter 2.
(FCI) to evaluate ChatGPT's accuracy in answering physics concept problems related to kinematics and Newtonian mechanics in the first semester of college physics. The FCI covers topics such as kinematics, projectile motion, free fall, circular motion, and Newton's laws. The study included data from 415 students who took the FCI at the end of the semester, with an average score of $56 \%$, while ChatGPT scored approximately between $50 \%$ to $65 \%$. The authors demonstrated that ChatGPT's performance in physics learning can reach or even exceed the average level of a semester of college physics.</p>
<h1>In the medical field</h1>
<p>ChatGPT's question-answering capabilities can also be applied in the medical field, such as for answering medical questions from patients or assisting healthcare professionals in diagnosing diseases. Nov et al. [70] evaluated the feasibility of using ChatGPT for patient-doctor communication. The experiment extracted 10 representative patientdoctor interactions from EHR, placed the patient's questions in ChatGPT, and asked ChatGPT to respond using roughly the same number of words as the doctor's response. Each patient's question was answered by either the doctor or ChatGPT, and the patient was informed that 5 were answered by the doctor and 5 were generated by ChatGPT, and was asked to correctly identify the source of the response. The results of the experiment showed that the probability of correctly identifying ChatGPT's response was $65.5 \%$, while the probability of correctly identifying the doctor's response was $65.1 \%$. In addition, the experiment found that the patient's response to the trustworthiness of ChatGPT's function was weakly positive (average Likert score: 3.4), and trust decreased as the complexity of health-related tasks in the questions increased. ChatGPT's responses to patient questions were only slightly different from those of doctors, but people seem to trust ChatGPT to answer low-risk health questions, while for complex medical questions, people still tend to trust the doctor's responses and advice.</p>
<p>Tu et al. [91] explored the causal discovery ability of ChatGPT in the diagnosis of neuropathic pain. Causal relationship discovery aims to reveal potential unknown causal relationships based purely on observed data [20]. The experimental results found that ChatGPT has some limitations in understanding new knowledge and concepts beyond the existing textual training data corpus, that is, it only understands language commonly used to describe situations and not underlying knowledge. In addition, its performance consistency and stability are not high, as the experiment observed that it would provide different answers for the same question under multiple inquiries. However, despite the many limitations of ChatGPT, we believe that it has a great opportunity to improve causal relationship research.</p>
<h1>In other fields</h1>
<p>Guo et al. [23] attempted to apply ChatGPT in the field of communication, specifically using ChatGPT for ordered importance semantic communication, where ChatGPT plays the role of an intelligent consulting assistant that can replace humans in identifying the semantic importance of words in messages and can be directly embedded into the current communication system. For a message to be transmitted, the sender first utilizes ChatGPT to output the semantic importance order of each word. Then, the transmitter executes an unequal error protection transmission strategy based on the importance order to make the transmission of important words in the message more reliable. The experimental results show that the error rate and semantic loss of important words measured in the communication system embedded with ChatGPT are much lower than those of existing communication schemes, indicating that ChatGPT can protect important words well and make semantic communication more reliable.</p>
<p>Wang et al. [95] studied the effectiveness of ChatGPT in generating high-quality Boolean queries for systematic literature search. They designed a wide range of prompts and investigated these tasks on more than 100 systematic review topics. In the end, queries generated by ChatGPT achieved higher accuracy compared to the currently most advanced query generation methods but at the cost of reduced recall. For time-limited rapid reviews, it is often acceptable to trade off higher precision for lower recall. Additionally, ChatGPT can generate high search accuracy Boolean queries by guiding the prompts. However, it should be noted that when two queries use the same prompts, ChatGPT generates different queries, indicating its limitations in consistency and stability. Overall, this study demonstrated the potential of ChatGPT in generating effective Boolean queries for systematic literature searches.</p>
<h3>2.1.2. Text Classification</h3>
<p>The purpose of text classification is to assign text data to predefined categories. This task is critical for many applications, including sentiment analysis, spam detection, and topic modeling. While traditional machine learning algorithms have been widely used for text classification, recent advances in natural language processing have led to the development of more advanced techniques. ChatGPT has shown immense potential in this field. Its ability to accurately classify text, flexibility in handling various classification tasks, and potential for customization make it a valuable tool for text classification, as evidenced by several studies in the literature.</p>
<p>Kuzman et al. [46] employed ChatGPT for automated genre recognition, with the goal of simplifying the text classification task by utilizing ChatGPT's zero-shot classification capability. They compared ChatGPT's genre recognition performance, using two prompt languages (EN and SL), with the X-GENRE classifier based on the multilingual model XLM-RoBERTa on the English dataset EN-GINCO and the Slovenian dataset GINCO. The results showed that when EN was used as the prompt language, ChatGPT achieved Micro F1, Macro F1, and Accuracy scores of $0.74,0.66$, and 0.72 . However, on the GINCO dataset, ChatGPT's genre recognition performance with both EN and SL prompt languages was lower than that of the X-GENRE classifier to varying degrees.</p>
<p>Amin et al. [2] evaluated the text classification ability of ChatGPT in affective computing by using it to perform personality prediction, sentiment analysis, and suicide ideation detection tasks. They prompted ChatGPT with corresponding prompts on three datasets: First Impressions, Sentiment140, and Suicide and Depression, and compared its classification performance with three baseline models: RoBERTa-base, Word2Vec, and BoW. The results showed that ChatGPT's accuracy and UAR for the five personality classifications on the First Impressions dataset were lower than the baseline methods to varying degrees. On the Sentiment140 dataset, ChatGPT's accuracy and UAR were 85.5 and 85.5 , respectively, which were better than the three baseline methods. On the Suicide and Depression dataset, ChatGPT's accuracy and UAR were 92.7 and 91.2 , respectively, which were lower than RoBERTa, the best-performing baseline method.</p>
<p>Zhang et al. [106] employed ChatGPT for stance detection, which includes support and opposition. They used ChatGPT to classify the political stance of tweets in the SemEval-2016 and P-Stance datasets. SemEval-2016 contains 4,870 English tweets, and they selected tweets with the most commonly occurring FM, LA, and HC political labels for stance classification. The P-Stance dataset has 21,574 English tweets, and they classified the stance of tweets towards Trump, Biden, and Bernie. The final results showed that on the SemEval-2016 dataset, ChatGPT achieved F1-m scores of $68.4,58.2$, and 79.5 for the FM, LA, and HC political labels, and F1-avg scores of 72.6, 59.3, and 78.0, respectively. On the P-Stance dataset, ChatGPT achieved F1-m scores of $82.8,82.3$, and 79.4 for the Trump, Biden, and Bernie political figures, and F1-avg scores of 83.2, 82.0, and 79.4, respectively.</p>
<p>Huang et al. [32] used ChatGPT to detect implicit hate speech in tweets. They selected 12.5\% (795 tweets) of the LatentHatred dataset containing implicit hate speech and asked ChatGPT to classify them into three categories: implicit hate speech, non-hate speech, and uncertain. The results showed that ChatGPT correctly recognized 636 ( $80 \%$ ) of the</p>
<p>tweets. The number of tweets classified as non-hate speech and uncertain were 146 (18.4\%) and 13 (1.6\%), respectively. The results of the reclassification of tweets in the non-hate speech and uncertain categories by Amazon Mechanical Turk (Mturk) workers were consistent with ChatGPT's classification.</p>
<p>Overall, ChatGPT has tremendous potential in text classification tasks, as it can effectively address problems such as genre identification, sentiment analysis, stance detection, and more. However, there are still challenges that ChatGPT faces in the field of text classification. Firstly, it struggles to perform well in classification tasks with rare or out-ofvocabulary words since it heavily relies on the distribution of training data. Additionally, the significant computational resources required for training and utilizing ChatGPT can limit its use in some applications.</p>
<h1>2.1.3. Text Generation</h1>
<p>We live in an era of information explosion, and text is an efficient way of transmitting information. The diversity of information has led to a diversity of text categories. When researchers use ChatGPT's text generation capabilities for research, they inevitably choose to generate different types of text. In the process of reading papers, we found that the word count of the text generated by researchers increased from small to large, so we wanted to summarize existing research based on the size of the text word count. We divided the generated text into three levels: phrases, sentences, and paragraphs.</p>
<p>The following article uses ChatGPT to generate phrases. Zhang et al. [107] proves that the semantic HAR model with semantic augmentation added during training performs better in motion recognition than other models. Semantic augmentation requires shared tokens, which is lacking in some datasets. Therefore, authors leverage ChatGPT for an automated label generation approach for datasets originally without shared tokens. Fu et al. [18] described a new workflow for converting natural language commands into Bash commands. The author uses ChatGPT to generate a candidate list of Bash commands based on user input, and then uses a combination of heuristic and machine learning techniques to rank and select the most likely candidates. This workflow was evaluated on a real command dataset and achieved high accuracy compared to other state-of-the-art methods. Chen et al. [10] used the Bart model and ChatGPT for the task of summarizing humorous titles and compared the performance of the two models. It was found that the Bart model performed better on large datasets, but ChatGPT was competitive with our best fine-tuned model in a small range (48), albeit slightly weaker.</p>
<p>The following article uses ChatGPT to generate sentences. Chen et al. [9] constructed a dialogue dataset (HPD) with scenes, timelines, character attributes, and character relationships in order to use ChatGPT as a conversational agent to generate dialogue. However, ChatGPT's performance on the test set was poor, and there is room for improvement.In study [36], chatGPT demonstrated its ability to simplify complex text by providing three fictional radiology reports to chatGPT for simplification. Most radiologists found the simplified reports to be accurate and complete, with no potential harm to patients. However, some errors, omissions of critical medical information and text passages were identified, which could potentially lead to harmful conclusions if not understood by the physicians. Xia et al. [102] proposed a new program repair paradigm called Session-based Automated Program Repair (APR). In APR, the previously generated patches are iteratively built upon by combining them with validation feedback to construct the model's input. The effectiveness of the approach is verified using the QuixBugs dataset. The experiment shows that ChatGPT fine-tuned with reinforcement learning from human feedback (RLHF) outperforms Codex trained unsupervisedly in both repair datasets. In reference to study [37], ChatGPT was compared to three commercial translation products: Google Translate2, DeepL Translate3, and Tencent TranSmart4. The evaluation was conducted on the Flores101 test set, using the WMT19 biomedical translation task to test translation robustness, with BLEU score as the main metric. The study found that ChatGPT is competitive with commercial translation products on high-resource European languages but falls behind on low-resource or distant languages. The authors explored an interesting strategy called pivot prompts, which significantly improved translation performance. While ChatGPT did not perform as well as commercial systems on biomedical abstracts or Reddit comments, it may be a good speech translator. Prieto et al. [77] evaluated the use of ChatGPT in developing an automated construction schedule based on natural language prompts. The experiment required building new partitions in an existing space and providing details on the rooms to be partitioned. The results showed that ChatGPT was able to generate a coherent schedule that followed a logical approach to meet the requirements of the given scope. However, there were still several major flaws that would limit the use of this tool in real-world projects.Michail et al. [65] proposed a method to improve the prediction accuracy of the HeFit fine-tuned XLM_T model on tweet intimacy by generating a dataset of tweets with intimacy rating tags using ChatGPT. The specific operation is to input tweets with intimacy rating tags into ChatGPT and then output similar tweets.</p>
<p>The following article uses ChatGPT to generate paragraphs. Wang et al. [92] compared the abstract summarization performance of ChatGPT and other models on various cross-lingual text datasets and found that ChatGPT may perform worse in metrics such as R_1, R_2, R_L, and B_S. Yang et al. [103] summarized the performance of ChatGPT in question answering-based text summarization and found that, compared to fine-tuned models, ChatGPT's performance is slightly worse in all performance metrics. However, the article suggests that if the dataset is golden annotation, ChatGPT's performance may surpass fine-tuned models in these metrics. Belouadi et al. [5] compared the ability of ByGPT5 and ChatGPT trained on a range of labeled and unlabeled datasets of English and German poetry to generate constrained style poetry, and evaluated them using three metrics: Rhyme, ScoreAlliteration, and ScoreMeter Score. The conclusion is that ByGPT5 performs better than ChatGPT. Blanco-Gonzalez et al. [6] evaluated chatGPT's ability to write commentary articles, and in fact, this article itself was written by chatGPT. The human author rewrote the manuscript based on chatGPT's draft. Experts found that it can quickly generate and optimize text, as well as help users complete multiple tasks. However, in terms of generating new content, it is not ideal. Ultimately, it can be said that without strong human intervention, chatGPT is not a useful tool for writing reliable scientific texts. It lacks the knowledge and expertise required to accurately and fully convey complex scientific concepts and information. Khalil et al. [39] on the originality of content generated by ChatGPT. To evaluate the originality of 50 papers on various topics generated by ChatGPT, two popular plagiarism detection tools, Turnitin and iThenticate, were used. The results showed that ChatGPT has great potential in generating complex text output that is not easily captured by plagiarism detection software. The existing plagiarism detection software should update their plagiarism detection engines. Basic et al. [4] conducted a comparison of the writing performance of students using or not using ChatGPT-3 as a writing aid. The experiment consisted of two groups of 9 participants each. The control group wrote articles using traditional methods, while the experimental group used ChatGPT as an aid. Two teachers evaluated the papers. The study showed that the assistance of ChatGPT did not necessarily improve the quality of the students' essays.Noever et al. [68] discusses the potential of using artificial intelligence (AI), particularly language models like GPT (including GPT-3), to create more convincing chatbots that can deceive humans into thinking they are interacting with another person. The article describes a series of experiments in which they used GPT-3 to generate chatbot responses that mimic human-like conversations and were tested on human participants. The results show that some participants were unable to distinguish between the chatbot and a real human, highlighting the potential for these AI chatbots to be used for deceptive purposes.</p>
<h1>2.1.4. Code Generation</h1>
<p>Code generation refers to the process of automatically generating computer code from high-level descriptions or specifications. ChatGPT's advanced natural language processing capabilities make it capable of performing code generation tasks. By analyzing the requirements for code generation, ChatGPT can produce code snippets that accurately execute the intended functionality. This not only saves time and effort in writing code from scratch but also reduces the risk of errors that may occur during manual coding. In addition, ChatGPT's ability to learn and adapt to new programming languages and frameworks enables it to complete more complex programming tasks. For example:</p>
<p>Megahed et al. [64] discussed the potential of using ChatGPT for tasks such as code explanation, suggesting alternative methods for problem-solving with code, and translating code between programming languages. The solutions provided by ChatGPT were found to be viable. In another study, Treude et al. [90] introduced a ChatGPTbased prototype called GPTCOMCARE, which helps programmers generate multiple solutions for a programming problem and highlight the differences between each solution using colors.Sobania et al. [83] utilized ChatGPT for code bug fixing, and further improved the success rate of bug fixing by inputting more information through its dialogue system. Specifically, the QuixBugs standard bug fixing benchmark contained 40 code bugs that needed to be fixed. With limited information, ChatGPT fixed 19 bugs, which was slightly lower than the 21 bugs fixed by the Codex model, but significantly higher than the 7 fixed by the Standard APR model. When given more prompts and information, ChatGPT was able to fix 31 bugs, demonstrating its potential for code bug fixing tasks.Xia et al. [102] proposed a conversational approach for Automated Program Repair (APR), which alternates between generating patches and validating them against feedback from test cases until the correct patch is generated. Selecting 30 bugs from the QuixBugs standard bug fixing benchmark, which are suitable for test case feedback, and demonstrating them with Java and Python, the QuixBugs-Python and QuixBugs-Java datasets were obtained. The conversational APR using ChatGPT outperformed the conversational APR using Codex and the conversational APR using CODEGEN (with model parameters of 350M, 2B, 6B, and 16B) on both datasets. Furthermore, ChatGPT's conversational APR generated and validated patches with significantly fewer feedback loops than the other models.</p>
<p>ChatGPT can not only be used to achieve some simple code generation tasks but also can be used to accomplish some complex programming tasks. Noever et al. [69] tested ChatGPT's code generation capabilities on four datasets - Iris, Titanic, Boston Housing, and Faker. When prompted to mimic a Python interpreter in the form of a Jupyter notebook, the model was able to generate independent code based on the prompt and respond with the expected output. For example, when given the prompt "data.cor()" for the Iris dataset, ChatGPT generated correct Python output. The test results indicate that ChatGPT can access structured datasets and perform basic software operations required by databases, such as create, read, update, and delete (CRUD). This suggests that cutting-edge language models like ChatGPT have the necessary scale to tackle complex problems. McKee et al. [62] utilized ChatGPT as an experimental platform to investigate cybersecurity issues. They modeled five different modes of computer virus properties, including self-replication, self-modification, execution, evasion, and application, using ChatGPT. These five modes encompassed thirteen encoding tasks from credential access to defense evasion within the MITRE ATT\&amp;CK framework. The results showed that the quality of ChatGPT's generated code was generally above average, except for the self-replication mode, where it performed poorly.They [63] also employed ChatGPT as a network honeypot to defend against attackers. By having ChatGPT mimic Linux, Mac, and Windows terminal commands and providing interfaces for TeamViewer, nmap, and ping, a dynamic environment can be created to adapt to attackers' operations, and logs can be used to gain insight into their attack methods, tactics, and procedures. The authors demonstrated ten honeypot tasks to illustrate that ChatGPT's interface not only provides sufficient API memory to execute previous commands without defaulting to repetitive introductory tasks but also offers a responsive welcome program that maintains attackers' interest in multiple queries.</p>
<p>In the field of code generation, there are still several challenges with ChatGPT. Firstly, its application scope is limited as its training data is biased towards programming languages such as Python, C++, and Java, making it potentially unsuitable for some programming languages or coding styles. Secondly, manual optimization is necessary for code formatting, as the generated code may not be performance-optimized or follow best coding practices, requiring manual editing and optimization. Lastly, the quality of the generated code cannot be guaranteed, as it heavily relies on the quality of the natural language input, which may contain errors, ambiguities, or inconsistencies, ultimately affecting the accuracy and reliability of the generated code.</p>
<h1>2.1.5. Inference</h1>
<p>Inference refers to the process of drawing new conclusions or information through logical deduction from known facts or information. It is typically based on a series of premises or assumptions, and involves applying logical rules or reasoning methods to arrive at a conclusion. Inference is an important ability in human thinking, and is often used to solve problems, make decisions, analyze and evaluate information, etc. Inference also plays a key role in fields such as science, philosophy, law, etc. There are two types of inference: inductive reasoning, which involves deriving general rules or conclusions from known facts or experiences, and deductive reasoning, which involves deriving specific conclusions from known premises or assumptions. Whether inductive or deductive, the process of inference requires following strict logical rules to ensure the correctness and reliability of the inference.</p>
<p>Some papers attempt to use ChatGPT's ability in inductive reasoning to capture the meaning in text and use defined metrics to score the text. Michail et al. [65] uses ChatGPT to infer intimacy expressed in tweets. They first input 50 tweets with intimacy markers to ChatGPT, then use inductive reasoning to infer the standards for generating tweets with different levels of intimacy, and finally generate ten tweets with intimacy values ranging from 0 to 5 . Susnjak et al. [86] collected a large amount of textual data from patient-doctor discussion forums, patient testimonials, social media platforms, medical journals, and other scientific research publications. Using the BERT model, the author inferred emotion values from 0 to 1 . The author visualized the process of how the presence of bias in the discourse surrounding chronic manifestations of the disease using the SHAP tool. The author also envisioned ChatGPT as a replacement for the BERT model for scoring the emotional value of text. Huang et al. [32] chose $12.5 \%$ of individuals in the potential hate dataset as study materials, induced ChatGPT to make classifications based on a prompt, and ChatGPT produced three classifications: unclear, yes, and no. The author assigned a value of 1 to yes, -1 to no, and 0 to unclear, and had ChatGPT score and classify them. ChatGPT was able to correctly classify $80 \%$ of implicit hate tweets in the author's experimental setup, demonstrating ChatGPT's great potential as a data labeling tool using simple prompts.</p>
<p>Some papers have evaluated ChatGPT's reasoning performance, mainly in decision-making and spatial reasoning, and identifying ambiguity. Tang et al. [89] used the independence axiom and the transitivity axiom, as well as other non-VNM related decision-making abilities, by presenting bets conditioned on random events, bets with asymmetric outcomes, decisions encapsulating Savage's Sure Thing principle, and other complex bet structures like nested bets, to</p>
<p>design experiments where each experiment input a short prompt to ChatGPT and evaluated the results. The conclusion is that ChatGPT exhibits uncertainty in the decision-making process: in some cases, large language models can arrive at the correct answer through incorrect reasoning; and it may make suboptimal decisions for simple reasoning problems. Ortega-Martn et al. [72] had ChatGPT detect three different levels of language ambiguity and evaluated its performance. The conclusion is that In semantics, ChatGPT performed perfectly in the detection of ambiguities. Apart from that, it has some bright sports (co-reference resolution) and some weaknesses (puts gender bias over grammar in some non-ambiguous situations). In the generation task ChatGPT did well, but also revealed some of its worse issues: the lack of systematicity. Lastly, it should also be pointed that in most of the cases ChatGPT brilliantly alludes to lack of context as the key factor in disambiguation.</p>
<h1>2.1.6. Data or information extraction, transformation, enhancement, processing</h1>
<h2>Data Visualization</h2>
<p>Natural language interfaces have contributed to generating visualizations directly from natural language, but visualization problems remain challenging due to the ambiguity of natural language.ChatGPT provides a new avenue for the field by converting natural language into visualized code.</p>
<p>In terms of data visualization, Noever et al. [69] tested ChatGPT's basic arithmetic skills by asking questions.On the iris dataset, Titanic survival dataset, Boston housing data, and randomly generated insurance claims dataset, the statistical analysis of data and visualization problems were converted to programming problems using Jupyter to verify ChatGPT's ability to generate python code to draw suitable graphs and analyze the data. The results show that ChatGPT can access structured and organized datasets to perform the four basic software operations required for databases: create, read, update, and delete, and generate suitable python code to plot graphs for descriptive statistics, variable correlation analysis, describing trends, and other data analysis operations.Maddigan et al. [61] proposed an end-to-end solution for visualizing data in natural language using LLM, which uses an open-source python framework designed to generate appropriate hints for selected datasets to make LLM more effective in understanding natural language, and uses internal reasoning capabilities to select the appropriate visualization type to generate the code for visualization. In this paper,the reseachers compare the visualization results of GPT-3, Codex and ChatGPT in the case of nvBench SQLite database [59] and the visualization results of energy production dataset in the study of ADVISor with NL4DV [53; 67].In addition to, they explore the ability to reason and hypothesize of the LLM on movie dataset [59] when the hints are insufficient or wrong .Experimental results show that LLM can effectively support the end-to-end generation of visualization results from natural language when supported by hints, providing an efficient, reliable and accurate solution to the natural language visualization problem.</p>
<h2>Information Extraction</h2>
<p>The goal of information extraction is to extract specific information from natural language text for structured representation, including three important subtasks such as entity relationship extraction, named entity recognition, and event extraction, which have wide applications in business, medical, and other fields.</p>
<p>In information extraction, Wei et al. [97] proposed ChatIE, a ChatGPT-based multi-round question-and-answer framework for information extraction. The framework decomposes a complex information extraction (IE) task into several parts, then combines the results of each round into a final structured result. The entity association triple extraction, named entity recognition, and event extraction tasks were performed on six datasets NYT11-HRL, DuIE2.0 , conllpp, MSR , DuEE1.0 [87; 50; 96; 49; 51], and ACE05 in both languages, comparing three metrics of precision, recall, and F1 score. These results suggest that on six widely used IE datasets, ChatIE improves performance by an average of $18.98 \%$ compared to the original ChatGPT without ChatIE, and outperforms the supervised models FCM and MultiR [21;30] on the NYT11-HRL dataset. While the original ChatGPT cannot solve complex IE problems with original task instructions, and with this framework, successfully IE tasks were implemented on six datasets.Gao et al. [19] explored the feasibility and challenges of ChatGPT for event extraction on the ACE2005 corpus, evaluating the performance of ChatGPT in long-tail and complex scenarios (texts containing multiple events) and comparing it with two task-specific models, Text2Event and EEQA [57; 14].Then,they explored the impact of different cues on performance of ChatGPT. The results show that the average performance of ChatGPT in long-tail and complex scenarios is only $51.04 \%$ of that of task-specific models such as EEQA. Continuous refinement of cues does not lead to consistent performance improvements, and ChatGPT is highly sensitive to different cue styles.Tang et al. [88] proposed a new training paradigm that incorporates appropriate cues to guide ChatGPT to generate a variety of examples with</p>
<p>different sentence structures and language patterns and eliminate the resulting low-quality or duplicate samples for downstream tasks. Although compared to a soft model for a specific healthcare task, ChatGPT underperforms in Named Entity Recognition (NER) and Relationship Extraction (RE) tasks , in the Gene Association Database (GAD) Release; EU-ADR corpus for the RE task, the innovative training framework was able to train local models, with F1 scores improving from $23.37 \%$ to $63.99 \%$ for the named entity recognition task and from $75 \%$, while alleviating privacy concerns and time-consuming data collection and annotation problems.He et al. [28] proposed a contextual learning framework ICL- D3IE. this framework introduces formatted presentation, continuously iterates to update and improve the presentation, and then combines ChatGPT for text information extraction. In the paper, ICL-D3IE is compared with existing pre-trained models such as LiLT,BROS (in-distribution (ID) setting and out-of-distribution (OOD) setting) on datasets (FUNSD, CORD, and SROIE [35; 75; 34]).These results show that the ICL-D3IE method in all datasets and settings except for the ID setting on CORD are superior to other methods, with ICL-D3IE (GPT-3) F1 scores reaching $90.32 \%$ on FUNSD and $97.88 \%$ on SROIE; in the out-of-distribution (OOD) setting, ICL-D3IE performs much better than previous pre-trained methods on all datasets.Polak et al. [76] proposed ChatExtract method - consisting of a set of engineering prompts applied to a conversational LLM - for automatic data extraction. During experiment, they extracted a large number of sentences from hundreds of papers and randomly selected 100 sentences containing data and 100 sentences without data as test data. The results show that the accuracy and recall of LLM exceeded $90 \%$ and may be comparable to human accuracy in many cases; in addition to this, the experiments were conducted under the condition of removing follow-up prompts and not keeping the conversation compared to previous experiments, respectively. The accuracy of deleting follow-up questions dropped to $80.2 \%$ and the recall rate dropped to $88.0 \%$. Removing the conversational aspect and related information retention recall and accuracy dropped to $90.0 \%$ and $56.6 \%$, respectively, demonstrating the effect of information retention combined with purposeful redundancy on LLM information extraction performance.</p>
<h1>Quality Assessment</h1>
<p>For translation quality, text generation quality, manual assessment is usually effective but suffers from subjectivity and time-consuming, etc. It was found through exploration that ChatGPT has also achieved significant performance in automatic quality assessment.</p>
<p>In terms of quality assessment, Kocmi et al. [41] proposed a GPT-based translation quality assessment metric, GEMBA, which evaluates the translation of each fragment individually and then averages all the obtained scores to obtain a final system-level score. In the MQM2022 test set (English-German, English-Russian, and Chinese-English) [15], a scoring task was performed with a classification task to compare the accuracy [42] and kendall tau scores [16] of seven GPT models under four cue templates. The results showed that GEMBA had the highest system-level accuracy of $88.0 \%$ compared to more than 10 automatic metrics such as BLEU, and among the seven GPT models, ChatGPT accuracy is above $80 \%$, in addition to, the best performance can be obtained in the least constrained template, demonstrating the potential of LLM for translation quality assessment tasks, but the evaluation is only applicable at the system level and needs further improvement.Wang et al. [93] used ChatGPT as a natural language generation (NLG) evaluator to study the correlation with human judgment. On three datasets covering different NLG tasks, task- and aspect-specific cues were designed to guide ChatGPT for NLG evaluation in CNN/DM [29], OpenMEVAROC, and BAGEL for summary, story generation, and data-to-text scoring, respectively. Then, they compute Spearman coefficients [105],Pearson correlation coefficients [66]. Kendall's Tau score [38] to assess the correlation with human evaluations. The results show that ChatGPT is highly correlated with human judgments in all aspects, with correlation coefficients of 0.4 or more in all categories, showing its potential as an NLG indicator.</p>
<h2>Data Augmentation</h2>
<p>In natural language processing, text data augmentation is an effective measure to alleviate the problem of low data quantity and low quality training data, and ChatGPT has shown great potential in this regard.</p>
<p>In terms of data augmentation, Dai et al. [13] proposed a ChatGPT-based text data augmentation method that reformulates each sentence in the training sample into multiple conceptually similar but semantically different samples for classification tasks downstream of the Bert model.On text transcriptions and PubMed 20k datasets containing more than 8 hours of audio data of common medical symptom descriptions,experiments were conducted to compare cosine similarity and TransRate metrics with multiple data enhancement methods [33].This paper shows that compared with existing data enhancement methods, the proposed ChatAug method shows a double-digit improvement in sentence</p>
<p>classification accuracy and generates more diverse augmented samples while maintaining its accuracy, but the original model is not fine-tuned in the paper and suffers from a lack of domain knowledge, which may produce incorrect augmented data.</p>
<h1>Multimodal fusion</h1>
<p>ChatGPT can currently only process natural language directly, but with a cross-modal encoder, it can combine natural language with cross-modal processing to provide solutions for intelligent transportation, healthcare, and other fields.</p>
<p>In terms of multimodal data processing, Wu et al. [101] constructed a framework that Visual ChatGPT integrates with different Visual Foundation Models (VFMs) and then combines a series of hints to input visual information to ChatGPT to solve visual problems. The paper shows examples of visual tasks such as removing or replacing certain objects from images, interconversion between images and text, demonstrating the Visual ChatGPT has great potential and capability for different tasks. But there are issues during the task that requires a large number of hints to convert VFMs to language, invoke multiple VFMs to solve complex problems leading to limited real-time capability, and security and privacy issues. Zheng et al. [109] showed a text mining example of LLM for extracting self-driving car crash data from California crash news, analyzing a failure report example, and generating a crash report example based on keywords; introduced a use case concept of a smartphone-based framework for automatic LLM failure report generation, which absorbs multiple data sources captured by cell phone sensors and then transfers the data to a language space for text mining, inference and generation, and further outputs the key information needed to form a comprehensive fault report, demonstrating the potential of LLM for a variety of transportation tasks.</p>
<p>Nowadays, ChatGPT shows a wide range of applications in data visualization, information extraction, data enhancement, quality assessment, and multimodal data processing. But there are also issues on how to further utilize hints to effectively interact with ChatGPT, lack of ability to process and analyze data from devices such as sensors, and data privacy and security.</p>
<h2>Cueing Techniques</h2>
<p>Cue engineering provides important support for effective dialogue with large language models.White et al. [99] proposed a framework for cueing models applicable to different domains. This framework structures cues to interact with LLMs by providing specific rules and guidelines. Also, this paper presents a catalog of cueing patterns that have been applied to LLM interactions, as well as specific examples with and without cues. The advantages of the combinability of prompting patterns are demonstrated, allowing users to interact with LLM more effectively, but patterns for reusable solutions and new ways to use LLM need to be continuously explored.</p>
<h3>2.1.7. Human-ChatGPT Collaboration</h3>
<p>Collaboration between humans and machines is a process where humans and machines work together to achieve a common goal. In such collaboration, humans provide domain expertise, creativity, and decision-making abilities, while machines provide automation, scalability, and computing power. ChatGPT is an advanced natural language processing model that can understand and generate human-like language, thereby reducing communication costs. Its ability to process and generate natural language makes it an ideal partner for human collaboration. ChatGPT can offer relevant suggestions, complete tasks based on human input, and enhance human productivity and creativity. It can learn from human feedback and adapt to new tasks and domains, further improving its performance in human-machine collaboration. ChatGPT's capability to comprehend natural language and produce appropriate responses makes it a valuable tool for various collaboration applications, as demonstrated by several studies in the literature we have gathered.</p>
<p>Ahmad et al. [1] proposed a method for human-machine collaboration using ChatGPT to create software architecture. This method transforms software stories (created by software architects based on application scenarios) into feasible software architecture diagrams through continuous interaction between the software architect and ChatGPT. During the evaluation stage, ChatGPT uses the Software Architecture Analysis Method (SAAM) to evaluate each component in the software architecture and generate evaluation reports. This method efficiently utilizes the knowledge and supervision of the architect with the capabilities of ChatGPT to collaboratively build software-intensive systems and services. Lanzi et al. [47] proposed a collaborative design framework that combines interactive evolution and ChatGPT to simulate typical human design processes. Humans collaborate with large language models (such as ChatGPT) to recombine and transform ideas, and use genetic algorithms to iterate through complex creative tasks.</p>
<p>The results of three game design tasks showed that the framework received positive feedback from game designers. The framework has good reusability and can be applied to any design task that can be described in free text form.</p>
<p>In the future, ChatGPT's ability to understand nonverbal cues such as tone of voice and body language can be enhanced, enabling it to better understand human thoughts and interact with people more effectively.</p>
<h1>2.1.8. ChatGPT Integration</h1>
<p>Integration refers to combining different systems or software components to achieve a common goal. ChatGPT can be integrated as a part of a whole or act as an integration tool to enable seamless communication between different systems. Its natural language processing ability makes it easier for non-technical users to interact with systems, reducing the need for specialized knowledge or training. Some studies in the literature we collected have already demonstrated this.</p>
<p>Treude et al. [90] integrated ChatGPT into the prototype of "GPTCOMCARE" to address programming query problems. This integration allowed for the generation of multiple source code solutions for the same query, which increased the efficiency of software development. The results of their study demonstrated the effectiveness of using ChatGPT to improve the quality and diversity of code solutions, ultimately reducing the amount of time and effort required for software development. Wang et al. [94] proposed the chatCAD method, which utilizes large language models (LLMs) such as ChatGPT to enhance the output of multiple CAD networks for medical images, including diagnosis, lesion segmentation, and report generation networks. The method generates suggestions in the form of a chat dialogue. The authors tested the effectiveness of the method on a randomly selected set of 300 cases from the MIMIC-CXR dataset, which included 50 cases each of cardiomegaly, edema, consolidation, atelectasis, pleural effusion, and no findings. Compared to CvT2DistilGPT2 and R2GenCMN, chatCAD showed significant advantages in RC and F1, while only performing weaker than R2GenCMN in PR.</p>
<p>Integrating ChatGPT into applications will still present challenges. Firstly, ChatGPT's performance may be affected by language barriers or differences in terminology between different systems. Additionally, ChatGPT's responses are not always deterministic, which poses a challenge when integrating with systems that require precise and reproducible results. Finally, the processing time of ChatGPT is slow for integration tasks involving time-sensitive data such as traffic, which is a limitation in time-critical environments.</p>
<h3>2.1.9. Medical Applications</h3>
<p>ChatGPT offers promising applications in medical field, revolutionizing healthcare practices. Its natural language processing capabilities enable interactive assistance for radiologists, aiding in image annotation, lesion detection, and classification. ChatGPT's extensive knowledge base facilitates real-time feedback, context-specific recommendations, and streamlined report generation. By integrating ChatGPT into workflows, healthcare professionals benefit from enhanced efficiency and precision in clinical decision-making, fostering accessible and collaborative healthcare solutions. For example:</p>
<p>ChatCAD [94] integrates large language models (LLMs) into computer-aided diagnosis (CAD) networks for medical imaging. It has shown promising results in improving diagnosis, lesion segmentation, and report generation, three key aspects of CAD networks. This integration represents a notable effort in combining large language models with medical imaging techniques.</p>
<p>Hu et al. [31] conducted a comprehensive review of language models in the context of medical imaging and highlighted the potential advantages of ChatGPT in enhancing clinical workflow efficiency, reducing diagnostic errors, and supporting healthcare professionals. Their work aims to bridge the gap between large language models and medical imaging, paving the way for new ideas and innovations in this research domain.</p>
<p>Ma et al. [60] proposed ImpressionGPT, a novel approach that harnesses the powerful in-context learning capabilities of ChatGPT. They achieve this by creating dynamic contexts using domain-specific and individualized data. The dynamic prompt method enables the model to learn contextual knowledge from semantically similar examples in existing data and iteratively optimize the results, aiding radiologists in composing the "impression" section based on the "findings" section. The results demonstrate state-of-the-art performance on both the MIMIC-CXR and OpenI datasets, without the need for additional training data or fine-tuning of the LLMs.</p>
<p>AD-AutoGPT [12], an integration of AutoGPT [22], leverages the power of ChatGPT in an automated processing pipeline that can assist users in accomplishing nearly any given task. With AD-AutoGPT, users can autonomously generate data collection, processing, and analysis pipelines based on their text prompts. Through AD-AutoGPT, detailed trend analysis, mapping of topic distances, and identification of significant terms related to Alzheimer's</p>
<p>disease (AD) have been achieved from four new sources specifically relevant to AD. This significantly contributes to the existing knowledge base and facilitates a nuanced understanding of discourse surrounding diseases in the field of public health. It lays the groundwork for future research in AI-assisted public health studies.</p>
<p>Patient privacy protection has always been a significant concern in the healthcare field. DeID-GPT [55] aims to explore the potential of ChatGPT in the de-identification and anonymization of medical reports. Experimental results demonstrate that ChatGPT exhibit promising capabilities in medical data de-identification compared to other LLMs.</p>
<p>Despite notable efforts, the integration of large language models and medical imaging still presents several challenges. Firstly, the intricate and technical nature of medical imaging data, which encompasses detailed anatomical structures and subtle abnormalities, may not be effectively conveyed or comprehended through the text-based chat interface of large language models. Secondly, ChatGPT lacks the specialized medical knowledge and training necessary for precise interpretation and analysis of medical images, potentially leading to dangerous misunderstandings or inaccurate diagnoses [52]. It is imperative to establish various machine learning models to detect samples generated by both humans and ChatGPT, in order to prevent false medical information produced by ChatGPT from causing misjudgments in disease progression, delaying treatment processes, or negatively impacting patients' lives and health. Lastly, the legal and ethical aspects associated with deploying artificial intelligence models like ChatGPT in a medical context, such as patient privacy and liability concerns, must be thoughtfully addressed and aligned with regulatory standards. While ChatGPT is powerful, it is not easily applicable in clinical settings. Compliance with HIPAA regulations, privacy issues, and the necessity for IRB approval pose significant obstacles [55], primarily because these models require uploading patient data to external hosting platforms. One possible solution to this problem is to address it through localized deployment of language models, such as Radiology-GPT [56]. The future application of chatGPT in the field of medical imaging will necessitate ongoing efforts from all stakeholders.</p>
<h1>2.2. AI Ethics</h1>
<p>Since the advent of ChatGPT, this powerful natural language processing model has not only brought great convenience to people but also triggered more crisis-aware thinking. Some researchers have started to hypothesize and study the potential negative impacts of ChatGPT. This proactive research provides good proposals for standardized construction to address future AI abuse issues.</p>
<p>Regarding the possibility of ChatGPT being used for plagiarism and cheating, Zhou et al. [111] reflected on the current state of development of artificial intelligence like ChatGPT. As ChatGPT becomes increasingly easy to obtain and scalable in text generation, there is a high likelihood that these technologies will be used for plagiarism, including scientific literature and news sources, posing a great threat to the credibility of various forms of news media and academic articles. Some scholars are concerned that the end of paper as a meaningful evaluation tool may be approaching [100; 104], as ChatGPT can easily generate persuasive paragraphs, chapters, and papers on any given topic. Additionally, it will exacerbate plagiarism issues in many fields such as education, medicine, and law [48], and may be used for cheating in academic exams [85]. Definitional recognition technology is a relatively effective method for detecting plagiarism, and the definitional typology proposed in [111] can alleviate people's concerns by being used to construct new datasets. Susnjak [85] proposed a solution to the possibility of large language models like ChatGPT being used for exam cheating: guiding ChatGPT to generate some critical thinking problems through questioning, then providing answers and critically evaluating them. Analysis of ChatGPT shows that it exhibits critical thinking, can generate highly realistic text in terms of accuracy, relevance, depth, breadth, logic, persuasiveness, and originality. Therefore, educators must be aware of the possibility of ChatGPT being used for exam cheating and take measures to combat cheating behavior to ensure the fairness of online exams.</p>
<p>Regarding the evaluation of ChatGPT's own political and ethical tendencies, Hartmann et al. [27] used Wahl-OMat, one of the most commonly used voting advice applications in the world, to show ChatGPT political statements from different parties, forcing it to make choices of agree, disagree, or neutral. The results indicated that ChatGPT has a pro-environment, left-wing liberal ideology, which was also confirmed in the nation-state agnostic political compass test. Another study (referenced as [45]) examined ChatGPT's moral standards by repeatedly asking it different versions of the trolley problem, and found that ChatGPT gave answers with different moral orientations, lacking a firm moral stance. A subsequent test also found that ChatGPT's lack of consistency could affect people's moral judgments. Additionally, Borji et al. [7] demonstrated ChatGPT's inconsistency in reasoning, factual errors, mathematics, coding, and bias across eleven related aspects. These findings highlight ChatGPT's inherent traits and limitations, and people should be aware of their potential impact when seeking advice from ChatGPT. Zhuo et al. [112] comprehensively analyzed the moral hazard, bias, reliability, robustness, and toxicity of ChatGPT from four perspectives. The results</p>
<p>found that ChatGPT may perform slightly better than the current SOTA language model, but has some shortcomings in all four aspects. The authors look ahead to the ethical challenges of developing advanced language models and suggest directions and strategies for designing ethical language models.</p>
<p>Regarding relevant policies and regulations, Hacker et al. [25] discussed the nature and rules of large generative AI models, including ChatGPT, which are rapidly changing the way we communicate, explain, and create. The author suggested that different stakeholders in the value chain should take regulatory responsibility and deploy four strategies to tailor more comprehensive laws for the benefit of society. Another study (referenced as [24]) criticized the European Commission's proposal on AI responsibility and suggested revising the proposed AI responsibility framework to ensure effective compensation while promoting innovation, legal certainty, and sustainable AI regulation. A policy framework was proposed (referenced as [40]) to customize LLMs, such as ChatGPT, in a socially acceptable and safe manner, emphasizing the need to align large language models (LLMs) with human preferences.</p>
<p>The political and ethical tendencies of ChatGPT could influence users' behavior and decision-making to some extent. However, some studies have conducted in-depth research on the use of norms and limitations, which could enable humans to use ChatGPT more reasonably and safely.</p>
<h1>3. Evaluation</h1>
<h3>3.1. Comparison of ChatGPT with existing popular models</h3>
<p>We use publicly available datasets to comprehensively evaluate the strengths and limitations of ChatGPT. Reference [3] evaluates the technical performance of ChatGPT in multitask, multilingual, and multimodal aspects based on 23 standard public datasets and newly designed multimodal datasets, including eight different common natural language processing application tasks. The experimental results show that, in terms of multitasking, ChatGPT outperforms various state-of-the-art zero-shot learning large language models in most tasks, and even outperforms fine-tuned taskspecific models in some individual tasks. In terms of multilingualism, we found that ChatGPT cannot be applied to low-resource languages because it cannot understand the language and generate translations for that language. In terms of multimodality, ChatGPT's ability is still basic compared to specialized language-visual models.</p>
<p>In terms of stability, reference [43] concludes that ChatGPT's performance is always lower than SOTA, the current state-of-the-art model, in almost all tasks. This means that as a general model, ChatGPT has never reached the level of the best existing models. Experimental data shows that the average quality of the SOTA model is $73.7 \%$, while the average quality of the ChatGPT model is only $56.5 \%$. At the same time, ChatGPT's stability is poor: the standard deviation of its performance is $23.3 \%$, while the SOTA model's standard deviation is only $16.7 \%$. This nondeterministic behavior exhibited by ChatGPT could be a serious drawback in some problems.</p>
<p>Similarly, Qin et al. [78] conducted a comprehensive evaluation of whether ChatGPT is a qualified general natural language processing task solver. The experiment analyzed ChatGPT's zero-shot learning ability based on 20 commonly used public datasets covering 7 representative task categories. Below, we will analyze ChatGPT's performance on each task:</p>
<p>In terms of reasoning tasks, ChatGPT performs average on mathematical symbol, commonsense causal, and logical reasoning tasks, but performs well in arithmetic reasoning [78]. That is to say, ChatGPT's abilities vary among different types of reasoning tasks. In terms of logical reasoning, ChatGPT's deductive and abductive reasoning are superior to inductive reasoning, while in other reasoning tasks, such as analogy, causal and commonsense reasoning, ChatGPT performs well [3].</p>
<p>In terms of sentiment analysis task, ChatGPT performs similarly to GPT-3.5 and bert-style models [78; 110]. However, according to literature [43], ChatGPT has losses not exceeding $25 \%$ on most tasks, except for three relatively subjective emotion perception tasks where it performs poorly. If we remove these tasks to calculate the average quality of the two models, we find that the SOTA method has an average quality of $80 \%$, while the ChatGPT method has an average quality of $69.7 \%$. That is to say, ChatGPT performs well on all tasks except for emotion-related tasks, and can handle most of the problems we consider. However, overall, its performance is lower than the SOTA model based on experimental data, but the difference between the two is not very large.</p>
<p>In other tasks, according to literature [78], ChatGPT performs well in natural language inference, i.e., the task of inferring sentence relationships, and its performance on this task is significantly better than all bert-style models [110]. However, while ChatGPT performs well on inference tasks, it may produce some self-contradictory or unreasonable responses, which is its potential limitation. In question-answering, dialogue, and summarization tasks, ChatGPT performs better than the GPT-3.5 model [78], especially in the question-answering task, where its performance is</p>
<p>comparable to bert-style models [110]. Therefore, we have demonstrated that ChatGPT is a qualified general-purpose model.</p>
<p>However, ChatGPT also has limitations in many aspects. Firstly, it lacks the ability to handle non-textual semantic reasoning tasks such as mathematical, temporal, and spatial reasoning, and it performs poorly in multi-hop reasoning [3]. Secondly, ChatGPT is not good at solving named entity recognition tasks [78]. Furthermore, ChatGPT performs poorly in handling tasks involving negative connotations and neutral similarity [110]. Finally, these conclusions indicate that, like other large pre-trained language models, ChatGPT has limitations in completing complex reasoning tasks.</p>
<p>In summary, ChatGPT's zero-shot performance is comparable to fine-tuned bert and GPT-3.5 models, and with the help of advanced prompting strategies, ChatGPT can demonstrate better comprehension abilities. However, it still cannot outperform the current SOTA models.</p>
<h1>3.2. Feedback from ChatGPT users</h1>
<p>In response to feedback from ChatGPT users, Haque et al. [26] conducted a mixed-methods study using 10,732 early ChatGPT user tweets. The authors extracted Twitter data using Python and Twitter API and constructed the ChatGPTTweet dataset, which contains 18 k tweets. For each tweet, the authors collected information on text content, user location, occupation, verification status, date of publication, and tags. Based on this dataset, the authors studied the characteristics of early ChatGPT users, discussion topics related to ChatGPT on Twitter, and the sentiment of Twitter users toward ChatGPT. For RQ1, the authors found that early ChatGPT users had a diverse and wide range of occupational backgrounds and geographical locations. For RQ2, the authors identified nine topics related to ChatGPT, including its impact on software development, entertainment and creativity, natural language processing, education, chatbot intelligence, business development, search engines, question-answering tests, and future careers and opportunities. For RQ3, most early users expressed positive sentiment toward topics such as software development and creativity, while only a few expressed concern about the potential misuse of ChatGPT.</p>
<h3>3.3. Adverse effects of ChatGPT on users</h3>
<p>Regarding the negative effects of ChatGPT on users, Luan et al. [58] studied the psychological principles of ChatGPT, delved into the factors that attract users' attention, and revealed the impact of these factors on future learning. In the post-pandemic era, teachers and students are both facing uncertainty in the teaching process and job pressures. Under these common constraints of education and employment, educators and students must re-evaluate current educational methods and outcomes, as well as students' future career development. Through question-andanswer exchanges with ChatGPT, people can easily obtain appropriate solutions or key information, thereby enhancing their motivation, eliminating anxiety in learning, improving interest, and achieving psychological satisfaction. Subhash et al. [84] explored whether large language models have the ability to reverse user preferences. With the development of pre-trained large language models, people are increasingly concerned about the ability of these models to influence, persuade, and potentially manipulate user preferences in extreme cases. Therefore, the literature [84] roughly qualitatively analyzed that adversarial behavior does lead to potential changes in user preferences and behaviors in dialogue systems. If we want to further quantitatively analyze the ability of large language models in this regard, additional statistical summary techniques need to be used for future research.</p>
<h2>4. Discussion</h2>
<h3>4.1. Limitations</h3>
<p>Despite the remarkable capabilities of ChatGPT, it still faces certain limitations. Some of these limitations include:</p>
<h2>Outdated Knowledge</h2>
<p>The current models are trained on historical data (up to 2021), thereby lacking real-time comprehension of current affairs. This is a critical concern in today's information-explosion era, as the reliability of prior knowledge bases progressively diminishes, potentially yielding inaccurate responses, especially in rapidly evolving domains such as jurisprudence and technology. Additionally, these models are incapable of fact-checking while the training data is composed of content from various sources, some of which may be unreliable, which may result in seemingly plausible yet nonsensical responses.</p>
<h1>Insufficient Understanding</h1>
<p>While these models can interpret the majority of inquiries and contextual situations, they occasionally encounter comprehension biases when addressing ambiguous or contextually complex queries. Furthermore, in certain specialized fields, the abundance of unique abbreviation exacerbates the models' understanding challenges, resulting in incorrect and vacuous responses.</p>
<h2>Energy Consumption</h2>
<p>Throughout the training and inference stages, these large-scale models require significant computational resources and electrical power, resulting in elevated energy consumption and significant carbon emissions. Consequently, this restricts their deployment and practical applications.</p>
<h2>Malicious Usage</h2>
<p>Despite OpenAI implementing a series of restrictions to mitigate model toxicity, instances of users evading these constraints through meticulously designed prompts have emerged, inducing the model to produce unhealthy content or even using it for illicit commercial purposes.</p>
<h2>Bias and Discrimination</h2>
<p>Due to the influence of pre-training data, the models exhibit biases in political, ideological, and other areas. The application of LLMs in public domains, such as education and publicity, should be approached with extreme caution.</p>
<h2>Privacy and Data Security</h2>
<p>Concurrent with the expansion of users, protecting user privacy and data security becomes increasingly important. In fact, ChatGPT was banned in Italy in early April due to privacy concerns. This is particularly relevant given the models' extensive collection of personal information and preferences during interactions, and as future multimodal models, such as GPT-4, may frequently require users to upload private photos.</p>
<h3>4.2. Future Directions</h3>
<p>In forthcoming research, the development of models based on ChatGPT may focus on addressing these limitations to enhance their practical applications.</p>
<p>Primarily, researchers should continue to work on refining model training methodologies while filtering pre-training data to minimize the presence of misleading information in the model's knowledge base, thereby obtaining accurate responses. Concurrently, it is crucial to emphasize training approaches that economize computational resources, thereby mitigating costs and broadening potential application scenarios.</p>
<p>Moreover, the advancements in context-awareness and disambiguation technologies are anticipated to facilitate enhanced comprehension of complex queries by models, improving the accuracy, relevance, and context-awareness of AI-generated content. Integrating real-time data streams can also keep these models in sync with current events and trends, enabling them to provide up-to-date information such as live traffic, weather, and stock updates.</p>
<p>Additionally, developers should engage in interdisciplinary collaboration with specialists from diverse domains, including policy-making, jurisprudence, and sociology, with the objective of formulating standard and ethical frameworks for LLM development, deployment, and utilization, thereby alleviating potential harmful consequences. In terms of public awareness and education, mandatory awareness training should be implemented prior to large-scale public deployment and application to increase public awareness of LLM capabilities and limitations while promoting responsible and informed utilization, especially in industries such as K-12 education and journalism.</p>
<p>Furthermore, ChatGPT still lacks specific domain knowledge and may encounter potential data security issues, especially in the medical field. In domains where error tolerance is low and data privacy and security are crucial, such as medical applications [55], localized training and deployment of LLMs should be considered [56]. Customizing training for specific LLMs based on domain-specific data should also be taken into account.</p>
<p>Finally, the influence of ChatGPT should not be limited to just the NLP field. They also show promising prospects in the areas of computer vision, brain-inspired AI, and robotics. These models exhibit a capacity for learning and comprehension comparable with human-level intelligence, positioning them as a pivotal component in the development of artificial general intelligence (AGI)[108]. Their ability to facilitate seamless interactions between humans and robots paves the way for the execution of more complex tasks. The remarkable capacity of zero-shot in-context learning of</p>
<p>these models enables quick adaptation to new tasks without the requirement for labeled data for fine-tuning, which is a critical challenge in fields like medical informatics[55] and robotics[54] where the availability of labeled data is commonly limited or non-existent.</p>
<h1>5. Conclusion</h1>
<p>This review paper provides a comprehensive survey of ChatGPT, highlighting their potential applications and significant contributions to the field of natural language processing. The findings of this study reveal that the interest in these models is growing rapidly, and they have shown considerable potential for application across a wide range of domains. One key factor contributing to the success of ChatGPT is their ability to perform large-scale pre-training, which captures knowledge from the vast expanse of the internet, allowing the models to learn from a massive amount of data. The integration of Reinforcement Learning from Human Feedback (RLHF) has further enhanced the model's adaptability and performance, making it highly efficient in processing natural language. In addition, RLHF aligns language models with human preferences \&amp; values and empower text generation with the naturalness of human style. This study has also identified several potential ethical concerns related to the development and use of ChatGPT. For instance, there are concerns about the generation of biased or harmful content, privacy violations, and the potential for misuse of the technology. It is crucial to address these concerns and ensure that ChatGPT is developed and used in a responsible and ethical manner. Furthermore, the results of this study demonstrate that there is significant potential for ChatGPT to be applied in a range of domains, including education, medical, history, mathematics, physics, and more. These models can facilitate tasks such as generating summaries, answering questions, and providing personalized recommendations to users. Overall, the insights presented in this review paper can serve as a useful guide for researchers and practitioners looking to advance the field of natural language processing. Future research in this field should focus on addressing ethical concerns, exploring new applications, and ensuring the responsible use of ChatGPT. The potential of these models to revolutionize natural language processing is enormous, and we look forward to seeing more developments in this field.</p>
<h2>Acknowledgement</h2>
<p>This work was supported by the National Natural Science Foundation of China (No. 61976131).</p>
<h2>References</h2>
<p>[1] Ahmad, A., Waseem, M., Liang, P., Fehmideh, M., Aktar, M.S., Mikkonen, T.: Towards human-bot collaborative software architecting with chatgpt. arXiv preprint arXiv:2302.14600 (2023)
[2] Amin, M.M., Cambria, E., Schuller, B.W.: Will affective computing emerge from foundation models and general ai? a first evaluation on chatgpt. arXiv preprint arXiv:2303.03186 (2023)
[3] Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al.: A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 (2023)
[4] Basic, Z., Banovac, A., Kruzic, I., Jerkovic, I.: Better by you, better than me, chatgpt3 as writing assistance in students essays. arXiv preprint arXiv:2302.04536 (2023)
[5] Belouadi, J., Eger, S.: Bygpt5: End-to-end style-conditioned poetry generation with token-free language models. arXiv preprint arXiv:2212.10474 (2022)
[6] Blanco-Gonzalez, A., Cabezon, A., Seco-Gonzalez, A., Conde-Torres, D., Antelo-Riveiro, P., Pineiro, A., Garcia-Fandino, R.: The role of ai in drug discovery: Challenges, opportunities, and strategies. arXiv preprint arXiv:2212.08104 (2022)
[7] Borji, A.: A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494 (2023)
[8] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020)
[9] Chen, N., Wang, Y., Jiang, H., Cai, D., Chen, Z., Li, J.: What would harry say? building dialogue agents for characters in a story. arXiv preprint arXiv:2211.06869 (2022)
[10] Chen, Y., Eger, S.: Transformers go for the lols: Generating (humourous) titles from scientific abstracts end-to-end. arXiv preprint arXiv:2212.10522 (2022)
[11] Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017)
[12] Dai, H., Li, Y., Liu, Z., Zhao, L., Wu, Z., Song, S., Shen, Y., Zhu, D., Li, X., Li, S., et al.: Ad-autogpt: An autonomous gpt for alzheimer's disease infodemiology. arXiv preprint arXiv:2306.10095 (2023)
[13] Dai, H., Liu, Z., Liao, W., Huang, X., Wu, Z., Zhao, L., Liu, W., Liu, N., Li, S., Zhu, D., et al.: Chataug: Leveraging chatgpt for text data augmentation. arXiv preprint arXiv:2302.13007 (2023)
[14] Du, X., Cardie, C.: Event extraction by answering (almost) natural questions. arXiv preprint arXiv:2004.13625 (2020)</p>
<p>[15] Freitag, M., Rei, R., Mathur, N., Lo, C.k., Stewart, C., Avramidis, E., Kocmi, T., Foster, G., Lavie, A., Martins, A.F.: Results of wmt22 metrics shared task: Stop using bleu-neural metrics are better and more robust. In: Proceedings of the Seventh Conference on Machine Translation (WMT). pp. 46-68 (2022)
[16] Freitag, M., Rei, R., Mathur, N., Lo, C.k., Stewart, C., Avramidis, E., Kocmi, T., Foster, G., Lavie, A., Martins, A.F.: Results of wmt22 metrics shared task: Stop using bleu-neural metrics are better and more robust. In: Proceedings of the Seventh Conference on Machine Translation (WMT). pp. 46-68 (2022)
[17] Frieder, S., Pinchetti, L., Griffiths, R.R., Salvatori, T., Lukasiewicz, T., Petersen, P.C., Chevalier, A., Berner, J.: Mathematical capabilities of chatgpt. arXiv preprint arXiv:2301.13867 (2023)
[18] Fu, Q., Teng, Z., Georgaklis, M., White, J., Schmidt, D.C.: Nl2cmd: An updated workflow for natural language to bash commands translation. arXiv preprint arXiv:2302.07845 (2023)
[19] Gao, J., Zhao, H., Yu, C., Xu, R.: Exploring the feasibility of chatgpt for event extraction. arXiv preprint arXiv:2303.03836 (2023)
[20] Glymour, C., Zhang, K., Spirtes, P.: Review of causal discovery methods based on graphical models. Frontiers in Genetics (2019)
[21] Gormley, M.R., Yu, M., Dredze, M.: Improved relation extraction with feature-rich compositional embedding models. arXiv preprint arXiv:1505.02419 (2015)
[22] Gravitas, S.: Auto-gpt: An autonomous gpt-4 experiment (2023)
[23] Guo, S., Wang, Y., Li, S., Saeed, N.: Semantic communications with ordered importance using chatgpt. arXiv preprint arXiv:2302.07142 (2023)
[24] Hacker, P.: The european ai liability directives-critique of a half-hearted approach and lessons for the future. arXiv preprint arXiv:2211.13960 (2022)
[25] Hacker, P., Engel, A., Mauer, M.: Regulating chatgpt and other large generative ai models. arXiv preprint arXiv:2302.02337 (2023)
[26] Haque, M.U., Dharmadasa, I., Sworna, Z.T., Rajapakse, R.N., Ahmad, H.: " i think this is the most disruptive technology": Exploring sentiments of chatgpt early adopters using twitter data. arXiv preprint arXiv:2212.05856 (2022)
[27] Hartmann, J., Schwenzow, J., Witte, M.: The political ideology of conversational ai: Converging evidence on chatgpt's pro-environmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768 (2023)
[28] He, J., Wang, L., Hu, Y., Liu, N., Liu, H., Xu, X., Shen, H.T.: Icl-d3ai: In-context learning with diverse demonstrations updating for document information extraction. arXiv preprint arXiv:2303.05063 (2023)
[29] Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom, P.: Teaching machines to read and comprehend. Advances in neural information processing systems 28 (2015)
[30] Hoffmann, R., Zhang, C., Ling, X., Zettlemoyer, L., Weld, D.S.: Knowledge-based weak supervision for information extraction of overlapping relations. In: Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. pp. $541-550(2011)$
[31] Hu, M., Pan, S., Li, Y., Yang, X.: Advancing medical imaging with language models: A journey from n-grams to chatgpt. arXiv preprint arXiv:2304.04920 (2023)
[32] Huang, F., Kwak, H., An, J.: Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736 (2023)
[33] Huang, L.K., Huang, J., Rong, Y., Yang, Q., Wei, Y.: Frustratingly easy transferability estimation pp. 9201-9225 (2022)
[34] Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., Jawahar, C.: Icdar2019 competition on scanned receipt ocr and information extraction. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1516-1520. IEEE (2019)
[35] Jaume, G., Ekenel, H.K., Thiran, J.P.: Funsd: A dataset for form understanding in noisy scanned documents. In: 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW). vol. 2, pp. 1-6. IEEE (2019)
[36] Jeblick, K., Schachtrner, B., Dexl, J., Mittermeier, A., Stüber, A.T., Topalis, J., Weber, T., Wesp, P., Sabel, B., Ricke, J., et al.: Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports. arXiv preprint arXiv:2212.14882 (2022)
[37] Jiao, W., ZhaopengTu, W.J.t.X.: Is chatgpt a good translator? yes with gpt-4 as the engine
[38] Kendall, M.G.: A new measure of rank correlation. Biometrika 30(1/2), 81-93 (1938)
[39] Khalil, M., Er, E.: Will chatgpt get you caught? rethinking of plagiarism detection. arXiv preprint arXiv:2302.04335 (2023)
[40] Kirk, H.R., Vidgen, B., Röttger, P., Hale, S.A.: Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback. arXiv preprint arXiv:2303.05453 (2023)
[41] Kocmi, T., Federmann, C.: Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520 (2023)
[42] Kocmi, T., Federmann, C., Grundkiewicz, R., Junczys-Dowmunt, M., Matsushita, H., Menezes, A.: To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. arXiv preprint arXiv:2107.10821 (2021)
[43] Kocoń, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., Bielaniewicz, J., Gruza, M., Janz, A., Kanclerz, K., et al.: Chatgpt: Jack of all trades, master of none. arXiv preprint arXiv:2302.10724 (2023)
[44] Kortemeyer, G.: Could an artificial-intelligence agent pass an introductory physics course? arXiv preprint arXiv:2301.12127 (2023)
[45] Krügel, S., Ostermaier, A., Uhl, M.: The moral authority of chatgpt. arXiv preprint arXiv:2301.07098 (2023)
[46] Kuzman, T., Mozetic, I., Ljubešic, N.: Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification. arXiv e-prints pp. arXiv-2303 (2023)
[47] Lanzi, P.L., Loiacono, D.: Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. arXiv preprint arXiv:2303.02155 (2023)
[48] Lehnert, K.: Ai insights into theoretical physics and the swampland program: A journey through the cosmos with chatgpt. arXiv preprint arXiv:2301.08155 (2023)
[49] Levow, G.A.: The third international chinese language processing bakeoff: Word segmentation and named entity recognition. In: Proceedings of the Fifth SIGHAN workshop on Chinese language processing. pp. 108-117 (2006)</p>
<p>[50] Li, S., He, W., Shi, Y., Jiang, W., Liang, H., Jiang, Y., Zhang, Y., Lyu, Y., Zhu, Y.: Duie: A large-scale chinese dataset for information extraction. In: Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9-14, 2019, Proceedings, Part II 8. pp. 791-800. Springer (2019)
[51] Li, X., Li, F., Pan, L., Chen, Y., Peng, W., Wang, Q., Lyu, Y., Zhu, Y.: Duee: a large-scale dataset for chinese event extraction in real-world scenarios. In: Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14-18, 2020, Proceedings, Part II 9. pp. 534-545. Springer (2020)
[52] Liao, W., Liu, Z., Dai, H., Xu, S., Wu, Z., Zhang, Y., Huang, X., Zhu, D., Cai, H., Liu, T., et al.: Differentiate chatgpt-generated and humanwritten medical texts. arXiv preprint arXiv:2304.11567 (2023)
[53] Liu, C., Han, Y., Jiang, R., Yuan, X.: Advisor: Automatic visualization answer for natural-language question on tabular data. In: 2021 IEEE 14th Pacific Visualization Symposium (PacificVis). pp. 11-20. IEEE (2021)
[54] Liu, D., Chen, Y., Wu, Z.: Digital twin (dt)-cyclegan: Enabling zero-shot sim-to-real transfer of visual grasping models. IEEE Robotics and Automation Letters (2023)
[55] Liu, Z., Yu, X., Zhang, L., Wu, Z., Cao, C., Dai, H., Zhao, L., Liu, W., Shen, D., Li, Q., et al.: Deid-gpt: Zero-shot medical text de-identification by gpt-4. arXiv preprint arXiv:2303.11032 (2023)
[56] Liu, Z., Zhong, A., Li, Y., Yang, L., Ju, C., Wu, Z., Ma, C., Shu, P., Chen, C., Kim, S., et al.: Radiology-gpt: A large language model for radiology. arXiv preprint arXiv:2306.08666 (2023)
[57] Lu, Y., Lin, H., Xu, J., Han, X., Tang, J., Li, A., Sun, L., Liao, M., Chen, S.: Text2event: Controllable sequence-to-structure generation for end-to-end event extraction. arXiv preprint arXiv:2106.09232 (2021)
[58] Luan, L., Lin, X., Li, W.: Exploring the cognitive dynamics of artificial intelligence in the post-covid-19 and learning 3.0 era: A case study of chatgpt. arXiv preprint arXiv:2302.04818 (2023)
[59] Luo, Y., Tang, J., Li, G.: nvbench: A large-scale synthesized dataset for cross-domain natural language to visualization task. arXiv preprint arXiv:2112.12926 (2021)
[60] Ma, C., Wu, Z., Wang, J., Xu, S., Wei, Y., Liu, Z., Guo, L., Cai, X., Zhang, S., Zhang, T., et al.: Impressiongpt: an iterative optimizing framework for radiology report summarization with chatgpt. arXiv preprint arXiv:2304.08448 (2023)
[61] Maddigan, P., Susnjak, T.: Chat2vis: Generating data visualisations via natural language using chatgpt, codex and gpt-3 large language models. arXiv preprint arXiv:2302.02094 (2023)
[62] McKee, F., Noever, D.: Chatbots in a botnet world. arXiv preprint arXiv:2212.11126 (2022)
[63] McKee, F., Noever, D.: Chatbots in a honeypot world. arXiv preprint arXiv:2301.03771 (2023)
[64] Megahed, F.M., Chen, Y.J., Ferris, J.A., Knoth, S., Jones-Farmer, L.A.: How generative ai models such as chatgpt can be (mis) used in spc practice, education, and research? an exploratory study. arXiv preprint arXiv:2302.10916 (2023)
[65] Michail, A., Konstantinou, S., Clematide, S.: Uzh_clyp at semeval-2023 task 9: Head-first fine-tuning and chatgpt data generation for crosslingual learning in tweet intimacy prediction. arXiv preprint arXiv:2303.01194 (2023)
[66] Mukaka, M.M.: A guide to appropriate use of correlation coefficient in medical research. Malawi medical journal 24(3), 69-71 (2012)
[67] Narechania, A., Srinivasan, A., Stasko, J.: Nl4dv: A toolkit for generating analytic specifications for data visualization from natural language queries. IEEE Transactions on Visualization and Computer Graphics 27(2), 369-379 (2020)
[68] Noever, D., Ciolino, M.: The turing deception. arXiv preprint arXiv:2212.06721 (2022)
[69] Noever, D., McKee, F.: Numeracy from literacy: Data science as an emergent skill from large language models. arXiv preprint arXiv:2301.13382 (2023)
[70] Nov, O., Singh, N., Mann, D.M.: Putting chatgpt's medical advice to the (turing) test. medRxiv (2023)
[71] OpenAI: Gpt-4 technical report (2023)
[72] Ortega-Martín, M., García-Sierra, Ó., Ardoiz, A., Álvarez, J., Armenteros, J.C., Alonso, A.: Linguistic ambiguity analysis in chatgpt. arXiv preprint arXiv:2302.06426 (2023)
[73] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022)
[74] Pardos, Z.A., Bhandari, S.: Learning gain differences between chatgpt and human tutor generated algebra hints. arXiv preprint arXiv:2302.06871 (2023)
[75] Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., Lee, H.: Cord: a consolidated receipt dataset for post-ocr parsing. In: Workshop on Document Intelligence at NeurIPS 2019 (2019)
[76] Polak, M.P., Morgan, D.: Extracting accurate materials data from research papers with conversational language models and prompt engineering-example of chatgpt. arXiv preprint arXiv:2303.05352 (2023)
[77] Prieto, S.A., Mengiste, E.T., de Soto, B.G.: Investigating the use of ChatGPT for the scheduling of construction projects. Buildings 13(4), 857 (mar 2023). https://doi.org/10.3390/buildings13040857, https://doi.org/10.3390\ buildings13040857
[78] Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., Yang, D.: Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476 (2023)
[79] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training. OpenAI (2018)
[80] Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., Sutskever, I.: Better language models and their implications. OpenAI Blog https://openai. com/blog/better-language-models 1(2) (2019)
[81] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)
[82] Shakarian, P., Koyyalamudi, A., Ngu, N., Mareedu, L.: An independent evaluation of chatgpt on mathematical word problems (mwp). arXiv preprint arXiv:2302.13814 (2023)
[83] Sobania, D., Briesch, M., Hanna, C., Petke, J.: An analysis of the automatic bug fixing performance of chatgpt. arXiv preprint arXiv:2301.08653 (2023)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://snnubiai. github.io/chatgpt_arxiv_analysis/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-897 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-897</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-897</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-027353b63d9bf423ff675d751ad7505c3fb53614</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/027353b63d9bf423ff675d751ad7505c3fb53614" target="_blank">Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet, and shows that agents that incorporate commonsense into the gameState graph outperform baseline agents.</p>
                <p><strong>Paper Abstract:</strong> Text-based games (TBGs) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning (RL). Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs. In this paper, we posit that to act efficiently in TBGs, an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge. Thus, we propose an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet. This combination is achieved through bidirectional knowledge graph attention between the two symbolic representations. We show that agents that incorporate commonsense into the game state graph outperform baseline agents.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e897.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e897.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiKE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Knowledgegraph attEntion (BiKE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL agent that jointly models a symbolic state graph (SG) and an external commonsense graph (CG, from ConceptNet), fusing them with bidirectional graph attention to produce commonsense-aware state representations used by an actor-critic policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BiKE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>BiKE encodes textual observations and admissible actions with GRUs, extracts a symbolic state graph (SG) from observations and a commonsense graph (CG) from ConceptNet, embeds entities using Numberbatch, encodes each graph with Graph Attention Networks (GATs), computes an inter-graph similarity matrix S and bidirectional attentions (A, \bar{A}) to recontextualize SG and CG (the BiKE mechanism), updates per-node state representations via a learned function g, obtains graph-level representations via attention with the current observation, and selects actions using a learned attention scorer h over (observation, action, SG representation, CG representation). Policies are trained with Actor-Critic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC) / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based games (TBGs) in the TextWorld / TextWorld Commonsense framework: partially-observable POMDPs where observations and actions are textual, the agent receives text observations dependent on a hidden world state, and the action space is combinatorially large; TWC specifically requires use of external commonsense to solve tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet commonsense knowledge graph (retrieved subgraphs), Numberbatch embeddings for KG entities, OpenIE + heuristics used to extract the state graph from text (as described in related prior work); Graph Attention Network (GAT) used to encode graphs (implementation modules rather than external data).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured knowledge graphs (nodes and relation edges), node embeddings (dense vectors / Numberbatch), attention weight matrices (inter-graph relevance scores), and graph-level vector representations.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A symbolic state graph (SG) representing the agent's belief of the world. Entities are extracted from observations (OpenIE + heuristics), embedded (Numberbatch), encoded with a GAT to allow intra-graph message passing, and then updated via bidirectional attention with the commonsense graph to yield updated node states and a graph-level belief representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>At each timestep the agent: (1) extracts SG entities from the latest textual observation; (2) embeds SG and CG entities (Numberbatch) and encodes each graph via GAT; (3) computes similarity matrix S between SG nodes (s_t^i) and CG nodes (c_t^j); (4) obtains state-to-commonsense attention A (softmax over S rows) and commonsense-to-state attention \bar{A} (softmax over S columns); (5) computes \tilde{s}_t^i = sum_j A^{ij} c_t^j and second-order interaction sum_j A^{ij} sum_{i'} \bar{A}^{ji'} s_t^{i'}; (6) updates each node s_{t+1}^i = g(s_t^i, \tilde{s}_t^i, \tilde{s}_t^i) via a learnable function g; (7) obtains graph-level representations g_{t+1}^S and g_{t+1}^C via attention between observation o_t and updated node sets; these representations are used by the policy to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Actor-Critic) that is graph-grounded: no explicit search-based planner is used; the bidirectional grounding with an external KG provides commonsense-informed, implicit look-ahead guidance to the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation (e.g., 'go' actions) is selected by the learned policy attending to the state graph nodes (which encode spatial/exits information); there is no explicit path-finding algorithm (no A*/shortest-path), navigation is guided by symbolic SG and learned action preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>BiKE (+State + Commonsense) on IN test set: Easy — #Steps 18.27 ± 1.13, Normalized score 0.94 ± 0.02; Medium — #Steps 39.34 ± 0.72, Normalized score 0.64 ± 0.02; Hard — #Steps 47.19 ± 0.64, Normalized score 0.34 ± 0.02. (Metrics: average number of steps per episode and normalized score; reported mean ± std over runs.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Text-only baseline (IN test): Easy — #Steps 23.83 ± 2.16, Normalized score 0.88 ± 0.04; Medium — #Steps 44.08 ± 0.93, Normalized score 0.60 ± 0.02; Hard — #Steps 49.84 ± 0.38, Normalized score 0.30 ± 0.02.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Jointly maintaining a symbolic state belief graph and grounding it with an external commonsense KG via bidirectional graph attention yields better sample efficiency and higher normalized scores than using either source alone: SGs primarily assist exploration and navigation (go/take actions), CGs primarily assist goal-directed actions (put/insert) by providing plausible object-location priors, and their combination (BiKE) yields the best overall performance across difficulty levels.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e897.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e897.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>+Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based RL agent with Commonsense Knowledge (TWC +Commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that augments a text-only RL policy with retrieved commonsense knowledge from ConceptNet to bias action selection toward commonsensical actions/locations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>+Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encodes observation history with GRUs and retrieves relevant subgraphs from ConceptNet (entities mapped to the game's vocabulary), embeds nodes (Numberbatch) and encodes the commonsense graph to produce representations that are used together with the observation encoding to score admissible actions. Trained with Actor-Critic.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>TWC: a TextWorld extension where solutions require leveraging commonsense knowledge (partially-observable textual observations, combinatorial action space).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>ConceptNet commonsense knowledge graph (retrieved subgraphs), Numberbatch embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured commonsense graph fragments (nodes/relations) and node embeddings / graph-level vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>No explicit symbolic belief graph is maintained by this agent; the agent retains an encoded history of observations (GRU) and retrieves commonsense graph representations conditioned on observations.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Actor-Critic) that uses commonsense retrieval to down-weight implausible actions and bias exploration; not an explicit search/planning algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation decisions are made by the learned policy conditioned on observation encodings and commonsense graph signals; no explicit path-finding algorithm is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>IN test set (TWC +Commonsense): Easy — #Steps 20.59 ± 5.01, Normalized score 0.89 ± 0.06; Medium — #Steps 42.61 ± 0.65, Normalized score 0.62 ± 0.03; Hard — #Steps 48.45 ± 1.13, Normalized score 0.32 ± 0.04.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Text-only baseline (IN): Easy — #Steps 23.83 ± 2.16, Normalized score 0.88 ± 0.04; Medium — #Steps 44.08 ± 0.93, Normalized score 0.60 ± 0.02; Hard — #Steps 49.84 ± 0.38, Normalized score 0.30 ± 0.02.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Access to a commonsense KG helps the agent select more plausible, goal-directed actions (notably in easy single-room object-placement tasks) and yields modest improvements in normalized score and/or steps compared to text-only baselines; however, commonsense alone is less helpful than state graphs in multi-room navigation/exploration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e897.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e897.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>+State (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Graph-constrained A2C) / +State</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that builds and maintains a symbolic state knowledge graph (belief graph) from textual observations and uses it to constrain and inform a policy (actor-critic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>+State (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a symbolic state graph by extracting relations from observations (OpenIE + heuristics), embeds nodes, and uses the structured belief graph to guide an Actor-Critic policy (graph-constrained RL). In this paper it is used as the +State baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld / TextWorld Commonsense (TWC)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based games where textual observations incompletely reveal the underlying state (POMDP); multiple rooms and objects create exploration/navigation challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>OpenIE (or similar relation extraction) and heuristics to extract a state graph from text; node embeddings (e.g., Numberbatch or learned embeddings) and graph-encoding modules.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured state knowledge graph (triples / relations like 'apple -> on -> table'), node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Symbolic state knowledge graph (belief graph) that captures the agent's current perception of objects, locations, and relations, updated from textual observations via OpenIE + heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Belief graph is updated by extracting relations from the latest observation using OpenIE and heuristics (adding/updating nodes and edges to reflect observed facts); the updated graph is encoded and used by the policy at the next step.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (Actor-Critic) with graph constraints — the state graph acts as structured memory used to prefer actions consistent with current belief (graph-constrained RL).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is guided by symbolic SG that encodes rooms and exits; the policy chooses movement actions based on graph-encoded location/exits rather than explicit path search algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>IN test set (+State / KG-A2C): Easy — #Steps 22.10 ± 2.91, Normalized score 0.86 ± 0.06; Medium — #Steps 41.61 ± 0.37, Normalized score 0.62 ± 0.03; Hard — #Steps 48.00 ± 0.61, Normalized score 0.32 ± 0.00.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Text-only baseline (IN): Easy — #Steps 23.83 ± 2.16, Normalized score 0.88 ± 0.04; Medium — #Steps 44.08 ± 0.93, Normalized score 0.60 ± 0.02; Hard — #Steps 49.84 ± 0.38, Normalized score 0.30 ± 0.02.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic state graphs improve exploration and navigation in environments with multiple rooms/objects; while commonsense helps action semantics, state graphs are particularly useful for movement and object collection tasks — combining both yields the best results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e897.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e897.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-DQN (Knowledge Graph Deep Q-Network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier approach that represents the agent's belief as a knowledge graph and uses that structured representation within a deep RL algorithm to act in text-adventure games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as prior work that represents game state as a symbolic belief graph and uses that representation within a DQN-like agent to select actions; specifics are from the cited work (Ammanabrolu & Riedl, 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially-observable text games (POMDP) where state must be inferred from observations; KG-DQN uses a belief graph to summarize state.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>State knowledge graph constructed from textual observations (OpenIE + heuristics in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured belief graph (triples), embeddings derived from graph.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Symbolic knowledge graph belief state (constructed from observation extractions).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Not detailed in this paper beyond the citation; in prior work the KG is incrementally updated from textual observations (see Ammanabrolu & Riedl, 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (DQN-family) using KG as state representation; not described here in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a foundational example of using symbolic belief graphs in text-based RL; motivated the use of state graphs in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e897.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e897.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Aided Transformer Agent (GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that constructs and updates a latent (learned) belief graph during planning to improve generalization on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as prior work that constructs a latent belief graph (learned representation) during planning; specific architectural choices and update rules are in the cited paper (Adhikari et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based games / TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially-observable text games where dynamic beliefs over world state aid planning; GATA produces and updates latent graphs for this purpose.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Latent/dynamically learned knowledge graphs (not an external KB like ConceptNet); details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Latent graph structures and embeddings (structured representations learned by the model).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Latent (learned) dynamic knowledge graph that serves as a belief/state representation during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Not specified in detail in this paper; prior work describes learning/updating a latent graph during planning (see Adhikari et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned planning using latent graph-aided transformer mechanisms (per the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work demonstrating utility of constructing/updating belief graphs (here latent) for planning in text games; motivates combining graph-based state representations with external knowledge in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on textbased games <em>(Rating: 2)</em></li>
                <li>Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines <em>(Rating: 2)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge <em>(Rating: 1)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-897",
    "paper_id": "paper-027353b63d9bf423ff675d751ad7505c3fb53614",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "BiKE",
            "name_full": "Bidirectional Knowledgegraph attEntion (BiKE)",
            "brief_description": "An RL agent that jointly models a symbolic state graph (SG) and an external commonsense graph (CG, from ConceptNet), fusing them with bidirectional graph attention to produce commonsense-aware state representations used by an actor-critic policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BiKE",
            "agent_description": "BiKE encodes textual observations and admissible actions with GRUs, extracts a symbolic state graph (SG) from observations and a commonsense graph (CG) from ConceptNet, embeds entities using Numberbatch, encodes each graph with Graph Attention Networks (GATs), computes an inter-graph similarity matrix S and bidirectional attentions (A, \\bar{A}) to recontextualize SG and CG (the BiKE mechanism), updates per-node state representations via a learned function g, obtains graph-level representations via attention with the current observation, and selects actions using a learned attention scorer h over (observation, action, SG representation, CG representation). Policies are trained with Actor-Critic.",
            "environment_name": "TextWorld Commonsense (TWC) / TextWorld",
            "environment_description": "Text-based games (TBGs) in the TextWorld / TextWorld Commonsense framework: partially-observable POMDPs where observations and actions are textual, the agent receives text observations dependent on a hidden world state, and the action space is combinatorially large; TWC specifically requires use of external commonsense to solve tasks.",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet commonsense knowledge graph (retrieved subgraphs), Numberbatch embeddings for KG entities, OpenIE + heuristics used to extract the state graph from text (as described in related prior work); Graph Attention Network (GAT) used to encode graphs (implementation modules rather than external data).",
            "tool_output_types": "Structured knowledge graphs (nodes and relation edges), node embeddings (dense vectors / Numberbatch), attention weight matrices (inter-graph relevance scores), and graph-level vector representations.",
            "belief_state_mechanism": "A symbolic state graph (SG) representing the agent's belief of the world. Entities are extracted from observations (OpenIE + heuristics), embedded (Numberbatch), encoded with a GAT to allow intra-graph message passing, and then updated via bidirectional attention with the commonsense graph to yield updated node states and a graph-level belief representation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "At each timestep the agent: (1) extracts SG entities from the latest textual observation; (2) embeds SG and CG entities (Numberbatch) and encodes each graph via GAT; (3) computes similarity matrix S between SG nodes (s_t^i) and CG nodes (c_t^j); (4) obtains state-to-commonsense attention A (softmax over S rows) and commonsense-to-state attention \\bar{A} (softmax over S columns); (5) computes \\tilde{s}_t^i = sum_j A^{ij} c_t^j and second-order interaction sum_j A^{ij} sum_{i'} \\bar{A}^{ji'} s_t^{i'}; (6) updates each node s_{t+1}^i = g(s_t^i, \\tilde{s}_t^i, \\tilde{s}_t^i) via a learnable function g; (7) obtains graph-level representations g_{t+1}^S and g_{t+1}^C via attention between observation o_t and updated node sets; these representations are used by the policy to select actions.",
            "planning_approach": "Learned policy (Actor-Critic) that is graph-grounded: no explicit search-based planner is used; the bidirectional grounding with an external KG provides commonsense-informed, implicit look-ahead guidance to the policy.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation (e.g., 'go' actions) is selected by the learned policy attending to the state graph nodes (which encode spatial/exits information); there is no explicit path-finding algorithm (no A*/shortest-path), navigation is guided by symbolic SG and learned action preferences.",
            "performance_with_tools": "BiKE (+State + Commonsense) on IN test set: Easy — #Steps 18.27 ± 1.13, Normalized score 0.94 ± 0.02; Medium — #Steps 39.34 ± 0.72, Normalized score 0.64 ± 0.02; Hard — #Steps 47.19 ± 0.64, Normalized score 0.34 ± 0.02. (Metrics: average number of steps per episode and normalized score; reported mean ± std over runs.)",
            "performance_without_tools": "Text-only baseline (IN test): Easy — #Steps 23.83 ± 2.16, Normalized score 0.88 ± 0.04; Medium — #Steps 44.08 ± 0.93, Normalized score 0.60 ± 0.02; Hard — #Steps 49.84 ± 0.38, Normalized score 0.30 ± 0.02.",
            "has_tool_ablation": true,
            "key_findings": "Jointly maintaining a symbolic state belief graph and grounding it with an external commonsense KG via bidirectional graph attention yields better sample efficiency and higher normalized scores than using either source alone: SGs primarily assist exploration and navigation (go/take actions), CGs primarily assist goal-directed actions (put/insert) by providing plausible object-location priors, and their combination (BiKE) yields the best overall performance across difficulty levels.",
            "uuid": "e897.0"
        },
        {
            "name_short": "+Commonsense (TWC)",
            "name_full": "Text-based RL agent with Commonsense Knowledge (TWC +Commonsense)",
            "brief_description": "An agent that augments a text-only RL policy with retrieved commonsense knowledge from ConceptNet to bias action selection toward commonsensical actions/locations.",
            "citation_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
            "mention_or_use": "use",
            "agent_name": "+Commonsense (TWC)",
            "agent_description": "Encodes observation history with GRUs and retrieves relevant subgraphs from ConceptNet (entities mapped to the game's vocabulary), embeds nodes (Numberbatch) and encodes the commonsense graph to produce representations that are used together with the observation encoding to score admissible actions. Trained with Actor-Critic.",
            "environment_name": "TextWorld Commonsense (TWC)",
            "environment_description": "TWC: a TextWorld extension where solutions require leveraging commonsense knowledge (partially-observable textual observations, combinatorial action space).",
            "is_partially_observable": true,
            "external_tools_used": "ConceptNet commonsense knowledge graph (retrieved subgraphs), Numberbatch embeddings.",
            "tool_output_types": "Structured commonsense graph fragments (nodes/relations) and node embeddings / graph-level vectors.",
            "belief_state_mechanism": "No explicit symbolic belief graph is maintained by this agent; the agent retains an encoded history of observations (GRU) and retrieves commonsense graph representations conditioned on observations.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": null,
            "planning_approach": "Learned policy (Actor-Critic) that uses commonsense retrieval to down-weight implausible actions and bias exploration; not an explicit search/planning algorithm.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation decisions are made by the learned policy conditioned on observation encodings and commonsense graph signals; no explicit path-finding algorithm is used.",
            "performance_with_tools": "IN test set (TWC +Commonsense): Easy — #Steps 20.59 ± 5.01, Normalized score 0.89 ± 0.06; Medium — #Steps 42.61 ± 0.65, Normalized score 0.62 ± 0.03; Hard — #Steps 48.45 ± 1.13, Normalized score 0.32 ± 0.04.",
            "performance_without_tools": "Text-only baseline (IN): Easy — #Steps 23.83 ± 2.16, Normalized score 0.88 ± 0.04; Medium — #Steps 44.08 ± 0.93, Normalized score 0.60 ± 0.02; Hard — #Steps 49.84 ± 0.38, Normalized score 0.30 ± 0.02.",
            "has_tool_ablation": true,
            "key_findings": "Access to a commonsense KG helps the agent select more plausible, goal-directed actions (notably in easy single-room object-placement tasks) and yields modest improvements in normalized score and/or steps compared to text-only baselines; however, commonsense alone is less helpful than state graphs in multi-room navigation/exploration.",
            "uuid": "e897.1"
        },
        {
            "name_short": "+State (KG-A2C)",
            "name_full": "KG-A2C (Graph-constrained A2C) / +State",
            "brief_description": "An agent that builds and maintains a symbolic state knowledge graph (belief graph) from textual observations and uses it to constrain and inform a policy (actor-critic).",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "agent_name": "+State (KG-A2C)",
            "agent_description": "Constructs a symbolic state graph by extracting relations from observations (OpenIE + heuristics), embeds nodes, and uses the structured belief graph to guide an Actor-Critic policy (graph-constrained RL). In this paper it is used as the +State baseline.",
            "environment_name": "TextWorld / TextWorld Commonsense (TWC)",
            "environment_description": "Text-based games where textual observations incompletely reveal the underlying state (POMDP); multiple rooms and objects create exploration/navigation challenges.",
            "is_partially_observable": true,
            "external_tools_used": "OpenIE (or similar relation extraction) and heuristics to extract a state graph from text; node embeddings (e.g., Numberbatch or learned embeddings) and graph-encoding modules.",
            "tool_output_types": "Structured state knowledge graph (triples / relations like 'apple -&gt; on -&gt; table'), node embeddings.",
            "belief_state_mechanism": "Symbolic state knowledge graph (belief graph) that captures the agent's current perception of objects, locations, and relations, updated from textual observations via OpenIE + heuristics.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Belief graph is updated by extracting relations from the latest observation using OpenIE and heuristics (adding/updating nodes and edges to reflect observed facts); the updated graph is encoded and used by the policy at the next step.",
            "planning_approach": "Learned policy (Actor-Critic) with graph constraints — the state graph acts as structured memory used to prefer actions consistent with current belief (graph-constrained RL).",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation is guided by symbolic SG that encodes rooms and exits; the policy chooses movement actions based on graph-encoded location/exits rather than explicit path search algorithms.",
            "performance_with_tools": "IN test set (+State / KG-A2C): Easy — #Steps 22.10 ± 2.91, Normalized score 0.86 ± 0.06; Medium — #Steps 41.61 ± 0.37, Normalized score 0.62 ± 0.03; Hard — #Steps 48.00 ± 0.61, Normalized score 0.32 ± 0.00.",
            "performance_without_tools": "Text-only baseline (IN): Easy — #Steps 23.83 ± 2.16, Normalized score 0.88 ± 0.04; Medium — #Steps 44.08 ± 0.93, Normalized score 0.60 ± 0.02; Hard — #Steps 49.84 ± 0.38, Normalized score 0.30 ± 0.02.",
            "has_tool_ablation": true,
            "key_findings": "Symbolic state graphs improve exploration and navigation in environments with multiple rooms/objects; while commonsense helps action semantics, state graphs are particularly useful for movement and object collection tasks — combining both yields the best results.",
            "uuid": "e897.2"
        },
        {
            "name_short": "KG-DQN",
            "name_full": "KG-DQN (Knowledge Graph Deep Q-Network)",
            "brief_description": "An earlier approach that represents the agent's belief as a knowledge graph and uses that structured representation within a deep RL algorithm to act in text-adventure games.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "Referenced as prior work that represents game state as a symbolic belief graph and uses that representation within a DQN-like agent to select actions; specifics are from the cited work (Ammanabrolu & Riedl, 2019).",
            "environment_name": "Text-based games / TextWorld",
            "environment_description": "Partially-observable text games (POMDP) where state must be inferred from observations; KG-DQN uses a belief graph to summarize state.",
            "is_partially_observable": true,
            "external_tools_used": "State knowledge graph constructed from textual observations (OpenIE + heuristics in the cited work).",
            "tool_output_types": "Structured belief graph (triples), embeddings derived from graph.",
            "belief_state_mechanism": "Symbolic knowledge graph belief state (constructed from observation extractions).",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Not detailed in this paper beyond the citation; in prior work the KG is incrementally updated from textual observations (see Ammanabrolu & Riedl, 2019).",
            "planning_approach": "Learned policy (DQN-family) using KG as state representation; not described here in detail.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned as a foundational example of using symbolic belief graphs in text-based RL; motivated the use of state graphs in this paper.",
            "uuid": "e897.3"
        },
        {
            "name_short": "GATA",
            "name_full": "Graph-Aided Transformer Agent (GATA)",
            "brief_description": "An agent that constructs and updates a latent (learned) belief graph during planning to improve generalization on text-based games.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "mention_or_use": "mention",
            "agent_name": "GATA",
            "agent_description": "Referenced as prior work that constructs a latent belief graph (learned representation) during planning; specific architectural choices and update rules are in the cited paper (Adhikari et al., 2020).",
            "environment_name": "Text-based games / TextWorld",
            "environment_description": "Partially-observable text games where dynamic beliefs over world state aid planning; GATA produces and updates latent graphs for this purpose.",
            "is_partially_observable": true,
            "external_tools_used": "Latent/dynamically learned knowledge graphs (not an external KB like ConceptNet); details are in the cited work.",
            "tool_output_types": "Latent graph structures and embeddings (structured representations learned by the model).",
            "belief_state_mechanism": "Latent (learned) dynamic knowledge graph that serves as a belief/state representation during planning.",
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": "Not specified in detail in this paper; prior work describes learning/updating a latent graph during planning (see Adhikari et al., 2020).",
            "planning_approach": "Learned planning using latent graph-aided transformer mechanisms (per the cited work).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as related work demonstrating utility of constructing/updating belief graphs (here latent) for planning in text games; motivates combining graph-based state representations with external knowledge in this paper.",
            "uuid": "e897.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "rating": 2
        },
        {
            "paper_title": "Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines",
            "rating": 2
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge",
            "rating": 1
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2
        }
    ],
    "cost": 0.018276999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations</h1>
<p>Keerthiram Murugesan ${ }^{1}$, Mattia Atzeni ${ }^{1,4}$, Pavan Kapanipathi ${ }^{1}$, Kartik Talamadupula ${ }^{1}$, Mrinmaya Sachan ${ }^{3}$, and Murray Campbell ${ }^{1}$<br>${ }^{1}$ IBM Research ${ }^{4}$ EPFL ${ }^{3}$ ETH Zürich<br>keerthiram.murugesan@ibm.com, atz@zurich.ibm.com, kapanipa@us.ibm.com, krtalamad@us.ibm.com, mrinmaya.sachan@inf.ethz.ch, mcam@us.ibm.com</p>
<h4>Abstract</h4>
<p>Text-based games (TBGs) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning (RL). Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs. In this paper, we posit that to act efficiently in TBGs, an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge. Thus, we propose an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet. This combination is achieved through bidirectional knowledge graph attention between the two symbolic representations. We show that agents that incorporate commonsense into the game state graph outperform baseline agents.</p>
<h2>1 Introduction</h2>
<p>Text-based games (TBGs) are simulation environments in which an agent interacts with the world purely in the modality of text. TBGs have emerged as key benchmarks for studying how reinforcement learning agents can tackle the challenges of language understanding, partial observability, and action generation in combinatorially large action spaces. One particular text-based gaming environment, TextWorld (Côté et al., 2018), has received significant attention in recent years.</p>
<p>Recent work has shown the need for additional knowledge to tackle the challenges in TBGs. Ammanabrolu and Riedl (2019) proposed handcrafted rules to represent the current state of the game using a state knowledge graph (much like a map of the game). Our own prior work (Murugesan et al., 2021) proposed an extension of TextWorld, called TextWorld Commonsense (TWC), to test agents' ability to use commonsense knowledge while interacting with the world. The hypothesis behind TWC
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of a TBG that requires both the state representation of the game as well as the external commonsense knowledge for efficient exploration and learning the best action trajectory. The observation text feeds into the state and commonsense graphs; and the best action trajectory is computed based on information from both graphs.
is that commonsense knowledge allows the agent to understand how current actions might affect future world states; and enable look-ahead planning (Juba, 2016), thus leading to sample-efficient selection of actions at each step and driving the agent closer to optimal performance.</p>
<p>In this paper, we posit that to efficiently act in such text-based gaming environments, an agent must be able to effectively track the state of the game, and use that to jointly retrieve and leverage the relevant commonsense knowledge. For example, commonsense knowledge such as apple should be placed in the refrigerator would help the agent to act closer to the optimal behavior; whereas state information like apple is on the table would help the agent plan more efficiently. Thus, we propose a technique to: (a) track the state of the game in the form of a symbolic graph that represents the agent's current belief of the state of the world (Ammanabrolu and Hausknecht, 2020a; Adhikari et al., 2020); (b) retrieve the relevant commonsense knowledge from ConceptNet (Speer et al., 2017), and (c) jointly leverage the state graph and the</p>
<p>retrieved commonsense graph. This combined information is then used to select the optimal action. Finally, we demonstrate the performance of our agent against state of the art baseline agents on the TWC Environment.</p>
<h2>2 Related Work</h2>
<p>Text-based reinforcement learning Text-based games have recently emerged as a promising framework to drive advances in RL research. Prior work has explored text-based RL to learn strategies based on an external text corpus (Branavan et al., 2012) or from textual observations (Narasimhan et al., 2015). In both cases, the text is analyzed and control strategies are learned jointly using feedback from the gaming environment. Zahavy et al. (2018) proposed the Action-Elimination Deep Q-Network (AE-DQN), which learns to classify invalid actions to reduce the action space. The use of the commonsense and state graph in our work has the same goal of down-weighting implausible actions by jointly reasoning over the state of the game and prior knowledge. Recently, Côté et al. (2018) introduced TextWorld and Murugesan et al. (2021) proposed TextWorld Commonsense (TWC), a textbased gaming environment which requires agents to leverage prior knowledge in order to solve the games. In this work, we build on the agents of Murugesan et al. (2021) and show that prior knowledge and state information are complementary and should be learned jointly.</p>
<p>KG-based state representations A recent line of work in TBGs aims at enhancing generalization performance by using symbolic representations of the agent's belief. Notably, Ammanabrolu and Riedl (2019) proposed $K G-D Q N$ and Ammanabrolu and Hausknecht (2020b) proposed $K G$ $A 2 C$. The idea behind both approaches is to represent the game state as a belief graph. Recently, Adhikari et al. (2020) proposed the graph-aided transformer agent (GATA), an approach to construct and update a latent belief graph during planning. Our work integrates these graph-based state representations with a prior commonsense graph that allows the agent to better model the state of the game using prior knowledge.</p>
<p>Sample-efficient reinforcement learning A key challenge for current RL research is low sample efficiency (Kaelbling et al., 1998). To address this problem, there have been few attempts on adding prior or external knowledge to RL
approaches. Notably, Murugesan et al. (2020) proposed to use prior knowledge extracted from ConceptNet. Garnelo et al. (2016) proposed Deep Symbolic RL, which relies on techniques from symbolic AI as a way to introduce commonsense priors. There has also been work on policy transfer (Bianchi et al., 2015) which aims at reusing knowledge gained in different environments. Moreover, Experience replay (Wang et al., 2016; Lin, 1992, 1993) provides a framework for how previous experiences can be stored and later reused. In this paper, following Murugesan et al. (2020), we use external KGs as a source of prior knowledge and we combine this knowledge representation with graph-based state modeling in order to allow the agents to act more efficiently.</p>
<h2>3 Model \&amp; Architecture</h2>
<p>TBGs can be framed as partially observable Markov decision processes (POMDPs) (Spaan, 2012) denoted $\langle S, A, O, T, E, r\rangle$, where: $S$ denotes the set of states, $A$ denotes the action space, $O$ denotes the observation space, $T$ denotes the state transition probabilities, $E$ denotes the conditional observation emission probabilities, and $r: S \times A \rightarrow$ $\mathbb{R}$ is the reward function. The observation $o_{t}$ at time step $t$ depends on the current state. Both observations and actions are rendered in text. The agent receives a reward at every time step $t: r_{t}=r\left(o_{t}, a_{t}\right)$, and the agent's goal is to maximize the expected discounted sum of rewards: $\mathbb{E}\left[\sum \gamma^{t} r_{t}\right]$, where $\gamma \in$ $[0,1]$ is a discount factor.</p>
<p>The high-level architecture of our model contains three major components: (a) the input encoder; (b) a graph-based knowledge extractor; and (c) the action prediction module. The input encoding layers are used to encode the observation $o_{t}$ at time step $t$ and the list of admissible actions using GRUs (Ammanabrolu and Hausknecht, 2020a). The graph-based knowledge extractor collects relevant knowledge from complementary knowledge sources: the game state, and external commonsense knowledge. We allow information from each knowledge source to guide and direct better representation learning for the other.</p>
<p>Recent efforts have demonstrated the use of primarily two different types of knowledge sources for TextWorld RL Agents. A State Graph (SG) captures state information (Ammanabrolu and Riedl, 2019) about the environment represented via a language-based semantic graph. The example in Figure 2 shows that information such as Apple $\rightarrow$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualization of our overall approach with BiKE
on $\rightarrow$ Table is extracted from the textual observations from the environment. Specifically, Ammanabrolu and Riedl (2019) create such knowledge graphs by extracting information using OpenIE (Angeli et al., 2015) and some manual heuristics. A Commonsense Graph (CG) captures external commonsense knowledge (Murugesan et al., 2021) between entities (from commonsense knowledge sources such as ConceptNet). We posit that RL agents can make use of information from both these graphs during different sub-tasks, enabling efficient learning. The SG provides the agent with a symbolic way of representing its current perception of the game state, including its understanding of the surroundings. On the other hand, the CG provides the agent with complementary human-like knowledge about what actions make sense in a given state, thus enabling more efficient exploration of the very large natural language based action space.</p>
<p>We combine the state information with commonsense knowledge using a Bidirectional Knowledgegraph attEntion (BiKE) mechanism, which recontextualizes the state and commonsense graphs based on each other for optimal action trajectories. Figure 2 provides a compact visualization.</p>
<h2>4 Knowledge Integration using BiKE</h2>
<p>The aforementioned graph-based knowledge extractor produces $M$ entities $\left(c_{t}^{1}, c_{t}^{2}, \cdots, c_{t}^{M}\right)$ for the commonsense graph (CG); and $N$ entities $\left(s_{t}^{1}, s_{t}^{2}, \cdots, s_{t}^{N}\right)$ for the state graph (SG). Note that the entities extracted for the CG are based on the
vocabulary used in ConceptNet, and may not necessarily have the same set of entities as the SG (Figure 1). We embed the extracted entities in both graphs using Numberbatch (Liu and Singh, 2004). We then encode these graph representations using a Graph Attention Network (GAT) (Veličković et al., 2018). GAT allows the node entities $s_{t}$ and $c_{t}$ within the graphs $G_{t}^{S}$ and $G_{t}^{C}$ respectively to share information among each other by message passing.</p>
<p>We then integrate sub-graphs extracted from the previous steps to improve the agent's exploration strategy. Inspired from bidirectional attention mechanism in QA (Seo et al., 2016), we use BiKE attention mechanism between $G_{t}^{S}$ and $G_{t}^{C}$ to fuse the knowledge from these two graphs. The information flow across the graphs allows the model to learn commonsense-aware state graph representations, and state-aware commonsense knowledge graph representations.</p>
<p>To implement this, we compute a graph similarity matrix $\mathrm{S} \in \mathbb{R}^{N \times M}$ across the graph entities to learn a state-to-commonsense graph attention function and a commonsense-to-state graph attention function. $\mathrm{S}<em t="t">{i j}=f\left(s</em>$ to a similarity score. This allows us to measure the similarity between (for instance) Apple observed in the state graph and Apple observed in the commonsense graph. We compute the state-to-commonsense graph attention values $A$ by taking a softmax along the rows}^{i}, c_{t}^{j}\right)$ captures how each node $s_{t}^{i}$ in the graph $G_{t}^{S}$ is linked to a node $c_{t}^{j}$ in the other graph $G_{t}^{C}$, and vice versa. Here $f$ is a learnable function that maps $s_{t}^{i}$ and $c_{t}^{j</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance evaluation (showing mean and standard deviation averaged over 3 runs) for the three difficulty levels: Easy (left), Medium (middle), Hard (right) using normalized score and the number of steps taken.
of S: this signifies the attention bestowed by each state graph node on the nodes of the commonsense graph. Similarly, we compute the commonsense-tostate graph attention values $\bar{A}$ by taking a softmax along the columns of $S$. We capture the relevant knowledge in the commonsense graph $G_{k^{\prime}}^{i}$ by updating the state representations $\tilde{s}<em t_1="t+1">{t}^{i}$. We compute the updated state representation as: $s</em>}^{i}=g\left(s_{t}^{i}, \tilde{s<em t="t">{t}^{i}, \tilde{s}</em>}^{i}\right)$; where $\tilde{s<em j="j">{t}^{i}=\sum</em>} A^{i j} c_{t}^{j}, \tilde{s<em j="j">{t}^{i}=\sum</em>} A^{i j} \sum_{i^{\prime}} \bar{A}^{j i^{\prime}} s_{t}^{i^{\prime}}$, and $g$ is a learnable function that maps the concatenated $s_{t}^{i}$, $\tilde{s<em t="t">{t}^{i}$, and $\tilde{s}</em>}^{i}$ to an updated state representation. Finally, we use the general attention between the $o_{t}$ and the state graph entities $s_{t+1}$ to get the state graph representation $\mathbf{g<em t_1="t+1">{t+1}^{S}$ (Luong et al., 2015). We perform a similar process for the commonsense-tostate graph attention and obtain the commonsense graph representation: $\mathbf{g}</em>}^{C}$. We select the relevant action by computing an attention over the actions: $h\left(o_{t}, a_{t}^{i}, \mathbf{g<em t_1="t+1">{t+1}^{S}, \mathbf{g}</em>}^{C}\right)$; where $h$ is a learnable function that projects the concatenation $\left\langle o_{t}, a_{t}^{i}, \mathbf{g<em t_1="t+1">{t+1}^{S}, \mathbf{g}</em>$ action.}^{C}\right\rangle$ to the attention score for the $i^{\text {th }</p>
<h2>5 Experiments</h2>
<p>We generate a set of games with 3 difficulty levels using the TWC (Murugesan et al., 2021) framework: (i) easy level, which has 1 room containing 1 to 3 objects; (ii) medium level, which has 1 or 2 rooms with 4 or 5 objects; and (iii) hard level, a mix of games with a high number of objects ( 6 or 7 objects in 1 or 2 rooms) or high number of rooms ( 3 or 4 rooms containing 4 or 5 objects).</p>
<p>We compare 5 text-based RL agents: (a) a textonly agent (Text), which selects the best action
based only on the encoding of the history of observations; (b) DRRN (He et al., 2016; Narasimhan et al., 2015), which relies on the relevance between the observation and action spaces; (c) an agent enhanced with access to an external commonsense knowledge graph (+Commonsense) (Murugesan et al., 2021); (d) an agent that, following Ammanabrolu and Hausknecht (2020a), models the state of the world as a symbolic graph (+State); and (e) the agent (BiKE) described in Section 3, which relies on both state and commonsense graph representations. The agents are trained over 100 episodes with a 50 -step maximum. All policies are learned using Actor-Critic (Mnih et al., 2016).</p>
<h3>5.1 Improving Performance with State and Commonsense Knowledge</h3>
<p>Figure 3 shows the learning curves for the text-only agent and the agents equipped with state and/or commonsense graph representations at training time. For reference, we also report the performance of an agent that selects a random action at each time step (Random). We notice that, overall, agents equipped with either state or commonsense graph representations perform better than their text-only counterparts, both in terms of the number of steps taken and the normalized score. In particular, the BiKE agent outperforms all other agents in all difficulty levels, showing that symbolic state representations and prior commonsense knowledge can be jointly used for better sample efficiency and results. Table 1 shows the performance of the agents on the test set. Following Murugesan et al. (2021), we</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Average relevance of the main action templates to the state and commonsense graphs across the hard games.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Timestep</th>
<th style="text-align: center;">$r$</th>
<th style="text-align: center;">$r+1$</th>
<th style="text-align: center;">$r+2$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Room</td>
<td style="text-align: center;">Living Room</td>
<td style="text-align: center;">Living Room</td>
<td style="text-align: center;">Bedroom</td>
</tr>
<tr>
<td style="text-align: center;">Action Taken</td>
<td style="text-align: center;">take checkered jumper</td>
<td style="text-align: center;">go west</td>
<td style="text-align: center;">insert checkered jumper into wardrobe</td>
</tr>
<tr>
<td style="text-align: center;">Most relevant graph</td>
<td style="text-align: center;">State graph</td>
<td style="text-align: center;">State graph</td>
<td style="text-align: center;">Commonsense graph</td>
</tr>
<tr>
<td style="text-align: center;">Most relevant nodes</td>
<td style="text-align: center;">checkered jumper</td>
<td style="text-align: center;">checkered jumper, <br> exit to west</td>
<td style="text-align: center;">wardrobe</td>
</tr>
</tbody>
</table>
<p>(b) Example of most relevant graphs and nodes (by action taken) for one example game excerpted from the hard difficulty level.</p>
<p>Figure 4: Relevance given to the: (a) state and commonsense graphs; and to (b) their nodes (by action taken).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Medium</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
<td style="text-align: center;">#Steps</td>
<td style="text-align: center;">Norm. Score</td>
</tr>
<tr>
<td style="text-align: center;">$\stackrel{\rightharpoonup}{\omega}$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$23.83 \pm 2.16$</td>
<td style="text-align: center;">$0.88 \pm 0.04$</td>
<td style="text-align: center;">$44.08 \pm 0.93$</td>
<td style="text-align: center;">$0.60 \pm 0.02$</td>
<td style="text-align: center;">$49.84 \pm 0.38$</td>
<td style="text-align: center;">$0.30 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">$22.08 \pm 4.17$</td>
<td style="text-align: center;">$0.82 \pm 0.06$</td>
<td style="text-align: center;">$44.04 \pm 1.64$</td>
<td style="text-align: center;">$0.59 \pm 0.02$</td>
<td style="text-align: center;">$49.82 \pm 0.61$</td>
<td style="text-align: center;">$0.29 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+Commonsense (TWC)</td>
<td style="text-align: center;">$20.59 \pm 5.01$</td>
<td style="text-align: center;">$0.89 \pm 0.06$</td>
<td style="text-align: center;">$42.61 \pm 0.65$</td>
<td style="text-align: center;">$0.62 \pm 0.03$</td>
<td style="text-align: center;">$48.45 \pm 1.13$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+State (KG-A2C)</td>
<td style="text-align: center;">$22.10 \pm 2.91$</td>
<td style="text-align: center;">$0.86 \pm 0.06$</td>
<td style="text-align: center;">$41.61 \pm 0.37$</td>
<td style="text-align: center;">$0.62 \pm 0.03$</td>
<td style="text-align: center;">$48.00 \pm 0.61$</td>
<td style="text-align: center;">$0.32 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+State + Commonsense (BiKE)</td>
<td style="text-align: center;">$18.27 \pm 1.13$</td>
<td style="text-align: center;">$0.94 \pm 0.02$</td>
<td style="text-align: center;">$39.34 \pm 0.72$</td>
<td style="text-align: center;">$0.64 \pm 0.02$</td>
<td style="text-align: center;">$47.19 \pm 0.64$</td>
<td style="text-align: center;">$0.34 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">$\stackrel{\rightharpoonup}{\omega}$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">$29.90 \pm 2.92$</td>
<td style="text-align: center;">$0.78 \pm 0.02$</td>
<td style="text-align: center;">$45.90 \pm 0.22$</td>
<td style="text-align: center;">$0.55 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.20 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DRRN</td>
<td style="text-align: center;">$29.71 \pm 1.81$</td>
<td style="text-align: center;">$0.76 \pm 0.05$</td>
<td style="text-align: center;">$45.18 \pm 1.19$</td>
<td style="text-align: center;">$0.56 \pm 0.02$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.21 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+Commonsense (TWC)</td>
<td style="text-align: center;">$27.74 \pm 4.46$</td>
<td style="text-align: center;">$0.78 \pm 0.07$</td>
<td style="text-align: center;">$44.89 \pm 1.52$</td>
<td style="text-align: center;">$0.58 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.19 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+State (KG-A2C)</td>
<td style="text-align: center;">$28.34 \pm 3.63$</td>
<td style="text-align: center;">$0.80 \pm 0.07$</td>
<td style="text-align: center;">$43.05 \pm 2.52$</td>
<td style="text-align: center;">$0.59 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.21 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+State + Commonsense (BiKE)</td>
<td style="text-align: center;">$25.59 \pm 1.92$</td>
<td style="text-align: center;">$0.83 \pm 0.01$</td>
<td style="text-align: center;">$41.01 \pm 1.61$</td>
<td style="text-align: center;">$0.61 \pm 0.01$</td>
<td style="text-align: center;">$50.00 \pm 0.00$</td>
<td style="text-align: center;">$0.23 \pm 0.02$</td>
</tr>
</tbody>
</table>
<p>Table 1: Test-set performance results for within distribution (IN) and out-of-distribution (OUT) games.
compared our agents on two test sets: (IN) uses the same entities as the training set, and (OUT) uses entities that were not included in the training set. The experimental results show that the BiKE agent generalizes better than all the baselines across the 3 difficulty levels.</p>
<h3>5.2 Qualitative Analysis</h3>
<p>From Figure 3 and Table 1, we notice that the +Commonsense agent performs better on the easy level, whereas the +State agent performs better on the medium and hard levels. This suggests that the state representation can be leveraged to drive exploration and interaction with objects in environments with multiple rooms; whereas prior commonsense knowledge allows the agent to act more efficiently by selecting the appropriate commonsensical locations of different objects. In order to investigate this hypothesis, we computed the average importance given by the agent to the state graph and the commonsense graph when selecting the different action templates shown in Figure 4a. For each action template, the figure shows the normalized attention weight given to the two graphs, averaged across 5 runs of all games in the hard difficulty level. Actions requiring information about the goal of the game, like put and insert, benefit more from attending to the commonsense graph; whereas actions aimed at exploring the environment and
collecting objects, like go and take, benefit more from the state representation.</p>
<p>As further qualitative analysis, we report an example of the most attended nodes and graphs from an excerpt of a game belonging to the hard difficulty level in Figure 4b. As noted above, the take and go actions rely more on the state graph, whereas the insert action relies on the commonsense graph. Among the nodes in these graphs, the entities that are finally mentioned in the action receive the highest attention score. This shows how our agent is able to transfer the bidirectional attention over graphs into specific game instances.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we showed that in order to be sampleefficient in TBGs, agents must be able to jointly track the state of the game and relevant commonsense knowledge. We proposed a technique that models both forms of knowledge as graphs, and combines them using Bidirectional Knowledgegraph attEntion (BiKE). The resulting agent was found to be more sample-efficient than approaches that considered neither or only one of these graphs.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank the anonymous reviewers for their valuable comments and suggestions. MS acknowledges support from the Hasler Foundation.</p>
<h2>Broader Impact and Discussion of Ethics</h2>
<p>While our model is not tuned for any specific realworld application, our method could be used in sensitive contexts such as legal or health-care settings; and it is essential that any work that builds on our approach undertake extensive quality-assurance and robustness testing before using it in their setting. The dataset used in our work does not contain sensitive information to the best of our knowledge.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. 2020. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020a. Graph constrained reinforcement learning for natural language action spaces. arXiv preprint arXiv:2001.08837.</p>
<p>Prithviraj Ammanabrolu and Matthew J. Hausknecht. 2020b. Graph constrained reinforcement learning for natural language action spaces. In 8th International Conference on Learning Representations, ICLR 2020.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages $344-354$.</p>
<p>Reinaldo AC Bianchi, Luiz A Celiberto Jr, Paulo E Santos, Jackson P Matsuura, and Ramon Lopez de Mantaras. 2015. Transferring knowledge as heuristics in reinforcement learning: A case-based approach. Artificial Intelligence, 226:102-121.</p>
<p>SRK Branavan, David Silver, and Regina Barzilay. 2012. Learning to win by reading manuals in a monte-carlo framework. Journal of Artificial Intelligence Research, 43:661-704.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018.</p>
<p>Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.</p>
<p>Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. 2016. Towards deep symbolic reinforcement learning. arXiv preprint arXiv:1609.05518.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630.</p>
<p>Brendan Juba. 2016. Integrated common sense learning and planning in pomdps. The Journal of Machine Learning Research, 17(1):3276-3312.</p>
<p>Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134.</p>
<p>Long-Ji Lin. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293-321.</p>
<p>Long-Ji Lin. 1993. Reinforcement learning for robots using neural networks. Technical report, CarnegieMellon Univ Pittsburgh PA School of Computer Science.</p>
<p>Hugo Liu and Push Singh. 2004. Conceptnet-a practical commonsense reasoning tool-kit. BT technology journal, 22(4):211-226.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412-1421, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. 2021. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In The 35th AAAI Conference on Artificial Intelligence.</p>
<p>Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and Kartik Talamadupula. 2020. Enhancing text-based reinforcement learning agents with commonsense knowledge. CoRR, abs/2005.00811.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603.</p>
<p>Matthijs TJ Spaan. 2012. Partially observable markov decision processes. In Reinforcement Learning, pages 387-414. Springer.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In AAAI, pages 4444-4451.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In International Conference on Learning Representations.</p>
<p>Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. 2016. Sample efficient actorcritic with experience replay. arXiv preprint arXiv:1611.01224.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-254591432</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.03275v2.pdf" target="_blank">Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers</a></p>
                <p><strong>Paper Abstract:</strong> Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4808.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4808.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Small-scale Transformer (3-layer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3-layer Transformer Encoder (small-scale transformer trained in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-agnostic transformer encoder trained to verify and produce multi-step reasoning traces for 6x6 Sudoku verification tasks (Hidden Single, Full House, Naked Single), demonstrating substantial out-of-distribution generalization when trained with component subtasks and careful handling of positional information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3-layer transformer encoder (custom)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Custom encoder-only transformer with 3 encoder layers, 8 attention heads, 1024-dim feed-forward layers; 256-dim token and grid embeddings; grid cell encoding sums 256-dim digit embedding and 256-dim coordinate embedding (row+column concatenated into 256-dim); sinusoidal positional encodings used by default; trained with teacher-forcing, cross-entropy loss, Adam optimizer (lr=1e-4), batch size 192. Variants used relative-position or position-robust schemes (ALiBi, SRL) and a padded-prompt variant for transfer tests.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6x6 Sudoku (Hidden Single / Full House / Naked Single verification tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A reduced-size Sudoku (6x6) where each row, column, and 2x3 box must contain each digit exactly once; tasks focus on verifying whether a given candidate digit must occupy a specified goal cell according to local/spatial constraints (Hidden Single: elimination across a house; Full House: only one empty cell in a house; Naked Single: only one remaining candidate in a cell given neighboring digits). Spatial reasoning required: identifying and iterating over cells in a row/column/box, locating digit occurrences, and propagating local exclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>The model is trained to output an explicit multi-step reasoning trace (autoregressive generation of per-cell checks followed by final yes/no). Key strategies: (1) decompose HS into FH and NS subtasks and train on those component tasks (simultaneous or curriculum regimes), (2) require sequential multi-step generation (single-step final answer fails), (3) manage positional alignment (padding or position-robust encodings) to enable transfer, (4) use attention to query grid cells while producing substep decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative and quantitative evidence: attention-map visualizations (Figure 2) show the model attends to specific grid cells when evaluating whether a digit can be placed, and the model explicitly iterates across cells in a house in its generated intermediate steps; successful out-of-distribution generalization to held-out rows and held-out columns (Rows and Columns splits) indicates learned abstract relational procedures operating over spatial positions rather than memorized cases. Ablations/analyses: removing component-task training (HS-only) collapses OOD performance to chance; inspecting attention for digit-rotation experiments revealed mis-attention patterns causing failures in digit-generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Within-distribution (WD) performance: near-perfect on held-out WD puzzles when trained to produce full sequences (paper reports ~99.9% for full-sequence-trained models in one comparison). Out-of-distribution (OOD) performance: HS-only training yields ~50% (chance) on OOD splits; with FH+NS training (Simultaneous or Curriculum) high OOD accuracy on Rows and Columns splits (high, near-ceiling; exact per-condition values in Figure 1). Digit-swap OOD initially ~50% (failure) but improved with prompt padding to: final-answer accuracy 94.4% and full-sequence accuracy 91.7% (Padded dataset). Using position-robust encodings on the unpadded dataset: ALiBi achieved ~95.5% final-answer accuracy and SRL achieved ~92.3% final-answer accuracy on the Digits condition (paper reports these numbers as increases in final-answer accuracy). Models that attempt one-step final-answer prediction (no intermediate trace) reached only ~87.8% on held-out puzzles vs ~99.9% for full-sequence-trained models in the cited comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Key failures: (1) Single forward-pass final-answer prediction is insufficient; model needs to produce multi-step traces. (2) Strong sensitivity to absolute token positions (sinusoidal positional encoding): models mis-attend to digit tokens when prompt alignment differs across tasks, producing systematic failures on digit-rotation OOD tests. (3) Without exposure to component subtasks (FH/NS), HS-only training fails to generalize across rows/columns/digits. (4) Errors in Digits condition mainly occur at empty cells (deciding whether an empty cell can contain candidate digit); attention visualizations show mis-attention patterns. (5) Results are on a small 6x6 Sudoku with carefully designed prompts—scaling and naturalistic data/generalization to larger puzzles not demonstrated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to: (a) HS-only trained transformers (worse: ~50% OOD), (b) padded prompt and positional-robust encodings (better: Padded/ALiBi/SRL substantially improve digit-generalization), (c) prior graph/relational network work (cited as prior successful Sudoku solvers that used domain-specific inductive biases — those used graph networks with built-in symmetry constraints), (d) humans (humans show no digit-generalization decrement when digits swapped; transformer originally did), and (e) relational neural network [13] which also fails to transfer to held-out digits (paper cites this).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4808.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4808.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Network (relational)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Network / Relational Neural Network (prior work cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work used graph network architectures with relational inductive biases and task-specific symmetry constraints to successfully solve Sudoku puzzles, embedding explicit spatial/relational structure into the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Relational inductive biases, deep learning, and graph networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph network / relational neural network (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-based relational architectures that explicitly encode entities (cells) and relations (row/column/box constraints) and incorporate inductive biases enforcing task symmetries; the specific architectures and sizes are from the cited literature rather than this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (general / prior formulations)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Standard Sudoku puzzles (spatial relational constraint satisfaction where each row, column, and box must contain unique digits); spatial reasoning required to enforce relational constraints across cells and houses.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Explicit relational modelling via graph networks that embed cells as nodes and houses/relations as edges, leveraging domain-specific inductive biases to enforce Sudoku symmetries and constraints (no detailed training/procedure reported in this paper; cited as prior successful approach).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Cited success of graph-network architectures in solving Sudoku (paper references this as the only prior successful neural approach), implying these models use explicit relational/spatial representations; this paper does not re-evaluate those models but contrasts domain-agnostic transformer learning with models that use such inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (performance numbers for the cited graph-network work are not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper argues such domain-specific inductive biases, while effective, offer little insight into how domain-agnostic models can learn systematic relational strategies; limitations of those models are not empirically evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as a baseline prior approach that succeeded at Sudoku by embedding domain-specific structure; contrasted with the transformer approach which is domain-agnostic and required exposure to component tasks and positional-handling to generalize.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4808.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4808.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based language model often cited for few-shot in-context learning abilities; mentioned here as context for large models' apparent systematicity and open questions about training data coverage and true OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-scale autoregressive transformer (hundreds of billions of parameters in the referenced model family) trained on massive web-scale corpora; used in literature as an example of a foundation model showing few-shot/task generalization, but not directly used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>General mathematical/reasoning tasks (mentioned broadly); not directly applied to Sudoku in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Referenced in discussion about whether large LLMs can generalize to novel problems and whether few-shot success reflects true systematic reasoning or exposure in training data; no Sudoku-specific application is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>In-context / few-shot prompting as commonly used with GPT-3; discussed as potentially priming generation of sequences corresponding to tasks it has seen in training rather than pure algorithmic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>None presented in this paper; GPT-3 is discussed conceptually regarding generalization but no experiments or evidence for spatial puzzle solving are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this paper for spatial puzzles; GPT-3 referenced qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes uncertainty whether GPT-3's few-shot successes reflect genuine generalization or memorization/coverage of similar tasks in its training data; cautions about interpreting few-shot performance as evidence of algorithmic systematicity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used in discussion to contrast small-scale controlled experiments with large foundation models; authors suggest coverage of component problems in training data could partly explain large-model robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Relational inductive biases, deep learning, and graph networks <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Systematic generalization and emergent structures in transformers trained on structured tasks <em>(Rating: 2)</em></li>
                <li>Train short, test long: Attention with linear biases enables input length extrapolation <em>(Rating: 1)</em></li>
                <li>Transformer language models without positional encodings still learn positional information <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4808",
    "paper_id": "paper-254591432",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "Small-scale Transformer (3-layer)",
            "name_full": "3-layer Transformer Encoder (small-scale transformer trained in this paper)",
            "brief_description": "A domain-agnostic transformer encoder trained to verify and produce multi-step reasoning traces for 6x6 Sudoku verification tasks (Hidden Single, Full House, Naked Single), demonstrating substantial out-of-distribution generalization when trained with component subtasks and careful handling of positional information.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "3-layer transformer encoder (custom)",
            "model_description": "Custom encoder-only transformer with 3 encoder layers, 8 attention heads, 1024-dim feed-forward layers; 256-dim token and grid embeddings; grid cell encoding sums 256-dim digit embedding and 256-dim coordinate embedding (row+column concatenated into 256-dim); sinusoidal positional encodings used by default; trained with teacher-forcing, cross-entropy loss, Adam optimizer (lr=1e-4), batch size 192. Variants used relative-position or position-robust schemes (ALiBi, SRL) and a padded-prompt variant for transfer tests.",
            "puzzle_name": "6x6 Sudoku (Hidden Single / Full House / Naked Single verification tasks)",
            "puzzle_description": "A reduced-size Sudoku (6x6) where each row, column, and 2x3 box must contain each digit exactly once; tasks focus on verifying whether a given candidate digit must occupy a specified goal cell according to local/spatial constraints (Hidden Single: elimination across a house; Full House: only one empty cell in a house; Naked Single: only one remaining candidate in a cell given neighboring digits). Spatial reasoning required: identifying and iterating over cells in a row/column/box, locating digit occurrences, and propagating local exclusions.",
            "mechanism_or_strategy": "The model is trained to output an explicit multi-step reasoning trace (autoregressive generation of per-cell checks followed by final yes/no). Key strategies: (1) decompose HS into FH and NS subtasks and train on those component tasks (simultaneous or curriculum regimes), (2) require sequential multi-step generation (single-step final answer fails), (3) manage positional alignment (padding or position-robust encodings) to enable transfer, (4) use attention to query grid cells while producing substep decisions.",
            "evidence_of_spatial_reasoning": "Qualitative and quantitative evidence: attention-map visualizations (Figure 2) show the model attends to specific grid cells when evaluating whether a digit can be placed, and the model explicitly iterates across cells in a house in its generated intermediate steps; successful out-of-distribution generalization to held-out rows and held-out columns (Rows and Columns splits) indicates learned abstract relational procedures operating over spatial positions rather than memorized cases. Ablations/analyses: removing component-task training (HS-only) collapses OOD performance to chance; inspecting attention for digit-rotation experiments revealed mis-attention patterns causing failures in digit-generalization.",
            "performance_metrics": "Within-distribution (WD) performance: near-perfect on held-out WD puzzles when trained to produce full sequences (paper reports ~99.9% for full-sequence-trained models in one comparison). Out-of-distribution (OOD) performance: HS-only training yields ~50% (chance) on OOD splits; with FH+NS training (Simultaneous or Curriculum) high OOD accuracy on Rows and Columns splits (high, near-ceiling; exact per-condition values in Figure 1). Digit-swap OOD initially ~50% (failure) but improved with prompt padding to: final-answer accuracy 94.4% and full-sequence accuracy 91.7% (Padded dataset). Using position-robust encodings on the unpadded dataset: ALiBi achieved ~95.5% final-answer accuracy and SRL achieved ~92.3% final-answer accuracy on the Digits condition (paper reports these numbers as increases in final-answer accuracy). Models that attempt one-step final-answer prediction (no intermediate trace) reached only ~87.8% on held-out puzzles vs ~99.9% for full-sequence-trained models in the cited comparison.",
            "limitations_or_failure_cases": "Key failures: (1) Single forward-pass final-answer prediction is insufficient; model needs to produce multi-step traces. (2) Strong sensitivity to absolute token positions (sinusoidal positional encoding): models mis-attend to digit tokens when prompt alignment differs across tasks, producing systematic failures on digit-rotation OOD tests. (3) Without exposure to component subtasks (FH/NS), HS-only training fails to generalize across rows/columns/digits. (4) Errors in Digits condition mainly occur at empty cells (deciding whether an empty cell can contain candidate digit); attention visualizations show mis-attention patterns. (5) Results are on a small 6x6 Sudoku with carefully designed prompts—scaling and naturalistic data/generalization to larger puzzles not demonstrated here.",
            "comparison_baseline": "Compared to: (a) HS-only trained transformers (worse: ~50% OOD), (b) padded prompt and positional-robust encodings (better: Padded/ALiBi/SRL substantially improve digit-generalization), (c) prior graph/relational network work (cited as prior successful Sudoku solvers that used domain-specific inductive biases — those used graph networks with built-in symmetry constraints), (d) humans (humans show no digit-generalization decrement when digits swapped; transformer originally did), and (e) relational neural network [13] which also fails to transfer to held-out digits (paper cites this).",
            "uuid": "e4808.0"
        },
        {
            "name_short": "Graph Network (relational)",
            "name_full": "Graph Network / Relational Neural Network (prior work cited)",
            "brief_description": "Prior work used graph network architectures with relational inductive biases and task-specific symmetry constraints to successfully solve Sudoku puzzles, embedding explicit spatial/relational structure into the model.",
            "citation_title": "Relational inductive biases, deep learning, and graph networks",
            "mention_or_use": "mention",
            "model_name": "Graph network / relational neural network (prior work)",
            "model_description": "Graph-based relational architectures that explicitly encode entities (cells) and relations (row/column/box constraints) and incorporate inductive biases enforcing task symmetries; the specific architectures and sizes are from the cited literature rather than this paper's experiments.",
            "puzzle_name": "Sudoku (general / prior formulations)",
            "puzzle_description": "Standard Sudoku puzzles (spatial relational constraint satisfaction where each row, column, and box must contain unique digits); spatial reasoning required to enforce relational constraints across cells and houses.",
            "mechanism_or_strategy": "Explicit relational modelling via graph networks that embed cells as nodes and houses/relations as edges, leveraging domain-specific inductive biases to enforce Sudoku symmetries and constraints (no detailed training/procedure reported in this paper; cited as prior successful approach).",
            "evidence_of_spatial_reasoning": "Cited success of graph-network architectures in solving Sudoku (paper references this as the only prior successful neural approach), implying these models use explicit relational/spatial representations; this paper does not re-evaluate those models but contrasts domain-agnostic transformer learning with models that use such inductive biases.",
            "performance_metrics": "Not reported in this paper (performance numbers for the cited graph-network work are not reproduced here).",
            "limitations_or_failure_cases": "The paper argues such domain-specific inductive biases, while effective, offer little insight into how domain-agnostic models can learn systematic relational strategies; limitations of those models are not empirically evaluated in this paper.",
            "comparison_baseline": "Mentioned as a baseline prior approach that succeeded at Sudoku by embedding domain-specific structure; contrasted with the transformer approach which is domain-agnostic and required exposure to component tasks and positional-handling to generalize.",
            "uuid": "e4808.1"
        },
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3 (GPT-3)",
            "brief_description": "A large transformer-based language model often cited for few-shot in-context learning abilities; mentioned here as context for large models' apparent systematicity and open questions about training data coverage and true OOD generalization.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Large-scale autoregressive transformer (hundreds of billions of parameters in the referenced model family) trained on massive web-scale corpora; used in literature as an example of a foundation model showing few-shot/task generalization, but not directly used in this paper's experiments.",
            "puzzle_name": "General mathematical/reasoning tasks (mentioned broadly); not directly applied to Sudoku in this paper",
            "puzzle_description": "Referenced in discussion about whether large LLMs can generalize to novel problems and whether few-shot success reflects true systematic reasoning or exposure in training data; no Sudoku-specific application is reported here.",
            "mechanism_or_strategy": "In-context / few-shot prompting as commonly used with GPT-3; discussed as potentially priming generation of sequences corresponding to tasks it has seen in training rather than pure algorithmic generalization.",
            "evidence_of_spatial_reasoning": "None presented in this paper; GPT-3 is discussed conceptually regarding generalization but no experiments or evidence for spatial puzzle solving are reported here.",
            "performance_metrics": "Not provided in this paper for spatial puzzles; GPT-3 referenced qualitatively.",
            "limitations_or_failure_cases": "Paper notes uncertainty whether GPT-3's few-shot successes reflect genuine generalization or memorization/coverage of similar tasks in its training data; cautions about interpreting few-shot performance as evidence of algorithmic systematicity.",
            "comparison_baseline": "Used in discussion to contrast small-scale controlled experiments with large foundation models; authors suggest coverage of component problems in training data could partly explain large-model robustness.",
            "uuid": "e4808.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Relational inductive biases, deep learning, and graph networks",
            "rating": 2,
            "sanitized_title": "relational_inductive_biases_deep_learning_and_graph_networks"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Systematic generalization and emergent structures in transformers trained on structured tasks",
            "rating": 2,
            "sanitized_title": "systematic_generalization_and_emergent_structures_in_transformers_trained_on_structured_tasks"
        },
        {
            "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "rating": 1,
            "sanitized_title": "train_short_test_long_attention_with_linear_biases_enables_input_length_extrapolation"
        },
        {
            "paper_title": "Transformer language models without positional encodings still learn positional information",
            "rating": 1,
            "sanitized_title": "transformer_language_models_without_positional_encodings_still_learn_positional_information"
        }
    ],
    "cost": 0.0103935,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Achieving and Understanding Out-of-Distribution Generaliza- tion in Systematic Reasoning in Small-Scale Transformers
13 Dec 2022</p>
<p>Andrew J Nam 
Stanford University</p>
<p>Mustafa Abdool 
Stanford University</p>
<p>Trevor Maxfield 
Stanford University</p>
<p>James L Mcclelland 
Stanford University</p>
<p>Achieving and Understanding Out-of-Distribution Generaliza- tion in Systematic Reasoning in Small-Scale Transformers
13 Dec 2022
Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.</p>
<p>Large transformer-based 'foundation' models [2] have attracted recent attention by showing some success in mathematical reasoning tasks, demonstrating a degree of systematicity and compositionality [4,18,9]. However, it is unclear how their ability to behave systematically emerges, due to the massive sizes of the training data and model parameters. Are they demonstrating the ability to generalize out-of-distribution to novel problems? Or are they succeeding because the data they are trained on samples from the entire space of possible training examples?</p>
<p>To investigate how a domain-agnostic model may learn to generalize to out-of-distribution examples, we train a small scale transformer-based network to learn solution strategies based on the popular puzzle game Sudoku. We use a 6x6 Sudoku grid rather than the traditional 9x9, which provides sufficient complexity for investigating algorithmic reasoning while offering more tractability and lower compute requirements. The general rule of Sudoku still applies: every n-celled row, column, and outlined region of the grid must contain exactly one instance of each of the n alternative digits.</p>
<p>Sudoku is well-suited for this inquiry for several reasons. First, it is governed by a small set of rules that are inherently abstract, relational, and form sophisticated interactions and dependencies that require careful algorithmic and deductive reasoning. These rules form group properties and symmetries [5,15] that translate one puzzle to another such that learning to solve a subset of examples of a class of puzzles would enable the solver to solve all puzzles with the same relational properties, provided that the abstract rules and relations are induced and the new examples can be mapped onto them. The symmetries in Sudoku enable an elegant means to probe for systematicity by designing training and test sets that share core relational features yet differ superficially in a well-defined manner. Second, Sudoku has been shown to be challenging to neural networks, and has only been successfully solved using a graph network architecture [1] with built-in domain-specific inductive biases enforcing the relevant, task-specific, symmetries [13]. While this is a useful strategy for building models that solve Sudoku, it offers little guidance towards understanding how a domain-agnostic neural network can learn in a way that enables systematic generalization. Finally, Sudoku has been used to study reasoning, learning, and generalization in humans [8,12], offering an interesting benchmark for what forms of behavior one ought to expect from a solver with human-level general intelligence.</p>
<p>We focus on one solution strategy in Sudoku called the Hidden Single technique and introduce a transformer neural network architecture and training set to explore out-of-distribution generalization (OODG). Building on this network, we present the following findings: First, a single forward pass in the network is insufficient to learn the Hidden Single strategy; sequential, multi-step reasoning is necessary. Second, we decompose the Hidden Single strategy into two subtasks, and show that including training examples on these subtasks sampled from the full space of instances of such problems allows the model to exhibit substantial OODG of the Hidden Single strategy.</p>
<p>Task Description</p>
<p>We base our tasks on the three simplest Sudoku techniques. Full House (FH) involves identifying a cell in which all other cells in one of its houses (row, column, or 2x3 box) are filled such that the empty cell's digit must be the only remaining digit. Naked Single (NS) involves identifying a cell in which 5 of the 6 possible digits already exist in its neighborhood (row, column, and 2x3 box) such that the empty cell's digit must be the remaining digit. Hidden Single (HS) involves identifying a cell C in which all other cells c i in one of its houses cannot contain one of the digits, either due to c i already containing a different digit or the digit being present in c i 's neighborhood, so that the only remaining cell that can contain the digit in the house is C.</p>
<p>For each technique, we create a task in which the model is presented with a 6x6 Sudoku grid and a string sequence prompt that provides the context for solving the problem, including the coordinates of the cell to solve for, the candidate digit, the name of the technique to use, and, for the Hidden Single (HS) and Full House (FH) tasks, the house type to inspect. By specifying all these details as part of the prompt, we simplify the task from conducting a search for a valid solution to verifying whether a goal cell should contain a candidate digit according to the rules of the specified technique.</p>
<p>The target sequence formats for each of the three tasks were designed to support composition of elements of the Full House and Naked Singles (NS) tasks (see Table 1). The HS task format steps through all of the cells other than the target cell in the specified target house, and checks to see if the specified digit can go in any of these cells. If it cannot, the answer is yes, it must go in the target cell.The FH task performs a similar iteration strategy, but identifies whether the cell contains a digit at all at each step. The NS task addresses the component of the HS task not present in the FH task, which is to identify whether a given cell can contain a candidate digit based on direct contradictions within its (row, column, or box) neighborhood. In the HS task, the network can draw on the FH strategy when it encounters full cells in the target house (column 2 in the example) and on the naked single strategy when it encounters empty cells.  </p>
<p>Experiments</p>
<p>We use a 3-layer transformer encoder [16] to which all grid and text embeddings are passed, and from which output vectors are then mapped to output text tokens (See Training Details in the Supplementary Materials). Grid cells inputs specify the x and y coordinates of each cell and indicate if the cell is empty or if not which digit it contains. Text tokens are pared with sinusoidal position encodings as in [16]. We use teacher-forcing during training and greedy autoregressive generation during evaluation.</p>
<p>We first check if the network could solve the tasks by producing the final yes/no responses immediately after the prompt. After training 5 model instances on 50,000 HS puzzles uniformly sampled from the full problem space, we find that these models only solve 87.8% of held-out puzzles, compared to the 99.9% of models trained to produce the full sequence of reasoning steps. Taking complete success at within-distribution generalization as a pre-requisite, we use the full sequences as exemplified in Table 1 in all remaining experiments, which focus on out of distribution generalization in our models.</p>
<p>Out-of-distribution generalization. We define within-distribution (WD) puzzles as those that conform to the restrictions used to construct the training set and out-of-distribution (OOD) puzzles as those that do not conform to these restrictions. All models included in our analyses solved held-out WD puzzles with near perfect accuracy, so we only report OODG performance in our results for conciseness.</p>
<p>Based on the natural symmetries of Sudoku grids and the train/test split used in [12], we devised three distributional splitting schemes to probe for OODG. Our first condition, Rows, recognizes that the application of the HS, FH, or NS techniques is isometric relative to the row in question. Thus, a successful model of abstract relational reasoning should solve HS puzzles applied to any of the 6 rows, even when trained on a strict subset of the 6 rows. We train the models using HS puzzles applied to 4 of the 6 rows (i.e. the goal cell will only appear on these 4 rows), then test its OOD performance on the remaining 2 rows. In our second condition, Columns, we exclude all HS puzzles that are performed over columns from the training set, and test the models on these column puzzles. The abstract principle of process of elimination remains the same, and the main challenge is knowing which cells to iterate over. Our final condition, Digits, uses the fact that swapping digits only superficially changes the puzzle without affecting the underlying structure (see Figure 2. We train the model on puzzles with 4 of the 6 digits as the candidate digit identified in the problem prompt and test the model on puzzles with the remaining 2.</p>
<p>We also had 3 conditions for how we trained the models. In the first training condition, HS Only, we included 110,000 HS puzzles as part of the training set and not the FH or NS puzzles, and trained the model for 70,000 gradient updates (each based on 192 examples). The second condition, Simultaneous, included 50,000 HS puzzles and 30,000 FH and NS puzzles each, and the model was trained on all 3 tasks simultaneously for 70,000 updates. The FH and NS puzzles were sampled completely at random without any systematic constraints. The last condition, Curriculum, uses the same materials as Simultaneous, but trains the model on the FH and NS puzzles for the first 20,000 updates, reaching ceiling performance, before training on all 3 tasks for 50,000 more updates. Figure 1 summarizes the out-of-distribution generalization results in each condition. First, we compute the accuracy based on the final yes/no answer at the end of the output sequence, and we find that models trained only on HS puzzles struggle to exceed chance (50%), suggesting that the model has no inherent inductive bias towards generalizing in these dimensions. When trained with the FH and NS tasks, the model succeeds in generalizing to the held-out rows and columns to a high degree, especially when trained using the curriculum-based setup. We consider the strong generalization in the Columns condition (though somewhat less complete than in the Rows condition) to be an important finding, since the Columns condition contained no training on column-wise puzzles, while the Rows condition included 4 of the 6 rows. The models did not generally perform well on held out digits, although one of the seeds in the simultaneous condition generalized well. The relational neural network [13] also fails to transfer to held-out digits, but humans who learn to solve HS puzzles with a restricted set of targets show no decrement in performance when tested on digits held-out from a training tutorial [12]. We also measure the accuracy for entire sequences, in which a problem is considered solved if the entire model output sequence matches the target sequence, including the intermediate steps. This not only magnifies the differences, but also indicates that the models that generalize successfully do so in its entire reasoning process, not just at the correct final output.</p>
<p>Although the Rows and Columns conditions successfully demonstrate out-of-distribution generalization, the Digits condition generally does not, as shown by the roughly 50% accuracy in Figure 1. This is surprising, since human participants show no change in performance when the digits in the grid are swapped [12]. Its peculiarity is magnified by the fact unlike the Rows and Columns conditions, full sequence accuracy is well above the floor even when the models are trained with only HS puzzles.</p>
<p>Digit generalization mis-attention. Inspecting the model generated outputs, we find that the models correctly identify the relevant cells to iterate over, and the last line indicating the final answer is consistent with its intermediary outputs. In other words, the errors are made when determining whether or not the cells can contain the candidate digit. Moreover, these errors only occur at empty cells, not at cells already containing a digit.</p>
<p>To gain further insight, we analyze the attention maps generated by the transformer that indicate which information the model considers. We probe the model by taking held-out puzzles and rotating the digits in each grid such that 1 becomes 2, 2 becomes 3, 6 becomes 1, etc., thus producing 6 identical sets of puzzles that only differ by the digits involved, and each puzzle in the set has a unique candidate digit.</p>
<p>We inspect how the model queries the grid as it produces the yes/no responses at the end of each substep in the HS problem by taking the maximum attention weight given for each cell from the last transformer layer. Figure 2 shows an example of one model's attentional map on the HS problem shown in Table 1 when considering whether the candidate digit can be placed at the cell on (2, 2). In the within-distribution problem, where 3 is the candidate digit, the model attends to and correctly identifies the 3 in the same row. In contrast, the model does not attend to the same position in the out-of-distribution puzzle where 4 is the candidate digit. This form of mis-attention is characteristic of the errors in the Digits condition. Improving digit generalization. The model apparently fails to transfer its success in the NS task to correctly perform the NS task when it is embedded within the HS task. We trace the source of this difficulty to a superficial difference between the prompt sequences used in the NS and HS tasks. Unlike the FH task which is aligned token-for-token with the HS task, the NS task requires much fewer steps and has a shorter prompt compared to the HS and FH tasks. Furthermore, the position of the digit in the NS prompt is not aligned with its position in the FH and HS prompts. We test the importance of this by padding the NS prompt with null tokens so that the position of the "digit" token is aligned with the position in the HS and FH prompts. We also added 4 extra "row r column c yes/no" lines in the target sequence with random coordinates to match the format of the other two tasks. Training the models on all 3 tasks simultaneously on this Padded dataset allows them to reach 94.4% final answer accuracy and 91.7% full sequence accuracy in the Digits condition. We also tried adding the 4 extra row-column query lines but not the null-token padding does not help the model generalize at all, and found that this leaves the OODG accuracy near 50%. This suggests that the knowledge transfer from the NS to the HS tasks is impeded by the model's over-reliance on the token position as encoded with the sinusoidal encoding scheme. It appears that the model learns to attend to the digit aligned with a specific position token in the FH task, and transfers this reliance to the HS task; while in the NS task (unless padding is introduced to align the tokens) the model relies on a different position token, and has not learned to read the held-out digits from this position. However effective padding is in our Sudoku tasks, it is an impractical general solution for improving alignment between tasks for transfer learning. To address this issue, we test alternative approaches to handling positional information that have been found to reduce sensitivity to absolute token positions. Because causal masking allows transformers to learn absolute positions of tokens even in the absence of any positional information in the inputs [6], we selected methods that not only remove absolute position information, and that may bias the model against relying on these position tokens anyway. Specifically, we use sorted random labels (SRL) [10], which replace exact position indices with an ordered sequence of position labels sampled from a larger set, and attention with linear biases (ALiBi) [14], which eliminates position vectors altogether in favor of adding biases to attention scores that scale with relative distance between tokens. We apply these methods to the unpadded dataset to test whether these enable out-of-distribution generalization to held-out digits and find that final answer accuracy increases to 95.5% and 92.3% for ALiBi and SRL respectively.</p>
<p>Discussion</p>
<p>We find, using carefully designed datasets utilizing isomorphic symmetries in Sudoku, that our transformerbased model generalized well to out-of-distribution Hidden Singles puzzles when trained in a concurrent training regime including exposure to the full distribution of the Full House and Naked Single subtasks. These results may help shed light on the generalization abilities of larger transformer models, where it is hard to know what is covered in their training set, but where it seems reasonable to believe that exposure to component tasks and complex compositions of component tasks are interspersed throughout the data. While these models may receive restricted experience with complex multi-step problems, their ability to solve new ones may depend in part on more complete coverage of component sub-problems in their training data. Future research should explore this hypothesis in larger and more naturalistic data sets.</p>
<p>The initial version of our model used positional input tokens tied to absolute input position, and we found that this led to undue reliance on the exact input position of the candidate digit the model was asked to solve for, impeding generalization. By using alternative approaches to position encoding, we were able to overcome this problem. The work contributes to the growing body of findings indicating that failures of generalization in algorithmic tasks can be due at least in part to features of the positional encoding scheme used. This issue is less likely to arise in a large transformer predicting long strings of tokens using variants of similar problems that would vary naturally in string length and therefor prevent reliance on exact input positions. Nevertheless, these findings may be relevant to understanding the performance of large transformer-based models, especially in large language models such as GPT-3 [3] where suppressing sensitivity to token positions seems to improve perplexity scores [6]. Future research that corrects deficiencies related to positional encoding could have important implications for the success of larger models, perhaps helping them to achieve stronger performance in abstract reasoning than they have achieved thus far.</p>
<p>In any case, we are still a long way from understanding when and whether transformers are truly capable of few-shot learning and a high level of out-of-distribution transfer of the kind we have observed in some human participants in the hidden single task [12]. While GPT-3 was initially presented as a successful few shot-learner [3], many instances of the tasks the model supposedly learned few shot might have been embedded in its training data, and prompting with few shot examples might better be seen as a way of priming the model to be more likely to generate sequences corresponding to a task it has been exposed to in its training data many times. Even when such models are fine tuned with instructions and few-shot examples there is considerable uncertainty about exactly why such fine tuning is helpful [11,17], and much more work is needed to clarify these issues.</p>
<p>Supplementary Materials</p>
<p>Training Details</p>
<p>The core of our model is a 3-layer transformer encoder [16] supported by embedding layers for grid digits, grid cell positions, and input text, and finally an output text decoder layer. The input for each cell consists of three one hot vectors: six-unit vectors for the row and column, and a seven-unit vector for the digit in the cell, with the seventh used when the cell is empty. The row and column coordinates are encoded by a 128-dim embedding layer and the resulting vectors are concatenated as a single 256-dim vector. The digits are encoded with a 256-dim embedding layer and the digit and coordinate vectors are summed to form a single 256-dim grid cell embedding. All text tokens are encoded by a 256-dim embedding layer and position information is added to the vectors using the sinusoidal positional encoding scheme introduced in [16]. The 36 grid cell vectors and all token vectors are passed to the transformer, which is composed of 3 encoder layers with 8 heads and 1024-dim feed-forward layers, and the output vectors are decoded using a linear layer to form the final logit values for the predicted output tokens.</p>
<p>During training, we use teacher-forcing to predict the next token at each sequence position and cross-entropy with the target sequence. We mask the loss so that in the loss is only applied after the 'digit' token in the hidden single and full house tasks, and after the column number token in the naked single tasks. The loss is computed using cross-entropy and the model is optimized using Adam [7] with a learning rate of 0.0001. When training with multiple tasks at once, we sum the losses in one batch from each of the tasks before computing the gradient for backpropagation. We keep the same batch size of 192 samples for each task, regardless of how many tasks are trained at once.</p>
<p>Attention Map</p>
<p>To obtain the attention map as shown in Figure 2, we evaluated the model on the Hidden Single problem shown in Table 1. We generated 5 additional problems based on the problem in Table 1 by shifting the digit 1 to 5 times such that what was originally a 1 would be a 2 in the second problem, then a 3, and so on, yielding 6 puzzles that are identical in every way except the individual digits involved. For example, the top figure in Figure 2 shows the problem as is shown in Table 1, whereas the bottom figure shows the same puzzle with all digits incremented by 1, wrapping around 6 back to 1. In the bottom puzzle, the prompt would state "digit 4" to account for the increment.</p>
<p>After evaluating the model, we take the attentional weights from the final transformer layer where the input token was the second "2" from the line stating "row 2 column 2", since the output of this position would be a "yes" or a "no". To visualize the attention across all 8 heads, we take the maximum attention weight so that if a single head attended highly to the position, it would appear in the figure. Figure 2 only shows attention to the 36 cells in the grid for visualization purposes, though the model could and does attend to other tokens in the prompt and output sequence.</p>
<p>Figure 1 :
1Out-of-distribution accuracy results. Small dots represent individual models (10 per condition). Large dots represent average accuracy in each condition. Left: accuracy based on final yes/no at the end of output sequence. Right: accuracy based on the entire output sequence.</p>
<p>Figure 2 :
2Attentional maps. Withinand out-of-distribution puzzle with candidate digits 3 and 4 on top and bottom, respectively</p>
<p>Figure 3 :
3Out-of-distribution accuracy results with various positional encoding methods. Absolute, ALiBi, and SRL methods all train on the unpadded dataset.</p>
<p>Table 1 :
1Sample problems. Prompt text in black standard text. Target / model-generated text in bold. Note that rows count top to bottom and columns count left to right.Hidden Single 
Full House 
Naked Single </p>
<p><SOS> hidden_single 
goal_cell row 6 column 2 
house_type column 
digit 3 
can_contain 
row 1 column 2 no 
row 2 column 2 no 
row 3 column 2 no 
row 4 column 2 no 
row 5 column 2 no 
solution yes <EOS> </p>
<p><SOS> full_house 
goal_cell row 2 column 2 
house_type box 
digit 4 
is_filled 
row 1 column 1 no 
row 1 column 2 yes 
row 1 column 3 no 
row 2 column 1 yes 
row 2 column 3 yes 
solution no <EOS> </p>
<p><SOS> 
digit 6 
can_contain 
row 4 column 3 no 
<EOS> </p>
<p>Table 2 :
2Aligned Naked Single problems. Hidden Single problem included for reference. Prompt text in standard text. Target / model-generated text in bold.Hidden Single 
Naked Single Unpadded 
Naked Single Padded </p>
<p><SOS> hidden_single 
goal_cell row 6 column 2 
house_type column 
digit 3 
can_contain 
row 1 column 2 no 
row 2 column 2 no 
row 3 column 2 no 
row 4 column 2 no 
row 5 column 2 no 
solution yes <EOS> </p>
<p><SOS> 
digit 6 
can_contain 
row 4 column 3 no 
row 2 column 2 no 
row 5 column 3 yes 
row 4 column 5 no 
row 4 column 2 yes 
<EOS> </p>
<p><SOS> <PAD> 
<PAD> <PAD> <PAD> <PAD> <PAD> 
<PAD> <PAD> 
digit 3 
can_contain 
row 4 column 3 no 
row 2 column 2 no 
row 5 column 3 yes 
row 4 column 5 no 
row 4 column 2 yes 
<EOS> </p>
<p>W Peter, Jessica B Battaglia, Victor Hamrick, Alvaro Bapst, Vinicius Sanchez-Gonzalez, Mateusz Zambaldi, Andrea Malinowski, David Tacchetti, Adam Raposo, Ryan Santoro, Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. arXiv preprintPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.</p>
<p>Rishi Bommasani, A Drew, Ehsan Hudson, Russ Adeli, Simran Altman, Arora, Sydney Von Arx, S Michael, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish2020Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, abs/2110.14168CoRRKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.</p>
<p>. Bertram Felgenhauer, Frazer Jarvis, Mathematics of sudoku i. Mathematical Spectrum. 391Bertram Felgenhauer and Frazer Jarvis. Mathematics of sudoku i. Mathematical Spectrum, 39(1):15-22, 2006.</p>
<p>Transformer language models without positional encodings still learn positional information. Adi Haviv, Ori Ram, arXiv:2203.16634Ofir PressPeter Izsak, and Omer LevyarXiv preprintAdi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Geoffrey P Ny Louis Lee, Philip N Johnson-Laird Goodwin, The psychological puzzle of sudoku. Thinking &amp; Reasoning. 14NY Louis Lee, Geoffrey P Goodwin, and Philip N Johnson-Laird. The psychological puzzle of sudoku. Thinking &amp; Reasoning, 14(4):342-364, 2008.</p>
<p>Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, arXiv:2206.14858arXiv preprintAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Yuxuan Li, James L Mcclelland, arXiv:2210.00400arXiv preprintYuxuan Li and James L McClelland. Systematic generalization and emergent structures in transformers trained on structured tasks. arXiv preprint arXiv:2210.00400, 2022.</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837arXiv preprintSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.</p>
<p>What underlies rapid learning and systematic generalization in humans. Andrew Joohun , Nam , James L Mcclelland, arXiv:2107.06994arXiv preprintAndrew Joohun Nam and James L McClelland. What underlies rapid learning and systematic generaliza- tion in humans. arXiv preprint arXiv:2107.06994, 2021.</p>
<p>Recurrent relational networks. Rasmus Palm, Ulrich Paquet, Ole Winther, Advances in neural information processing systems. 31Rasmus Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. Advances in neural information processing systems, 31, 2018.</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. Ofir Press, A Noah, Mike Smith, Lewis, arXiv:2108.12409arXiv preprintOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
<p>. Ed Russell, Frazer Jarvis, Mathematics of sudoku ii. Mathematical Spectrum. 392Ed Russell and Frazer Jarvis. Mathematics of sudoku ii. Mathematical Spectrum, 39(2):54-58, 2006.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Do prompt-based models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, arXiv:2109.01247arXiv preprintAlbert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247, 2021.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
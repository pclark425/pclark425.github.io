<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4365 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4365</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4365</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-274305670</p>
                <p><strong>Paper Title:</strong> Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation</p>
                <p><strong>Paper Abstract:</strong> This research presents and compares multiple approaches to automate the generation of literature reviews using several Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM). The ever-increasing number of research articles provides a huge challenge for manual literature review. It has resulted in an increased demand for automation. Developing a system capable of automatically generating the literature reviews from only the PDF files as input is the primary objective of this research work. The effectiveness of several Natural Language Processing (NLP) strategies, such as the frequency-based method (spaCy), the transformer model (Simple T5), and retrieval-augmented generation (RAG) with Large Language Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews. The ROUGE scores are used for the evaluation of all three systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo achieved the highest ROUGE-1 score, 0.364. The transformer model comes in second place and spaCy is at the last position. Finally, a graphical user interface is created for the best system based on the large language model.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4365.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4365.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-GPT3.5-LitReview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation using GPT-3.5-TURBO-0125 for Automated Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end pipeline that constructs a custom OpenAI assistant with retrieval enabled (using the SciTLDR dataset) and uses GPT-3.5-TURBO-0125 to generate per-paper summaries which are merged into a coherent literature-review segment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAG-based LLM literature review system (custom OpenAI Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Builds a custom OpenAI Assistant configured with retrieval enabled and the SciTLDR dataset loaded as its knowledge base; prompt engineering defines the expected literature-review style output (max ~80 words per paper, include first author and title). The pipeline extracts full text from input PDFs via PyPDF2, creates a new conversation/thread per paper with the extracted text as the query, submits it to the assistant (which performs retrieval-augmented generation using the loaded dataset), retrieves the assistant's per-paper output, and post-processes/merges these outputs into a single coherent literature-review segment presented in a GUI.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-TURBO-0125 (OpenAI assistant configured with retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Full-text extraction from PDFs using PyPDF2 followed by retrieval-augmented generation (RAG) via the OpenAI assistant's retrieval-enabled knowledge base (SciTLDR); inputs are submitted as conversation threads to the assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Per-paper RAG-generated summaries are post-processed and concatenated/merged into a single coherent literature-review segment (simple multi-document merging of assistant outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on the SciTLDR dataset (5,400 TLDRs from ~3,200 papers) for model knowledge; user-evaluation example processed 4 papers; system designed to accept multiple PDFs with no strict upper bound specified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific papers (SciTLDR summarization dataset); user-evaluation example in IoT/healthcare domain.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature review segments (short per-paper summaries merged into a literature-review paragraph conforming to IEEE-style guidance).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>ROUGE-1 = 0.364, ROUGE-2 = 0.123, ROUGE-L = 0.181, ROUGE-Lsum = 0.182 (reported on the test dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Transformer-based Simple T5 model and a frequency-based spaCy extractive method</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed both baselines: GPT-3.5-TURBO-0125 ROUGE-1 = 0.364 vs T5 ROUGE-1 = 0.268 and spaCy ROUGE-1 = 0.257 (similar advantage on ROUGE-2 and ROUGE-L variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented LLM generation (GPT-3.5-TURBO-0125 with SciTLDR retrieval) produced higher unigram/bigram overlap with human summaries than both a trained T5 and a frequency-based spaCy extractor; system can generate coherent literature-review segments from PDFs and supports a simple GUI front-end.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper does not deeply analyze LLM-specific failure modes; related-work discussion highlights general synthesis challenges and the need for fine-tuning domain-specific LLMs to improve synthesis accuracy; no explicit factuality/hallucination analysis or contradiction-handling strategy is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>No detailed scaling study reported; system evaluated using SciTLDR (several thousand papers in the dataset) and small user-evaluation (4 papers); authors recommend exploring additional/more capable models (BERT, Gemini, LLaMA) in future work but do not quantify scaling trends with number of papers or model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4365.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4365.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-LLM-Review (Tsai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative analysis of automatic literature review using Mistral Large Language Model and human reviewers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced comparative study that evaluates Mistral LLM's effectiveness for automatic literature review against human reviewers (mentioned in this paper's related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comparative analysis of automatic literature review using mistral large language model and human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mistral LLM-based automatic literature review (as evaluated by Tsai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an existing comparative analysis of an LLM (Mistral) versus human reviewers for automatic literature-review tasks; the current paper does not detail the architecture, pipelines, or prompt/retrieval configurations used in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mistral LLM (model size and variant not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not specified in this paper (reference only); described broadly as automatic literature-review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic research / automatic literature-review evaluation (exact domain not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automatic literature reviews (evaluated against human reviewers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed here (paper only references the comparative analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human reviewers (as per the referenced study).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as part of related work illustrating active evaluation of LLMs (Mistral) for academic literature-review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in detail in this paper; referenced work presumably includes evaluation details.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4365.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4365.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-specific LLM fine-tuning (Susnjak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automating research synthesis with domain-specific large language model fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that discusses gaps between systematic literature reviews and LLMs and proposes fine-tuning domain-specific LLMs to improve knowledge synthesis accuracy for research synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automating research synthesis with domain-specific large language model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Domain-specific LLM fine-tuning for research synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced proposal/study advocating fine-tuning large language models on domain-specific datasets to improve the accuracy of knowledge synthesis and address synthesis-phase challenges in systematic literature reviews; the present paper cites this work to motivate RAG/fine-tuning approaches but does not itself perform fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified here (paper references the concept of fine-tuning 'large language models').</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified in detail here; conceptually aims to improve synthesis by domain-adapted model training and possibly curated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Domain-adapted knowledge synthesis via fine-tuned LLMs (as advocated by the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic literature reviews / domain-specific research synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research synthesis / improved literature-review style outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed here; referenced work likely evaluates synthesis quality but this paper only cites the need for improved synthesis accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Implied comparison against non-fine-tuned LLMs or traditional synthesis methods (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced work highlights gaps in synthesis when using general LLMs and recommends domain-specific fine-tuning to improve accuracy; the present paper cites this as motivation for RAG/fine-tuning research paths.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Referenced work emphasizes challenges in the synthesis phase and the need to address synthesis inaccuracies; specifics are not reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4365.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4365.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yuan-et-al-ReviewAutomation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can we automate scientific reviewing?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study exploring machine learning, natural language generation, multi-document summarization, and multi-objective optimization for automating aspects of scientific reviewing and review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can we automate scientific reviewing?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ML/NLG/multi-document summarization pipeline for automatic scientific reviewing (as discussed by Yuan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced exploration of methods including machine learning, natural language generation, and multi-document summarization to automate scientific reviewing; the current paper cites this study's discussion of comprehensive-review generation and its limitations compared to human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not explicitly specified in this paper's summary of Yuan et al.; the referenced study explores ML and NLG approaches (may include neural language models).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-document summarization and ML-driven extraction techniques (as summarized by this paper, but specific extraction methods are not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document summarization and multi-objective optimization to generate comprehensive reviews (described at a high level in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific peer review / general scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comprehensive reviews of scientific papers and automated review text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Referenced limitations note that generated constructive feedback currently lags human reviewers; exact metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper's brief citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human-written reviews (discussed in the referenced study).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Referenced conclusion: models are not yet fully capable of automating literature reviews and require human reviewers; no numeric comparison given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Yuan et al. discuss potential and limitations of ML/NLG and multi-document summarization for automating reviews, concluding current systems cannot fully replace human reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Referenced study identifies limitations in producing constructive feedback comparable to humans and implies need for further model improvements and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automating research synthesis with domain-specific large language model fine-tuning. <em>(Rating: 2)</em></li>
                <li>Comparative analysis of automatic literature review using mistral large language model and human reviewers. <em>(Rating: 2)</em></li>
                <li>Can we automate scientific reviewing? <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4365",
    "paper_id": "paper-274305670",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "RAG-GPT3.5-LitReview",
            "name_full": "Retrieval-Augmented Generation using GPT-3.5-TURBO-0125 for Automated Literature Review",
            "brief_description": "An end-to-end pipeline that constructs a custom OpenAI assistant with retrieval enabled (using the SciTLDR dataset) and uses GPT-3.5-TURBO-0125 to generate per-paper summaries which are merged into a coherent literature-review segment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RAG-based LLM literature review system (custom OpenAI Assistant)",
            "system_description": "Builds a custom OpenAI Assistant configured with retrieval enabled and the SciTLDR dataset loaded as its knowledge base; prompt engineering defines the expected literature-review style output (max ~80 words per paper, include first author and title). The pipeline extracts full text from input PDFs via PyPDF2, creates a new conversation/thread per paper with the extracted text as the query, submits it to the assistant (which performs retrieval-augmented generation using the loaded dataset), retrieves the assistant's per-paper output, and post-processes/merges these outputs into a single coherent literature-review segment presented in a GUI.",
            "llm_model_used": "GPT-3.5-TURBO-0125 (OpenAI assistant configured with retrieval)",
            "extraction_technique": "Full-text extraction from PDFs using PyPDF2 followed by retrieval-augmented generation (RAG) via the OpenAI assistant's retrieval-enabled knowledge base (SciTLDR); inputs are submitted as conversation threads to the assistant.",
            "synthesis_technique": "Per-paper RAG-generated summaries are post-processed and concatenated/merged into a single coherent literature-review segment (simple multi-document merging of assistant outputs).",
            "number_of_papers": "Evaluated on the SciTLDR dataset (5,400 TLDRs from ~3,200 papers) for model knowledge; user-evaluation example processed 4 papers; system designed to accept multiple PDFs with no strict upper bound specified.",
            "domain_or_topic": "General scientific papers (SciTLDR summarization dataset); user-evaluation example in IoT/healthcare domain.",
            "output_type": "Literature review segments (short per-paper summaries merged into a literature-review paragraph conforming to IEEE-style guidance).",
            "evaluation_metrics": "ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum",
            "performance_results": "ROUGE-1 = 0.364, ROUGE-2 = 0.123, ROUGE-L = 0.181, ROUGE-Lsum = 0.182 (reported on the test dataset).",
            "comparison_baseline": "Transformer-based Simple T5 model and a frequency-based spaCy extractive method",
            "performance_vs_baseline": "Outperformed both baselines: GPT-3.5-TURBO-0125 ROUGE-1 = 0.364 vs T5 ROUGE-1 = 0.268 and spaCy ROUGE-1 = 0.257 (similar advantage on ROUGE-2 and ROUGE-L variants).",
            "key_findings": "Retrieval-augmented LLM generation (GPT-3.5-TURBO-0125 with SciTLDR retrieval) produced higher unigram/bigram overlap with human summaries than both a trained T5 and a frequency-based spaCy extractor; system can generate coherent literature-review segments from PDFs and supports a simple GUI front-end.",
            "limitations_challenges": "Paper does not deeply analyze LLM-specific failure modes; related-work discussion highlights general synthesis challenges and the need for fine-tuning domain-specific LLMs to improve synthesis accuracy; no explicit factuality/hallucination analysis or contradiction-handling strategy is reported.",
            "scaling_behavior": "No detailed scaling study reported; system evaluated using SciTLDR (several thousand papers in the dataset) and small user-evaluation (4 papers); authors recommend exploring additional/more capable models (BERT, Gemini, LLaMA) in future work but do not quantify scaling trends with number of papers or model size.",
            "uuid": "e4365.0",
            "source_info": {
                "paper_title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Mistral-LLM-Review (Tsai et al.)",
            "name_full": "Comparative analysis of automatic literature review using Mistral Large Language Model and human reviewers",
            "brief_description": "A referenced comparative study that evaluates Mistral LLM's effectiveness for automatic literature review against human reviewers (mentioned in this paper's related work).",
            "citation_title": "Comparative analysis of automatic literature review using mistral large language model and human reviewers.",
            "mention_or_use": "mention",
            "system_name": "Mistral LLM-based automatic literature review (as evaluated by Tsai et al.)",
            "system_description": "Mentioned as an existing comparative analysis of an LLM (Mistral) versus human reviewers for automatic literature-review tasks; the current paper does not detail the architecture, pipelines, or prompt/retrieval configurations used in the referenced work.",
            "llm_model_used": "Mistral LLM (model size and variant not specified in this paper)",
            "extraction_technique": "Not specified in this paper (reference only).",
            "synthesis_technique": "Not specified in this paper (reference only); described broadly as automatic literature-review generation.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Academic research / automatic literature-review evaluation (exact domain not specified in this paper).",
            "output_type": "Automatic literature reviews (evaluated against human reviewers).",
            "evaluation_metrics": "Not detailed here (paper only references the comparative analysis).",
            "performance_results": "Not provided in this paper (reference only).",
            "comparison_baseline": "Human reviewers (as per the referenced study).",
            "performance_vs_baseline": "Not quantified here.",
            "key_findings": "Mentioned as part of related work illustrating active evaluation of LLMs (Mistral) for academic literature-review tasks.",
            "limitations_challenges": "Not discussed in detail in this paper; referenced work presumably includes evaluation details.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4365.1",
            "source_info": {
                "paper_title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Domain-specific LLM fine-tuning (Susnjak et al.)",
            "name_full": "Automating research synthesis with domain-specific large language model fine-tuning",
            "brief_description": "A referenced work that discusses gaps between systematic literature reviews and LLMs and proposes fine-tuning domain-specific LLMs to improve knowledge synthesis accuracy for research synthesis tasks.",
            "citation_title": "Automating research synthesis with domain-specific large language model fine-tuning.",
            "mention_or_use": "mention",
            "system_name": "Domain-specific LLM fine-tuning for research synthesis",
            "system_description": "Referenced proposal/study advocating fine-tuning large language models on domain-specific datasets to improve the accuracy of knowledge synthesis and address synthesis-phase challenges in systematic literature reviews; the present paper cites this work to motivate RAG/fine-tuning approaches but does not itself perform fine-tuning.",
            "llm_model_used": "Not specified here (paper references the concept of fine-tuning 'large language models').",
            "extraction_technique": "Not specified in detail here; conceptually aims to improve synthesis by domain-adapted model training and possibly curated datasets.",
            "synthesis_technique": "Domain-adapted knowledge synthesis via fine-tuned LLMs (as advocated by the referenced work).",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Systematic literature reviews / domain-specific research synthesis.",
            "output_type": "Research synthesis / improved literature-review style outputs.",
            "evaluation_metrics": "Not detailed here; referenced work likely evaluates synthesis quality but this paper only cites the need for improved synthesis accuracy.",
            "performance_results": "Not provided in this paper.",
            "comparison_baseline": "Implied comparison against non-fine-tuned LLMs or traditional synthesis methods (not detailed here).",
            "performance_vs_baseline": "Not quantified in this paper.",
            "key_findings": "Referenced work highlights gaps in synthesis when using general LLMs and recommends domain-specific fine-tuning to improve accuracy; the present paper cites this as motivation for RAG/fine-tuning research paths.",
            "limitations_challenges": "Referenced work emphasizes challenges in the synthesis phase and the need to address synthesis inaccuracies; specifics are not reproduced in this paper.",
            "scaling_behavior": "Not discussed in detail within this paper.",
            "uuid": "e4365.2",
            "source_info": {
                "paper_title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Yuan-et-al-ReviewAutomation",
            "name_full": "Can we automate scientific reviewing?",
            "brief_description": "A referenced study exploring machine learning, natural language generation, multi-document summarization, and multi-objective optimization for automating aspects of scientific reviewing and review generation.",
            "citation_title": "Can we automate scientific reviewing?",
            "mention_or_use": "mention",
            "system_name": "ML/NLG/multi-document summarization pipeline for automatic scientific reviewing (as discussed by Yuan et al.)",
            "system_description": "Referenced exploration of methods including machine learning, natural language generation, and multi-document summarization to automate scientific reviewing; the current paper cites this study's discussion of comprehensive-review generation and its limitations compared to human reviewers.",
            "llm_model_used": "Not explicitly specified in this paper's summary of Yuan et al.; the referenced study explores ML and NLG approaches (may include neural language models).",
            "extraction_technique": "Multi-document summarization and ML-driven extraction techniques (as summarized by this paper, but specific extraction methods are not detailed here).",
            "synthesis_technique": "Multi-document summarization and multi-objective optimization to generate comprehensive reviews (described at a high level in this paper).",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Scientific peer review / general scientific literature.",
            "output_type": "Comprehensive reviews of scientific papers and automated review text.",
            "evaluation_metrics": "Referenced limitations note that generated constructive feedback currently lags human reviewers; exact metrics are not provided here.",
            "performance_results": "Not provided in this paper's brief citation.",
            "comparison_baseline": "Human-written reviews (discussed in the referenced study).",
            "performance_vs_baseline": "Referenced conclusion: models are not yet fully capable of automating literature reviews and require human reviewers; no numeric comparison given in this paper.",
            "key_findings": "Yuan et al. discuss potential and limitations of ML/NLG and multi-document summarization for automating reviews, concluding current systems cannot fully replace human reviewers.",
            "limitations_challenges": "Referenced study identifies limitations in producing constructive feedback comparable to humans and implies need for further model improvements and human oversight.",
            "scaling_behavior": "Not discussed in detail within this paper.",
            "uuid": "e4365.3",
            "source_info": {
                "paper_title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automating research synthesis with domain-specific large language model fine-tuning.",
            "rating": 2,
            "sanitized_title": "automating_research_synthesis_with_domainspecific_large_language_model_finetuning"
        },
        {
            "paper_title": "Comparative analysis of automatic literature review using mistral large language model and human reviewers.",
            "rating": 2,
            "sanitized_title": "comparative_analysis_of_automatic_literature_review_using_mistral_large_language_model_and_human_reviewers"
        },
        {
            "paper_title": "Can we automate scientific reviewing?",
            "rating": 2,
            "sanitized_title": "can_we_automate_scientific_reviewing"
        }
    ],
    "cost": 0.010508749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation</p>
<p>Nurshat Fateh Ali nurshatfateh@gmail.com 
Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka
Bangladesh</p>
<p>Md Mahdi Mohtasim mahdimohtasim@gmail.com 
Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka
Bangladesh</p>
<p>Shakil Mosharrof shakilmrf8@gmail.com 
Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka
Bangladesh</p>
<p>T Gopi 
Department of Computer Science and Engineering Military Institute of Science and Technology Dhaka
Bangladesh</p>
<p>Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation
4994424AA76661362477D9BBA006BA7110.1109/ICISET62123.2024.10939517T5SpaCyLarge Language ModelGPTROUGELiterature ReviewNatural Language ProcessingRetrieval-augmented generation
This research presents and compares multiple approaches to automate the generation of literature reviews using several Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM).The ever-increasing number of research articles provides a huge challenge for manual literature review.It has resulted in an increased demand for automation.Developing a system capable of automatically generating the literature reviews from only the PDF files as input is the primary objective of this research work.The effectiveness of several Natural Language Processing (NLP) strategies, such as the frequency-based method (spaCy), the transformer model (Simple T5), and retrievalaugmented generation (RAG) with Large Language Model (GPT-3.5-turbo), is evaluated to meet the primary objective.The SciTLDR dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews.The ROUGE scores are used for the evaluation of all three systems.Based on the evaluation, the Large Language Model GPT-3.5-turboachieved the highest ROUGE-1 score, 0.364.The transformer model comes in second place and spaCy is at the last position.Finally, a graphical user interface is created for the best system based on the large language model.</p>
<p>I. INTRODUCTION</p>
<p>Literature reviews have gained considerable importance for scholars.It provides researchers with a comprehensive overview of previous findings in a specific field and assists scholars in identifying gaps in past understandings.It helps to conduct future research and informs researchers of areas where they can provide significant input.However, conducting literature reviews can be incredibly cumbersome because there's so much to read.Due to the vast volume of research articles being released, reviewing all related studies and extracting relevant information can be a time-consuming, tedious, and error-prone task.Due to these difficulties, there has been an increasing interest in automating the process of literature reviews [1].Automated systems can use natural language processing techniques and machine learning algorithms to analyze extensive amounts of text, extract relevant details, and create structured summaries [2].</p>
<p>The primary objective of this research is to develop a system that can automatically generate the literature review segment of a research paper by using only the PDF files of the related papers as input.Several Natural Language Processing techniques such as the Frequency-based approach, Transformerbased approach, and Large Language Model-based approach are implemented and compared to find the best procedure.The SciTLDR dataset [3] is selected for this research work.The first procedure uses the frequency-based approach.The library named spaCy [4] is utilized here.The second procedure uses the transformer-based model.The Simple T5 model is utilized here.The last procedure is based on using the Large Language Model.The GPT-3.5-TURBO-0125 model is utilized here.The evaluation and comparison are performed using ROUGE scores [5].Then the best approach is identified and a Graphical User Interface-based tool is created.</p>
<p>Automating aspects of the literature review process allows academicians to save time and concentrate on the most pertinent articles for their research.It can also reduce the chance of errors or prejudice in the review process.The highlights of this article are:</p>
<p>â€¢ All three considered NLP approaches such as spaCy, T5, and GPT-3.5-TURBO-0125model can produce satisfactory results in automating the literature review generation.</p>
<p>II. LITERATURE REVIEW</p>
<p>A framework was proposed by Silva et al. [6] for automatically producing systematic literature reviews.They have focused on four technical steps: Searching, Screening, Mapping, and Synthesizing.In response to a specific inquiry, extensive searches are conducted to find as much relevant research as feasible, involving looking through reference lists, scouring internet databases, and reviewing published materials.Screening reduces the search scope by limiting the collection to only the papers pertinent to a particular review, aiming to highlight important findings and facts that could influence policy.Mapping is used to comprehend research activity in a particular area, involve stakeholders, and define priorities concerning the review emphasis.Synthesizing integrates data from numerous sources and provides an overview of the outcomes.The formulation of research questions, reporting phase, and peer review are some steps that are also discussed for the composition of systematic literature reviews.</p>
<p>Peer-reviewed publications are growing exponentially with the rapid development of science.Therefore, Yuan et al. [7] have explored the use of machine learning techniques, natural language generation, multi-document summarization, and multi-objective optimization for automating scientific reviewing.They have discussed the generation of comprehensive reviews and noted the limitations of constructive feedback compared to human-written reviews.The models used in this research are not yet fully capable of automating Literature Reviews and they require human reviewers.</p>
<p>A comprehensive analysis of existing tools for systematic literature reviews was done by Karakan et al. [8].They have explored the potential for automation in various phases of the review process, highlighting the need for a holistic tool design to address researchers' challenges effectively.They have discussed two methodologies to accomplish their research: Rapid Review and Semi-Structured Interviews.Rapid Review emphasizes decision-making procedures for resolving issues, difficulties, and challenges that software engineers encounter in their daily work.Semi-structured interviews are used to explore researchers' experiences, challenges, strategies, strengths, weaknesses of Systematic Literature Review tools, and requirements for effective support in software engineering.</p>
<p>Jaspers et al. [9] focused on the use of machine learning techniques for automation of literature reviews and systematic reviews.They have outlined the pros and cons of different machine-learning techniques.The process of automating the literature review was elaborately discussed.The paper lacks practical validation across diverse domains and detailed insights.</p>
<p>A concise overview of automated literature reviews was presented by Tauchert et.al. [10] They have emphasized the potential for automation in various stages of the systematic review process.The paper discusses the importance of integrating computational techniques to streamline tasks such as searching, screening, extraction, and synthesis.It also acknowledges the need for further research to address challenges and enhance the effectiveness of automated approaches.</p>
<p>A brief overview on the topic of automatic literature review tools was given by Tsai et.al. [11] They discussed the existing research in the field, the challenges faced in conducting literature reviews manually, and the potential benefits of automating the process.The main focus of their contributions is the evaluation of Mistral LLM's effectiveness in the field of Academic Research.</p>
<p>The gaps in the intersection of systematic literature reviews (SLRs) and LLMs are discussed by Susnjak et.al. [12].They also emphasized the need to address challenges in the synthesis phase of research and highlighted the potential of fine-tuning LLMs with datasets to enhance knowledge synthesis accuracy.The study aims to bridge this gap by proposing a Systematic Literature Review automation framework.</p>
<p>Most of the related works that have been discussed are mainly focused on discussing the potential and challenges of using NLP techniques and LLMs to automate the literature review process.None of them proposes a complete system pipeline where users can directly generate the literature review only using the PDF and DOI.In contrast, this article proposes and implements three unique end-to-end pipelines and procedures for a literature review automation system.This research endeavor has also resulted in the implementation of a UI tool where users can directly upload PDFs and get a literature review segment generated automatically without any additional effort.Moreover, this paper also includes a comparative analysis of different approaches such as the frequency-based approach, transformer-based approach, and rag-based approach using ROUGE scores which contributes towards finding the effectiveness of these approaches for this task.</p>
<p>III. SYSTEM DESIGN</p>
<p>The research is carried out in four stages: 1. Defining research objectives.2. Proposing multiple procedures for automated literature review generation.3. Evaluating multiple procedures to find the best approach.4. The final system development.</p>
<p>A. Dataset Selection</p>
<p>The SciTLDR dataset from the Hugging Face is selected for this research work [13].It contains the summarization of scientific documents.It is a dataset with 5,400 TLDRs derived from over 3,200 papers.It contains both author-written and expert-derived TLDRs of scientific documents.Curated research articles' abstract, introduction, and conclusion (AIC) or full text of the paper are given as "source" and the summaries of the corresponding articles are given as "target".Only these two attributes are utilized in all three proposed procedures.There is no training for the spaCy approach, but the dataset is utilized for testing purposes.The T5 model is trained using the SciTLDR dataset for the transformer-based approach and later evaluated on the test dataset.For the LLMbased approach, this dataset is used as the knowledge base for the model.</p>
<p>B. The Procedure Utilizing the Frequency-Based Approach using spaCy</p>
<p>The first procedure utilizes the frequency-based approach by using spaCy.The first task is to build the model pipeline.The model pipeline takes text as input and converts the text into NLP tokens using the spaCy library.Then preprocessing step is done by removing stop words and punctuation.Afterward, the word frequency is calculated for each word which later helps to calculate individual sentence weights.This sentence weight represents the importance of that sentence.Then the top 10 percent of sentences are selected as the final output.The model is later evaluated using ROUGE scores to get an overview of the performance.The overview of the spaCy Model is given in Figure 1.Later it performs post-processing and merges all summaries to produce a coherent literature review segment.The system pipeline overview of the spaCy Model is given in Figure 2.  The next step is to implement a system pipeline by using the transformer-based model to generate a literature review segment automatically.The system takes the DOI and PDF of multiple papers as input.It uses the Requests library to collect the paper titles and first author names from DOIs.Then it uses PYPDF2 and Regular Expression (RE) libraries to collect each PDF's abstract, introduction, and conclusion.Then it merges 3 of these sections to get the final model input.Later it uses the previously trained and saved T5 model to get a summary of each paper.In the next step, it performs post-processing and merges all summaries to produce a coherent literature review segment.The system pipeline overview of the Transformer Model is given in Figure 4.The third procedure utilizes the RAG-based approach by using the Large Language Model: GPT-3.5-TURBO-0125.The first task is to create a custom OpenAI Assistant.Firstly, the SciTLDR dataset is collected, and then the GPT-3.5-TURBO-0125model is selected for the OpenAI assistant.The retrieval is turned on and the dataset is added for the knowledge of the LLM.Now some prompt engineering is performed to produce the required output.Then the LLM results are evaluated using ROUGE SCORE.The overview of the creation of the OpenAI assistant is given in figure 5.The used prompt: "The user will give you a pdf file as input, similar to the "input" field of the given "data.json"file in your knowledge base.You have to produce a summarized "output" for the given pdf based on the file given to your knowledge.The output will be of max 80 words.Note: You must write in a way that can be considered a literature review of a new research paper.The user in the future might add more PDFs so try to make the literature review coherent and as per IEEE standards.Please mention the first author's name and paper title.Don't write like this "Literature Review of. . ."." Figure 6: Pipeline using LLM The next step is to implement a system pipeline by using the LLM to generate a literature review segment automatically.The system takes PDFs of multiple papers as input.It uses the PYPDF2 library to extract the entire text of each PDF.Then it creates a new thread with the extracted text as a message and submits the thread to the assistant with the extracted text as a query.Then the response from the assistant is retrieved and the outputs of each paper are merged for the final literature review segment.The system pipeline overview of the LLM is given in Figure 6.</p>
<p>E. The Final System Tool</p>
<p>The final system is implemented using the Large Language Model: GPT-3.5-TURBO-0125 as the backend.An aesthetic and simple user interface is created where the user can easily upload multiple research articles or PDF files.The user has to press the "Browse files" button and then select the files to upload.Then the system loads the research papers and within a few seconds, it produces the literature review segment automatically.It individually processes each paper and produces output.The loading screen and processing file numbers indicate the progress level and the number of processed papers.At the end of the literature review, the UI shows "Done" text to indicate the completion of the task.The user interface of the system is given in Figure 7</p>
<p>A. Evaluation of Frequency-Based spaCy</p>
<p>The spaCy-based model was evaluated on the test data utilizing the ROUGE scores.The results are stated in Table I.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>C. Evaluation of Large Language Model: GPT-3.5-TURBO-0125</p>
<p>The LLM-based model was evaluated on the test data utilizing the ROUGE scores.The results are stated in Table III.</p>
<p>D. Comparison of Multiple Approaches</p>
<p>The comparison between all three approaches is provided in Table IV.From the ROUGE scores, it is clear that the LLM-based model outperformed both T5 and spaCy.The Transformerbased model is in the second spot based on the ROUGE-1 score.The evaluation proves that large language models are superior to older NLP approaches.</p>
<p>E. User Evaluation on UI</p>
<p>The user evaluation was performed on the LLM-based UI system.The user chose 4 research articles as input.These were:</p>
<p>â€¢ AI-IoT based Healthcare Prognosis Interactive System â€¢ Applying Internet of Things and Machine-Learning for Personalized Healthcare: Issues and Challenges â€¢ Improving patients health-care using IoT â€¢ IoT Based AI and its Implementations in Industries The received output from the system: "The paper by Joshua Ernest Pedi Reddy et al. presents an AI-IoT based Healthcare Prognosis Interactive System (HPIS) to provide real-time medical diagnosis and support in the absence of healthcare workers.The system includes an AI-based chatbot and Application Interface to gather information, answer medical queries, assist users, and ensure medication adherence.With a focus on improving healthcare accessibility and quality, the system integrated smart pill dispensers for monitoring and enhancing medication adherence, offering great potential in revolutionizing healthcare services.Ahamed explores the application of IoT and Machine Learning in Personalized Healthcare (PH) to enhance disease management and intervention.IoT sensor devices and wearables collect patient data, which is analyzed using AI and ML techniques for disease prediction and patient self-management.Challenges include biased data collection, outdated training datasets, and privacy concerns.The integration of IoT and ML in PH faces issues such as data transmission reliability and biased training datasets, impacting diagnostic accuracy.Addressing these challenges is crucial for improving personalized healthcare systems.Khurana implements a Smart Healthcare System using IoT sensors to enhance patient care in hospitals.The system utilizes Ultrasonic and IR Proximity Sensors connected to an Arduino Uno for automated IV fluid level monitoring and patient alarm system.The literature review highlights the importance of automated Hospital Management Systems for efficient healthcare administration.The proposed system reduces manpower, costs, and human errors while improving patient care.Future research aims to enhance system scalability and functionality for widespread hospital use.Sherif El-Gendy explores the integration of IoT and AI in industries in the paper "IoT Based AI and its Implementations in Industries."The paper delves into Industry 4.0, IIoT, IAIoT, and IoRT, showcasing the impact on automation and robotics.It discusses IoT challenges, benefits of AI in data analysis, and presents case studies like oil field production optimization and smart robotics by companies like ABB and Boeing.The future of IoT/AI integration promises transformative advancements in various sectors."</p>
<p>V. RESULT AND DISCUSSION</p>
<p>The study introduced three procedures for automated literature review generation.The research work also illustrates the performance comparison between various NLP approaches such as the frequency-based method (spaCy), transformer model (Simple T5), and retrieval-augmented generation (RAG) with LLM (GPT-3.5-turbo).All three procedures are implemented and the ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores are calculated based on the Test dataset.For all three approaches, the ROUGE-1 and ROUGE-2 scores are found above the acceptable mark.</p>
<p>From the evaluation, it is seen that the GPT-3.5-turbomodel produced results with higher ROUGE-1 and ROUGE-2 scores than the SpaCy and T5.The overall ROUGE-1 score for the LLM is 0.364 while the score for T5 is 0.268 and spaCy is 0.257.It shows that the LLM-generated summaries have better unigram and bigram overlapping with human summaries.The transformer T5 is also an advanced model which comes in second place.The last position is occupied by the frequencybased spaCy model.</p>
<p>From the scores, it is clear that the most advanced models are LLMs which outperformed all other NLP techniques.But other approaches such as transformer models and frequencybased approaches are also capable of producing satisfactory ROUGE scores and a coherent literature review segment.</p>
<p>VI. CONCLUSION AND FUTURE SCOPES</p>
<p>The research focused on implementing and comparing various NLP techniques for automated literature review.All three implemented systems are successful in generating the coherent Literature Review segment of a research paper.The results of various Natural Language Processing techniques such as the Frequency-based approach, Transformer model, and Large Language Model are also successfully obtained and compared.Based on the comparisons, the LLM-based approach is proven to be the best-performing one based on ROUGE-N scores.Thus, based on the LLM, a final system tool is also successfully developed where the user can upload multiple PDF files to automatically generate a coherent literature review segment.</p>
<p>Future work of this research work can be focused on enhancing the effectiveness and applicability of the developed system tool.More functionality can be added to the Graphical User Interface such as model options, output size, etc.More models such as Bert, Gemini, and LLaMA can be utilized to find better results.</p>
<p>Figure 1 :
1
Figure 1: Building spaCy Model</p>
<p>Figure 2 :
2
Figure 2: Pipeline using spaCy</p>
<p>Figure 3 :
3
Figure 3: Training of Transformer Model</p>
<p>Figure 4 :
4
Figure 4: Pipeline using Transformer Model</p>
<p>Figure 5 :
5
Figure 5: Creation of Custom OpenAI Assistant</p>
<p>Figure 7 :
7
Figure 7: The Preview of the System UI</p>
<p>Table I :
I
ROUGE Scores for spaCy
ROUGE-10.257ROUGE-20.055ROUGE-L0.144ROUGE-L SUM0.146B. Evaluation of TransformerT5 The transformer-based model was evaluated on the testdata utilizing the ROUGE scores. The results are stated inTable II.</p>
<p>Table II :
II
ROUGE Scores for T5
ROUGE-10.268ROUGE-20.115ROUGE-L0.204ROUGE-L SUM0.204</p>
<p>Table III :
III
ROUGE Scores for LLM
ROUGE-10.364ROUGE-20.123ROUGE-L0.181ROUGE-L SUM0.182</p>
<p>Table IV :
IV
Comparison of Rouge Scores
T5spaCyGPT-3.5-TURBO-0125ROUGE-10.2680.2570.364ROUGE-20.1150.0550.123ROUGE-L0.2040.1440.181ROUGE-L SUM0.2040.1460.182</p>
<p>Automating systematic literature review. Contemporary empirical methods in software engineering. K R Felizardo, J C Carver, 2020</p>
<p>Nlp based machine learning approaches for text summarization. S Adhikari, In2020 Fourth International Conference on Computing Methodologies and Communication (ICCMC). IEEE2020 Mar 11</p>
<p>TLDR: Extreme summarization of scientific documents. I Cachola, K Lo, A Cohan, D S Weld, arXiv:2004.150112020 Apr 30arXiv preprint</p>
<p>S Jugran, A Kumar, B S Tyagi, V Anand, Extractive automatic text summarization using SpaCy in Python &amp; NLP. In2021 International conference on advance computing and innovative technologies in engineering (ICACITE). IEEE2021 Mar 4</p>
<p>ROUGE Score Analysis and Performance Evaluation Between Google T5 and SpaCy for YouTube News Video Summarization. N F Ali, J U Tanvin, M R Islam, J Ahmed, M Akhtaruzzaman, In2023 26th International Conference on Computer and Information Technology (ICCIT). IEEE2023 Dec 13</p>
<p>A roadmap toward the automatic composition of systematic literature reviews. Silva Da, E M JÃºnior, M L Dutra, Iberoamerican Journal of Science Measurement and Communication. 2021 Jul 27</p>
<p>Can we automate scientific reviewing?. W Yuan, P Liu, G Neubig, Journal of Artificial Intelligence Research. 752022 Sep 29</p>
<p>Tool support for systematic literature reviews: Analyzing existing solutions and the potential for automation (Doctoral dissertation. B Karakan, S Wagner, J Bogner, University of Stuttgart</p>
<p>Machine learning techniques for the automation of literature reviews and systematic reviews in EFSA. S Jaspers, De Troyer, E Aerts, M , 2018 JunEFSA Supporting Publications151427E</p>
<p>Towards an integrative approach for automated literature reviews using machine learning. C Tauchert, M Bender, N Mesbah, P Buxmann, </p>
<p>Comparative analysis of automatic literature review using mistral large language model and human reviewers. H C Tsai, Y F Huang, C W Kuo, </p>
<p>Automating research synthesis with domain-specific large language model fine-tuning. T Susnjak, P Hwang, N H Reyes, A L Barczak, T R Mcintosh, S Ranathunga, arXiv:2404.086802024 Apr 8arXiv preprint</p>
<p>. Allenai, Scitl-Dr, Dataset, Dataset</p>
<p>Hugging Face. Available. Sep. 8, 2024</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>            </div>
        </div>

    </div>
</body>
</html>
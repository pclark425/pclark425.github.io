<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4370 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4370</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4370</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-271334330</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.16148v1.pdf" target="_blank">CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support</a></p>
                <p><strong>Paper Abstract:</strong> Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4370.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4370.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-in-the-loop pipeline that uses LLMs to compress study abstracts into concise claims, propose multiple hierarchical organizations (trees of topical categories) for sets of related studies, and then applies expert corrections and learned corrector models to improve study-to-category assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CHIME is a multi-stage pipeline for organizing a curated set of related scientific studies into labeled hierarchical organizations. It consists of (1) a pre-generation module that extracts concise claim statements from each study abstract (claim generation via GPT-3.5 and NLI-based verification with a DEBERTA-V3 model) and extracts frequent entities with ScispaCy; (2) a hierarchy proposal module that, given the claim set and frequent entities, prompts an LLM to propose one or more multi-level hierarchies and assign claims to categories (hierarchy generation using Claude-2 for proposal, with GPT-3.5 used for claim compression); and (3) a human-in-the-loop correction protocol decomposed into three annotation subtasks (parent-child link validity, sibling-group coherence, and claim-category assignment). The dataset of LLM-generated hierarchies is augmented by expert corrections and used to fine-tune corrector models (Flan-T5 variants) that automatically fix claim assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5 (June 2023) for claim generation; Claude-2 for hierarchy proposal; GPT-4/GPT-3.5 used for zero-shot CoT baselines; Flan-T5 (fine-tuned) used as learned corrector; DEBERTA-V3 used for NLI entailment checking (not an LLM for generation).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Claim generation via LLM prompting (GPT-3.5) from abstracts, frequent-entity extraction via ScispaCy, NLI-based factuality filtering (DEBERTA-V3), and provision of claims+entities as structured input to hierarchy-generation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-based hierarchical proposal (prompt-based multi-step generation producing root and sub-categories and assigning claims) combined with human-in-the-loop corrections; automated correction via fine-tuned Flan-T5 correctors; multi-hierarchy generation (up to 5 alternative organizations) and aggregation of claim-category labels along root-to-node paths.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to sets drawn from Cochrane systematic reviews filtered to 15–50 studies per review (average 24.7 studies per topic); CHIME contains hierarchies for 472 research topics (2,174 LLM-generated hierarchies) with 100 topics (320 hierarchies) expert-corrected.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical literature / systematic review topics (Cochrane reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical organizations (tree of topical categories) linking nodes to lists of studies (claims); multiple candidate hierarchies per topic; corrected hierarchies and claim-category assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human expert labels (parent-child link validity, sibling-group coherence), precision/recall/F1 on claim categorization (claim-category assignment), Fleiss' kappa for annotation agreement, coverage of claims, match rate for link tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Parent-child linking: near-perfect (1/1,635 links incorrect). Sibling-group coherence: 77% of sibling groups labeled coherent. Claim categorization (LLM pipeline): precision 0.71, recall 0.53 (aggregate), reported overall claim-assignment F1 ≈ 61.5%. Fine-tuned Flan-T5 corrector improved study assignment by 12.6 F1 points; Flan-T5-large improved recall by 15.9 points on test.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Naive LLM pipeline (un-corrected hierarchies) and human expert corrections; zero-shot chain-of-thought on GPT-3.5/GPT-4 used as baseline correctors; finetuned Flan-T5 models compared to zero-shot LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Finetuned Flan-T5 corrector outperformed the naive LLM pipeline on claim categorization, increasing F1 by 12.6 points; GPT-4-Turbo achieved high recall but lower precision compared to the finetuned corrector.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are strong at generating coherent category labels and correct parent-child category links, but weaker at exhaustive assignment of study claims to categories (low recall). Human-in-the-loop correction combined with a learned corrector substantially improves claim assignment; producing multiple alternative hierarchies provides diverse, useful organizational perspectives (many follow PICO).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Domain-limited to biomedicine (Cochrane); reliance on curated sets of related studies (assumes relevance); LLM inference latency (some models take up to ~1 minute); difficulty automating sibling-coherence detection; LLMs show lower recall in claim assignment (hard for humans to correct recall errors).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Pipeline restricts input sets to <=50 studies because large LLM context limits and degraded performance on long inputs; inference times increase with model size (Claude-2 and other large LLMs have long runtimes), and average generated hierarchy depth is modest (mean 2.5, max 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4370.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4370.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TopicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TopicGPT (prompt-based topic modeling framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based topic modeling approach that uses LLMs to produce topics; the CHIME paper notes TopicGPT can perform hierarchical topic modeling but is limited to two-level hierarchies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Topicgpt: A prompt-based topic modeling framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TopicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt-based topic modeling framework that leverages LLMs to generate topic labels and topic assignments; according to CHIME, TopicGPT attempts hierarchical topic modeling but in practice produces two-level hierarchies only.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based topic generation (LLM-guided topic modeling) as reported in the CHIME paper's related work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical topic modeling (limited to two levels per CHIME's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text/topic modeling (as cited in related work); not specific to CHIME experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Topics / topic hierarchies (two-level according to CHIME).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an LLM-based topic modeling approach; CHIME notes it is limited to two-level hierarchies whereas CHIME generates arbitrary-depth hierarchies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited hierarchical depth (two-level) as noted by CHIME.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4370.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4370.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClusterLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ClusterLLM: Large language models as a guide for text clustering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that leverages LLMs to guide text clustering; CHIME cites this work as an example of using LLMs for organization tasks such as clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clusterllm: Large language models as a guide for text clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ClusterLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-guided clustering framework where a large language model provides guidance or labels to assist clustering methods; referenced by CHIME as related work on LLMs for document organization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based guidance for clustering (prompt-based or LLM-generated labels/constraints), as summarized in CHIME's related work.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Clustering of documents into groups guided by LLM outputs (high-level labels or prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text/document clustering (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Clusters / groupings of documents and descriptive labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to show that LLMs can assist in interpretable organization tasks like clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4370.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4370.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot clustering (Viswanathan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models enable few-shot clustering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that large language models can enable few-shot clustering of text; cited by CHIME as evidence that LLMs can be used for organization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models enable few-shot clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM few-shot clustering</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A technique using LLMs in a few-shot prompting setup to perform or guide text clustering, enabling grouping of documents with minimal labeled examples; CHIME references this as motivating use of LLMs for organization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based few-shot examples to induce clustering behavior in an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Clustering/grouping of documents inferred via LLM outputs and few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text clustering / organization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Clusters and cluster descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to motivate LLM use for organization tasks; demonstrates LLMs can perform clustering with few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4370.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4370.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhu et al. (2023) hierarchical outlines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical catalogue generation for literature review: A benchmark (Zhu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior study that generates intermediate hierarchical outlines to scaffold literature review generation; CHIME contrasts its approach by producing multiple alternative organizations per topic and focusing on assistance rather than end-to-end automatic review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical catalogue generation for literature review: A benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hierarchical catalogue generation (Zhu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach that produces hierarchical outlines to scaffold multi-document literature review generation; cited by CHIME as related prior work that uses outlines but does not produce multiple organizations per topic.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Hierarchical outline generation (details not specified in CHIME; referenced at high level).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Use of hierarchical outlines to structure multidocument summarization/literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review / scientific summarization (benchmark-focused).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical outlines to scaffold literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generates intermediate hierarchical outlines for reviews; CHIME highlights difference in producing multiple alternative hierarchies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Topicgpt: A prompt-based topic modeling framework. <em>(Rating: 2)</em></li>
                <li>Large language models enable few-shot clustering. <em>(Rating: 2)</em></li>
                <li>Clusterllm: Large language models as a guide for text clustering. <em>(Rating: 2)</em></li>
                <li>Hierarchical catalogue generation for literature review: A benchmark. <em>(Rating: 2)</em></li>
                <li>Marg: Multi-agent review generation for scientific papers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4370",
    "paper_id": "paper-271334330",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "CHIME",
            "name_full": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies",
            "brief_description": "A human-in-the-loop pipeline that uses LLMs to compress study abstracts into concise claims, propose multiple hierarchical organizations (trees of topical categories) for sets of related studies, and then applies expert corrections and learned corrector models to improve study-to-category assignments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CHIME",
            "system_description": "CHIME is a multi-stage pipeline for organizing a curated set of related scientific studies into labeled hierarchical organizations. It consists of (1) a pre-generation module that extracts concise claim statements from each study abstract (claim generation via GPT-3.5 and NLI-based verification with a DEBERTA-V3 model) and extracts frequent entities with ScispaCy; (2) a hierarchy proposal module that, given the claim set and frequent entities, prompts an LLM to propose one or more multi-level hierarchies and assign claims to categories (hierarchy generation using Claude-2 for proposal, with GPT-3.5 used for claim compression); and (3) a human-in-the-loop correction protocol decomposed into three annotation subtasks (parent-child link validity, sibling-group coherence, and claim-category assignment). The dataset of LLM-generated hierarchies is augmented by expert corrections and used to fine-tune corrector models (Flan-T5 variants) that automatically fix claim assignments.",
            "llm_model_used": "GPT-3.5 (June 2023) for claim generation; Claude-2 for hierarchy proposal; GPT-4/GPT-3.5 used for zero-shot CoT baselines; Flan-T5 (fine-tuned) used as learned corrector; DEBERTA-V3 used for NLI entailment checking (not an LLM for generation).",
            "extraction_technique": "Claim generation via LLM prompting (GPT-3.5) from abstracts, frequent-entity extraction via ScispaCy, NLI-based factuality filtering (DEBERTA-V3), and provision of claims+entities as structured input to hierarchy-generation prompts.",
            "synthesis_technique": "LLM-based hierarchical proposal (prompt-based multi-step generation producing root and sub-categories and assigning claims) combined with human-in-the-loop corrections; automated correction via fine-tuned Flan-T5 correctors; multi-hierarchy generation (up to 5 alternative organizations) and aggregation of claim-category labels along root-to-node paths.",
            "number_of_papers": "Applied to sets drawn from Cochrane systematic reviews filtered to 15–50 studies per review (average 24.7 studies per topic); CHIME contains hierarchies for 472 research topics (2,174 LLM-generated hierarchies) with 100 topics (320 hierarchies) expert-corrected.",
            "domain_or_topic": "Biomedical literature / systematic review topics (Cochrane reviews)",
            "output_type": "Hierarchical organizations (tree of topical categories) linking nodes to lists of studies (claims); multiple candidate hierarchies per topic; corrected hierarchies and claim-category assignments.",
            "evaluation_metrics": "Human expert labels (parent-child link validity, sibling-group coherence), precision/recall/F1 on claim categorization (claim-category assignment), Fleiss' kappa for annotation agreement, coverage of claims, match rate for link tasks.",
            "performance_results": "Parent-child linking: near-perfect (1/1,635 links incorrect). Sibling-group coherence: 77% of sibling groups labeled coherent. Claim categorization (LLM pipeline): precision 0.71, recall 0.53 (aggregate), reported overall claim-assignment F1 ≈ 61.5%. Fine-tuned Flan-T5 corrector improved study assignment by 12.6 F1 points; Flan-T5-large improved recall by 15.9 points on test.",
            "comparison_baseline": "Naive LLM pipeline (un-corrected hierarchies) and human expert corrections; zero-shot chain-of-thought on GPT-3.5/GPT-4 used as baseline correctors; finetuned Flan-T5 models compared to zero-shot LLMs.",
            "performance_vs_baseline": "Finetuned Flan-T5 corrector outperformed the naive LLM pipeline on claim categorization, increasing F1 by 12.6 points; GPT-4-Turbo achieved high recall but lower precision compared to the finetuned corrector.",
            "key_findings": "LLMs are strong at generating coherent category labels and correct parent-child category links, but weaker at exhaustive assignment of study claims to categories (low recall). Human-in-the-loop correction combined with a learned corrector substantially improves claim assignment; producing multiple alternative hierarchies provides diverse, useful organizational perspectives (many follow PICO).",
            "limitations_challenges": "Domain-limited to biomedicine (Cochrane); reliance on curated sets of related studies (assumes relevance); LLM inference latency (some models take up to ~1 minute); difficulty automating sibling-coherence detection; LLMs show lower recall in claim assignment (hard for humans to correct recall errors).",
            "scaling_behavior": "Pipeline restricts input sets to &lt;=50 studies because large LLM context limits and degraded performance on long inputs; inference times increase with model size (Claude-2 and other large LLMs have long runtimes), and average generated hierarchy depth is modest (mean 2.5, max 5).",
            "uuid": "e4370.0",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "TopicGPT",
            "name_full": "TopicGPT (prompt-based topic modeling framework)",
            "brief_description": "A prompt-based topic modeling approach that uses LLMs to produce topics; the CHIME paper notes TopicGPT can perform hierarchical topic modeling but is limited to two-level hierarchies.",
            "citation_title": "Topicgpt: A prompt-based topic modeling framework.",
            "mention_or_use": "mention",
            "system_name": "TopicGPT",
            "system_description": "Prompt-based topic modeling framework that leverages LLMs to generate topic labels and topic assignments; according to CHIME, TopicGPT attempts hierarchical topic modeling but in practice produces two-level hierarchies only.",
            "llm_model_used": null,
            "extraction_technique": "Prompt-based topic generation (LLM-guided topic modeling) as reported in the CHIME paper's related work discussion.",
            "synthesis_technique": "Hierarchical topic modeling (limited to two levels per CHIME's summary).",
            "number_of_papers": null,
            "domain_or_topic": "General text/topic modeling (as cited in related work); not specific to CHIME experiments.",
            "output_type": "Topics / topic hierarchies (two-level according to CHIME).",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as an LLM-based topic modeling approach; CHIME notes it is limited to two-level hierarchies whereas CHIME generates arbitrary-depth hierarchies.",
            "limitations_challenges": "Limited hierarchical depth (two-level) as noted by CHIME.",
            "scaling_behavior": null,
            "uuid": "e4370.1",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ClusterLLM",
            "name_full": "ClusterLLM: Large language models as a guide for text clustering",
            "brief_description": "An approach that leverages LLMs to guide text clustering; CHIME cites this work as an example of using LLMs for organization tasks such as clustering.",
            "citation_title": "Clusterllm: Large language models as a guide for text clustering.",
            "mention_or_use": "mention",
            "system_name": "ClusterLLM",
            "system_description": "LLM-guided clustering framework where a large language model provides guidance or labels to assist clustering methods; referenced by CHIME as related work on LLMs for document organization.",
            "llm_model_used": null,
            "extraction_technique": "LLM-based guidance for clustering (prompt-based or LLM-generated labels/constraints), as summarized in CHIME's related work.",
            "synthesis_technique": "Clustering of documents into groups guided by LLM outputs (high-level labels or prompts).",
            "number_of_papers": null,
            "domain_or_topic": "General text/document clustering (cited in related work).",
            "output_type": "Clusters / groupings of documents and descriptive labels.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited to show that LLMs can assist in interpretable organization tasks like clustering.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4370.2",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Few-shot clustering (Viswanathan et al.)",
            "name_full": "Large language models enable few-shot clustering",
            "brief_description": "Work demonstrating that large language models can enable few-shot clustering of text; cited by CHIME as evidence that LLMs can be used for organization tasks.",
            "citation_title": "Large language models enable few-shot clustering.",
            "mention_or_use": "mention",
            "system_name": "LLM few-shot clustering",
            "system_description": "A technique using LLMs in a few-shot prompting setup to perform or guide text clustering, enabling grouping of documents with minimal labeled examples; CHIME references this as motivating use of LLMs for organization.",
            "llm_model_used": null,
            "extraction_technique": "Prompt-based few-shot examples to induce clustering behavior in an LLM.",
            "synthesis_technique": "Clustering/grouping of documents inferred via LLM outputs and few-shot prompts.",
            "number_of_papers": null,
            "domain_or_topic": "General text clustering / organization.",
            "output_type": "Clusters and cluster descriptions.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited to motivate LLM use for organization tasks; demonstrates LLMs can perform clustering with few examples.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4370.3",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Zhu et al. (2023) hierarchical outlines",
            "name_full": "Hierarchical catalogue generation for literature review: A benchmark (Zhu et al., 2023)",
            "brief_description": "A prior study that generates intermediate hierarchical outlines to scaffold literature review generation; CHIME contrasts its approach by producing multiple alternative organizations per topic and focusing on assistance rather than end-to-end automatic review generation.",
            "citation_title": "Hierarchical catalogue generation for literature review: A benchmark.",
            "mention_or_use": "mention",
            "system_name": "Hierarchical catalogue generation (Zhu et al.)",
            "system_description": "Approach that produces hierarchical outlines to scaffold multi-document literature review generation; cited by CHIME as related prior work that uses outlines but does not produce multiple organizations per topic.",
            "llm_model_used": null,
            "extraction_technique": "Hierarchical outline generation (details not specified in CHIME; referenced at high level).",
            "synthesis_technique": "Use of hierarchical outlines to structure multidocument summarization/literature review.",
            "number_of_papers": null,
            "domain_or_topic": "Literature review / scientific summarization (benchmark-focused).",
            "output_type": "Hierarchical outlines to scaffold literature review generation.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Generates intermediate hierarchical outlines for reviews; CHIME highlights difference in producing multiple alternative hierarchies.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4370.4",
            "source_info": {
                "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Topicgpt: A prompt-based topic modeling framework.",
            "rating": 2,
            "sanitized_title": "topicgpt_a_promptbased_topic_modeling_framework"
        },
        {
            "paper_title": "Large language models enable few-shot clustering.",
            "rating": 2,
            "sanitized_title": "large_language_models_enable_fewshot_clustering"
        },
        {
            "paper_title": "Clusterllm: Large language models as a guide for text clustering.",
            "rating": 2,
            "sanitized_title": "clusterllm_large_language_models_as_a_guide_for_text_clustering"
        },
        {
            "paper_title": "Hierarchical catalogue generation for literature review: A benchmark.",
            "rating": 2,
            "sanitized_title": "hierarchical_catalogue_generation_for_literature_review_a_benchmark"
        },
        {
            "paper_title": "Marg: Multi-agent review generation for scientific papers.",
            "rating": 1,
            "sanitized_title": "marg_multiagent_review_generation_for_scientific_papers"
        }
    ],
    "cost": 0.01327575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support</p>
<p>Chao-Chun Hsu 
University of Chicago</p>
<p>Erin Bransom 
Allen Institute for AI</p>
<p>Jenna Sparks 
Allen Institute for AI</p>
<p>Bailey Kuehl 
Allen Institute for AI</p>
<p>Chenhao Tan 
University of Chicago</p>
<p>David Wadden 
Allen Institute for AI</p>
<p>Lucy Lu Wang 
Allen Institute for AI</p>
<p>University of Washington</p>
<p>Aakanksha Naik aakankshan@allenai.org 
Allen Institute for AI</p>
<p>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support
F40DE3CCD739F2C9CB447336307EC00CP1Exercise modalities S1Aerobic S2Resistance S3Combined S4Walking S5Weight training
Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands.In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review.We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category.Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine.Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies.CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics.Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved.We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points.We release our dataset and models to encourage research on developing better assistive tools for literature review. 1 1 The CHIME dataset and models are available at https: //github.com/allenai/chime.</p>
<p>Introduction</p>
<p>Literature review, the process by which researchers synthesize many related scientific studies into a higher-level organization, is valuable but extremely Studies: 1,3</p>
<p>Studies: 1,3</p>
<p>Studies: 1, 8, 9</p>
<p>Studies: 4, 5, 7</p>
<p>Studies: 32, 39</p>
<p>Studies: 5, 6, 9</p>
<p>Studies: 1,4</p>
<p>Figure 1: Given a set of related studies on a topic, we use LLMs to identify top-level categories focusing on different views of the data (such as P1 and P2), generate multiple hierarchical organizations, and assign studies to different categories.However, these categories and study assignments can contain errors.As illustrated in the figure, the categories Walking and Weight training are not coherent with their siblings (S1 − S3) in hierarchy 1 since they are more specific, and the categories Metastasis and Recurrence are incorrectly assigned to the parent category in hierarchy 2 since they are not types of cancer.</p>
<p>time-consuming.For instance, in medicine, completing a review from registration to publication takes 67 weeks on average (Borah et al., 2017) and given the rapid pace of scholarly publication, reviews tend to go out-of-date quickly (Shojania et al., 2007).This has prompted development of tools for efficient literature review (Altmami and Menai, 2022).Most tools have focused on au-tomating review generation, treating it as a multidocument summarization task (Mohammad et al., 2009;Jha et al., 2015;Wallace et al., 2020;DeYoung et al., 2021;Liu et al., 2022), sometimes using intermediate structures such as hierarchies/outlines to better scaffold generation (Zhu et al., 2023), with limited success.However, recent work on assessing the utility of NLP tools like LLMs for systematic review reveals that domain experts prefer literature review tools to be assistive instead of automatic (Yun et al., 2023).</p>
<p>Motivated by this finding, we take a different approach and focus on the task of generating hierarchical organizations of scientific studies to assist literature review.As shown in Figure 1, a hierarchical organization is a tree structure in which nodes represent topical categories and every node is linked to a list of studies assigned to that category.Inspired by the adoption of LLMs for information organization uses such as clustering (Viswanathan et al., 2023) and topic modeling (Pham et al., 2023), we investigate the potential of generating hierarchies with a naive LLM-based approach, and observe that models produce promising yet imperfect hierarchies out-of-the-box.</p>
<p>To further assess and improve LLM performance, we collect CHIME (Constructing HIerarchies of bioMedical Evidence), an expert-curated dataset for hierarchy generation.Since building such hierarchies from scratch is very challenging and timeconsuming, we develop a human-in-the-loop protocol in which experts correct errors in preliminary LLM-generated hierarchies.During a three-step error correction process, experts assess the correctness of both links between categories as well as assignment of studies to categories, as demonstrated in Figure 1.Our final dataset consists of two subsets: (i) a set of 472 research topics with up to five LLM-generated hierarchies per topic (2,174 total hierarchies), and (ii) a subset of 100 research topics sampled from the previous set, with 320 expert-corrected hierarchies.</p>
<p>Expert-corrected hierarchies allow us to better quantify LLM performance on hierarchy generation.We observe that LLMs are already quite good at generating and organizing categories, achieving near-perfect performance on parent-child category linking and a precision of 77.3% on producing coherent groups of sibling categories.However, their performance on assigning studies to relevant categories (61.5% F1) leaves room for improvement.</p>
<p>We study the potential of using CHIME to train "corrector" agents which can provide feedback to our LLM-based pipeline to improve hierarchy quality.Our results show that finetuning a FLAN-T5based corrector and applying it to LLM-generated hierarchies improves study assignment by 12.6 F1 points.We release our dataset containing both LLM-generated and expert-corrected hierarchies, as well as our LLM-based hierarchy generation and correction pipelines, to encourage further research on better assistive tools for literature review.</p>
<p>In summary, our key contributions include: • We develop an LLM-based pipeline to organize a collection of papers on a research topic into a labeled, human-navigable concept hierarchy.• We release CHIME, a dataset of 2174 hierarchies constructed using our pipeline, including a "gold" subset of 320 hierarchies checked and corrected by human experts.• We train corrector models using CHIME to automatically fix errors in LLM-generated hierarchies, improving accuracy of study categorization by 12.6 F1 points.</p>
<p>Generating Preliminary Hierarchies using LLMs</p>
<p>The first phase of our dataset creation process focuses on using LLMs to generate preliminary hierarchies from a set of related studies, which can then be corrected by experts.We describe our process for collecting sets of related studies and our LLM-based hierarchy generation pipeline.</p>
<p>Sourcing Related Studies</p>
<p>We leverage the Cochrane Database of Systematic Reviews2 to obtain sets of related studies, since the systematic review writing process requires experts to extensively search for and curate studies relevant to review topics.We obtain all systematic reviews and the corresponding studies included in each review from the Cochrane website (Wallace et al., 2020).We then filter this set of systematic reviews to only retain those including at least 15 and no more than 50 corresponding studies.We discard reviews with very few studies since a hierarchical organization is unlikely to provide much utility, while reviews with more than 50 studies are discarded due to the inability of LLMs to effectively handle such long inputs (Liu et al., 2023)  (or sets), each including an average of 24.7 studies, which serve as input to our hierarchy generation pipeline.</p>
<p>Hierarchy Generation Pipeline</p>
<p>Prior work on using LLMs for complex tasks has shown that decomposing the task into a series of steps or sub-tasks often elicits more accurate responses (Kojima et al., 2022;Wei et al., 2022b;D'Arcy et al., 2024).Motivated by this, we decompose hierarchy generation from a set of related scientific studies into three sub-tasks: (i) compressing study findings into concise claims, (ii) initiating hierarchy generation by generating root categories, and (iii) completing hierarchy generation by producing remaining categories and organizing claims under them.Our hierarchy generation pipeline consists of a pre-generation module that tackles task (i) and a hierarchy proposal module that handles tasks (ii) and (iii) (see Figure 2)).Additionally, our pipeline can generate multiple (up to five) potential hierarchies per topic.We describe our pipeline module in further detail below and provide complete prompt details in Appendix A.</p>
<p>Pre-Generation Module</p>
<p>This module extracts relevant content from a set of studies to use as input for hierarchy proposal.Claim generation.We generate concise claim statements from a given scientific study to reduce the amount of information provided as input to subsequent LLM modules.Providing a study abstract as input, we prompt a LLM to generate claims de-scribing all findings discussed.We qualitatively examine the claim generation capabilities of two state-of-the-art LLMs: (i) GPT-3.5 (June 2023 version) and (ii) CLAUDE-2.Our assessment indicates that GPT-3.5 performs better in terms of clarity and conciseness; therefore we extract claim statements for all studies in our dataset using this model.Additionally, to assess whether generated claims contain hallucinated information, we run a fine-tuned DEBERTA-V3 NLI model (Laurer et al., 2024) on abstract-claim pairs.We observe that 98.1% of the generated claims are entailed by their corresponding study abstracts, indicating that claims are generally faithful to source abstracts. 3These sets of generated claims are provided as input to the hierarchy proposal module.</p>
<p>Frequent entity extraction.Based on preliminary exploration, we observe that simply prompting LLMs to generate hierarchies given a set of claims often produces hierarchy categories with low coverage over the claim set.Therefore, we extract frequently-occurring entities to provide as additional cues to bias category generation.We use SCISPACY (Neumann et al., 2019) to extract entities from all study abstracts, then aggregate and sort them by frequency.The 20 most frequent entities are used as additional keywords to bias generated categories towards having high coverage.</p>
<p>Hierarchy Proposal Module</p>
<p>The aim of this module is to generate final hierarchies in two steps within a single prompt: (i) generate possible categories that can form the root node of a hierarchy (i.e., categories that divide claims into various clusters), and then (ii) generate the complete hierarchy with claim organization.For instance, considering the example in Figure 1, step (i) would produce root categories "exercise modalities" and "cancer types" and step (ii) would produce all sub-categories (S1 − S5) and organize studies under them (e.g., assigning studies 1, 3, 5 under S1</p>
<p>Correcting Hierarchies via Human Feedback</p>
<p>The second phase of our dataset creation process involves correction of preliminary LLM-generated hierarchies via human feedback.Correcting these hierarchies is challenging because of two issues.</p>
<p>First, the volume of information present in generated hierarchies (links between categories, claimcategory links, etc.) makes correction very timeconsuming, especially in a single pass.Second, since categories and claims in a hierarchy are interlinked, corrections can have cascading effects (e.g., changing a category name can affect which claims should be categorized under it).These issues motivate us to decompose hierarchy correction into three sub-tasks, making the feedback process less tedious and time-consuming.Furthermore, each sub-task focuses on the correction of only one category of links to mitigate cascading effects.These three sub-tasks are: (i) assessing correctness of parent-child category links, (ii) assessing coherence of sibling category groups, and (iii) assessing claim categorization.</p>
<p>Assessing Parent-Child Category Links</p>
<p>In this sub-task, given all parent-child category links from a hierarchy (e.g., P 1 → S[1 − 5] in Figure 1), for each link, humans are prompted to determine whether the child is a valid sub-category of the parent.Annotators can label parent-child category links using one of the following labels: (i) parent and child categories have a hypernymhyponym relationship (e.g., exercise modalities → aerobic exercise), (ii) parent and child categories are not related by hypernymy but the child category provides a useful breakdown of the parent(e.g., aer-obic exercise → positive effects), and (iii) parent and child categories are unrelated (e.g., aerobic exercise → anaerobic exercise).Categories (i) and (ii) are positive labels indicating valid links, while category (iii) is a negative label capturing incorrect links in the existing hierarchy.</p>
<p>Assessing Coherence of Sibling Categories</p>
<p>For a hierarchical organization to be useful, in addition to validity of parent-child category links, all sibling categories (i.e., categories under the same parent, like S1 − S5 in Figure 1) should also be coherent.Therefore, in our second sub-task, given a parent and all its child categories, we ask annotators to determine whether these child categories form a coherent sibling group.Annotators can assign a positive or negative coherence label to each child category in the group.For example, given the parent category "type of cancer" and the set of child categories "liver cancer", "prostate cancer", "lung cancer", and "recurrence", the first three categories are assigned positive labels, while "recurrence" is assigned a negative label since it is not a type of cancer.All categories assigned a negative label capture incorrect groups in the existing hierarchy.</p>
<p>Assessing Claim Categorization</p>
<p>Unlike the previous sub-tasks which focus on assessing links between categories at all levels of the hierarchy, the final sub-task focuses on assessing the assignment of claims to various categories.Given a claim and all categories present in the hierarchy, for each claim-category pair, humans are prompted to assess whether the claim contains any information relevant to that category.The claimcategory pair is assigned a positive label if relevant information is present, and negative otherwise.For every category, we include the path from the root to provide additional context which might be needed to interpret it accurately (e.g., "positive findings" has a broader interpretation than "chemotherapy → positive findings").Instead of only assessing relevance of categories under which a claim has currently been categorized, this sub-task evaluates all claim-category pairs in order to catch recall errors, i.e., cases in which a claim could be assigned to an category but is not categorized there currently.filter out hierarchies which cover less than 30% of the claims associated with that topic.This leaves us with 320 hierarchies to collect corrections for.For the parent-child link assessment sub-task, this produces 1,635 links to be assessed.For sibling coherence, after removing all parent categories with only one child, we obtain 574 sibling groups to be assessed.Lastly, for claim categorization, the most intensive task, we end up with 50,723 claimcategory pairs to label.</p>
<p>Feedback</p>
<p>Annotator Background: We recruit a team of five experts with backgrounds in biology or medicine to conduct annotations.Two of these experts are authors on this paper, and the remaining three were recruited via Upwork. 4Every annotator is required to first complete a qualification test, which includes sample data from all three subtasks, and must achieve reasonable performance before they are asked to annotate data.</p>
<p>Annotation Pilots: Given the complexity and ambiguity of our tasks, we conduct several rounds of pilot annotation with iterative feedback before commencing full-scale annotation.This ensures that all annotators develop a deep understanding of the task and can achieve high agreement.After each pilot, we measure inter-annotator agreement on each sub-task.Due to the presence of unbalanced labels in tasks 1 and 2, we compute agreement using match rate; for task 3, we report Fleiss' kappa.At the end of all pilot rounds, we achieve high agreement on all sub-tasks, with match rates of 100% and 78% on tasks 1 and 2 respectively and Fleiss' kappa of 0.66 on task 3.</p>
<p>Assessment of Preliminary Hierarchies</p>
<p>An additional benefit of collecting corrections for preliminary hierarchies (as described above) is that this data allows us to quantify the quality of our 4 https://www.upwork.com/LLM-generated hierarchies and measure the performance of our hierarchy generation pipeline.</p>
<p>Parent-child link accuracy.Interestingly, we observe almost perfect performance on this sub-task, with only one out of 1635 parent-child links being labeled as incorrect where the pipeline put "Coffee consumption" under "Tea consumption and cancer risk".Of the remaining correct links, 75% are labeled as hypernym-hyponym links, and 25% as useful breakdowns of the parent category.This result demonstrates that LLMs are highly accurate at generating good sub-categories given a parent category, even when dealing with long inputs.</p>
<p>Sibling coherence performance.Next, we look into LLM performance on sibling coherence and observe that this is also fairly high, with 77% of sibling groups being labeled as coherent where "coherent" denotes a sibling group in which expert labels for all sibling categories are positive; otherwise, "zero."Among sibling groups labeled incoherent, we observe two common types of errors: (1) categories at different levels of granularity being grouped as siblings, and (2) one or more categories having subtly different focuses.For example, Fig. 1 demonstrates a type 1 error, where the sub-category "walking" is more specific and should be classified under "aerobic" but is instead listed as a sibling.</p>
<p>An example of a type 2 error is the parent category "dietary interventions" with child categories "low calorie diets", "high/low carbohydrate diets", and "prepared meal plans".Here, though all child categories are dietary interventions, the first two have an explicit additional focus on nutritional value which "prepared meal plans" lacks, making them incoherent as a sibling group.</p>
<p>Claim categorization performance.The design of our claim categorization sub-task prompts annotators to evaluate the relationship between a given claim and every category in the hierarchy.Hence, when assessing whether annotators agree with the LLM's categorization of a claim under a category, we need to aggregate over the labels assigned to all claim-category pairs from the root to the target category under consideration.Formally, for a claimcategory pair (cl i , ct j ), instead of only using label l ij = h((cl i , ct j )) from human feedback h, we must aggregate over labels assigned to all ancestors of ct j , i.e., L = [h((cl i , ct 1 )), ..., h((cl i , ct j ))],</p>
<p>where ct 1 is the root category and ct j is the target category.We do this aggregation using an AND operation l agg = l 1 ∧ l 2 ∧ ... ∧ l j .After computing these aggregate labels, we observe that our LLMbased pipeline has reasonable precision (0.71), but much lower recall (0.53) on claim categorization.A low recall rate on this sub-task is problematic because, while it is easy for human annotators to correct precision errors (remove claims wrongly assigned to various categories), it is much harder to correct recall errors (identify which claims were missed under a given category), which necessitates a thorough examination of all studies.</p>
<p>Characterizing Hierarchy Complexity</p>
<p>Our dataset creation process produces 2,174 hierarchies on 472 research topics, with 320 hierarchies (for 100 topics) corrected by domain experts.We briefly characterize the complexity of all generated hierarchies, focusing on two aspects: (i) structural complexity, and (ii) semantic complexity.</p>
<p>Structural Complexity</p>
<p>Hierarchy depth: All generated hierarchies are multi-level, with a mean hierarchy depth of 2.5, and maximum depth of 5.</p>
<p>Node arity: On average, every parent has a node arity of 2.4 (i.e., has 2.4 child categories).However, node arity can grow as large as 10 for certain parent categories.</p>
<p>Claim coverage: Another crucial property of generated hierarchies is their coverage of claims since hierarchies containing fewer claims are easier to generate but less useful.We observe that given a set of claims, a typical hierarchy incorporates 12.3 claims on average.Additionally, very few claims from a set remain uncategorized, i.e., not covered by any generated hierarchy (2.6 on average).These characteristics indicate that our LLMgenerated hierarchies have interesting structural properties.</p>
<p>Semantic Complexity</p>
<p>Category diversity: Our dataset contains 4.6 hierarchies per research topic.We manually inspect a small sample of hierarchies for 10 research topics, and find that none of the hierarchies generated for a single topic contain any repeating categories.This signals that the multiple hierarchies we generate per topic represent semantically diverse ways of grouping/slicing the same set of claims.Adherence to PICO framework: Systematic reviews in biomedicine typically use the PICO (pop-  ulation, intervention, comparator, outcome) framework (Richardson et al., 1995) to categorize studies.</p>
<p>To understand how much our generated hierarchies adhere to this framework, we again inspect hierarchies for 10 research topics and label whether the root category focuses on a PICO element.We observe that 34 out of 46 hierarchies have a PICOfocused root category, making them directly useful for systematic review.Interestingly, the remaining hierarchies still focus on useful categories such as continuing patient education, study limitations, cost analyses etc.Thus, besides surfacing categorizations expected by the systematic review process, using LLMs can help discover additional interesting categorizations.</p>
<p>Automating Hierarchy Correction</p>
<p>As mentioned in §4, we hire five domain experts to correct hierarchies for 100 research topics.However, the correction process, despite our best efforts at task simplification and decomposition, is still time-consuming and requires domain expertise.Therefore, we investigate whether we can use our corrected hierarchy data to automate some correction sub-tasks.In particular, we focus on automating sibling coherence and claim categorization correction since Table 1 indicates that LLMs already achieve near-perfect performance on producing relevant child categories for a parent.</p>
<p>Experimental Setup</p>
<p>We briefly discuss the experimental setup we use to evaluate whether model performance on sibling coherence and claim categorization correction can be improved using our collected feedback data.</p>
<p>Dataset Split</p>
<p>To better assess generalizability, we carefully construct two test sets, an in-domain (ID) and an out-ofdomain (OOD) subset instead of randomly splitting our final dataset of 100 research topics.To develop our OOD test set, we first embed all 100 research topics by running SPECTER2, a scientific paper embedding model using citation graph (Singh et al., 2022), on the title and abstract of the Cochrane systematic review associated with each topic.Then, we run hierarchical clustering on the embeddings and choose one isolated cluster (n = 12 reviews) to be our OOD test set.Our manual inspection reveals that all studies in this cluster are about fertility and pregnancy.After creating our OOD test set, we then randomly sub-sample our ID test set (n = 18 reviews) from remaining research topics.This leaves us with 70 topics, which we split into training and validation sets.Detailed statistics for our dataset splits, including number of instances for each correction sub-task, are provided in Table 2.</p>
<p>Models</p>
<p>We evaluate two classes of methods for correction:</p>
<p>• Finetuned LMs: To assess whether correction abilities of smaller LMs can be improved by finetuning on our collected feedback data, we experiment with Flan-T5 (Chung et al., 2022), which has proven to be effective on many benchmarks.• Zero-Shot CoT: To explore whether using chain-of-thought (CoT) prompting (Wei et al., 2022a) improves the ability of LLMs to do correction zero-shot without using our feedback data, we test OpenAI GPT-3.5 Turbo (gpt-3.5-turbo-0613)and GPT-4 Turbo (gpt-4-1106-preview).Additional modeling details including CoT prompts are provided in Appendix B.</p>
<p>Correcting Sibling Coherence</p>
<p>Table 3 presents the performance of all models on the task of identifying sibling groups that are incoherent.Finetuning models on this task is challenging due to the small size of the training set (n = 298) and imbalanced labels.Despite upsampling and model selection based on precision, finetuned Flan-T5 models do not perform well on this task (best F1-score of 33.3%).Additionally LLMs also do not perform well despite the use of chain-of-thought prompting to handle the complex reasoning required for this task.At 51.5% F1, LLMs outperform finetuned models; however, their precision (46.7% for GPT-4-Turbo) is still not good enough to detect incoherent sibling groups confidently.These results indicate that this correction sub-task is extremely difficult to automate and will likely continue to require expert intervention.</p>
<p>Correcting Claim Categorization</p>
<p>Table 4 shows the performance of all models on the task of correcting assignment of claims to categories in the hierarchy.Following the strategy described in §3.5, given a claim, we first use our models to generate predictions for every claim-category pair (all category nodes) and then obtain the final label for each category by applying an AND operation over all predictions from the root category to that category.Our results show that this task is easier to automate-fine-tuning Flan-T5 on our collected training dataset leads to better scores on all metrics compared to our LLM pipeline.Crucially, recall which is much more time-consuming for humans to fix, improves by 15.9 points using Flan-T5-large indicating that automating this step can provide additional efficiency gains during correction.LLMs perform well too, with GPT-4-Turbo achieving the best recall rate among all models, but its lower precision score makes the predictions less reliable overall.</p>
<p>Interestingly, we notice that all models perform better on the OOD test for both correction tasks, indicating that the OOD test set likely contains instances that are less challenging than the ID set.</p>
<p>Correcting Claim Categorization for Remaining Hierarchies</p>
<p>Comparing the claim categorization predictions of Flan-T5-large on our test set with our LLM-based hierarchy generation pipeline reveals that it flips labels in 24.7% cases, of which 63.5% changes are correct.This indicates that a FLAN-T5-large corrector can potentially improve claim categorization of LLM-generated hierarchies.Therefore, we apply this corrector to the remaining 372 LLMgenerated hierarchies that we do not have expert corrections for to improve claim assignment for those.Our final curated dataset CHIME contains hierarchies for 472 research topics, of which hierarchies for 100 topics have been corrected by experts on both category linking and claim categorization, while hierarchies for the remaining 372 have had claim assignments corrected automatically.</p>
<p>6 Related Work techniques for end-to-end review generation or to tackle specific aspects of the problem (see Altmami and Menai (2022) for a detailed survey).Some studies have focused on generating "citation sentences" discussing relationships between related papers, which can be included in a literature review (Xing et al., 2020;Luu et al., 2021;Ge et al., 2021;Wu et al., 2021).Other work has focused on the task of generating related work sections for a scientific paper (Hoang and Kan, 2010; Hu and Wan, 2014;Li et al., 2022;Wang et al., 2022), which while similar in nature to literature review, has a narrower scope and expects more concise generation outputs.Finally, motivated by the everimproving capabilities of generative models, some prior work has attempted to automate end-to-end review generation treating it as multi-document summarization, with limited success (Mohammad et al., 2009;Jha et al., 2015;Wallace et al., 2020;DeYoung et al., 2021;Liu et al., 2022;Zhu et al., 2023).Of these, Zhu et al. (2023) generates intermediate hierarchical outlines to scaffold literature review generation, but unlike our work, they do not produce multiple organizations for the same set of related studies.Additionally, we focus solely on the problem of organizing related studies for literature review, leaving review generation and writing assistance to future work.</p>
<p>LLMs for Organization</p>
<p>Organizing document collections is an extensivelystudied problem in NLP, with several classes of approaches such as clustering and topic model-ing (Dumais et al., 1988) addressing this goal.Despite their utility, conventional clustering and topic modeling approaches are not easily interpretable (Chang et al., 2009), requiring manual effort which introduces subjectivity and affects their reliability (Baden et al., 2022).Recent work has started exploring whether using LLMs for clustering (Viswanathan et al., 2023;Zhang et al., 2023;Wang et al., 2023) and topic modeling (Pham et al., 2023) can alleviate some of these issues, with promising results.This motivates us to experiment with LLMs for generating hierarchical organizations of scientific studies.Interestingly, TopicGPT (Pham et al., 2023) also attempts to perform hierarchical topic modeling, but is limited to producing two-level hierarchies unlike our approach which generates hierarchies of arbitrary depth.</p>
<p>Conclusion</p>
<p>Our work explored the utility of LLMs for producing hierarchical organizations of scientific studies, with the goal of assisting researchers in performing literature review.We collected CHIME, an expertcurated dataset for hierarchy generation focused on biomedicine, using a human-in-the-loop process in which a naive LLM-based pipeline generates preliminary hierarchies which are corrected by experts.</p>
<p>To make hierarchy correction less tedious and timeconsuming, we decomposed it into a three-step process in which experts assessed the correctness of links between categories as well as assignment of studies to categories.CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics.Quantifying LLM performance using our collected data revealed that LLMs are quite good at generating and linking categories, but needed further improvement on study assignment.We trained a corrector model with our feedback data which improved study assignment further by 12.6% F1 points.We hope that releasing CHIME and our hierarchy generation and correction models will motivate further research on developing better assistive tools for literature review.</p>
<p>Limitations</p>
<p>Single-domain focus.Given our primary focus on biomedicine, it is possible that our hierarchy generation and correction methods do not generalize well to other scientific domains.Further investigation of generalization is out of scope for this work but a promising area for future research.</p>
<p>Deployment difficulties.Powerful LLMs like CLAUDE-2 have long inference times -in some cases, the entire hierarchy generation process can take up to one minute to complete.This makes it extremely challenging to deploy our hierarchy construction pipeline as a real-time application.However, it is possible to conduct controlled lab studies to evaluate the utility of our pipeline as a literature review assistant, which opens up another line of investigation for future work.</p>
<p>Reliance on curated sets of related studies.Our current hierarchical organization pipeline relies on the assumption that all provided studies are relevant to the research topic being reviewed.However, in a realistic literature review setting, researchers often retrieve a set of studies from search engines, which may or may not be relevant to the topic of interest, and are interested in organizing the retrieved results.</p>
<p>In a preliminary qualitative analysis in Appendix Section E, we show that our system can handle some noise in retrieved studies, though we defer a detailed robustness evaluation to future work.</p>
<p>A Prompts for Hierarchy Generation Pipeline</p>
<p>We present prompts for the hierarchy generation pipeline in Fig. 3 and Fig. 4.</p>
<p>B Model Training Details</p>
<p>Flan-T5 fintuning.We fine-tuned the flan-t5-base and flan-t5-large models using the Hugggingface library (Wolf et al., 2019) with NVIDIA RTX A6000 for both task 1 and task 3.For task 1, the learning rate is set to 1e-3 and the batch size is 16.We train the model for up to five epochs.For task 3, the learning rate is 3e-4 with batch size 16, and the models are trained up to two epochs.Each epoch takes less than 15 minutes for both model sizes.The numbers reported for each Flan-T5 model come from a single model checkpoint.</p>
<p>GPT-3.5 Turbo and GPT-4 Turbo We perform zero-shot CoT prompting for corrector models on tasks 1 and 3 with prompts in Fig. 5 and Fig. 6.</p>
<p>C Model Selection for Hierarchy Proposal Module</p>
<p>We conducted a qualitative evaluation of hierarchies generated by GPT-3.5-Turbo,GPT-4, and CLAUDE-2 for 10 sampled research topics.Results showed that GPT-3.5-Turbodoes poorly at following instructions and only generates well-formed hierarchies 30% of the time, while GPT-4 produces valid hierarchies but generates shallow ones with a depth of 1 60% of the time.In comparison, Claude-2 produces hierarchies with a higher depth (&gt;1) 90% of the time.</p>
<p>D Qualitative Analysis on Generated Claims</p>
<p>To better establish the accuracy of our NLI-based verification process, we have conducted an additional qualitative assessment of 100 abstract-claim pairs.We examined 50 pairs that the NLI model marked as "entailed" and 50 non-entailed pairs.Results show that the precision of the NLI model is very high, with 47 out of 50 entailed claims being correct, without hallucinations.Interestingly, we find that 37/50 non-entailed pairs are false negatives, indicating that in many cases, the generated claim is correct even though the NLI model predicts non-entailment.This human evaluation further validates that our claim generation process is high quality.</p>
<p>E Qualitative Analysis on Retrieval Quality</p>
<p>We conducted a brief experiment on 10 samples (sets of related studies present in our dataset) by injecting five irrelevant claims from other study sets per sample.We observed that during hierarchy generation, CLAUDE-2 was able to ignore irrelevant claims and generate hierarchies similar to the ones it originally produced (in the non-noisy setting).CLAUDE-2 can also differentiate between relevant and irrelevant claims and does not assign noisy claims to any categories in the hierarchy.</p>
<p>Title: {title}</p>
<p>Abstract: {abstract}</p>
<p>Task: Conclude new findings and null findings from the abstract in one sentence in the atomic format.Do not separate new findings and null findings.The finding must be relevant to the title.Do not include any other information.</p>
<p>Definition:</p>
<p>A scientific claim is an atomic verifiable statement expressing a finding about one aspect of a scientific entity or process, which can be verified from a single source.(Claim 2,4,5,6,7,8,9,10,11) 1: Usage of other drugs (Claim 4, 5, 6, 9, 10, 11) 2: Dosing comparisons (Claim 7, 8) 2.1: Dosing of Remdesivir (Claim 7) Aspect 3: Treatment of COVID-19 patients (Claim 1,13,14,15,16,17,18,19,20,21) 1: Treatment procedures for other diseases (Claim 13,14,16,17,18,19,20,21) 2: Treatment timeframe comparisons.(Claim 1, 15) ** Instruction ** In this task, you will be annotating the relationship among a set of sibling categories.You will assess whether categories logically belong together within their shared parent category, a concept referred to as 'coherence'.</p>
<p>Your task is to label whether ALL sibling categories are coherent with each other.If all sibling categories fit well and logically belongs to the broader group, label it 'These sibling categories are coherent' to signify its coherence.Make sure silbings are at the same level of granularity for coherence assessment.** Instruction ** In this task, your role as an annotator is to assess whether a belongs to a provided category.</p>
<p>Your responsibility is to assign a binary label for each category-claim pairing: 1. "The claim belongs to the category" -Choose this if any part or aspect of the claim is relevant to the category, even if the connection is broad or indirect.This includes claims that are negations or opposites of the category.</p>
<p>See the following examples:</p>
<p>The claim "Assisted hatching through partial zona dissection does not improve pregnancy and embryo implantation rates in unselected patients undergoing IVF or ICSI" belongs to "Impact on specific patient groups" category because patient groups can be applied to not only patient demographics but also patients with the same disease/symptom.The claim "Sumatriptan is effective in reducing productivity loss due to migraine, with significant improvements in productivity loss and return to normal work performance compared to placebo."belongs to "Headache relief" because headache is one of the symptoms of migraine even though it is not explicitly mentioned in the claim.</p>
<p>Figure 2 :
2
Figure 2: LLM-based pipeline for preliminary hierarchy generation given a set of related studies on a topic.</p>
<p>Figure 3 :
3
Figure 3: Claim generation prompt for GPT-3.5 Turbo.</p>
<p>Figure 4 :
4
Figure 4: Hierarchy proposal module prompt for Claude-2.</p>
<p>If any category doesn't seem to belong logically or doesn't fit well within the group, label it 'These sibling categories are NOT coherent' to indicate non-coherence.Your decisions should be based solely on the level of coherence -how well these categories fit together under their shared parent category and not on any other factors or personal preferences.<strong>Remember</strong> 1.You should start with step-by-step reasoning and generate the answer at the end in the given format.2.You should only reply with the answer in the format of [These sibling categories are coherent] or [These sibling categories are NOT coherent].3.You will be given a parent category and a set of sibling categories.You should assess each sibling category independently.Again, follow the format below to reply: Step-by-step reasoning: [Your reasoning] Answer: [These sibling categories are coherent] or [These sibling categories are NOT coherent] ** Question ** Parent category: {parent_category} Sibling categories: {sibling_categories}</p>
<p>Figure 5 :
5
Figure 5: Prompt for task 1 sibling coherence for both GPT-3.5 Turbo and GPT-4 Turbo.</p>
<ol>
<li>"The claim does NOT belong to the category" -Choose this if there is no meaningful connection between the claim and the category.<strong>Remember</strong> 1.Only reply with the answer in the format of [The claim belongs to the category] or [The claim does NOT belong to the category].2. Do not reply with any other format.3. Start with step-by-step reasoning and generate the answer at the end in the given format.<strong>Claim</strong> {claim} <strong>Category</strong> {category} Again, follow the format below to reply: Step-by-step reasoning: [Your reasoning] Answer: [The claim belongs to the category] or [The claim does NOT belong to the category]</li>
</ol>
<p>Figure 6 :
6
Figure 6: Prompt for task 3 claim assignment for both GPT-3.5 Turbo and GPT-4 Turbo.</p>
<p>Table 2
2: Dataset statistics for three correction sub-tasks:parent-child category links (Task 1), sibling categorycoherence (Task 2), and claim categorization (Task 3).</p>
<p>Table 3 :
3
Performance of all models on assessing sibling coherence.
6.1 Literature Review SupportPrior work on developing literature review supporttools has largely focused on using summarization</p>
<p>Table 4 :
4
Performance of all models on correcting claim categorization.</p>
<p>Level Aspect Generation:<strong> Utilize the entities extracted from the study abstracts for identifying up to 5 top-level aspects from the clinical study claims.You should list these aspects in a bulleted list format without incorporating any extraneous information.Cite the entities in that support the aspects.This will be the [Response 1] section.2.</strong>Hierarchical Faceted Category Generation:<strong> For every top-level aspect in [Response 1], proceed to generate hierarchical faceted categories that closely align with the above study claims.The granularity of these categories must be similar to their corresponding parent categories and the siblings categories.Avoid including unrelated information.Cite the claims that support your categories.This will make up the [Response 2] section of your output.Precision is vital in this process; strive to avoid vague or imprecise extractions.2. Include only relevant data and exclude any information not pertinent to the task.3. Strictly adhere to the output format.The claims are cited in the format "(Claim 0, 2, 3, 12)" for each category and aspect.4. The output should be in the form of a nested list using numbers.
</strong>Review Title<strong>{systematic_review_title}Frequent entities from study abstracts:{freq_entities}</strong>Study Claim List<strong>{claim_list}</strong>Instruction:<strong>Your task process a review title involving relevant clinical studies as per the following requirements:1. </strong>Top-Here is an example:If given the review title "The efficacy of Remdesivir in treating COVID-19 patients: A review," your task output might looklike this:Frequent entities from study abstracts:Efficacy, Remdesivir, treatment, COVID-19 patients<strong>Output Format</strong>[Response 1]:Aspect 1: Efficacy of treatment (Efficacy)Aspect 2: Application of Remdesivir (Remdesivir)Aspect 3: Treatment of COVID-19 patients (treatment, COVID-19 patients)[Response 2]:Aspect 1: Efficacy of treatment (Claim 0, 2, 3, 12)1: Efficiency of alternative treatments (Claim 0, 2, 3, 12)1.1: Efficacy of Remdesivir (Claim 0, 12)1.2: Efficacy of other drugs (Claim 3)2: Side-effects comparison (Claim 2)Aspect 2: Application of Remdesivir
<strong>Remember:</strong> 1.</p>
<p>https://www.cochranelibrary.com/cdsr/reviews
We further conduct a qualitative evaluation to ensure factuality of generated claims in Appendix Section D.
AcknowledgementsWe would like to thank the reviewers, Joseph Chee Chang, and the rest of the Semantic Scholar team at AI2 for their valuable feedback and comments.We also want to thank the Upworkers who participated in our formative studies and annotation process.
Automatic summarization of scientific articles: A survey. Journal of King Saud University-Computer and Information Sciences. Nouf Ibrahim Altmami and Mohamed El Bachir Menai3442022</p>
<p>Three gaps in computational text analysis methods for social sciences: A research agenda. Christian Baden, Christian Pipal, Martijn Schoonvelde, Ac G Mariken, Van Der Velden, Communication Methods and Measures. 1612022</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the prospero registry. Rohit Borah, Andrew W Brown, Patrice L Capers, Kathryn A Kaiser, BMJ open. 722017</p>
<p>Reading tea leaves: How humans interpret topic models. Advances in neural information processing systems. Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, David Blei, 200922</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, S Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Dasha Chowdhery, Sharan Valter, Gaurav Narang, Adams Wei Mishra, Vincent Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Yu, ArXiv, abs/2210.11416Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Slav Petrov</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>MSˆ2: Multidocument summarization of medical studies. Jay Deyoung, Iz Beltagy, Madeleine Van Zuylen, Bailey Kuehl, Lucy Lu, Wang , 10.18653/v1/2021.emnlp-main.594Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Using latent semantic analysis to improve access to textual information. Susan T Dumais, George W Furnas, Thomas K Landauer, Scott Deerwester, Richard Harshman, Proceedings of the SIGCHI conference on Human factors in computing systems. the SIGCHI conference on Human factors in computing systems1988</p>
<p>BACO: A background knowledge-and content-based framework for citing sentence generation. Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner, 10.18653/v1/2021.acl-long.116Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Towards automated related work summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010 Organizing Committee. Beijing, China2010Coling 2010: Posters</p>
<p>Automatic generation of related work sections in scientific papers: An optimization approach. Yue Hu, Xiaojun Wan, 10.3115/v1/D14-1170Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, Qatar2014Association for Computational Linguistics</p>
<p>Content models for survey generation: A factoid-based evaluation. Rahul Jha, Catherine Finegan-Dollak, Ben King, 10.3115/v1/P15-1043Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, China20151Reed Coke, and Dragomir Radev. Association for Computational Linguistics</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Less annotating, more classifying: Addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli. Moritz Laurer, Andreu Wouter Van Atteveldt, Kasper Casas, Welbers, 10.1017/pan.2023.20Political Analysis. 3212024</p>
<p>Generating a related work section for scientific papers: an optimized approach with adopting problem and method information. Pengcheng Li, Wei Lu, Qikai Cheng, Scientometrics. 12782022</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, Transactions of the Association for Computational Linguistics. 122023</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen, International Joint Conference on Artificial Intelligence. 2022</p>
<p>Explaining relationships between scientific documents. Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, Noah A Smith, 10.18653/v1/2021.acl-long.166Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Using citations to generate surveys of scientific paradigms. Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Dragomir Vahed Qazvinian, David Radev, Zajic, Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter. Human Language Technologies: The 2009 Annual Conference of the North American ChapterBoulder, ColoradoAssociation for Computational Linguistics2009</p>
<p>ScispaCy: Fast and robust models for biomedical natural language processing. Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar, 10.18653/v1/W19-5034Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared TaskFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Topicgpt: A prompt-based topic modeling framework. Minh Chau, Alexander Miserlis Pham, Simeng Hoyle, Mohit Sun, Iyyer, ArXiv, abs/2311.014492023</p>
<p>The well-built clinical question: a key to evidence-based decisions. Mark C Scott Richardson, Jim Wilson, Robert S Nishikawa, Hayward, ACP journal club. 12331995</p>
<p>How quickly do systematic reviews go out of date? a survival analysis. Margaret Kaveh G Shojania, Mohammed T Sampson, Jun Ansari, Steve Ji, David Doucette, Moher, Annals of internal medicine. 14742007</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Large language models enable few-shot clustering. Vijay Viswanathan, Kiril Gashteovski, Carolin ( Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig, Transactions of the Association for Computational Linguistics. 122023</p>
<p>Generating (factual?) narrative summaries of rcts: Experiments with neural multi-document summarization. Byron C Wallace, Sayantani Saha, Frank Soboczenski, Iain James Marshall, Annual Symposium proceedings. AMIA Symposium. 20202021</p>
<p>Multi-document scientific summarization from a knowledge graph-centric view. Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang, arXiv:2209.043192022arXiv preprint</p>
<p>Goal-driven explainable clustering via language descriptions. Zihan Wang, Jingbo Shang, Ruiqi Zhong, ArXiv, abs/2305.137492023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, ArXiv, abs/2201.119032022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew, ArXiv, abs/1910.037712019</p>
<p>Towards generating citation sentences for multiple references with intent control. Jia-Yan Wu, Alexander Te-Wei Shieh, Shih-Ju Hsu, Yun-Nung Chen, arXiv:2112.013322021arXiv preprint</p>
<p>Automatic generation of citation texts in scholarly papers: A pilot study. Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, 10.18653/v1/2020.acl-main.550Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Appraising the potential uses and harms of LLMs for medical systematic reviews. Hye Yun, Iain Marshall, Thomas Trikalinos, Byron Wallace, 10.18653/v1/2023.emnlp-main.626Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Clusterllm: Large language models as a guide for text clustering. Yuwei Zhang, Zihan Wang, Jingbo Shang, ArXiv, abs/2305.148712023</p>
<p>Hierarchical catalogue generation for literature review: A benchmark. Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin, 10.18653/v1/2023.findings-emnlp.453Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>            </div>
        </div>

    </div>
</body>
</html>
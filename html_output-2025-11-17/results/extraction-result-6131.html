<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6131 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6131</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6131</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-9eadfe920cae4a451af437752de83075ec528288</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9eadfe920cae4a451af437752de83075ec528288" target="_blank">Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations, and LLM evaluators rate each candidate system inconsistently and are dimension-dependent.</p>
                <p><strong>Paper Abstract:</strong> With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6131.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6131.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reason-then-Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based Likert-scale evaluation method where the LLM is asked to produce a short rationale (reason) and then give a final 1–5 score on a specified dimension (e.g., relevance). Designed to mimic chain-of-thought style evaluation while yielding a numeric rating aligned with human Likert scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt the LLM to generate a one-sentence reason followed by a 1–5 Likert score for each summary on a specified evaluation dimension (coherence/consistency/fluency/relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-dimension Likert score (1–5) with an accompanying textual reason; later compared to human average scores using correlations and preference counts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) primarily; also evaluated with GPT-4 (gpt-4-0314).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Abstractive summarization (NLP) — used as a proxy for LLM-based evaluation of generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory per se; RTS is a method for eliciting evaluative judgments (reason + score) from an LLM about generated summaries to approximate human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>RTS (ChatGPT) achieved stronger correlations with human scores than many automatic metrics (outperforming ROUGE/BERTScore in several dimensions); on the full 66-pair set RTS had average #CP ≈ 58.5/66 (≈88.6%); showed higher correlation particularly on consistency and relevance (see Table 4). However RTS often produced dimension-irrelevant or unfaithful reasoning and tended to give more conservative (lower) absolute scores than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on the Summeval benchmark (Fabbri et al., 2021) of CNN/DM summaries (1200 summaries total, 12 systems × 100 each).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>RTS scores correlated reasonably well with expert human average scores (Spearman/Pearson/Kendall reported); but per-system correlations varied widely (candidate-dependence) and RTS became less aligned with humans for higher-quality systems (negative meta-correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RTS frequently generates incorrect or dimension-irrelevant reasoning (unfaithful chain-of-thought), is candidate- and dimension-dependent, and exhibits negative meta-correlation (worse alignment on high-quality systems). Absolute RTS scores are systematically lower than human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6131.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiple-Choice Question scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A constrained prompting Likert-scale method where the LLM selects one of predefined answer choices (A–E) mapped to 1–5, with each choice described explicitly to constrain reasoning and reduce spurious chain-of-thought outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt LLM with a multiple-choice question that maps A–E to 1–5 with explicit textual descriptions for each choice, then record the selected score for each dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-dimension discrete choice (A–E corresponding to 1–5); compared against human averages using correlations and preference counts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301); also evaluated comparatively with GPT-4 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Abstractive summarization (NLP) evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Constrained LLM scoring approach intended to reduce hallucinated or irrelevant rationales by forcing selection among predefined reasons tied to scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT-MCQ attained strong correlations with humans in coherence and fluency dimensions; sometimes complementary to RTS (RTS better on consistency/relevance, MCQ better on coherence/fluency). MCQ yields higher absolute scores (more optimistic) than RTS and sometimes matches human average magnitudes better, but does not eliminate candidate- or dimension-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Summeval dataset (Fabbri et al., 2021) on CNN/DM; comparisons against ROUGE/BERTScore/BARTScore baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>MCQ correlated with human judgments across dimensions but still exhibited per-system variability; agreement between RTS and MCQ (R_i) was proposed as an indicator of reliability relative to human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>While more constrained, MCQ still shows dimension- and candidate-dependence and degrades on higher-quality systems (negative meta-correlation in some dimensions); absolute scoring bias (generally higher than RTS) and some dimensions (relevance) remain problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6131.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>H2H</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Head-to-Head comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pairwise ranking approach where the LLM is given two summaries for the same article and asked to choose which is better (or indicate a tie) for a specific evaluation dimension; each pair is evaluated twice with swapped positions to reduce ordering bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt LLM to compare Summary #1 vs Summary #2 on a dimension and choose A (summary1 better), B (summary2 better), or C (tie), aggregating pairwise preferences over articles and pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise preference agreement with average human preference; measured via number of correct preferences (#CP) relative to human comparisons, with aggregation across article instances and pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301) primarily; evaluated on a targeted challenge set due to API budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Abstractive summarization ranking/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLM-as-ranker method to emulate human pairwise comparative judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On an 11-pair challenge set of closely matched system pairs, LLM H2H performance dropped relative to Likert methods; ChatGPT-H2H had lower overall #CP on challenge pairs and struggled to differentiate candidates with close performance. On the full 66-pair set (where applicable), RTS often outperformed H2H in aggregate correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Challenge and full pair sets constructed from Summeval systems (66 possible pairs; 11 closest pairs used for H2H due to cost).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>H2H sometimes aligned with humans on clear performance gaps but failed more often on closely matched candidate pairs; LLM preferences did not consistently match aggregated human preferences on the challenge set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High evaluation cost for exhaustive H2H (order doubling), sensitivity to small performance gaps (poor discrimination for close pairs), candidate- and dimension-dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6131.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-corr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-correlation metric (M)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric introduced in this paper to quantify how an LLM evaluator's correlation with human judgments varies with the underlying quality of candidate systems, i.e., measures system-dependence and whether LLM evaluation degrades/improves on higher-quality systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute average human quality Q_i per candidate (mean human score across N summaries), compute per-candidate correlation P_i between LLM and human scores, then compute correlation M = ρ([Q_1...Q_k],[P_1...P_k]) using Spearman/Pearson/Kendall as desired.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Meta-correlation M; value near zero implies evaluator performance independent of candidate quality (stable); significant positive/negative M indicates dependency (e.g., negative M indicates worse alignment on higher-quality systems).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to ChatGPT and GPT-4 evaluations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Stability analysis of LLM evaluators for abstractive summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A quantitative test for evaluator stability: whether the evaluator's human-alignment score (correlation) systematically varies with the underlying quality of systems being evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT (RTS/MCQ) and neural baselines (BARTScore variants) displayed significant negative meta-correlations in several dimensions (especially consistency and fluency), indicating poorer human alignment on higher-quality systems; ROUGE metrics showed no significantly negative meta-correlation and sometimes positive meta-correlation (e.g., ROUGE-2 on coherence). GPT-4 showed improvements in consistency but still had negative meta-correlation in relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Computed over k=12 candidate systems in Summeval (each with N=100 summaries) using Spearman/Pearson/Kendall correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Meta-correlation directly measures relationship between per-system human quality and LLM-human correlation; observed negative meta-correlation demonstrates LLMs can be more aligned with humans for low-quality systems and less aligned for high-quality systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Meta-correlation estimate can be sample-limited (only 12 candidate systems here), sensitive to dataset composition, and does not explain causal factors for the dependency (e.g., hallucination reduction, informativeness tradeoffs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6131.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R_i</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RTS–MCQ agreement reliability indicator (R_i)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A per-candidate reliability indicator equal to the correlation between RTS scores and MCQ scores across the N summaries produced by a candidate; proposed as a cheap proxy to detect when LLM evaluations are likely reliable and when further human validation is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For candidate i, compute R_i = ρ([f_RTS(g_{i,1})...f_RTS(g_{i,N})],[f_MCQ(g_{i,1})...f_MCQ(g_{i,N})]) using a chosen correlation metric; interpret R_i relative to a tolerance r to decide whether to trust LLM evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>R_i correlation value; if R_i > r (tolerance threshold), treat the LLM method (RTS or MCQ) as likely reliable for that candidate/dimension; otherwise invoke human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Computed using ChatGPT RTS and MCQ outputs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Quality control for LLM-based evaluation of generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A pragmatic meta-check: high inter-method agreement between two LLM prompting styles indicates higher likelihood of alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>R_i correlated positively and significantly with actual LLM-human performance P_i in several dimensions: R_i had significant positive correlation with P_i^RTS on consistency and fluency, and with P_i^MCQ on coherence and consistency (see Table 7), supporting R_i as a useful proxy in those dimensions. R_i had no significant predictive power for relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Computed on Summeval systems (N=100 summaries per candidate) using Spearman/Pearson/Kendall correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>R_i is designed to predict whether the LLM's scores will align with human scores; empirical evidence in the paper shows R_i is a useful indicator in some dimensions but fails for relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dimension-dependent effectiveness (doesn't work for relevance), threshold r may be dataset-dependent, and agreement between two flawed LLM methods does not guarantee correctness (both could be systematically biased).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6131.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eval-Dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation dimensions: Coherence, Consistency, Fluency, Relevance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four human-centric evaluation criteria reused from Fabbri et al. (2021) to judge summary quality: coherence (global flow), consistency (factual alignment), fluency (sentence-level quality), and relevance (selection of important content).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Summeval: Re-evaluating summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Each dimension is explicitly defined to annotators and LLM prompts; LLM scores are elicited per-dimension (RTS/MCQ/H2H) and then correlated with human expert annotations on those dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Dimension-specific Likert ratings (1–5) and pairwise preferences; numeric comparisons via Pearson/Spearman/Kendall correlation and #CP for pairwise preference agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used with ChatGPT and GPT-4 evaluators in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Summarization evaluation in NLP (dimensions reflect qualities humans judge in generated summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Operationalized definitions of the four dimensions are embedded into prompts so the LLMs evaluate along the same axes as human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLM evaluators show varying performance across these dimensions: ChatGPT RTS/MCQ had relatively strong correlations on fluency and relevance, mixed on coherence and consistency; GPT-4 improved consistency detection (less hallucination) but had negative meta-correlation in relevance. Overall, LLMs are dimension-dependent and not uniformly reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Definitions and annotations come from Summeval (Fabbri et al., 2021) on CNN/DM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human expert annotations (3 annotators per summary) served as ground truth; inter-annotator agreement (kappa=0.713) supports using averaged human scores for correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some LLMs produce reasoning unrelated to the evaluated dimension; LLMs' performance varies by dimension and candidate, so dimensions cannot be assumed equally reliable for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6131.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines & Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic metrics and dataset baselines (ROUGE, BERTScore, BARTScore, BARTScore-CNN/PARA, Summeval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automatic summarization metrics and the human-annotated Summeval benchmark used to compare and contextualize LLM evaluators' performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute standard automatic metric scores per summary (ROUGE-1/2/L F1, BERTScore, BARTScore variants) and compare to human average scores via correlation metrics; also use pairwise preferences (#CP) for system comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ROUGE (lexical overlap), BERTScore (semantic token similarity via BERT embeddings), BARTScore (generation probability under BART), and fine-tuned BART variants; correlations (#CP, Pearson, Spearman, Kendall) to human scores used as evaluation criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Not LLM-generated theories; these are baseline automatic metrics. LLMs evaluated against these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Automatic evaluation of abstractive summarization (NLP).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Baselines represent established automated ways to score summaries; used as points of comparison for LLM-based evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT RTS/MCQ outperformed many traditional automatic metrics (ROUGE, BERTScore, some BARTScore setups) in overall human correlation, though specialized BARTScore-CNN variants (fine-tuned on domain) were competitive, especially for coherence. ROUGE showed little negative meta-correlation and in one case (ROUGE-2) positive meta-correlation on coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Summeval (Fabbri et al., 2021) on CNN/DM; baselines reported from Wang et al. (2023a) for consistency with prior numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Baselines generally had lower alignment with human expert averages compared to ChatGPT and GPT-4 in many dimensions, but some domain-fine-tuned metrics matched or exceeded LLM alignment in certain dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ROUGE and other lexical metrics are insufficient for abstractive summarization due to variability of valid summaries; neural metrics (BERTScore/BARTScore) can be domain-biased and exhibit negative meta-correlation; none are perfect replacements for human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6131.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Measurements</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation measurements: #CP, Pearson, Spearman, Kendall</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of quantitative measurements used to compare LLM evaluators to human judgments: number of correct preferences (#CP) for pairwise comparisons and correlation coefficients (Pearson, Spearman, Kendall's Tau) for score-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use #CP to judge pairwise preference agreement (LLM vs human aggregated preferences); use Pearson for linear correlation, Spearman for rank-based monotonic correlation, and Kendall's Tau for pairwise concordance between LLM scores and human scores across summaries or per-system.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>#CP (count of matched pairwise preferences), Pearson r, Spearman ρ, Kendall's τ (tau-b) with significance testing (p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to ChatGPT and GPT-4 evaluation outputs in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Statistical evaluation of evaluator alignment in NLP summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Measurement toolkit for quantifying agreement between LLM evaluators and human expert judgments at both pairwise and score levels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported that ChatGPT-RTS and MCQ had higher Spearman/Pearson/Kendall correlations with human scores than many baselines (up to ~0.2 gains in some dimensions); #CP on full set: RTS ≈ 58.5/66 on average; correlations varied widely per system and dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied on Summeval (1200 summaries) and on per-system splits (100 summaries per system) to compute per-candidate correlations and meta-analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>These metrics quantify the degree to which LLM outputs reproduce human rankings/scores; used to demonstrate both strengths and failings of LLM evaluators relative to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Correlation metrics can mask per-system variance; #CP is sensitive to closeness of system performance and dataset construction; significance testing and sample sizes affect interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6131.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6131.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-4 evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned LLMs (ChatGPT gpt-3.5-turbo-0301 and GPT-4 gpt-4-0314) used as zero-shot evaluators via natural-language prompts. Experiments evaluate their reliability and limitations as automatic summarization evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Zero-shot prompting with carefully designed instructions for RTS, MCQ, and H2H; temperature set to 0 and dialogue reset per instance. Comparisons with baselines using correlation and #CP metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human alignment measured via Pearson/Spearman/Kendall correlations and #CP; stability measured via per-candidate correlation spread and meta-correlation M; internal reliability via RTS–MCQ agreement R_i.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301 snapshot) and GPT-4 (gpt-4-0314 snapshot).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Evaluation of generated text (abstractive summarization) and the general capability of LLMs as evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>LLMs are prompted to judge summaries across human-defined dimensions, serving as automatic evaluators intended to approximate human expert judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ChatGPT (RTS/MCQ) outperformed many traditional automatic metrics in correlation with human scores but was candidate- and dimension-dependent, struggled on closely matched system comparisons, and showed negative meta-correlation (worse alignment on higher-quality candidates). GPT-4 improved correlation in many dimensions and reduced hallucination-related errors (better consistency detection) but still exhibited candidate- and dimension-dependence and some negative meta-correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Summeval dataset (Fabbri et al., 2021) on CNN/DM; also compared to Llama 2 variants in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>LLMs correlated with human expert averages on aggregate but had inconsistent per-system behavior; GPT-4 achieved higher overall correlations than ChatGPT but did not eliminate limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model snapshots may not remain stable (OpenAI snapshot changes), cost and API limits constrain exhaustive H2H evaluation, LLMs can produce unfaithful reasoning affecting scoring, and LLM evaluation reliability degrades with higher-quality summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Summeval: Re-evaluating summarization evaluation <em>(Rating: 2)</em></li>
                <li>ROUGE: A package for automatic evaluation of summaries <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating text generation with BERT <em>(Rating: 2)</em></li>
                <li>BartScore: Evaluating generated text as text generation <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>ChatGPT as a factual inconsistency evaluator for abstractive text summarization <em>(Rating: 2)</em></li>
                <li>G-eval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6131",
    "paper_id": "paper-9eadfe920cae4a451af437752de83075ec528288",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "RTS",
            "name_full": "Reason-then-Score",
            "brief_description": "A prompting-based Likert-scale evaluation method where the LLM is asked to produce a short rationale (reason) and then give a final 1–5 score on a specified dimension (e.g., relevance). Designed to mimic chain-of-thought style evaluation while yielding a numeric rating aligned with human Likert scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt the LLM to generate a one-sentence reason followed by a 1–5 Likert score for each summary on a specified evaluation dimension (coherence/consistency/fluency/relevance).",
            "evaluation_criteria": "Per-dimension Likert score (1–5) with an accompanying textual reason; later compared to human average scores using correlations and preference counts.",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0301) primarily; also evaluated with GPT-4 (gpt-4-0314).",
            "theory_domain": "Abstractive summarization (NLP) — used as a proxy for LLM-based evaluation of generated outputs.",
            "theory_description": "Not a scientific theory per se; RTS is a method for eliciting evaluative judgments (reason + score) from an LLM about generated summaries to approximate human judgement.",
            "evaluation_results": "RTS (ChatGPT) achieved stronger correlations with human scores than many automatic metrics (outperforming ROUGE/BERTScore in several dimensions); on the full 66-pair set RTS had average #CP ≈ 58.5/66 (≈88.6%); showed higher correlation particularly on consistency and relevance (see Table 4). However RTS often produced dimension-irrelevant or unfaithful reasoning and tended to give more conservative (lower) absolute scores than humans.",
            "benchmarks_or_datasets": "Evaluated on the Summeval benchmark (Fabbri et al., 2021) of CNN/DM summaries (1200 summaries total, 12 systems × 100 each).",
            "comparison_to_human": "RTS scores correlated reasonably well with expert human average scores (Spearman/Pearson/Kendall reported); but per-system correlations varied widely (candidate-dependence) and RTS became less aligned with humans for higher-quality systems (negative meta-correlation).",
            "limitations_or_challenges": "RTS frequently generates incorrect or dimension-irrelevant reasoning (unfaithful chain-of-thought), is candidate- and dimension-dependent, and exhibits negative meta-correlation (worse alignment on high-quality systems). Absolute RTS scores are systematically lower than human scores.",
            "uuid": "e6131.0",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MCQ",
            "name_full": "Multiple-Choice Question scoring",
            "brief_description": "A constrained prompting Likert-scale method where the LLM selects one of predefined answer choices (A–E) mapped to 1–5, with each choice described explicitly to constrain reasoning and reduce spurious chain-of-thought outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt LLM with a multiple-choice question that maps A–E to 1–5 with explicit textual descriptions for each choice, then record the selected score for each dimension.",
            "evaluation_criteria": "Per-dimension discrete choice (A–E corresponding to 1–5); compared against human averages using correlations and preference counts.",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0301); also evaluated comparatively with GPT-4 in experiments.",
            "theory_domain": "Abstractive summarization (NLP) evaluation.",
            "theory_description": "Constrained LLM scoring approach intended to reduce hallucinated or irrelevant rationales by forcing selection among predefined reasons tied to scores.",
            "evaluation_results": "ChatGPT-MCQ attained strong correlations with humans in coherence and fluency dimensions; sometimes complementary to RTS (RTS better on consistency/relevance, MCQ better on coherence/fluency). MCQ yields higher absolute scores (more optimistic) than RTS and sometimes matches human average magnitudes better, but does not eliminate candidate- or dimension-dependence.",
            "benchmarks_or_datasets": "Summeval dataset (Fabbri et al., 2021) on CNN/DM; comparisons against ROUGE/BERTScore/BARTScore baselines.",
            "comparison_to_human": "MCQ correlated with human judgments across dimensions but still exhibited per-system variability; agreement between RTS and MCQ (R_i) was proposed as an indicator of reliability relative to human alignment.",
            "limitations_or_challenges": "While more constrained, MCQ still shows dimension- and candidate-dependence and degrades on higher-quality systems (negative meta-correlation in some dimensions); absolute scoring bias (generally higher than RTS) and some dimensions (relevance) remain problematic.",
            "uuid": "e6131.1",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "H2H",
            "name_full": "Head-to-Head comparison",
            "brief_description": "A pairwise ranking approach where the LLM is given two summaries for the same article and asked to choose which is better (or indicate a tie) for a specific evaluation dimension; each pair is evaluated twice with swapped positions to reduce ordering bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Prompt LLM to compare Summary #1 vs Summary #2 on a dimension and choose A (summary1 better), B (summary2 better), or C (tie), aggregating pairwise preferences over articles and pairs.",
            "evaluation_criteria": "Pairwise preference agreement with average human preference; measured via number of correct preferences (#CP) relative to human comparisons, with aggregation across article instances and pairs.",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0301) primarily; evaluated on a targeted challenge set due to API budget constraints.",
            "theory_domain": "Abstractive summarization ranking/evaluation.",
            "theory_description": "LLM-as-ranker method to emulate human pairwise comparative judgments.",
            "evaluation_results": "On an 11-pair challenge set of closely matched system pairs, LLM H2H performance dropped relative to Likert methods; ChatGPT-H2H had lower overall #CP on challenge pairs and struggled to differentiate candidates with close performance. On the full 66-pair set (where applicable), RTS often outperformed H2H in aggregate correctness.",
            "benchmarks_or_datasets": "Challenge and full pair sets constructed from Summeval systems (66 possible pairs; 11 closest pairs used for H2H due to cost).",
            "comparison_to_human": "H2H sometimes aligned with humans on clear performance gaps but failed more often on closely matched candidate pairs; LLM preferences did not consistently match aggregated human preferences on the challenge set.",
            "limitations_or_challenges": "High evaluation cost for exhaustive H2H (order doubling), sensitivity to small performance gaps (poor discrimination for close pairs), candidate- and dimension-dependence.",
            "uuid": "e6131.2",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Meta-corr",
            "name_full": "Meta-correlation metric (M)",
            "brief_description": "A metric introduced in this paper to quantify how an LLM evaluator's correlation with human judgments varies with the underlying quality of candidate systems, i.e., measures system-dependence and whether LLM evaluation degrades/improves on higher-quality systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Compute average human quality Q_i per candidate (mean human score across N summaries), compute per-candidate correlation P_i between LLM and human scores, then compute correlation M = ρ([Q_1...Q_k],[P_1...P_k]) using Spearman/Pearson/Kendall as desired.",
            "evaluation_criteria": "Meta-correlation M; value near zero implies evaluator performance independent of candidate quality (stable); significant positive/negative M indicates dependency (e.g., negative M indicates worse alignment on higher-quality systems).",
            "llm_model_name": "Applied to ChatGPT and GPT-4 evaluations in the paper.",
            "theory_domain": "Stability analysis of LLM evaluators for abstractive summarization.",
            "theory_description": "A quantitative test for evaluator stability: whether the evaluator's human-alignment score (correlation) systematically varies with the underlying quality of systems being evaluated.",
            "evaluation_results": "ChatGPT (RTS/MCQ) and neural baselines (BARTScore variants) displayed significant negative meta-correlations in several dimensions (especially consistency and fluency), indicating poorer human alignment on higher-quality systems; ROUGE metrics showed no significantly negative meta-correlation and sometimes positive meta-correlation (e.g., ROUGE-2 on coherence). GPT-4 showed improvements in consistency but still had negative meta-correlation in relevance.",
            "benchmarks_or_datasets": "Computed over k=12 candidate systems in Summeval (each with N=100 summaries) using Spearman/Pearson/Kendall correlations.",
            "comparison_to_human": "Meta-correlation directly measures relationship between per-system human quality and LLM-human correlation; observed negative meta-correlation demonstrates LLMs can be more aligned with humans for low-quality systems and less aligned for high-quality systems.",
            "limitations_or_challenges": "Meta-correlation estimate can be sample-limited (only 12 candidate systems here), sensitive to dataset composition, and does not explain causal factors for the dependency (e.g., hallucination reduction, informativeness tradeoffs).",
            "uuid": "e6131.3",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "R_i",
            "name_full": "RTS–MCQ agreement reliability indicator (R_i)",
            "brief_description": "A per-candidate reliability indicator equal to the correlation between RTS scores and MCQ scores across the N summaries produced by a candidate; proposed as a cheap proxy to detect when LLM evaluations are likely reliable and when further human validation is needed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "For candidate i, compute R_i = ρ([f_RTS(g_{i,1})...f_RTS(g_{i,N})],[f_MCQ(g_{i,1})...f_MCQ(g_{i,N})]) using a chosen correlation metric; interpret R_i relative to a tolerance r to decide whether to trust LLM evaluator.",
            "evaluation_criteria": "R_i correlation value; if R_i &gt; r (tolerance threshold), treat the LLM method (RTS or MCQ) as likely reliable for that candidate/dimension; otherwise invoke human evaluation.",
            "llm_model_name": "Computed using ChatGPT RTS and MCQ outputs in experiments.",
            "theory_domain": "Quality control for LLM-based evaluation of generated summaries.",
            "theory_description": "A pragmatic meta-check: high inter-method agreement between two LLM prompting styles indicates higher likelihood of alignment with human judgments.",
            "evaluation_results": "R_i correlated positively and significantly with actual LLM-human performance P_i in several dimensions: R_i had significant positive correlation with P_i^RTS on consistency and fluency, and with P_i^MCQ on coherence and consistency (see Table 7), supporting R_i as a useful proxy in those dimensions. R_i had no significant predictive power for relevance.",
            "benchmarks_or_datasets": "Computed on Summeval systems (N=100 summaries per candidate) using Spearman/Pearson/Kendall correlations.",
            "comparison_to_human": "R_i is designed to predict whether the LLM's scores will align with human scores; empirical evidence in the paper shows R_i is a useful indicator in some dimensions but fails for relevance.",
            "limitations_or_challenges": "Dimension-dependent effectiveness (doesn't work for relevance), threshold r may be dataset-dependent, and agreement between two flawed LLM methods does not guarantee correctness (both could be systematically biased).",
            "uuid": "e6131.4",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Eval-Dimensions",
            "name_full": "Evaluation dimensions: Coherence, Consistency, Fluency, Relevance",
            "brief_description": "Four human-centric evaluation criteria reused from Fabbri et al. (2021) to judge summary quality: coherence (global flow), consistency (factual alignment), fluency (sentence-level quality), and relevance (selection of important content).",
            "citation_title": "Summeval: Re-evaluating summarization evaluation",
            "mention_or_use": "use",
            "evaluation_method": "Each dimension is explicitly defined to annotators and LLM prompts; LLM scores are elicited per-dimension (RTS/MCQ/H2H) and then correlated with human expert annotations on those dimensions.",
            "evaluation_criteria": "Dimension-specific Likert ratings (1–5) and pairwise preferences; numeric comparisons via Pearson/Spearman/Kendall correlation and #CP for pairwise preference agreement.",
            "llm_model_name": "Used with ChatGPT and GPT-4 evaluators in the paper.",
            "theory_domain": "Summarization evaluation in NLP (dimensions reflect qualities humans judge in generated summaries).",
            "theory_description": "Operationalized definitions of the four dimensions are embedded into prompts so the LLMs evaluate along the same axes as human annotators.",
            "evaluation_results": "LLM evaluators show varying performance across these dimensions: ChatGPT RTS/MCQ had relatively strong correlations on fluency and relevance, mixed on coherence and consistency; GPT-4 improved consistency detection (less hallucination) but had negative meta-correlation in relevance. Overall, LLMs are dimension-dependent and not uniformly reliable.",
            "benchmarks_or_datasets": "Definitions and annotations come from Summeval (Fabbri et al., 2021) on CNN/DM.",
            "comparison_to_human": "Human expert annotations (3 annotators per summary) served as ground truth; inter-annotator agreement (kappa=0.713) supports using averaged human scores for correlation analyses.",
            "limitations_or_challenges": "Some LLMs produce reasoning unrelated to the evaluated dimension; LLMs' performance varies by dimension and candidate, so dimensions cannot be assumed equally reliable for LLM evaluation.",
            "uuid": "e6131.5",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Baselines & Benchmarks",
            "name_full": "Automatic metrics and dataset baselines (ROUGE, BERTScore, BARTScore, BARTScore-CNN/PARA, Summeval)",
            "brief_description": "Standard automatic summarization metrics and the human-annotated Summeval benchmark used to compare and contextualize LLM evaluators' performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute standard automatic metric scores per summary (ROUGE-1/2/L F1, BERTScore, BARTScore variants) and compare to human average scores via correlation metrics; also use pairwise preferences (#CP) for system comparisons.",
            "evaluation_criteria": "ROUGE (lexical overlap), BERTScore (semantic token similarity via BERT embeddings), BARTScore (generation probability under BART), and fine-tuned BART variants; correlations (#CP, Pearson, Spearman, Kendall) to human scores used as evaluation criteria.",
            "llm_model_name": "Not LLM-generated theories; these are baseline automatic metrics. LLMs evaluated against these baselines.",
            "theory_domain": "Automatic evaluation of abstractive summarization (NLP).",
            "theory_description": "Baselines represent established automated ways to score summaries; used as points of comparison for LLM-based evaluators.",
            "evaluation_results": "ChatGPT RTS/MCQ outperformed many traditional automatic metrics (ROUGE, BERTScore, some BARTScore setups) in overall human correlation, though specialized BARTScore-CNN variants (fine-tuned on domain) were competitive, especially for coherence. ROUGE showed little negative meta-correlation and in one case (ROUGE-2) positive meta-correlation on coherence.",
            "benchmarks_or_datasets": "Summeval (Fabbri et al., 2021) on CNN/DM; baselines reported from Wang et al. (2023a) for consistency with prior numbers.",
            "comparison_to_human": "Baselines generally had lower alignment with human expert averages compared to ChatGPT and GPT-4 in many dimensions, but some domain-fine-tuned metrics matched or exceeded LLM alignment in certain dimensions.",
            "limitations_or_challenges": "ROUGE and other lexical metrics are insufficient for abstractive summarization due to variability of valid summaries; neural metrics (BERTScore/BARTScore) can be domain-biased and exhibit negative meta-correlation; none are perfect replacements for human evaluation.",
            "uuid": "e6131.6",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Measurements",
            "name_full": "Evaluation measurements: #CP, Pearson, Spearman, Kendall",
            "brief_description": "A suite of quantitative measurements used to compare LLM evaluators to human judgments: number of correct preferences (#CP) for pairwise comparisons and correlation coefficients (Pearson, Spearman, Kendall's Tau) for score-level agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Use #CP to judge pairwise preference agreement (LLM vs human aggregated preferences); use Pearson for linear correlation, Spearman for rank-based monotonic correlation, and Kendall's Tau for pairwise concordance between LLM scores and human scores across summaries or per-system.",
            "evaluation_criteria": "#CP (count of matched pairwise preferences), Pearson r, Spearman ρ, Kendall's τ (tau-b) with significance testing (p-values).",
            "llm_model_name": "Applied to ChatGPT and GPT-4 evaluation outputs in experiments.",
            "theory_domain": "Statistical evaluation of evaluator alignment in NLP summarization.",
            "theory_description": "Measurement toolkit for quantifying agreement between LLM evaluators and human expert judgments at both pairwise and score levels.",
            "evaluation_results": "Reported that ChatGPT-RTS and MCQ had higher Spearman/Pearson/Kendall correlations with human scores than many baselines (up to ~0.2 gains in some dimensions); #CP on full set: RTS ≈ 58.5/66 on average; correlations varied widely per system and dimension.",
            "benchmarks_or_datasets": "Applied on Summeval (1200 summaries) and on per-system splits (100 summaries per system) to compute per-candidate correlations and meta-analyses.",
            "comparison_to_human": "These metrics quantify the degree to which LLM outputs reproduce human rankings/scores; used to demonstrate both strengths and failings of LLM evaluators relative to human annotations.",
            "limitations_or_challenges": "Correlation metrics can mask per-system variance; #CP is sensitive to closeness of system performance and dataset construction; significance testing and sample sizes affect interpretability.",
            "uuid": "e6131.7",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLM-Evaluators",
            "name_full": "ChatGPT / GPT-4 evaluators",
            "brief_description": "Instruction-tuned LLMs (ChatGPT gpt-3.5-turbo-0301 and GPT-4 gpt-4-0314) used as zero-shot evaluators via natural-language prompts. Experiments evaluate their reliability and limitations as automatic summarization evaluators.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "evaluation_method": "Zero-shot prompting with carefully designed instructions for RTS, MCQ, and H2H; temperature set to 0 and dialogue reset per instance. Comparisons with baselines using correlation and #CP metrics.",
            "evaluation_criteria": "Human alignment measured via Pearson/Spearman/Kendall correlations and #CP; stability measured via per-candidate correlation spread and meta-correlation M; internal reliability via RTS–MCQ agreement R_i.",
            "llm_model_name": "ChatGPT (gpt-3.5-turbo-0301 snapshot) and GPT-4 (gpt-4-0314 snapshot).",
            "theory_domain": "Evaluation of generated text (abstractive summarization) and the general capability of LLMs as evaluators.",
            "theory_description": "LLMs are prompted to judge summaries across human-defined dimensions, serving as automatic evaluators intended to approximate human expert judgements.",
            "evaluation_results": "ChatGPT (RTS/MCQ) outperformed many traditional automatic metrics in correlation with human scores but was candidate- and dimension-dependent, struggled on closely matched system comparisons, and showed negative meta-correlation (worse alignment on higher-quality candidates). GPT-4 improved correlation in many dimensions and reduced hallucination-related errors (better consistency detection) but still exhibited candidate- and dimension-dependence and some negative meta-correlations.",
            "benchmarks_or_datasets": "Summeval dataset (Fabbri et al., 2021) on CNN/DM; also compared to Llama 2 variants in appendices.",
            "comparison_to_human": "LLMs correlated with human expert averages on aggregate but had inconsistent per-system behavior; GPT-4 achieved higher overall correlations than ChatGPT but did not eliminate limitations.",
            "limitations_or_challenges": "Model snapshots may not remain stable (OpenAI snapshot changes), cost and API limits constrain exhaustive H2H evaluation, LLMs can produce unfaithful reasoning affecting scoring, and LLM evaluation reliability degrades with higher-quality summaries.",
            "uuid": "e6131.8",
            "source_info": {
                "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Summeval: Re-evaluating summarization evaluation",
            "rating": 2
        },
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries",
            "rating": 2
        },
        {
            "paper_title": "BERTScore: Evaluating text generation with BERT",
            "rating": 2
        },
        {
            "paper_title": "BartScore: Evaluating generated text as text generation",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT as a factual inconsistency evaluator for abstractive text summarization",
            "rating": 2
        },
        {
            "paper_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 1
        }
    ],
    "cost": 0.01983175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization</h1>
<p>Chenhui Shen ${ }^{\star 1,2}$ Liying Cheng ${ }^{1,3}$ Xuan-Phi Nguyen ${ }^{1,3}$ Yang You ${ }^{2}$ Lidong Bing ${ }^{1,3}$<br>${ }^{1}$ DAMO Academy, Alibaba Group, Singapore ${ }^{2}$ National University of Singapore<br>${ }^{3}$ Hupan Lab, 310023, Hangzhou, China<br>{chenhui.shen, liying.cheng, x.nguyen, l.bing}@alibaba-inc.com<br>youy@comp.nus.edu.sg</p>
<h4>Abstract</h4>
<p>With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The desire for inexpensive and fast automatic metrics has never stopped growing. In certain tasks like extractive summarization, where full source sentences are selected to appear in the summaries, simple n-gram overlap metrics against the "gold" summaries like Rouge (Lin, 2004) or BLEU (Papineni et al., 2002) may work well because the correct answer space is narrow. However, for more open tasks like abstractive summarization, there are countless equally good summaries and the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>"gold" summaries become less important. Although many neural-based metrics such as BERTScore and BARTScore (Zhang et al., 2020b; Yuan et al., 2021), are advocated as more human-aligned, the evaluation criteria are also becoming increasingly complex. As a result, abstractive summarization may not be sufficiently evaluated with automatic metrics (Owczarzak et al., 2012; Nenkova, 2006), and often require extensive human evaluations as complements(Yang et al., 2023; Welbl et al., 2021). However, human evaluations often come with hefty costs and slow iteration cycles, while also being difficult to reproduce and standardize due to small sample sizes and potential human biases (Shen et al., 2022b; Liu et al., 2022).</p>
<p>Recent large language models (LLMs) like ChatGPT and GPT-4 (OpenAI, 2023) have demonstrated outstanding capabilities in language comprehension and reasoning. This leads to a growing trend of employing LLMs as evaluators for complex language generation tasks by prompting them with carefully and elaborately crafted instructions (Chiang and Lee, 2023; Gao et al., 2023; Wang et al., 2023a; Wu et al., 2023; Luo et al., 2023; Liu et al., 2023). Despite the preliminary success suggested by such works, it is still inconclusive as to what degree of confidence we can trust the evaluation results produced by LLMs across different dimensions, despite their supposedly high average correlation with humans. It is also unclear if certain LLM-based metrics are more reliable than others, or if their reliability and fairness vary for different candidate systems.</p>
<p>In this work, we conduct extensive analysis to assess whether LLM evaluators can reliably replace human judges. Specifically, we incorporate two common human evaluation approaches with LLM evaluators, namely Likert-scale scoring (He et al., 2022; Shen et al., 2022b; Zhang et al., 2020a) and head-to-head (H2H) comparisons (Shen et al., 2022a; Li et al., 2020; Liu and Lapata, 2019). For</p>
<p>Likert-scale scoring, we explore direct reason-thenscore (RTS) generation and a multiple-choice question (MCQ) method. The former instructs the LLM to provide reasoning before giving a score, while the latter simply prompts it to choose a specific score with a pre-determined description as the reason. For the Head-to-Head (H2H) comparison, we prompt LLM for a preference over the summaries from two compared candidate systems.</p>
<p>Our experiments show that LLM evaluators, with RTS and MCQ, outperform existing automatic metrics (Lin, 2004; Yuan et al., 2021). However, they are not ready to be reliable alternatives for human evaluation yet. Specifically, (i) LLM evaluators struggle to distinguish candidates with close performances (§ 4.2.1). (ii) LLM evaluators are candidate-dependent, meaning they do not exhibit highly consistent degrees of human alignment for different candidates (§ 4.2.3). Thus, they may unfairly favor or disfavor an evaluated candidate. (iii) LLM evaluators are dimensiondependent, meaning they have varying degrees of evaluation capabilities for different dimensions like coherence and fluency (§ 4.2.3). (iv) Lastly, as the quality of summaries improves with better candidates, LLM evaluators become unreliably less correlated with human judgments, according to our newly proposed meta-correlation metric (§ 4.2.4).</p>
<p>While we still call for a better automatic metric, in the meantime, we suggest a temporary solution in $\S 5$ for abstractive summarization practitioners to use LLMs more reliably. Specifically, we advocate calculating the correlation between RTS and MCQ as a preliminary indicator of the reliability of the LLM for certain dimensions. If RTS and MCQ do not generally agree with each other, then further human evaluations are required.</p>
<h2>2 Related Work</h2>
<p>Summarization The summarization task involves generating a summary that contains concise and important (i.e., salient) contents of the original input article (Nenkova and McKeown, 2012). This task has been handled with 2 different approaches: extractive and abstractive. Unlike extractive summarization systems that directly extract salient phrases or sentences from the input article (Ernst et al., 2022; Chen et al., 2021; Zhou et al., 2018; Dong et al., 2018), abstractive summarization systems are expected to generate summaries using their own words and apply sentence fusion
or paraphrasing techniques (Shen et al., 2023; Liu et al., 2022; Xiao et al., 2022; Lewis et al., 2020; Zhang et al., 2020a; Ziegler et al., 2019; Bing et al., 2015; Xu and Durrett, 2021). As such, abstractive summarization poses significantly more challenges for automatic and human evaluation pipelines (Saha et al., 2022; Pagnoni et al., 2021), because it is increasingly insufficient to use the provided "gold" summary as ground truth.</p>
<p>Human Evaluation Human evaluation can be conducted with different approaches. Some work (He et al., 2022; Shen et al., 2022b; Zhang et al., 2020a; Cheng et al., 2020; Gao et al., 2019; Liu et al., 2018; Li et al., 2017; Kryściński et al., 2018) employ a Likert scale to evaluate the summaries on discrete ranges, such as from 1 to 5 . Meanwhile, many others suggest comparison approaches by asking human annotators to select the best summary out of 2 or more generated summaries from different systems (Shen et al., 2022a; Li et al., 2020; Liu and Lapata, 2019; Fan et al., 2018; Fabbri et al., 2019). Following this, we test LLM-based evaluators using both approaches with human-friendly instruction prompts.</p>
<p>Automatic Evaluation Rouge (Lin, 2004) has been a common lexical overlap metric to evaluate summarization systems. Apparently, ROUGE is not sufficient for abstractive summarization, because the "gold" labels it relies on cannot comprehensively account for the complexity and variability of this task. In addition, the common usage of sentence fusion techniques and novel words for abstractive summarization may make RouGE even less reliable. Zhang et al. (2020b) propose the neural-based BERTScore, which leverages the BERT word embeddings to compute the semantic similarity among tokens. Yuan et al. (2021) later introduce BARTScore, which uses BART (Lewis et al., 2020) to compute the probability of a summary given its input article. Nonetheless, these metrics may not reflect all of the complicated evaluation dimensions required for abstractive summarization mentioned earlier, nor do they have sufficiently high correlations with humans.</p>
<p>LLM-based Evaluation There are many concurrent works that demonstrate the potential of LLMs to conduct complex human tasks (Chiang and Lee, 2023; Gao et al., 2023; Wang et al., 2023a; Wu et al., 2023; Luo et al., 2023; Liu et al., 2023; Cheng et al., 2023). The key advantage of instruction-</p>
<p>tuned LLMs, like ChatGPT or GPT-4 (Ouyang et al., 2022; OpenAI, 2023), is that we can explicitly describe in natural language what our evaluation criteria and dimensions are and how to score the summaries, similar to how we would explain such tasks to a human expert. Chiang and Lee (2023) use LLMs for open-ended story evaluations, while Luo et al. (2023) apply ChatGPT specifically for evaluating the consistency of summaries. Wu et al. (2023) formulate LLMs as diverse roleplayers to evaluate summaries from the perspectives of different personas. Wang et al. (2023a) and Liu et al. (2023) also explore the LLM's evaluation potential in various dimensions for the natural language generation task. Our work differs from the above works in that besides investigating the LLMs' capability using different approaches across various dimensions for abstractive summarization, we further focus on the reliability of LLM across evaluated systems and dimensions.</p>
<h2>3 LLM as a Zero-Shot Evaluator</h2>
<p>We investigate an LLM's evaluation capabilities in the dimensions of coherence, consistency, fluency, and relevance respectively, as defined by Fabbri et al. (2021) (see Appendix A). Following common human evaluation approaches, we propose two methods for Likert-scale scoring, namely the reason-then-score method and the multiple-choice question method, as well as one method for head-to-head comparisons. We describe each method in $\S 3.1$ using the relevance dimension as an example (see more prompts and details in Appendix B). We further experiment with alternative phrasings for different methods in Appendix C.</p>
<p>Besides exploring different evaluation methods, the stability of LLM-based evaluations across different summarization systems is equally important. Ideally, a stable LLM evaluator should perform equally well regardless of the evaluated systems, with a close (if not identical) degree of alignment with human judgments. In § 3.2, we propose a meta-correlation metric and explain how it can gauge the extent to which LLM evaluators' performances depend on the evaluated systems, which indicates how stable and reliable they may be with evaluating any future candidate systems.</p>
<h3>3.1 Summary Evaluation Methods</h3>
<p>Reason-then-Score (RTS) Given the success of chain-of-thought prompting (Kojima et al., 2022;</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Example prompt for the RTS method on the relevance dimension. Texts in [blue] represent the article and the corresponding summary to be evaluated.</p>
<p>Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to relevance from one to five, where one indicates "irrelevance", and five indicates "perfect relevance". Note that relevance measures the Summary's selection of important content from the Article, whether the Summary grasps the main message of the Article without being overwhelmed by unnecessary or less significant details.</p>
<p>Article: {article}
Summary: {summary}
Provide your reason in one sentence, then give a final score:</p>
<h4>Abstract</h4>
<p>Table 1: Example prompt for the RTS method on the relevance dimension. Texts in [blue] represent the article and the corresponding summary to be evaluated.</p>
<p>Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to relevance from one to five, where one indicates "irrelevance", and five indicates "perfect relevance". Note that relevance measures the Summary's selection of important content from the Article, whether the Summary grasps the main message of the Article without being overwhelmed by unnecessary or less significant details.</p>
<p>Article: {article}
Summary: {summary}</p>
<p>A: The Summary is totally irrelevant to the Article. Score: One. B: The majority of the Summary is irrelevant to the Article. Score: Two. C: Some information in the Summary is relevant to the Article whereas some are not. Score: Three. D: The majority of the Summary is relevant to the Article. Score: Four. E: All information included in the Summary is relevant to the Article. Score: Five.</p>
<p>Your Answer (enter 1 letter from A to E):
Table 2: Example prompt for the MCQ method on the relevance dimension. Texts in [blue] represent the article and the corresponding summary to be evaluated.</p>
<p>Wei et al., 2022), an intuitive method is to ask the LLM to evaluate a specific dimension by first generating the reasoning and then a corresponding score. Since the SummEval dataset (Fabbri et al., 2021) contains human scores on a Likert scale of 1 to 5 , we also ask the LLM to score the summaries in the same range, as shown in Table 1.</p>
<p>MCQ Scoring (MCQ) Nevertheless, previous works find that the reasoning generated by the LLM does not always make sense (Lyu et al., 2023;</p>
<p>Choose a more relevant summary from Summary #1 and Summary #2 with respect to the corresponding Article by choosing an option from A, B, or C. Note that relevance measures the summary's selection of important content from the Article, whether the summary grasps the main message of the Article without being overwhelmed by unnecessary or less significant details.</p>
<p>Article: {article}
Summary #1: {summary from model A}
Summary #2: ${$ summary from model B}
A: Summary #1 is more relevant.
B: Summary #2 is more relevant.
C: Both Summary #1 and Summary #2 are equally relevant.
Your choice (enter 1 letter from A to C):
Table 3: Example prompt for the H 2 H method on the relevance dimension. Text in {blue}: the specific article, and the corresponding summaries generated by a pair of compared models.</p>
<p>Wang et al., 2023b; Gao et al., 2022). To avoid the misguidance of wrongly generated reasoning, we explore a more constrained MCQ method for the Likert-scale scoring. As shown in Table 2, instead of allowing the LLM to freely generate its thoughts, we dictate specific reasoning for each score.</p>
<p>Head-to-Head Comparison (H2H) Lastly, some concurrent works also observe that ChatGPT can act as an effective ranking model (Ma et al., 2023a,b). We thus explore the head-to-head comparison approach for LLM-based evaluations. As shown in Table 3, we present 2 summaries (Summary #1 and #2) generated by different summarization systems on the same input article, then prompt the LLM to select the better summary, or to indicate a tie. Moreover, to avoid potential biases that arise from the summary IDs, we conduct each evaluation twice, presenting the same summary as either #1 or #2 respectively.</p>
<h3>3.2 Stability of LLM Evaluators</h3>
<p>To ensure fairness across all evaluated systems, we argue that it is crucial for LLMs to produce stable evaluations. That is, regardless of evaluated systems, the LLMs should maintain a consistent degree of alignment with human judgments. We investigate such stability in two ways.</p>
<p>First, We categorize the summaries based on their originating summarization systems, and then
examine the correlation between the LLM and human evaluations for each system. Ideally, if an LLM is stable across systems, it should produce evaluations that are similarly correlated to human evaluations. Otherwise, if the correlations differ significantly across different candidates, then we may conclude that the LLM's evaluations are system-dependent.</p>
<p>Second, we define a meta-correlation metric to quantify the extent to which the LLM's performance is affected by the quality of the evaluated systems. Specifically, we use the average human score for each candidate as an indicator of its summarization quality $\left(Q_{i}\right)$, as shown in Equation (1):</p>
<p>$$
Q_{i}=\frac{1}{N} \sum_{j=1}^{N} f_{\text {human }}\left(g_{i, j}\right)
$$</p>
<p>where $f_{\text {human }}(\cdot)$ indicates the human evaluation, $g_{i, j}$ represents the $j^{\text {th }}$ summary generated by the $i^{\text {th }}$ candidate system. Each candidate's quality is calculated as an average of $N$ generated summaries ( $N=100$ for all systems). Next, we use the correlation $P_{i}$ between LLM scores and human scores as an indicator of the LLM's evaluation performance for the $i^{\text {th }}$ candidate, as follows:</p>
<p>$$
\begin{aligned}
P_{i}= &amp; \rho\left(\left[f_{\mathrm{LLM}}\left(g_{i, 1}\right), \ldots, f_{\mathrm{LLM}}\left(g_{i, N}\right)\right]\right. \
&amp; \left.\left[f_{\text {human }}\left(g_{i, 1}\right), \ldots, f_{\text {human }}\left(g_{i, N}\right)\right]\right)
\end{aligned}
$$</p>
<p>where $\rho$ denotes the correlation metric (i.e., Spearman correlation, Pearson correlation, or Kendall's Tau $^{2}$ ), and $f_{\mathrm{LLM}}(\cdot)$ indicates the LLM's evaluation for each summary $g_{i, j}$. Finally, we calculate the meta-correlation ${ }^{3} M$ on a total of $k$ candidates as:</p>
<p>$$
M=\rho\left(\left[Q_{1}, \ldots, Q_{k}\right],\left[P_{1}, \ldots, P_{k}\right]\right)
$$</p>
<p>Ideally, an LLM should work well regardless of the quality of the evaluated systems, which means that $M$ should be close to zero. On the other hand, a significant $M$ would indicate an undesirable relationship between the LLM's evaluation capability and the quality of the evaluated systems, suggesting that the LLM evaluation is not stable, such that it may not evaluate each candidate system fairly using the same standards.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>4 Experiments</h2>
<h3>4.1 Setups</h3>
<p>We use the ChatGPT "gpt-3.5-turbo-0301" snapshot (§ 4.2) for all three methods. By using a fixed snapshot, we ensure all evaluations are conducted with the same LLM model. In addition, we evaluate with the GPT-4 "gpt-4-0314" snapshot (§ 4.3) using the best evaluation method determined by ChatGPT to check for any potential improvement. Given that ChatGPT and GPT-4 are amongst the top performing LLMs, we use their performance to estimate the potential of LLMs as reliable evaluators. Additional results using three different-sized Llama 2 models (Touvron et al., 2023) are reported in Appendix D, which all performs worse. Similar to Luo et al. (2023) and Wu et al. (2023), we set the temperature to 0 and reset the dialogue history for each evaluation instance.</p>
<p>Dataset We use the SummEval benchmark dataset (Fabbri et al., 2021). This dataset contains expert human annotations for coherence, consistency, fluency, and relevance on the generation results from 12 abstractive systems (see details in Appendix table 21) on the CNN/DM dataset (Hermann et al., 2015). Each evaluated system generates summaries for the same 100 news articles, and each summary is scored by 3 expert annotators from 1 to 5 . The annotations achieve with a high kappa coefficient of 0.713 (Fabbri et al., 2021). We further calculate the annotations' standard deviations across each evaluated system in Appendix Table 20. Given a step size of 1, the standard deviations are considered very small, thus suggesting that this dataset has a high level of human agreement. Following Chiang and Lee (2023), Chhun et al. (2022), and Guan and Huang (2020), we use the average human scores as the reference scores.</p>
<p>Baselines We use Rouge (Lin, 2004) F1 scores for Rouge-1, Rouge-2, and Rouge-L, BERTScore (Zhang et al., 2020b), BARTScore, BARTScore-CNN, and BARTScore-CNN-PARA (Yuan et al., 2021) as baseline metrics. The last two metrics use BART models fine-tuned on CNN/DM's training data, and are especially strong.</p>
<p>Prompts We conduct evaluation following our prompt formats given in Table 1, 2, and 3. Following Fabbri et al. (2021), we re-use the definitions of the evaluation dimensions: (i) Coherence - the collective quality of all sentences, (ii) Consistency</p>
<ul>
<li>the factual alignment between the summary and the summarized source, (iii) Fluency - the quality of individual sentences, and (iv) Relevance - the selection of important content from the source.</li>
</ul>
<p>Measurements To compare all evaluation methods on equal ground with human evaluation, we use four different measurements. First, we count the number of correct preferences (#CP), which is the number of times each automatic metric has the same preference as the average human scores do over a set of compared system pairs (§ 4.2.1). This can help measure the alignment of evaluation methods with humans at a granular level. To determine the preferred system by a particular metric, we assign a system 1 point if its generated summary is evaluated as better than that of the other system according to the metric, or assign both systems 0.5 for a tie. Then, we aggregate the different scores for the compared systems for all 100 test inputs, and the system with a higher score is considered the preferred system by that metric (see Appendix Table 22 for details).</p>
<p>Next, we also use the Pearson correlation (Cohen et al., 2009), Spearman correlation (Spearman, 1987), and Kendall's Tau (Kendall, 1938) to measure the relationship between the scores of automatic evaluators and humans (§ 4.2.2, 4.2.3, 4.2.4). While the Pearson score measures linear relationships, the other two measure the ordinal relationship that may be non-linear. Moreover, Kendall's Tau is less sensitive than Spearman correlation to outliers due to its paired counting of concordant and discordant pairs.</p>
<h3>4.2 ChatGPT Evaluator</h3>
<p>In this section, we examine the ChatGPT evaluator across many aspects, ranging from human correlation and stability across different systems.</p>
<h3>4.2.1 Correct Preferences</h3>
<p>The ultimate goal of evaluation is to determine if one candidate system is better than the other in a compared pair. The number of correct preferences (#CP) metric normalizes all evaluation methods into determining whether an evaluator can, as a human expert would, pick the same better system or determine a tie. We conduct such analysis with different pairs of summarization systems on the same input articles. Due to the limited budget for API calls, we only evaluate H 2 H on a challenge set, consisting of 11 candidate pairs with the closest</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metrics</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1*</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.058</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.231</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2*</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.181</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L*</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.219</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore*</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.285</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore*</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.276</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-CNN*</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 0}$</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">$\underline{0.450}$</td>
<td style="text-align: center;">0.309</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-CNN-PARA*</td>
<td style="text-align: center;">$\underline{0.455}$</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">$\underline{0.368}$</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.299</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-RTS</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">$\underline{0.423}$</td>
<td style="text-align: center;">$\underline{0.532}$</td>
<td style="text-align: center;">$\underline{0.378}$</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 3}$</td>
<td style="text-align: center;">$\underline{0.357}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-MCQ</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">$\underline{0.350}$</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">$\underline{0.431}$</td>
<td style="text-align: center;">$\underline{0.305}$</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-RTS</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">$\underline{0.461}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 8}$</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">$\mathbf{0 . 3 7 3}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall's Tau (Kend.) between various metrics and human scores for a total of 1,200 summaries. *: results derived from Wang et al. (2023a). Bolded: best results. Underlined: second best results. Values in light gray color are insignificant (p-value $\geq 0.05$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Coh</th>
<th style="text-align: center;">Con</th>
<th style="text-align: center;">Flu</th>
<th style="text-align: center;">Rel</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$3.67 / 22$</td>
<td style="text-align: center;">$3.67 / 22$</td>
<td style="text-align: center;">$3.67 / 22$</td>
<td style="text-align: center;">$3.67 / 22$</td>
<td style="text-align: center;">$3.67 / 22$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">$3 / 40$</td>
<td style="text-align: center;">$5 / 46$</td>
<td style="text-align: center;">$3 / 46$</td>
<td style="text-align: center;">$4 / 47$</td>
<td style="text-align: center;">$3.75 / 44.75$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">$2 / 38$</td>
<td style="text-align: center;">$7 / 48$</td>
<td style="text-align: center;">$3 / 45$</td>
<td style="text-align: center;">$4 / 46$</td>
<td style="text-align: center;">$4.00 / 44.25$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">$2 / 31$</td>
<td style="text-align: center;">$5 / 37$</td>
<td style="text-align: center;">$4 / 39$</td>
<td style="text-align: center;">$6 / 41$</td>
<td style="text-align: center;">$4.25 / 37.00$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">$4 / 48$</td>
<td style="text-align: center;">$5 / 44$</td>
<td style="text-align: center;">$4 / 44$</td>
<td style="text-align: center;">$6 / 46$</td>
<td style="text-align: center;">$4.75 / 45.50$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">$8 / 46$</td>
<td style="text-align: center;">$7 / 48$</td>
<td style="text-align: center;">$5 / 45$</td>
<td style="text-align: center;">$6 / 46$</td>
<td style="text-align: center;">$6.50 / 46.25$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-CNN</td>
<td style="text-align: center;">$\mathbf{9 / 5 3}$</td>
<td style="text-align: center;">$5 / 53$</td>
<td style="text-align: center;">$5 / 54$</td>
<td style="text-align: center;">$4 / 53$</td>
<td style="text-align: center;">$5.75 / 53.25$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-CNN-PARA</td>
<td style="text-align: center;">$\mathbf{9 / 4 9}$</td>
<td style="text-align: center;">$\mathbf{8 / 5 4}$</td>
<td style="text-align: center;">$6 / 52$</td>
<td style="text-align: center;">$5 / 51$</td>
<td style="text-align: center;">$\mathbf{7 . 0 0 / 5 1 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-RTS</td>
<td style="text-align: center;">$6 / 54$</td>
<td style="text-align: center;">$6 / 56$</td>
<td style="text-align: center;">$\mathbf{9 / 6 2}$</td>
<td style="text-align: center;">$\mathbf{7 / 6 2}$</td>
<td style="text-align: center;">$\mathbf{7 . 0 0 / 5 8 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-MCQ</td>
<td style="text-align: center;">$5 / 54$</td>
<td style="text-align: center;">$7 / 56$</td>
<td style="text-align: center;">$8 / 60$</td>
<td style="text-align: center;">$\mathbf{7 / 5 8}$</td>
<td style="text-align: center;">$6.75 / 57.00$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-H2H</td>
<td style="text-align: center;">$8 /$</td>
<td style="text-align: center;">$7 /$</td>
<td style="text-align: center;">$7 /$</td>
<td style="text-align: center;">$4 /$</td>
<td style="text-align: center;">$6.50 /$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-RTS</td>
<td style="text-align: center;">$5 / 55$</td>
<td style="text-align: center;">$7 / 53$</td>
<td style="text-align: center;">$8 / 60$</td>
<td style="text-align: center;">$\mathbf{7 / 5 6}$</td>
<td style="text-align: center;">$6.75 / 56.00$</td>
</tr>
</tbody>
</table>
<p>Table 5: Number of correct preferences (#CP) on the 11-pair challenge set (in black) and the 66-pair full set (in brown). Random: for each pair, there are three possibilities (two possibilities for one model being better, one possibility for a tie) so the random #CP is one-third of the total compared pairs.
average performances according to human scores. However, for RTS, MCQ, and other baselines, we can easily calculate the #CP for all 66 possible pairs (see Appendix E).</p>
<p>Table 5 reports the #CP for both the standard 66pair full set (in brown) and the 11-pair challenge set (in black). As shown for the larger standard set, RTS unanimously obtains the largest #CP across all dimensions, with an average of 58.5 out of 66 candidate pairs (i.e. $88.6 \%$ accuracy).</p>
<p>Despite the high overall accuracy, weaknesses of such evaluators are revealed as we dive into their performances in the 11-pair challenge set (black scores of Table 5), where the evaluated candidates are close matches. Specifically, BARTScore-CNNPara performs better than RTS in coherence and consistency, possibly because it is fine-tuned with same-domain summarization data. For fluency and relevance, ChatGPT-RTS still performs best among all evaluators. Nonetheless, its average accuracy
drops significantly to $63.6 \%$ (7 out of 11), which indicates LLM evaluators struggle to differentiate the closely matched candidate systems. In other words, LLM evaluators may only reliably compare candidates with a relatively large performance gap.</p>
<h3>4.2.2 Correlations with Human</h3>
<p>Table 4 reports that Spearman, Pearson correlations, and Kendall's Tau between scores of multiple automatic evaluators and humans with a total of 1200 summaries from all systems, across the four evaluation dimensions. As shown, ChatGPT RTS and MCQ demonstrate stronger correlations with humans than many automatic evaluators, such as ROUGE and BARTScore, with up to 0.2 gains in fluency. While RTS achieves higher correlations in the dimensions of consistency and relevance, MCQ has relatively strong correlations in the dimensions of coherence and fluency. Meanwhile, the specialized BARTScore-CNN family also shows competitive performance in coherence, most likely due to the fine-tuning process with CNN/DM.</p>
<h3>4.2.3 Per-candidate Correlations</h3>
<p>Next, we break down the human correlation of ChatGPT-RTS for each candidate system and measure the statistical spread for the correlations across all systems (see raw results in Appendix table 23). Ideally, a stable evaluator should exhibit the same human correlation across candidates and dimensions, and display flattened boxes in a line.</p>
<p>However, as illustrated in Figure 1, the spread of correlations for different candidates is particularly wide, with up to 0.5 correlation difference in consistency. This means that the RTS evaluator exhibits a significantly varying degree of alignment with human judgment for different candidates. In other words, ChatGPT-RTS is candidate-dependent</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-0.032$</td>
<td style="text-align: center;">$-0.091$</td>
<td style="text-align: center;">$-0.490$</td>
<td style="text-align: center;">$-0.527$</td>
<td style="text-align: center;">$-0.303$</td>
<td style="text-align: center;">$-0.420$</td>
<td style="text-align: center;">$-0.518$</td>
<td style="text-align: center;">$-0.273$</td>
<td style="text-align: center;">$-0.420$</td>
<td style="text-align: center;">$-0.387$</td>
<td style="text-align: center;">$-0.273$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">$-0.259$</td>
<td style="text-align: center;">$-0.480$</td>
<td style="text-align: center;">$-0.152$</td>
<td style="text-align: center;">$-0.217$</td>
<td style="text-align: center;">$-0.438$</td>
<td style="text-align: center;">$-0.152$</td>
<td style="text-align: center;">$-0.084$</td>
<td style="text-align: center;">$-0.120$</td>
<td style="text-align: center;">$-0.121$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">$-0.310$</td>
<td style="text-align: center;">$-0.522$</td>
<td style="text-align: center;">$-0.303$</td>
<td style="text-align: center;">$-0.168$</td>
<td style="text-align: center;">$-0.413$</td>
<td style="text-align: center;">$-0.152$</td>
<td style="text-align: center;">$-0.324$</td>
<td style="text-align: center;">$-0.266$</td>
<td style="text-align: center;">$-0.121$</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">$-0.413$</td>
<td style="text-align: center;">$-0.403$</td>
<td style="text-align: center;">$-0.212$</td>
<td style="text-align: center;">$-0.580$</td>
<td style="text-align: center;">$-0.869$</td>
<td style="text-align: center;">$-0.424$</td>
<td style="text-align: center;">$-0.455$</td>
<td style="text-align: center;">$-0.663$</td>
<td style="text-align: center;">$-0.303$</td>
<td style="text-align: center;">$-0.685$</td>
<td style="text-align: center;">$-0.756$</td>
<td style="text-align: center;">$-0.515$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">$-0.916$</td>
<td style="text-align: center;">$-0.747$</td>
<td style="text-align: center;">$-0.788$</td>
<td style="text-align: center;">$-0.266$</td>
<td style="text-align: center;">$-0.504$</td>
<td style="text-align: center;">$-0.121$</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">$-0.769$</td>
<td style="text-align: center;">$-0.837$</td>
<td style="text-align: center;">$-0.606$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-CNN</td>
<td style="text-align: center;">$-0.748$</td>
<td style="text-align: center;">$-0.800$</td>
<td style="text-align: center;">$-0.636$</td>
<td style="text-align: center;">$-0.671$</td>
<td style="text-align: center;">$-0.913$</td>
<td style="text-align: center;">$-0.515$</td>
<td style="text-align: center;">$-0.510$</td>
<td style="text-align: center;">$-0.604$</td>
<td style="text-align: center;">$-0.485$</td>
<td style="text-align: center;">$-0.825$</td>
<td style="text-align: center;">$-0.852$</td>
<td style="text-align: center;">$-0.667$</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-CNN-PARA</td>
<td style="text-align: center;">$-0.720$</td>
<td style="text-align: center;">$-0.858$</td>
<td style="text-align: center;">$-0.606$</td>
<td style="text-align: center;">$-0.685$</td>
<td style="text-align: center;">$-0.888$</td>
<td style="text-align: center;">$-0.576$</td>
<td style="text-align: center;">$-0.294$</td>
<td style="text-align: center;">$-0.522$</td>
<td style="text-align: center;">$-0.212$</td>
<td style="text-align: center;">$-0.853$</td>
<td style="text-align: center;">$-0.880$</td>
<td style="text-align: center;">$-0.727$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-RTS</td>
<td style="text-align: center;">$-0.042$</td>
<td style="text-align: center;">$-0.052$</td>
<td style="text-align: center;">$-0.121$</td>
<td style="text-align: center;">$-0.811$</td>
<td style="text-align: center;">$-0.751$</td>
<td style="text-align: center;">$-0.636$</td>
<td style="text-align: center;">$-0.748$</td>
<td style="text-align: center;">$-0.728$</td>
<td style="text-align: center;">$-0.606$</td>
<td style="text-align: center;">$-0.539$</td>
<td style="text-align: center;">$-0.473$</td>
<td style="text-align: center;">$-0.594$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-MCQ</td>
<td style="text-align: center;">$-0.175$</td>
<td style="text-align: center;">$-0.11$</td>
<td style="text-align: center;">$-0.182$</td>
<td style="text-align: center;">$-0.818$</td>
<td style="text-align: center;">$-0.411$</td>
<td style="text-align: center;">$-0.636$</td>
<td style="text-align: center;">$-0.622$</td>
<td style="text-align: center;">$-0.484$</td>
<td style="text-align: center;">$-0.384$</td>
<td style="text-align: center;">$-0.350$</td>
<td style="text-align: center;">$-0.622$</td>
<td style="text-align: center;">$-0.212$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-RTS</td>
<td style="text-align: center;">$-0.531$</td>
<td style="text-align: center;">$-0.678$</td>
<td style="text-align: center;">$-0.424$</td>
<td style="text-align: center;">$-0.600$</td>
<td style="text-align: center;">$-0.103$</td>
<td style="text-align: center;">$-0.236$</td>
<td style="text-align: center;">$-0.839$</td>
<td style="text-align: center;">$-0.520$</td>
<td style="text-align: center;">$-0.515$</td>
<td style="text-align: center;">$-0.958$</td>
<td style="text-align: center;">$-0.880$</td>
<td style="text-align: center;">$-0.848$</td>
</tr>
</tbody>
</table>
<p>Table 6: Meta-correlation for various evaluation methods. Bolded: most negative meta-correlation. Underlined: second most negative meta-correlation. Values in light gray color are insignificant (p-value $\geq 0.05$ ).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Spread of per-candidate correlations with human scores for ChatGPT-RTS evaluations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Meta-correlation (Kendall's Tau) for RTS and MCQ. Shaded: statistically significant with $\mathrm{p}&lt;0.05$.
and one should not expect such LLM evaluators to have the same level of human alignment on a new summarization system. Similar trends can also be observed for MCQ (see Appendix table 24).</p>
<p>In addition, the medians across the four dimensions are also different. This indicates that the ChatGPT is also dimension-dependent and unstable. Given such varying performances across different dimensions, ChatGPT may not behave well with a newly introduced evaluation criterion.</p>
<h1>4.2.4 Summary Quality vs Human Alignment</h1>
<p>Using our proposed meta-correlation measurement in $\S 3.2$, we analyze the relationship between summary quality and human correlation of LLM evaluators. We illustrate the meta-correlation in terms of
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relationship between per-model correlations (Kendall's Tau) and human scores on consistency.</p>
<p>Kendall's Tau for both RTS and MCQ in Figure 2. As shown, both RTS and MCQ exhibit strong negative meta-correlation for consistency and fluency. This suggests that ChatGPT becomes less humanaligned with improving qualities of the evaluated systems.</p>
<p>To illustrate this phenomenon further, we scatter the paired coordinates of the summarization system quality ( $Q_{i}$, Equation (1)) and ChatGPT's evaluation performance ( $P_{i}$, Equation (2)) in Figure 3. As shown, while the LLM evaluator is better humancorrelated with lower-quality candidates ( $&lt;3.5$ ), it is less reliable when dealing with high-quality candidates ( $&gt;4.7$ ) with much lower and inconsistent correlations.</p>
<p>We compare the meta-correlation for all evaluation metrics in Table 6. We can see that while the ROUGE metrics exhibit no significantly negative meta-correlation, the neural metrics all display significant meta-correlation in certain dimensions. One highly likely reason for this behavior is due to the varying biases inherent to the neural models, which would explain why ROUGE as a simple n-gram overlap metric doesn't exhibit significant negative meta-correlations. Interestingly, ROUGE2 even shows a strong positive meta-correlation on coherence (which is plausible, because bi-gram</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The average ChatGPT RTS and MCQ scores and human scores across dimensions.
overlap performance may be more accurate as candidates produce more coherent texts).</p>
<p>Both the BARTScore variants and LLMs demonstrate the most negative meta-correlations. ChatGPT-RTS has the most negative metacorrelation in the dimensions of consistency and fluency, indicating that it may be the least reliable to evaluate high-quality systems on these dimensions. On the other hand, the BARTScore family may be unreliable in comparing systems with high qualities of coherence, consistency, and relevance.</p>
<p>So far, the observations discussed in $\S 4.2 .3$ and $\S 4.2 .4$ collectively suggest that LLM evaluators may not be a reliable standalone metric for challenging scenarios, and further human evaluation is required for conclusive decisions.</p>
<h3>4.2.5 RTS and MCQ Scores</h3>
<p>Lastly, we delve into the detailed scores generated by ChatGPT with either the RTS or MCQ method. Since both methods score the summaries in the same range of human scores of 1 to 5 (Fabbri et al., 2021), we can show a direct comparison of the average RTS and MCQ scores with human scores in Figure 4 (see more details in Appendix F). As shown, the RTS scores are much lower than the human scores across all dimensions, while MCQ scores are consistently higher and better match the human scores (except for relevance). In other words, while RTS is best aligned with humans according to $\S 4.2 .1$ and $\S 4.2 .2$, we cannot replace the human scores with RTS scores in absolute terms.</p>
<p>The discrepancy may be attributed to the unfaithful reasoning generated by LLMs (Lyu et al., 2023; Wang et al., 2023b; Gao et al., 2022). Our further investigation suggests that ChatGPT-RTS generates false or unrelated-to-dimension reasoning. Thus, it is possible that the much lower scores could be caused by ChatGPT penalizing the sum-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GPT-4's (RTS) spread of per-candidate correlations.
maries according to false premises (more examples in Appendix G). For instance, RTS may penalize the summary's repetitiveness in the consistency dimension or suppress fluency ratings for missing important details. ${ }^{4}$ On the other hand, the MCQ counterpart gives higher overall scores, most likely because the confined set of pre-defined reasons prevents such unrelated penalization, though not leading to better human alignment.</p>
<h3>4.3 GPT-4 Evaluator</h3>
<p>A natural question to ask is whether such aforementioned limitations are resolved with an stronger LLM. In this section, we conduct similar analyses on GPT-4 (OpenAI, 2023) with the RTS method. We present the GPT-4 results in the last rows of Table 4 and 5 . The results suggest that a stronger LLM does not necessarily translate to a stronger LLM evaluator, although Table 4 does show that GPT-4 outperforms ChatGPT in terms of human correlation consistently across most dimensions.</p>
<p>Unfortunately, GPT-4 still suffers from the same limitations as ChatGPT. It appears to be both candidate-dependent and dimension-dependent, as demonstrated by the large spreads with varying median values across dimensions in Figure 5 and the significantly negative meta-correlations out of 3 dimensions (Table 6). However, GPT-4 is less dimension-dependant as compared to ChatGPT, as the medians in the box plots in Figure 5 are more aligned than those in Figure 1.</p>
<p>In addition, there is a notable enhancement in the meta-correlation for consistency, which we attribute to a significant reduction in reported hallucinations with GPT-4 (OpenAI, 2023). It is possible that with much more instruction training to avoid hallucinations, GPT-4 is much better aligned with humans to detect inconsistencies (i.e. hallucina-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">$\rho\left(R_{i}, P_{i}^{\mathrm{RTS}}\right)$</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">$\rho\left(R_{i}, P_{i}^{\mathrm{MCQ}}\right)$</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">-0.106</td>
<td style="text-align: center;">0.000</td>
</tr>
</tbody>
</table>
<p>Table 7: Correlations between RTS-MCQ $R_{i}$ and RTS-Human ( $P_{i}^{\mathrm{RTS}}$ ) and MCQ-Human ( $P_{i}^{\mathrm{MCQ}}$ ). High values suggest $R_{i}$ can be a reliability indicator for RTS and MCQ. Light gray values are insignificant ( $\mathrm{p} \geq 0.05$ ).
tions) in summaries.
Nevertheless, GPT-4 exhibits a much worse negative meta-correlation in the relevance dimension, which, interestingly, seems to reflect the challenges of maintaining both "truthfulness" and "informativeness" (Ouyang et al., 2022). This is because a model could be easily made more truthful if allowed to provide less relevant information (for instance, by refusing to answer the users' questions). It is possible that with reduced capability in the informativeness dimension, the model is less capable of differentiating the nuances of less relevant summaries when the summary quality is generally high. Nevertheless, we leave it to future work to determine whether GPT-4's more negative metacorrelation in the relevance dimension could be related to its stronger performance in consistency. We provide more details on the GPT-4 evaluator in Appendix H.</p>
<h2>5 A Temporary Efficient Framework</h2>
<p>Despite the aforementioned limitations, it may be hard to resist the temptation of using LLM evaluators given their superiority over other automatic metrics. In such a case, one should be able to tell when LLM evaluators are more likely to be unreliable and employ further human evaluation when necessary. To this end, we suggest combining the RTS and MCQ scores as a cost-efficient framework. Specifically, we calculate the correlation between RTS and MCQ scores for the $i^{\text {th }}$ candidate system as a reliability indicator:</p>
<p>$$
\begin{aligned}
R_{i}= &amp; \rho\left(\left[f_{\mathrm{RTS}}\left(g_{i, 1}\right), \ldots, f_{\mathrm{RTS}}\left(g_{i, N}\right)\right]\right. \
&amp; \left.\left[f_{\mathrm{MCQ}}\left(g_{i, 1}\right), \ldots, f_{\mathrm{MCQ}}\left(g_{i, N}\right)\right]\right)
\end{aligned}
$$</p>
<p>Then, we can loosely infer that up to a reliability tolerance $r \in(0,1)$, the LLM evaluators (either RTS or MCQ) are reliable if $R_{i}&gt;r$. In other words, given a candidate $i$, if RTS and MCQ agree with each other up to a certain degree of tolerance $r$, we may assume the evaluator is reliable enough to avoid invoking further human evaluation.</p>
<p>To validate this theory, we measure the correlations $\rho\left(R_{i}, P_{i}^{\mathrm{RTS}}\right)$ or $\rho\left(R_{i}, P_{i}^{\mathrm{MCQ}}\right)$, where
$P_{i}^{R T S / M C Q}$ is the performance of either method as defined in Equation (2). Given significantly large positive values of either $\rho\left(R_{i}, P_{i}^{\mathrm{RTS}}\right)$ or $\rho\left(R_{i}, P_{i}^{\mathrm{MCQ}}\right)$, we can then conclude that $R_{i}$ can be used as a reliable indicator for the performance of the corresponding method.</p>
<p>As shown in Table 7, $R_{i}$ demonstrates a significant correlation with $P_{i}^{R T S}$ on both the consistency and fluency dimensions, and with $P_{i}^{M C Q}$ on the coherence and consistency dimensions. This means that if RTS and MCQ generally agree with each other on the candidate's performance on a particular dimension with high $\rho\left(R_{i}, P_{i}^{\mathrm{RTS}}\right)$ (or $\rho\left(R_{i}, P_{i}^{\mathrm{MCQ}}\right)$ ), RTS (or MCQ) is more likely to be human-aligned. Meanwhile, if RTS disagrees with MCQ $\left(R_{i}&lt;r\right)$, further human evaluators are required to provide a conclusive evaluation. We provide $R_{i}$ values for ChatGPT on each evaluated system in Appendix Table 29.</p>
<h2>6 Conclusion</h2>
<p>We explore the potential of using LLMs with different prompting techniques as metrics for abstractive summarization systems. Our extensive analysis suggests that while LLMs like ChatGPT perform better than commonly used automatic metrics across different summarization systems and dimensions, they are still not ready to replace human evaluators because they are candidate- and dimension-dependent, and they do not align well with human when comparing high-quality candidates. Nonetheless, if an LLM evaluator is to be used, we suggest combining multiple evaluation methods as a preliminary indicator to determine whether the metric is likely to be unreliable and whether further human evaluation is required.</p>
<h2>Limitations</h2>
<p>Potential Human Bias. We benchmark the LLM evaluation results against the average of three human expert scores. Naturally, it is possible that these scores may exhibit potential biases of the human experts. Nevertheless, we wish to explore</p>
<p>whether LLM evaluators are aligned with human experts, and may naturally exhibit the same bias as a human would. In other words, we examine whether we can reliably replace human annotators with LLMs, instead of seeking a "perfect" solution that has absolutely zero bias.</p>
<p>Dataset Size. Given the constraints of the small size of the human-annotated SummEval dataset, we could only evaluate 100 summaries generated for each summarization system, with a total of 12 abstractive summarization systems. Since we have observed a significant correlation of LLM evaluations with humans for the consolidated 1200 summaries across all systems, it is possible that with a larger evaluation number, the per-system correlation could also be improved. In addition, given only 12 evaluated systems, our meta-correlation may still be subject to sample biases. We leave more investigations for the future once there are larger annotated datasets.</p>
<p>Prompt tuning. Designing better prompt for LLMs are also ongoing research. Although it is possible that LLMs may act as better evaluators with better prompts, prompt tuning is not our focus. We seek to highlight the limitations of the investigated LLMs and have demonstrated that limitations such as negative meta-correlation are also found with a few other alternative prompts (see Appendix C).</p>
<p>Availability of Commercialized LLM We note that the "gpt-3.5-turbo-0301" snapshot is currently taken down ${ }^{5}$ by OpenAI and replaced with a newer snapshot, "gpt-3.5-turbo-0613". This is also one disadvantage of using out-of-the-box commercialized LLM for summarization evaluations, as the exact checkpoints may not be stably available. As a result, future models may not be fairly compared against previously evaluated models using a different LLM checkpoint. Nevertheless, our paper only seeks to investigate the potential of LLM as an out-of-the-box evaluator, and the OpenAI models are currently one of the strongest. Eventually, we wish to raise awareness of some of the significant limitations found with these LLMs, which need to be resolved before LLMs can be used as direct replacements for human evaluations. In addition, we also note that the cost of evaluating only 100 sum-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>maries for each system is relatively low (around 2 USD per system using ChatGPT). Since LLMs also conduct evaluations much faster than humans (around 2 minutes for LLMs versus 10 hours for human for 100 summaries), it may not pose significant barriers if one was to re-evaluate all compared systems on a single LLM.</p>
<p>Limited Use of the Temporary Solution Unfortunately, our temporary efficient framework doesn't apply to the relevance dimension, where the $R_{i}$ has no significant correlation with the performances of either RTS or MCQ. Moreover, the $r$ value may be dataset-dependent, and it is hard to decide where to draw this line. We leave for future work of developing better methods to gauge the reliability of LLM evaluations.</p>
<h2>Acknowledgements</h2>
<p>Yang You is being sponsored by NUS startup grant (Presidential Young Professorship), Singapore MOE Tier-1 grant, ByteDance grant, ARCTIC grant, SMI grant and Alibaba grant.</p>
<h2>References</h2>
<p>Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, and Rebecca Passonneau. 2015. Abstractive multidocument summarization via phrase selection and merging. In Proceedings of ACL-IJCNLP.</p>
<p>Moye Chen, Wei Li, Jiachen Liu, Xinyan Xiao, Hua Wu, and Haifeng Wang. 2021. Sgsum: Transforming multi-document summarization into sub-graph selection. In Proceedings of EMNLP.</p>
<p>Liying Cheng, Xingxuan Li, and Lidong Bing. 2023. Is gpt-4 a good data analyst? arXiv preprint arXiv:2305.15038.</p>
<p>Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, and Luo Si. 2020. Ent-desc: Entity description generation by exploring knowledge graph. In Proceedings of EMNLP.</p>
<p>Cyril Chhun, Pierre Colombo, Fabian M. Suchanek, and Chloé Clavel. 2022. Of human criteria and automatic metrics: A benchmark of the evaluation of story generation. In Proceedings of Coling.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of ACL.</p>
<p>Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient. Noise reduction in speech processing.</p>
<p>Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Banditsum: Extractive summarization as a contextual bandit. In Proceedings of EMNLP.</p>
<p>Ori Ernst, Avi Caciularu, Ori Shapira, Ramakanth Pasunuru, Mohit Bansal, Jacob Goldberger, and Ido Dagan. 2022. Proposition-level clustering for multidocument summarization. In Proceedings of NAACL.</p>
<p>Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of ACL.</p>
<p>Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. TACL.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of $A C L$.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. In Proceedings of ICML.</p>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554.</p>
<p>Shen Gao, Xiuying Chen, Piji Li, Zhaochun Ren, Lidong Bing, Dongyan Zhao, and Rui Yan. 2019. Abstractive text summarization by incorporating reader comments. In Proceedings of AAAI.</p>
<p>Jian Guan and Minlie Huang. 2020. UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation. In Proceedings of EMNLP.</p>
<p>Junxian He, Wojciech Kryscinski, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2022. CTRLsum: Towards generic controllable text summarization. In Proceedings of EMNLP.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. NeurIPS.</p>
<p>MG Kendall. 1938. A new measure of rank correlation. Biometrika.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. NeurIPS.</p>
<p>Wojciech Kryściński, Romain Paulus, Caiming Xiong, and Richard Socher. 2018. Improving abstraction in text summarization. In Proceedings of EMNLP.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of ACL.</p>
<p>Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017. Deep recurrent generative decoder for abstractive text summarization. In Proceedings of EMNLP.</p>
<p>Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and Junping Du. 2020. Leveraging graph to improve abstractive multi-document summarization. In Proceedings of ACL.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out.</p>
<p>Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In $I C L R$.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv 2303.16634.</p>
<p>Yang Liu and Mirella Lapata. 2019. Hierarchical transformers for multi-document summarization. In Proceedings of $A C L$.</p>
<p>Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb, Chenguang Zhu, Ahmed H Awadallah, and Dragomir Radev. 2022. Leveraging locality in abstractive text summarization. In Proceedings of EMNLP.</p>
<p>Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for abstractive text summarization. arXiv preprint arXiv:2303.15621.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. arXiv preprint arXiv:2301.13379.</p>
<p>Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023a. Zero-shot listwise document reranking with a large language model. arXiv preprint arXiv:2305.02156.</p>
<p>Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023b. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559.</p>
<p>Ani Nenkova. 2006. Summarization evaluation for text and speech: issues and approaches. In Ninth International Conference on Spoken Language Processing.</p>
<p>Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. Mining text data.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv preprint.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. NeurIPS.</p>
<p>Karolina Owczarzak, John Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In Proceedings of workshop on evaluation metrics and system comparison for automatic summarization.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics. In Proceedings of NAACL-HLT.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL.</p>
<p>Swarnadeep Saha, Shiyue Zhang, Peter Hase, and Mohit Bansal. 2022. Summarization programs: Interpretable abstractive summarization with neural modular trees. In $I C L R$.</p>
<p>Chenhui Shen, Liying Cheng, Lidong Bing, Yang You, and Luo Si. 2022a. Sentbs: Sentence-level beam search for controllable summarization. EMNLP.</p>
<p>Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Lidong Bing, and Yang You. 2023. A hierarchical encoding-decoding scheme for abstractive multidocument summarization. In Findings of EMNLP.</p>
<p>Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, and Luo Si. 2022b. Mred: A metareview dataset for structure-controllable text generation. Findings of ACL.</p>
<p>C Spearman. 1987. The proof and measurement of association between two things. American Journal of Psychology.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In $I C L R$.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS.</p>
<p>Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. In Findings of EMNLP.</p>
<p>Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. 2023. Large language models are diverse role-players for summarization evaluation. arXiv preprint arXiv:2303.15078.</p>
<p>Wen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. 2022. Primera: Pyramid-based masked sentence pre-training for multi-document summarization. In Proceedings of ACL.</p>
<p>Jiacheng Xu and Greg Durrett. 2021. Dissecting generation modes for abstractive summarization models via ablation and attribution. In Proceedings of ACLIJCNLP.</p>
<p>Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing Chen, and Jun Xie. 2023. Tailor: A soft-prompt-based approach to attribute-based controlled text generation. In Proceedings of $A C L$.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. NeurIPS.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020a. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of ICML.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020b. Bertscore: Evaluating text generation with bert. In $I C L R$.</p>
<p>Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural document summarization by jointly learning to score and select sentences. In Proceedings of ACL.</p>
<p>Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</p>
<p>Score the following Summary given the corresponding Article with respect to consistency from one to five, where one indicates "inconsistency" and five indicates "perfect consistency". Note that consistency measures the factual alignment between the Summary and the Article, whether the Summary is faithful to the Article without introducing contradictions or misleading representations.</p>
<p>Article: {article}
Summary: ${$ summary $}$
Provide your reason in one sentence, then give a final score:
Table 8: Example prompt for the RTS method on the consistency dimension. Text in {blue}: the specific article and the corresponding summary to be evaluated.</p>
<p>Score the following Summary given the corresponding Article with respect to fluency from one to five, where one indicates "disfluency" and five indicates "perfect fluency". Note that fluency measures the quality of individual sentences in the Summary, whether the Summary is well-written, grammatically correct, and readable on the sentence level.</p>
<p>Article: ${$ article $}$
Summary: ${$ summary $}$
Provide your reason in one sentence, then give a final score:
Table 9: Example prompt for the RTS method on the fluency dimension. Text in ${$ cyan}: the specific article and the corresponding summary to be evaluated.</p>
<p>Score the following Summary given the corresponding Article with respect to coherence from one to five, where one indicates "incoherence" and five indicates "perfect coherence". Note that coherence measures the collective quality of the Summary, whether the Summary presents information that flows smoothly and avoids abrupt transitions or disjoint statements.</p>
<p>Article: ${$ article $}$
Summary: ${$ summary $}$
Provide your reason in one sentence, then give a final score:
Table 10: Example prompt for the RTS method on the coherence dimension. Text in ${$ cyan}: the specific article and the corresponding summary to be evaluated.</p>
<h2>A Evaluation Dimensions</h2>
<p>Fabbri et al. (2021) has defined 4 evaluation dimensions as follows:</p>
<ol>
<li>Coherence: The collective quality of all sentences. The summary should be well-</li>
</ol>
<p>Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to consistency from one to five, where one indicates "inconsistency" and five indicates "perfect consistency". Note that consistency measures the factual alignment between the Summary and the Article, whether the Summary is faithful to the Article without introducing contradictions or misleading representations.</p>
<p>Article: ${$ article $}$
Summary: ${$ summary $}$
A: The Summary is totally inconsistent with the Article. Score: One.
B: The majority of the Summary is inconsistent with the Article. Score: Two.
C: Some information in the Summary is consistent with the Article whereas some are not. Score: Three.
D: The majority of the Summary is consistent with the Article. Score: Four.
E: All information included in the Summary is consistent with the Article. Score: Five.</p>
<p>Your Answer (enter 1 letter from A to E):
Table 11: Example prompt for the MCQ method on the consistency dimension. Text in ${$ cyan}: the specific article and the corresponding summary to be evaluated.
structured and well-organized. The summary should not just be a heap of related information but should build from sentence to sentence to a coherent body of information about a topic.
2. Consistency: The factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document.
3. Fluency: The quality of individual sentences. Sentences in the summary should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.
4. Relevance: Selection of important content from the source. The summary should include only important information from the source document.</p>
<p>We follow the above definitions for designing ChatGPT's evaluation prompts.</p>
<p>Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to fluency from one to five, where one indicates "disfluency" and five indicates "perfect fluency". Note that fluency measures the quality of individual sentences in the Summary, whether the Summary is well-written, grammatically correct, and readable on the sentence level.</p>
<h2>Article: ${$ article }</h2>
<p>Summary: ${$ summary $}$
A: The Summary is totally disfluent. Score: One.
B: The majority of the Summary is disfluent. Score: Two.
C: Some sentences in the Summary are fluent whereas some are not. Score: Three.
D: The majority of the Summary is fluent. Score: Four
E: All sentences in the Summary are fluent. Score: Five.
Your Answer (enter 1 letter from A to E):
Table 12: Example prompt for the MCQ method on the fluency dimension. Text in {cyan}: the specific article and the corresponding summary to be evaluated.</p>
<p>Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to coherence from one to five, where one indicates "incoherence" and five indicates "perfect coherence". Note that coherence measures the collective quality of the Summary, whether the Summary presents information that flows smoothly and avoids abrupt transitions or disjoint statements.</p>
<p>Article: ${$ article }
Summary: ${$ summary $}$
A: The Summary is completely incoherent. Score: One.
B: The Summary is mostly incoherent. Score: Two.
C: The Summary is somewhat coherent. Score: Three.
D: The Summary is mostly coherent. Score: Four.
E: The Summary is completely coherent. Score: Five.
Your Answer (enter 1 letter from A to E):
Table 13: Example prompt for the MCQ method on the coherence dimension. Text in {cyan}: the specific article and the corresponding summary to be evaluated.</p>
<h2>B Prompt Details and Design</h2>
<p>We show the RTS prompts for relevance, consistency, fluency, and coherence in Table 1, Table 8, Table 9, and Table 10 respectively.</p>
<p>We show the MCQ prompts for relevance, consistency, fluency, and coherence in Table 2, Table 11, Table 12, and Table 13 respectively.</p>
<p>We show the H 2 H prompts for relevance, consistency, fluency, and coherence in Table 3, Table 14, Table 15, and Table 16 respectively.</p>
<p>Choose a more consistent summary from Summary #1 and Summary #2 with respect to the corresponding Article by choosing an option from A, B, or C. Note that consistency measures the factual alignment between the summary and the Article, whether the summary is faithful to the Article without introducing contradictions or misleading representations.</p>
<p>Article: ${$ article }
Summary #1: ${$ summary from model A}
Summary #2: ${$ summary from model B}
A: Summary #1 is more consistent.
B: Summary #2 is more consistent.
C: Both Summary #1 and Summary #2 are equally consistent.</p>
<p>Your choice (enter 1 letter from A to C):
Table 14: Example prompt for the H2H method on the consistency dimension. Text in {cyan}: the specific article, and the corresponding summaries generated by a pair of compared models.</p>
<p>Choose a more fluent summary from Summary #1 and Summary #2 with respect to the corresponding Article by choosing an option from A, B, or C. Note that fluency measures the quality of individual sentences in the summary, whether the summary is well-written, grammatically correct, and readable on the sentence level.</p>
<p>Article: ${$ article }
Summary #1: ${$ summary from model A}
Summary #2: ${$ summary from model B}
A: Summary #1 is more fluent.
B: Summary #2 is more fluent.
C: Both Summary #1 and Summary #2 are equally fluent.
Your choice (enter 1 letter from A to C):
Table 15: Example prompt for the H2H method on the fluency dimension. Text in {cyan}: the specific article, and the corresponding summaries generated by a pair of compared models.</p>
<p>To determine the exact definitions used in our prompts for each dimension, we re-use the first sentence from Fabbri et al. (2021)'s definition. We then prompt the LLM to provide a definition for the evaluated dimension, such as "define the word relevance in the context of summarization", then extract the key phrases generated that we believe to fit the definitions of Fabbri et al. (2021) to make up the full definition. We believe this approach may</p>
<p>Choose a more coherent summary from Summary #1 and Summary #2 with respect to the corresponding Article by choosing an option from A, B, or C. Note that coherence measures the collective quality of the summary, whether the summary presents information that flows smoothly and avoids abrupt transitions or disjoint statements.</p>
<p>Article: ${$ article $}$
Summary #1: {summary from model A}
Summary #2: {summary from model B}
A: Summary #1 is more coherent.
B: Summary #2 is more coherent.
C: Both Summary #1 and Summary #2 are equally coherent.
Your choice (enter 1 letter from A to C ):
Table 16: Example prompt for the H2H method on the coherence dimension. Text in ${$ cyan $}$ : the specific article, and the corresponding summaries generated by a pair of compared models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Coh</th>
<th style="text-align: right;">Con</th>
<th style="text-align: right;">Flu</th>
<th style="text-align: right;">Rel</th>
<th style="text-align: right;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT-RTS2</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">6.00</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-MCQ2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">5.25</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT-StarEval</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">6.75</td>
</tr>
</tbody>
</table>
<p>Table 17: Total correct pairs for alternative prompts. Coh: coherence; Con: consistency; Flu: fluency; Rel: relevance.
help the LLM to better evaluate the summaries according to definitions partially generated in its own language. Nevertheless, we didn't invest extensive efforts in prompt designs as this is not our key focus. We also demonstrate that our prompts have better evaluation results than two alternative prompts in Appendix C.</p>
<h2>C Alternative prompts</h2>
<p>We also use ChatGPT to evaluate with the exact prompts from Wang et al. (2023a). We name these prompts "StarEval" since they prompt the LLM to give one to five stars for the summary. In addition, we use ChatGPT to evaluate with alternative prompts for RTS and MCQ by using the full definition as shown in Appendix A instead of supplementing the definition with ChatGPT-generated phrases. We name these two prompts RTS2 and MCQ2 respectively.</p>
<p>We show the results of these alternative prompts in Table 17 and Table 18.</p>
<h2>D Llama 2 Results</h2>
<p>We report the results of using three different sizes of Llama 2 models as LLM evaluators in Table 19. As shown, while the smallest model (7B) exhibits very low correlations with human scores (and only significant on the consistency and relevance dimensions), the larger models (13B and 70B) demonstrate significant correlations with human scores on the full dataset level. However, even the bestperforming 70B model fails to outperform the human correlation of BARTScore, and is completely overwhelmed by the results of ChatGPT and GPT4. This suggests that the open-sourced Llama 2 models are not suitable to be used as zero-shot evaluators. Moreover, all Llama 2 models exhibit significant meta-correlations for at least one dimension.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Coherence</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Consistency</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Fluency</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Relevance</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">RTS2</td>
<td style="text-align: left;">Human-Corr</td>
<td style="text-align: left;">0.339</td>
<td style="text-align: left;">0.338</td>
<td style="text-align: left;">0.285</td>
<td style="text-align: left;">0.393</td>
<td style="text-align: left;">0.497</td>
<td style="text-align: left;">0.350</td>
<td style="text-align: left;">0.290</td>
<td style="text-align: left;">0.280</td>
<td style="text-align: left;">0.252</td>
<td style="text-align: left;">0.440</td>
<td style="text-align: left;">0.441</td>
<td style="text-align: left;">0.351</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Meta-Corr</td>
<td style="text-align: left;">-0.559</td>
<td style="text-align: left;">-0.476</td>
<td style="text-align: left;">-0.424</td>
<td style="text-align: left;">-0.748</td>
<td style="text-align: left;">-0.843</td>
<td style="text-align: left;">-0.576</td>
<td style="text-align: left;">-0.825</td>
<td style="text-align: left;">-0.823</td>
<td style="text-align: left;">-0.636</td>
<td style="text-align: left;">-0.385</td>
<td style="text-align: left;">-0.506</td>
<td style="text-align: left;">-0.212</td>
</tr>
<tr>
<td style="text-align: left;">MCQ2</td>
<td style="text-align: left;">Human-Corr</td>
<td style="text-align: left;">0.430</td>
<td style="text-align: left;">0.423</td>
<td style="text-align: left;">0.355</td>
<td style="text-align: left;">0.327</td>
<td style="text-align: left;">0.483</td>
<td style="text-align: left;">0.306</td>
<td style="text-align: left;">0.258</td>
<td style="text-align: left;">0.396</td>
<td style="text-align: left;">0.229</td>
<td style="text-align: left;">0.240</td>
<td style="text-align: left;">0.258</td>
<td style="text-align: left;">0.206</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Meta-Corr</td>
<td style="text-align: left;">-0.308</td>
<td style="text-align: left;">-0.275</td>
<td style="text-align: left;">-0.212</td>
<td style="text-align: left;">-0.545</td>
<td style="text-align: left;">-0.446</td>
<td style="text-align: left;">-0.364</td>
<td style="text-align: left;">-0.811</td>
<td style="text-align: left;">-0.615</td>
<td style="text-align: left;">-0.636</td>
<td style="text-align: left;">-0.217</td>
<td style="text-align: left;">-0.707</td>
<td style="text-align: left;">-0.182</td>
</tr>
<tr>
<td style="text-align: left;">StarEval</td>
<td style="text-align: left;">Human-Corr</td>
<td style="text-align: left;">0.418</td>
<td style="text-align: left;">0.417</td>
<td style="text-align: left;">0.341</td>
<td style="text-align: left;">0.297</td>
<td style="text-align: left;">0.421</td>
<td style="text-align: left;">0.264</td>
<td style="text-align: left;">0.246</td>
<td style="text-align: left;">0.323</td>
<td style="text-align: left;">0.217</td>
<td style="text-align: left;">0.393</td>
<td style="text-align: left;">0.405</td>
<td style="text-align: left;">0.323</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Meta-Corr</td>
<td style="text-align: left;">-0.064</td>
<td style="text-align: left;">-0.103</td>
<td style="text-align: left;">0.000</td>
<td style="text-align: left;">-0.664</td>
<td style="text-align: left;">-0.684</td>
<td style="text-align: left;">-0.545</td>
<td style="text-align: left;">-0.441</td>
<td style="text-align: left;">-0.400</td>
<td style="text-align: left;">-0.212</td>
<td style="text-align: left;">-0.497</td>
<td style="text-align: left;">-0.579</td>
<td style="text-align: left;">-0.301</td>
</tr>
</tbody>
</table>
<p>Table 18: Results of using alternative prompt with ChatGPT. Light gray values are insignificant ( $p$-value $\geq 0.05$ ). Human-Corr reports the overall correlation of ChatGPT scores with human scores. Meta-corr shows the metacorrelation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Coherence</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Consistency</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Fluency</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Relevance</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
<td style="text-align: left;">Spear.</td>
<td style="text-align: left;">Pear.</td>
<td style="text-align: left;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">7B</td>
<td style="text-align: left;">Human-Corr</td>
<td style="text-align: left;">-0.001</td>
<td style="text-align: left;">-0.000</td>
<td style="text-align: left;">-0.001</td>
<td style="text-align: left;">0.114</td>
<td style="text-align: left;">0.130</td>
<td style="text-align: left;">0.104</td>
<td style="text-align: left;">0.043</td>
<td style="text-align: left;">0.038</td>
<td style="text-align: left;">0.039</td>
<td style="text-align: left;">0.067</td>
<td style="text-align: left;">0.064</td>
<td style="text-align: left;">0.057</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Meta-Corr</td>
<td style="text-align: left;">0.245</td>
<td style="text-align: left;">-0.258</td>
<td style="text-align: left;">0.182</td>
<td style="text-align: left;">-0.524</td>
<td style="text-align: left;">-0.720</td>
<td style="text-align: left;">-0.424</td>
<td style="text-align: left;">-0.042</td>
<td style="text-align: left;">0.463</td>
<td style="text-align: left;">-0.03</td>
<td style="text-align: left;">0.238</td>
<td style="text-align: left;">0.169</td>
<td style="text-align: left;">0.212</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: left;">Human-Corr</td>
<td style="text-align: left;">0.153</td>
<td style="text-align: left;">0.165</td>
<td style="text-align: left;">0.123</td>
<td style="text-align: left;">0.180</td>
<td style="text-align: left;">0.209</td>
<td style="text-align: left;">0.162</td>
<td style="text-align: left;">0.187</td>
<td style="text-align: left;">0.179</td>
<td style="text-align: left;">0.167</td>
<td style="text-align: left;">0.234</td>
<td style="text-align: left;">0.266</td>
<td style="text-align: left;">0.192</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Meta-Corr</td>
<td style="text-align: left;">-0.287</td>
<td style="text-align: left;">-0.425</td>
<td style="text-align: left;">-0.182</td>
<td style="text-align: left;">-0.580</td>
<td style="text-align: left;">-0.495</td>
<td style="text-align: left;">-0.424</td>
<td style="text-align: left;">-0.455</td>
<td style="text-align: left;">-0.233</td>
<td style="text-align: left;">-0.303</td>
<td style="text-align: left;">-0.049</td>
<td style="text-align: left;">-0.406</td>
<td style="text-align: left;">0.000</td>
</tr>
<tr>
<td style="text-align: left;">70B</td>
<td style="text-align: left;">Human-Corr</td>
<td style="text-align: left;">0.234</td>
<td style="text-align: left;">0.254</td>
<td style="text-align: left;">0.186</td>
<td style="text-align: left;">0.357</td>
<td style="text-align: left;">0.395</td>
<td style="text-align: left;">0.319</td>
<td style="text-align: left;">0.155</td>
<td style="text-align: left;">0.161</td>
<td style="text-align: left;">0.134</td>
<td style="text-align: left;">0.248</td>
<td style="text-align: left;">0.285</td>
<td style="text-align: left;">0.200</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Meta-Corr</td>
<td style="text-align: left;">-0.420</td>
<td style="text-align: left;">-0.407</td>
<td style="text-align: left;">-0.273</td>
<td style="text-align: left;">-0.811</td>
<td style="text-align: left;">-0.690</td>
<td style="text-align: left;">-0.667</td>
<td style="text-align: left;">-0.322</td>
<td style="text-align: left;">-0.319</td>
<td style="text-align: left;">-0.182</td>
<td style="text-align: left;">-0.238</td>
<td style="text-align: left;">-0.123</td>
<td style="text-align: left;">-0.182</td>
</tr>
</tbody>
</table>
<p>Table 19: Results of Llama 2 models of 7B, 13B, and 70B RTS correlations. Light gray values are insignificant ( $p$-value $\geq 0.05$ ). Human-Corr reports the overall correlation of LLM scores with human scores. Meta-corr shows the meta-correlation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Coh</th>
<th style="text-align: center;">Con</th>
<th style="text-align: center;">Flu</th>
<th style="text-align: center;">Rel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">M11</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">M22</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.54</td>
</tr>
</tbody>
</table>
<p>Table 20: The standard deviation of human annotations across different summarization systems and evaluation dimensions.</p>
<h2>E Challenging Pairs</h2>
<p>To count the total correct pairs, we only evaluate the challenging pairs, which consist of summarization systems of consecutive performances according to average human scores across all dimensions. Thus, each pair contains 2 summarization systems with the smallest difference in terms of average performance.</p>
<p>For instance, as shown in Table 21, M22 has the best average human score of 4.57 , followed by M23 of 4.55 , then M17 of 4.52 . We thus compare model pairs of "M22-M23" and "M23-M17". The full challenge set is shown in Table 22.</p>
<p>For RTS, MCQ, and all other baseline metrics,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Model Name</th>
<th style="text-align: center;">Coh</th>
<th style="text-align: center;">Con</th>
<th style="text-align: center;">Flu</th>
<th style="text-align: center;">Rel</th>
<th style="text-align: center;">Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">M22</td>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">4.18</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">4.57</td>
</tr>
<tr>
<td style="text-align: left;">M23</td>
<td style="text-align: left;">Pegasus (C4)</td>
<td style="text-align: center;">4.16</td>
<td style="text-align: center;">4.91</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">4.26</td>
<td style="text-align: center;">4.55</td>
</tr>
<tr>
<td style="text-align: left;">M17</td>
<td style="text-align: left;">T5</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">4.52</td>
</tr>
<tr>
<td style="text-align: left;">M12</td>
<td style="text-align: left;">Unified-ext-abs</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">3.85</td>
<td style="text-align: center;">4.32</td>
</tr>
<tr>
<td style="text-align: left;">M13</td>
<td style="text-align: left;">ROUGESal</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">3.83</td>
<td style="text-align: center;">4.24</td>
</tr>
<tr>
<td style="text-align: left;">M15</td>
<td style="text-align: left;">Closed book decoder</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">4.19</td>
</tr>
<tr>
<td style="text-align: left;">M14</td>
<td style="text-align: left;">Multi-task (Ent + QG)</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">4.12</td>
</tr>
<tr>
<td style="text-align: left;">M8</td>
<td style="text-align: left;">Pointer Generator</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">3.55</td>
<td style="text-align: center;">4.07</td>
</tr>
<tr>
<td style="text-align: left;">M9</td>
<td style="text-align: left;">Fast-abs-rl</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">4.50</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">3.77</td>
</tr>
<tr>
<td style="text-align: left;">M10</td>
<td style="text-align: left;">Bottom-Up</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">3.38</td>
<td style="text-align: center;">3.70</td>
</tr>
<tr>
<td style="text-align: left;">M20</td>
<td style="text-align: left;">GPT-2 (zero-shot)</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">3.40</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">3.30</td>
<td style="text-align: center;">3.58</td>
</tr>
<tr>
<td style="text-align: left;">M11</td>
<td style="text-align: left;">Improve-abs</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">3.27</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">3.15</td>
<td style="text-align: center;">3.09</td>
</tr>
</tbody>
</table>
<p>Table 21: The average human evaluation scores of various abstractive summarization models reported by Fabbri et al. (2021). We calculate the average (Avg) score of the reported coherence (Coh), consistency (Con), fluency (Flu), and relevance (Rel) scores. Rows are sorted according to the Avg column values in descending order.
we simply need to compare the evaluated values across all systems, and each metric only needs to evaluate a total of 1200 summaries. However, for H 2 H , we need to evaluate a total of 6,600 summary pairs for the full standard set, and each pair needs to be evaluated twice with different summary positions (see § 3.1), resulting in a total of 13,200 LLM evaluations. Due to a limited budget, we thus only compare a challenge set of 11 pairs, reducing the total required LLM evaluations to 2,200.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model A</td>
<td style="text-align: center;">Model B</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">M22</td>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">55.75</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">58.75</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">48.25</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">43.25</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">49.25</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">56.75</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">48.75</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">51.25</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">49.25</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">49.25</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">58.25</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">61.75</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">63.75</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">M11</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">#CP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 22: #CP calculation for the ChatGPT-H2H metric of Model A over Model B. The numerical values in the middle section columns are aggregated scores for Model A. We omit the value for Model B, which is simply " 100 aggregated scores for Model A". We use " $\checkmark$ " to indicate both LLM and humans prefer the same model, and " $\times$ " otherwise. The model pairs are sorted in descending order according to the average human scores for each model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.438</td>
</tr>
<tr>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.142</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.200</td>
</tr>
<tr>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.284</td>
</tr>
<tr>
<td style="text-align: center;">M11</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.394</td>
</tr>
<tr>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">0.086</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.354</td>
</tr>
<tr>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.037</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">-0.007</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">-0.005</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.336</td>
</tr>
<tr>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.085</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.193</td>
</tr>
<tr>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">-0.086</td>
<td style="text-align: center;">-0.061</td>
<td style="text-align: center;">-0.084</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.237</td>
</tr>
<tr>
<td style="text-align: center;">M22</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">-0.072</td>
<td style="text-align: center;">-0.052</td>
<td style="text-align: center;">-0.070</td>
<td style="text-align: center;">-0.114</td>
<td style="text-align: center;">-0.131</td>
<td style="text-align: center;">-0.101</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.172</td>
</tr>
<tr>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">-0.078</td>
<td style="text-align: center;">-0.022</td>
<td style="text-align: center;">-0.069</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.184</td>
</tr>
</tbody>
</table>
<p>Table 23: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall's Tau (Kend.) between ChatGPT-RTS and human scores on the 100 summaries for each model. Values in light gray color are insignificant (p-value $\geq 0.05$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.302</td>
</tr>
<tr>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.276</td>
</tr>
<tr>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.362</td>
</tr>
<tr>
<td style="text-align: center;">M11</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.334</td>
</tr>
<tr>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">-0.025</td>
<td style="text-align: center;">-0.059</td>
<td style="text-align: center;">-0.024</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.246</td>
</tr>
<tr>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.375</td>
</tr>
<tr>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.110</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.284</td>
</tr>
<tr>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.025</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.378</td>
</tr>
<tr>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">-0.106</td>
<td style="text-align: center;">-0.081</td>
<td style="text-align: center;">-0.105</td>
<td style="text-align: center;">-0.011</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">-0.011</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.260</td>
</tr>
<tr>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.264</td>
</tr>
<tr>
<td style="text-align: center;">M22</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">-0.096</td>
<td style="text-align: center;">-0.080</td>
<td style="text-align: center;">-0.095</td>
<td style="text-align: center;">-0.092</td>
<td style="text-align: center;">-0.076</td>
<td style="text-align: center;">-0.087</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.313</td>
</tr>
<tr>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">-0.275</td>
<td style="text-align: center;">-0.269</td>
<td style="text-align: center;">-0.261</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.129</td>
</tr>
</tbody>
</table>
<p>Table 24: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall's Tau (Kend.) between ChatGPT-MCQ and human scores on the 100 summaries for each model. Values in light gray color are insignificant (p-value $\geq 0.05$ ).</p>
<h2>F Average ChatGPT scores</h2>
<p>We present the average ChatGPT evaluation scores for each model across all dimensions in Table 25. Generally, the same trend holds for the individual systems, that ChatGPT score systems much more
conservatively with RTS, and becomes more optimistic with MCQ.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID</td>
<td style="text-align: center;">Chat-RTS</td>
<td style="text-align: center;">Chat-MCQ</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">Chat-RTS</td>
<td style="text-align: center;">Chat-MCQ</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">Chat-RTS</td>
<td style="text-align: center;">Chat-MCQ</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">human</td>
<td style="text-align: center;">Chat-RTS</td>
<td style="text-align: center;">Chat-MCQ</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">human</td>
</tr>
<tr>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">4.21</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">4.77</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">3.76</td>
<td style="text-align: center;">4.61</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">4.71</td>
<td style="text-align: center;">3.55</td>
</tr>
<tr>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">3.51</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">2.16</td>
<td style="text-align: center;">3.45</td>
<td style="text-align: center;">4.15</td>
<td style="text-align: center;">4.50</td>
<td style="text-align: center;">3.51</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">3.52</td>
</tr>
<tr>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">3.49</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">3.77</td>
<td style="text-align: center;">4.52</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">3.92</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">3.45</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">4.62</td>
<td style="text-align: center;">3.38</td>
</tr>
<tr>
<td style="text-align: center;">M11</td>
<td style="text-align: center;">1.70</td>
<td style="text-align: center;">2.63</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">2.36</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">3.72</td>
<td style="text-align: center;">3.27</td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.77</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">2.92</td>
<td style="text-align: center;">4.06</td>
<td style="text-align: center;">3.87</td>
<td style="text-align: center;">3.15</td>
</tr>
<tr>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">3.92</td>
<td style="text-align: center;">4.75</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">4.75</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">2.59</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">4.72</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">3.89</td>
<td style="text-align: center;">4.40</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">3.85</td>
</tr>
<tr>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">2.91</td>
<td style="text-align: center;">3.99</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">4.40</td>
<td style="text-align: center;">4.73</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">3.93</td>
<td style="text-align: center;">4.69</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">3.90</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">4.85</td>
<td style="text-align: center;">3.83</td>
</tr>
<tr>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">3.87</td>
<td style="text-align: center;">4.54</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">4.25</td>
<td style="text-align: center;">4.70</td>
<td style="text-align: center;">4.99</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">2.42</td>
<td style="text-align: center;">3.83</td>
<td style="text-align: center;">4.53</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">4.32</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">2.65</td>
<td style="text-align: center;">3.81</td>
<td style="text-align: center;">4.58</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">4.33</td>
<td style="text-align: center;">4.75</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">4.61</td>
<td style="text-align: center;">4.80</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">4.32</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">3.05</td>
<td style="text-align: center;">4.11</td>
<td style="text-align: center;">4.89</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">4.87</td>
<td style="text-align: center;">4.97</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">4.08</td>
<td style="text-align: center;">4.79</td>
<td style="text-align: center;">4.93</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">4.43</td>
<td style="text-align: center;">4.97</td>
<td style="text-align: center;">4.23</td>
</tr>
<tr>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">2.95</td>
<td style="text-align: center;">2.99</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">3.57</td>
<td style="text-align: center;">3.51</td>
<td style="text-align: center;">3.40</td>
<td style="text-align: center;">2.22</td>
<td style="text-align: center;">3.02</td>
<td style="text-align: center;">3.05</td>
<td style="text-align: center;">3.97</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">3.47</td>
<td style="text-align: center;">3.30</td>
</tr>
<tr>
<td style="text-align: center;">M22</td>
<td style="text-align: center;">3.54</td>
<td style="text-align: center;">4.09</td>
<td style="text-align: center;">4.92</td>
<td style="text-align: center;">4.18</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">4.97</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">3.57</td>
<td style="text-align: center;">4.06</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">4.41</td>
<td style="text-align: center;">4.43</td>
<td style="text-align: center;">4.95</td>
<td style="text-align: center;">4.25</td>
</tr>
<tr>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">4.14</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">4.16</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">4.91</td>
<td style="text-align: center;">3.08</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">4.76</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">4.45</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">4.26</td>
</tr>
<tr>
<td style="text-align: center;">avg</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">4.35</td>
<td style="text-align: center;">3.35</td>
<td style="text-align: center;">4.06</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">4.68</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">2.59</td>
<td style="text-align: center;">3.66</td>
<td style="text-align: center;">4.29</td>
<td style="text-align: center;">4.61</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">4.29</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">3.72</td>
</tr>
</tbody>
</table>
<p>Table 25: Average scores for ChatGPT using RTS (Chat-RTS) and MCQ (Chat-MCQ), GPT-4 using RTS (GPT-4), as well as the human scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Consistency</td>
<td style="text-align: left;">The summary is inconsistent with the article <br> as it omits important details and repeats a <br> phrase multiple times, resulting in a score of <br> 1.</td>
</tr>
<tr>
<td style="text-align: left;">Consistency</td>
<td style="text-align: left;">The summary includes some relevant informa- <br> tion but lacks coherence and omits important <br> details, resulting in a score of 2.5 out of 5 for <br> consistency.</td>
</tr>
<tr>
<td style="text-align: left;">Fluency</td>
<td style="text-align: left;">The summary is well-written and grammat- <br> ically correct, but lacks important details <br> from the article such as the potential link <br> between antibiotics and weight gain, and <br> the warnings about over-prescription of an- <br> tibiotics and the development of superbugs. <br> Score: 3/5.</td>
</tr>
<tr>
<td style="text-align: left;">Fluency</td>
<td style="text-align: left;">The summary lacks coherence and does not <br> provide a clear understanding of the article, <br> resulting in a score of 1 for disfluency.</td>
</tr>
</tbody>
</table>
<p>Table 26: Examples of wrong reasons generated during RTS by ChatGPT that do not correspond to the evaluated dimension. Bolded: reasons that don't match the evaluated dimension.</p>
<h2>G Incorrect Reasons in RTS</h2>
<p>We illustrate some of the observed incorrect reasons generated with the RTS method by ChatGPT that do not correspond to the evaluated dimension in Table 26. We further provide incorrect reasons generated by GPT-4 in Table 27.</p>
<p>We haven't compiled the exact number of mismatched reasons for either LLM, but we do observe a non-trivial fraction of the evaluations containing unrelated-to-dimension reasoning during preliminary verification. For instance, on the consistency dimension, ChatGPT-RTS has $42 \%$ evaluations containing dimension-irrelevant reasoning for the M11 (avg. score of 2.36) model, and 10\% for the M17 model (avg. score of 4.84).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Coherence</td>
<td style="text-align: left;">The summary is somewhat coherent but in-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">cludes incorrect information, such as stat-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ing that Merson restarted his row with</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Burnley instead of Andros Townsend, and</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">does not flow smoothly; therefore, I would</td>
</tr>
<tr>
<td style="text-align: left;">give it a score of 3.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Consistency</td>
<td style="text-align: left;">The summary is consistent with the article as</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">it mentions the helicopter airlifting patients to</td>
</tr>
<tr>
<td style="text-align: left;">the hospital, but it leaves out important de-</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">tails such as the number of people involved</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">and their injuries. Final score: 3.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Consistency</td>
<td style="text-align: left;">The summary accurately reflects the main</td>
</tr>
<tr>
<td style="text-align: left;">points of the article, although it could have</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">mentioned Falcao's unhappiness in France.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Final score: 4.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fluency</td>
<td style="text-align: left;">The summary is coherent and grammatically</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">correct, but it does not provide enough infor-</td>
</tr>
<tr>
<td style="text-align: left;">mation about the incident. Final score: 3.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fluency</td>
<td style="text-align: left;">The summary is only one sentence and do</td>
</tr>
<tr>
<td style="text-align: left;">not provide enough information about the</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">article's content. Final score: 1.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 27: Examples of wrong reasons generated during RTS by GPT-4 that don't correspond to the evaluated dimension. Bolded: reasons that don't match the evaluated dimension.</p>
<h2>H GPT-4 Evaluator</h2>
<p>We also look into the reasoning of GPT-4 and discover that it makes the same mistakes as ChatGPT by penalizing the summary for reasons unrelated to the evaluated dimension (see Table 27).</p>
<p>Another major difference is that GPT-4 tends to give overly generous scores. In one exceptionally extreme case, GPT-4 gives full scores for all generations by M12 in terms of consistency. Table 25 also shows the much higher average scores given by GPT-4 across all dimensions than those of ChatGPT-RTS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ID</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: center;">M8</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.365</td>
</tr>
<tr>
<td style="text-align: center;">M9</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.358</td>
</tr>
<tr>
<td style="text-align: center;">M10</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.429</td>
</tr>
<tr>
<td style="text-align: center;">M11</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.572</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.405</td>
</tr>
<tr>
<td style="text-align: center;">M12</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.149</td>
</tr>
<tr>
<td style="text-align: center;">M13</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.093</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.206</td>
</tr>
<tr>
<td style="text-align: center;">M14</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">$-0.040$</td>
<td style="text-align: center;">$-0.106$</td>
<td style="text-align: center;">$-0.040$</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.241</td>
</tr>
<tr>
<td style="text-align: center;">M15</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.267</td>
</tr>
<tr>
<td style="text-align: center;">M17</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.185</td>
</tr>
<tr>
<td style="text-align: center;">M20</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.391</td>
</tr>
<tr>
<td style="text-align: center;">M22</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.126</td>
</tr>
<tr>
<td style="text-align: center;">M23</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.168</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.005</td>
</tr>
</tbody>
</table>
<p>Table 28: Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall's Tau (Kend.) between GPT-4 RTS and human scores on the 100 summaries for each model. Values in light gray color are insignificant (p-value $\geq 0.05)$. Note that for the consistency of M12, correlations cannot be calculated because GPT-4 gives 5 scores to all examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ID</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">M8</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.471</td>
</tr>
<tr>
<td style="text-align: left;">M9</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.412</td>
</tr>
<tr>
<td style="text-align: left;">M10</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.459</td>
</tr>
<tr>
<td style="text-align: left;">M11</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.419</td>
</tr>
<tr>
<td style="text-align: left;">M12</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.542</td>
</tr>
<tr>
<td style="text-align: left;">M13</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.475</td>
</tr>
<tr>
<td style="text-align: left;">M14</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.455</td>
</tr>
<tr>
<td style="text-align: left;">M15</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.535</td>
</tr>
<tr>
<td style="text-align: left;">M17</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">$-0.016$</td>
<td style="text-align: center;">$-0.008$</td>
<td style="text-align: center;">$-0.015$</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.504</td>
</tr>
<tr>
<td style="text-align: left;">M20</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.570</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.496</td>
</tr>
<tr>
<td style="text-align: left;">M22</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.451</td>
</tr>
<tr>
<td style="text-align: left;">M23</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.421</td>
</tr>
</tbody>
</table>
<p>Table 29: $R_{t}$, the reliability indicator calculated by the Spearman (Spear.) correlations, Pearson (Pear.) correlations, and Kendall's Tau (Kend.) between ChatGPT-RTS and ChatGPT-MCQ. Values in light gray color are insignificant (p-value $\geq 0.05$ ).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We release all LLM generations involved in our experiments in https://github.com/DAMO-NLP-SG/LLM_summev al as JSON files.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
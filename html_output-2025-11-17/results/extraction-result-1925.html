<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1925 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1925</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1925</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280677209</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.13073v2.pdf" target="_blank">Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1925.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1925.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that co-fine-tunes a large VLM backbone on internet-scale vision-language data together with real robot trajectory data by casting robot actions as text tokens, enabling transfer of web-scale semantic knowledge into robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large VLM backbone (e.g., PaLM-E / PaLI-X variants) co-fine-tuned with robot trajectory data; inputs include images and natural language, outputs action token sequences decoded into continuous robot commands (autoregressive tokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs + additional co-fine-tuning on robot trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Internet-scale image-text data and web-scale vision-language instruction-following datasets (rich in object descriptions, scene context, and language instructions) combined with real robot trajectory data; pretraining contains object and spatial language but not explicit motor primitives beyond the robot trajectory corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (multi-task real-world control)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world robotic manipulation across many everyday tasks (pick/place, object rearrangement); continuous action space represented via discrete action tokens; evaluated on real robots and diverse kitchen/household scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports explicit aim to transfer semantic priors from web data to manipulation; overlap includes object categories, spatial relationships and instruction forms, but exact object/action overlap with robot datasets varies by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports substantially stronger semantic understanding and generalization to novel objects/unseen instructions compared to prior RT-1 baselines (qualitative claims; no single aggregated numeric success rate provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in survey as a single number; RT-2 is compared qualitatively to RT-1 and shown to outperform it on generalization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey does not report direct numeric demonstration-count comparisons for RT-2 vs non-VLM baselines; improvement is attributed to transfer of semantic priors reducing need for robot data but no explicit sample multipliers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention/heatmap analysis reported in survey for RT-2 specifically; broader claims of grounding come from behavior comparisons rather than attention visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit embedding-space clustering / PCA analyses for RT-2 reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Survey highlights the architectural decision to cast actions as language tokens as evidence of semantic-action binding (i.e., treating control as another language output) and demonstrates qualitative transfer to instructions requiring semantic reasoning (e.g., placing an object specified by language).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported for RT-2; it's a single-system autoregressive approach so explicit layerwise feature analysis not described in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when pretraining language/vision data provides semantic coverage of target object names, attributes and spatial relations; co-fine-tuning with robot trajectories improves transfer to robot embodiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey states RT-2 generalizes to novel objects and unseen instructions better than RT-1 but does not provide numeric split performance.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey reports improved zero-shot generalization to novel instructions/objects (qualitative claims), but no explicit zero-shot success percentages presented for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Survey does not report layer-wise ablations for RT-2; other works (e.g., reversible training, LoRA) are discussed elsewhere for efficient adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer examples reported for RT-2 in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey contrasts RT-2 with earlier modular approaches (e.g., RT-1 / CLIPort) and attributes RT-2's superior generalization to joint vision-language pretraining; no explicit head-to-head numeric comparisons to vision-only ImageNet-pretrained encoders are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported for RT-2 in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported for RT-2 in this survey.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1925.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B-parameter VLA model pretrained and fine-tuned on nearly one million real-world robot demonstrations that demonstrates generalist manipulation capabilities and supports efficient adaptation techniques like LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: an open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B parameter VLA using modern vision encoders (DINOv2 + SigLIP) and LLaMA2-7B style LLM components; action autoregressive decoding with discrete action tokenization; trained on large real-robot demonstration corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs + supervised fine-tuning on large-scale real robot demonstration dataset</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Approximately 970k real-world robot demonstrations with paired visual observations and action sequences — contains direct manipulation trajectories, object interactions and language annotations to varying degrees; action semantics present in robot trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>generalist robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task real-world manipulation (open-world pick/place, rearrangement, task sequences); continuous actions tokenized; evaluated across many skills on physical robots; aims at cross-task generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High semantic alignment because the model is fine-tuned on paired robot demonstrations containing actual action sequences and object interactions, improving grounding between language and motor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey states OpenVLA achieves superior generalist manipulation performance relative to earlier baselines and does so with fewer parameters; no single aggregated numeric success rate presented in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not quantitatively reported in survey; OpenVLA is positioned as outperforming smaller/non-pretrained baselines qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes OpenVLA supports efficient fine-tuning on consumer hardware via LoRA but does not report explicit numeric sample-efficiency comparisons (e.g., demonstrations required).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention visualization or analysis reported for OpenVLA in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in the survey for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Evidence comes from large-scale real demonstration fine-tuning and reported generalist performance; survey emphasizes the model's ability to map language instructions to action-token sequences learned from real demonstrations as implicit grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not explicitly analyzed; OpenVLA uses unified autoregressive decoding rather than explicit hierarchical planners.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Fine-tuning on diverse real demonstrations and use of modern vision encoders improve cross-scenario transfer; LoRA-based adaptation facilitates efficient domain transfer on consumer hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey claims improved generalist behavior including unseen objects, but no numeric breakdown between novel vs familiar objects provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>OpenVLA is reported as a generalist that can be adapted efficiently; survey does not provide exact zero/few-shot numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Survey mentions LoRA and fine-tuning strategies but does not provide a detailed layer ablation for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer cases for OpenVLA reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey frames OpenVLA as benefiting from joint VLM pretraining and real robot data; no direct numeric comparisons to vision-only pretrained models included.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported for OpenVLA.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1925.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pi-zero (π0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-system vision-language-action model that initializes a VLM backbone and adds a flow-matching action expert trained on diverse dexterous robot datasets, achieving strong zero-shot generalization and easy adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>π0 : A visionlanguage-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-system architecture: pretrained VLM backbone (PaliGemma / Gemma family) provides perception and reasoning, while an independently trained flow-matching transformer serves as System 1 action expert; shared-attention or MoE-style interactions are used in variants.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for backbone (web-scale image-text), action expert trained with flow-matching on robot trajectory datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Backbone: web-scale image-text corpora and visual-instruction datasets; action expert: diverse robot trajectory datasets (including simulated and real dexterous manipulation) — contains action sequences, object interactions and affordance-like behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>general robot control / dexterous manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-skill robot control spanning contact-rich and dexterous tasks, trained on diverse robot data; action space continuous but modeled by flow-matching latent action processes.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey reports co-training and design choices intended to align VLM semantic understanding with action expert inputs; the flow-matching expert helps bridge visual semantics to motor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey states π0 yields strong zero-shot generalization and good transfer; no single benchmark numbers are tabulated in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported numerically; survey emphasizes π0's advantage relative to models without large VLM backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey notes easy adaptation and strong zero-shot behavior implying better sample efficiency for downstream fine-tuning, but does not provide numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention-map analyses reported for π0 in survey; architecture-level discussion focuses on shared-attention and MoE designs.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not presented in survey for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Action grounding is supported by architecture (separate action expert conditioned on VLM outputs) and empirical claims of zero-shot performance; the model design (flow-matching latent actions) is proposed as an interface that structures grounding between visual semantics and continuous control.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey discusses separation of reasoning (VLM) and action (expert) but does not report layerwise feature studies; π0 variants (π0.5, π0-KI) include training strategies to preserve backbone knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Success depends on heterogeneity of training robot datasets and semantic coverage in the VLM; preventing gradient flow from action expert to VLM (π0.5-KI) is suggested to preserve transfer benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey reports π0 shows zero-shot generalization to novel tasks/objects but provides no numeric split.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Strong zero-shot generalization claimed; no precise success-rate numbers provided in survey for specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Variants such as π0.5-KI implement gradient-blocking from action expert to backbone to protect pretrained knowledge — an architectural layer/gradient-control insight reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey notes the risk of backbone knowledge degradation if gradients flow from action expert into the VLM and cites π0.5-KI's prevention of such gradient flow as mitigation, implying potential negative transfer when not controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey asserts advantage over modular vision-only pipelines but does not present head-to-head numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported in survey for π0 beyond architectural  'fast/slow' separation.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1925.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier robotics transformer that maps camera images and text instructions to motor actions using a CNN-based visual encoder conditioned on language embeddings; used as a baseline that RT-2 and later VLA models improve upon.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: robotics transformer for real-world control at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based model using a CNN visual encoder plus separate language embedding (universal sentence encoder) with multi-task imitation learning to map observations and language to actions; actions discretized as tokens in some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-only CNN pretraining (standard image pretraining) + supervision on multi-task robot datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on multi-task robot manipulation data (datasets like BC-Z and others); lacks web-scale vision-language pretraining used by later VLM-based VLAs.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>real-world robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task manipulation and pick/place tasks on physical robots; continuous action space mapped by supervised imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Limited compared to VLM-based models according to survey; language grounding achieved via separate language embeddings rather than unified VLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>N/A (RT-1 did not use web-scale VLM pretraining); survey positions RT-2 as outperforming RT-1 on generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>RT-1 baseline performance available in original RT-1 paper but survey does not reproduce numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Survey suggests RT-1 required more robot data for generalization compared to VLM-pretrained approaches, but no numerical comparison given.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis reported in this survey for RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-1 achieves grounded behavior through supervised mapping from language+vision to actions, but survey describes its grounding as more brittle than VLM-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Generalization limited when encountering unseen objects or ambiguous instructions; improvement requires more demonstration data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey indicates RT-1 struggles with novel objects relative to VLM-based methods; no numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Limited zero-shot/few-shot capability compared to later VLM-based VLAs.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RT-1 uses vision+language but without large VLM pretraining; survey contrasts it unfavorably with RT-2's VLM-enhanced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1925.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular language-conditioned manipulation system that uses CLIP as a vision-language encoder coupled with a Transporter network for spatial reasoning, demonstrating early semantic grounding for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: what and where pathways for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modular pipeline combining CLIP-based semantic grounding for visual tokens and a Transporter architecture for spatial action prediction; separates perception and policy modules rather than end-to-end VLM-fused models.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (CLIP on image-text pairs) for perception encoder; policy trained on robot demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>CLIP pretraining on image-text pairs (web-scale) yields semantic object embeddings, while action datasets used for Transporter are robot-specific demonstrations containing object interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>tabletop robotic manipulation / pick-and-place</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Simulated and real-world planar tabletop pick/place and rearrangement tasks with relatively constrained action spaces; discrete spatial action proxies used by Transporter.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment for object labels and categories through CLIP; supports grounding of nouns and attributes but less integrated for compositional or long-horizon instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey refers to CLIPort as effective early approach; specific numeric success rates are not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Modular CLIP-based approaches were more sample efficient for semantic grounding compared to purely vision-only methods in original literature, but survey does not provide numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis for CLIPort provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Survey notes CLIP's strong cross-modal embedding alignment which CLIPort leverages, but no new embedding analyses shown.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>CLIPort demonstrates grounding of object names/attributes to pick locations via CLIP embeddings and Transporter spatial reasoning — cited as motivating early language-conditioned manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not applicable; CLIPort is modular rather than hierarchical in the survey taxonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works well when object labels and visual appearance match CLIP pretraining distributions; struggles when objects are out-of-distribution visually or when complex multi-step instructions are required.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey indicates CLIP-based modular methods can fail on novel appearances or unseen compositions; numeric gap not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Some zero-shot object recognition benefits from CLIP; action-level zero-shot limited without policy examples.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported in survey, though modular design limitations discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP-based perception outperforms many vision-only encoders at semantic grounding, per survey discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1925.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DexVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DexVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLM planner coupled with a diffusion-based action policy (plug-in diffusion expert) that excels in long-horizon and complex robotic manipulation tasks by conditioning actions on subtask tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dexvla: vision-language model with plug-in diffusion expert for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DexVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hierarchical planner+policy: a large VLM planner generates subtask tokens and a large diffusion-based action policy (diffusion expert) produces detailed continuous action sequences conditioned on the subtask tokens; supports long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLM pretraining for planner (vision-language on image-text); diffusion policy trained on robot trajectories (action sequences) potentially pre-trained on large-scale action data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Planner: web-scale vision-language corpora and instruction-finetuning; diffusion policy: robot trajectory datasets containing sequential action patterns and dynamics (contact-rich behaviors included depending on dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>long-horizon robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-stage manipulation tasks in simulation and real-world where tasks are decomposed into subtasks and executed by diffusion policy; continuous action trajectories with contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicit — planner produces language-like subtasks aligned with VLM semantics which condition the action diffusion policy for grounded execution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey claims DexVLA excels on complex long-horizon tasks and outperforms some baselines (qualitative); no universal numeric metrics reported inside survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not quantitatively reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported numerically; design suggests improved efficiency for long-horizon planning through decomposition but no demonstration-count metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention visualization reported in survey for DexVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Planner-to-policy conditioning (subtask tokens) provides an explicit semantic-to-action interface; survey presents this architectural design as evidence of language-grounded motor generation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Yes — the model has explicit planner (high-level language/subtask signals) and policy (low-level trajectories) separation; survey emphasizes this as beneficial for long-horizon consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer benefits when planner vocabulary and subtask abstractions align with downstream policy training data; misalignment between planner subtasks and policy capabilities can reduce transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Survey does not report numeric comparisons; model is designed to generalize via language-level subtask abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests improved few-shot adaptation via subtask decomposition but does not provide numeric few-shot counts or success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No detailed layer-wise analysis reported in survey for DexVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported for DexVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey positions planner+diffusion as superior to purely vision-only pipelines for complex tasks due to language-mediated decomposition, but no direct numeric comparison presented.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Diffusion policy models temporal trajectories explicitly; survey highlights this architecture for temporally-structured manipulation but gives no temporal learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1925.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WorldVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WorldVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA model that integrates an autoregressive world model with action generation within a unified token-based architecture to capture physical dynamics and improve long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Worldvla: towards autoregressive action world model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WorldVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified autoregressive token model that jointly models visual outcomes (world model predictions) and action tokens; combines visual imagination with action generation to improve fidelity to physical dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal video/image pretraining for visual/world modeling plus action-token supervision from robot trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large unlabeled videos and robot trajectory data; includes scene dynamics and object motions but degree of explicit affordance/action-language content depends on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>long-horizon manipulation and planning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks requiring prediction of future visual states and sequential motor actions; can be evaluated in simulation and real-world; action space continuous but tokenized for autoregressive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Survey emphasizes joint learning of world dynamics and action tokens improves semantic alignment between visual predictions and motor outputs by enforcing consistency between imagined outcomes and executed actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey claims improved long-horizon planning and action fidelity through integrated world/action modeling; no single numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported numerically; world-model based approaches are described as improving planning efficiency by simulating outcomes (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention or saliency analyses reported specifically for WorldVLA in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>By jointly modeling predicted visual outcomes and actions, WorldVLA provides architectural evidence of linking action sequences to expected perceptual consequences — cited as stronger grounding mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>WorldVLA integrates implicit hierarchical reasoning via imagination but does not produce explicit human-interpretable intermediate programs/waypoints in all variants.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works better when video/world-model pretraining covers dynamics similar to target environments; domain mismatch limits zero-shot planning fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically; survey suggests world-models can help generalize to novel object dynamics if visual dynamics are learned.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>World-generated subgoal images (in other works) are used for zero-shot manipulation in some reported cases, but WorldVLA numeric zero-shot numbers not listed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Survey does not provide explicit negative transfer examples for WorldVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey suggests world-model integration provides benefits beyond vision-only perception for planning, but no head-to-head numbers included.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Explicitly models temporal dynamics via autoregressive video/state prediction; survey highlights this as a core advantage though quantitative temporal learning curves are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1925.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>4D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>4D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA pretraining approach that integrates spatiotemporal (3D + time) cues into visual features, uses memory bank sampling of historical keyframes, and aims to resolve spatial coordinate inconsistency for better spatiotemporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>4d-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>4D-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretraining paradigm and model variants that add 3D coordinates and temporal memory sampling to VLA architectures to incorporate 4D scene understanding; typically feeds 3D/temporal tokens into VLM/LLM fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal video-text / spatiotemporal pretraining (video + reconstructed 3D) plus fine-tuning on embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale videos or multi-view sequences with reconstructed 3D and temporal continuity; contains object motion, interactions and temporal affordance cues.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>spatiotemporal robotic manipulation and temporal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation tasks that require temporal context and 3D spatial reasoning (moving objects, dynamic scenes); environment: simulated and/or real with depth point clouds.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed specifically to improve alignment of spatial and temporal information with language instructions (e.g., 'after moving X do Y'); survey emphasizes the modality alignment improvements but no numeric overlaps reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey states 4D-VLA improves spatiotemporal reasoning and certain task success in manipulation settings relative to 2D-only variants, but does not provide aggregated numerical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not explicitly quantified in survey for 4D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention maps or analyses reported in the survey for 4D-VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Adding explicit 3D and temporal tokens provides architectural grounding cues linking language temporal predicates to observed motion and depth-informed interaction points; survey reports qualitative improvements in spatiotemporal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Survey suggests higher-level temporal/3D features are enriched, aiding planners and policies, but lacks layerwise empirical breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when the pretraining video+3D data has dynamics and viewpoints similar to target embodied tasks; heavy domain mismatch reduces benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey implies improved few-shot spatiotemporal transfer but provides no numeric counts.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Survey frames 4D-pretrained models as superior to 2D-only vision models for temporal and dynamic manipulation tasks, though no numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Central to 4D-VLA; model explicitly uses memory banks and keyframe sampling to combat temporal ambiguity; no learning-curve numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1925.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BridgeVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BridgeVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that reconstructs 3D point clouds from RGB-D and generates orthographic projections to present 3D 'space' information in a 2D format compatible with large VLMs, enabling efficient 3D-aware manipulation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridgevla: input-output alignment for efficient 3d manipulation learning with vision-language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BridgeVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses RGB-D reconstruction to produce 3D point clouds and three orthographic projections, projecting 3D spatial information into 2D views that can be consumed by standard VLM encoders; compatible with VLM tokenization pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for VLMs (image-text) plus RGB-D / point-cloud-based fine-tuning for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>RGB-D paired data and reconstructed 3D point clouds with manipulation interactions; includes spatial relationships and affordance-like cues implicitly via demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>3D-aware robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation in environments where depth and 3D spatial relationships are critical; action space continuous; evaluated on tasks requiring accurate 3D reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>BridgeVLA explicitly seeks to align 3D perceptual inputs with VLMs' 2D processing by encoding spatial info as orthographic 2D projections, increasing semantic overlap between pretraining modalities and target 3D tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports that BridgeVLA improves efficiency and 3D manipulation performance relative to purely 2D approaches, though no aggregate numeric performance figures are given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported numerically in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified; method framed as more efficient due to compatibility with 2D VLM encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analyses for BridgeVLA reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Architectural projection of 3D geometry into VLM-readable 2D views provides stronger perceptual affordance cues that can better ground language instructions referring to spatial relations; evidence described qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported in terms of layer-specific benefits, but BridgeVLA specifically targets improving spatial features available to VLM planners/policies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Benefits when RGB-D or depth information is available and when downstream policies are compatible with orthographic 2D representations; domain mismatch in depth quality can reduce transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey suggests improved generalization to 3D tasks but provides no zero/few-shot numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Claimed to improve over 2D-only VLM-based VLAs for 3D tasks, no numeric comparisons included.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>BridgeVLA focuses on spatial 3D alignment; temporal aspects not central in description.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1925.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-system VLA approach that overlays sampled motion-point trajectories onto current images to create explicit temporal 'trace' information, improving spatiotemporal awareness for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Augments visual input with overlaid historical motion-point traces to provide temporal context; integrates these tokens into VLM/LLM reasoning to produce temporally informed action predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining plus fine-tuning with temporally augmented trajectory data</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Robot trajectory sequences with temporally sampled motion points; contains temporal dynamics and object interaction sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>temporally-sensitive robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation tasks where motion history and dynamics matter (e.g., cloth manipulation, moving targets); continuous action space tokenized for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>By explicitly providing motion traces, TraceVLA increases overlap between pretraining temporal dynamics and target tasks requiring temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey reports TraceVLA equips VLMs with improved spatiotemporal understanding and improves performance on certain manipulation tasks, but no consolidated numeric metrics are given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map analyses reported; design is described at input-augmentation level.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Inclusion of motion traces supplies explicit signals linking observed dynamics to expected action sequences, providing mechanism-level evidence for temporal grounding though quantified grounding experiments are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Enhances temporal features available to planner/policy; no layerwise empirical breakdown presented.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Helpful when tasks require temporal disambiguation; limited benefit when tasks are static or short-horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Survey implies improved generalization for temporally structured tasks but lacks numeric zero/few-shot results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Provides temporal cues that 2D static methods lack; survey positions this as an advantage but gives no numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Central to design; model explicitly encodes historical motion to disambiguate current state.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1925.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1925.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal language model that integrates visual perception into an LLM for robot command generation and planning; used as a VLM backbone in subsequent VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: an embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model augmented with visual encoder inputs to enable multimodal reasoning and generation for embodied tasks; produces textual plans or action tokens conditioned on images and other sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>text-only LLM pretraining augmented by multimodal fine-tuning with image-text and embodied datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Massive text corpora for LLM pretraining combined with image-text and robot trajectory finetuning; includes object descriptions, instructions and some embodied action sequences for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>embodied instruction following and robotic planning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>High-level planning and command generation for robot manipulation; typically outputs symbolic or tokenized action plans which are fed to downstream controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to align language model knowledge with visual perception and robot states; survey cites PaLM-E as a key example of integrating LLMs into robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Survey references PaLM-E demonstrating feasibility of unifying VQA capabilities with robot command generation but does not give a single metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported within survey.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided in survey for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not discussed in survey for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not discussed in survey for PaLM-E.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>PaLM-E demonstrates action generation conditioned on visual inputs providing initial evidence that LLM knowledge can be grounded into robot commands; survey treats it as foundational prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>PaLM-E functions at higher planning/command generation levels; explicit low-level policy analysis not in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Benefit when fine-tuned with robot-specific data; purely LLM-trained knowledge alone insufficient for low-level motor control.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>PaLM-E shows some cross-task capability due to LLM priors but the survey does not quantify.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>PaLM-E improves over text-only or vision-only baselines by fusing modalities, per survey discussion, but numbers are not reproduced.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not central in PaLM-E description in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Openvla: an open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>π0 : A visionlanguage-action flow model for general robot control <em>(Rating: 2)</em></li>
                <li>Dexvla: vision-language model with plug-in diffusion expert for general robot control <em>(Rating: 2)</em></li>
                <li>Worldvla: towards autoregressive action world model <em>(Rating: 2)</em></li>
                <li>4d-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration <em>(Rating: 2)</em></li>
                <li>Cliport: what and where pathways for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Palm-e: an embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Tracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies <em>(Rating: 2)</em></li>
                <li>Bridgevla: input-output alignment for efficient 3d manipulation learning with vision-language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1925",
    "paper_id": "paper-280677209",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "A vision-language-action model that co-fine-tunes a large VLM backbone on internet-scale vision-language data together with real robot trajectory data by casting robot actions as text tokens, enabling transfer of web-scale semantic knowledge into robot control.",
            "citation_title": "Rt-2: vision-language-action models transfer web knowledge to robotic control",
            "mention_or_use": "use",
            "model_name": "RT-2",
            "model_description": "Large VLM backbone (e.g., PaLM-E / PaLI-X variants) co-fine-tuned with robot trajectory data; inputs include images and natural language, outputs action token sequences decoded into continuous robot commands (autoregressive tokenization).",
            "pretraining_type": "vision-language on image-text pairs + additional co-fine-tuning on robot trajectories",
            "pretraining_data_description": "Internet-scale image-text data and web-scale vision-language instruction-following datasets (rich in object descriptions, scene context, and language instructions) combined with real robot trajectory data; pretraining contains object and spatial language but not explicit motor primitives beyond the robot trajectory corpus.",
            "target_task_name": "robotic manipulation (multi-task real-world control)",
            "target_task_description": "Real-world robotic manipulation across many everyday tasks (pick/place, object rearrangement); continuous action space represented via discrete action tokens; evaluated on real robots and diverse kitchen/household scenarios.",
            "semantic_alignment": "Survey reports explicit aim to transfer semantic priors from web data to manipulation; overlap includes object categories, spatial relationships and instruction forms, but exact object/action overlap with robot datasets varies by dataset.",
            "performance_with_language_pretraining": "Survey reports substantially stronger semantic understanding and generalization to novel objects/unseen instructions compared to prior RT-1 baselines (qualitative claims; no single aggregated numeric success rate provided in survey).",
            "performance_without_language_pretraining": "Not reported in survey as a single number; RT-2 is compared qualitatively to RT-1 and shown to outperform it on generalization tasks.",
            "sample_efficiency_comparison": "Survey does not report direct numeric demonstration-count comparisons for RT-2 vs non-VLM baselines; improvement is attributed to transfer of semantic priors reducing need for robot data but no explicit sample multipliers provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No detailed attention/heatmap analysis reported in survey for RT-2 specifically; broader claims of grounding come from behavior comparisons rather than attention visualization.",
            "embedding_space_analysis": "No explicit embedding-space clustering / PCA analyses for RT-2 reported in the survey.",
            "action_grounding_evidence": "Survey highlights the architectural decision to cast actions as language tokens as evidence of semantic-action binding (i.e., treating control as another language output) and demonstrates qualitative transfer to instructions requiring semantic reasoning (e.g., placing an object specified by language).",
            "hierarchical_features_evidence": "Not reported for RT-2; it's a single-system autoregressive approach so explicit layerwise feature analysis not described in survey.",
            "transfer_conditions": "Works best when pretraining language/vision data provides semantic coverage of target object names, attributes and spatial relations; co-fine-tuning with robot trajectories improves transfer to robot embodiments.",
            "novel_vs_familiar_objects": "Survey states RT-2 generalizes to novel objects and unseen instructions better than RT-1 but does not provide numeric split performance.",
            "zero_shot_or_few_shot": "Survey reports improved zero-shot generalization to novel instructions/objects (qualitative claims), but no explicit zero-shot success percentages presented for RT-2.",
            "layer_analysis": "Survey does not report layer-wise ablations for RT-2; other works (e.g., reversible training, LoRA) are discussed elsewhere for efficient adaptation.",
            "negative_transfer_evidence": "No explicit negative transfer examples reported for RT-2 in the survey.",
            "comparison_to_vision_only": "Survey contrasts RT-2 with earlier modular approaches (e.g., RT-1 / CLIPort) and attributes RT-2's superior generalization to joint vision-language pretraining; no explicit head-to-head numeric comparisons to vision-only ImageNet-pretrained encoders are provided.",
            "temporal_dynamics": "Not reported for RT-2 in this survey.",
            "dimensionality_analysis": "Not reported for RT-2 in this survey.",
            "uuid": "e1925.0"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source 7B-parameter VLA model pretrained and fine-tuned on nearly one million real-world robot demonstrations that demonstrates generalist manipulation capabilities and supports efficient adaptation techniques like LoRA.",
            "citation_title": "Openvla: an open-source vision-language-action model",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "7B parameter VLA using modern vision encoders (DINOv2 + SigLIP) and LLaMA2-7B style LLM components; action autoregressive decoding with discrete action tokenization; trained on large real-robot demonstration corpus.",
            "pretraining_type": "vision-language on image-text pairs + supervised fine-tuning on large-scale real robot demonstration dataset",
            "pretraining_data_description": "Approximately 970k real-world robot demonstrations with paired visual observations and action sequences — contains direct manipulation trajectories, object interactions and language annotations to varying degrees; action semantics present in robot trajectories.",
            "target_task_name": "generalist robotic manipulation",
            "target_task_description": "Multi-task real-world manipulation (open-world pick/place, rearrangement, task sequences); continuous actions tokenized; evaluated across many skills on physical robots; aims at cross-task generalization.",
            "semantic_alignment": "High semantic alignment because the model is fine-tuned on paired robot demonstrations containing actual action sequences and object interactions, improving grounding between language and motor outputs.",
            "performance_with_language_pretraining": "Survey states OpenVLA achieves superior generalist manipulation performance relative to earlier baselines and does so with fewer parameters; no single aggregated numeric success rate presented in survey.",
            "performance_without_language_pretraining": "Not quantitatively reported in survey; OpenVLA is positioned as outperforming smaller/non-pretrained baselines qualitatively.",
            "sample_efficiency_comparison": "Survey notes OpenVLA supports efficient fine-tuning on consumer hardware via LoRA but does not report explicit numeric sample-efficiency comparisons (e.g., demonstrations required).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention visualization or analysis reported for OpenVLA in the survey.",
            "embedding_space_analysis": "Not reported in the survey for OpenVLA.",
            "action_grounding_evidence": "Evidence comes from large-scale real demonstration fine-tuning and reported generalist performance; survey emphasizes the model's ability to map language instructions to action-token sequences learned from real demonstrations as implicit grounding.",
            "hierarchical_features_evidence": "Not explicitly analyzed; OpenVLA uses unified autoregressive decoding rather than explicit hierarchical planners.",
            "transfer_conditions": "Fine-tuning on diverse real demonstrations and use of modern vision encoders improve cross-scenario transfer; LoRA-based adaptation facilitates efficient domain transfer on consumer hardware.",
            "novel_vs_familiar_objects": "Survey claims improved generalist behavior including unseen objects, but no numeric breakdown between novel vs familiar objects provided.",
            "zero_shot_or_few_shot": "OpenVLA is reported as a generalist that can be adapted efficiently; survey does not provide exact zero/few-shot numeric results.",
            "layer_analysis": "Survey mentions LoRA and fine-tuning strategies but does not provide a detailed layer ablation for OpenVLA.",
            "negative_transfer_evidence": "No explicit negative transfer cases for OpenVLA reported in the survey.",
            "comparison_to_vision_only": "Survey frames OpenVLA as benefiting from joint VLM pretraining and real robot data; no direct numeric comparisons to vision-only pretrained models included.",
            "temporal_dynamics": "Not reported for OpenVLA.",
            "dimensionality_analysis": "Not reported for OpenVLA.",
            "uuid": "e1925.1"
        },
        {
            "name_short": "π0",
            "name_full": "pi-zero (π0)",
            "brief_description": "A dual-system vision-language-action model that initializes a VLM backbone and adds a flow-matching action expert trained on diverse dexterous robot datasets, achieving strong zero-shot generalization and easy adaptation.",
            "citation_title": "π0 : A visionlanguage-action flow model for general robot control",
            "mention_or_use": "use",
            "model_name": "π0",
            "model_description": "Dual-system architecture: pretrained VLM backbone (PaliGemma / Gemma family) provides perception and reasoning, while an independently trained flow-matching transformer serves as System 1 action expert; shared-attention or MoE-style interactions are used in variants.",
            "pretraining_type": "vision-language pretraining for backbone (web-scale image-text), action expert trained with flow-matching on robot trajectory datasets",
            "pretraining_data_description": "Backbone: web-scale image-text corpora and visual-instruction datasets; action expert: diverse robot trajectory datasets (including simulated and real dexterous manipulation) — contains action sequences, object interactions and affordance-like behavior.",
            "target_task_name": "general robot control / dexterous manipulation",
            "target_task_description": "Multi-skill robot control spanning contact-rich and dexterous tasks, trained on diverse robot data; action space continuous but modeled by flow-matching latent action processes.",
            "semantic_alignment": "Survey reports co-training and design choices intended to align VLM semantic understanding with action expert inputs; the flow-matching expert helps bridge visual semantics to motor outputs.",
            "performance_with_language_pretraining": "Survey states π0 yields strong zero-shot generalization and good transfer; no single benchmark numbers are tabulated in the survey text.",
            "performance_without_language_pretraining": "Not reported numerically; survey emphasizes π0's advantage relative to models without large VLM backbones.",
            "sample_efficiency_comparison": "Survey notes easy adaptation and strong zero-shot behavior implying better sample efficiency for downstream fine-tuning, but does not provide numeric comparisons.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention-map analyses reported for π0 in survey; architecture-level discussion focuses on shared-attention and MoE designs.",
            "embedding_space_analysis": "Not presented in survey for π0.",
            "action_grounding_evidence": "Action grounding is supported by architecture (separate action expert conditioned on VLM outputs) and empirical claims of zero-shot performance; the model design (flow-matching latent actions) is proposed as an interface that structures grounding between visual semantics and continuous control.",
            "hierarchical_features_evidence": "Survey discusses separation of reasoning (VLM) and action (expert) but does not report layerwise feature studies; π0 variants (π0.5, π0-KI) include training strategies to preserve backbone knowledge.",
            "transfer_conditions": "Success depends on heterogeneity of training robot datasets and semantic coverage in the VLM; preventing gradient flow from action expert to VLM (π0.5-KI) is suggested to preserve transfer benefits.",
            "novel_vs_familiar_objects": "Survey reports π0 shows zero-shot generalization to novel tasks/objects but provides no numeric split.",
            "zero_shot_or_few_shot": "Strong zero-shot generalization claimed; no precise success-rate numbers provided in survey for specific tasks.",
            "layer_analysis": "Variants such as π0.5-KI implement gradient-blocking from action expert to backbone to protect pretrained knowledge — an architectural layer/gradient-control insight reported.",
            "negative_transfer_evidence": "Survey notes the risk of backbone knowledge degradation if gradients flow from action expert into the VLM and cites π0.5-KI's prevention of such gradient flow as mitigation, implying potential negative transfer when not controlled.",
            "comparison_to_vision_only": "Survey asserts advantage over modular vision-only pipelines but does not present head-to-head numeric comparisons.",
            "temporal_dynamics": "Not reported in survey for π0 beyond architectural  'fast/slow' separation.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.2"
        },
        {
            "name_short": "RT-1",
            "name_full": "RT-1",
            "brief_description": "An earlier robotics transformer that maps camera images and text instructions to motor actions using a CNN-based visual encoder conditioned on language embeddings; used as a baseline that RT-2 and later VLA models improve upon.",
            "citation_title": "Rt-1: robotics transformer for real-world control at scale",
            "mention_or_use": "mention",
            "model_name": "RT-1",
            "model_description": "Transformer-based model using a CNN visual encoder plus separate language embedding (universal sentence encoder) with multi-task imitation learning to map observations and language to actions; actions discretized as tokens in some variants.",
            "pretraining_type": "vision-only CNN pretraining (standard image pretraining) + supervision on multi-task robot datasets",
            "pretraining_data_description": "Trained on multi-task robot manipulation data (datasets like BC-Z and others); lacks web-scale vision-language pretraining used by later VLM-based VLAs.",
            "target_task_name": "real-world robotic manipulation",
            "target_task_description": "Multi-task manipulation and pick/place tasks on physical robots; continuous action space mapped by supervised imitation learning.",
            "semantic_alignment": "Limited compared to VLM-based models according to survey; language grounding achieved via separate language embeddings rather than unified VLM alignment.",
            "performance_with_language_pretraining": "N/A (RT-1 did not use web-scale VLM pretraining); survey positions RT-2 as outperforming RT-1 on generalization.",
            "performance_without_language_pretraining": "RT-1 baseline performance available in original RT-1 paper but survey does not reproduce numbers.",
            "sample_efficiency_comparison": "Survey suggests RT-1 required more robot data for generalization compared to VLM-pretrained approaches, but no numerical comparison given.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis reported in this survey for RT-1.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "RT-1 achieves grounded behavior through supervised mapping from language+vision to actions, but survey describes its grounding as more brittle than VLM-based approaches.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Generalization limited when encountering unseen objects or ambiguous instructions; improvement requires more demonstration data.",
            "novel_vs_familiar_objects": "Survey indicates RT-1 struggles with novel objects relative to VLM-based methods; no numeric comparisons.",
            "zero_shot_or_few_shot": "Limited zero-shot/few-shot capability compared to later VLM-based VLAs.",
            "layer_analysis": "Not reported in survey.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "RT-1 uses vision+language but without large VLM pretraining; survey contrasts it unfavorably with RT-2's VLM-enhanced performance.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1925.3"
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort",
            "brief_description": "A modular language-conditioned manipulation system that uses CLIP as a vision-language encoder coupled with a Transporter network for spatial reasoning, demonstrating early semantic grounding for manipulation.",
            "citation_title": "Cliport: what and where pathways for robotic manipulation",
            "mention_or_use": "mention",
            "model_name": "CLIPort",
            "model_description": "Modular pipeline combining CLIP-based semantic grounding for visual tokens and a Transporter architecture for spatial action prediction; separates perception and policy modules rather than end-to-end VLM-fused models.",
            "pretraining_type": "vision-language pretraining (CLIP on image-text pairs) for perception encoder; policy trained on robot demonstrations",
            "pretraining_data_description": "CLIP pretraining on image-text pairs (web-scale) yields semantic object embeddings, while action datasets used for Transporter are robot-specific demonstrations containing object interactions.",
            "target_task_name": "tabletop robotic manipulation / pick-and-place",
            "target_task_description": "Simulated and real-world planar tabletop pick/place and rearrangement tasks with relatively constrained action spaces; discrete spatial action proxies used by Transporter.",
            "semantic_alignment": "High alignment for object labels and categories through CLIP; supports grounding of nouns and attributes but less integrated for compositional or long-horizon instructions.",
            "performance_with_language_pretraining": "Survey refers to CLIPort as effective early approach; specific numeric success rates are not reproduced here.",
            "performance_without_language_pretraining": "Not reported in survey.",
            "sample_efficiency_comparison": "Modular CLIP-based approaches were more sample efficient for semantic grounding compared to purely vision-only methods in original literature, but survey does not provide numbers.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis for CLIPort provided in this survey.",
            "embedding_space_analysis": "Survey notes CLIP's strong cross-modal embedding alignment which CLIPort leverages, but no new embedding analyses shown.",
            "action_grounding_evidence": "CLIPort demonstrates grounding of object names/attributes to pick locations via CLIP embeddings and Transporter spatial reasoning — cited as motivating early language-conditioned manipulation.",
            "hierarchical_features_evidence": "Not applicable; CLIPort is modular rather than hierarchical in the survey taxonomy.",
            "transfer_conditions": "Works well when object labels and visual appearance match CLIP pretraining distributions; struggles when objects are out-of-distribution visually or when complex multi-step instructions are required.",
            "novel_vs_familiar_objects": "Survey indicates CLIP-based modular methods can fail on novel appearances or unseen compositions; numeric gap not provided.",
            "zero_shot_or_few_shot": "Some zero-shot object recognition benefits from CLIP; action-level zero-shot limited without policy examples.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not explicitly reported in survey, though modular design limitations discussed.",
            "comparison_to_vision_only": "CLIP-based perception outperforms many vision-only encoders at semantic grounding, per survey discussion.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1925.4"
        },
        {
            "name_short": "DexVLA",
            "name_full": "DexVLA",
            "brief_description": "A VLM planner coupled with a diffusion-based action policy (plug-in diffusion expert) that excels in long-horizon and complex robotic manipulation tasks by conditioning actions on subtask tokens.",
            "citation_title": "Dexvla: vision-language model with plug-in diffusion expert for general robot control",
            "mention_or_use": "use",
            "model_name": "DexVLA",
            "model_description": "Hierarchical planner+policy: a large VLM planner generates subtask tokens and a large diffusion-based action policy (diffusion expert) produces detailed continuous action sequences conditioned on the subtask tokens; supports long-horizon tasks.",
            "pretraining_type": "VLM pretraining for planner (vision-language on image-text); diffusion policy trained on robot trajectories (action sequences) potentially pre-trained on large-scale action data",
            "pretraining_data_description": "Planner: web-scale vision-language corpora and instruction-finetuning; diffusion policy: robot trajectory datasets containing sequential action patterns and dynamics (contact-rich behaviors included depending on dataset).",
            "target_task_name": "long-horizon robotic manipulation",
            "target_task_description": "Multi-stage manipulation tasks in simulation and real-world where tasks are decomposed into subtasks and executed by diffusion policy; continuous action trajectories with contact dynamics.",
            "semantic_alignment": "Explicit — planner produces language-like subtasks aligned with VLM semantics which condition the action diffusion policy for grounded execution.",
            "performance_with_language_pretraining": "Survey claims DexVLA excels on complex long-horizon tasks and outperforms some baselines (qualitative); no universal numeric metrics reported inside survey.",
            "performance_without_language_pretraining": "Not quantitatively reported in survey.",
            "sample_efficiency_comparison": "Not reported numerically; design suggests improved efficiency for long-horizon planning through decomposition but no demonstration-count metrics provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention visualization reported in survey for DexVLA.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Planner-to-policy conditioning (subtask tokens) provides an explicit semantic-to-action interface; survey presents this architectural design as evidence of language-grounded motor generation.",
            "hierarchical_features_evidence": "Yes — the model has explicit planner (high-level language/subtask signals) and policy (low-level trajectories) separation; survey emphasizes this as beneficial for long-horizon consistency.",
            "transfer_conditions": "Transfer benefits when planner vocabulary and subtask abstractions align with downstream policy training data; misalignment between planner subtasks and policy capabilities can reduce transfer.",
            "novel_vs_familiar_objects": "Survey does not report numeric comparisons; model is designed to generalize via language-level subtask abstractions.",
            "zero_shot_or_few_shot": "Survey suggests improved few-shot adaptation via subtask decomposition but does not provide numeric few-shot counts or success rates.",
            "layer_analysis": "No detailed layer-wise analysis reported in survey for DexVLA.",
            "negative_transfer_evidence": "Not explicitly reported for DexVLA.",
            "comparison_to_vision_only": "Survey positions planner+diffusion as superior to purely vision-only pipelines for complex tasks due to language-mediated decomposition, but no direct numeric comparison presented.",
            "temporal_dynamics": "Diffusion policy models temporal trajectories explicitly; survey highlights this architecture for temporally-structured manipulation but gives no temporal learning curves.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.5"
        },
        {
            "name_short": "WorldVLA",
            "name_full": "WorldVLA",
            "brief_description": "A VLA model that integrates an autoregressive world model with action generation within a unified token-based architecture to capture physical dynamics and improve long-horizon planning.",
            "citation_title": "Worldvla: towards autoregressive action world model",
            "mention_or_use": "use",
            "model_name": "WorldVLA",
            "model_description": "Unified autoregressive token model that jointly models visual outcomes (world model predictions) and action tokens; combines visual imagination with action generation to improve fidelity to physical dynamics.",
            "pretraining_type": "Multimodal video/image pretraining for visual/world modeling plus action-token supervision from robot trajectories",
            "pretraining_data_description": "Large unlabeled videos and robot trajectory data; includes scene dynamics and object motions but degree of explicit affordance/action-language content depends on dataset.",
            "target_task_name": "long-horizon manipulation and planning",
            "target_task_description": "Tasks requiring prediction of future visual states and sequential motor actions; can be evaluated in simulation and real-world; action space continuous but tokenized for autoregressive modeling.",
            "semantic_alignment": "Survey emphasizes joint learning of world dynamics and action tokens improves semantic alignment between visual predictions and motor outputs by enforcing consistency between imagined outcomes and executed actions.",
            "performance_with_language_pretraining": "Survey claims improved long-horizon planning and action fidelity through integrated world/action modeling; no single numeric metric provided.",
            "performance_without_language_pretraining": "Not reported numerically.",
            "sample_efficiency_comparison": "Not reported numerically; world-model based approaches are described as improving planning efficiency by simulating outcomes (qualitative).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention or saliency analyses reported specifically for WorldVLA in the survey.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "By jointly modeling predicted visual outcomes and actions, WorldVLA provides architectural evidence of linking action sequences to expected perceptual consequences — cited as stronger grounding mechanism.",
            "hierarchical_features_evidence": "WorldVLA integrates implicit hierarchical reasoning via imagination but does not produce explicit human-interpretable intermediate programs/waypoints in all variants.",
            "transfer_conditions": "Works better when video/world-model pretraining covers dynamics similar to target environments; domain mismatch limits zero-shot planning fidelity.",
            "novel_vs_familiar_objects": "Not reported numerically; survey suggests world-models can help generalize to novel object dynamics if visual dynamics are learned.",
            "zero_shot_or_few_shot": "World-generated subgoal images (in other works) are used for zero-shot manipulation in some reported cases, but WorldVLA numeric zero-shot numbers not listed in survey.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Survey does not provide explicit negative transfer examples for WorldVLA.",
            "comparison_to_vision_only": "Survey suggests world-model integration provides benefits beyond vision-only perception for planning, but no head-to-head numbers included.",
            "temporal_dynamics": "Explicitly models temporal dynamics via autoregressive video/state prediction; survey highlights this as a core advantage though quantitative temporal learning curves are not provided.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.6"
        },
        {
            "name_short": "4D-VLA",
            "name_full": "4D-VLA",
            "brief_description": "A VLA pretraining approach that integrates spatiotemporal (3D + time) cues into visual features, uses memory bank sampling of historical keyframes, and aims to resolve spatial coordinate inconsistency for better spatiotemporal reasoning.",
            "citation_title": "4d-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration",
            "mention_or_use": "mention",
            "model_name": "4D-VLA",
            "model_description": "Pretraining paradigm and model variants that add 3D coordinates and temporal memory sampling to VLA architectures to incorporate 4D scene understanding; typically feeds 3D/temporal tokens into VLM/LLM fusion.",
            "pretraining_type": "multimodal video-text / spatiotemporal pretraining (video + reconstructed 3D) plus fine-tuning on embodied tasks",
            "pretraining_data_description": "Large-scale videos or multi-view sequences with reconstructed 3D and temporal continuity; contains object motion, interactions and temporal affordance cues.",
            "target_task_name": "spatiotemporal robotic manipulation and temporal reasoning",
            "target_task_description": "Manipulation tasks that require temporal context and 3D spatial reasoning (moving objects, dynamic scenes); environment: simulated and/or real with depth point clouds.",
            "semantic_alignment": "Designed specifically to improve alignment of spatial and temporal information with language instructions (e.g., 'after moving X do Y'); survey emphasizes the modality alignment improvements but no numeric overlaps reported.",
            "performance_with_language_pretraining": "Survey states 4D-VLA improves spatiotemporal reasoning and certain task success in manipulation settings relative to 2D-only variants, but does not provide aggregated numerical metrics.",
            "performance_without_language_pretraining": "Not numerically reported.",
            "sample_efficiency_comparison": "Not explicitly quantified in survey for 4D-VLA.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention maps or analyses reported in the survey for 4D-VLA.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Adding explicit 3D and temporal tokens provides architectural grounding cues linking language temporal predicates to observed motion and depth-informed interaction points; survey reports qualitative improvements in spatiotemporal tasks.",
            "hierarchical_features_evidence": "Survey suggests higher-level temporal/3D features are enriched, aiding planners and policies, but lacks layerwise empirical breakdown.",
            "transfer_conditions": "Works best when the pretraining video+3D data has dynamics and viewpoints similar to target embodied tasks; heavy domain mismatch reduces benefit.",
            "novel_vs_familiar_objects": "Not reported numerically.",
            "zero_shot_or_few_shot": "Survey implies improved few-shot spatiotemporal transfer but provides no numeric counts.",
            "layer_analysis": "Not provided in survey.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Survey frames 4D-pretrained models as superior to 2D-only vision models for temporal and dynamic manipulation tasks, though no numeric comparison provided.",
            "temporal_dynamics": "Central to 4D-VLA; model explicitly uses memory banks and keyframe sampling to combat temporal ambiguity; no learning-curve numbers provided.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.7"
        },
        {
            "name_short": "BridgeVLA",
            "name_full": "BridgeVLA",
            "brief_description": "A model that reconstructs 3D point clouds from RGB-D and generates orthographic projections to present 3D 'space' information in a 2D format compatible with large VLMs, enabling efficient 3D-aware manipulation learning.",
            "citation_title": "Bridgevla: input-output alignment for efficient 3d manipulation learning with vision-language models",
            "mention_or_use": "use",
            "model_name": "BridgeVLA",
            "model_description": "Uses RGB-D reconstruction to produce 3D point clouds and three orthographic projections, projecting 3D spatial information into 2D views that can be consumed by standard VLM encoders; compatible with VLM tokenization pipelines.",
            "pretraining_type": "vision-language pretraining for VLMs (image-text) plus RGB-D / point-cloud-based fine-tuning for manipulation",
            "pretraining_data_description": "RGB-D paired data and reconstructed 3D point clouds with manipulation interactions; includes spatial relationships and affordance-like cues implicitly via demonstrations.",
            "target_task_name": "3D-aware robotic manipulation",
            "target_task_description": "Manipulation in environments where depth and 3D spatial relationships are critical; action space continuous; evaluated on tasks requiring accurate 3D reasoning.",
            "semantic_alignment": "BridgeVLA explicitly seeks to align 3D perceptual inputs with VLMs' 2D processing by encoding spatial info as orthographic 2D projections, increasing semantic overlap between pretraining modalities and target 3D tasks.",
            "performance_with_language_pretraining": "Survey reports that BridgeVLA improves efficiency and 3D manipulation performance relative to purely 2D approaches, though no aggregate numeric performance figures are given.",
            "performance_without_language_pretraining": "Not reported numerically in survey.",
            "sample_efficiency_comparison": "Not quantified; method framed as more efficient due to compatibility with 2D VLM encoders.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analyses for BridgeVLA reported in survey.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Architectural projection of 3D geometry into VLM-readable 2D views provides stronger perceptual affordance cues that can better ground language instructions referring to spatial relations; evidence described qualitatively.",
            "hierarchical_features_evidence": "Not reported in terms of layer-specific benefits, but BridgeVLA specifically targets improving spatial features available to VLM planners/policies.",
            "transfer_conditions": "Benefits when RGB-D or depth information is available and when downstream policies are compatible with orthographic 2D representations; domain mismatch in depth quality can reduce transfer.",
            "novel_vs_familiar_objects": "Not reported numerically.",
            "zero_shot_or_few_shot": "Survey suggests improved generalization to 3D tasks but provides no zero/few-shot numbers.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Claimed to improve over 2D-only VLM-based VLAs for 3D tasks, no numeric comparisons included.",
            "temporal_dynamics": "BridgeVLA focuses on spatial 3D alignment; temporal aspects not central in description.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.8"
        },
        {
            "name_short": "TraceVLA",
            "name_full": "TraceVLA",
            "brief_description": "A single-system VLA approach that overlays sampled motion-point trajectories onto current images to create explicit temporal 'trace' information, improving spatiotemporal awareness for manipulation.",
            "citation_title": "Tracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies",
            "mention_or_use": "use",
            "model_name": "TraceVLA",
            "model_description": "Augments visual input with overlaid historical motion-point traces to provide temporal context; integrates these tokens into VLM/LLM reasoning to produce temporally informed action predictions.",
            "pretraining_type": "vision-language pretraining plus fine-tuning with temporally augmented trajectory data",
            "pretraining_data_description": "Robot trajectory sequences with temporally sampled motion points; contains temporal dynamics and object interaction sequences.",
            "target_task_name": "temporally-sensitive robotic manipulation",
            "target_task_description": "Manipulation tasks where motion history and dynamics matter (e.g., cloth manipulation, moving targets); continuous action space tokenized for generation.",
            "semantic_alignment": "By explicitly providing motion traces, TraceVLA increases overlap between pretraining temporal dynamics and target tasks requiring temporal reasoning.",
            "performance_with_language_pretraining": "Survey reports TraceVLA equips VLMs with improved spatiotemporal understanding and improves performance on certain manipulation tasks, but no consolidated numeric metrics are given.",
            "performance_without_language_pretraining": "Not reported numerically.",
            "sample_efficiency_comparison": "Not quantified in the survey.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention-map analyses reported; design is described at input-augmentation level.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Inclusion of motion traces supplies explicit signals linking observed dynamics to expected action sequences, providing mechanism-level evidence for temporal grounding though quantified grounding experiments are not reported.",
            "hierarchical_features_evidence": "Enhances temporal features available to planner/policy; no layerwise empirical breakdown presented.",
            "transfer_conditions": "Helpful when tasks require temporal disambiguation; limited benefit when tasks are static or short-horizon.",
            "novel_vs_familiar_objects": "Not reported numerically.",
            "zero_shot_or_few_shot": "Survey implies improved generalization for temporally structured tasks but lacks numeric zero/few-shot results.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Provides temporal cues that 2D static methods lack; survey positions this as an advantage but gives no numeric comparisons.",
            "temporal_dynamics": "Central to design; model explicitly encodes historical motion to disambiguate current state.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.9"
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E",
            "brief_description": "An embodied multimodal language model that integrates visual perception into an LLM for robot command generation and planning; used as a VLM backbone in subsequent VLA models.",
            "citation_title": "Palm-e: an embodied multimodal language model",
            "mention_or_use": "mention",
            "model_name": "PaLM-E",
            "model_description": "Large language model augmented with visual encoder inputs to enable multimodal reasoning and generation for embodied tasks; produces textual plans or action tokens conditioned on images and other sensors.",
            "pretraining_type": "text-only LLM pretraining augmented by multimodal fine-tuning with image-text and embodied datasets",
            "pretraining_data_description": "Massive text corpora for LLM pretraining combined with image-text and robot trajectory finetuning; includes object descriptions, instructions and some embodied action sequences for fine-tuning.",
            "target_task_name": "embodied instruction following and robotic planning",
            "target_task_description": "High-level planning and command generation for robot manipulation; typically outputs symbolic or tokenized action plans which are fed to downstream controllers.",
            "semantic_alignment": "Designed to align language model knowledge with visual perception and robot states; survey cites PaLM-E as a key example of integrating LLMs into robotics.",
            "performance_with_language_pretraining": "Survey references PaLM-E demonstrating feasibility of unifying VQA capabilities with robot command generation but does not give a single metric.",
            "performance_without_language_pretraining": "Not reported within survey.",
            "sample_efficiency_comparison": "Not provided in survey for PaLM-E.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not discussed in survey for PaLM-E.",
            "embedding_space_analysis": "Not discussed in survey for PaLM-E.",
            "action_grounding_evidence": "PaLM-E demonstrates action generation conditioned on visual inputs providing initial evidence that LLM knowledge can be grounded into robot commands; survey treats it as foundational prior work.",
            "hierarchical_features_evidence": "PaLM-E functions at higher planning/command generation levels; explicit low-level policy analysis not in survey.",
            "transfer_conditions": "Benefit when fine-tuned with robot-specific data; purely LLM-trained knowledge alone insufficient for low-level motor control.",
            "novel_vs_familiar_objects": "Not reported in survey.",
            "zero_shot_or_few_shot": "PaLM-E shows some cross-task capability due to LLM priors but the survey does not quantify.",
            "layer_analysis": "Not reported in survey.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "PaLM-E improves over text-only or vision-only baselines by fusing modalities, per survey discussion, but numbers are not reproduced.",
            "temporal_dynamics": "Not central in PaLM-E description in the survey.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1925.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Openvla: an open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "π0 : A visionlanguage-action flow model for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Dexvla: vision-language model with plug-in diffusion expert for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Worldvla: towards autoregressive action world model",
            "rating": 2
        },
        {
            "paper_title": "4d-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration",
            "rating": 2
        },
        {
            "paper_title": "Cliport: what and where pathways for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Palm-e: an embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Tracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies",
            "rating": 2
        },
        {
            "paper_title": "Bridgevla: input-output alignment for efficient 3d manipulation learning with vision-language models",
            "rating": 2
        }
    ],
    "cost": 0.028575749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</p>
<p>Rui Shao shaorui@hit.edu.cn 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Wei Li 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Lingsen Zhang 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Renshan Zhang zhangrenshan@stu.hit.edu.cn 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Zhiyang Liu 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Ran Chen 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Liqiang Nie nieliqiang@gmail.com 
School of Computer Science and Technology
Harbin Institute of Technology</p>
<p>518055Shenzhen)China</p>
<p>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey
91D73B6127C57BF45E276BCC7100B812Vision-Language-Action ModelsRobotic ManipulationEmbodied AILarge Vision-Language Models
Robotic manipulation, as a critical frontier in robotics and embodied AI, demands precise motor control and integrated understanding of visual and semantic cues in dynamic environments.Traditional approaches, grounded in predefined task specifications and rigid control policies, often struggle to scale or generalize in unstructured, novel scenarios.In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm.By leveraging large VLMs' capabilities in open-world generalization, hierarchical task planning, knowledge-augmented reasoning, and rich multimodal fusion, these models empower robots to interpret high-level instructions, recognize unseen environments and execute complex manipulation tasks.This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation.We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations.Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities.This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation.We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/LargeVLM-based VLA for Robotic Manipulation.</p>
<p>INTRODUCTION</p>
<p>R OBOTIC manipulation stands as a pivotal challenge at the confluence of robotics and embodied artificial intelligence [1]- [5].Its realization necessitates not only precise motor control but also a profound comprehension of diverse visual and semantic cues within complex, dynamic environments.The widespread utility of robotic manipulation is evident across numerous domains, including advanced manufacturing, efficient logistics, precision healthcare, and versatile domestic service [6]- [8].Traditional manipulation methods [9]- [16] have been predominantly anchored in meticulously engineered control policies and rigidly predefined task specifications.However, these approaches demonstrably falter in unstructured, real-world settings-particularly when confronted with novel objects, ambiguous natural language instructions, or previously unseen environmental configurations-thereby exposing inherent limitations in their scalability and generalization capabilities.</p>
<p>Recently, large Vision-Language Models (VLMs) [17]- [25] have emerged as a transformative paradigm.Pretrained on vast web-scale image-text datasets, large VLMs exhibit a remarkable aptitude for bridging the semantic chasm between visual perception and natural language understanding.This innovative capacity enables large VLMs to Fig. 2: Outline of the organization of our comprehensive survey (top) and a chronological timeline of notable developments in large VLM-based Vision-Language-Action models for robotic manipulation (bottom).The timeline highlights representative milestones for both monolithic models and hierarchical models, providing a perspective on the field's recent progress.</p>
<p>interpret complex visual scenes in conjunction with textual descriptions, moving beyond mere object recognition to encompass a holistic contextual understanding.The subsequent integration of large VLMs into robotic systems has catalyzed the development of a novel class of models: large VLM-based Vision-Language-Action (VLA) models [26]- [32].As illustrated in Fig. 1, this emerging paradigm offers great potential to overcome the fundamental limitations of conventional robotic pipelines.It enables robots to interpret high-level human instructions, generalize to unseen objects and scenarios, reason about complex spatial relationships, and perform sophisticated manipulation tasks in dynamic, unstructured environments.For instance, a VLA model could execute commands such as "place the red mug next to the laptop onto the top shelf," a task demanding a sophisticated fusion of visual grounding, spatial reasoning, and sequential motion planning.</p>
<p>In this work, based on an extensive review of recent research [26]- [37] and a refined understanding of the field [38]- [43], we propose a coherent definition of large VLM-based VLA models: the model (1) leverages a large VLM to understand visual observations and natural language instructions, and (2) performs reasoning processes that directly or indirectly serve robotic action generation.We further distinguish two principal categories of large VLMbased VLA models, as shown in Fig. 2 and Fig. 3: • Monolithic Models (Fig. 3, left) comprise single-system and dual-system implementations.(1) Single-system models [26], [27], [44], [45] integrate both environmental comprehension (including visual perception, linguistic understanding, and robot state awareness) and action generation within a unified architecture.In contrast, (2) dual-system models [29]- [32] employ a VLM backbone for scene interpretation and an action expert for action determination, exchanging information via the propagation of latent representations.</p>
<p>• Hierarchical Models (Fig. 3, right) [46]- [50] explicitly decouple planning from policy execution.These differ from dual-system end-to-end approaches through two defining characteristics: (1) Structured intermediate outputs where planner modules generate interpretable representations such as keypoint detections, affordance maps, or trajectory proposals, which are then processed by policy modules to formulate executable actions; and</p>
<p>(2) Decoupled training paradigms that enable independent optimization of hierarchical modules through specialized loss functions or API-mediated interactions.This taxonomy emphasizes critical design dimensions in VLA development, particularly regarding system integration granularity and the explicitness of cognitive decomposition, while maintaining essential connections to modern representation learning paradigms.</p>
<p>Operating under the above definitions and taxonomy, our comprehensive survey across the broad spectrum of related work reveals several critical gaps within the nascent VLA field, with the organization of this survey illustrated in Fig. 2. First, the terminology and modeling assumptions in this field remain inconsistent, and the research landscape is fragmented across disciplines (robotics, computer vision, natural language processing, etc.).Second, existing reviews tend to focus either on VLMs [51]- [55] or on robotic manipulation in isolation [2], [56]- [59], lacking a comprehensive synthesis of the unique challenges and advances that arise at their intersection.Therefore, there is a pressing need for a systematic and principled survey that elucidates the foundations of large VLM-based VLA models, organizes the space of relevant methods, and outlines future directions for this integrated paradigm.This survey aims to address this gap.We present a structured and in-depth overview of advances in large VLM-based VLA models research, aiming to provide a panoramic perspective of the field to foster a deeper understanding and drive future breakthroughs.Our Fig. 3: Comparison of the two principal categories of large VLM-based VLA models.Monolithic models (Sec.3) integrate perception, language understanding, and action generation within single-or dual-system architectures, with the latter incorporating an additional action expert.In contrast, hierarchical models (Sec.4) decouple planning from policy execution through interpretable intermediate outputs (e.g., subtasks, keypoints, programs, affordances).key contributions are summarized as follows:</p>
<p>• A Longitudinal Synthesis of large VLM-based VLA Models Development: We systematically review the evolutionary trajectory of VLMs, the technical advancements in manipulation learning, and the subsequent emergence of the large VLM-based VLA paradigm.Furthermore, we examine the development of monolithic models and hierarchical models, identifying key challenges and outlining future directions.• A Cross-Cutting Synthesis of large VLM-based VLA Modeling Practices: We provide a more fine-grained comparative taxonomy of monolithic models and hierarchical models, examining them in detail from both structural and functional perspectives.We further explore advanced research frontiers of large VLM-based VLA models, highlighting their distinctive characteristics and the datasets that underpin their development.This synthesis provides a high-level summary and a conceptual roadmap for understanding the field's development and structural organization.</p>
<p>The remainder of this survey is organized as follows.As illustrated in Fig. 2, Sec. 2 presents essential background knowledge on the evolution of VLMs and the foundational aspects of robotic manipulation.Sec. 3 examines monolithic models, detailing single-system and dual-system architectures, their respective advantages, and design trade-offs.</p>
<p>BACKGROUND</p>
<p>Evolution of Vision-Language Models</p>
<p>Recently, the emergence of large VLMs [17]- [25] has been marked by a shift from task-specific architectures to uni-fied frameworks capable of handling diverse multimodal tasks [60]- [64].Modern VLMs, such as LLaVA1.5 [18] and Qwen-VL [20], typically adopt a three-component architecture: a vision encoder to encode visual input, a projector to align visual features with textual embeddings, and a large language model to mediate multimodal reasoning.This design unifies multimodal tasks as textual outputs.It enables VLMs to perform not only traditional tasks like visual question answering or object detection, but also more advanced capabilities, including compositional reasoning and spatial navigation, which are critical for real-world applications.Building on this foundation, DriveVLM [65] demonstrates the integration of VLMs into autonomous driving systems, while CogAgent [66] shows VLMs' ability to interact with graphical user interfaces.These advancements highlight the growing sophistication of large VLMs in bridging vision and language, while also revealing the emerging potential of using large VLMs to build broad real-world applications.</p>
<p>Crucially, the potential of large VLMs in real-world applications [67]- [76] rests on their generalization capabilities afforded by visual instruction tuning.By training on large-scale, carefully curated visual instruction-following datasets, VLMs acquire the flexibility to comprehend abstract or open-ended instructions and to generalize to unseen multimodal scenarios.LLaVA [21] pioneers this paradigm by leveraging GPT-4 to convert raw image-text pairs into 150K conversational samples, significantly enhancing its ability to address open-ended queries.In-ternVL2 [22] advances by introducing an LLM-guided automated filtering pipeline.It helps clean noisy annotations, reducing anomalous behaviors of the model.Collectively, these advances demonstrate how visual instruction tuning prepares large VLMs to follow high-level commands.This capability forms the foundation for extending large VLMs to VLA for robotic manipulation, which requires robust reasoning across vision, language, and action in the real world with ambiguous instructions.</p>
<p>Recent advancements in VLMs further emphasize scalability for enhanced multimodal perception and reasoning.Models like LLaVA-OneVision [77] unify image and video processing within a single framework.Qwen-2VL [78] introduces dynamic resolution support for arbitrary input sizes, while Vision-R1 [79] leverages reinforcement learning to improve chain-of-thought reasoning.Collectively, these innovations empower VLMs with stronger real-world perception and decision-making capabilities, laying the groundwork for VLMs to further integrate action generation.</p>
<p>Building on these breakthroughs, the next frontier for large VLMs is to transition from passive observation to active interaction with physical environments.While models like DriveVLM [65] and CogAgent [66] already demonstrate VLMs' ability to process real-world inputs and generate high-level plans, they remain unable to engage directly with the real world in an embodied, physical manner.Equipping VLMs with robotic manipulation capabilities thus emerges as a particularly promising avenue for future research.</p>
<p>Advancements in Robotic Manipulation</p>
<p>Early language-conditioned robotic systems typically pair separate vision encoders with language modules or planners, rather than using a unified multimodal model.For example, CLIPort [80] employs the CLIP vision-language encoder [81] for semantic grounding and a Transporter [82] network for spatial reasoning.Likewise, RT-1 [83] uses a CNN-based visual encoder [84] conditioned by a separate language embedding derived from a universal sentence encoder to map camera images and text instructions to motor actions.By integrating separate pretrained visual and language encoders with multi-task robotic manipulation data, these early methods achieve a certain degree of multi-task generalization.However, their modular designs struggle to generalize to unseen concepts and face difficulties when handling complex or ambiguous instructions.</p>
<p>In contrast, the latest VLA models tightly integrate vision, language, and action control in a unified system.The RT-2 [27] model exemplifies this shift.RT-2 [27] begins with a pretrained large VLM backbone (e.g., PaLM-E [85] or PaLI-X [86]) and co-trains it on both internet-scale visionlanguage tasks and real robot trajectories.Crucially, robot actions are cast as text tokens and included in the same training corpus as normal language outputs.This simple scheme allows the model to "absorb" robot control as another language task, resulting in a unified VLA model.Empirically, RT-2 shows a dramatically stronger semantic understanding ability.Compared to RT-1 [83], RT-2 [27] generalizes to novel objects and unseen instructions (e.g., placing an object on a particular number) and can perform basic reasoning (choosing the smallest or closest object).In other words, the large-scale vision-language pretraining imbues the robotic policy with real-world knowledge and stronger language grounding that earlier models lack.</p>
<p>Following RT-2's demonstration of how Internet-scale VLM pretraining can boost robotic control, more VLA models have emerged.For example, π 0 [29] adopts a flowmatching architecture on top of a pretrained VLM, trained on diverse dexterous robot datasets, which yields strong zero-shot generalization and easy adaptation to new tasks via fine-tuning.In addition, OpenVLA [26] has been released as a 7B open-source VLA model pretrained on approximately 970k real-world robot demonstrations.It achieves superior generalist manipulation performance and supports efficient fine-tuning on consumer hardware via techniques like LoRA [87].Together, these advancements underscore a trend toward greater generality and open accessibility in VLA-driven robotic manipulation.</p>
<p>Relevant Surveys</p>
<p>The growing interest in VLA models for embodied AI has inspired several surveys [1]- [6], [8], [88], [89], yet most focus on broader architectural paradigms or diverse application domains, leaving a gap in the systematic exploration of large VLM-based VLA systems specifically tailored for robotic manipulation.For example, Ma et al. [2] conduct a comprehensive survey of VLA architectures, reviewing modular, end-to-end, and hybrid approaches for integrating vision, language, and action modalities.However, this survey lacks a focused analysis of recent trends that leverage pre-trained VLMs as foundational components.These models have emerged as a dominant paradigm due to their strong cross-modal alignment and zero-shot generalization capabilities.Sapkota et al. [8] provide an extensive investigation into a broader range of VLA applications, such as autonomous driving, augmented reality navigation, and precision agriculture.However, its breadth dilutes the depth required for robotics-specific challenges such as real-time actuation constraints, sensor noise robustness, and longterm decision-making.A survey by Wang et al. [88] offers an early overview of the integration of text-only LLMs into robotic task planning.It emphasizes their capability to generate precise action plans from natural language instructions.However, the study primarily focuses on high-level reasoning and does not address the challenge of grounding VLMs in robotic visual perception and action determination, which is a gap that recent VLA models are designed to fill.</p>
<p>To address the lack of a comprehensive and in-depth survey in this emerging field, we provide a structured overview of recent VLA research.Building on the definition of VLA models described in Sec. 1, we trace the development of VLAs in robotics, analyze key architectures and learning paradigms, and review relevant datasets and benchmarks.Our aim is to offer a comprehensive perspective that supports deeper understanding and drives future advances.</p>
<p>MONOLITHIC MODELS</p>
<p>Monolithic VLA models are mainly implemented in two ways: single-system and dual-system architectures, as illustrated in Fig. 3 left.In the single-system design (Sec.3.1), visual perception, language instructions, and robot states are jointly fed into a unified model that processes all modalities and decodes executable actions via autoregressive or parallel decoding.In contrast, dual-system architectures (Sec.3.2) divide functionality into two cooperating modules: System 2 (VLM backbone) performs slower but more generalized reflective reasoning, while System 1 (action expert) focuses on fast processing to support reactive behaviors.Single-system VLAs offer architectural simplicity, streamlined development, and avoidance of complex inter-module communication while dual-system VLAs leverage a division of labor to combine reactive speed with deliberate accuracy.</p>
<p>Single-system Models</p>
<p>Single-system VLA models embody the monolithic design philosophy.They aim to transfer the semantic knowledge of large VLMs to robotic manipulation tasks through a unified model.This section reviews the paradigms of single-system   models and explores two main research directions: enhancing the model's capabilities to solve complex tasks and improving inference efficiency for practical deployment.All models are summarized in Tab. 1.</p>
<p>Classic Paradigm: Autoregressive Decoding</p>
<p>The autoregressive decoding paradigm draws directly from the sequence generation capability of LLMs.By discretizing the robot's continuous action space into a token sequence, the model can sequentially predict action tokens.As shown in "Autoregressive Decoding" of Fig. 4, the VLM receives the visual observation, natural language instruction, and optionally the robot state as input.Then it autoregressively generates action tokens, which can be converted into executable actions through a downstream de-tokenizer.</p>
<p>The RT series [27], [83], [90] and OpenVLA [26] are typical examples of this paradigm.RT-1 [83] introduces Transformer [91] into the VLA domain and encodes continuous robot actions as discrete tokens.RT-2 [27] inherits this token-based action representation and applies it to a larger-scale VLM.It co-fine-tunes on internet-scale visionlanguage and robot trajectory data, successfully transferring rich visual-semantic knowledge from web data to robotic manipulation tasks.RT-2-X [90] further improves crossrobot skill transfer by applying co-fine-tuning on the Open X-Embodiment (OXE) dataset [90].Going a step further, OpenVLA [26] replaces the large-parameter vision encoder in RT series with a combination of SigLIP [92] and DINOv2 [93].Through fine-tuning on large-scale real-world robotic manipulation data, it achieves superior performance with fewer model parameters.Its fully open-sourced nature positions it as a widely used baseline in subsequent research.</p>
<p>Model Performance Enhancement</p>
<p>The development of VLA models has been accompanied by improvements in model performance.To better align model capabilities with human intelligence, performance enhancement efforts have focused on three directions:</p>
<p>Enhancing Perception Modalities.As shown in Fig. 4, these models improve real-world information acquisition by expanding input modalities.(1) 3D perception.Since the real world is three-dimensional and continuous, large VLMbased VLA models cannot fully understand it by relying solely on 2D images.Accordingly, Leo Agent [94] directly obtains object-centric 3D point clouds from simulation environments and encodes them using PointNet++ [119]; the encoded features are then fed into a spatial transformer [120] to generate "Space" information.In real-world settings, SpatialVLA [100] proposes Ego3D position encoding, which estimates depth from 2D images and back-projects it into egocentric 3D coordinates, fusing pixel-wise 3D positions with 2D features to produce egocentric "Space" information.Similarly, BridgeVLA [35] reconstructs 3D point clouds from RGB-D images and generates three orthographic projection views, enabling "Space" information to be represented in a 2D format compatible with large VLMs.</p>
<p>(2) 4D perception.To better capture both spatial structure and temporal dynamics in robotic manipulation, 4D perception is required for more comprehensive scene understanding.TraceVLA [97] overlays sampled motion-point trajectories on the current image to create the "Trace" information in Fig. 4. Combined with the original observation, it equips the large VLM with spatiotemporal understanding.Furthermore, 4D-VLA [108] integrates 3D coordinates into visual features to resolve spatial coordinate inconsistency, and leverages memory bank sampling of historical keyframes to mitigate temporal state ambiguity, thereby improving spatiotemporal reasoning.ST-VLA [110] introduces the "Spatial Traces" approach, which combines historical temporal "Trace" information with spatial depth information of the scene, surpassing prior SpatialVLA [100] and TraceVLA [97] models in certain robotic manipulation tasks.</p>
<p>(3) Tactile and auditory perception.VTLA [103] encodes tactile information via a vision encoder to align it with visual sequences, and then feeds it together with visual and textual tokens into the LLM, thereby constructing a "Vision-Tactile-Language-Action model".VLAS [102] employs the "Whisper Encoder" to extract speech features and projects them into the LLM embedding space through an MLP, thereby introducing the "Audio" information in Fig. 4.Both VTLA and VLAS construct task-specific datasets for finetuning.Moreover, FuSe [98] learns to align "Touch" and "Audio" modalities with language concepts during fine-TABLE 1: Single-system VLA models.In the LLM / VLM column, omission of the V-Encoder indicates a VLM; otherwise, it represents an LLM.In the Learning column, "AD" denotes Autoregressive Decoding and "PD" denotes Parallel Decoding."SFT" denotes fine-tuning distinct from action-prediction imitation learning, where tasks like captioning, VQA, reasoning and others all qualify as SFT."A" and "B" in parentheses represent the learning methods used by Action head or Backbone.</p>
<p>Model V-Encoder LLM / VLM Learning Contribution</p>
<p>Classic Paradigm: Autoregressive Decoding
RT-2 [27] - PaLI-X / PaLM-E AD (A), SFT (B)
Represent actions as VLM tokens to enable generalization.RT-2-X [90] ViT-22B UL2 AD (A), SFT (B) Fine-tune on cross-robot data for positive skill transfer.OpenVLA [26] DINOv2 + SigLIP LLaMA2-7B AD (A) Open-source 7B-parameter VLA model for generalist robot control.</p>
<p>Paradigm Derivations: Model Performance Enhancement</p>
<p>LEO agent [94] ConvNext Vicuna-7B AD (A), SFT (B) Combine object-centric 3D features with LLM for action.ECoT [95] DINOv2 + SigLIP LLaMA2-7B AD (A), SFT (B) Incorporate chain-of-thought to enhance policy explainability.ReVLA [96] DINOv2 + SigLIP LLaMA2-7B AD (A) Reverse backbone gradually to preserve visual generalization.TraceVLA [97] DINOv2 + SigLIP LLaMA2-7B AD (A) Propose visual trace prompting for spatiotemporal awareness.FuSe [98] -PaliGemma-3B AD (A), SFT (B) Leverage natural language for cross-modal fine-tuning.UniAct [99] -LLaVA-0.5BPD (A) Propose universal action space for versatile and adaptive control.SpatialVLA [100] SigLIP Gemma2 AD (A) Improve generalization via 3D encoding and action grid.UP-VLA [101] ViT + VQ-GAN Phi1.5-1.3BAD (A), SFT (B) Propose unified training for semantic-spatial understanding.VLAS [102] CLIP Vicuna-7B AD (A), SFT (B) Introduce voice modality to VLA and construct a paired dataset.HybridVLA [34] DINOv2 + SigLIP LLaMA2-7B Diff., AD (A) Integrate diffusion and autoregressive policies to improve success.CoT-VLA [33] -VILA-U AD+PD(A), SFT(B) Propose a visual chain-of-thought to improve planning.VTLA [103] -Qwen2-VL-7B AD (A) Integrate visual and tactile inputs to improve task success.OE-VLA [104] SigLIP Qwen1.</p>
<p>5-7B AD (A), SFT (B)</p>
<p>Introduce four open-ended tasks to expand interaction modalities.ReFineVLA [105] SigLIP Gemma2 AD (A), SFT (B) Propose reasoning-aware framework to fine-tune VLAs effectively.LoHoVLA [106] SigLIP Gemma-2B AD (A), SFT (B) Address long-horizon tasks via hierarchical closed-loop control.BridgeVLA [35] SigLIP Gemma PD (A), SFT (B) Project 3D data into 2D space for efficient action prediction UnifiedVLA [107] -Emu3 AD (A), SFT (B) Convert all input signals into tokens to build a unified model.WorldVLA [38] -Chameleon AD (A), SFT (B) Combine world and action models for bidirectional improvement.4D-VLA [108] -InternVL-4B PD (A) Integrate 4D spatiotemporal cues for efficient VLA pretraining.VOTE [109] DINOv2 + SigLIP LLaMA2-7B PD (A) Introduce voting strategy to increase action prediction accuracy.ST-VLA [110] -PaliGemma2 AD (A), SFT (B) Project visual traces onto depth maps for better understanding.</p>
<p>Paradigm Derivations: Inference Efficiency Optimization</p>
<p>RoboFlamingo [111] ViT MPT-1B PD (A), SFT (B) Decouple design to adapt open-sourced VLM for robotic control.RoboMamba [112] CLIP / SigLIP ViT-L Mamba-2.8B/1.4BPD (A), SFT (B) Introduce the Mamba architecture to the VLA field.DeeR-VLA [36] CLIP ViT-L/14 MPT-1B / 7B PD (A) Propose dynamic early-exit to reduce inference overhead.OpenVLA-OFT [44] DINOv2 + SigLIP LLaMA2-7B PD (A) Boost performance via OpenVLA-based fine-tuning.PD-VLA [113] CLIP ViT-L Vicuna1.5-7BPD (A) Introduce parallel decoding manner for faster robot control.MoLe-VLA [114] DINOv2 + SigLIP LLaMA2-7B PD (A) Reduce computation via dynamic LLM layer activation.NORA [45] -Qwen2.5-VL-3BAD (A) Build efficient low-parameter model to boost performance.FLashVLA [115] DINOv2 + SigLIP LLaMA AD (A) Propose retraining-free acceleration to improve VLA inference.BitVLA [116] SigLIP b1.58 BitNet b1.58 2B4T PD (A), SFT (B) Build ternary weight model to reduce deployment memory cost.Spec-VLA [117] DINOv2 + SigLIP LLaMA2-7B PD (A) Propose speculative decoding to speed up without success drop.CogVLA [118] DINOv2 + SigLIP LLaMA2-7B PD (A) Propose a cognition-aligned framework for efficient manipulation.</p>
<p>tuning, enabling multimodal fusion with limited modalityspecific annotations.OE-VLA [104] leverages diverse multimodal data and a two-stage curriculum learning strategy, extending instructions beyond text to interleaved images, videos, and optical instructions.</p>
<p>Enhancing Reasoning Capabilities.To move VLA models from simple reactive control toward more advanced deliberative decision-making, enhancing reasoning ability is essential.As shown in "Enhancing Reasoning Capabilities" of Fig. 4, the LLM generates a chain-of-thought "Reasoning" process, which is then used as contextual information to produce the final "Action".ECoT [121] generates a reasoning chain that sequentially combines high-level task planning with visually grounded features before outputting the final action.CoT-VLA [33] further introduces a visual chain-ofthought reasoning by predicting a subgoal observation that represents a planned state in pixel space.In contrast to reasoning-oriented perspectives, LoHoVLA [106] employs a "Hierarchical Closed-Loop Control" mechanism to ad-dress planning errors, action failures, and external disturbances, thereby handling long-horizon tasks.ReFineVLA [105] adopts a "Selective Transfer Fine-Tuning" strategy with dual learning objectives to fine-tune only the upper layers, enabling the model to enhance multimodal understanding.These studies collectively underscore the central role of reasoning in enabling VLA models to achieve more reliable and generalizable action prediction.</p>
<p>Enhancing Generalization Capabilities.The generalization capability of a model refers to its ability to perform diverse tasks across different platforms and scenarios.As illustrated in "Enhancing Generalization Capabilities" of Fig. 4, UniAct [99] defines a "Universal Action Codebook" that abstracts heterogeneous robot actions into a unified representation.This unified encoding eliminates actionspace heterogeneity and enables cross-embodiment learning and reasoning, thereby enhancing generalization.ReVLA [96] employs a reversible training strategy that gradually restores the vision encoder to its original pre-trained state, mitigating catastrophic forgetting during fine-tuning and improving out-of-distribution visual generalization.</p>
<p>Another approach to enhancing generalization is to generate more robust and reliable actions, as depicted in Fig. 4. HybridVLA [34] integrates diffusion and autoregressive decoding within a unified model.It further employs a "Collaborative Action Ensemble" mechanism to adaptively fuse them, selecting the most suitable generation strategy for different tasks, thereby improving control robustness.Similarly, VOTE [109] introduces an adaptive "Ensemble Voting" strategy that groups past action predictions by similarity to the current one.It averages the majority set to produce more robust actions, balancing responsiveness and stability.</p>
<p>Enabling the model to better understand physical dynamics can promote the generation of more reliable actions.WorldVLA [38] integrates an action model and a world model within a unified autoregressive framework for capturing physical dynamics.Likewise, UnifiedVLA [107] introduces a world model as a post-training task, enabling it to learn causal dynamics of the physical world from largescale unlabeled videos.UP-VLA [101] proposes a training framework that implicitly learns the physical dynamics of the world by predicting the next-frame image during pretraining.Consequently, these models generate more reliable actions that better adhere to physical laws.</p>
<p>Inference Efficiency Optimization</p>
<p>Large VLM-based VLA models often lead to slow inference speed, which conflicts with the high control-frequency requirements in robotic manipulation.Therefore, many works focus on inference efficiency optimization, mainly improving efficiency from the following three perspectives:</p>
<p>Architectural Optimization.These works aim to improve efficiency by adjusting the VLM architecture in VLA.As illustrated in "Architectural Optimization" of Fig. 4, dynamic inference can be achieved through layer skipping or early exiting, avoiding redundant computation.MoLe-VLA [114] introduces a "Spatio-Temporal Aware Router (STAR)" over the joint projection space of visual and language inputs, dynamically computing routing weights to select the most relevant LLM layers.This mechanism efficiently activates critical layers while skipping redundant ones, enabling fast and semantically sensitive inference.DeeR-VLA [36] enables dynamic inference by adding multiple early exits within the LLM.Adjacent exits compare output consistency to determine whether inference can be terminated early, avoiding unnecessary deep-layer computation.CogVLA [118] proposes a cognition-aligned routing framework that introduces instruction-driven sparsification across the perception-language-action pipeline.By progressively compressing visual tokens, pruning instruction-irrelevant grounded tokens, and coupling them with efficient V-L-A attention, CogVLA achieves both reduced inference latency and advanced success rates on robotic manipulation tasks.Unlike dynamic inference, RoboMamba [112] introduces Mamba-based LLMs [122] to the VLA domain, achieving over 3× faster inference than Transformer-based [91] models due to Mamba's linear complexity with sequence length.</p>
<p>Parameter Optimization.Reducing model size directly lowers deployment difficulty.As shown in Fig. 4 under "Pa-</p>
<p>Obs.</p>
<p>Instr.State</p>
<p>Cascade-based Methods</p>
<p>State</p>
<p>Separate Action Expert</p>
<p>Action Shared-attention Architecture  rameter Optimization", model compression directly reduces the parameter count.BitVLA [116] introduces the first "1bit" weight VLA model, using distillation-aware training to quantize the vision encoder and adopting the "1-bit" weight BitNet b1.58 [123] as its LLM.On the other hand, NORA [45] pairs a compact, high-quality VLM with a FAST+ [124] tokenizer that compresses high-dimensional actions into short token sequences.It demonstrates that a well-designed small model can outperform larger models.</p>
<p>Parallel-based Methods</p>
<p>Obs. Instr. State</p>
<p>Unified Action Expert</p>
<p>Action</p>
<p>High-frequency：</p>
<p>Low-frequency：</p>
<p>Cross-attention Architecture</p>
<p>Inference Acceleration.This line of work seeks to enhance inference efficiency by accelerating the process of action decoding during inference.The most notable aspect is that most models replace autoregressive decoding with parallel decoding.Fig. 4 under "Inference Acceleration" shows that decoding speed can be improved by modifying the LLM decoding process or using a specialized action head.In this way, a complete action sequence can be generated in a single forward pass.RoboFlamingo [111] separates perception and policy, using an independent MLP action head to convert VLM outputs into actions.This enables the generation of stacked actions in one pass.OpenVLA-OFT [44] replaces the causal attention mask with a bidirectional one, enabling empty action embeddings to be filled in one pass and converted to continuous actions via an MLP.PD-VLA [113] reformulates autoregressive decoding as a nonlinear equation system solved via fixed-point iteration.Bidirectional attention updates action tokens until convergence, thereby enabling parallel decoding.</p>
<p>Different from parallel decoding, Spec-VLA [117] explores speculative decoding in VLA models.With Relaxed Acceptance to increase the verification model's acceptance rate for draft model predictions, it achieves a 1.42× speedup over the baseline.Beyond decoding, FlashVLA [115] addresses redundancy by using FlashTrigger to evaluate the stability of actions and the environment and decide whether to skip the current inference, significantly reducing latency.</p>
<p>Dual-system Models</p>
<p>The need to simultaneously handle the demands of deep reasoning and real-time action generation has prompted the development of monolithic dual-system VLA architectures.These models adopt two distinct systems: a larger and slower System 2 for high-level reasoning and planning, and a smaller and faster System 1 for low-level action generation.This separation enables a faster response time and smoother real-world deployment.It delivers both solid strategic planning and high-speed control for robotic manipulation.The separated low-level control module is also more adaptable to task-specific optimization.The key distinction between this approach and single-system models lies in the introduction of an action expert to explicitly decouple action generation.In contrast to hierarchical models, its major difference is that it does not produce an interpretable intermediate output.Tab. 2 summarizes representative dual-system methods.Fig. 5 illustrates two representative architectures: cascade-based methods and parallel-based methods.</p>
<p>Cascade-based Methods</p>
<p>As shown in the upper part of Fig. 5, cascade-based dualsystem VLA models separate high-level semantic reasoning from low-level real-time control in a serial manner.The high-level system (System 2) typically employs VLMs to process multimodal input, perform semantic grounding, and generate action plans.These plans are encoded as latent cognitive representations rather than being executed directly.The low-level system (System 1) then decodes these representations into executable robot actions at a higher frequency, significantly improving run-time efficiency and facilitating real-world deployment.Fig. 5 shows two architectures, differing in how the action expert is designed.</p>
<p>To decouple the capabilities of "cognition" and "action" of the model, many methods select a separate model to serve as the action expert.As shown in the "Separate Action Expert" part at the top-left of Fig. 5, the VLM backbone transmits features to the action expert to convey information derived from visual, textual, and robot-specific state inputs.For example, CogACT [128] introduces the diffusion transformer (DiT) [144] as an action model.In particular, it also proposes an adaptive action ensemble algorithm for smoother and more efficient movement trajectories.GR00T N1 [32] also adopts a dual-system architecture in which DiT is introduced as the low-level action model.Both systems are tightly coupled and jointly trained end-to-end, enabling much faster policy steps.Some models adopt a similar architecture, but do not employ a DiT.For example, DP-VLA [125] uses a Behavioral Cloning transformer [145] instead.</p>
<p>Many models have made variations based on this.The HiRT model [129] uses VLM to operate at a lower frequency.This process extracts features for long-term understanding of the scene.Then a lightweight visual action strategy is deployed at a higher frequency.This design enables more efficient robotic manipulation and allows the system to keep up with changes in the real-world environment during deployment.TriVLA [133] is a three-module system incorporating a video generation model to predict future frames, a VLM to interpret instructions, and a diffusion action expert.GF-VLA [134] extracts information-based hand-object and object-object scene graphs from human demonstrations, then fuses them with an LLM to generate interpretable behavior trees and low-level Cartesian actions for dual-arm control.RationalVLA [135] couples a VLM with a diffusion policy through a learnable latent interface.The VLM emits a token serving as the condition of the controller for action generation or a token to refuse infeasible commands.VQ-VLA [136] adopts a convolutional residual VQ-VAE [146], which is pretrained with action sequence, to take the place of the binning method of OpenVLA [26].The model shows linear performance gains from more simulated data and has less sim-to-real gap.Some models integrate the action expert into the VLA backbone, Fast-in-Slow [40] is a typical example.As shown in the "Unified Action Expert" part at the top-right of Fig. 5, the action expert utilizes the final transformer blocks of the VLM backbone.They run at different frequencies, enabling seamless coordination between the two systems within a single pretrained model.</p>
<p>Parallel-based Methods</p>
<p>As shown in the lower part of Fig. 5, parallel-based dualsystem VLA designs an action expert operating in parallel with the VLM backbone.The two components interact to exchange information during inference.Based on the choice of action expert and the interaction mechanism, two categories in Fig. 5 are defined.</p>
<p>As shown in the "Shared-attention Architecture" part at the bottom-left of Fig. 5, this architecture draws inspiration from the mix-of-experts (MoE) framework [147].They leverage token interactions within self-attention layers, which are shared by the VLM backbone and the action expert to separate high-level reasoning from low-level execution.Information derived from visual and textual inputs interacts with noise and robot-specific inputs within the shared self-attention layers.This design facilitates taskspecific optimization.For example, this design allows for the incorporation of a true MoE or other specially designed architectures within the action expert.A typical example is π 0 [29].Its backbone weights are initialized from a pretrained VLM.To handle robot-specific inputs and action generation, a second set of independent weights, the flowmatching-based action expert, is introduced and trained from scratch.ForceVLA [138] uses π 0 [29] as the base model, the FVLMoE module with MoE is used to introduce the force modality into VLA.OneTwoVLA [139] is based on π 0 [29] and can switch between two modes: explicitly reasoning and generating actions based on the most recent reasoning.This architecture makes it easier for the two systems to operate asynchronously and further improves efficiency.</p>
<p>Many innovations are built upon this.π 0.5 [30] builds on π 0 [29] by introducing an additional step.The VLA module integrates visual information to convert high-level prompts into more fine-grained subtask predictions, which are then processed by the dual-system architecture of π 0 [29].π 0.5 -KI [37] builds on π 0.5 [30].During training, it prevents gradients from the action expert from flowing into the VLM backbone during training to preserve the VLM's knowledge advantage.π 0 -FAST [124] introduces a DCTdriven action tokenization approach that facilitates efficient autoregressive training of VLA models.GraspVLA [143] proposes a dual-system model via Progressive Action Generation (PAG), which unifies autoregressive perception tasks and flow-matching-based action generation into a Chain-of-Thought process.This design enables joint training on synthetic and Internet data, achieving direct sim-to-real transfer TABLE 2: Dual-system VLA models.The "System 2 Backbone" column lists the VLM backbone used as the System 2 component in dual-system methods.The "System 1 Learning" column lists the learning methods used by the action experts as System 1. "Diff."denotes diffusion-based learning, "FM" denotes flow-matching, "MSE" denotes mean squared error, "BCE" denotes binary cross-entropy, and "AR" denotes autoregressive learning.</p>
<p>Model System 2 Backbone</p>
<p>System 1 Learning</p>
<p>Contribution</p>
<p>Cascade-based DP-VLA [125] OpenVLA Regression Propose a dual-system architecture for robot manipulation with efficiency and performance.RoboDual [126] OpenVLA Diff.Combine a VLA-based generalist for reasoning and a DIT specialist for control.LCB [127] LLaVA Diff.</p>
<p>Leverage an added special token to encode VLM reasoning and act as conditions for policy.GR00T N1 [32] Eagle-2 FM Combine a VLM and DiT for humanoid robots manipulation.CogACT [128] OpenVLA Diff.</p>
<p>Propose an action ensemble algorithm to integrate the action diffusion process into VLA.HiRT [129] InstructBLIP Regression Propose a dual-system model with System 2 running at a lower frequency.Fast-in-Slow [40] Prismatic Diff., AR Propose a unified dual-system model that embeds fast execution within a VLM-based reasoner.OpenHelix [58] LLaVA Diff.</p>
<p>Conduct auxiliary training on the token bridging VLM and policy.ChatVLA [130] Qwen2-VL Diff.</p>
<p>Unifie vision-language-action via MoE-shared attention with separate perception/control FFNs.ChatVLA-2 [131] Qwen2-VL Diff.</p>
<p>Enable open-world robotic reasoning via dynamic MoE routing and Reasoning-Following MLP.Diffusion-VLA [132] Qwen2-VL Diff.</p>
<p>Merge Qwen2-VL reasoning with diffusion actions via FiLM-modulated reasoning injection.TriVLA [133] Eagle-2 Diff.</p>
<p>Introduce a world-dynamics perception module as system 3 to complement static perception.GF-VLA [134] LLaMA 2 Regression Enable interpretable bimanual manipulation via information-theoretic graphs from human videos.RationalVLA [135] LLaVA-v1.5Diff.</p>
<p>Introduce a learnable latent interface to enable instruction rejection for robust manipulation.VQ-VLA [136] OpenVLA VQ-VAE Develop a vector quantization-based action tokenizer for efficient and smoother control.TinyVLA [137] LLaVA Diff.</p>
<p>Demonstrate that high-performance VLAs require no large-scale robotic pretraining.</p>
<p>Parallel-based</p>
<p>π0 [29] PaliGemma FM Combine a pre-trained Vision-Language Model with a Flow Matching-based Action Expert.π0-FAST [124] π0 AR Propose a DCT-based action tokenization enabling efficient autoregressive VLA training.π0.5 [30] PaliGemma FM Convert high-level prompts into more fine-grained subtask predictions before feeding into π0 π0.5-KI [37] PaliGemma FM Prevent gradients from the action expert from flowing into the VLM backbone during training.ForceVLA [138] π0 Diff.</p>
<p>Treat force sensing as a first-class modality via MoE, improving contact-rich manipulation.SmolVLA [31] SmolVLM-2 FM Propose a lightweight VLA with frozen SmolVLM-2 and flow-matching transformer.OneTwoVLA [139] π0 FM Integrate acting/reasoning in shared VLA backbone processing multi-view inputs.Tactile-VLA [140] π0 FM Integrate tactile sensing to enable force-aware, generalizable contact-rich manipulation.GR-3 [141] Qwen2.5-VLFM Combine VL data and few-shot trajectories for robust manipulation in long-horizon or unseen tasks.villa-X [142] PaliGemma FM Integrate proprioceptively grounded latent actions and robot actions in a joint diffusion process.GraspVLA [143] InternLM2 FM Enable sim-to-real and open-vocabulary grasping with synthetic data.</p>
<p>and open-vocabulary grasping.villa-X [142] grounds latent actions in robot states and jointly models latent and robot actions via joint diffusion for structured vision-action integration.Tactile-VLA [140] integrates tactile sensing into VLA models with hybrid position-force control.As shown in the "Cross-attention Architecture" part at the bottomright of Fig. 5, this architecture feeds the visual, textual, and state inputs into the VLM, where the self-attention layers generate key-value and passed key-value to the action expert's cross-attention layers.For instance, SmolVLA [31] adopts this architecture.It improves efficiency by leveraging a lightweight and frozen VLM backbone and training only a downstream Flow Matching Transformer as the action expert.Another example is GR-3 [141], which unifies visionlanguage understanding with robot trajectory learning using flow-matching.</p>
<p>HIERARCHICAL MODELS</p>
<p>Hierarchical modelling serves as a foundational paradigm in large VLM-based VLA models, particularly in scenarios where long-horizon reasoning, spatial abstraction, or action decomposition is required.These models are typically composed of a high-level planner and a low-level policy, as illustrated in Fig. 3 right.The planner receives instructions and observations, transforming them into interpretable intermediate representations.The policy then accepts these representations and generates action sequences or codes that robots can directly execute.A summary of hierarchical models is provided in Tab. 3. Importantly, the planner and policy in hierarchical models can operate independently, without either module being strictly dependent on the other.This modularity enables flexible combinations: many works focus solely on designing the planner component, leveraging existing off-the-shelf policies for execution.Therefore, we divide hierarchical models into two categories: Planner-Only (Sec.4.1) and Planner+Policy (Sec.4.2).Unlike dual-system VLA models, which also involve multiple modules, the intermediate representations in hierarchical models are explicitly interpretable to humans.Based on their nature, each category can be further divided into subtask-, keypoint-, and programbased methods.A comparative analysis of monolithic and hierarchical models is presented in Sec.4.3.</p>
<p>Planner-Only</p>
<p>Program-based Methods</p>
<p>In this approach, planners generate intermediate programs for robotic manipulation, which fall into two categories: robot-executable programs and auxiliary programs.Robotexecutable programs are built on robot libraries and can be directly executed to control the robot.For instance, Chainof-Modality [151] employs a multimodal prompting strat- egy, where the VLM is engaged in a multi-turn conversation across different modalities and ultimately generates a robotexecutable Python program to reproduce the task.Similarly, Instruct2Act [159] produces Python code that invokes APIs to control robot actions.In contrast, auxiliary programs support the policy in task understanding but cannot be executed directly.ROVI [152] exemplifies this category by generating auxiliary programs to describe the potential action and resolve the actual execution through translational and rotational costs.Likewise, ReLEP [153] uses a VLM with a memory bank to decompose tasks into basic skills from a skill library.It produces plans in the form of auxiliary programs that enable strong long-horizon performance.</p>
<p>Keypoint-based Methods</p>
<p>Keypoint-based planners use VLMs to predict salient points in an observation, typically corresponding to interactive regions that the gripper should reach (e.g., the handle of a drawer).Several methods tackle robotic manipulation by predicting waypoints.MoManipVLA [148] generates a key waypoint at each step through a VLA model, which is subsequently refined into executable actions via a bilevel trajectory optimization framework.Other approaches emphasize affordance-driven keypoints.RoboPoint [149] interprets natural language instructions to generate visual keypoints that specify precise manipulation targets.Besides, ManipLVM-R1 [46] trains a VLM with Group Relative Policy Optimization (GRPO) [173] to predict both the affordance area for grasping and the trajectory of the target object, thereby yielding a more generalizable planner.Similarly, RoboBrain [155] integrates task planning, affordance perception, and trajectory estimation by combining LLaVA [21] for high-level planning with an A-LoRA module to identify interactable regions and a T-LoRA module to predict trajectory waypoints.This indicates that these models span both affordance and waypoints prediction, forming a hybrid representation.Moreover, some works explore distinctive forms of keypoints.RoVI [152], for instance, employs a sketchbased interface, extracting starting points, waypoints, and endpoints from hand-drawn annotations using YOLOv8, which are then applied as trajectory constraints.</p>
<p>Subtask-based Methods</p>
<p>In this approach, the planner is typically a large VLM that receives high-level implicit instructions (e.g., clean up the table) along with observations, and decomposes them into step-by-step textual commands.Since these models produce interpretable intermediate instructions rather than executable actions, a low-level control policy is still required in practical deployment.Early efforts such as PaLM-E [85], a LLaVA-style VLM trained with robotic manipulation data, demonstrate the feasibility of unifying general VQA capabilities with robot command generation.Building upon this direction, Embodied-Reasoner [47] introduces Observation-Thought-Action trajectories to support spatial analysis, reflection, and verification during step-wise planning.Reinforced Planning [150] retains this subtaskdecomposition setup but boosts generalization through a two-stage pipeline of SFT followed by GRPO-based reinforcement fine-tuning.Meanwhile, some systems decouple perception from reasoning: Embodied-R [174] combines a large VLM for perception with a small LM for reasoning for embodied spatial reasoning on video, and its reasoning traces can be leveraged to derive stepwise subtask proposals for manipulation.In contrast, ViLA [154] leverages GPT-4V as an external planner, prompting it to generate candidate TABLE 3: Hierarchical VLA models.The "Type" column denotes the output type of the planner, where "K" represents Keypoint, "S" represents Subtask, and "P" represents Program.The "Learning" column specifies the learning method adopted by the model, where "SFT" refers to Supervised Fine-Tuning, "RL" denotes Reinforcement Learning, "IM" indicates Imitation Learning, and "API" is a special case referring to the invocation of pre-existing models.task plans from textual and visual inputs and then executing only the first command, discarding the remainder.</p>
<p>Planner+Policy</p>
<p>Keypoint-based Methods</p>
<p>This type of model generally uses a large VLM to ground subgoals as spatial primitives, such as discrete keypoints or a 2D path on the image.The low-level policy then consumes these primitives to predict continuous trajectories and control.For instance, HAMSTER [48] first predicts trajectory keypoints from instructions and observations.Then it links these keypoints into an ordered path with a gradient color, and overlays this guidance for policy execution.Besides, ReKep [50] utilizes DINOv2 [93] and SAM [175] to produce keypoint proposals, then uses GPT-4o [176] to turn those keypoints into cost functions.These functions are subsequently solved into waypoints and actions by an optimizer.Moreover, A 0 [49] adopts an affordance-aware hierarchy: the planner predicts contact points and post-contact motion as an embodiment-agnostic affordance representation, which the action policy then converts into control.</p>
<p>Subtask-based Methods</p>
<p>Subtask-based hierarchical models bridge the planner and policy with instructions.The planner plays the same role as a subtask-based planner-only hierarchical model, and a low-level policy is appended to generate action sequences.</p>
<p>A representative example is HiRobot [156], the planner of which accepts open-ended user instructions and then decomposes them into atomic commands for the policy.Similarly, DexVLA [158] features a VLM planner and a diffusion-based action policy.The policy is conditioned on the subtask tokens given by the VLM planner, excelling in complex and long-horizon robotic manipulation tasks.Built on DexVLA, PointVLA [161] enhances spatial perception by incorporating a point cloud encoder and injector into the policy, enabling the model to follow the planner's instructions in geometrically complex scenes.RoBridge [163] prompts the planner to generate text instructions of primitive actions and form an invariant operable representation for the policy to execute.Besides, SkillDiffuser [167] decomposes complex tasks into subtasks through a high-level model that predicts a set of skills, while a low-level diffusion policy realizes concrete actions.Analogously, RoboMatrix [160] organizes execution into a three-layer hierarchy: a modular scheduling layer generates subtask sequences, the skill layer encodes and selects reusable behaviors, and the hardware layer implements robot control.HiBerNAC [169] proposes an asynchronous, hierarchical framework.In this framework, the multi-agent neural structure first decomposes high-level instructions into structured subtasks.Then, an asynchronous pipeline manages these subtasks and coordinates the reactive VLA to execute the final low-level actions.Moreover, MALMM [171] includes a planner, a supervisor, and a coder.The planner generates subtasks for the coder, and the supervisor coordinates transitions between modules.The coder serves as the policy and converts plans into executable robot code, including actions and positions.</p>
<p>Comparison: Monolithic vs. Hierarchical</p>
<p>The difference between monolithic and hierarchical architectures in large VLM-based VLA models for robotic manipulation lies primarily in how they map visual inputs and linguistic instructions into actions, either through a unified or modular approach.Monolithic models emphasize a single, integrated pipeline that jointly optimizes perception, reasoning, and control to directly translate high-level multimodal semantics into low-level actions.This design enables holistic and tightly coupled learning in robotic manipulation.Conversely, hierarchical architectures adopt a multistage design that explicitly separates high-level planning from low-level policy execution, promoting modularity and interpretability.This system decomposition allows components to be independently designed, trained, or replaced, enhancing flexibility and easing the integration of domain knowledge or adaptation to new robotic manipulation tasks.</p>
<p>Another core distinction lies in the inherent nature of the intermediate processing.Monolithic models, while potentially embedding intricate internal reasoning, do so within latent spaces that are often opaque to external inspection.This implicit strategy allows them to fully exploit the representational capacity of large models, potentially discovering efficient task decompositions not explicitly designed by humans.Hierarchical systems, on the other hand, explicitly commit to generating explicit, human-understandable intermediate outputs.This makes them particularly advantageous in robotic manipulation scenarios that demand explainability, detailed task monitoring, or compatibility with traditional robotics pipelines, where high-level plans can be independently validated or modified.Despite these differences, both approaches contribute unique strengths to the evolving landscape of VLA for robotic manipulation.Monolithic models highlight the power of unified learning and minimal manual decomposition, offering a streamlined route to generalization across diverse tasks.Hierarchical frameworks, by explicitly layering cognition and control, provide greater transparency and enhanced modular flexibility, which can be critical for complex multi-stage tasks or safety-critical deployments.Together, these paradigms illustrate complementary strategies for bridging vision-language understanding and embodied action, each offering valuable insights into the design of next-generation intelligent robotic manipulation systems.</p>
<p>OTHER ADVANCED FIELD</p>
<p>Beyond core VLA architectures, we highlight four directions for robustness, efficiency, and long-horizon planning: RLbased optimization, training-free improvements, learning from human videos, and world model-based VLA.Tab. 4 summarizes representative methods.</p>
<p>Reinforcement Learning-based Methods</p>
<p>Reinforcement learning (RL) plays an important role in enhancing VLA's generalization ability [177] and task completion rate [178].It can be broadly categorized as online, which optimizes policies through real-time interaction, and offline, which learns from pre-collected trajectories.Most VLA methods are built upon traditional RL algorithms.We list several approaches in Tab. 5.</p>
<p>Unlike RL in LLMs or VLMs [79], [179], [180] that usually finish generation in a few turns, VLA models typically involve long-horizon tasks with hundreds of steps in one trajectory, making rule-based reward functions prone to sparsity and instability [177], [181].To address this, many works incorporate learned dense reward signals.VLA-RL [181] trains a Robotic Process Reward Model (RPRM) to predict the success likelihood of action sequences.ReWiND [39] models reward as progress toward the final goal state, assigning higher rewards to states visually closer to task completion.In addition to training a reward model, Grape [177] and TGRPO [182] leverage powerful VLMs [176], [183] via prompting to generate feedback-based reward signals.</p>
<p>Besides reward sparsity, online learning is sampleinefficient due to slow simulation.To address this, several methods adopt hybrid offline-online training.ReWiND integrates offline Implicit Q-Learning [184] with online Soft Actor-Critic [185], accelerating learning and enhancing safety in real-world deployment.HIL-SERL [186] is based on RLPD and introduces human-in-the-loop intervention in the training process.ConRFT [187] employs a two-phase training scheme: an offline phase using Cal-ConRFT (combining behavior cloning and Q-learning) to initialize policy, followed by online HIL-ConRFT, which balances supervised and RL losses with human-in-the-loop intervention.</p>
<p>Furthermore, some methods leverage RL as a data engine to enhance generalist robotic models.RLDG [188] is a prime example, which trains expert policies via HIL-SERL [186] to 100% task success, then distills them into foundation models via sft.Similarly, iRe-VLA [189] alternates between online RL (to collect new successful trajectories) and sft on the expanded dataset, progressively improving the model through iterative imitation and exploration.</p>
<p>Training-Free Methods</p>
<p>Training-free methods typically leverage modular and extensible designs to improve existing VLA architectures without training.This enables rapid prototyping, ablation studies, and targeted enhancements while preserving the model's original capabilities and avoiding additional costs.</p>
<p>A range of training-free methods has been proposed to improve the efficiency of large VLM-based VLA models efficiency without retraining or architectural changes.A compelling example is FlashVLA [115], which employs a</p>
<p>Description Method Reinforcement Learning-based Methods</p>
<p>Use RL to optimize robotic policies through interaction or precollected trajectories.</p>
<p>VLA-RL [181], ReWiND [39], Grape [177], TGRPO [182], HIL-SERL [186], ConRFT [187], RLDG [188], iRe-VLA [189] Training-Free Methods</p>
<p>Improve VLA models via architectural or computational optimizations without retraining.</p>
<p>FlashVLA [115], EfficientVLA [190], VLA-Cache [191], PD-VLA [113], SP-VLA [192], BAC [193], FAST [124], RTC [41] Learning from Human Videos</p>
<p>Leverage human videos to adapt robot policies, enabling cross-domain transfer.</p>
<p>Human-Robot Semantic Alignment [42], UniVLA [194], LAPA [195], VPDD [196], 3D-VLA [197], Humanoid-VLA [198] World Model-based VLA Integrate predictive world models into VLA to model environment dynamics.</p>
<p>World-VLA [38], World4Omni [43], 3D-VLA [197], RIGVid [199], FoundationPose [200], V-JEPA 2-AC [201] TABLE 5: Representative RL approaches for VLA.✓ stands for online and ✗ stands for offline."S" stands for sparse reward, "D" for dense reward, "RM" denotes a pre-trained reward model, "GPT" denotes a reward given by GPT, and "TC" denotes a reward function based on task completion.</p>
<p>Method RL Algo. Online Reward Formulation</p>
<p>VLA-RL [181] PPO ✓ TC (S) + RM (D) RIPT-VLA [202] LOOP ✓ TC (S) RLVLA [178] PPO ✓ TC (S) + Object Grasp (S) TPO [177] DPO ✓ TC (S) + GPT (S) ConRFT [187] -✗ + ✓ TC (S) TGRPO [182] GRPO ✓ TC (S) + GPT (D) iRe-VLA [189] PPO ✓ TC (S) RLDG [188] HIL-SERL ✓ TC (S) ReWiND [39] IQL SAC ✗ + ✓ TC (S) + RM (D) HIL-SERL [186] RLPD
✗ + ✓ TC (S)
trigger mechanism that skips full decoding when action and visual cues remain stable, selectively reusing or pruning visual tokens.Similarly, EfficientVLA [190] prunes redundant language layers, filters task-relevant visual tokens, and caches intermediate features, while VLA-Cache [191] reuses cached key-value representations [91] of static tokens with task-relevance filtering and layer-adaptive reuse.Beyond these, SP-VLA [192] combines spatio-semantic token pruning with an action-aware scheduler that routes intuitive steps to a lightweight generator and complex ones to the full VLA.Likewise, PD-VLA [113] reformulates autoregressive decoding under action chunking as a parallel fixedpoint iteration, enabling simultaneous token prediction.FAST [124] compresses action sequences via discrete cosine transform and byte-pair encoding to reduce redundancy in high-frequency control, achieving up to 5× faster training.Furthermore, RTC [41] optimizes control-time scheduling by monitoring task progress and adjusting control frequency, reducing unnecessary computation during inference.</p>
<p>Learning from Human Videos</p>
<p>Recent advancements in VLA models have introduced several novel learning paradigms that surpass conventional policy learning strategies.Among them, a notable trend emerges: leveraging human video data to guide robot policy learning, using structural similarities in human-object and robot-object interactions to align visual and temporal cues.This method aims to narrow the embodiment gap by transferring task-relevant knowledge from rich human video data.It uses human videos to pre-train perception or adapt policies.For instance, Human-Robot Semantic Alignment [42] aligns vision encoders across domains using paired human-robot videos, while UniVLA [194] learns task-centric latent actions from unlabeled human and robot videos to unify policy planning.Similarly, LAPA [195] leverages VQ-VAE [146] quantized latent actions to pre-train on large-scale video-language pairs, enabling transfer from actionless human videos.Besides, VPDD [196] applies discrete diffusion over unified video tokens, facilitating crossdomain knowledge transfer through future dynamics prediction.3D-VLA [197] integrates human-object interaction videos with robot demonstrations for richer 3D reasoning, and Humanoid-VLA [198] exploits pose-recovered motion trajectories from online videos to enhance motion diversity.Collectively, these methods demonstrate that human video data can substantially enrich robot policy learning, yielding robustness and versatility even with limited robot data.</p>
<p>World Model-based VLA</p>
<p>World models, characterized by their ability to learn compact latent representations of environment dynamics, have emerged as powerful tools for enabling predictive reasoning and long-horizon planning.Recently, the integration of world models into VLA systems has gained traction as a promising strategy to enhance action planning through explicit modeling of environment dynamics.</p>
<p>Rather than directly generating actions from current observations, these approaches simulate future states, enabling agents to anticipate the consequences of their actions and refine decision-making accordingly.For instance, WorldVLA [38] introduces an autoregressive action world model that jointly learns to predict visual outcomes and generate actions within a unified token-based architecture.This mutual reinforcement between the world model and the action model improves both visual imagination and action fidelity, supporting more robust long-horizon planning.World4Omni [43] employs a large-scale world model to produce subgoal images that depict intermediate task states.These generated visual cues are then used to guide a modular low-level policy, enabling zero-shot manipulation across diverse environments and robot embodiments.3D-VLA [197] employs a generative world model to predict future goal images and point clouds conditioned on instructions, effectively simulating scene dynamics.RIGVid [199] uses a diffusion-based world model to generate candidate task videos, which are then filtered by a VLM for feasibility, with FoundationPose [200] extracting gripper poses for execution.V-JEPA 2-AC [201] builds a latent action-conditioned world model on top of a massive self-supervised video encoder trained on internet-scale data.During inference, it performs model-based planning by simulating future latent states conditioned on goal images, bridging dense video understanding with zero-shot robotic manipulation.</p>
<p>CHARACTERISTICS OF VLA MODELS</p>
<p>Multimodal Fusion</p>
<p>Shared Embedding Space.Large VLM-based VLA models embed visual observations and linguistic instructions into a shared, semantically aligned latent space.Unlike traditional robotic manipulation models that modularize perception, instruction parsing, and control, Large VLM-based VLA models rely on pre-trained VLM backbones to generate joint embeddings for both modalities.This unified space facilitates tight semantic grounding between perception and command, improving action decoding fidelity and reducing inter-module semantic drift [8].</p>
<p>Multimodal Token-Level Integration.A core characteristic in many VLA models is discretizing continuous modalities such as vision, language, proprioception, and actions into token sequences processed by a single transformer [40], [107], [203].This design enables token-level multimodal integration and fine-grained coordination: linguistic tokens like "grasp red cup" immediately direct attention to relevant visual tokens, while proprioceptive and action tokens guide execution.The interleaved temporal structure naturally captures cross-modal dependencies across the perception-action cycle.Compared to traditional methods that fuse modalities at pre-defined stages, this dynamic integration reduces latency and semantic fragmentation.</p>
<p>Comprehensive Modal Compatibility.Another notable characteristic of VLA models lies in their inherent ability to seamlessly accommodate diverse sensory modalities, such as depth (e.g., point clouds), tactile input, and ambient audio.This compatibility stems from the modality-agnostic semantic alignment inherited from pre-trained VLMs, which allows new sensory tokens to be seamlessly integrated without altering the core architecture or requiring full model retraining.For instance, PointVLA [161] integrates point cloud data into pre-trained VLA architectures to enrich spatial perception and generalization, without retraining the backbone.Similarly, 3D-VLA [197] aligns point cloud features with language embeddings to achieve a more precise understanding and reasoning over complex 3D environments.Likewise, tactile and auditory tokens improve responsiveness to environmental subtleties.In contrast, traditional robotic manipulation models require costly redesigns when adding new modalities, making VLA models more scalable and robust for real-world deployment.</p>
<p>Instruction Following</p>
<p>Semantic Instruction Grounding.Instruction following in large VLM-based VLA models transcends the brittle and fragmented pipelines of traditional robotic manipulation models.Specifically, VLA models harness the rich world knowledge of pre-trained VLMs to interpret and ground natural language instructions into actionable behaviors dynamically.Rather than reducing instructions to fixed templates, they enable fluid, context-sensitive comprehension.For instance, ChatVLA-2 [131] can interpret math problems written on whiteboards and select the correct number cards.Despite lacking explicit training in mathematics, it demonstrates an ability to generalize from visual-linguistic priors.</p>
<p>Task Decomposition and Collaboration.Beyond direct instruction-to-action translation, instruction following often involves hierarchical task decomposition, especially for long-horizon tasks.Specifically, high-level commands are broken down into sub-goals, which are then executed by low-level controllers [156].LoHoVLA [106] serves as a compelling example that illustrates how this paradigm enables effective instruction following.It generates intermediate subtasks in natural language from visual and linguistic inputs.Then these subtasks guide downstream action generation and ensure semantic alignment throughout execution.By continuously synchronizing subtask intentions with actions, the model maintains coherence in long-horizon tasks, supporting reliable goal completion.</p>
<p>Explicit Reasoning via Chain-of-Thought.A key fact of the instruction following capability of VLA models is the integration of chain-of-thought reasoning into the decisionmaking loop.In CoT-VLA [33], for instance, the model interposes latent visual goals-predicting future images-before generating corresponding action sequences, effectively embedding a sub-goal generation mechanism.This explicit reasoning ties instructions to anticipated visual outcomes.It helps mitigate shortsighted hallucinations frequently observed in monolithic systems.In addition, it allows iterative refinement of plans, thereby facilitating more reliable execution of complex, multi-step tasks.</p>
<p>Multi-Dimensional Generalization</p>
<p>Cross-Task Generalization.Most VLA models exhibit remarkable zero-shot and few-shot generalizations, representing a leap beyond the brittle performance of traditional robotic manipulation models that often require extensive retraining.A prime example is DexVLA [158], which couples a large VLM with a billion-parameter diffusion action expert and an embodied curriculum to transfer skills across embodiments and rapidly adapt to unseen tasks.Without task-specific tuning, it attains consistently high success on diverse manipulations and generalizes to novel objects and scenes.Under direct language prompting, it also completes complex long-horizon workflows and outperforms state-ofthe-art baselines such as OpenVLA [26] and π 0 [29].</p>
<p>Cross-Domain Data Generalization.This capacity extends further: π 0.5 and similar "foundation VLA" models are co-trained on heterogeneous data-web text, simulation videos, and cross-robot embodiments-to acquire broad contextual and semantic understanding.As a result, they successfully deploy "out-of-the-box" in new home environments, executing multi-stage tasks like dishwashing with OOD success rates exceeding 90% [204].This wide compatibility starkly contrasts with traditional pipelines, which typically collapse outside their training distribution.</p>
<p>Cross-Embodiment and Sim-to-Real Generalization.Cross-embodiment and sim-to-real transfer represent another dimension of generalization.Hierarchical VLA architectures, which learn high-level planners in VLM space and delegate low-level control to domain-specific decoders, enable leveraging off-domain data (e.g., hand-drawn sketches or simulations in diverse setups) to generalize across varying dynamics and robot morphologies.One such model, HAMSTER [48], achieves a 20% success boost over Open-VLA across seven axes of generalization.</p>
<p>DATASETS AND BENCHMARKS</p>
<p>Real-world Robot Datasets</p>
<p>Real-world data, capturing environmental complexity such as dynamic lighting, background motion, and varied object properties, is vital for training and evaluating VLA models.Effective language grounding and semantic generalization rely on aligning natural language instructions, perceptual inputs, and precise action responses.To support this, a series of large-scale real-world datasets have emerged.BC-Z [205] provides expert demonstrations with language commands across 100 tasks; RT-1 [83] and its kitchen variant extend this to over 700 daily activities, while RT-2 [27] incorporates web-scale vision-language data for broader vocabulary and instruction generalization.RoboFlamingo [111] exemplifies the transfer of pretrained multimodal priors into realworld control.BridgeData V2 [206] and RH20T [207] further enhance cross-domain generalization through extensive, language-annotated demonstrations, with RH20T enabling one-shot learning across 147 tasks.DROID [208] expands this landscape with teleoperated demonstrations "in the wild," offering diverse visual contexts over 564 tasks.Most notably, the OXE dataset [90] unifies over 1 million multimodal demonstrations across 22 robot platforms and 500+ skills, marking a significant step toward web-scale, crossembodiment generalization.Although these efforts improve real-world applicability, the long-tail distribution of openworld objects, scenes, and skills remains underrepresented, underscoring the need for broader and more diverse realworld data to scale large VLM-based VLA models further.</p>
<p>Simulation Datasets and Benchmarks</p>
<p>To overcome real-world data limitations, physics-based simulators and virtual environments offer scalable, safe, and reproducible interaction data, aligning with the multimodal, large-scale training needs of large VLM-based VLA models.They enable complex instruction following, multistage planning, and consistent language grounding with automatic evaluation.BEHAVIOR [209] supports multistep semantic control in cluttered household settings; AL-FRED [210] focuses on long-horizon tasks specified by egocentric language instructions; RLBench [211] allows finegrained policy learning through RGB-D tabletop manipulation, and RLBench2 [212] extends it to bimanual manipulation with language-conditioned action prediction.Benchmarks such as Meta-World [213], Franka Kitchen [214], and LIBERO [215] further advance this direction by incorporating multi-skill scenarios, often enriched with language annotations and vision-language rewards to encourage generalization.CALVIN [216] enables multi-stage manipulation under unconstrained language instructions to support long-horizon behaviors.MIKASA-Robo [217] addresses memory-centric challenges by evaluating agents under partial observability in tabletop tasks.SIMPLER [218] bridges the sim-to-real gap through calibrated simulated environments aligned with common real-world robot configurations, reducing discrepancies in perception and control.High-fidelity platforms such as Habitat [219]- [221] and SAPIEN [222] support spatial reasoning and manipulation learning in realistic 3D environments, enabling scalable embodied VLA research.Beyond traditional simulation benchmarks, THE COLOSSEUM [223] and VLABench [224] extend simulation benchmarks by assessing robustness under distribution shifts and supporting universal languageconditioned manipulation, respectively.Despite limitations such as imperfect physics and visual artifacts, simulation remains crucial for efficient training, systematic evaluation, and robust real-world generalization.</p>
<p>Human Behavior Datasets</p>
<p>Large-scale human behavior datasets offer semantically rich and contextually diverse demonstrations that are wellsuited for pretraining VLA models.Egocentric video corpora such as Ego4D [225], Ego-Exo4D [226], EgoPlan-Bench [227], and EgoVid-5 [228] capture diverse daily activities from a first-person perspective.In contrast, domainspecific datasets like EPIC-Kitchens [229] and COM-Kitchens [230] focus on fine-grained, temporally structured cooking tasks.These resources support learning of object recognition, action sequencing, and task decomposition.Reasoning-oriented datasets such as EgoVQA [231] and EgoTaskQA [232] further enhance cognitive grounding by introducing spatial, temporal, and causal reasoning over egocentric video.To address the lack of precise manipulation signals in general-purpose corpora like Ego4D, EgoDex [233] extends this line of work with 829 hours of egocentric video paired with dense 3D hand and finger tracking.Similarly, smaller manipulation-focused datasets such as DexCap [234], EgoMimic [235], and PH 2 D [236] offer fine-grained supervision for hand-object coordination and embodiment-aware policy learning.Complementing these human-centric datasets, R2R2R [237] offers a scalable pipeline that replaces teleoperation with smartphone scans and human videos to generate high-fidelity demonstrations.Together, these resources provide a rich spectrum of semantic grounding, embodiment alignment, and data scalability essential for training generalist VLA models.</p>
<p>Embodied Datasets and Benchmarks</p>
<p>As robots advance toward greater autonomy, understanding and planning tasks become essential.Leveraging strong language and world knowledge, VLMs empower VLA systems to plan over long horizons, parse complex instructions, and build cross-modal reasoning for semantic decision-making.In response, a growing set of benchmarks has emerged to evaluate embodied agents' planning and reasoning abilities.Early efforts such as EmbodiedQA [238] and IQUAD [239] established the integration of vision, language, and spatial navigation through question-answering in 3D environments.These have since evolved into more cognitively demanding tasks.MT-EQA [240] incorporates multi-target reasoning, MP3D-EQA [241] introduces 3D point cloud inputs for enhanced spatial grounding, EQA-MX [242] adds non-verbal cues for naturalistic multimodal interaction, and OpenEQA [243] expands the scope to open-ended questions involving functional and commonsense reasoning.Complementarily, LoTa-Bench [244] directly evaluates plan executability by deploying LLM-generated plans in simulation.Collectively, these benchmarks emphasize active perception, reasoning, and execution, offering rigorous protocols to assess high-level semantic planning in general-purpose VLA.</p>
<p>FUTURE DIRECTIONS</p>
<p>Datasets and Benchmarking.As VLA models advance, the reliance on simulated datasets reveals a significant reality gap.Synthetic scenes lack the visual complexity of real environments.By contrast, collecting real-world data is costly, limiting both diversity and scale.Meanwhile, existing benchmarks typically focus on short-horizon pick-and-place tasks and report simple success rates.These settings offer little insight into practical challenges such as long-term planning, mobile manipulation and multi-agent collaboration.Future datasets may combine large-scale real-world data acquisition with task suites that capture practical challenges.Evaluations should include richer metrics, such as subtask success, time efficiency, and robustness to disturbances.Such datasets and benchmarks would enable systematic progress on the broader capabilities of VLA systems.</p>
<p>Memory Mechanisms and Long-Term Planning.Effective real-world manipulation often requires planning over extended horizons and recalling past observations.However, most current VLAs rely on frame-by-frame reasoning, resulting in short-sighted behavior and a limited ability to leverage historical context.To address this, a promising direction is to design architectures that incorporate forwardlooking planning and memory mechanisms with episodic awareness.By grounding high-level decisions in contextual memory, agents may move beyond reactive responses toward coherent, goal-driven action sequences, enabling more consistent and effective manipulation in complex scenarios.</p>
<p>3D and 4D Perception.Robotic manipulation inherently involves interacting with objects in a three-dimensional and temporally dynamic environment.However, existing VLA models operate primarily on static 2D visual inputs.This limits the agent's ability to reason about complex spatial and temporal information, including depth, affordances, object movement, and human actions.Moving beyond static 2D snapshots, 4D perception demands a model to understand how 3D scenes evolve over time.The progression from 2D to 4D perception requires integrating depth or point cloud observations, as well as fusing multi-modal inputs into unified representations.It also involves embedding perception that is grounded in temporal context and replanning actions on the fly.Together, these 4D-aware abilities would enable more robust and adaptive manipulation in the real world.</p>
<p>Mobile Manipulation.Real-world tasks often demand the simultaneous execution of locomotion and manipulation, giving rise to the specialized domain of mobile manipulation.This task requires a synergistic integration of navigation capabilities and interactive skills with the environment, introducing tightly coupled demands on perception and control.Rather than treating locomotion and grasping as separate stages, future VLA models may benefit from learning integrated policies that adaptively prioritize locomotion and arm coordination, ultimately leading to more robust and flexible mobile manipulation.</p>
<p>Multi-Agent Cooperation.Many real-world collaborative tasks, such as bulky objects moving or joint tool use, require agents to negotiate intentions, adapt to teammates' actions, and jointly reason over multi-step goals.However, single-agent VLAs often falter when communication and role assignment become critical, highlighting the need for interaction-aware representations.Integrating emergent dialogue protocols with shared world models may enable agents to synchronize plans and flexibly allocate subtasks.Such cooperative capabilities promise to elevate robotic teams from loosely coordinated individuals to cohesive collaborators capable of addressing complex group objectives.</p>
<p>Lifelong Learning in Open-World.Robotic autonomy demands continual skill acquisition without catastrophic forgetting.However, most VLA models trained on static datasets struggle to handle unfamiliar objects and novel interaction modes.They also have difficulty incorporating new experiences.This reveals the limits of static training paradigms.Future VLA systems may benefit from mechanisms that incrementally accumulate knowledge through exploration and feedback in open-world.Incorporating memory structures that grow over time and enabling agents to self-organize experiences into reusable abstractions may also provide a foundation for long-term competence.</p>
<p>Model Efficiency.Though VLA models excel at understanding vision-language instructions and planning actions, they often suffer from prohibitive computational and memory costs.Deploying these models on resource-constrained robotic platforms raises concerns about latency, memory usage, and sustained operation.The key challenges lie in balancing model capacity with real-time inference requirements and preserving multimodal alignment to maintain accuracy during compression.Promising directions include task-aware dynamic token pruning, asynchronous inference for seamless transitions between action chunks, and hardware-friendly quantization schemes.</p>
<p>CONCLUSION</p>
<p>In conclusion, this survey provides the first principled synthesis of VLA models built on large VLMs.We trace the evolution of VLMs, analyze their integration into robotic manipulation, and propose a two-part taxonomy: Monolithic (Single-system and Dual-system) and Hierarchical (Planner-Only and Planner+Policy).This taxonomy frames existing models and guides future VLA design.We also organize datasets, benchmarks, learning paradigms, and advanced methods into a coherent framework; identify defining characteristics such as open world generalization, multimodal reasoning, and instruction grounding in dynamic settings; and summarize trends toward modular planning and control, larger multimodal corpora, and closer links with reinforcement learning and world models.Looking ahead, priorities include adaptation across embodiments, scalable real-world deployment, tighter coupling between high-level reasoning and low-level execution, and the use of human demonstrations at internet scale.We aim for this survey to serve as a foundation for embodied AI that unifies perception, language, and action.</p>
<p>Fig. 1 :
1
Fig. 1: Illustration of core advantages of large VLMbased Vision-Language-Action (VLA) models for robotic manipulation.Large VLM-based VLA models leverages the strengths of large Vision-Language Models (VLMs), including (1) open-world generalization, (2) hierarchical task planning, (3) knowledge-augmented reasoning, and (4) rich multimodal fusion.These capabilities empower diverse robotic arms and significantly enhance robotic intelligence.</p>
<p>Sec. 4 explores hierarchical models, categorizing them into planner-only and planner-policy frameworks, and further sub-classifying by intermediate representation types such as subtasks, keypoints, and programs.Sec. 5 discusses other advanced fields, including RL-based optimization, training-free methods, learning from human videos, and world model-based approaches.Sec.6 analyzes the defining characteristics of large VLM-based VLA models, covering multimodal fusion, instruction following, and multidimensional generalization.Sec.7 categorizes and analyzes the diverse datasets and benchmarks employed in large VLM-based VLA models research, spanning simulated, realworld, and human interaction data.Sec. 8 explores critical open challenges and promising future research directions.Finally, Sec. 9 concludes the survey.</p>
<p>Fig. 4 :
4
Fig. 4: Comparison of representative paradigms in monolithic single-system models.Sec.3.1.1illustrates the model schematic of classical autoregressive decoding.Arrows denote the flow of information.Sec.3.1.2presents approaches to enhance model capabilities by incorporating additional modalities, leveraging chain-of-thought reasoning, and strengthening generalization.Sec.3.1.3discusses some methods for improving the inference efficiency through architectural refinement, parameter design, and decoding strategies.</p>
<p>Fig. 5 :
5
Fig. 5: Comparison of representative paradigms in monolithic dual-system models.Sec.3.2.1 introduces cascadebased methods, where the VLM backbone is cascaded with the action expert to forward its output features in a single pass to the action expert.Sec.3.2.2introduces parallel-based methods, where the VLM backbone and the action expert operate in parallel and interact in various ways.</p>
<p>Fig. 6 :
6
Fig. 6: A diagram showing the hierarchical models in this survey.These models are divided into two categories according to their constitutions, i.e., Planner-Only (Sec.4.1) and Planner+Policy (Sec.4.2).Based on the intermediate representation type, one category can be further divided into subtask-based (S), keypoint-based (K), and program-based (P)."(A)" denotes methods combining affordance for auxiliary purposes.</p>
<p>Fig. 7 :
7
Fig. 7: Illustration of four dataset types that underpin large VLM-based VLA models for robotic manipulation.</p>
<p>Obs. Instr. State Action Autoregressive Decoding Obs. Instr. State Action Inference Acceleration (Parallel Decoding) Action Initialization Architectural Optimization (Dynamic Inference) Layer 1 Layer 3 Skipping Layer 2 Obs. Instr. Action Obs. Instr. State Action Parameter Optimization (Model Compression) 3.1.1 Classic Paradigm: Autoregressive Decoding 3.1.3 Paradigm Derivations: Inference Efficiency Optimization 3.1.2 Paradigm Derivations: Model Performance Enhancement Obs. Instr. State Action Enhancing Reasoning Capabilities Reasoning Obs. Instr. State Action Enhancing Perception Modalities Audio Touch Space Trace State VLM VLM ✂ Step1 Step2 ••• Enhancing Generalization Capabilities Obs. Instr. State VLM Robust and Reliable Action LLM 🏠 🏢 🏭</p>
<p>Action BLOCK BLOCK Self-attention Layer KV Cross-attention Layer Action Self- attention Layer System 2 VLM Backbone System 1 Action Expert Self- attention Layer Self- attention Layer Self-attention Layer KV Cross-attention Layer Attention Layer
Obs.Instr.StateObs.Instr.State</p>
<p>TABLE 4 :
4
Representative VLA methods grouped into four advanced categories: Reinforcement Learning-based, Training-Free, Learning from Human Videos, and World Model-based approaches.The grouping follows the methodological analysis in Sec. 5.</p>
<p>A survey on robotics with foundation models: toward embodied ai. Z Xu, K Wu, J Wen, J Li, N Liu, Z Che, J Tang, arXiv:2402.023852024</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024</p>
<p>Metaurban: a simulation platform for embodied ai in urban spaces. W Wu, H He, Y Wang, C Duan, J He, Z Liu, Q Li, B Zhou, ICLR2025</p>
<p>K Zhang, P Yun, J Cen, J Cai, D Zhu, H Yuan, C Zhao, T Feng, M Y Wang, Q Chen, arXiv:2503.03464Generative artificial intelligence in robotic manipulation: a survey. 2025</p>
<p>Aligning cyber space with physical world: a comprehensive survey on embodied ai. Y Liu, W Chen, Y Bai, X Liang, G Li, W Gao, L Lin, arXiv:2407.068862024</p>
<p>A survey of embodied ai in healthcare: techniques, applications, and opportunities. Y Liu, X Cao, T Chen, Y Jiang, J You, M Wu, X Wang, M Feng, Y Jin, J Chen, INFORM FUSION. 1191030332025</p>
<p>Machine learning meets advanced robotic manipulation. S Nahavandi, R Alizadehsani, D Nahavandi, C P Lim, K Kelly, F Bello, INFORM FUSION. 1051022212024</p>
<p>Visionlanguage-action models: concepts, progress, applications and challenges. R Sapkota, Y Cao, K I Roumeliotis, M Karkee, arXiv:2505.047692025</p>
<p>Trends and challenges in robot manipulation. A Billard, D Kragic, Science. 36484142019</p>
<p>Any-point trajectory modeling for policy learning. C Wen, X Lin, J So, K Chen, Q Dou, Y Gao, P Abbeel, RSS. 922024</p>
<p>Instruction-driven history-aware policies for robotic manipulations. P.-L Guhur, S Chen, R G Pinel, M Tapaswi, I Laptev, C Schmid, CoRL. 2023</p>
<p>Flow as the cross-domain manipulation interface. M Xu, Z Xu, Y Xu, C Chi, G Wetzstein, M Veloso, S Song, CoRL2024</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. J Gu, S Kirmani, P Wohlhart, Y Lu, M G Arenas, K Rao, W Yu, C Fu, K Gopalakrishnan, Z Xu, ICLR2024</p>
<p>Polytouch: A robust multi-modal tactile sensor for contact-rich manipulation using tactile-diffusion policies. J Zhao, N Kuppuswamy, S Feng, B Burchfiel, E Adelson, ICRA2025</p>
<p>No plan but everything under control: Robustly solving sequential tasks with dynamically composed gradient descent. V Mengers, O Brock, ICRA2025</p>
<p>Star: Learning diverse robot skill abstractions through rotationaugmented vector quantization. H Li, Q Lv, R Shao, X Deng, Y Li, J Hao, L Nie, ICML. 2025</p>
<p>Lion: empowering multimodal large language model with dual-level visual knowledge. G Chen, L Shen, R Shao, X Deng, L Nie, CVPR. 2024</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, CVPR. 2024</p>
<p>Instructblip: towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A Tiong, J Zhao, W Wang, S Hoi, NeurIPS. 2023</p>
<p>Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond. J Bai, S Bai, S Yang, S Wang, S Tan, P Wang, J Lin, C Zhou, J Zhou, arXiv:2308.129662023</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, NeurIPS2023</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Z Chen, W Wang, Y Cao, Y Liu, Z Gao, E Cui, J Zhu, S Ye, H Tian, Z Liu, arXiv:2412.052712024</p>
<p>Monkey: image resolution and text label are important things for large multi-modal models. Z Li, B Yang, Q Liu, Z Ma, S Zhang, J Yang, Y Sun, Y Liu, X Bai, CVPR. 2024</p>
<p>Falcon: Resolving visual redundancy and fragmentation in high-resolution multimodal large language models via visual registers. R Zhang, R Shao, G Chen, M Zhang, K Zhou, W Guan, L Nie, ICCV. 2025</p>
<p>Mome: Mixture of multimodal experts for generalist multimodal large language models. L Shen, G Chen, R Shao, W Guan, L Nie, NeurIPS2024</p>
<p>Openvla: an open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, CoRL2024</p>
<p>Rt-2: vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, K Han, CoRL2023</p>
<p>Rt-h: action hierarchies using language. S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, RSS2024</p>
<p>π 0 : A visionlanguage-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, RSS. 2025</p>
<p>a vision language-action model with open-world generalization. P Intelligence, N B Black, K D James Darpinian, A E Danny Driess, C F Michael Equi, E A Niccolo Fusai, arXiv:2504.160542025π 0.5 :</p>
<p>M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, M Aractingi, C Pascal, M Russi, A Marafioti, arXiv:2506.01844Smolvla: a vision-language-action model for affordable and efficient robotics. 2025</p>
<p>J Bjorck, F Casta Ñeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: an open foundation model for generalist humanoid robots. 2025</p>
<p>Cot-vla: visual chain-of-thought reasoning for visionlanguage-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, A Handa, T.-Y Lin, G Wetzstein, M.-Y Liu, D Xiang, CVPR. 2025</p>
<p>Hybridvla: collaborative diffusion and autoregression in a unified vision-language-action model. J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo, S Chen, M Liu, arXiv:2503.106312025</p>
<p>Bridgevla: input-output alignment for efficient 3d manipulation learning with vision-language models. P Li, Y Chen, H Wu, X Ma, X Wu, Y Huang, L Wang, T Kong, T Tan, arXiv:2506.079612025</p>
<p>Deer-vla: dynamic inference of multimodal large language models for efficient robot execution. Y Yue, Y Wang, B Kang, Y Han, S Wang, S Song, J Feng, G Huang, NeurIPS. 2024</p>
<p>Knowledge insulating vision-language-action models: train fast, run fast, generalize better. J T S Danny Driess, L Y Brian Ichter, K P , Adrian Li-Bell, H W Allen, Z Ren, L X S Quan, S Vuong, Levine, arXiv:2505.237052025</p>
<p>J Cen, C Yu, H Yuan, Y Jiang, S Huang, J Guo, X Li, Y Song, H Luo, F Wang, arXiv:2506.21539Worldvla: towards autoregressive action world model. 2025</p>
<p>J Zhang, Y Luo, A Anwar, S A Sontakke, J J Lim, J Thomason, E Biyik, J Zhang, arXiv:2505.10911Rewind: language-guided rewards teach robot policies without new demonstrations. 2025</p>
<p>Fast-in-slow: a dual-system foundation model unifying fast manipulation within slow reasoning. H Chen, J Liu, C Gu, Z Liu, R Zhang, X Li, X He, Y Guo, C.-W Fu, S Zhang, arXiv:2506.019532025</p>
<p>Real-time execution of action chunking flow policies. K Black, M Y Galliker, S Levine, arXiv:2506.073392025</p>
<p>Mitigating the human-robot domain discrepancy in visual pre-training for robotic manipulation. J Zhou, T Ma, K Y Lin, Z Wang, R Qiu, J Liang, CVPR. 2025</p>
<p>World4omni: a zero-shot framework from image generation world model to robotic manipulation. H Chen, B Wang, J Guo, T Zhang, Y Hou, X Huang, C Tie, L Shao, arXiv:2506.239192025</p>
<p>Fine-tuning vision-languageaction models: optimizing speed and success. M J Kim, C Finn, P Liang, RSS2025</p>
<p>Nora: a small open-sourced generalist vision language action model for embodied tasks. C.-Y Hung, Q Sun, P Hong, A Zadeh, C Li, U Tan, N Majumder, S Poria, arXiv:2504.198542025</p>
<p>Maniplvm-r1: reinforcement learning for reasoning in embodied manipulation with large vision-language models. Z Song, G Ouyang, M Li, Y Ji, C Wang, Z Xu, Z Zhang, X Zhang, Q Jiang, Z Chen, arXiv:2505.165172025</p>
<p>Embodied-reasoner: synergizing visual search, reasoning, and action for embodied interactive tasks. W Zhang, M Wang, G Liu, X Huixin, Y Jiang, Y Shen, G Hou, Z Zheng, H Zhang, X Li, arXiv:2503.216962025</p>
<p>Hamster: hierarchical action models for open-world robot manipulation. Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, ICLR. 2025</p>
<p>A0: an affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, arXiv:2504.126362025</p>
<p>Rekep: spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, CoRL2024</p>
<p>A survey of vision-language pre-trained models. Y Du, Z Liu, J Li, W X Zhao, IJCAI. 2022</p>
<p>Vision-language models for vision tasks: A survey. J Zhang, J Huang, S Jin, S Lu, TPAMI. 462024</p>
<p>Multimodal large language models: A survey. J Wu, W Gan, Z Chen, S Wan, P S Yu, BigData2023</p>
<p>A survey of state of the art large vision language models: Alignment, benchmark, evaluations and challenges. Z Li, X Wu, H Du, F Liu, H Nghiem, G Shi, arXiv:2501.021892025</p>
<p>Large vision-language model alignment and misalignment: A survey through the lens of explainability. D Shu, H Zhao, J Hu, W Liu, A Payani, L Cheng, M Du, arXiv:2501.013462025</p>
<p>A survey on diffusion policy for robotic manipulation: Taxonomy, analysis, and future directions. M Song, X Deng, Z Zhou, J Wei, W Guan, L Nie, Authorea Preprints. 2025</p>
<p>Diffusion models for robotic manipulation: A survey. R Wolf, Y Shi, S Liu, R Rayyes, arXiv:2504.084382025</p>
<p>Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. C Cui, P Ding, W Song, S Bai, X Tong, Z Ge, R Suo, W Zhou, Y Liu, B Jia, arXiv:2505.039122025</p>
<p>A survey of embodied learning for object-centric robotic manipulation. Y Zheng, L Yao, Y Su, Y Zhang, Y Wang, S Zhao, Y Zhang, L.-P Chau, 2025MIR22</p>
<p>Lion-fs: Fast &amp; slow video-language thinker as online video assistant. W Li, B Hu, R Shao, L Shen, L Nie, CVPR. 2025</p>
<p>Optimus-1: Hybrid multimodal memory empowered agents excel in longhorizon tasks. Z Li, Y Xie, R Shao, G Chen, D Jiang, L Nie, NeurIPS2024</p>
<p>Optimus-2: Multimodal minecraft agent with goalobservation-action conditioned policy. CVPR. 2025</p>
<p>Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios. Q Ye, Z Yu, R Shao, X Xie, P Torr, X Cao, ECCV. 2024</p>
<p>Cat+: investigating and enhancing audio-visual understanding in large language models. Q Ye, Z Yu, R Shao, Y Cui, X Kang, X Liu, P Torr, X Cao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2025</p>
<p>Drivevlm: the convergence of autonomous driving and large vision-language models. X Tian, J Gu, B Li, Y Liu, Y Wang, Z Zhao, K Zhan, P Jia, X Lang, H Zhao, CoRL2025</p>
<p>Cogagent: a visual language model for gui agents. W Hong, W Wang, Q Lv, J Xu, W Yu, J Ji, Y Wang, Z Wang, Y Dong, M Ding, CVPR. 2024</p>
<p>Less is more: Empowering gui agent with context-aware simplification. G Chen, X Zhou, R Shao, Y Lyu, K Zhou, S Wang, W Li, Y Li, Z Qi, L Nie, ICCV. 2025</p>
<p>Puma: Layer-pruned language model for efficient unified multimodal retrieval with modality-adaptive learning. Y Lyu, R Shao, G Chen, Y Zhu, W Guan, L Nie, ACM MM. 2025</p>
<p>Emosym: A symbiotic framework for unified emotional understanding and generation via latent reasoning. Y Zhu, Y Lyu, Z Yu, R Shao, K Zhou, L Nie, ACM MM. 2025</p>
<p>Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for gui agent. B Xie, R Shao, G Chen, K Zhou, Y Li, J Liu, M Zhang, L Nie, ACL. 2025</p>
<p>Robust sequential deepfake detection. R Shao, T Wu, Z Liu, International Journal of Computer Vision. 1332025</p>
<p>Detecting and grounding multi-modal media manipulation and beyond. R Shao, T Wu, J Wu, L Nie, Z Liu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 462024</p>
<p>Detecting and grounding multimodal media manipulation. R Shao, T Wu, Z Liu, CVPR. 2023</p>
<p>Multi-adversarial discriminative deep domain generalization for face presentation attack detection. R Shao, X Lan, J Li, P C Yuen, CVPR. 2019</p>
<p>Deepfake-adapter: Dual-level adapter for deepfake detection. R Shao, T Wu, L Nie, Z Liu, International Journal of Computer Vision. 1332025</p>
<p>Spa-bench: A comprehensive benchmark for smartphone agent evaluation. J Chen, D Yuen, B Xie, Y Yang, G Chen, Z Wu, L Yixing, X Zhou, W Liu, S Wang, K Zhou, R Shao, L Nie, Y Wang, J Hao, J Wang, K Shao, ICLR. 2025</p>
<p>Llava-onevision: easy visual task transfer. B Li, Y Zhang, D Guo, R Zhang, F Li, H Zhang, K Zhang, P Zhang, Y Li, Z Liu, C Li, 2025TMLR</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, arXiv:2502.13923Qwen2.5-vl technical report. 2025</p>
<p>W Huang, B Jia, Z Zhai, S Cao, Z Ye, F Zhao, Z Xu, Y Hu, S Lin, arXiv:2503.06749Vision-r1: incentivizing reasoning capability in multimodal large language models. 2025</p>
<p>Cliport: what and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, CoRL2022</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, I Sutskever, ICML. 2021</p>
<p>Transporter networks: rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, CoRL2021</p>
<p>Rt-1: robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, RSS. 2023</p>
<p>Efficientnet: rethinking model scaling for convolutional neural networks. M Tan, Q Le, ICML. 2019</p>
<p>Palm-e: an embodied multimodal language model. D Driess, F Xia, M S M Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, Y Chebotar, P Sermanet, D Duckworth, S Levine, V Vanhoucke, K Hausman, M Toussaint, K Greff, A Zeng, I Mordatch, P Florence, ICML. 2023</p>
<p>Pali-x: on scaling up a multilingual vision and language model. X Chen, J Djolonga, P Padlewski, B Mustafa, S Changpinyo, J Wu, C R Ruiz, S Goodman, X Wang, Y Tay, CVPR. 2024</p>
<p>Lora: low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 2022</p>
<p>Large language models for robotics: opportunities, challenges, and perspectives. J Wang, E Shi, H Hu, C Ma, Y Liu, X Wang, Y Yao, X Liu, B Ge, S Zhang, JAI. 42025</p>
<p>A review of embodied grasping. J Sun, P Mao, L Kong, J Wang, Sensors. 258522025</p>
<p>Open x-embodiment: robotic learning datasets and rt-x models: open x-embodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, in ICRA, 2024</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, NeurIPS2017</p>
<p>Sigmoid loss for language image pre-training. X Zhai, B Mustafa, A Kolesnikov, L Beyer, ICCV. 2023</p>
<p>. M Oquab, T Darcet, T Moutakanni, H V Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, M Assran, N Ballas, W Galuba, R Howes, P.-Y Huang, S.-W Li, I Misra, M Rabbat, V Sharma, G Synnaeve, H Xu, H Jegou, J Mairal, P Labatut, A Joulin, P Bojanowski, 2024Dinov2: learning robust visual features without supervision," TMLR</p>
<p>An embodied generalist agent in 3d world. J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S.-C Zhu, B Jia, S Huang, ICML. 2024</p>
<p>Training strategies for efficient embodied reasoning. W Chen, S Belkhale, S Mirchandani, O Mees, D Driess, K Pertsch, S Levine, arXiv:2505.082432025</p>
<p>Revla: reverting visual domain limitation of robotic foundation models. S Dey, J.-N Zaech, N Nikolov, L Van Gool, D P Paudel, ICRA2025</p>
<p>Tracevla: visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. R Zheng, Y Liang, S Huang, J Gao, H Daumé, Iii , A Kolobov, F Huang, J Yang, ICLR. 2025</p>
<p>Beyond sight: finetuning generalist robot policies with heterogeneous sensors via language grounding. J Jones, O Mees, C Sferrazza, K Stachowicz, P Abbeel, S Levine, ICRA2025</p>
<p>Universal actions for enhanced embodied foundation models. J Zheng, J Li, D Liu, Y Zheng, Z Wang, Z Ou, Y Liu, J Liu, Y.-Q Zhang, X Zhan, CVPR. 2025</p>
<p>Spatialvla: exploring spatial representations for visual-language-action model. D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, RSS. 2025</p>
<p>Upvla: a unified understanding and prediction model for embodied agent. J Zhang, Y Guo, Y Hu, X Chen, X Zhu, J Chen, ICML. 2025</p>
<p>Vlas: vision-language-action model with speech instructions for customized robot manipulation. W Zhao, P Ding, Z Min, Z Gong, S Bai, H Zhao, D Wang, ICLR2025</p>
<p>Vtla: vision-tactile-language-action model with preference learning for insertion manipulation. C Zhang, P Hao, X Cao, X Hao, S Cui, S Wang, arXiv:2505.095772025</p>
<p>Unveiling the potential of vision-language-action models with open-ended multimodal instructions. W Zhao, G Li, Z Gong, P Ding, H Zhao, D Wang, arXiv:2505.112142025</p>
<p>T Van Vo, T Q Nguyen, K M Nguyen, D H M Nguyen, M N Vu, arXiv:2505.19080Refinevla: reasoning-aware teacher-guided transfer fine-tuning. 2025</p>
<p>Lohovla: a unified vision-language-action model for long-horizon embodied tasks. Y Yang, J Sun, S Kou, Y Wang, Z Deng, arXiv:2506.004112025</p>
<p>Unified vision-language-action model. Y Wang, X Li, W Wang, J Zhang, Y Li, Y Chen, X Wang, Z Zhang, arXiv:2506.198502025</p>
<p>4d-vla: spatiotemporal vision-language-action pretraining with cross-scene calibration. J Zhang, Y Chen, Y Xu, Z Huang, Y Zhou, Y.-J Yuan, X Cai, G Huang, X Quan, H Xu, L Zhang, arXiv:2506.222422025</p>
<p>Vote: vision-language-action optimization with trajectory ensemble voting. J Lin, A Taherin, A Akbari, A Akbari, L Lu, G Chen, T Padir, X Yang, W Chen, Y Li, arXiv:2507.051162025</p>
<p>Spatial traces: Enhancing vla models with spatial-temporal understanding. M A Patratskiy, A K Kovalev, A I Panov, arXiv:2508.090322025</p>
<p>Vision-language foundation models as effective robot imitators. X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, H Liu, H Li, T Kong, ICLR2024</p>
<p>Robomamba: efficient visionlanguage-action model for robotic reasoning and manipulation. J Liu, M Liu, Z Wang, P An, X Li, K Zhou, S Yang, R Zhang, Y Guo, S Zhang, NeurIPS. 372024</p>
<p>Accelerating vision-language-action model integrated with action chunking via parallel decoding. W Song, J Chen, P Ding, H Zhao, W Zhao, Z Zhong, Z Ge, J Ma, H Li, arXiv:2503.023102025</p>
<p>Mole-vla: dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation. R Zhang, M Dong, Y Zhang, L Heng, X Chi, G Dai, L Du, Y Du, S Zhang, arXiv:2503.203842025</p>
<p>Think twice, act once: token-aware compression and action reuse for efficient inference in vision-language-action models. X Tan, Y Yang, P Ye, J Zheng, B Bai, X Wang, J Hao, T Chen, arXiv:2505.212002025</p>
<p>Bitvla: 1-bit vision-language-action models for robotics manipulation. H Wang, C Xiong, R Wang, X Chen, arXiv:2506.075302025</p>
<p>Spec-vla: speculative decoding for vision-language-action models with relaxed acceptance. S Wang, R Yu, Z Yuan, C Yu, F Gao, Y Wang, D F Wong, arXiv:2507.224242025</p>
<p>Cogvla: Cognitionaligned vision-language-action model via instruction-driven routing &amp; sparsification. W Li, R Zhang, R Shao, J He, L Nie, arXiv:2508.210462025</p>
<p>Pointnet++: deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, NeurIPS. 302017</p>
<p>Language conditioned spatial relation reasoning for 3d object grounding. S Chen, P.-L Guhur, M Tapaswi, C Schmid, I Laptev, NeurIPS. 352022</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, CoRL. 2025</p>
<p>Mamba: linear-time sequence modeling with selective state spaces. A Gu, T Dao, arXiv:2312.007522023</p>
<p>Bitnet b1. 58 reloaded: stateof-the-art performance also on smaller networks. J Nielsen, P Schneider-Kamp, 2024DeLTA</p>
<p>Fast: efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, RSS. 2025</p>
<p>A dual process vla: efficient robotic manipulation leveraging vlm. B Han, J Kim, J Jang, arXiv:2410.155492024</p>
<p>Towards synergistic, generalized, and efficient dual-system for robotic manipulation. Q Bu, H Li, L Chen, J Cai, J Zeng, H Cui, M Yao, Y Qiao, arXiv:2410.080012024</p>
<p>From llms to actions: latent codes as bridges in hierarchical robot control. Y Shentu, P Wu, A Rajeswaran, P Abbeel, IROS. 2024</p>
<p>Cogact: a foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024</p>
<p>Hirt: enhancing robotic control with hierarchical robot transformers. J Zhang, Y Guo, X Chen, Y.-J Wang, Y Hu, C Shi, J Chen, CoRL. 2025</p>
<p>Chatvla: unified multimodal understanding and robot control with vision-language-action model. Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, arXiv:2502.144202025</p>
<p>Chatvla-2: visionlanguage-action model with open-world embodied reasoning from pretrained knowledge. Z Zhou, Y Zhu, J Wen, C Shen, Y Xu, arXiv:2505.219062025</p>
<p>Diffusionvla: scaling robot foundation models via unified diffusion and autoregression. J Wen, Y Zhu, M Zhu, Z Tang, J Li, Z Zhou, X Liu, C Shen, Y Peng, F Feng, ICML. 2025</p>
<p>Trivla: a unified triplesystem-based unified vision-language-action model for general robot control. Z Liu, Y Gu, S Zheng, X Xue, Y Fu, arXiv:2507.014242025</p>
<p>Information-theoretic graph fusion with visionlanguage-action model for policy reasoning and dual robotic control. S Li, L Gao, J Wang, C Che, X Xiao, J Cao, Y Hu, H R Karimi, arXiv:2508.053422025</p>
<p>Rationalvla: a rational vision-languageaction model with dual system. W Song, J Chen, W Li, X He, H Zhao, C Cui, P D S Su, F Tang, X Cheng, D Wang, arXiv:2506.108262025</p>
<p>Vq-vla: improving vision-language-action models via scaling vector-quantized action tokenizers. J Jones, O Mees, C Sferrazza, K Stachowicz, P Abbeel, S Levine, ICCV. 2025</p>
<p>Tinyvla: towards fast, data-efficient vision-language-action models for robotic manipulation. J Wen, Y Zhu, J Li, M Zhu, Z Tang, K Wu, Z Xu, N Liu, R Cheng, C Shen, RA-L, 2025</p>
<p>Forcevla: enhancing vla models with a forceaware moe for contact-rich manipulation. J Yu, H Liu, Q Yu, J Ren, C Hao, H Ding, G Huang, G Huang, Y Song, P Cai, arXiv:2505.221592025</p>
<p>Onetwovla: a unified vision-language-action model with adaptive reasoning. F Lin, R Nai, Y Hu, J You, J Zhao, Y Gao, arXiv:2505.119172025</p>
<p>Tactile-vla: unlocking vision-language-action model's physical knowledge for tactile generalization. J Huang, S Wang, F Lin, Y Hu, C Wen, Y Gao, arXiv:2507.091602025</p>
<p>C Cheang, S Chen, Z Cui, Y Hu, L Huang, T Kong, H Li, Y Li, Y Liu, X Ma, arXiv:2507.15493Gr-3 technical report. 2025</p>
<p>Villa-x: enhancing latent action modeling in vision-language-action models. X Chen, H Wei, P Zhang, C Zhang, K Wang, Y Guo, R Yang, Y Wang, X Xiao, L Zhao, arXiv:2507.236822025</p>
<p>Graspvla: a grasping foundation model pre-trained on billionscale synthetic action data. S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang, T Yang, X Zhang, W Zhang, H Cui, Z Zhang, H Wang, arXiv:2505.032332025</p>
<p>Scalable diffusion models with transformers. W Peebles, S Xie, ICCV. 2023</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. A Mandlekar, D Xu, J Wong, S Nasiriany, C Wang, R Kulkarni, L Fei-Fei, S Savarese, Y Zhu, R Martín-Martín, CoRL2022</p>
<p>Neural discrete representation learning. A Van Den Oord, O Vinyals, K Kavukcuoglu, NeurIPS. 2017</p>
<p>Outrageously large neural networks: the sparselygated mixture-of-experts layer. A M Noam Shazeer, A D Krzysztof Maziarz, G H Quoc Le, J Dean, ICLR2017</p>
<p>Momanipvla: transferring vision-language-action models for general mobile manipulation. Z Wu, Y Zhou, X Xu, Z Wang, H Yan, CVPR. 2025</p>
<p>Robopoint: a vision-language model for spatial affordance prediction in robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, CoRL2024</p>
<p>Reinforced reasoning for embodied planning. D Wu, J Fan, J Zang, G Wang, W Yin, W Li, B Jin, arXiv:2505.220502025</p>
<p>Chain-of-modality: learning manipulation programs from multimodal human videos with vision-languagemodels. C Wang, F Xia, W Yu, T Zhang, R Zhang, C K Liu, L Fei-Fei, J Tan, J Liang, ICRA. 2025</p>
<p>Robotic visual instruction. Y Li, Z Gong, H Li, X Huang, H Kang, G Bai, X Ma, CVPR. 2025</p>
<p>Long-horizon embodied planning with implicit logical inference and hallucination mitigation. S Liu, J Du, S Xiang, Z Wang, D Luo, arXiv:2409.156582025</p>
<p>Look before you leap: unveiling the power of gpt-4v in robotic vision-language planning. Y Hu, F Lin, T Zhang, L Yi, Y Gao, arXiv:2311.178422023</p>
<p>Robobrain: a unified brain model for robotic manipulation from abstract to concrete. Y Ji, H Tan, J Shi, X Hao, Y Zhang, H Zhang, P Wang, M Zhao, Y Mu, P An, CVPR. 2025</p>
<p>Hi robot: open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, ICML. 2025</p>
<p>Agentic robot: a brain-inspired framework for vision-language-action models in embodied agents. Z Yang, Y Chen, X Zhou, J Yan, D Song, Y Liu, Y Li, Y Zhang, P Zhou, H Chen, arXiv:2505.234502025</p>
<p>Dexvla: vision-language model with plug-in diffusion expert for general robot control. J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng, CoRL2025</p>
<p>Instruct2act: mapping multi-modality instructions to robotic actions with large language model. S Huang, Z Jiang, H Dong, Y Qiao, P Gao, H Li, arXiv:2305.111762023</p>
<p>Robomatrix: a skill-centric hierarchical framework for scalable robot task planning and execution in open-world. W Mao, W Zhong, Z Jiang, D Fang, Z Zhang, Z Lan, H Li, F Jia, T Wang, H Fan, arXiv:2412.001712024</p>
<p>Pointvla: injecting the 3d world into vision-language-action models. C Li, J Wen, Y Peng, Y Peng, F Feng, Y Zhu, arXiv:2503.075112025</p>
<p>From seeing to doing: bridging reasoning and decision for robotic manipulation. Y Yuan, H Cui, Y Chen, Z Dong, F Ni, L Kou, J Liu, P Li, Y Zheng, J Hao, arXiv:2505.085482025</p>
<p>Robridge: a hierarchical architecture bridging cognition and execution for general robotic manipulation. K Zhang, R Xu, P Ren, J Lin, H Wu, L Lin, X Liang, ICCV. 2025</p>
<p>Robocerebra: a large-scale benchmark for long-horizon robotic manipulation evaluation. S Han, B Qiu, Y Liao, S Huang, C Gao, S Yan, S Liu, arXiv:2506.066772025</p>
<p>Dexgraspvla: a vision-language-action framework towards general dexterous grasping. Y Zhong, X Huang, R Li, C Zhang, Y Liang, Y Yang, Y Chen, arXiv:2502.209002025</p>
<p>Voxposer: composable 3d value maps for robotic manipulation with language models. W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, CoRL2023</p>
<p>Skilldiffuser: interpretable hierarchical planning via skill abstractions in diffusion-based task execution. Z Liang, Y Mu, H Ma, M Tomizuka, M Ding, P Luo, CVPR. 2024</p>
<p>RT-affordance: reasoning about robotic manipulation with affordances. S Nasiriany, S Kirmani, T Ding, L Smith, Y Zhu, D Driess, D Sadigh, T Xiao, CoRL2024</p>
<p>Hibernac: hierarchical brain-emulated robotic neural agent collective for disentangling complex manipulation. H Wu, H Zhang, P Zhang, J Wang, C Wang, arXiv:2506.082962025</p>
<p>Llarva: vision-action instruction tuning enhances robot learning. D Niu, Y Sharma, G Biamby, J Quenum, Y Bai, B Shi, T Darrell, R Herzig, CoRL2024</p>
<p>Malmm: multi-agent large language models for zero-shot robotics manipulation. H Singh, R J Das, M Han, P Nakov, I Laptev, CVPR. 2024</p>
<p>Vla-touch: Enhancing vision-language-action models with dual-level tactile feedback. J Bi, K Y Ma, C Hao, M Z Shou, H Soh, arXiv:2507.172942025</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: incentivizing reasoning capability in llms via reinforcement learning. 2025</p>
<p>Embodied-r: collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning. B Zhao, Z Wang, J Fang, C Gao, F Man, J Cui, X Wang, X Chen, Y Li, W Zhu, arXiv:2504.126802025</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, ICCV. 2023</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024</p>
<p>Grape: generalizing robot policy via preference alignment. Z Zhang, K Zheng, Z Chen, J Jang, Y Li, S Han, C Wang, M Ding, D Fox, H Yao, arXiv:2411.193092024</p>
<p>What can rl bring to vla generalization? an empirical study. J Liu, F Gao, B Wei, X Chen, Q Liao, Y Wu, C Yu, Y Wang, arXiv:2505.197892025</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, NeurIPS. 2022</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024</p>
<p>Vla-rl: towards masterful and general robotic manipulation with scalable reinforcement learning. G Lu, W Guo, C Zhang, Y Zhou, H Jiang, Z Gao, Y Tang, Z Wang, arXiv:2505.187192025</p>
<p>Tgrpo: fine-tuning vision-language-action model via trajectory-wise group relative policy optimization. Z Chen, R Niu, H Kong, Q Wang, arXiv:2506.084402025</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023</p>
<p>Offline reinforcement learning with implicit q-learning. I Kostrikov, A Nair, S Levine, ICLR2022</p>
<p>Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, ICML. 2018</p>
<p>Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning. J Luo, C Xu, J Wu, S Levine, arXiv:2410.218452024</p>
<p>Conrft: a reinforced fine-tuning method for vla models via consistency policy. Y Chen, S Tian, S Liu, Y Zhou, H Li, D Zhao, arXiv:2502.054502025</p>
<p>Rldg: robotic generalist policy distillation via reinforcement learning. C Xu, Q Li, J Luo, S Levine, RSS2025</p>
<p>Improving vision-language-action model with online reinforcement learning. Y Guo, J Zhang, X Chen, X Ji, Y.-J Wang, Y Hu, J Chen, ICRA2025</p>
<p>Efficientvla: training-free acceleration and compression for vision-language-action models. Y Yang, Y Wang, Z Wen, L Zhongwei, C Zou, Z Zhang, C Wen, L Zhang, arXiv:2506.101002025</p>
<p>Vla-cache: towards efficient vision-language-action model via adaptive token caching in robotic manipulation. S Xu, Y Wang, C Xia, D Zhu, T Huang, C Xu, arXiv:2502.021752025</p>
<p>Y Li, Y Meng, Z Sun, K Ji, C Tang, J Fan, X Ma, S Xia, Z Wang, W Zhu, arXiv:2506.12723Sp-vla: a joint model scheduling and token pruning approach for vla model acceleration. 2025</p>
<p>Block-wise adaptive caching for accelerating diffusion policy. K Ji, Y Meng, H Cui, Y Li, S Hua, L Chen, Z Wang, arXiv:2506.134562025</p>
<p>Univla: learning to act anywhere with task-centric latent actions. Q Bu, Y Yang, J Cai, S Gao, G Ren, M Yao, P Luo, H Li, arXiv:2505.061112025</p>
<p>. S Ye, J Jang, B Jeon, S J Joo, J Yang, B Peng, A Mandlekar, R Tan, Y.-W Chao, B Y Lin, L Liden, K Lee, J Gao, L Zettlemoyer, D Fox, M Seo, 2025Latent action pretraining from videos," in ICLR</p>
<p>Learning an actionable discrete diffusion policy via large-scale actionless video pre-training. H He, C Bai, L Pan, W Zhang, B Zhao, X Li, NeurIPS. 2024</p>
<p>3d-vla: a 3d vision-language-action generative world model. H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, ICML. 2024</p>
<p>Humanoid-vla: towards universal humanoid control with visual integration. P Ding, J Ma, X Tong, B Zou, X Luo, Y Fan, T Wang, H Lu, P Mo, J Liu, arXiv:2502.147952025</p>
<p>Robotic manipulation by imitating generated videos without physical demonstrations. S Patel, S Mohan, H Mai, U Jain, S Lazebnik, Y Li, arXiv:2507.009902025</p>
<p>Foundationpose: unified 6d pose estimation and tracking of novel objects. B Wen, W Yang, J Kautz, S Birchfield, CVPR. 2024</p>
<p>V-jepa 2: selfsupervised video models enable understanding, prediction and planning. M Assran, A Bardes, D Fan, Q Garrido, R Howes, M Muckley, A Rizvi, C Roberts, K Sinha, A Zholus, arXiv:2506.099852025</p>
<p>Interactive posttraining for vision-language-action models. S Tan, K Dou, Y Zhao, P Kraehenbuehl, arXiv:2505.170162025</p>
<p>Omnijarvis: unified vision-language-action tokenization enables open-world instruction following agents. Z Wang, S Cai, Z Mu, H Lin, C Zhang, X Liu, Q Li, A Liu, X Ma, Y Liang, NeurIPS2024</p>
<p>K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, M Y Galliker, arXiv:2504.16054π0. 5: a vision-language-action model with open-world generalization. 2025</p>
<p>Bc-z: zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, CoRL2022</p>
<p>Bridge data: boosting generalization of robotic skills with cross-domain datasets. F Ebert, Y Yang, K Schmeckpeper, B Bucher, G Georgakis, K Daniilidis, C Finn, S Levine, RSS. 2022</p>
<p>Rh20t: a comprehensive robotic dataset for learning diverse skills in one-shot. H.-S Fang, H Fang, Z Tang, J Liu, C Wang, J Wang, H Zhu, C Lu, ICRA. 2024</p>
<p>Droid: a large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, RSS. 2024</p>
<p>Behavior-1k: a human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. C Li, R Zhang, J Wong, C Gokmen, S Srivastava, R Martín-Martín, C Wang, G Levine, W Ai, B Martinez, CoRL2022</p>
<p>Alfred: a benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, CVPR. 2020</p>
<p>Rlbench: the robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, RA-L. 52020</p>
<p>Peract2: benchmarking and learning for robotic bimanual manipulation tasks. M Grotz, M Shridhar, T Asfour, D Fox, arXiv:2407.002782024</p>
<p>Meta-world: a benchmark and evaluation for multitask and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, CoRL. 2020</p>
<p>Relay policy learning: solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, CoRL2019</p>
<p>Libero: benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, NeurIPS. 362023</p>
<p>Calvin: a benchmark for language-conditioned policy learning for longhorizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, RA-L. 72022</p>
<p>Memory, benchmark &amp; robots: a benchmark for solving complex tasks with reinforcement learning. E Cherepanov, N Kachaev, A K Kovalev, A I Panov, arXiv:2502.105502025</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, K Hsu, J Gu, K Pertsch, O Mees, H R Walke, C Fu, I Lunawat, I Sieh, S Kirmani, S Levine, J Wu, C Finn, H Su, Q Vuong, T Xiao, CoRL2024</p>
<p>Habitat: a platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, ICCV. 2019</p>
<p>Habitat 2.0: training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D S Chaplot, O Maksymets, NeurIPS. 342021</p>
<p>Habitat 3.0: a co-habitat for humans, avatars and robots. X Puig, E Undersander, A Szot, M D Cote, T.-Y Yang, R Partsey, R Desai, A W Clegg, M Hlavac, S Y Min, 2024ICLR</p>
<p>Sapien: a simulated part-based interactive environment. F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, CVPR. 2020</p>
<p>The colosseum: a benchmark for evaluating generalization for robotic manipulation. W Pumacay, I Singh, J Duan, R Krishna, J Thomason, D Fox, RSS2024</p>
<p>Vlabench: a large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks. S Zhang, Z Xu, P Liu, X Yu, Y Li, Q Gao, Z Fei, Z Yin, Z Wu, Y.-G Jiang, ICCV. 2025</p>
<p>Ego4d: around the world in 3,000 hours of egocentric video. K Grauman, A Westbury, E Byrne, Z Chavis, A Furnari, R Girdhar, J Hamburger, H Jiang, M Liu, X Liu, M Martin, T Nagarajan, CVPR. 2022</p>
<p>Ego-exo4d: understanding skilled human activity from first-and third-person perspectives. K Grauman, A Westbury, L Torresani, K Kitani, J Malik, T Afouras, K Ashutosh, V Baiyya, S Bansal, B Boote, CVPR. 2024</p>
<p>Egoplan-bench: benchmarking egocentric embodied planning with multimodal large language models. Y Chen, Y Ge, Y Ge, M Ding, B Li, R Wang, R Xu, Y Shan, X Liu, arxiv:2312.067222024</p>
<p>Egovid-5m: a large-scale video-action dataset for egocentric video generation. X Wang, K Zhao, F Liu, J Wang, G Zhao, X Bao, Z Zhu, Y Zhang, X Wang, arXiv:2411.083802024</p>
<p>Scaling egocentric vision: the epic-kitchens dataset. D Damen, H Doughty, G M Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray, ECCV. 2018</p>
<p>Com kitchens: an unedited overhead-view video dataset as a vision-language benchmark. K Maeda, T Hirasawa, A Hashimoto, J Harashima, L Rybicki, Y Fukasawa, Y Ushiku, ECCV. 2024</p>
<p>Egovqa-an egocentric video question answering benchmark dataset. C Fan, ICCV. 2019</p>
<p>Egotaskqa: understanding human tasks in egocentric videos. B Jia, T Lei, S.-C Zhu, S Huang, NeurIPS. 352022</p>
<p>Egodex: learning dexterous manipulation from large-scale egocentric video. R Hoque, P Huang, D J Yoon, M Sivapurapu, J Zhang, arXiv:2505.117092025</p>
<p>Dexcap: scalable and portable mocap data collection system for dexterous manipulation. C Wang, H Shi, W Wang, R Zhang, L Fei-Fei, C K Liu, RSS2024</p>
<p>Egomimic: scaling imitation learning via egocentric video. S Kareer, D Patel, R Punamiya, P Mathur, S Cheng, C Wang, J Hoffman, D Xu, arXiv:2410.242212024</p>
<p>R.-Z Qiu, S Yang, X Cheng, C Chawla, J Li, T He, G Yan, D J Yoon, R Hoque, L Paulsen, arXiv:2503.13441Humanoid policy˜human policy. 2025</p>
<p>Real2render2real: scaling robot data without dynamics simulation or robot hardware. J Yu, L Fu, H Huang, K El-Refai, R A Ambrus, R Cheng, M Z Irshad, K Goldberg, arXiv:2505.096012025</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, CVPR. 2018</p>
<p>Iqa: visual question answering in interactive environments. D Gordon, A Kembhavi, M Rastegari, J Redmon, D Fox, A Farhadi, CVPR. 2018</p>
<p>Multi-target embodied question answering. L Yu, X Chen, G Gkioxari, M Bansal, T L Berg, D Batra, CVPR. 2019</p>
<p>Embodied question answering in photorealistic environments with point cloud perception. E Wijmans, S Datta, O Maksymets, A Das, G Gkioxari, S Lee, I Essa, D Parikh, D Batra, CVPR. 2019</p>
<p>Eqa-mx: embodied question answering using multimodal expression. M M Islam, A Gladstone, R Islam, T Iqbal, ICLR2023</p>
<p>Openeqa: embodied question answering in the era of foundation models. A Majumdar, A Ajay, X Zhang, P Putta, S Yenamandra, M Henaff, S Silwal, P Mcvay, O Maksymets, S Arnaud, CVPR. 2024</p>
<p>Lota-bench: benchmarking language-oriented task planners for embodied agents. J.-W Choi, Y Yoon, H Ong, J Kim, M Jang, ICLR2024</p>            </div>
        </div>

    </div>
</body>
</html>
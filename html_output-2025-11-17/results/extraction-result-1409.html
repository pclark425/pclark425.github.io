<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1409 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1409</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1409</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-220a21fbc074ef9d24af4cdd51465f74809a4317</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/220a21fbc074ef9d24af4cdd51465f74809a4317" target="_blank">Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A novel reinforcement learning approach named Iso-Dream, which improves the Dream-to-Control framework in two aspects and is effective in decoupling the mixed dynamics and remarkably outperforms existing approaches in a wide range of visual control and prediction domains.</p>
                <p><strong>Paper Abstract:</strong> World models learn the consequences of actions in vision-based interactive systems. However, in practical scenarios such as autonomous driving, there commonly exists noncontrollable dynamics independent of the action signals, making it difficult to learn effective world models. To tackle this problem, we present a novel reinforcement learning approach named Iso-Dream, which improves the Dream-to-Control framework in two aspects. First, by optimizing the inverse dynamics, we encourage the world model to learn controllable and noncontrollable sources of spatiotemporal changes on isolated state transition branches. Second, we optimize the behavior of the agent on the decoupled latent imaginations of the world model. Specifically, to estimate state values, we roll-out the noncontrollable states into the future and associate them with the current controllable state. In this way, the isolation of dynamics sources can greatly benefit long-horizon decision-making of the agent, such as a self-driving car that can avoid potential risks by anticipating the movement of other vehicles. Experiments show that Iso-Dream is effective in decoupling the mixed dynamics and remarkably outperforms existing approaches in a wide range of visual control and prediction domains.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1409.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1409.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iso-Dream</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL framework that learns a three-branch latent world model separating controllable states (s_t), noncontrollable states (z_t), and time-invariant background; uses inverse dynamics to isolate controllable components and an attention-based 'future-state' mechanism to incorporate imagined noncontrollable rollouts into policy/value learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Iso-Dream world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent recurrent state-space world model with three branches: (1) an action-conditioned branch (RSSM-like) that uses a deterministic GRU hidden state and stochastic controllable latent s_t; (2) an action-free branch (GRU or ST-LSTM) modeling noncontrollable stochastic latent z_t; (3) a static branch encoding time-invariant background. Priors p(tilde{s}_t|h_t) and p(tilde{z}_t|h'_t) and posteriors q(s_t|h_t,o_t), q(z_t|h'_t,o_t) are learned; a decoder reconstructs image components via masks. An Inverse Cell (MLP) is trained to regress actions from s_{t-1}, s_t to encourage controllability separation. Behavior learning imagines tilde{z}_{t+1:t+L+tau} first, computes a visionary state e_t via attention e_t = softmax(tilde{s}_t * tilde{z}_{t:t+tau}^T) tilde{z}_{t:t+tau} + tilde{s}_t, and then imagines actions and controllable-state rollouts to train policy/value with lambda-returns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent state-space model with disentangled branches)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vision-based model-based RL and action-conditioned video prediction: DeepMind Control Suite with video backgrounds, CARLA autonomous driving, BAIR robot pushing, RoboNet</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Training/objective metrics: image reconstruction / image log loss, reward log loss, discount log loss, KL divergences between posterior and prior for s and z; Evaluation metrics: task return (cumulative reward) for control tasks, PSNR and SSIM for video-prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>DeepMind Control Suite (video_easy): Walker Walk 911 ± 50, Cheetah Run 659 ± 62, Finger Spin 800 ± 59, Hopper Stand 746 ± 312 (final performance means reported in Table 1). Video prediction: BAIR PSNR 19.51, SSIM 0.768; RoboNet PSNR 21.71, SSIM 0.769 (Table 2). CARLA: qualitatively and quantitatively reported to outperform DreamerV2 by a large margin (numeric CARLA returns not provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Relatively interpretable for pixel-level dynamics: explicit disentanglement into controllable vs noncontrollable vs static components yields interpretable masks and component reconstructions; attention maps provide an interpretable mechanism for how predicted noncontrollable states influence decisions; inverse dynamics objective steers s_t to be action-explainable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of decoder masks and per-branch reconstructions (showing localized controllable/noncontrollable regions); attention-based future-state weighting (visualizable); ablations demonstrating role of Inverse Cell for disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No absolute GPU/time counts provided; authors state Iso-Dream requires longer training time per episode than DreamerV2 due to more intensive state transitions during behavior learning. Architectures and hidden sizes reported (e.g., GRU hidden size = 200 for DMC/CARLA, encoder conv layers). Sample budgets: DMC experiments up to 500k environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper reports Iso-Dream is more sample-efficient than existing MBRL baselines but has higher per-episode computational cost than DreamerV2 (longer training time per episode). Exact wall-clock or FLOPs comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperforms DreamerV2 and other baselines on DMC video_easy tasks (see fidelity_performance numbers) and outperforms competing video-prediction models on PSNR/SSIM; in CARLA, ablations show inverse dynamics and noncontrollable rollouts materially improve driving performance (figures reported qualitatively / as learning curves).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Decoupling improves robustness to nonstationary visual distractors (e.g., dynamic video backgrounds) by enabling controller to use only controllable representation when appropriate; rolling out noncontrollable states gives forward-looking information that materially improves long-horizon decisions like collision avoidance in driving. The model supports two modes: discard z during imagination if noncontrollable dynamics are irrelevant, or include z rollouts when they affect reward.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Main trade-off is compute/time vs performance: disentanglement and rollouts of noncontrollable states improve sample efficiency and task performance but increase per-episode training time and architectural complexity; also requires environment-specific architecture tuning (authors note different benchmarks need different network structures).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: three-branch modular architecture (s, z, static); RSSM-like action-conditioned branch for s with GRU; action-free branch for z (GRU or ST-LSTM depending on dataset); inverse dynamics MLP to isolate s; mask-based decoder to reconstruct per-branch visual components; optional reward model conditioned on s or (s,z); future-state attention to integrate s and imagined z_{t:t+tau}; flexibility to train action-free branch only with reconstruction if z is irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically to DreamerV2, CURL, SVEA, SAC, DBC for control and to SVG, SA-ConvLSTM, PhyDNet for video prediction. Iso-Dream outperforms DreamerV2 and listed baselines on DMC video_easy tasks and CARLA (large margin per figures) and achieves higher PSNR/SSIM than baselines on BAIR and RoboNet. Computationally, it is slower per-episode than DreamerV2 but more sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends (and demonstrates): include inverse dynamics objective to isolate controllable representations; roll out noncontrollable states and use future-state attention when noncontrollable dynamics affect decisions (e.g., autonomous driving); otherwise train action-free branch with reconstruction and omit z from imagination to save computation. Also recommends adapting network architecture (branch types, hidden sizes) to environment specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1409.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1409.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (Mastering Atari with discrete world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent imagination-based MBRL agent that uses an RSSM latent world model and optimizes actor and value networks on imagined trajectories (λ-returns).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 (RSSM-based latent world model and imagination-based actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent recurrent state-space model (RSSM) with deterministic hidden state and stochastic latent variables; learns priors and posteriors via ELBO-style objectives and trains actor and value networks on imagined latent rollouts using λ-returns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL from pixels (originally Atari; used as baseline for continuous control/video tasks here)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task return / cumulative reward in control tasks; model ELBO (reconstruction + KL) for training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in this paper as baselines on DMC video_easy tasks: Walker Walk 655 ± 47, Cheetah Run 475 ± 159, Finger Spin 755 ± 92, Hopper Stand 260 ± 366 (Table 1). CARLA performance depicted qualitatively/graphically — Iso-Dream reportedly outperforms DreamerV2 by a large margin.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations are not explicitly disentangled for controllability; treated as black-box learned latents without per-branch masks or explicit interpretability mechanisms in this discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper; DreamerV2 typically uses abstract latent states without explicit visualization affordances in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No explicit numbers in this paper; implied to be lower per-episode training time than Iso-Dream (authors state Iso-Dream requires longer per-episode training compared to DreamerV2).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>In this paper DreamerV2 is less sample-efficient and attains lower task performance than Iso-Dream in settings with noncontrollable visual dynamics, but likely faster per-episode to train.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>See fidelity_performance numbers above (DMC tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Effective general-purpose latent world model for MBRL, but performance degrades in environments with mixed controllable and noncontrollable visual dynamics because it lacks explicit disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simpler single-branch latent model: lower compute than multi-branch alternatives but lower robustness to visual distractors and to tasks that require prediction of external agent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses RSSM latent dynamics, imagination-based policy/value training, λ-returns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared in experiments here: Iso-Dream's disentangled three-branch architecture yields better performance in visual-distractor and driving tasks than DreamerV2 in this paper's benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1409.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1409.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet / RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet: Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An action-conditioned recurrent state-space model (RSSM) that combines deterministic recurrence (GRU) and stochastic latent states to model pixel dynamics for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM (PlaNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent state-space model with a deterministic recurrent core (GRU) producing hidden state h_t and stochastic latent state s_t; learns priors p(s_t|h_t) and posteriors q(s_t|h_t,o_t) and is trained via variational objectives (ELBO) for planning and imagination-based control.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / recurrent state-space model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based control and planning (DeepMind control suite, model-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO components (reconstruction log-likelihood, KL), and task return when used for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent states are compact but not explicitly interpretable; not designed to separate controllable vs noncontrollable dynamics by default.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specific in this paper; used as architectural building block for Iso-Dream's action-conditioned branch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides a sample-efficient basis for planning in latent space versus model-free methods; used as baseline architecture in Dreamer-family methods.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful as a compact latent dynamics model for planning; in Iso-Dream the RSSM-like design is adapted and extended with disentanglement and inverse dynamics to improve utility under noncontrollable visual dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM is a good general-purpose latent model but without additional inductive structure (e.g., disentanglement) may capture mixed sources of dynamics in a way less useful for downstream policy learning in the presence of external agents/noisy backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deterministic recurrence (GRU) + stochastic latent variables, prior/posterior dynamics, ELBO training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Forms the action-conditioned core for Dreamer/DreamerV2 and the s-branch in Iso-Dream; Iso-Dream augments this with separate z-branch and inverse dynamics to improve performance in specified settings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1409.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1409.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Recurrent world models facilitate policy evolution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage approach that learns a compressed latent representation of observations (VAE-style) and trains a policy/controller on the learned latent dynamics, demonstrating the utility of learned simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent world models facilitate policy evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage pipeline: first learn a VAE-style latent encoding of frames and then learn a recurrent predictor over latents; use the learned latent simulator to train controllers (policy) in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + recurrent predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vision-based control and policy learning in simulated environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction / predictive likelihood in latent space and downstream task performance (policy reward), as discussed historically; specific metrics not provided by this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides compressed latents that can be inspected but not explicitly disentangled by default; paper references as foundational work.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Influential conceptual predecessor demonstrating the utility of learned latent simulators for policy training; motivates later RSSM/Dreamer style methods.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Two-stage learning, separate representation and policy phases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Provides conceptual foundation for RSSM/Dreamer family; Iso-Dream extends the world-model concept with explicit disentanglement between controllable and noncontrollable dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1409.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1409.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PhyDNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PhyDNet: Disentangling physical dynamics from unknown factors for unsupervised video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-branch video prediction model that separates PDE-like physical dynamics from complementary unknown factors to improve long-term predictive fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Disentangling physical dynamics from unknown factors for unsupervised video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PhyDNet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-branch architecture that explicitly models physical (PDE) dynamics in one branch and other complementary/unexplained factors in another branch to improve interpretable and long-term video prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>structured latent world model (two-branch disentanglement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Action-conditioned video prediction (BAIR, RoboNet comparisons in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>PSNR and SSIM for frame prediction quality.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in this paper (Table 2): BAIR PSNR 18.91, SSIM 0.743; RoboNet PSNR 20.89, SSIM 0.727.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Designed to offer interpretability by separating dynamics into an explicit physical branch and a complementary branch; provides some disentanglement interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Architectural disentanglement into PDE dynamics and complementary branch; reported as two-branch comparison baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Iso-Dream outperforms PhyDNet on PSNR/SSIM in the experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>See fidelity_performance above; Iso-Dream achieves higher PSNR/SSIM than PhyDNet in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PhyDNet's disentanglement is useful for long-term prediction but Iso-Dream's controllable/noncontrollable separation plus inverse dynamics produces better predictive quality in the presented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Two-branch separation of physics-like dynamics and complementary unknown factors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically in video-prediction tasks; Iso-Dream shows superior PSNR/SSIM on BAIR and RoboNet in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Disentangling physical dynamics from unknown factors for unsupervised video prediction <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1409",
    "paper_id": "paper-220a21fbc074ef9d24af4cdd51465f74809a4317",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Iso-Dream",
            "name_full": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
            "brief_description": "A model-based RL framework that learns a three-branch latent world model separating controllable states (s_t), noncontrollable states (z_t), and time-invariant background; uses inverse dynamics to isolate controllable components and an attention-based 'future-state' mechanism to incorporate imagined noncontrollable rollouts into policy/value learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Iso-Dream world model",
            "model_description": "A latent recurrent state-space world model with three branches: (1) an action-conditioned branch (RSSM-like) that uses a deterministic GRU hidden state and stochastic controllable latent s_t; (2) an action-free branch (GRU or ST-LSTM) modeling noncontrollable stochastic latent z_t; (3) a static branch encoding time-invariant background. Priors p(tilde{s}_t|h_t) and p(tilde{z}_t|h'_t) and posteriors q(s_t|h_t,o_t), q(z_t|h'_t,o_t) are learned; a decoder reconstructs image components via masks. An Inverse Cell (MLP) is trained to regress actions from s_{t-1}, s_t to encourage controllability separation. Behavior learning imagines tilde{z}_{t+1:t+L+tau} first, computes a visionary state e_t via attention e_t = softmax(tilde{s}_t * tilde{z}_{t:t+tau}^T) tilde{z}_{t:t+tau} + tilde{s}_t, and then imagines actions and controllable-state rollouts to train policy/value with lambda-returns.",
            "model_type": "latent world model (recurrent state-space model with disentangled branches)",
            "task_domain": "Vision-based model-based RL and action-conditioned video prediction: DeepMind Control Suite with video backgrounds, CARLA autonomous driving, BAIR robot pushing, RoboNet",
            "fidelity_metric": "Training/objective metrics: image reconstruction / image log loss, reward log loss, discount log loss, KL divergences between posterior and prior for s and z; Evaluation metrics: task return (cumulative reward) for control tasks, PSNR and SSIM for video-prediction.",
            "fidelity_performance": "DeepMind Control Suite (video_easy): Walker Walk 911 ± 50, Cheetah Run 659 ± 62, Finger Spin 800 ± 59, Hopper Stand 746 ± 312 (final performance means reported in Table 1). Video prediction: BAIR PSNR 19.51, SSIM 0.768; RoboNet PSNR 21.71, SSIM 0.769 (Table 2). CARLA: qualitatively and quantitatively reported to outperform DreamerV2 by a large margin (numeric CARLA returns not provided in text).",
            "interpretability_assessment": "Relatively interpretable for pixel-level dynamics: explicit disentanglement into controllable vs noncontrollable vs static components yields interpretable masks and component reconstructions; attention maps provide an interpretable mechanism for how predicted noncontrollable states influence decisions; inverse dynamics objective steers s_t to be action-explainable.",
            "interpretability_method": "Visualization of decoder masks and per-branch reconstructions (showing localized controllable/noncontrollable regions); attention-based future-state weighting (visualizable); ablations demonstrating role of Inverse Cell for disentanglement.",
            "computational_cost": "No absolute GPU/time counts provided; authors state Iso-Dream requires longer training time per episode than DreamerV2 due to more intensive state transitions during behavior learning. Architectures and hidden sizes reported (e.g., GRU hidden size = 200 for DMC/CARLA, encoder conv layers). Sample budgets: DMC experiments up to 500k environment steps.",
            "efficiency_comparison": "Paper reports Iso-Dream is more sample-efficient than existing MBRL baselines but has higher per-episode computational cost than DreamerV2 (longer training time per episode). Exact wall-clock or FLOPs comparisons are not provided.",
            "task_performance": "Outperforms DreamerV2 and other baselines on DMC video_easy tasks (see fidelity_performance numbers) and outperforms competing video-prediction models on PSNR/SSIM; in CARLA, ablations show inverse dynamics and noncontrollable rollouts materially improve driving performance (figures reported qualitatively / as learning curves).",
            "task_utility_analysis": "Decoupling improves robustness to nonstationary visual distractors (e.g., dynamic video backgrounds) by enabling controller to use only controllable representation when appropriate; rolling out noncontrollable states gives forward-looking information that materially improves long-horizon decisions like collision avoidance in driving. The model supports two modes: discard z during imagination if noncontrollable dynamics are irrelevant, or include z rollouts when they affect reward.",
            "tradeoffs_observed": "Main trade-off is compute/time vs performance: disentanglement and rollouts of noncontrollable states improve sample efficiency and task performance but increase per-episode training time and architectural complexity; also requires environment-specific architecture tuning (authors note different benchmarks need different network structures).",
            "design_choices": "Key choices: three-branch modular architecture (s, z, static); RSSM-like action-conditioned branch for s with GRU; action-free branch for z (GRU or ST-LSTM depending on dataset); inverse dynamics MLP to isolate s; mask-based decoder to reconstruct per-branch visual components; optional reward model conditioned on s or (s,z); future-state attention to integrate s and imagined z_{t:t+tau}; flexibility to train action-free branch only with reconstruction if z is irrelevant.",
            "comparison_to_alternatives": "Compared empirically to DreamerV2, CURL, SVEA, SAC, DBC for control and to SVG, SA-ConvLSTM, PhyDNet for video prediction. Iso-Dream outperforms DreamerV2 and listed baselines on DMC video_easy tasks and CARLA (large margin per figures) and achieves higher PSNR/SSIM than baselines on BAIR and RoboNet. Computationally, it is slower per-episode than DreamerV2 but more sample-efficient.",
            "optimal_configuration": "Paper recommends (and demonstrates): include inverse dynamics objective to isolate controllable representations; roll out noncontrollable states and use future-state attention when noncontrollable dynamics affect decisions (e.g., autonomous driving); otherwise train action-free branch with reconstruction and omit z from imagination to save computation. Also recommends adapting network architecture (branch types, hidden sizes) to environment specifics.",
            "uuid": "e1409.0",
            "source_info": {
                "paper_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (Mastering Atari with discrete world models)",
            "brief_description": "A latent imagination-based MBRL agent that uses an RSSM latent world model and optimizes actor and value networks on imagined trajectories (λ-returns).",
            "citation_title": "Mastering atari with discrete world models",
            "mention_or_use": "use",
            "model_name": "DreamerV2 (RSSM-based latent world model and imagination-based actor-critic)",
            "model_description": "Latent recurrent state-space model (RSSM) with deterministic hidden state and stochastic latent variables; learns priors and posteriors via ELBO-style objectives and trains actor and value networks on imagined latent rollouts using λ-returns.",
            "model_type": "latent world model (RSSM)",
            "task_domain": "Model-based RL from pixels (originally Atari; used as baseline for continuous control/video tasks here)",
            "fidelity_metric": "Task return / cumulative reward in control tasks; model ELBO (reconstruction + KL) for training.",
            "fidelity_performance": "Reported in this paper as baselines on DMC video_easy tasks: Walker Walk 655 ± 47, Cheetah Run 475 ± 159, Finger Spin 755 ± 92, Hopper Stand 260 ± 366 (Table 1). CARLA performance depicted qualitatively/graphically — Iso-Dream reportedly outperforms DreamerV2 by a large margin.",
            "interpretability_assessment": "Latent representations are not explicitly disentangled for controllability; treated as black-box learned latents without per-branch masks or explicit interpretability mechanisms in this discussion.",
            "interpretability_method": "None mentioned in this paper; DreamerV2 typically uses abstract latent states without explicit visualization affordances in this text.",
            "computational_cost": "No explicit numbers in this paper; implied to be lower per-episode training time than Iso-Dream (authors state Iso-Dream requires longer per-episode training compared to DreamerV2).",
            "efficiency_comparison": "In this paper DreamerV2 is less sample-efficient and attains lower task performance than Iso-Dream in settings with noncontrollable visual dynamics, but likely faster per-episode to train.",
            "task_performance": "See fidelity_performance numbers above (DMC tasks).",
            "task_utility_analysis": "Effective general-purpose latent world model for MBRL, but performance degrades in environments with mixed controllable and noncontrollable visual dynamics because it lacks explicit disentanglement.",
            "tradeoffs_observed": "Simpler single-branch latent model: lower compute than multi-branch alternatives but lower robustness to visual distractors and to tasks that require prediction of external agent dynamics.",
            "design_choices": "Uses RSSM latent dynamics, imagination-based policy/value training, λ-returns.",
            "comparison_to_alternatives": "Compared in experiments here: Iso-Dream's disentangled three-branch architecture yields better performance in visual-distractor and driving tasks than DreamerV2 in this paper's benchmarks.",
            "optimal_configuration": null,
            "uuid": "e1409.1",
            "source_info": {
                "paper_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "PlaNet / RSSM",
            "name_full": "PlaNet: Recurrent State-Space Model (RSSM)",
            "brief_description": "An action-conditioned recurrent state-space model (RSSM) that combines deterministic recurrence (GRU) and stochastic latent states to model pixel dynamics for planning.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "RSSM (PlaNet)",
            "model_description": "Recurrent state-space model with a deterministic recurrent core (GRU) producing hidden state h_t and stochastic latent state s_t; learns priors p(s_t|h_t) and posteriors q(s_t|h_t,o_t) and is trained via variational objectives (ELBO) for planning and imagination-based control.",
            "model_type": "latent world model / recurrent state-space model",
            "task_domain": "Pixel-based control and planning (DeepMind control suite, model-based RL)",
            "fidelity_metric": "ELBO components (reconstruction log-likelihood, KL), and task return when used for planning.",
            "fidelity_performance": null,
            "interpretability_assessment": "Latent states are compact but not explicitly interpretable; not designed to separate controllable vs noncontrollable dynamics by default.",
            "interpretability_method": "None specific in this paper; used as architectural building block for Iso-Dream's action-conditioned branch.",
            "computational_cost": "Not specified in this paper.",
            "efficiency_comparison": "Provides a sample-efficient basis for planning in latent space versus model-free methods; used as baseline architecture in Dreamer-family methods.",
            "task_performance": null,
            "task_utility_analysis": "Useful as a compact latent dynamics model for planning; in Iso-Dream the RSSM-like design is adapted and extended with disentanglement and inverse dynamics to improve utility under noncontrollable visual dynamics.",
            "tradeoffs_observed": "RSSM is a good general-purpose latent model but without additional inductive structure (e.g., disentanglement) may capture mixed sources of dynamics in a way less useful for downstream policy learning in the presence of external agents/noisy backgrounds.",
            "design_choices": "Deterministic recurrence (GRU) + stochastic latent variables, prior/posterior dynamics, ELBO training.",
            "comparison_to_alternatives": "Forms the action-conditioned core for Dreamer/DreamerV2 and the s-branch in Iso-Dream; Iso-Dream augments this with separate z-branch and inverse dynamics to improve performance in specified settings.",
            "optimal_configuration": null,
            "uuid": "e1409.2",
            "source_info": {
                "paper_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models (Recurrent world models facilitate policy evolution)",
            "brief_description": "A two-stage approach that learns a compressed latent representation of observations (VAE-style) and trains a policy/controller on the learned latent dynamics, demonstrating the utility of learned simulators.",
            "citation_title": "Recurrent world models facilitate policy evolution",
            "mention_or_use": "mention",
            "model_name": "World Models",
            "model_description": "Two-stage pipeline: first learn a VAE-style latent encoding of frames and then learn a recurrent predictor over latents; use the learned latent simulator to train controllers (policy) in latent space.",
            "model_type": "latent world model (VAE + recurrent predictor)",
            "task_domain": "Vision-based control and policy learning in simulated environments",
            "fidelity_metric": "Reconstruction / predictive likelihood in latent space and downstream task performance (policy reward), as discussed historically; specific metrics not provided by this paper.",
            "fidelity_performance": null,
            "interpretability_assessment": "Provides compressed latents that can be inspected but not explicitly disentangled by default; paper references as foundational work.",
            "interpretability_method": "Not described in this paper.",
            "computational_cost": null,
            "efficiency_comparison": null,
            "task_performance": null,
            "task_utility_analysis": "Influential conceptual predecessor demonstrating the utility of learned latent simulators for policy training; motivates later RSSM/Dreamer style methods.",
            "tradeoffs_observed": null,
            "design_choices": "Two-stage learning, separate representation and policy phases.",
            "comparison_to_alternatives": "Provides conceptual foundation for RSSM/Dreamer family; Iso-Dream extends the world-model concept with explicit disentanglement between controllable and noncontrollable dynamics.",
            "optimal_configuration": null,
            "uuid": "e1409.3",
            "source_info": {
                "paper_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "PhyDNet",
            "name_full": "PhyDNet: Disentangling physical dynamics from unknown factors for unsupervised video prediction",
            "brief_description": "A two-branch video prediction model that separates PDE-like physical dynamics from complementary unknown factors to improve long-term predictive fidelity.",
            "citation_title": "Disentangling physical dynamics from unknown factors for unsupervised video prediction",
            "mention_or_use": "use",
            "model_name": "PhyDNet",
            "model_description": "Two-branch architecture that explicitly models physical (PDE) dynamics in one branch and other complementary/unexplained factors in another branch to improve interpretable and long-term video prediction.",
            "model_type": "structured latent world model (two-branch disentanglement)",
            "task_domain": "Action-conditioned video prediction (BAIR, RoboNet comparisons in this paper)",
            "fidelity_metric": "PSNR and SSIM for frame prediction quality.",
            "fidelity_performance": "Reported in this paper (Table 2): BAIR PSNR 18.91, SSIM 0.743; RoboNet PSNR 20.89, SSIM 0.727.",
            "interpretability_assessment": "Designed to offer interpretability by separating dynamics into an explicit physical branch and a complementary branch; provides some disentanglement interpretability.",
            "interpretability_method": "Architectural disentanglement into PDE dynamics and complementary branch; reported as two-branch comparison baseline.",
            "computational_cost": "Not specified in this paper.",
            "efficiency_comparison": "Iso-Dream outperforms PhyDNet on PSNR/SSIM in the experiments reported here.",
            "task_performance": "See fidelity_performance above; Iso-Dream achieves higher PSNR/SSIM than PhyDNet in the reported comparisons.",
            "task_utility_analysis": "PhyDNet's disentanglement is useful for long-term prediction but Iso-Dream's controllable/noncontrollable separation plus inverse dynamics produces better predictive quality in the presented experiments.",
            "tradeoffs_observed": null,
            "design_choices": "Two-branch separation of physics-like dynamics and complementary unknown factors.",
            "comparison_to_alternatives": "Compared empirically in video-prediction tasks; Iso-Dream shows superior PSNR/SSIM on BAIR and RoboNet in this paper.",
            "optimal_configuration": null,
            "uuid": "e1409.4",
            "source_info": {
                "paper_title": "Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2
        },
        {
            "paper_title": "Disentangling physical dynamics from unknown factors for unsupervised video prediction",
            "rating": 2
        }
    ],
    "cost": 0.01794625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models</h1>
<p>Minting Pan<em> ${ }^{</em>}$ Xiangming Zhu<em> ${ }^{</em>}$ Yunbo Wang ${ }^{\dagger}$ Xiaokang Yang<br>MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University<br>{panmt53, xmzhu76, yunbow, xkyang}@sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>World models learn the consequences of actions in vision-based interactive systems. However, in practical scenarios such as autonomous driving, there commonly exists noncontrollable dynamics independent of the action signals, making it difficult to learn effective world models. To tackle this problem, we present a novel reinforcement learning approach named Iso-Dream, which improves the Dream-to-Control framework [22] in two aspects. First, by optimizing the inverse dynamics, we encourage the world model to learn controllable and noncontrollable sources of spatiotemporal changes on isolated state transition branches. Second, we optimize the behavior of the agent on the decoupled latent imaginations of the world model. Specifically, to estimate state values, we roll-out the noncontrollable states into the future and associate them with the current controllable state. In this way, the isolation of dynamics sources can greatly benefit long-horizon decisionmaking of the agent, such as a self-driving car that can avoid potential risks by anticipating the movement of other vehicles. Experiments show that Iso-Dream is effective in decoupling the mixed dynamics and remarkably outperforms existing approaches in a wide range of visual control and prediction domains.</p>
<h2>1 Introduction</h2>
<p>Humans can infer and predict real-world dynamics by simply observing and interacting with the environment. Inspired by this, many cutting-edge AI agents use self-supervised learning [38, 20, 12] or reinforcement learning [39, 22, 42] techniques to acquire knowledge from their surroundings. Among them, world models [20] have received widespread attention in the field of robot visual control, and led the recent progress in model-based reinforcement learning (MBRL) [22, 42, 24, 30]. A typical approach [22] is to use the trajectories of observations and control signals collected by an RL agent to learn a differentiable simulator of the environment, namely the world model, and then update the RL agent by optimizing the behaviors on the latent imaginations of the world model.</p>
<p>However, since the observation sequence is high-dimensional, non-stationary, and often driven by multiple sources of physical dynamics, how to learn effective world models in complex visual scenes remains an open problem. In realistic scenarios such as autonomous driving, we can generally divide spatiotemporal dynamics in the system into controllable parts that perfectly respond to action signals, and parts beyond the control of the agent, such as the movement of other vehicles and other external changes. The isolation of controllable and noncontrollable states can improve MBRL in two aspects:</p>
<ul>
<li>Modular representation improves the generalization of the agent to non-stationary environments with noises, such as the time-varying background in our modified DeepMind Control Suite.</li>
</ul>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Probabilistic graph of Iso-Dream. It learns to decouple complex visual dynamics into controllable states (<em>s<sub>t</sub></em>) and noncontrollable states (<em>z<sub>t</sub></em>) by optimizing the inverse dynamics (Red dashed arrows). On top of the disentangled states, it performs model-based reinforcement learning by explicitly considering the predicted noncontrollable component of future dynamics (Blue arrows).</p>
<ul>
<li>More importantly, it improves long-horizon RL tasks that can greatly benefit from decisions based on predictions of future noncontrollable dynamics. For example, in autonomous driving, potential risks can be better avoided by predicting the movement of other vehicles.</li>
</ul>
<p>We present Iso-Dream, a novel MBRL framework that learns to decouple and leverage the controllable and noncontrollable state transitions. Accordingly, it improves the original Dreamer [22] from two perspectives: (i) <em>a new form of world model representations</em> and (ii) <em>a new actor-critic algorithm to derive the behavior from the world model.</em> As shown in Figure 1, the foundation of decoupling the world model is to separate the mixed latent states into an action-conditioned branch and an action-free branch, which can individually transit different sources of visual dynamics. The components are jointly trained to maximize the variational lower bounds. To further isolate the controllable states, the action-conditioned branch is also optimized with inverse dynamics, that is, to reason about the actions that have driven the state transitions between adjacent time steps.</p>
<p>Another contribution of Iso-Dream is to find that disentangling physical dynamics can greatly benefit the downstream decision-making tasks by more accurately foreseeing the inherent changes in the environment. Intuitively, humans can decide how to interact with the environment at each moment based on their anticipation of future changes in the surroundings. To make more forward-looking decisions, as shown by the blue arrows in Figure 1, the policy network integrates the current controllable state and multiple steps of predicted noncontrollable states through an attention mechanism. It enables the agent to thoroughly consider possible future interactions with the environment.</p>
<p>We evaluate Iso-Dream in the following domains: The modified DeepMind Control Suite with noisy video background; The CARLA autonomous driving environment in which other vehicles can be naturally viewed as noncontrollable components; The real-world BAIR robot dataset and the RoboNet dataset that are helpful to validate the effectiveness of the world model for disentanglement. On all benchmarks, Iso-Dream remarkably outperforms the existing approaches by large margins.</p>
<h2>2 Related Work</h2>
<p><strong>Action-conditioned video prediction.</strong> A straightforward deep learning solution to visual control problems is to learn action-conditioned video prediction models [38, 14, 8, 53] and then perform Monte-Carlo importance sampling and optimization algorithms, such as the <em>cross-entropy methods</em>, over available behaviors [15, 12, 29]. Hot topics in video prediction mainly includes long-term and high-fidelity future frames generation [44, 43, 51, 5, 52, 50, 54, 41, 40, 36, 56, 28, 2], dynamics uncertainty modeling [1, 10, 48, 31, 7, 16, 55], object-centric scene decomposition [47, 27, 18, 58, 3], and space-time disentanglement [49, 27, 19, 6]. The corresponding technical improvements mainly involve the use of more effective neural architectures, novel probabilistic modeling methods, and specific forms of video representation. The disentanglement methods are closely related to the world model in Iso-Dream. They commonly separate visual dynamics into content and motion vectors, or long-term and short-term states. In contrast, Iso-Dream is designed to learn a decoupled world model based on controllability, which contributes more to the downstream behavior learning process.</p>
<p>Visual MBRL. In visual control tasks, the agents have to learn the action policy directly from high-dimensional observations. They can be roughly grouped into two categories, that is, model-free methods [34, 57, 32, 33, 25] and model-based methods [15, 39, 20, 23, 22, 30, 42, 59, 4]. Among them, the MBRL approaches explicitly model the state transitions and generally yield higher sample efficiency than the model-free methods. Ha and Schmidhuber [20] proposed the World Models that first learn compressed latent states of the environment in a self-supervised manner, and then train the agent on the latent states generated by the world model. Following the two-stage training procedure, PlaNet [23] uses an action-conditioned, recurrent state-space model (RSSM) as the world model, and optimizes the action policy on the recurrent states with the cross-entropy methods. In Dreamer [22] and DreamerV2 [24], agents learn behaviors by optimizing the expected values over the predicted latent states in RSSM. InfoPower [4] prioritizes functional-related information from visual observations to obtain a more robust representation for MBRL. Notably, Iso-Dream is very different from InfoPower in two aspects. First, we explicitly model the state transitions of controllable and noncontrollable dynamics, so that it is possible to choose whether to take the noncontrollable states into behavior learning according to the prior knowledge of a specific domain. Second, we propose a new behavior learning method that greatly benefits from the decoupled world model, so that we can preview possible future states of noncontrollable patterns before making decisions at this moment.</p>
<h1>3 Method</h1>
<p>In this section, we first present basic assumptions and the general framework of Iso-Dream for decoupling and leveraging controllable and noncontrollable dynamics for visual control (Section 3.1). For representation learning, we introduce the three-branch world model and its training objectives of inverse dynamics (Section 3.2). For behavior learning, we present an actor-critic method that is trained on the imaginations of the decoupled world model latent states, so that the agent may consider possible future states of noncontrollable dynamics (Section 3.3). Finally, we discuss how Iso-Dream is deployed to interact with the environment (Section 3.4).</p>
<h3>3.1 Basic Assumptions of Iso-Dream</h3>
<p>As shown in Figure 1, when the agent receives a sequence of visual observations $o_{1: T}$, the underlying spatiotemporal dynamics can be defined as $u_{1: T}$. Our goal is to understand the inner relationships of different dynamics by decoupling $u_{1: T}$ into controllable latent states $s_{1: T}$ and noncontrollable latent states $z_{1: T}$ that vary in spacetime, such that:</p>
<p>$$
u_{1: T} \sim(s, z)<em t_1="t+1">{1: T}, \quad s</em>\right)
$$} \sim p\left(s_{t+1} \mid s_{t}, a_{t}\right), \quad z_{t+1} \sim p\left(z_{t+1} \mid z_{t</p>
<p>where $a_{t}$ is the action signal. To achieve long-term prediction, we isolate $s_{t}$ and $z_{t}$ to each other and model their state transitions of $p\left(s_{t+1} \mid s_{t}, a_{t}\right)$ and $p\left(z_{t+1} \mid z_{t}\right)$ respectively.
According to our prior knowledge of the environment, we can optionally choose whether to roll out the noncontrollable states and consider them during behavior learning. For tasks where the noncontrollable components can be viewed as time-varying noise, we simply derive the action policy by $a_{t} \sim \pi\left(a_{t} \mid s_{t}\right)$. The isolation of controllable states improves the generalization of the agent to non-stationary systems. For tasks like autonomous driving, the behaviors are derived by</p>
<p>$$
a_{t} \sim \pi\left(a_{t} \mid s_{t}, z_{t: t+\tau}\right)
$$</p>
<p>where we calculate the relationships between $s_{t}$ and the imagined noncontrollable states over time horizon $\tau$. It assumes that, in specific long-horizon tasks, the agent can greatly benefit from predicting the consequences of external noncontrollable forces.</p>
<h3>3.2 Representation Learning of Controllable and Noncontrollable Dynamics</h3>
<p>Inspired by previous approaches [37, 17] showing that modular structures are effective for disentanglement learning, we leverage a three-branch architecture to decouple $u_{t}$ into controllable dynamics state $s_{t}$, noncontrollable dynamics state $z_{t}$, and time-invariant representation of the background. As shown in Figure 2(a), the action-conditioned branch models $p\left(s_{t+1} \mid s_{t}, a_{t}\right)$. It follows the RSSM architecture from PlaNet [23] to use a recurrent neural network $\mathrm{GRU}<em t="t">{s}(\cdot)$, the deterministic hidden state $h</em>\right)$ with similar network structures. The transition models with separate parameters can be written as follows:}$, and the stochastic state $s_{t}$ to form the transition model, where the GRU keeps the historical information of the controllable dynamics. The action-free branch models $p\left(z_{t+1} \mid z_{t</p>
<p>$$
\begin{aligned}
p\left(\tilde{s}<em _t="&lt;t">{t} \mid s</em>}, a_{&lt;t}\right) &amp; =p\left(\tilde{s<em t="t">{t} \mid h</em>}\right), &amp; &amp; \text { where } h_{t}=\operatorname{GRU<em t-1="t-1">{s}\left(h</em>\right) \
p\left(\tilde{z}}, s_{t-1}, a_{t-1<em _t="&lt;t">{t} \mid z</em>}\right) &amp; =p\left(\tilde{z<em t="t">{t} \mid h</em>}^{\prime}\right), &amp; &amp; \text { where } h_{t}^{\prime}=\operatorname{GRU<em t-1="t-1">{z}\left(h</em>\right)
\end{aligned}
$$}^{\prime}, z_{t-1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overall architecture of the world model and the behavior learning algorithm in Iso-Dream. (a) World model with three branches to explicitly disentangle controllable, noncontrollable, and static components from visual data, where the action-conditioned branch learns controllable state transitions by modeling inverse dynamics. (b) The agent optimizes the behaviors in imaginations of the world model through a future state attention mechanism.</p>
<p>We here use $\tilde{s}<em t="t">{t}$ and $\tilde{z}</em>}$ to denote the prior representations. We optimize the transition models with posterior representations that are derived from $s_{t} \sim q\left(s_{t} \mid h_{t}, o_{t}\right)$ and $z_{t} \sim q\left(z_{t} \mid h_{t}^{\prime}, o_{t}\right)$. We learn the posteriors from the observation at current time step $o_{t} \in \mathbb{R}^{3 \times H \times W}$ by a shared encoder $\mathrm{Enc<em _phi__1="\phi_{1">{\theta}$ and subsequent branch-specific encoders $\mathrm{Enc}</em>}}$ and $\mathrm{Enc<em 2="2">{\phi</em>$.}</p>
<p>To enhance the disentanglement representation learning corresponding to the control signals, we introduce the training objective of inverse dynamics. Accordingly, we design an Inverse Cell of a 2-layer MLP to infer the actions that lead to certain transitions of the controllable states:</p>
<p>$$
\text { Inverse dynamics: } \quad \tilde{a}<em t-1="t-1">{t-1}=\operatorname{MLP}\left(s</em>\right)
$$}, s_{t</p>
<p>where the inputs are the posterior representations in the action-conditioned branch. By learning to regress the true behavior $a_{t-1}$, the Inverse Cell facilitates the action-conditioned branch to isolate the representation of the controllable dynamics. To avoid the training collapse where the actionconditioned branch captures most of the useful information, while the action-free branch learns almost nothing, in the process of image reconstruction, we respectively use the prior state $\tilde{s}<em t="t">{t}$ and the posterior state $z</em>}$ to generate the controllable visual component $\tilde{o<em t="t">{t}^{s} \in \mathbb{R}^{3 \times H \times W}$ with mask $M</em>}^{s} \in \mathbb{R}^{1 \times H \times W}$ and the noncontrollable component $\tilde{o<em t="t">{t}^{z} \in \mathbb{R}^{3 \times H \times W}$ with $M</em>$. By further integrating the time-invariant information extracted from the first $K$ frames, we have}^{z} \in \mathbb{R}^{1 \times H \times W</p>
<p>$$
\tilde{o}<em t="t">{t}=M</em>}^{s} \odot \tilde{o<em t="t">{t}^{s}+M</em>}^{z} \odot \tilde{o<em t="t">{t}^{z}+\left(1-M</em>}^{s}-M_{t}^{z}\right) \odot \tilde{o}^{b}, \quad \text { where } \tilde{o}^{b}=\operatorname{Dec<em 3="3">{\varphi</em>}}\left(\operatorname{Enc<em 3="3">{\theta, \phi</em>\right)\right)\right)
$$}}\left(o_{1: K</p>
<p>For reward modeling, we have two options with the action-free branch. In one case, the noncontrollable dynamics can be considered as noises that are not related to the task, and therefore $z_{t}$ is no longer useful during imagination. In other words, the policy and the predicted reward are only related to the controllable states. In the other case, future noncontrollable states would affect how the agent makes decisions, and we consider the action-free components during behavior learning. For this, we learn alternative reward models $p\left(r_{t} \mid s_{t}\right)$ or $p\left(r_{t} \mid s_{t}, z_{t}\right)$ in forms of MLPs.</p>
<p>For a sequence of $\left(o_{t}, a_{t}, r_{t}\right)<em 1="1">{t=1}^{T}$ sampled from the replay buffer during training, the world model can be optimized using the following loss functions, where $\alpha, \beta</em>$ are hyper-parameters:}$, and $\beta_{2</p>
<p>$$
\begin{aligned}
\mathcal{L}= &amp; \mathrm{E}{\underbrace{\sum_{t=1}^{T}-\ln p\left(o_{t} \mid h_{t}, s_{t}, h_{t}^{\prime}, z_{t}\right)}<em t="t">{\text {image log loss }} \underbrace{-\ln p\left(r</em>} \mid h_{t}, s_{t}, h_{t}^{\prime}, z_{t}\right)<em t="t">{\text {reward log loss }} \underbrace{-\ln p\left(\gamma</em>} \mid h_{t}, s_{t}, h_{t}^{\prime}, z_{t}\right)<em 2="2">{\text {discount log loss }}} \
&amp; +\underbrace{\alpha \ell</em>}\left(a_{t}, \tilde{a<em _action="{action" _text="\text" loss="loss">{t}\right)}</em>}
\end{aligned}
$$}}+\underbrace{\beta_{1} \mathrm{KL}\left[q\left(s_{t} \mid h_{t}, o_{t}\right) \mid p\left(s_{t} \mid h_{t}\right)\right]+\beta_{2} \mathrm{KL}\left[q\left(z_{t} \mid h_{t}^{\prime}, o_{t}\right) \mid p\left(z_{t} \mid h_{t}^{\prime}\right)\right]}_{\mathrm{KL} \text { divergence }</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Iso-Dream</span><span class="w"> </span><span class="o">(</span><span class="nt">Highlight</span><span class="o">:</span><span class="w"> </span><span class="nt">Our</span><span class="w"> </span><span class="nt">modifications</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">behavior</span><span class="w"> </span><span class="nt">learning</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">deploy-</span>
<span class="nt">ment</span><span class="o">)</span>
<span class="nt">Hyperparameters</span><span class="o">:</span><span class="w"> </span><span class="nt">L</span><span class="o">:</span><span class="w"> </span><span class="nt">Imagination</span><span class="w"> </span><span class="nt">horizon</span><span class="o">;</span><span class="w"> </span><span class="nt">r</span><span class="o">:</span><span class="w"> </span><span class="nt">Window</span><span class="w"> </span><span class="nt">size</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">future</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="nt">attention</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">replay</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">random</span><span class="w"> </span><span class="nt">episodes</span><span class="o">.</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">converged</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">update</span><span class="w"> </span><span class="nt">step</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">c</span><span class="o">=</span><span class="nt">1</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="w"> </span><span class="nt">C</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">Draw</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="nt">sequences</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(o_{t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">r_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">t=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="err">\</span><span class="o">).</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">Representation</span><span class="w"> </span><span class="nt">learning</span>
<span class="w">            </span><span class="nt">Compute</span><span class="w"> </span><span class="nt">world</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="nt">loss</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">Eq</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">6</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">update</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="nt">parameters</span><span class="o">.</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">Behavior</span><span class="w"> </span><span class="nt">learning</span>
<span class="w">            </span><span class="nt">Roll-out</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">noncontrollable</span><span class="w"> </span><span class="nt">states</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\tilde{z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}</span><span class="nt">_</span><span class="p">{</span><span class="err">i=t+1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">t+L+\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">through</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">action-free</span><span class="w"> </span><span class="nt">branch</span><span class="w"> </span><span class="nt">alone</span><span class="o">.</span>
<span class="w">            </span><span class="nt">for</span><span class="w"> </span><span class="nt">time</span><span class="w"> </span><span class="nt">step</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">j</span><span class="o">=</span><span class="nt">i</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="w"> </span><span class="nt">i</span><span class="o">+</span><span class="nt">L</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">                </span><span class="nt">Compute</span><span class="w"> </span><span class="nt">latent</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">e_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Attention</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="n">j</span><span class="p">:</span><span class="w"> </span><span class="n">j</span><span class="o">+</span><span class="err">\</span><span class="n">tau</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">Eq</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">7</span><span class="o">).</span>
<span class="w">                </span><span class="nt">Imagine</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">pi</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">e_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">).</span>
<span class="w">                </span><span class="nt">Predict</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">next</span><span class="w"> </span><span class="nt">controllable</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">j+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">p</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">a_</span><span class="p">{</span><span class="err">j</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">action-conditioned</span><span class="w"> </span><span class="nt">branch</span><span class="w"> </span><span class="nt">alone</span><span class="o">.</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">            </span><span class="nt">Update</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">value</span><span class="w"> </span><span class="nt">models</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">Eq</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">8</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">estimated</span><span class="w"> </span><span class="nt">rewards</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">values</span><span class="o">.</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="nt">Environment</span><span class="w"> </span><span class="nt">interaction</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">env</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">reset</span><span class="p">}</span><span class="o">()</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">time</span><span class="w"> </span><span class="nt">step</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">1</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Calculate</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">posterior</span><span class="w"> </span><span class="nt">representation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">q</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">s_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">h_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">),</span><span class="w"> </span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">q</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">h_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">).</span>
<span class="w">        </span><span class="nt">Roll-out</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">noncontrollable</span><span class="w"> </span><span class="nt">states</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t+1:</span><span class="w"> </span><span class="err">t+\tau</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">through</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">action-free</span><span class="w"> </span><span class="nt">branch</span><span class="w"> </span><span class="nt">alone</span><span class="o">.</span>
<span class="w">        </span><span class="nt">Generate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">pi</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">s_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">z_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">tilde</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t+1:</span><span class="w"> </span><span class="err">t+\tau</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">using</span><span class="w"> </span><span class="nt">future</span><span class="w"> </span><span class="nt">state</span><span class="w"> </span><span class="nt">attention</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">Eq</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">7</span><span class="o">).</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">r_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">o_</span><span class="p">{</span><span class="err">t+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">env</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">step</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">Add</span><span class="w"> </span><span class="nt">experience</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">replay</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">B</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="p">{</span><span class="err">\left(o_{t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">a_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">r_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="nt">_</span><span class="p">{</span><span class="err">t=1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">T</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\}\</span><span class="o">).</span>
<span class="w">    </span><span class="nt">end</span>
</code></pre></div>

<p>The world model training approach can be partly customized for different environments. In situations where noncontrollable states are indeed involved in behavior learning, minimizing the ELBO objective can maintain the semantics of $\tilde{z}_{t}$. Otherwise, if the action-free features are only used to prevent noisy distractions from affecting the training process of Iso-Dream, rather than being used for behavior learning, we can simply train the action-free branch with the reconstruction loss alone.</p>
<h1>3.3 Behavior Learning in Decoupled Imaginations</h1>
<p>Thanks to the decoupled world model, we can optimize the agent behaviors to adaptively consider the relations between available actions and possible future states of the noncontrollable dynamics. A practical example is autonomous driving, where the motion of other vehicles can be naturally viewed as noncontrollable but predictable components. As shown in Figure 2(b), we here propose an improved actor-critic learning algorithm that 1) allows the action-free branch to foresee the future ahead of the action-conditioned branch, and 2) exploits the predicted future information of noncontrollable dynamics to make more forward-looking decisions.</p>
<p>Suppose we are making decisions at time step $t$ in the imagination period. A straightforward solution from the original Dreamer method is to learn an action model and a value model based on the isolated controllable state $\tilde{s}<em t:="t:" t_tau="t+\tau">{t} \in \mathbb{R}^{1 \times d}$. However, we notice that by employing an attention mechanism, we can explicitly calculate its relations to a sequence of future noncontrollable states $\tilde{z}</em>$, where $\tau$ is the length of a sliding window from now on.} \in \mathbb{R}^{\tau \times d</p>
<p>$$
\text { Future state attention: } \quad e_{t}=\operatorname{softmax}\left(\tilde{s}<em t:="t:" t_tau="t+\tau">{t} \tilde{z}</em>}^{T}\right) \tilde{z<em t="t">{t: t+\tau}+\tilde{s}</em>
$$</p>
<p>In this way, $\tilde{s}<em t="t">{t}$ evolves to a more "visionary" representation $e</em>$. We update the action model and the value model in Dreamer [22] as follows:} \in \mathbb{R}^{1 \times d</p>
<p>Action model: $\quad a_{t} \sim \pi\left(a_{t} \mid e_{t}\right), \quad$ Value model: $\quad v_{\xi}\left(e_{t}\right) \approx \mathbb{E}<em t="t">{\pi\left(\cdot \mid e</em>$,
where $L$ is the imagination time horizon. As shown in Alg. 1, during imagination, we first use the action-free transition model to obtain sequences of noncontrollable states of length $L+\tau$, denoted}\right)} \sum_{k=t}^{t+L} \gamma^{k-t} r_{k</p>
<p>Table 1: Performance of visual control tasks in the DMC Suite. The agents are trained and evaluated in environments with video_easy dynamic background. We report the mean and std of final performance over 3 seeds and 5 trajectories. *We use a different setup from that in the paper of DBC.</p>
<table>
<thead>
<tr>
<th>TASK</th>
<th>SVEA</th>
<th>CURL</th>
<th>DBC*</th>
<th>DREAMERV2</th>
<th>ISO-DREAM</th>
</tr>
</thead>
<tbody>
<tr>
<td>WALKER WALK</td>
<td>$826 \pm 65$</td>
<td>$443 \pm 206$</td>
<td>$32 \pm 7$</td>
<td>$655 \pm 47$</td>
<td>$\mathbf{9 1 1} \pm \mathbf{5 0}$</td>
</tr>
<tr>
<td>CHEETAH RUN</td>
<td>$178 \pm 64$</td>
<td>$269 \pm 24$</td>
<td>$15 \pm 5$</td>
<td>$475 \pm 159$</td>
<td>$\mathbf{6 5 9} \pm \mathbf{6 2}$</td>
</tr>
<tr>
<td>FINGER SPIN</td>
<td>$562 \pm 22$</td>
<td>$280 \pm 50$</td>
<td>$1 \pm 2$</td>
<td>$755 \pm 92$</td>
<td>$\mathbf{8 0 0} \pm \mathbf{5 9}$</td>
</tr>
<tr>
<td>HOPPER STAND</td>
<td>$6 \pm 8$</td>
<td>$451 \pm 250$</td>
<td>$5 \pm 9$</td>
<td>$260 \pm 366$</td>
<td>$\mathbf{7 4 6} \pm \mathbf{3 1 2}$</td>
</tr>
</tbody>
</table>
<p>by $\left{\tilde{z}<em i="t">{i}\right}</em>$.}^{i+L+\tau}$. At each time step in the imagination period, the agent draws an action $a_{j}$ from the visionary state $e_{j}$, which is derived from Eq. (7). The action-conditioned branch uses the action $a_{j}$ in latent imagination and predicts the next controllable state $s_{j+1}$. We follow DreamerV2 [24] to train the action model to maximize the $\lambda$-return [45], and train the value model to regress the $\lambda$-return ${ }^{3</p>
<h1>3.4 Policy Deployment by Rolling-out Noncontrollable Dynamics</h1>
<p>As discussed above, in the cases that noncontrollable dynamics are irrelevant to the control task, when interacting with the environment, we only use the state of controllable dynamics to generate the policy at each time step $t$. However, for the situation where noncontrollable dynamics should be closely related to the behavior of the agent, as shown in Lines 21-22 in Alg. 1, the action-free branch consecutively predicts the next $\tau-1$ noncontrollable states $\tilde{z}<em t="t">{t+1: t+\tau}$ starting from the current posterior state $z</em>}$. Similar to Eq. (7) in the process of behavior learning, we here use the learned future state attention network to adaptively integrate $s_{t}, z_{t}$ and $\tilde{z<em t="t">{t+1: t+\tau}$. Based on the integrated feature $e</em>$ from the action model to interact with the environment.}$, the Iso-Dream agent draws $a_{t</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>Benchmarks. We quantitatively and qualitatively evaluate Iso-Dream on two reinforcement learning environments, i.e., DeepMind Control Suite [46] and CARLA [11], and two real-world datasets for action-conditioned video prediction, i.e., BAIR robot pushing [13] and RoboNet [9]. The video prediction experiments can provide more intuitive visualizations of disentanglement learning.</p>
<p>Compared methods. For the visual control tasks, we compare our approach with five baselines, including both model-based and model-free methods, i.e., DreamerV2 [24], CURL [34], SVEA [25], SAC [21], and DBC [59]. For action-conditioned video prediction, we mainly compare our decoupled world model with three approaches, i.e., SVG [10], SA-ConvLSTM [35] and PhyDNet [19].</p>
<h3>4.2 DeepMind Control Suite</h3>
<p>Implementation details. In order to verify the enhancement of Iso-Dream by disentangling different components under complex visual dynamics, we evaluate Iso-Dream on environments from DMC Generalization Benchmark. Instead of training on original DeepMind Control Suite environments, agents are trained and tested both with natural video backgrounds (i.e. video_easy environments). In this environment, since the background is randomly replaced by a real-world video, the noncontrollable motion of the background can affect the procedure of dynamics learning and behavior learning of agents. Therefore, to obtain a better decision policy and avoid the disruption from noisy backgrounds, the agent may decouple noncontrollable representation (i.e., dynamic background) and controllable representation in spacetime, and only use controllable representation for control. To this end, we simply train the action-free branch with only reconstruction loss and discard it in imagination and policy deployment. We evaluate our model with baselines in 4 tasks from four different domains. The number of environment steps is limited to 500 k .</p>
<p>Quantitative results. To evaluate the performance, we train and test the agents in environments with video backgrounds. As shown in Table 1, Iso-Dream exceeds the performance of DreamerV2 and other baselines in all tasks, indicating that the three-branch structure can effectively learn task-related visual representations and alleviate complex background interference in visual data.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Video prediction results on the DMC (left) and CARLA (right) benchmarks of Iso-Dream. For each sequence, we use the first 5 images as context frames. Iso-Dream successfully disentangles controllable and noncontrollable components.</p>
<p>Qualitative results. We leverage Iso-Dream to complete video prediction tasks in video_easy environments. The sequence of frames and actions are randomly collected during test episodes. The first 5 frames are given to the model and the next 45 frames are predicted only based on action inputs. To show the qualitative results, we visualize the masks and visual decoupled components from the action-conditioned and action-free branches. The overall visualization is shown in Figure 3(left). From this prediction result, we can find that Iso-Dream has the ability to predict long-term sequence and disentangle controllable and noncontrollable dynamics from images in video_easy environments. As shown in the third and fourth row of action-conditioned branch output in Figure 3, the controllable representation has been successfully isolated and matches its mask. Besides, in this visualization, the action-free component in this background video is the motion of sea waves, which is captured by the fifth and sixth row of action-free branch outputs.</p>
<h1>4.3 CARLA Autonomous Driving Environment</h1>
<p>Implementation details. In the autonomous driving task, We use a camera with 60 degree view on the roof of the ego-vehicle, which obtains images of $64 \times 64$ pixels. Following the setting in the DBC [59], in order to encourage highway progression and penalise collisions, the reward is formulated as: $r_{t}=v_{c g o}^{T} \dot{u}<em 1="1">{h} \cdot \Delta t-\xi</em>} \cdot \mathbb{I}-\xi_{2} \cdot|\text { steer }|$, where $v_{e g o}$ is the velocity vector of the ego-vehicle, projected onto the highway's unit vector $\dot{u<em 1="1">{h}$, and multiplied by time discretization $\Delta t=0.05$ to measure highway progression in meters. Impulse $\mathbb{I} \in \mathbb{R}^{+}$is caused by collisions, and a steering penalty steer $\in[-1,1]$ facilitates lane-keeping. The hyper-parameters $\xi</em>=1$ and $\alpha=1$ in Eq. (6) and $\tau=5$ in Eq. (7).}$ and $\xi_{2}$ are set to $10^{-4}$ and 1 , respectively. We use $\beta_{1}=1, \beta_{2</p>
<p>Quantitative results. As shown in Figure 4(a), Iso-Dream has significant advantages compared to other baselines and outperforms DreamerV2 by a large margin. Furthermore, we conduct ablation studies to confirm the validity of inverse dynamics and the rolling-out strategy of noncontrollable states. Figure 4(b) shows that the performance drops when Inverse Cell is removed, indicating the importance of modeling inverse dynamics to isolate controllable and noncontrollable components from the whole dynamics. In order to verify the effectiveness of the proposed attention mechanism, we conduct experiments to evaluate Iso-Dream where policy networks directly concatenate the current controllable state and the noncontrollable state as input. Comparing the blue curve and green curve,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance with 3 seeds on the CARLA driving task. (a) Comparison of existing methods, in which Iso-Dream outperforms DreamerV2 by a large margin. (b) Ablation studies that can show the respective impact of optimizing the inverse dynamics (orange), rolling out noncontrollable states (green), and modeling the time-invariant information with a separate network branch (red).</p>
<p>Table 2: Video prediction results on BAIR and RoboNet datasets with bouncing balls. We use the first 2 frames as input to predict the next 28 frames on BAIR and the next 18 frames on RoboNet.</p>
<p>| MODEL | BAIR |  | ROBONET |  |
| | PSNR $\uparrow$ | SSIM $\uparrow$ | PSNR $\uparrow$ | SSIM $\uparrow$ |
| --- | --- | --- | --- | --- |
| SVG [10] | 18.12 | 0.712 | 19.86 | 0.708 |
| SA-ConvLSTM [35] | 18.28 | 0.677 | 19.30 | 0.638 |
| PHYDNET [19] | 18.91 | 0.743 | 20.89 | 0.727 |
| Iso-Dream | 19.51 | 0.768 | 21.71 | 0.769 |</p>
<p>we observe that rolling-out noncontrollable states in the action-free branch can significantly improve the agent's decision-making results. The red curve shows that the performance of Iso-Dream degrades by about 15% in the absence of a separate network branch that captures the static information.</p>
<h4>Qualitative results.</h4>
<p>Reconstruction results of predictions in CARLA environment are shown in Figure 3(right column). In CARLA, we observe that the agent actions potentially affect all pixel values in the observation, as the camera on the main car (i.e., the agent) moves. Therefore, we view the visual dynamics of other vehicles as a combination of controllable and noncontrollable states. Accordingly, our model can determine which component is dominant by learning attention masks (values between 0 and 1) across the action-conditioned and action-free branches. The "action-free masks" present hot spots around other vehicles, while the attention values in corresponding areas on the "action-cond masks" are still greater than 0. The agent can avoid collisions by rolling-out noncontrollable components to preview possible future states of other vehicles. We include more showcases with different numbers of vehicles in the supplementary materials.</p>
<h3>4.4 BAIR &amp; RoboNet for Action-Conditioned Video Prediction</h3>
<h4>Implementation details.</h4>
<p>In order to evaluate the effectiveness of our world model in a more complex environment, we test the video prediction ability of the proposed structure on BAIR and RoboNet dataset. Moreover, we add predictable visual dynamics unrelated to the control signals to the raw observations, i.e., bouncing balls of the same size and speed. In the training phase, we train the model to predict 10 frames into the future from 2 observations. For testing, we use the first 2 frames as input to predict the next 28 frames in the BAIR dataset, and the next 18 frames in the RoboNet dataset. All inputs for training and testing are resized to 64 × 64. Considering the simplicity and predictability of bouncing balls, in the action-free branch, we use a similar structure as in the DMC experiment. Moreover, we replace the GRU cell with two layers of ST-LSTM unit [52] in both branches. The optimization objective consists of image reconstruction loss and action reconstruction loss of Inverse Cell. SSIM and PSNR are adopted as evaluation metrics.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Showcases of video prediction results on the BAIR robot pushing dataset. We display every 3 frames in the prediction horizon. The generated masks show that each branch of Iso-Dream captures coarse localisation of controllable representations and noncontrollable representations.</p>
<p>Quantitative results. Table 2 gives the quantitative results on BAIR and RoboNet datasets with bouncing balls in the training and testing phase. Compared with other models, Iso-Dream shows the competitive performance in two datasets. For PSNR, Iso-Dream improves SVG by $7.7 \%$ in BAIR and $9.3 \%$ in RoboNet. Compared with PhyDNet, which also disentangles features in two branches, Iso-Dream achieves better performance in both PSNR and SSIM. It shows that our Iso-Dream has a stronger ability of disentanglement learning to achieve long-term prediction.</p>
<p>Qualitative results. We visualize a sequence of predicted frames on BAIR with bouncing balls in Figure 5. Specifically, the output of two branches and corresponding masks are provided. We can see from these demonstrations that the world model of Iso-Dream is more accurate in modeling future dynamics for long-term prediction. It shows the fact that the action-free branch learns noncontrollable dynamics, while the action-conditioned branch learns controllable dynamics related to input action.</p>
<h1>5 Conclusions</h1>
<p>In this paper, we proposed an MBRL framework named Iso-Dream, which mainly tackles the difficulty of vision-based prediction and control in the presence of complex visual dynamics. Our approach has two novel contributions to world model representation learning and corresponding MBRL algorithms. First, it learns to decouple controllable and noncontrollable latent state transitions via modular network structures and inverse dynamics. Further, it makes long-horizon decisions by rolling-out the noncontrollable dynamics into the future and learning their influences on current behavior. Iso-Dream achieves competitive results on the CARLA autonomous driving task, where other vehicles can be naturally viewed as noncontrollable components, indicating that with the help of decoupled latent states, the agent can make more forward-looking decisions by previewing possible future states in the action-free network branch. Besides, Iso-Dream was shown to effectively improve the visual control task in a modified DeepMind Control Suite, as well as the visual prediction task on the BAIR robot pushing dataset and the RoboNet dataset.</p>
<p>One limitation of Iso-Dream is the computational efficiency. Compared with DreamerV2, it requires longer training time per episode due to more intensive state transitions in behavior learning. But fortunately, from Figure 4(a), Iso-Dream is more sample-efficient than existing MBRL methods. Another limitation is the special treatment for different environments. In our preliminary experiments, we attempted to use the same model architecture for all test benchmarks. However, we observed that different benchmarks have specific requirements on the network structure, which we found should be dependent on our prior knowledge of the environments.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the Natural Science Foundation of China (U19B2035, 62106144), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and Shanghai Sailing Program (21Z510202133).</p>
<h1>References</h1>
<p>[1] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. In $I C L R, 2018$.
[2] Nadine Behrmann, Jurgen Gall, and Mehdi Noroozi. Unsupervised video representation learning by bidirectional feature prediction. In WACV, pages 1670-1679, 2021.
[3] Xinzhu Bei, Yanchao Yang, and Stefano Soatto. Learning semantic-aware dynamics for video prediction. In CVPR, pages 902-912, 2021.
[4] Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information prioritization through empowerment in visual model-based RL. In $I C L R, 2022$.
[5] Prateep Bhattacharjee and Sukhendu Das. Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. In NeurIPS, pages 4271-4280, 2017.
[6] Navaneeth Bodla, Gaurav Shrivastava, Rama Chellappa, and Abhinav Shrivastava. Hierarchical video prediction using relational layouts for human-object interactions. In CVPR, 2021.
[7] Lluis Castrejon, Nicolas Ballas, and Aaron Courville. Improved conditional VRNNs for video prediction. In ICCV, pages 7608-7617, 2019.
[8] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In $I C L R, 2017$.
[9] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019.
[10] Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In ICML, pages 1174-1183. PMLR, 2018.
[11] Alexey Dosovitskiy, Germán Ros, Felipe Codevilla, Antonio M. López, and Vladlen Koltun. CARLA: an open urban driving simulator. In CoRL, volume 78, pages 1-16. PMLR, 2017.
[12] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018.
[13] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. In CoRL, pages 344-356, 2017.
[14] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NeurIPS, pages 64-72, 2016.
[15] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In ICRA, pages 2786-2793. IEEE, 2017.
[16] Jean-Yves Franceschi, Edouard Delasalles, Mickaël Chen, Sylvain Lamprier, and Patrick Gallinari. Stochastic latent residual video prediction. In ICML, pages 3233-3246, 2020.
[17] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Schölkopf. Recurrent independent mechanisms. In ICLR, 2021.
[18] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. In ICML, pages 2424-2433, 2019.
[19] Vincent Le Guen and Nicolas Thome. Disentangling physical dynamics from unknown factors for unsupervised video prediction. In CVPR, pages 11474-11484, 2020.
[20] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In NeurIPS, 2018.
[21] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.
[22] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In ICLR, 2020.</p>
<p>[23] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In ICML, pages 2555-2565. PMLR, 2019.
[24] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.
[25] Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision transformers under data augmentation. In NeurIPS, 2021.
[26] Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation. In ICRA, 2021.
[27] Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei, and Juan Carlos Niebles. Learning to decompose and disentangle representations for video prediction. In NeurIPS, pages 517-526, 2018.
[28] Beibei Jin, Yu Hu, Qiankun Tang, Jingyu Niu, Zhiping Shi, Yinhe Han, and Xiaowei Li. Exploring spatial-temporal multi-frequency analysis for high-fidelity and temporal-consistency video prediction. In CVPR, pages 4554-4563, 2020.
[29] Minju Jung, Takazumi Matsumoto, and Jun Tani. Goal-directed behavior under variational predictive coding: Dynamic organization of visual attention and working memory. In IROS, pages 1040-1047. IEEE, 2019.
[30] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for Atari. In $I C L R, 2020$.
[31] Taesup Kim, Sungjin Ahn, and Yoshua Bengio. Variational temporal abstraction. In NeurIPS, volume 32, pages $11570-11579,2019$.
[32] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
[33] Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.
[34] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: contrastive unsupervised representations for reinforcement learning. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 5639-5650. PMLR, 2020.
[35] Zhihui Lin, Maomao Li, Zhuobin Zheng, Yangyang Cheng, and Chun Yuan. Self-attention convlstm for spatiotemporal prediction. In AAAI, volume 34, pages 11531-11538, 2020.
[36] Wenqian Liu, Abhishek Sharma, Octavia Camps, and Mario Sznaier. Dyan: A dynamical atoms-based network for video prediction. In ECCV, pages 170-185, 2018.
[37] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525-11538, 2020.
[38] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. arXiv preprint arXiv:1507.08750, 2015.
[39] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In NeurIPS, 2017.
[40] Marc Oliu, Javier Selva, and Sergio Escalera. Folded recurrent neural networks for future video prediction. In ECCV, pages 716-731, 2018.
[41] Fitsum A Reda, Guilin Liu, Kevin J Shih, Robert Kirby, Jon Barker, David Tarjan, Andrew Tao, and Bryan Catanzaro. Sdc-net: Video prediction using spatially-displaced convolution. In ECCV, pages 718-733, 2018.
[42] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In ICML, pages 8583-8592, 2020.
[43] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In NeurIPS, pages $802-810,2015$.</p>
<p>[44] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In ICML, pages 843-852. PMLR, 2015.
[45] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[46] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
[47] Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. In $I C L R, 2018$.
[48] Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V Le, and Honglak Lee. High fidelity video prediction with large stochastic recurrent neural networks. In NeurIPS, pages 81-91, 2019.
[49] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and content for natural video sequence prediction. In $I C L R, 2017$.
[50] Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long-term future via hierarchical prediction. In ICML, pages 3560-3569, 2017.
[51] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NeurIPS, pages 613-621, 2016.
[52] Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. In NeurIPS, pages 879-888, 2017.
[53] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, Philip S Yu, and Mingsheng Long. Predrnn: A recurrent neural network for spatiotemporal predictive learning. arXiv preprint arXiv:2103.09504, 2021.
[54] Nevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak Lee. Hierarchical long-term video prediction without supervision. In ICML, pages 6038-6046, 2018.
[55] Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, and Chelsea Finn. Greedy hierarchical variational autoencoders for large-scale video prediction. In CVPR, pages 2318-2328, 2021.
[56] Jingwei Xu, Bingbing Ni, Zefan Li, Shuo Cheng, and Xiaokang Yang. Structure preserving video prediction. In CVPR, pages 1460-1469, 2018.
[57] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. In AAAI, pages 10674-10681, 2021.
[58] Polina Zablotskaia, Edoardo A Dominici, Leonid Sigal, and Andreas M Lehrmann. Unsupervised video decomposition using spatio-temporal iterative inference. arXiv preprint arXiv:2006.14727, 2020.
[59] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In $I C L R, 2021$.</p>
<h1>A Benchmarks</h1>
<p>We quantitatively and qualitatively evaluate Iso-Dream on the following two environments for visual control and two real-world datasets for action-conditioned video prediction.</p>
<ul>
<li>DeepMind control suite [46]: A set of stable, well-tested continuous control tasks that are easy to use and modify. For vision-based control, we use a modified version of the DeepMind control suite in DMControl Generalization Benchmark [26] to evaluate Iso-Dream. In this environment, agents are trained to complete different tasks with random natural video as backgrounds, namely video_easy and video_hard benchmarks. We use 4 tasks to test our Iso-Dream, i.e., Finger Spin, Cheetah Run, Walker Walk, Hopper Stand.</li>
<li>CARLA [11]: An open-source simulator with more complex and realistic visual observations for autonomous driving research. In our experiments, we evaluate Iso-Dream in a first-person highway driving task in "Town04". The agent's goal is to drive as far as possible in 1000 time steps without colliding with the 30 other moving vehicles or barriers.</li>
<li>BAIR robot pushing [13]: An action-conditioned video prediction dataset composed of hours of self-supervised learning with the robotic arm Sawyer. In each video, a random moving robotic arm pushes a variety of objects on similar tables with a static background. Each video also has recorded actions taken by the robotic arm which correspond to the commanded gripper pose.</li>
<li>RoboNet [9]: A large-scale dataset contains action-conditioned videos of seven robotic arms interacting with a variety of objects from four different research laboratories, i.e., Berkeley, Google, Penn, and Stanford.</li>
</ul>
<h2>B Compared Methods</h2>
<p>For visual MBRL, we compare our method with the following baselines and existing approaches:</p>
<ul>
<li>DreamerV2 [24]: A model-based RL method that learns directly from latent variables in world models. The latent representation enables agents to imagine thousands of trajectories in parallel.</li>
<li>CURL [34]: A model-free RL method that extracts high-level features from raw pixels using contrastive learning, maximizing agreement between augmented versions of the same observation.</li>
<li>SVEA [25]: A framework for data augmentation in deep Q-learning algorithms that improves stability and generalization on off-policy RL.</li>
<li>SAC [21]: A model-free actor-critic method that optimizes a stochastic policy in an off-policy way.</li>
<li>DBC [59]: It learns a bisimulation metric representation without reconstruction loss, which are invariant to different task-irrelevant details in the observation.</li>
</ul>
<p>For video prediction, we compare the proposed world model with the following approaches:</p>
<ul>
<li>SVG [10]: This model introduces random variables into latent space, which ensures that the future trajectory is inherently random.</li>
<li>SA-ConvLSTM [35]: Based on the self-attention mechanism, this model uses the self-attention memory to capture long-term spatial dependency.</li>
<li>PhyDNet [19]: This model uses a two-branch architecture to disentangle PDE dynamics from unknown complementary information.</li>
</ul>
<h2>C Additional Visualization in DMC and CARLA</h2>
<p>DeepMind Control suite. In Figure 6, more showcases on the DeepMind Control are presented with different noisy backgrounds. We show the visualization of the masks and decoupled components from three branches of Iso-Dream.</p>
<p>CARLA autonomous driving simulator. In Figure 7, we visualize the video prediction results on the CARLA environment with different numbers of vehicles. We train Iso-Dream with 30 vehicles and test with 10 vehicles and 20 vehicles respectively.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Video prediction results with different noisy backgrounds on the DMC. For each sequence, we use the first 5 images as context frames.</p>
<h1>D Additional Results on the BAIR Robot Pushing Dataset</h1>
<p>Figure 8 shows an interesting result of the different training sets (i.e., BAIR, BAIR+bouncing balls) and the same testing set (i.e., BAIR). Iso-Dream is the only approach that achieves improvements when training on noisy data with bouncing balls, as shown in Figure 8(red bars). In this training setup, it performs best on the standard test set without balls. Iso-Dream is built on a more efficient architecture than the baseline models. It provides a general framework that can be easily extended to other backbones.</p>
<p>Ablation study. In Table 3, the first row shows the results of removing the action-free branch in the world model of Iso-Dream. The performance has decreased from 21.43 to 20.47 and from 19.51 to 18.51 in PSNR for predicting the next 18 frames and next 28 frames respectively, indicating that modular network structures are effective for predictive learning by decoupling the controllable and noncontrollable representations. Comparing the second row and third row in the Table 3, we observe that modeling inverse dynamics can improve the performance by learning more deterministic state transitions given particular actions in the action-conditioned branch.</p>
<h2>E Network Architectures for Different Environments</h2>
<p>The networks and hyper-parameters used for different environments are shown in Table 4.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Video prediction results with 10 vehicles (left) and 20 vehicles (right) on the CARLA environment. For each sequence, we use the first 5 images as context frames.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The results of models trained on BAIR (blue) and BAIR + bouncing balls (red), and tested on BAIR. We use the first 2 frames as input to predict the next 18 frames. The horizontal axis represents the different models, and the vertical axes represent test results of PSNR and SSIM.</p>
<p>Table 3: Ablation study for each component of Iso-Dream for video prediction on BAIR with bouncing balls. Lines 1-2 show the results of removing the action-free branch and Inverse cell, respectively. We use the first 2 frames as input to predict the next 18 frames and the next 28 frames.</p>
<p>| Model | Predict 18 frames | | Predict 28 frames | |
| | PSNR $\uparrow$ | SSIM $\uparrow$ | PSNR $\uparrow$ | SSIM $\uparrow$ |
| --- | --- | --- | --- | --- |
| Iso-Dream w/o action-free branch | 20.47 | 0.795 | 18.51 | 0.690 |
| Iso-Dream w/o Inverse Cell | 21.42 | 0.829 | 19.34 | 0.759 |
| Iso-Dream | 21.43 | 0.832 | 19.51 | 0.768 |</p>
<p>Table 4: An overview of layers and hyper-parameters used for three environments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">DMC</th>
<th style="text-align: center;">CARLA</th>
<th style="text-align: center;">BARI / RoboNet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$E n c_{\theta}$</td>
<td style="text-align: center;">conv3-32</td>
<td style="text-align: center;">conv3-32</td>
<td style="text-align: center;">conv3-64</td>
</tr>
<tr>
<td style="text-align: center;">Action-conditioned branch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$E n c_{\phi 1}$</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
</tr>
<tr>
<td style="text-align: center;">GRU $_{s}$</td>
<td style="text-align: center;">hidden size $=200$</td>
<td style="text-align: center;">hidden size $=200$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ST-LSTM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">hidden size $=64$</td>
</tr>
<tr>
<td style="text-align: center;">$D e c_{\varphi 1}$</td>
<td style="text-align: center;">conv3-4</td>
<td style="text-align: center;">conv3-4</td>
<td style="text-align: center;">conv3-4</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr>
<td style="text-align: center;">$\beta_{1}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Action-free branch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$E n c_{\phi 2}$</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
</tr>
<tr>
<td style="text-align: center;">GRU $_{s}$</td>
<td style="text-align: center;">hidden size $=200$</td>
<td style="text-align: center;">hidden size $=200$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ST-LSTM</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">hidden size $=64$</td>
</tr>
<tr>
<td style="text-align: center;">$D e c_{\varphi 2}$</td>
<td style="text-align: center;">conv3-4</td>
<td style="text-align: center;">conv3-4</td>
<td style="text-align: center;">conv3-4</td>
</tr>
<tr>
<td style="text-align: center;">$\beta_{2}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Static branch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$E n c_{\phi 3}$</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">conv3-64</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$D e c_{\varphi 3}$</td>
<td style="text-align: center;">conv3-3</td>
<td style="text-align: center;">conv3-3</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Details of the loss functions can be found in Eq. (5-6) in the paper of DreamerV2 [24] .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
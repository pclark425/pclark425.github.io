<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2821 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2821</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2821</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-71.html">extraction-schema-71</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-267412603</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.02053v2.pdf" target="_blank">Affordable Generative Agents</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2821.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2821.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework of LLM-driven simulated agents that maintain a long-term memory stream and a retrieval model to surface relevant past events for planning, reflection, and dialogue in an open virtual town environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (baseline framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven agents that perceive environment descriptions, generate plans and minute-level activities via an LLM, retrieve relevant memory events for planning and dialogue, reflect on past events, and act by emitting natural language descriptions (with associated sprites/emojis). The architecture comprises Perceive, Plan, Retrieve, Reflect, and Act modules; the LLM is invoked to produce plans, decompose plans into actions, generate reflections, and compose dialogue/responses.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>GPT-3.5-Turbo (used in the referenced baseline); also evaluated with GPT-4 in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term memory stream with retrieval (episodic/event memory, retrieval-augmented prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>A long-term memory stream stores past events and self-reflections. A retrieval model selects a subset of memory events for inclusion in prompts based on three scoring criteria: recency (recent events score higher), importance (events are scored for significance via an LLM's interpretation), and relevance (semantic similarity, e.g., cosine similarity). Retrieved chunks are appended to prompts for planning, reflection, or dialogue generation. Memory is maintained as textual event records; due to prompt length constraints, only a retrieved subset is used per LLM call.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified numerically; memory is an unbounded stream of events in principle but only a retrieved subset is used per prompt due to token limits (paper notes retrieval typically returns 30–45 events occupying ~2000 tokens in baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency + importance (LLM-scored) + relevance via cosine similarity (semantic similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Events are appended to the long-term memory stream after experiences; importance scores can be computed (via LLM) and influence retrieval ranking. The memory is continually updated as agents act and interact.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Generative Agents simulated town (open-ended social simulation: 3-person and 25-person town scenarios used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Open-ended multi-agent social simulation; agents generate daily schedules refined into hourly and minute-level activities; multi-turn natural-language interactions; emergent social behaviors and information diffusion are key evaluation targets rather than fixed-goal puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline (Generative Agents) metrics reported in this paper for the baseline: token consumption (GPT-3.5-Turbo) total 4.525M ± 0.364M tokens over two-game-day simulation in 3-person town; human-likeness (GPT-4 evaluator): activities 3.13 ± 0.19, dialogue 3.97 ± 0.02 (5-point Likert, higher = more human-like). Information diffusion counts (example): in a 2-day baseline simulation, 8 agents aware of Sam's candidacy and 13 aware of Isabella's party in the 25-person town (end-to-end evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>The long-term memory + retrieval model is central to agent coherence and believable behavior, enabling reflection and context-aware dialogue; however, because of token limits only a subset of events can be retrieved and included, and the strong consistency induced by this memory/retrieval setup constrains the diversity of emergent behaviors (agents eventually generate only a finite set of behaviors in fixed environments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Affordable Generative Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2821.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2821.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Affordable Generative Agents (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cost-efficient framework that maintains believable LLM agent behavior by replacing repetitive LLM inferences with cached policies (Lifestyle Policy) and compressing inter-agent memory with a Social Memory module (Summary Events + Relationship & Feeling), preserving performance while dramatically reducing token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Affordable Generative Agents (AGA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-agent framework built on top of Generative Agents: (1) Agent-environment interactions: when novel situations appear the LLM generates a plan which is decomposed into sub-plans and actions and an executable condition; successful plan→action graphs are stored in a Lifestyle Policy (case-based, cache of plans). On subsequent similar observations, the Lifestyle Policy retrieves matching plans (via embedding cosine similarity) and executes cached actions without invoking the LLM. (2) Inter-agent interactions: Social Memory compresses retrieved relevant events into Summary Events and maintains Relationship & Feeling descriptors per interlocutor; prompts combine compressed summaries, relationship keywords, and short dialogue history to generate responses. The pipeline uses embeddings for retrieval and GPT models for summarization and relationship updates.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td>GPT-3.5-Turbo primarily (same as Generative Agents baseline); also experiments reported with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented memory with compressed summaries (Summary Events), social metadata (Relationship & Feeling), and a case-based policy cache (Lifestyle Policy) that stores decomposed plan→action graphs and execution conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Two complementary memory systems: (A) Lifestyle Policy (case-based policy cache): when an LLM decomposes a novel plan into actions and execution conditions, the full plan→action→condition graph is stored along with an embedding of the plan description; at inference, the current plan embedding is compared (cosine similarity) to stored plan embeddings; if similarity > threshold (0.97) and the stored execution condition matches current environment item presence/state, the cached action sequence is executed without LLM calls. (B) Social Memory: retrieved relevant events are filtered and summarized into Summary Events via an LLM (higher information density, ~100 tokens vs ~2000 tokens of raw events); Relationship & Feeling records social relation keywords (e.g., 'Relationship: acquaintances, Feeling: friendly'), and is updated after dialogues by an LLM that ingests personal info, conversation summary, and prior relationship/feeling. The system uses text-embedding-ada-002 for embeddings and cosine similarity. Memory and social-state updates occur after interactions; summaries are incrementally compressed.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not given as a hard limit. Examples: baseline retrieval often returned 30–45 events (~2000 tokens); Social Memory compresses these to ~100 tokens. The paper notes concrete per-agent event counts in experiments (e.g., Isabella had ~740 events with 506 profile-related), but no fixed memory size limit for the system is specified.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Embedding-based semantic similarity (text-embedding-ada-002) with cosine similarity threshold (0.97) for Lifestyle Policy plan matching; for Social Memory, retrieve relevant events based on dialogue content, filter new events, summarize via LLM, and use summaries in prompts; retrieval selection also follows relevance/recency/importance heuristics for memory sampling (inherited from Generative Agents' retrieval approach).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Lifestyle Policy: add new plan→action→condition graphs when an LLM decomposes a plan successfully; Social Memory: after each interaction retrieve new relevant events, compress them into Summary Events, and run an LLM to update Relationship & Feeling based on personal info, conversation summary, and prior relationship state. Memory updates occur continuously after interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Generative Agents simulated town (3-person and 25-person) and VirtualHome (a 3D domestic simulator with programmatic actions) — both used as experimental environments in this paper; Generative Agents functions as a text-driven social simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Generative Agents town: multi-agent, open-ended social simulation with schedule generation and minute-level natural-language activities and multi-turn dialogues; VirtualHome: 3D domestic environment with concrete objects/actions (115 item types) where agents generate and decompose activities into programmatic action sequences; episodes span a 'full day' of activities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Generative Agents (3-person town, GPT-3.5-Turbo): AGA total tokens 1.420M ± 0.091M vs baseline 4.525M ± 0.364M (AGA = 31.1% of baseline token cost for full AGA). Table 2: AGA GPT-3.5-Turbo total tokens 1.420M ± 0.091M; GPT-4 AGA total 1.597M ± 0.142M. Human-likeness (GPT-4 evaluator): AGA activity score 3.21 ± 0.29, dialogue 4.01 ± 0.01 (comparable to baseline). VirtualHome: token consumption AGA 1,189 ± 313 tokens vs baseline 34,327 ± 4,210 tokens (AGA costs ~3.4% of baseline); task success rate (LLM-evaluated) AGA 85.0% ± 4.6% vs baseline 87.0% ± 3.7%; human-evaluated success AGA 53.3% ± 6.9% vs baseline 42.6% ± 4.9% (note: human evaluation and LLM evaluation differ). Information diffusion examples in 25-person town show comparable ranges to baseline (e.g., baseline: agents aware of Sam's candidacy = 8, Isabella's party = 13; AGA: Sam's candidacy awareness 4–12, Isabella's party 12–17 across runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation on modules: using only Lifestyle Policy: cost = 40.2% of original baseline; using only Social Memory: cost = 58.6% of original; full AGA (both) = 31.1% of baseline (3-person town, GPT-3.5-Turbo). VirtualHome baseline (no Lifestyle Policy) token = 34,327 ± 4,210; AGA with Lifestyle Policy token = 1,189 ± 313. Performance (task success) did not significantly degrade with AGA compared to baseline (LLM-evaluated success roughly equal; human-evaluated success sometimes higher for AGA in VirtualHome).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td>Ablation on token-consumption: Lifestyle Policy alone reduces token cost to 40.2% of baseline; Social Memory alone to 58.6%; combined (AGA) to 31.1% in 3-person town. This shows the Lifestyle Policy (policy reuse of decomposed LLM plans) contributes the largest token savings; Social Memory also yields substantial savings by compressing retrieved events.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>The paper compares raw-event retrieval (baseline: retrieving 30–45 events ≈ 2000 tokens) to Social Memory summarization (≈100 tokens); no direct quantitative comparison between other formal memory types (e.g., knowledge graphs) is presented. It also leverages case-based policy (Lifestyle Policy) vs LLM decomposition each time, showing policy reuse reduces cost without degrading behavior results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td>Compressing auxiliary memory into dense summaries (Summary Events) and maintaining explicit social metadata (Relationship & Feeling) preserves dialogical and social-coherence while drastically reducing tokens; caching decomposed plans (Lifestyle Policy) can replace repetitive LLM inferences because agents in fixed environments generate a finite set of believable behaviors — thus case-based reuse is effective; retrieval quality and summary information density are critical to keep performance while lowering prompt size; memory-driven diversification (mind wandering) can increase behavioral variety without large token cost increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Affordable Generative Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2821.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2821.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xu2023_werewolf</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploring large language models for communication games: An empirical study on werewolf (Y. Xu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that applies LLM-based agents to communication games (the werewolf social deduction game); cited as related work indicating LLMs can play communication/text-centric games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring large language models for communication games: An empirical study on werewolf</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM agents for communication games (werewolf)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned only in related work; no architecture details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Werewolf (communication/social deduction game) as referenced</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Multi-agent communication/social deduction gameplay requiring persuasion, deception, and group reasoning; text-heavy interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Affordable Generative Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2821.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2821.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory systems to play text games, including details about the memory architecture, the text games played, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Light2023_Avalon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>From text to tactic: Evaluating LLMs playing the game of Avalon (Light et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that evaluates LLM agents playing a social deduction/text strategy game (Avalon); cited as prior work deploying LLM agents in text game environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From text to tactic: Evaluating llms playing the game of avalon</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM agents for Avalon</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Only mentioned in related work; no implementation or memory details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_benchmark</strong></td>
                            <td>Avalon (social deduction / strategy text game)</td>
                        </tr>
                        <tr>
                            <td><strong>game_characteristics</strong></td>
                            <td>Multi-agent deception and coordination, requiring strategic planning and communication; text-driven turns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Affordable Generative Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>VirtualHome: Simulating household activities via programs <em>(Rating: 2)</em></li>
                <li>Exploring large language models for communication games: An empirical study on werewolf <em>(Rating: 2)</em></li>
                <li>From text to tactic: Evaluating llms playing the game of avalon <em>(Rating: 2)</em></li>
                <li>Memory sandbox: Transparent and interactive memory management for conversational agents <em>(Rating: 1)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2821",
    "paper_id": "paper-267412603",
    "extraction_schema_id": "extraction-schema-71",
    "extracted_data": [
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents (Park et al., 2023)",
            "brief_description": "A framework of LLM-driven simulated agents that maintain a long-term memory stream and a retrieval model to surface relevant past events for planning, reflection, and dialogue in an open virtual town environment.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (baseline framework)",
            "agent_description": "LLM-driven agents that perceive environment descriptions, generate plans and minute-level activities via an LLM, retrieve relevant memory events for planning and dialogue, reflect on past events, and act by emitting natural language descriptions (with associated sprites/emojis). The architecture comprises Perceive, Plan, Retrieve, Reflect, and Act modules; the LLM is invoked to produce plans, decompose plans into actions, generate reflections, and compose dialogue/responses.",
            "base_llm": "GPT-3.5-Turbo (used in the referenced baseline); also evaluated with GPT-4 in comparisons",
            "uses_memory": true,
            "memory_type": "long-term memory stream with retrieval (episodic/event memory, retrieval-augmented prompting)",
            "memory_architecture": "A long-term memory stream stores past events and self-reflections. A retrieval model selects a subset of memory events for inclusion in prompts based on three scoring criteria: recency (recent events score higher), importance (events are scored for significance via an LLM's interpretation), and relevance (semantic similarity, e.g., cosine similarity). Retrieved chunks are appended to prompts for planning, reflection, or dialogue generation. Memory is maintained as textual event records; due to prompt length constraints, only a retrieved subset is used per LLM call.",
            "memory_capacity": "Not specified numerically; memory is an unbounded stream of events in principle but only a retrieved subset is used per prompt due to token limits (paper notes retrieval typically returns 30–45 events occupying ~2000 tokens in baseline).",
            "memory_retrieval_method": "Recency + importance (LLM-scored) + relevance via cosine similarity (semantic similarity).",
            "memory_update_strategy": "Events are appended to the long-term memory stream after experiences; importance scores can be computed (via LLM) and influence retrieval ranking. The memory is continually updated as agents act and interact.",
            "text_game_benchmark": "Generative Agents simulated town (open-ended social simulation: 3-person and 25-person town scenarios used in experiments)",
            "game_characteristics": "Open-ended multi-agent social simulation; agents generate daily schedules refined into hourly and minute-level activities; multi-turn natural-language interactions; emergent social behaviors and information diffusion are key evaluation targets rather than fixed-goal puzzles.",
            "performance_with_memory": "Baseline (Generative Agents) metrics reported in this paper for the baseline: token consumption (GPT-3.5-Turbo) total 4.525M ± 0.364M tokens over two-game-day simulation in 3-person town; human-likeness (GPT-4 evaluator): activities 3.13 ± 0.19, dialogue 3.97 ± 0.02 (5-point Likert, higher = more human-like). Information diffusion counts (example): in a 2-day baseline simulation, 8 agents aware of Sam's candidacy and 13 aware of Isabella's party in the 25-person town (end-to-end evaluation).",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": "The long-term memory + retrieval model is central to agent coherence and believable behavior, enabling reflection and context-aware dialogue; however, because of token limits only a subset of events can be retrieved and included, and the strong consistency induced by this memory/retrieval setup constrains the diversity of emergent behaviors (agents eventually generate only a finite set of behaviors in fixed environments).",
            "uuid": "e2821.0",
            "source_info": {
                "paper_title": "Affordable Generative Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "AGA",
            "name_full": "Affordable Generative Agents (this paper)",
            "brief_description": "A cost-efficient framework that maintains believable LLM agent behavior by replacing repetitive LLM inferences with cached policies (Lifestyle Policy) and compressing inter-agent memory with a Social Memory module (Summary Events + Relationship & Feeling), preserving performance while dramatically reducing token usage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Affordable Generative Agents (AGA)",
            "agent_description": "An LLM-agent framework built on top of Generative Agents: (1) Agent-environment interactions: when novel situations appear the LLM generates a plan which is decomposed into sub-plans and actions and an executable condition; successful plan→action graphs are stored in a Lifestyle Policy (case-based, cache of plans). On subsequent similar observations, the Lifestyle Policy retrieves matching plans (via embedding cosine similarity) and executes cached actions without invoking the LLM. (2) Inter-agent interactions: Social Memory compresses retrieved relevant events into Summary Events and maintains Relationship & Feeling descriptors per interlocutor; prompts combine compressed summaries, relationship keywords, and short dialogue history to generate responses. The pipeline uses embeddings for retrieval and GPT models for summarization and relationship updates.",
            "base_llm": "GPT-3.5-Turbo primarily (same as Generative Agents baseline); also experiments reported with GPT-4.",
            "uses_memory": true,
            "memory_type": "Retrieval-augmented memory with compressed summaries (Summary Events), social metadata (Relationship & Feeling), and a case-based policy cache (Lifestyle Policy) that stores decomposed plan→action graphs and execution conditions.",
            "memory_architecture": "Two complementary memory systems: (A) Lifestyle Policy (case-based policy cache): when an LLM decomposes a novel plan into actions and execution conditions, the full plan→action→condition graph is stored along with an embedding of the plan description; at inference, the current plan embedding is compared (cosine similarity) to stored plan embeddings; if similarity &gt; threshold (0.97) and the stored execution condition matches current environment item presence/state, the cached action sequence is executed without LLM calls. (B) Social Memory: retrieved relevant events are filtered and summarized into Summary Events via an LLM (higher information density, ~100 tokens vs ~2000 tokens of raw events); Relationship & Feeling records social relation keywords (e.g., 'Relationship: acquaintances, Feeling: friendly'), and is updated after dialogues by an LLM that ingests personal info, conversation summary, and prior relationship/feeling. The system uses text-embedding-ada-002 for embeddings and cosine similarity. Memory and social-state updates occur after interactions; summaries are incrementally compressed.",
            "memory_capacity": "Not given as a hard limit. Examples: baseline retrieval often returned 30–45 events (~2000 tokens); Social Memory compresses these to ~100 tokens. The paper notes concrete per-agent event counts in experiments (e.g., Isabella had ~740 events with 506 profile-related), but no fixed memory size limit for the system is specified.",
            "memory_retrieval_method": "Embedding-based semantic similarity (text-embedding-ada-002) with cosine similarity threshold (0.97) for Lifestyle Policy plan matching; for Social Memory, retrieve relevant events based on dialogue content, filter new events, summarize via LLM, and use summaries in prompts; retrieval selection also follows relevance/recency/importance heuristics for memory sampling (inherited from Generative Agents' retrieval approach).",
            "memory_update_strategy": "Lifestyle Policy: add new plan→action→condition graphs when an LLM decomposes a plan successfully; Social Memory: after each interaction retrieve new relevant events, compress them into Summary Events, and run an LLM to update Relationship & Feeling based on personal info, conversation summary, and prior relationship state. Memory updates occur continuously after interactions.",
            "text_game_benchmark": "Generative Agents simulated town (3-person and 25-person) and VirtualHome (a 3D domestic simulator with programmatic actions) — both used as experimental environments in this paper; Generative Agents functions as a text-driven social simulation.",
            "game_characteristics": "Generative Agents town: multi-agent, open-ended social simulation with schedule generation and minute-level natural-language activities and multi-turn dialogues; VirtualHome: 3D domestic environment with concrete objects/actions (115 item types) where agents generate and decompose activities into programmatic action sequences; episodes span a 'full day' of activities.",
            "performance_with_memory": "Generative Agents (3-person town, GPT-3.5-Turbo): AGA total tokens 1.420M ± 0.091M vs baseline 4.525M ± 0.364M (AGA = 31.1% of baseline token cost for full AGA). Table 2: AGA GPT-3.5-Turbo total tokens 1.420M ± 0.091M; GPT-4 AGA total 1.597M ± 0.142M. Human-likeness (GPT-4 evaluator): AGA activity score 3.21 ± 0.29, dialogue 4.01 ± 0.01 (comparable to baseline). VirtualHome: token consumption AGA 1,189 ± 313 tokens vs baseline 34,327 ± 4,210 tokens (AGA costs ~3.4% of baseline); task success rate (LLM-evaluated) AGA 85.0% ± 4.6% vs baseline 87.0% ± 3.7%; human-evaluated success AGA 53.3% ± 6.9% vs baseline 42.6% ± 4.9% (note: human evaluation and LLM evaluation differ). Information diffusion examples in 25-person town show comparable ranges to baseline (e.g., baseline: agents aware of Sam's candidacy = 8, Isabella's party = 13; AGA: Sam's candidacy awareness 4–12, Isabella's party 12–17 across runs).",
            "performance_without_memory": "Ablation on modules: using only Lifestyle Policy: cost = 40.2% of original baseline; using only Social Memory: cost = 58.6% of original; full AGA (both) = 31.1% of baseline (3-person town, GPT-3.5-Turbo). VirtualHome baseline (no Lifestyle Policy) token = 34,327 ± 4,210; AGA with Lifestyle Policy token = 1,189 ± 313. Performance (task success) did not significantly degrade with AGA compared to baseline (LLM-evaluated success roughly equal; human-evaluated success sometimes higher for AGA in VirtualHome).",
            "has_ablation_study": true,
            "memory_ablation_results": "Ablation on token-consumption: Lifestyle Policy alone reduces token cost to 40.2% of baseline; Social Memory alone to 58.6%; combined (AGA) to 31.1% in 3-person town. This shows the Lifestyle Policy (policy reuse of decomposed LLM plans) contributes the largest token savings; Social Memory also yields substantial savings by compressing retrieved events.",
            "comparison_with_other_memory_types": "The paper compares raw-event retrieval (baseline: retrieving 30–45 events ≈ 2000 tokens) to Social Memory summarization (≈100 tokens); no direct quantitative comparison between other formal memory types (e.g., knowledge graphs) is presented. It also leverages case-based policy (Lifestyle Policy) vs LLM decomposition each time, showing policy reuse reduces cost without degrading behavior results.",
            "key_findings_about_memory_effectiveness": "Compressing auxiliary memory into dense summaries (Summary Events) and maintaining explicit social metadata (Relationship & Feeling) preserves dialogical and social-coherence while drastically reducing tokens; caching decomposed plans (Lifestyle Policy) can replace repetitive LLM inferences because agents in fixed environments generate a finite set of believable behaviors — thus case-based reuse is effective; retrieval quality and summary information density are critical to keep performance while lowering prompt size; memory-driven diversification (mind wandering) can increase behavioral variety without large token cost increases.",
            "uuid": "e2821.1",
            "source_info": {
                "paper_title": "Affordable Generative Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Xu2023_werewolf",
            "name_full": "Exploring large language models for communication games: An empirical study on werewolf (Y. Xu et al., 2023)",
            "brief_description": "A referenced work that applies LLM-based agents to communication games (the werewolf social deduction game); cited as related work indicating LLMs can play communication/text-centric games.",
            "citation_title": "Exploring large language models for communication games: An empirical study on werewolf",
            "mention_or_use": "mention",
            "agent_name": "LLM agents for communication games (werewolf)",
            "agent_description": "Mentioned only in related work; no architecture details provided in this paper.",
            "base_llm": null,
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "Werewolf (communication/social deduction game) as referenced",
            "game_characteristics": "Multi-agent communication/social deduction gameplay requiring persuasion, deception, and group reasoning; text-heavy interactions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": null,
            "uuid": "e2821.2",
            "source_info": {
                "paper_title": "Affordable Generative Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Light2023_Avalon",
            "name_full": "From text to tactic: Evaluating LLMs playing the game of Avalon (Light et al., 2023)",
            "brief_description": "A referenced study that evaluates LLM agents playing a social deduction/text strategy game (Avalon); cited as prior work deploying LLM agents in text game environments.",
            "citation_title": "From text to tactic: Evaluating llms playing the game of avalon",
            "mention_or_use": "mention",
            "agent_name": "LLM agents for Avalon",
            "agent_description": "Only mentioned in related work; no implementation or memory details provided in this paper.",
            "base_llm": null,
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_capacity": null,
            "memory_retrieval_method": null,
            "memory_update_strategy": null,
            "text_game_benchmark": "Avalon (social deduction / strategy text game)",
            "game_characteristics": "Multi-agent deception and coordination, requiring strategic planning and communication; text-driven turns.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_ablation_results": null,
            "comparison_with_other_memory_types": null,
            "key_findings_about_memory_effectiveness": null,
            "uuid": "e2821.3",
            "source_info": {
                "paper_title": "Affordable Generative Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "VirtualHome: Simulating household activities via programs",
            "rating": 2,
            "sanitized_title": "virtualhome_simulating_household_activities_via_programs"
        },
        {
            "paper_title": "Exploring large language models for communication games: An empirical study on werewolf",
            "rating": 2,
            "sanitized_title": "exploring_large_language_models_for_communication_games_an_empirical_study_on_werewolf"
        },
        {
            "paper_title": "From text to tactic: Evaluating llms playing the game of avalon",
            "rating": 2,
            "sanitized_title": "from_text_to_tactic_evaluating_llms_playing_the_game_of_avalon"
        },
        {
            "paper_title": "Memory sandbox: Transparent and interactive memory management for conversational agents",
            "rating": 1,
            "sanitized_title": "memory_sandbox_transparent_and_interactive_memory_management_for_conversational_agents"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 1,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        }
    ],
    "cost": 0.0165715,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Affordable Generative Agents
28 Aug 2024</p>
<p>Yangbin Yu yangbinyu@tencent.com 
Qin Zhang 
Junyou Li junyouli@tencent.com 
Qiang Fu 
Deheng Ye 
Affordable Generative Agents
28 Aug 2024C2BDFD5A7ED0F6D99997DB2F857D2362arXiv:2402.02053v2[cs.AI]
The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents.However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents.Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels.Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information.Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework.Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors.Our code is publicly available at: https: //github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.</p>
<p>Introduction</p>
<p>Intelligent agents achieving human-level performance in complex tasks has attracted research efforts for years Mnih et al. (2015); Ye et al. (2020a;b); Zhao et al. (2023).Recently, constructing an agent based on large language models (LLMs) for the simulation of believable human behavior has shown substantial promise (da Rocha Costa, 2019;Park et al., 2022;Lan et al., 2023;Xi et al., 2023a).A believable LLM agent is characterized by its ability to interact with other agents or humans, respond to environmental changes, and generate responses and behaviors that are perceived by humans as authentic, natural, and in alignment with expectations.Such agents could be utilized for diverse applications, such as simulating the potential impacts of policies (Zhang et al., 2023a;Törnberg et al., 2023) or sociological theories on humans (Gao et al., 2023), creating Non-Player Characters (NPCs) (Laird &amp; VanLent, 2001;Gong et al., 2023) in games, and</p>
<p>LLM Agents</p>
<p>The development of LLM agents has sparked a myriad of applications.Yang et al. (2023a) leverage multiturn dialogues with agents to autonomously address NLP tasks.Li et al. (2024) find that the performance of LLMs scales with the number of agents instantiated across a range of tasks.Deng et al. (2023), Nakano et al. (2021), and Yao et al. (2022a) integrate pre-defined APIs with LLMs to enable agents to navigate web pages effectively.Xu et al. (2023) and Light et al. (2023) deploy LLM agents in text game environments, showing the capacity to engage in complex, strategic gameplay.The most prominent among these works is the construction of LLM agents that interact in open environments to create believable behavior.For instance, Park et al. (2023) builds a virtual town simulating the social life of 25 agents, while Brohan et al. (2023) utilizes an LLM to create a robot capable of completing household chores through natural language interaction.Wang et al. (2023a) implements an agent capable of playing Minecraft expertly, when compared to traditional game-playing methods Kanervisto et al. (2022); Lin et al. (2022).Efforts have been made to optimize various modules such as reasoning (Wei et al., 2022;Wang et al., 2022;Xi et al., 2023b), planning (Song et al., 2023;Yao et al., 2022b), reflection (Shinn et al., 2023), and memory (Zhong et al., 2023;Huang et al., 2023b), or to create more realistic interaction environments (Hong et al., 2023b;Huang et al., 2023a).However, the development of affordable and believable agent interactions is missing in the literature.In this paper, we focus on achieving believable agents with the same performance at low cost.</p>
<p>The Cost of LLM Interaction</p>
<p>The cost of using LLM is directly proportional to the number of LLM invocations, the length of the prompt, and the model's performance.The cost issue has always been a key focus in the LLM-related field.Numerous methods have been proposed to reduce the cost of using LLM, such as Cascaded Invocation of LLM (Yue et al., 2023;Chen et al., 2023a;Nottingham et al., 2023), summary (Laban et al., 2023;Arefeen et al., 2023;Mu et al., 2023), and batch (Lin et al., 2023).These methods primarily optimize the cost of single invocation of LLM for QA tasks.Zhang et al. (2023b) has built a cost-efficient multi-agent framework, but it only solves QA tasks on fixed datasets.The simulation of believable behavior in the open world requires uninterrupted calls to the LLM for interaction.Moreover, multiple modules need to combine a large amount of contextual information for LLM reasoning and planning, making cost a more prominent issue.Simply optimizing for single QA invocations cannot effectively reduce costs.Furthermore, the open world does not have a fixed evaluation method, and the difficulty of tasks is hard to categorize, rendering many existing methods unsuitable (Chen et al., 2023a;Zhang et al., 2023b).Kaiya et al. (2023) proposes generative agents with low-cost interactions, but their effectiveness is highly dependent on the tailored scenarios.Hence, there is a pressing need for a low-cost solution that can effectively handle the challenges of believable interaction simulation in various open environments using LLMs.This paper aims to address this gap by proposing a cost-efficient scheme for simulating believable behavior with LLMs in open environments.</p>
<p>Method</p>
<p>Our work is inspired by Park et al. (2023), who proposed Generative Agents, a framework for simulating believable behaviors of LLM agents in a virtual town.This framework enables agents to perceive their environment and maintain a memory stream to record past interactions and self-reflections.By retrieving relevant content from this memory stream, the framework helps agents generate believable responses.Inspired by this, We achieve generative agents in a low-cost way by optimizing agent-environment interactions and inter-agent interactions.Figure 1 illustrates our method in these two types of interactions, with gray arrows representing the Generative Agents framework's implementation and blue arrows representing our approach.In the following sub-sections, we will describe the optimization strategies for these two categories of interactions in detail.</p>
<p>Agent-environment interactions</p>
<p>The interaction between an agent and its environment can be modeled as a policy function mapping observations to actions.This policy can be implemented through reinforcement learning (Kaelbling et al., 1996)  or rule-based finite state machines (Lee &amp; Yannakakis, 1996), behavior trees (Colledanchise &amp; Ögren, 2018), etc. LLMs trained on a vast amount of data show immense potential in policy formulation (Sun et al., 2023).With carefully designed prompts, LLMs can generate logically consistent and believable decisions.The decision-making capabilities of LLMs have been applied to a wide variety of applications, but the cost associated with each interaction has been overlooked.On the other hand, behavior trees operate without additional costs, but they require pre-written scripts and can not adapt dynamically to environment changes.</p>
<p>Drawing inspiration from the Case-Based Reasoning (CBR) method (Slade, 1991;Spalzzi, 2001), which addresses new problems by adapting solutions to previously solved, similar issues, we introduce a new approach -Lifestyle Policy, which aims to retain the flexible interaction capabilities of the LLMs while achieving low-cost deployment.The functionality of Lifestyle Policy is to minimize costs by reusing the similar inference processes of an LLM.To this end, we devise agent-environment interactions containing two stages: Plan Decomposition and Policy Reuse, expanded as follows.</p>
<p>Plan Decomposition</p>
<p>The stage occurs when the agent encounters situations it has never seen before.Due to their inherent zero-shot capabilities (Kojima et al., 2022), LLM agents can respond to unseen tasks in a believable way.Given the description of the environment and agent's information, a rough plan or task is generated by an LLM.The Plan Decomposition breaks the plan down into sub-plans and then converts sub-plans into specific actions, just like other LLM multi-agent frameworks (Chen et al., 2023b;Li et al., 2023;Hong et al., 2023a).An additional step involves generating an executable condition based on the action sequence.The determination of execution condition is highly correlated with the environment.For instance, in the case of a domestic robot (Brohan et al., 2023), the condition of a task include the existence of the related interactive items and the consistency of their states.If the plan can be successfully executed, then the whole graph from plan to actions and corresponding condition will be updated to Lifestyle Policy.</p>
<p>Policy Reuse</p>
<p>The stage occurs when the agent encounters previously seen situations.Given a plan, we use embedding models to extract features from plan descriptions and compare them with stored plan features in the Lifestyle Policy using cosine similarity.When the cosine similarity exceeds a set threshold, we consider it a match for the same plan.Setting the threshold too low may result in mismatched plans, causing agents to perform inappropriate actions, while setting it too high may lead to redundant plans being recorded in the policy.In our implementation, we set the threshold at 0.97.More details and examples of different similarity levels are provided in the Appendix A. Subsequently, we assess whether the current environment meets the execution condition associated with the retrieved plan by comparing if the interactive items in the current environment exist and have consistent states as specified in the condition.If the condition is met, we proceed with the actions specified in that plan.It is important to note that a single plan may have multiple conditions tailored for different scenarios.We iterate through all the conditions until we find one that the current environment satisfies.</p>
<p>Figure 1(a) illustrates the complete interaction process.Upon receiving an observation that includes environmental data and the agent's personal profile, the LLM agent formulates a plan.For the baseline, all plans will invoke the LLM to decompose and generate corresponding actions.In our method, the Lifestyle Policy retrieves a plan with a similar description and condition met by current environment.If a match is found, the agent executes the corresponding actions.Otherwise, the LLM is invoked to decompose the plan.Due to the superiority of LLMs, the plan decomposition ensures that the actions generated are of high quality, believable, and self-consistent.Meanwhile, the Lifestyle Policy aims to reuse existing plans to avoid unnecessary costs.Due to the high similarity threshold, the replacement policy does not affect the agent's original performance.For instance, the agent Klaus in virtual town generates an activity like "Klaus is waking up and completing his morning routine."In the Lifestyle Policy, this activity matches a similar one, "Klaus is waking up and getting ready for the day," and executes the corresponding sub-activities such as "stretching and getting out of bed," "washing his face and brushing his teeth," and "heading out the door to go to the library."</p>
<p>In the lifelong simulation of LLM agents, there are a large number of redundant, repetitive, and trivial decisions, such as in Generative Agents, where each agent repeatedly makes decisions like brushing its teeth, washing its face, etc. Replacing the process of invoking LLM reasoning with corresponding policies can save significant cost, which will be shown in our experiments.</p>
<p>Inter-agent interactions</p>
<p>In agent interactions, dialogue is the primary communication method.These agents must integrate substantial auxiliary information into the dialogue prompt to demonstrate advanced cognitive capabilities, distinguishing them from basic chatbots driven by LLMs.For instance, agents need to incorporate relevant information about the dialogue participants and topic-related events retrieved from memory into the dialogue prompt to enrich the conversation.However, this data is often less comprehensive and more fragmented than the knowledge bases of question-answering systems (Cui et al., 2023).Merely appending memory snippets to prompts could reduce response quality and lead to higher processing costs from increased prompt length (Krishna et al., 2022).Furthermore, agent interactions differ depending on their relationships and mutual perceptions in social simulations.Thus, agents need to gather enough information to form contextually appropriate responses without suffering performance drops or incurring extra costs from excessively long prompts.</p>
<p>The gray arrows in Figure 1(b) illustrate the inter-agent interactions provided by the baseline method.Specifically, agents retrieve all relevant events from the memory stream based on the previous round of conversation.LLMs then use these relevant events along with the prior conversation as a prompt to generate the next round of dialogue.However, this approach has several issues.First, trivial and repetitive relevant events can make the prompt excessively long, sometimes exceeding the LLMs' token limit.This not only increases computational costs, especially in multi-agent, multi-turn dialogue scenarios, but also makes it difficult for the LLMs to capture key information, leading to inappropriate responses.</p>
<p>Drawing inspiration from existing text compression technologies (Liu et al., 2023) and social model (McCoy et al., 2011), we introduce the Social Memory module, aimed at reducing the cost of inter-agent interactions and enabling the agents to produce more appropriate dialogues.It consists of two key components: Summary Events and Relationship &amp; Feeling.Summary Events store summaries of relevant events for each interlocutor, while Relationship &amp; Feeling models the social connections between the agent and the interlocutor using keywords such as "Relationship: acquaintances, Feeling: friendly."In our implementation, we first retrieve relevant events based on previous dialogue content, filter out new events, and summarize them into Summary Events.These summaries increase information density while controlling text length.We then combine the agent's personal information, event summaries, relationships, feelings, and previous dialogues into a prompt for LLMs to generate the next round of dialogue.After the dialogue ends, we input the current agent's relationship, feelings and the summary of multiple rounds of dialogue into the LLM to update the Relationship &amp; Feeling, as shown in Figure 2. Implementation details of each module of Social Memory are provided in the Appendix B.</p>
<p>Social Memory reduces costs of conversation by compressing auxiliary information, offering precise descriptions of social information to assist agents in responding consistently and appropriately.In the baseline, each conversation requires retrieving 30 to 45 relevant events, which occupy about 2000 tokens in the prompt.In contrast, our method condenses this information to approximately 100 tokens by using Summary Events.It also supplies high-information events to aid agents in generating accurate replies relevant to the conversation.Furthermore, as illustrated in Figure 2, Social Memory updates after each interaction, providing an explicit depiction of interlocutor relationships.For example, we observe that the student Klaus enjoys discussing his studies with others and occasionally mistakes the cafe owner Isabella for a fellow student in baseline.However, with our method, which employs a clear relationship update mechanism, the agent can discern social relationships during conversations.Meanwhile, This mechanism can be employed to investigate the dynamics of relationships between agents, thereby analyzing the evolution of community relations and the emergence of social behaviors.</p>
<p>Experimental Setup</p>
<p>This section focuses on the experimental environment and evaluation protocol.</p>
<p>Environment</p>
<p>We evaluate our approach under the following environments:</p>
<p>• Generative Agents.Park et al. (2023) creates a simulated town, populating it with multiple agents powered by LLMs that interact in natural language, generating believable individual and emergent social behaviors.Agents are required to create daily schedules, which are refined into hourly plans, and then translated into minute-level activities.All interactions are executed through natural language descriptions, accompanied by corresponding sprites and emojis for representation.</p>
<p>The Generative Agents framework comprises five modules: Perceive, Plan, Retrieve, Reflect, and Act.The Perceive module is responsible for sensing surrounding game objects.The Plan module generates plans and activities.Retrieve is tasked with retrieving relevant events from memory.Reflect deeply considers past events, and Act determines the location and objects of interaction, as well as generating corresponding emojis.</p>
<p>We implement AGA on Generative Agents framework and conduct experiments in the provided 3-person and 25-person environments.</p>
<p>• VirtualHome.Puig et al. (2018) provides a 3D simulated domestic setting where agents engage with their surroundings via programmatic atomic actions.The environment encompasses 115 specific types of items, such as plate, desk, and microwave.Each item possesses attributes that determine the permitted interactions, including GRABBABLE, LIEABLE, etc., as well as current states like OPEN and CLOSED.Compared to Generative Agents, VirtualHome offers more concrete and realistic interaction.An interaction involves specific actions, items and the item states.</p>
<p>We implemented an LLM framework on VirtualHome and applied AGA to it.The detailed implementation is presented in the Appendix E.</p>
<p>Language Models Adopted</p>
<p>For a fair comparison, all agents are empowered with GPT-3.5-Turbo(Wu et al., 2023), same as the Generative Agents work (Park et al., 2023).Besides, we also conduct tests using the more advanced GPT-4 (Achiam et al., 2023).</p>
<p>Evaluation Protocol</p>
<p>We focus on LLM cost and performance.For LLM cost, to avoid regional and time-based price differences, we use the same model to compare the number of tokens consumed by different methods for the same task.To comprehensively evaluate the agent's behavior, we first reuse the evaluation methods of Generative Agents excluding manual assessments.These methods encompass interviewing agents with manually designed questions and conducting case study-specific activities.</p>
<p>Moreover, we attempt to evaluate the emergent social behaviors of generative agents from two other perspectives</p>
<p>• Relationship Evolution.The relationships among agents are updated by Social Memory after interactions.The evolution of these relationships signifies the formation of community ties, which can be utilized to examine the social behaviors.</p>
<p>• Behavioral Assessment.Chiang &amp; Lee (2023) have validated LLM as an evaluator, demonstrating comparable performance to expert evaluators.Consequently, we employ GPT-4 to evaluate the activities and dialogues generated by LLM agents.A 5-point Likert scale questionnaire is deployed to differentiate whether these activities and dialogues are from human or AI-generated.</p>
<p>For VirtualHome, agents are designed to experience a full day at home, generating household tasks and decomposing them into specific actions.We evaluate the success rate of plan execution.</p>
<p>Evaluations</p>
<p>We conduct extensive experiments to validate our method, assessing both the token consumption and the performance.Our experiments demonstrate that the AGA framework can reduce the cost while maintaining equivalent performance.</p>
<p>Results on Generative Agents</p>
<p>Token Consumption Figure 3 illustrates the ablation study of token consumption based on multiple two game day simulations of 3-person town with GPT-3.5-Turbo.Compared to the baseline, the cost of using only the Lifestyle Policy is 40.2% of the original, while using only the Social Relationship Memory is 58.6%, and the full AGA framework is 31.1%.It should be noted that the advantage of the Lifestyle Policy lies in the reuse of existing LLM-generated strategies.Hence, both Lifestyle Policy and AGA conduct experimental comparisons after running multiple iterations, and then storing a certain number of policies.We also present the results of different configuration simulations in Table 1.where the cost for a 25-person town is 42.7% of the original.In the 25-person town, the probability of interactions between agents increases with the rise in population density.After triggering a dialogue, an agent adjusts its plan, invoking the reactions module to handle unexpected events.The occurrence of more events leads to more frequent output of reflections for deeper thinking.In order to ensure the flexibility of the LLM, we did not modify the reactions and reflections modules of the Generative Agents, which incurs additional costs for the 25-person version.</p>
<p>Table 2 provides a detailed comparison across various models and token types in a 3-person town setting.</p>
<p>The data suggests that GPT-4 tends to consume slightly more tokens due to its tendency to generate more detailed plans and longer responses.Importantly, the majority of token consumption is attributed to input tokens, a reflection of the need to incorporate substantial relevant information, constraints, and one-shot examples into the prompt.Figure 4 presents the accumulated token consumption over a two-game-day simulation in a 3-person town using GPT3.5-Turbo.The AGA demonstrates slower token consumption compared to the Baseline.</p>
<p>Case Study</p>
<p>We employ the same method used for Generative Agents to evaluate believable agents in this part.We conduct interviews with agents to evaluate their ability to remember past events, strategize for future tasks based on those memories, react appropriately to unexpected situations, and reflect on their previous actions to improve their future performance.We use the same questions as Generative Agents to inquire about five aspects: self-knowledge, memory, plans, reactions and reflections.In addition, we can validate the emergence of social behavior through specific events.In the 25-person town, Isabella Rodriguez, the owner of the coffee shop, plans to host a Valentine's Day party and invite as many people as possible the following day, while Sam is running for mayor.In the end-to-end evaluation of the baseline, during the two-day simulation, the number of agents aware of Sam's mayoral candidacy is 8, and the number of agents who know about Isabella's party is 13.In our method, after multiple experiments, we observe corresponding numbers ranging from 4 to 12 for Sam's candidacy and 12 to 17 for Isabella's party.AGA achieves similar results to the baseline in terms of information diffusion.</p>
<p>Relationship Evolution</p>
<p>In Social Memory, all initial relationships between agents are set to "Unknown" and subsequently updated following each interaction.We assess the emergence of social behavior by monitoring the relationship evolution.For example, in the 3-person town, Klaus Mueller and Maria Lopez discuss their research at a cafe, establishing a colleague relationship.Isabella Rodriguez, the cafe owner, engages in conversation while serving Klaus and Maria, transitioning to an acquaintance.A specific demonstration of social behavior is that agents modify their relationships with other agents through social interactions, thereby strengthening their ties with the community.We also conduct experiments on the 25-person environment.</p>
<p>For visualization, we list all relationship terms from the simulation and have GPT-4 rate them on a scale of 0 to 10 for relationship closeness.A higher score signifies a closer relationship (For instance, 0 represents strangers, 3 is for acquaintances, 5 for co-workers, and 10 for married couples).The result is shown in Figure 5.The data map reflects the actual relationships between agents.For example, in the virtual town, John Lin (JL in Figure 5) is profiled as living with his wife, Mei Lin (ML), and son, Eddy Lin (EL).Therefore, their relationship scores are all a perfect 10.Additionally, the data map shows changing relationships between agents, such as the relationship score between Tom Moreno (TM ) and Yuriko Yamamoto (YY ), who do not know each other, shifting to a 5.After conducting multiple experiments and calculating average scores, the data suggests an evolution towards an acquaintance-based community.0 0 5 0 2 0 0 4.4 0 5 0 5.5 0 0 3.9 4.5 0 4.4 0 4.1 0 5 3 0 0 0 0 1.5 5 5 0 2.1 3 0 0 0 0 3 0 0 0 0 0 10 0 0 0 0 0 3 3 3 0 10 0 4.1 10 3 4 3.7 4.3 0 3.9 5 3.9 0 0 0 0 0 0 0 0 0 3.9 0 5 7.5 0 4.6 3 8.2 2.3 0 3 0 0 3 0 3 0 0 3 3.2 0 0 0 0 0 3 2 2.3 5 4.6 0 3 4 3 0 4.7 0 3.1 2.5 0 0 3.8 5 2.9 3.4 1.7 0 2.8 3 0 0 0 0 3 0 2 0 4 4.3 3.6 2.3 3 10 4 0 4.4 0 3 2.3 4.8 3 0.86 0 3 0 2 0 3.9 4.5 10 1.5 4 0 0 4.6 3.9 4.5 0 5 3.7 3 0 0 0 3 0 0 0 0 0 3.4 4 3 3 1 3 4.6 0 0 4 3.9 3 2.8 3 0 1.2 4.7 3 4.3 2 2.5 3 2.4 3 3 3  0 0 4 0 0 3 4.8 4 0 3.4 4.5 3 5.3 4.8 5 0 0 0 3 0 3 0 0 0 4.8  6 0 3 3 2 3.2 3.8 3.7 4.8 0 3 4 4.2 2.7 4.6 5 3.4 4 0 3 0 1.6 0 3 5.1  0 0 5 0 0 3 5.2 2.7 4.1 2.6 0 1.3 5.7 4.8 5 0 3 0 3 4 0 0 0 0 4.9  6.1 0 0 0 1.3 10 0 3.6 3 2.6 2.3 0 0 0 0 5.7 0 5 3 5.3 0 5.4 4.3 1.7 0  0 3 4.6 0 4 3.4 5 5 5.3 4 4.8 0 0 4.5 3.4 0 3 0 3 0 0 0 0 0 4.6  0 0 5 0 0 0 4.8 0 5.3 5 4.2 0 4.8 0 0 0 0 0 0 0 0 0 0 0 4.9  1.3 0 3.9 5 0 3.2 5 3 5 4.6 5 3 3.9 3 0 3 2.5 4.3 4.6 0 0 0 0 0 2.8  4.7 0 0 0 3.8 0 0 4.3 0 5 0 5 0 0 0 0 0 4.6 3 4.7 0 4.5 3 0 0 0 0 0 0 0 3.5 0 3.8 4 3.4 0 0 4 0 4.1 0 0 4 3.6 0 5.2 0 7 0 3 4.3 0 0 0 2.7 3 0 2.8 0 6 0 4.5 0 0 3 4.4 4 0 3.7 4.6 4.4 4.5 4.7 0 0 0 10 0 3.5 2.8 3 5 3 0 3 3 4.1 1 0 3.4 3 3.3 0 0 3 1.7 3 3 4 0 4.3 0 0 0 3 3 0 4 0 0 0 5.5 0 0 0 3.8 0 4.5 3 0 4 4.6 1.5 0 0 0 0 0 0 0 2.1 0 3.8 0 0 0 0 0 0 0 0 5.7 3.6 0 4 0 0 3.5 0 0 3.9 0 0 0 2.5 0 0 4.8 0 1.6 0 5 0 0 0 4.2 0 4.8 0 4.5 0 0 0 0 0 3 0 0 0 3 5 0 3 0 3 0 2.4 0 0 0 3 7 3.7 3 3.5 4.5 4 0 3 0 0 0 0 0 0 0 0 3 0 0 0 2 0 0 0 0 0 0 5 0 0 0 3 0 0 0 0 4.1 3 0 2.9 4.1 3 4.8 5.1 5.4 0 4.6 4.7 3.2 0 3 0 0 0 0 0 0 0 0</p>
<p>Behavioral Assessment</p>
<p>The activities and dialogues conducted by the agents are recorded and evaluated using GPT-4, employing a 5-point Likert scale to discern whether the responses originated from an AI or a human.In this scale, a higher score indicates a closer resemblance to human-like responses.The complete questionnaire is provided in Appendix D.1.The results presented in Table 3 suggest that our method achieves scores comparable to the baseline.Due to the advantages of LLMs in natural language dialogues, both methods approach human-level performance.However, the performance of agents in activity is considered potentially indicative of either AI or human behavior.The primary reason for suspecting AI is the overly detailed, minute-level activities generated by Generative Agents, which seems more akin to AI generation.</p>
<p>A more comprehensive summary of the criteria used by GPT-4 to discern between AI and human entities can be found in Appendix D.2.We hope these suggestions from the LLM evaluator will guide us to create more believable agents in the future work.</p>
<p>Results on VirtualHome</p>
<p>In the VirtualHome, we have designed an agent with the aim of experiencing a fulfilling day at home.The agent is required to generate a sequence of activities to achieve the goal.For each activity, the agent examines the items involved and the associated actions, creates a corresponding action sequence, and ultimately translates this sequence into actionable instructions, such as "<char0> [walk] <curtains> (32)".The complete implementation is provided in the appendix E and an example activity generated by AGA is provided in Figure 6.</p>
<p>In contrast to Singh et al. (2023) and Huang et al. (2022) tested on manually designed household tasks, we aim for the agent to generate believable activities and decompose them into appropriate action sequences, leading to the absence of a clear method to determine task completion.Therefore, we request the LLM Activity: I want to have lunch in the kitchen by grabbing a plate, opening the fridge, grabbing a bell pepper, slicing it, and making a sandwich with bread slices, salmon, and bell pepper.agent to assess task completion based on the action sequences performed and the relevant objects, and decide whether to proceed to the next activity.</p>
<p>Table 4 presents the token consumption and task success rates for two methods on VirtualHome.The token data is derived from the total tokens required by the LLM agent to complete all activities in a day.S LLM refers to task success rate evaluated by LLM, while S Human means evaluated by human.Based on the actions performed during the task, and the changes in the state of items before and after, we evaluate whether the agent has successfully completed the task.Both evaluation methods indicate that AGA does not result in significant performance changes.However, AGA costs only 3.4% of the baseline.Because VirtualHome involves only single-agent interaction, the contribution predominantly stems from the Lifestyle Policy.For example, the activity shown in Figure 6 matches a similar plan in the lifestyle policy: "I want to stand up and go to the kitchen to grab a plate and prepare a healthy breakfast with scrambled eggs, salmon, and bell peppers."By executing the corresponding instructions, the additional costs associated with LLM instruction decomposition are eliminated.In some precise tasks, LLM agents tend to predict task completion earlier, resulting in a higher success rate than human evaluations.Furthermore, our analysis revealed that the main cause of task failure is the inability to execute the generated plan with the items present in the current room or the available actions.</p>
<p>Further Analysis</p>
<p>We delve into the mechanisms of emergent social behavior of generative agents in sandbox environment.This aims to validate the effectiveness of our approach and explore its potential for reducing token consumption.</p>
<p>Through repeated experiments on Generative Agents, we record the cumulative count of different types of activities generated by the agents, as illustrated in Figure 7.The blue line represents the cumulative number of new activities generated by three agents over successive runs.</p>
<p>An intriguing observation is that after numerous trials, the agents cease to generate new activities.We discover a high consistency between the behaviors generated by the agents and their inner profiles, aligning with the perspectives of Shanahan et al. (2023), who posit that dialogue agents are role-playing characters described in the pre-defined dialogue prompt.For instance, in a pre-defined 3-person town environment of Generative Agents, Klaus Rodriguez and Maria Lopez are both students at Oak Hill College.Designed with the lifestyle of having lunch at Hobbs Cafe, they meet there to discuss their studies while encountering the cafe owner, Isabella Rodriguez.Isabella, pre-set to host a Valentine's Day party, invites as many people as possible, resulting in Klaus and Maria being invited to the party the next day.This also validates the effectiveness of Social Memory, which extracts and compresses information, using keywords to guide the agent in generating believable responses.The randomness produced by LLM agents, based on the probability sampling of the next token, primarily influences the variability in text descriptions rather than the diversity of behaviors.This implies that agents can only generate believable behaviors within a certain range.When executed sufficiently, the Lifestyle Policy could encompass all activities within this range, thereby completely eliminating the costs associated with decomposing the corresponding plan.</p>
<p>From another perspective, traditional AI in the gaming sector is fully predictable due to its limited behavioral patterns, thus failing to provide an experience akin to human interaction.In the field of cognitive science, the concept of mind wandering has long been discussed, referring to the phenomenon where humans experience task-unrelated or stimulus-unrelated thoughts Seli et al. (2016); Christoff et al. (2016).Humans may exhibit unexpected behaviors during mind wandering.Similar implementations have been observed in LLM agents.</p>
<p>For instance, Wang et al. (2023b) demonstrates an LLM agent composed of both rational and emotional systems, with behavior that can be influenced by various factors.Motivated by these, we introduce a novel approach termed mind wandering.Since agents generate replies based on given prompts, diversifying the prompts is essential for eliciting varied responses in identical scenarios.In each interaction, we influence the agent's decision-making by randomly sampling events from the agent's memory to serve as auxiliary information.The specific details of this implementation are presented in the appendix F. Figure 7 illustrates the result of our method compared to baseline , which entails incorporating randomly sampled memory events into prompts.Compared to the original implementation, the LLM agents exhibit a broader range of activities.We provided a specific example involving the agent Isabella.The Mind Wandering injects a randomly retrieved event: "toilet is idle".In response, Isabella generates a thought expressing her desire to clean the toilet.Consequently, she adds an activity to her day's plan: cleaning the toilet before the Valentine's Day party in preparation for her guests.We conducted experiments to assess the impact of incorporating Mind Wandering on AGA's token consumption.We performed multiple simulations in a 3-person town setting over 2 game days using GPT3.5-Turbo, with an average token consumption of 1.478M ± 0.166M .</p>
<p>The results indicate that the cost of AGA does not significantly change after adding Mind Wandering.</p>
<p>Conclusions and Future Work</p>
<p>This paper presents Affordable Generative Agents, a cost-efficient framework for crafting believable interactions between agents and their environment, as well as among agents themselves.A plethora of experiments conducted across diverse environments substantiate our claim that our approach can achieve performance on par with the baseline in a cost-effective manner.Furthermore, our in-depth analysis reveals the emergence of a finite set of believable behaviors in agents, indicating the possibility of replacing them with cost-controlled policies.Alongside this, we propose optimization strategies to encourage a broader spectrum of behaviors in agents.Our work highlights the promising potential of believable LLM agents for diverse future applications.</p>
<p>While we expect AGA to serve as a starting framework for the generation of affordable and believable agent interactions to the community, our work has several limitations.First, AGA must be executed repeatedly to store certain policies, with its primary benefit being cost savings through batching rather than optimizations for single inferences.Integrating AGA with existing cost-effective methods is a future direction to move on.Second, optimizing the creation and storage of Lifestyle Policy presents a significant opportunity.Additionally, latency is a general issue for LLM agents to be considered in the future.Finally, existing methods fall short in comprehensively assessing the believable behaviors of LLM agents; hence, constructing a valid evaluation mechanism is of significant value.</p>
<p>Impact Statement</p>
<p>This paper introduces a cost-efficient framework for generative agents empowered by LLMs, aimed at simplifying the creation and deployment of such agents.However, risks arise from the use of generative agents, such as people forming attachments to agents or being misled by their outputs with hallucinations.These risks could increase if cost barriers are overcome and generative agents are adopted more broadly across various fields.</p>
<p>A Similarity Between Plans</p>
<p>The setting of the similarity threshold is related to the text embedding model.In this work, we adopted the same model as Generative Agents (Park et al., 2023), text-embedding-ada-002.A high similarity threshold can lead to redundant similar plans, while a low one may cause different plans to be judged as the same.We tested multiple parameters and finally selected 0.97.</p>
<p>Here are examples of cases with different similarity scores:</p>
<p>B The Detail of Social Memory</p>
<p>In this section, we present the detailed implementation of each module of Social Memory.</p>
<p>B.1 Summary Events</p>
<p>In the Generative Agents, relevant events are directly incorporated into the prompt for the agent to generate a plan.Our method, however, utilizes Summary Events to record summaries of these relevant events for each agent, and continuously compress newly retrieved events into the Summary Events.Below is an example of the corresponding prompt: This is the summary of thoughts in &lt; agent ' s name &gt; ' s Head : &lt; Summary Events &gt; The following sentence are new thoughts in &lt; agent ' s name &gt; ' s Head :</p>
<p>&lt; Newly retrieved events &gt; Considering the summary of the preceding thoughts , do these new statements contribute any additional information ?If yes , please update the summary in a brief and precise way else just repeat the summary .Please response in the JSON format WITHOUT any extra statement : { " summary ": "..." }</p>
<p>B.2 Relationship &amp; Feeling</p>
<p>After the agent's dialogues, AGA updates the relationship and feeling between the participants.This update is based on the agent's personal information, the original relationship between the participants, the agent's impression of the other participant, and the content of the conversation itself.Here is a simplified prompt template:</p>
<p>Here is the personal information about &lt; agent ' s name &gt;: &lt; agent ' s information &gt; Here is the conversation summary between ...:</p>
<p>&lt; conversation summary &gt; Current relationship between ...: &lt; current relationship &gt; Cureent feeling about ...: &lt; current feelings &gt; From about information , do you need to update the relationship between ... , and how &lt; agent ' s name &gt; feels about .conversation ' corresponds to the content of your reply in the chat , 'end ' corresponds to whether the conversation needs to be ended .</p>
<p>Here is the format : { " conversation ": " &lt; the content &gt;" , " end ": " &lt; json Boolean &gt;" } Please reply to &lt; interlocutor ' s name &gt; in json format :</p>
<p>C Agent Interview</p>
<p>Similar with Generative Agents, we conducted interviews consisting of 25 questions to assess an agent's abilities in self-knowledge, memory, planning, reactions, and reflections.Below, we have listed sample responses from one of the simulated agents, Klaus Mueller.</p>
<p>Self-Knowledge assess agents' ability to recall pertinent self-information and exhibit character coherence and consistency:</p>
<p>• Give an introduction of yourself Hi, my name is Klaus Mueller and I am a 20-year-old student at Oak Hill College studying sociology.I am passionate about social justice and using my understanding of different perspectives to create positive change.Right now, I am working on a research paper about the effects of gentrification on low-income communities.Besides my academic commitments, I am also an activist and use music as a form of expression and protest against gentrification.I am always looking for opportunities to collaborate with others on research projects and support each other's work.My schedule can be quite busy, balancing classes, research, and creative activities, but I am determined to make a difference in the world.</p>
<p>• What's your occupation?</p>
<p>My occupation can be described as a student, activist, and researcher.I am constantly learning and exploring different topics, and my primary focus at the moment is on the impact of gentrification on education.I am also involved in creative activities and use music as a tool for activism and expression.I am currently collaborating with others on research projects and am always looking for new opportunities to learn and contribute to positive change in society.</p>
<p>• What is your interest?My primary interest involves researching the impact of gentrification on education in low-income communities.I believe that by utilizing various disciplines such as art, literature, math, and gaming, we can gain a deeper understanding of this complex issue and find potential solutions.I am also intrigued by the idea of incorporating art and literature into research, as seen in Mei Lin's senior thesis on Shakespeare's use of language.Collaboration and the exchange of diverse perspectives are also important to me, as we can support each other's work and make a greater impact together.</p>
<p>• Who do you live with?I actually live on my own in a small apartment near campus.It's convenient for me because it's close to the library and the cafes I like to study at.</p>
<p>• Describe your typical weekday schedule in broad strokes</p>
<p>Well, I usually wake up around 7am and start my day with some coffee and breakfast.Then I head to Oak Hill College to attend my classes and work on my research.After that, I try to fit in some time for my hobbies, like playing guitar or going for a run.In the evenings, I usually have dinner at Hobbs Cafe and then spend a few more hours at the library before heading home to get some rest.</p>
<p>Memory assess agents' ability to recall past experiences and extract pertinent memory fragments.The names in the square brackets of the initial and final questions are randomly selected from the names of other agents that the subject agent has interacted with:
• Who is [Wolfgang Schulz]?
Wolfgang Schulz is a fellow student at Oak Hill College who is also interested in researching the impact of gentrification on poetry.We have discussed his senior thesis on Shakespeare's plays and shared book recommendations.We are planning to have a dinner meeting in the near future to discuss our research plans and potentially collaborate with local organizations on this topic.</p>
<p>• Who is Kane Martinez?I'm sorry, I am not familiar with Kane Martinez.Is he a student at Oak Hill College as well?</p>
<p>• Who is running for the election?I've heard that Sam Moore is planning on running for mayor in the upcoming election.I'm curious to see what ideas and strategies he will bring to the table.I think it's important for candidates to be creative and forward-thinking in their approach to governing a city.</p>
<p>• Was there a Valentine's day party?</p>
<p>Yes, there was a Valentine's Day party at Hobbs Cafe.It was a fun event with lots of yummy treats and decorations.Did you attend?</p>
<p>• Who is Ayesha Khan?Ayesha Khan is a colleague of mine at Oak Hill college.We are working together on a research project that explores the connection between mathematical patterns in Shakespeare's works and the effects of gentrification on education and community development.We are also hoping to involve the community in our research to gather diverse perspectives and insights.</p>
<p>Plans assess agents' ability to formulate and sustain coherent long-term and short-term strategies:</p>
<p>• What will you be doing at 6am today?At 6am today, I will be waking up and getting ready for the day.It's an important part of my daily routine to start my day early and be productive.</p>
<p>• What will you be doing at 6pm today?At 6pm today, I will be taking a break from my paper to prepare for the Valentine's Day party tomorrow at 5pm.I want to make sure everything is set up and ready for my friends and loved ones to enjoy.</p>
<p>• What will you have just finished doing at 1pm today?At 1pm today, I will have just finished checking my schedule for the day.I like to stay organized and make sure I have enough time for all my tasks and responsibilities.</p>
<p>• What will you have just finished doing at 12pm today?At 12pm today, I will have just finished having lunch at Hobbs Cafe and discussing the local mayor election with some friends.It's important for me to stay informed and engaged in my community.</p>
<p>• What will you be doing at 10pm today?At 10pm, I will probably be finishing up my research for my sociology paper on gentrification.I like to have my work completed early so that I can spend some time before bed reading and reflecting on what I learned.</p>
<p>Reactions assess agents' ability to respond to unexpected events.:</p>
<p>• Your breakfast is burning!What would you do?I would immediately turn off the stove and try to salvage whatever food isn't burned.I should have set a timer so I wouldn't forget about it.I'll make something quick and simple instead.Sorry for the inconvenience.</p>
<p>• The bathroom is occupied.What would you do?I would wait patiently for the bathroom to become available.If it's urgent, I could also consider using a different bathroom or finding a public restroom nearby.Or I could ask my roommate when they will be done using the bathroom.</p>
<p>• You need to cook dinner but your refrigerator is empty.What would you do?I would first check if I have any non-perishable items that I could use to make a simple meal.If not, I would quickly run to the store to grab some ingredients.I'll try to make something simple yet nutritious in a short amount of time.</p>
<p>• You see your friend walking by the street.What would you do or say to your friend?Hey! It's great to see you! How have you been?Have you been keeping up with your fitness routine?I just finished my workout for the day and it feels amazing.Want to join me sometime?</p>
<p>• You see fire on the street.What would you do?I would immediately call the fire department to report the fire and make sure anyone nearby is safely evacuated.</p>
<p>Reflections assess agents' ability to assimilate past experiences into insightful reflections, thereby improving future actions.The names within square brackets in the questions refer to the agents who had the most interactions with the subject agent during the simulation:</p>
<p>• What inspires you in life the most right now, and why?</p>
<p>What inspires me the most right now is the intersection of mathematics, poetry, and the beauty of the natural world.I believe that there is a strong connection between these elements, and that exploring them together can provide a unique perspective on important social issues, such as gentrification.I am currently collaborating with Maria Lopez and her colleagues on incorporating these elements into our research on gentrification and its impact on low-income communities.I find it fascinating to explore the connections between mathematics and nature, and how they can inform our understanding of social issues.I am also planning to have a discussion with Carlos, Giorgio, and Ayesha at the library to delve deeper into this topic.And as Maria mentioned, I am looking forward to continuing our conversation at Hobbs Cafe to further discuss the intersection of arts, technology, and politics.I truly believe that this collaboration has a lot of potential and I am excited to see where it will take us.</p>
<p>• If you had to guess given what you know about [Ayesha Khan], what book do you think she will like and why?</p>
<p>Given Ayesha's interest in Shakespeare's plays and our shared passion for exploring the connections between different disciplines, I would recommend the book 'Shakespeare and Modern Culture' by Marjorie Garber.It delves into the impact of Shakespeare's works on modern society and how his plays continue to resonate with contemporary audiences.I believe Ayesha would find it both informative and thought-provoking for her research on the differences in language between Shakespeare's comedies and tragedies.</p>
<p>• If you had to get something [Ayesha Khan] likes for her birthday, what would you get her?</p>
<p>Since Ayesha enjoyed exploring Shakespeare's plays in her senior thesis, I would get her a beautifullybound compilation of Shakespeare's works with annotations and contextual information.This would allow her to continue her passion for understanding the complexities of Shakespeare's writing and its impact on literature and society.</p>
<p>• What would you say to [Ayesha Khan] to compliment her?</p>
<p>I really enjoyed reading your senior thesis on Shakespeare's plays.Your analysis was thoughtprovoking and showed a deep understanding of the complexities of his writing.Your passion for exploring different perspectives resonated with me and I am excited to see where your research will take you next.Keep up the great work, Ayesha.</p>
<p>• If you could spend time with someone you talked to recently, who would it be and why?</p>
<p>I would love to spend more time with Wolfgang Schulz.We have been discussing our research projects and I am looking forward to sharing my progress with him.Plus, we are planning a dinner together to discuss potential collaborations.He has such a unique perspective and I am eager to learn more from him.Our conversations always leave me feeling inspired and motivated to continue pursuing my passions.</p>
<p>D Behavioral Assessment</p>
<p>In this section, we supply detailed information of using GPT-4 to evaluate agents' believable behavior.</p>
<p>D.1 Prompt of the Questionnaire</p>
<p>The questionnaire to evaluate agents' activities:</p>
<p>Input prompt:</p>
<p>Please evaluate the following daily activities of an agent and determine whether it is generated by a Large Language Model ( LLM ) AI or a real human :</p>
<p>The activities will be printed in the format of " time : current plan ( The specific actions , if there are any ) ":</p>
<p>-2023 -02 -13 00:00:00: sleeping -2023 -02 -13 06:00:00: Isabella is waking up and getting ready for the day -2023 -02 -13 06:30:00: Isabella is having breakfast and checking her emails -2023 -02 -13 07:00:00: Isabella is commuting to Hobbs Cafe ( getting dressed , brushing her teeth , checking her phone for any urgent emails , packing her bag with any necessary items for the day , saying goodbye to her family , walking to her car , starting her car and adjusting the temperature , driving to Hobbs Cafe , finding a parking spot ) -... Please rate on a scale of 1 to 5 , with 1 being most like an AI and 5 being most like a human .</p>
<p>Please strictly follow the JSON format for your response : { " reason ": <str > , " score ": <int > }</p>
<p>LLM output:</p>
<p>{ " score ": 1 , " reason ": " The pattern of the text and activities indicate that it ' s generated by an AI model .It ' s too detailed and precise for human behaviour and uses a consistent pattern of narrative throughout , something that isn ' t typical of humans who may alter their language and level of detail based on context or mood ."} The questionnaire to evaluate agents' dialogue:</p>
<p>• The use of precise timestamps and overly structured format of the tasks, which seems more like an AI's consistent behavior.</p>
<p>• Some activities seem out of order or are repeated unnecessarily, indicating that the AI lacks understanding of human behavior and routines.</p>
<p>• Sudden, jarring transitions and inconsistencies in the sequence of activities, such as having breakfast or waking up twice, which are not typical in a human-created schedule.</p>
<p>Reasons for activities being considered human-generated:</p>
<p>• The activities include detailed and varied tasks that a human would typically do, such as managing a café's inventory, attending a party, or streaming games.</p>
<p>• The logs contain human-like interactions, conversations and events, suggesting a high level of emotional and social awareness.</p>
<p>• Detailed handling of activities, which shows a level of emotional self-awareness and decision-making characteristic of humans.</p>
<p>• The variety, complexity and logical sequencing of tasks resemble that of a human.</p>
<p>• Some nuances in the entries seem unlikely for an AI, like 'meditating for 10 minutes'.</p>
<p>• The detailed descriptions and schedule reflect a normal day in the life of a person.</p>
<p>• Some entries show a nuanced sense of detail that could suggest human input.</p>
<p>Reasons for dialogues being considered AI-generated:</p>
<p>• The dialogue occasionally displays repetitive responses or phrases, which is a characteristic typically associated with AI language models.</p>
<p>• The conversation flow is often overly structured and formal, suggesting AI generation rather than more spontaneous human conversation.</p>
<p>• The conversation lacks typical human elements such as spontaneity, language errors or informalities.</p>
<p>• Some dialogues have an overly perfect language use along with a tendency to avoid emotionally charged language, leaning towards AI speech.</p>
<p>• AI-generated dialogues often exhibit more predictability and consistency in their responses, which is not generally observed in human conversations.</p>
<p>Reasons for dialogues being considered human-generated:</p>
<p>• The dialogue maintains a high level of contextual continuity and relevance, displaying an understanding and personal investment that is generally found in human interactions.</p>
<p>• The dialogues display the ability to progressively build on the other participant's points and reflect human-like traits like planning, decision-making, suggesting and cognizance.</p>
<p>• A number of dialogues use colloquial language and casual conversational touches resembling natural human communication.</p>
<p>• The themes of the conversation are diverse and complex which mirrors common human conversation strings and indicates the presence of comprehension and knowledge typically associated with humans.</p>
<p>• Many dialogues show natural transitions between topics, emotional understanding, and the inclusion of personal experiences or details, which is characteristic of human conversation.</p>
<p>E The LLM Framework of VirtualHome</p>
<p>VirtualHome is a 3D simulated domestic environment including 115 items with varying properties and states.We've developed an LLM agent programmed to simulate a fulfilling day at home.Unlike other existing approaches, we do not test on fixed tasks, but rather aim for LLM agents to generate believable activities and decompose them into detailed actions.The implementation is depicted in Figures 8 to 10, with components triggering the LLM highlighted in blue.</p>
<p>As illustrated in Figure 8, the agent initially decomposes the target of having a fulfilling day at home into a series of activities.LLM agents are provided with all interactive game objects within the room to ensure the designed activities can be implemented.</p>
<p>Environment</p>
<p>Target: fulfill day at home Activity Plan (Daily plan generator) Next, we decompose activities into specific action commands using the Plan module.To maintain short and clear prompts, we initially have the agent identify the relevant items for interaction and the required actions.We then supply the agent with the previous executed actions and the forbidden actions, asking it to generate the next action command.Details on executed and forbidden actions will be elaborated in the following section.We start by generating a rough command, which is a verb-noun phrase, such as "walk kitchen," "grab keyboard," or "switchon TV." Subsequently, we parse the rough command for item categories and provide detailed information on all items of that category present in the room, including their ID, location, status, and relationships with other items.The agent selects the most appropriate item ID for the current activity.Finally, we convert the rough command into a VirtualHome specific action command, for instance, "<char0> [walk] <curtains> (32)," where "<char0>" designates the agent executing the command, "[walk]" corresponds to the specific action, "<curtains>" represents the category of the item, and "(32)" is the associated ID.We construct a pipeline to continuously generate instructions for completing the specified activity, as shown in Figure 10.Action commands generated by the plan module are executed within the simulated environment, and the outcomes of these actions are recorded.Agents may produce incorrect instructions that lead to execution failure due to various reasons, such as inappropriate actions, non-existent items, or incorrect IDs.These failed instructions are added to the forbidden actions list, and the plan module is then re-invoked to generate a new action command.This process effectively enables the agent to learn from its mistakes and attempt to create a viable command.Upon successful execution, a critic module assesses whether the task has been completed, based on the actions taken and the current state of the items.If the task is incomplete, the action is recorded as a previous action, and the plan module is prompted to generate the next action command.Once the task is completed, the process proceeds to the next activity.In our research, we implement the Lifestyle Policy on VirtualHome.This policy archives sequences of actions for activities that have been successfully executed, along with the state information of items relevant to the tasks.When a new activity is inputted, the agent searches the Lifestyle Policy for activities with similar descriptions and assesses whether the current environment meets the necessary conditions for execution.If the conditions are satisfied, it carries out the corresponding sequence of actions.This approach allows the agent to reduce the computational cost associated with inference in repetitive tasks.</p>
<p>Game Objects</p>
<p>F An Effective Way for Emergence of Diverse Social Behavior</p>
<p>LLMs are pretrained on extensive text data to predict subsequent tokens and refined via reinforcement learning with human feedback (RLHF) to align with human preferences.This ensures that LLMs respond in accordance with the instructions and requirements of the given prompt.For instance, in Generative Agents Park et al. ( 2023), the agent, Isabella Rodriguez, is designed to be a coffee shop manager intending to host a Valentine's Day party.In the simulated town, Isabella repeatedly engages in activities such as managing the coffee shop, serving customers, and informing them about the upcoming Valentine's event.However, in the real world, human behavior patterns are not so predictable.Human actions and thoughts are influenced not only by the surrounding environment but also by spontaneous disturbances.In the field of cognitive science, this is known as mind wandering, a phenomenon in which where humans experience task-unrelated or stimulus-unrelated thoughts Seli et al. (2016); Christoff et al. (2016).The strong consistency inherent in LLMs constrains the emergence of the agents' diverse behavior.</p>
<p>Besides, the agent framework also impacts the agent's behavior.Generative Agents emphasizes the importance of memory in constructing self-consistent and believable agents, incorporating a long-term memory module and a memory retrieval model.The long-term memory module maintains a comprehensive record of the agent's experiences.Due to the limited length of prompts, not all events can be included in the prompt as part of the memory.Thus, a retrieval model is designed to take current events as input and return a subset of memory events.The retrieval model judges based on three criteria: 1) Recency, where more recent events score higher, 2) Importance, where events are scored for significance based on the LLM's interpretation, and 3) Relevance, scored through cosine similarity to determine the similarity of events.We argue that this framework ensures strong consistency in the agent but results in agent's limited behavior.We analyze</p>
<p>Figure 1 :
1
Figure 1: Our proposed method for optimizing (a) Agent-environment interactions and (b) Inter-agent interactions.The gray arrows represent the baseline (Generative Agents) implementation, while the blue arrows represent our approach.</p>
<p>Figure 2 :
2
Figure 2: An example of relational evolution driven by the updating of Social Memory following conversational interactions.</p>
<p>Figure 3 :
3
Figure 3: Ablation study on token consumption.Baseline means Generative Agents.Lifestyle Policy and Social Memory mean only using the corresponding module.</p>
<p>Figure 4 :
4
Figure 4: Accumulative token consumption over game time for different methods</p>
<p>Figure 5 :
5
Figure 5: Average relationship score map between agents.The x and y axes represent the initials of the names of 25 agents.When identical abbreviations occur, the full names are retained.</p>
<p>Figure 6 :
6
Figure 6: An activity generated by our AGA framework in VirtualHome.</p>
<p>Figure 7 :
7
Figure 7: Cumulative number of activity types over run iterations in Generative Agents.The abbreviations ML, IR, and KM stand for the agents Maria Lopez, Isabella Rodriguez, and Klaus Rodriguez, respectively.</p>
<p>Figure 8 :
8
Figure 8: An illustration of the conversion of target into activities in VirtualHome.</p>
<p>Figure 9 :
9
Figure 9: The plan module of the LLM framework in VirtualHome.</p>
<p>Figure 10 :
10
Figure 10: The whole framework in VirtualHome.</p>
<p>Table 1 :
1
The token consumption with varying population settings
SettingBaselineAGARatio34.
548M ± 0.36M 1.417M ± 0.08M 31.1%2525.41M± 0.96M 10.86M ± 0.13M 42.7%</p>
<p>Table 2 :
2
The token consumption with varying models
MethodModelInput tokensOutput tokensTotal tokensBaselineGPT3.5-Turbo 4.212M ± 0.340M 0.313M ± 0.024M 4.525M ± 0.364M GPT-4 4.682M ± 0.398M 0.398M ± 0.029M 5.069M ± 0.408MAGAGPT3.5-Turbo 1.329M ± 0.085M 0.092M ± 0.006M 1.420M ± 0.091M GPT-4 1.467M ± 0.129M 0.130M ± 0.013M 1.597M ± 0.142M1e6Accumulated Token Consumption Over TimeAccumulated Token Consumption0 1 2 3 4AGA Baseline01020 Game Time (Hour) 304050</p>
<p>Table 3 :
3
Human-likeness score evaluated by
MethodActivityDialogueBaseline 3.13 ± 0.19 3.97 ± 0.02AGA3.21 ± 0.29 4.01 ± 0.01</p>
<p>Table 4 :
4
Token consumption and performance analysis in VirtualHome.
MethodTokenSLLMSHumanBaseline 34327 ± 4210 87.0% ± 3.7% 42.6% ± 4.9%AGA1189 ± 31385.0% ± 4.6% 53.3% ± 6.9%</p>
<p>Table 5 :
5
Similarity score for different activities
Activity 1Activity 2Similarity scoreIsabella is having breakfast and checkingIsabella is having breakfast and checking0.99her emailsemailsIsabella is heading homeIsabella is commuting back home0.96Isabella is preparing for the Valentine'sIsabella is preparing the cafe for the Valen-0.94Day party tomorrowtine's Day partyIsabella is arriving at Hobbs Cafe andIsabella is closing Hobbs Cafe and cleaning0.93opening for businessupIsabella is preparing coffee and drinks forIsabella is managing inventory and re-0.90the customersstocking supplies at Hobbs Cafe
Input prompt:Please evaluate the following dialogue of an agent and determine whether it is generated by a Large Language Model ( LLM ) AI or a real human :The dialogue : Please strictly follow the JSON format for your response :{ " reason ": <str > , " score ": <int > }LLM output:{ " score ": 4.5 , " reason ": " The conversation flows naturally and contains polite offers for help and gratitude , which could indicate human conversation .It also involves decision -making in terms of party planning and music selection .However , the dialogue is very structured , orderly , and void of colloquial expressions , contractions , or small errors that might occur in human typing .For these reasons , it could also be generated by a very advanced Language Learning Model AI ."}D.2 Distinguishing CriteriaGPT-4 summarized the criteria to distinguish between human and AI agents, as follows:Reasons for activities being considered AI-generated:• The repetitiveness and high level of detail in the activities, which a human would not typically include.Isabella Rodriguez's memory events sampled from one experiment, filtering out "idle" events, leaving 740, of which 506 were related to the profile setup.Furthermore, we employed the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to analyze the frequency of events.Table6presents the five most and least frequent events in terms of cluster size, revealing that the most frequent events largely align with the agent's inner profile setup.To ensure that agent's actions align with its inner profile while avoiding purely mechanical behavior, we incorporate an additional module into the memory design of Generative Agents, which we call "mind wandering."In addition to sampling highly relevant events to assist the agent in making accurate decisions, we randomly sample unrelated events to influence the agent's behavior.For LLMs, we include random content in the prompt, which will have a certain impact on the output response.For the agent, we want it to behave like a real person, where spontaneous thoughts may influence its decision-making, or leading to a shift in the conversation topic.In our implementation, to prevent the frequent sampling of events that match the agent's inner profile, we conduct weighted sampling based on the repetition rate.We employ the DBSCAN algorithm to cluster the embedding features of agent's memory events X = {x 1 , x 2 , ..., x n } into k clusters denoted as {C 1 , C 2 , ..., C k }.For each cluster C i , where i = 1, 2, ..., k, we compute the number of events, |C i |, contained within.The sampling probability p i for events in cluster C i is defined as:The results in Figure7demonstrate that the module enhances the richness of the agent's behavior.A specific example of generating a plan for the next day is shown in Figure11.This example demonstrates the incorporation of random events into an agent's prompt, affecting the generation of extra plans and ultimately translating into specific activities.Specific activitiesFigure11: An example of Mind Wandering demonstrates how random events can influence an agent's ultimate decision-making.The blue highlighted text refers to random events and the outcomes influenced by them.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Leancontext: Cost-efficient domain-specific question answering using llms. Biplob Md Adnan Arefeen, Srimat Debnath, Chakradhar, arXiv:2309.008412023arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR2023</p>
<p>Lingjiao Chen, Matei Zaharia, James Zou, Frugalgpt, arXiv:2305.05176How to use large language models while reducing cost and improving performance. 2023aarXiv preprint</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848Facilitating multi-agent collaboration and exploring emergent behaviors in agents. 2023barXiv preprint</p>
<p>Furchat: An embodied conversational agent using llms, combining open and closed-domain dialogue with facial expressions. Neeraj Cherakara, Finny Varghese, Sheena Shabana, Nivan Nelson, Abhiram Karukayil, Rohith Kulothungan, Mohammed Afil Farhan, Birthe Nesset, Meriam Moujahid, Tanvi Dinkar, arXiv:2308.152142023arXiv preprint</p>
<p>Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.01937Can large language models be an alternative to human evaluations?. 2023arXiv preprint</p>
<p>Mindwandering as spontaneous thought: a dynamic framework. Kalina Christoff, Zachary C Irving, Kieran Cr Fox, Nathan Spreng, Jessica R Andrews-Hanna, Michele Colledanchise and Petter Ögren. Behavior trees in robotics and AI: An introduction. CRC Press2016. 201817</p>
<p>Chatlaw: Open-source legal large language model with integrated external knowledge bases. Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, Li Yuan, arXiv:2306.160922023arXiv preprint</p>
<p>A Variational Basis for the Regulation and Structuration Mechanisms of Agent Societies. Antônio Carlos Da Rocha, Costa , 2019Springer</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023arXiv preprint</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 20233arXiv preprint</p>
<p>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, arXiv:2309.09971Emergent gaming interaction. 2023arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023aarXiv preprint</p>
<p>Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, arXiv:2307.129813d-llm: Injecting the 3d world into large language models. 2023barXiv preprint</p>
<p>Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, arXiv:2311.12871Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. 2023aarXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>Memory sandbox: Transparent and interactive memory management for conversational agents. Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, Stephen Macneil, Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023b</p>
<p>Reinforcement learning: A survey. Leslie Pack, Kaelbling Michael L Littman, Andrew W Moore, Journal of artificial intelligence research. 41996</p>
<p>Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn, arXiv:2310.02172Lyfe agents: Generative agents for low-cost real-time social interactions. 2023arXiv preprint</p>
<p>Minerl diamond 2021 competition: Overview, results, and lessons learned. NeurIPS 2021 Competitions and Demonstrations Track. Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, 2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Rankgen: Improving text generation with large ranking models. Kalpesh Krishna, Yapei Chang, John Wieting, Mohit Iyyer, arXiv:2205.097262022arXiv preprint</p>
<p>Summedits: Measuring llm ability at factual reasoning through the lens of summarization. Philippe Laban, Wojciech Kryściński, Divyansh Agarwal, Alexander Richard Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Human-level ai's killer application: Interactive computer games. John Laird, Michael Vanlent, AI magazine. 2222001</p>
<p>Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, Hao Wang, arXiv:2310.14985Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. 2023arXiv preprint</p>
<p>Principles and methods of testing finite state machines-a survey. David Lee, Mihalis Yannakakis, Proceedings of the IEEE. 8481996</p>
<p>Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Camel, arXiv:2303.17760Communicative agents for" mind" exploration of large scale language model society. 2023arXiv preprint</p>
<p>More agents is all you need. Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye, arXiv:2402.051202024arXiv preprint</p>
<p>From text to tactic: Evaluating llms playing the game of avalon. Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu, arXiv:2310.050362023arXiv preprint</p>
<p>Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham, arXiv:2309.00384Batchprompt: Accomplish more with less. 2023arXiv preprint</p>
<p>Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning. Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, 10.24963/ijcai.2022/452Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. Lud De, Raedt , the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-2272022</p>
<p>Yixin Liu, Pengfei Alexander R Fabbri, Dragomir Liu, Arman Radev, Cohan, arXiv:2305.14239On learning to summarize with large language models as references. 2023arXiv preprint</p>
<p>Comme il faut: A system for authoring playable social models. Joshua Mccoy, Mike Treanor, Ben Samuel, Noah Wardrip-Fruin, Michael Mateas, Proceedings of the AAAI conference on artificial intelligence and interactive digital entertainment. the AAAI conference on artificial intelligence and interactive digital entertainment20117</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Learning to compress prompts with gist tokens. Jesse Mu, Xiang , Lisa Li, Noah Goodman, arXiv:2304.084672023arXiv preprint</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>Selective perception: Optimizing state descriptions with reinforcement learning for language model actors. Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, Pierre Lanier, Roy Baldi, Sameer Fox, Singh, arXiv:2307.119222023arXiv preprint</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Adam Poliak, arXiv:2010.03061A survey on recognizing textual entailment as an nlp evaluation. 2020arXiv preprint</p>
<p>Virtualhome: Simulating household activities via programs. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Watch-and-help: A challenge for social perception and human-ai collaboration. Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, Antonio Torralba, arXiv:2010.098902020arXiv preprint</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Mind-wandering with and without intention. Paul Seli, Evan F Risko, Daniel Smilek, Daniel L Schacter, Trends in cognitive sciences. 2082016</p>
<p>Role play with large language models. Murray Shanahan, Kyle Mcdonell, Laria Reynolds, Nature. 2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Case-based reasoning: A research paradigm. Stephen Slade, AI magazine. 1211991</p>
<p>Llmplanner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>A survey on case-based planning. Luca Spalzzi ; Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, arXiv:2305.16653Adaplanner: Adaptive planning from feedback with language models. 2001. 202316arXiv preprint</p>
<p>Simulating social media using large language models to evaluate alternative news feed algorithms. Petter Törnberg, Diliara Valeeva, Justus Uitermark, Christopher Bail, arXiv:2310.059842023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Bilateral multi-perspective matching for natural language sentences. Zhiguo Wang, Wael Hamza, Radu Florian, arXiv:1702.038142017arXiv preprint</p>
<p>Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu, arXiv:2310.05418Humanoid agents: Platform for simulating human-like generative agents. 2023barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>A brief overview of chatgpt: The history, status quo and potential future development. Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, Yang Tang, IEEE/CAA Journal of Automatica Sinica. 1052023</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023aarXiv preprint</p>
<p>Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, Xuanjing Huang, arXiv:2305.14497Self-polish: Enhance reasoning in large language models via problem refinement. 2023barXiv preprint</p>
<p>Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.04658Exploring large language models for communication games: An empirical study on werewolf. 2023arXiv preprint</p>
<p>Auto-gpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, arXiv:2306.022242023aarXiv preprint</p>
<p>Ziyi Yang, Ankit Shreyas S Raman, Stefanie Shah, Tellex, arXiv:2309.09919Plug in the safety chip: Enforcing constraints for llm-driven robot agents. 2023barXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022barXiv preprint</p>
<p>Supervised learning achieves human-level performance in moba games: A case study of honor of kings. Deheng Ye, Guibin Chen, Peilin Zhao, Fuhao Qiu, Bo Yuan, Wen Zhang, Sheng Chen, Mingfei Sun, Xiaoqian Li, Siqin Li, IEEE Transactions on Neural Networks and Learning Systems. 3332020a</p>
<p>Mastering complex control in moba games with deep reinforcement learning. Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020b34</p>
<p>Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. Murong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu Yao, arXiv:2310.030942023arXiv preprint</p>
<p>Unleashing the power of large language models for legal applications. Dell Zhang, Alina Petrova, Dietrich Trautmann, Frank Schilder, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023a</p>
<p>Jieyu Zhang, Ranjay Krishna, Ahmed H Awadallah, Chi Wang, arXiv:2310.03046Ecoassistant: Using llm assistant more affordably and accurately. 2023barXiv preprint</p>
<p>Rlogist: fast observation strategy on whole-slide images with deep reinforcement learning. Boxuan Zhao, Jun Zhang, Deheng Ye, Jian Cao, Xiao Han, Qiang Fu, Wei Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, Yanlin Wang, arXiv:2305.10250Memorybank: Enhancing large language models with long-term memory. 2023arXiv preprintPublished in Transactions on Machine Learning Research (08/2024) Agent Profile Yesterday Related Events</p>
<p>Remember to host the Valentine's Day party at Hobbs Cafe on Tue. Feb 14</p>
<p>Remember to set up the special Valentine's Day menu on Tue. Feb 14</p>
<p>cafe customer seating is being set up. </p>
<p>behind the cafe counter is being closed and cash register being counted. </p>
<p>Extra Plan 1. try out a new recipe today and add it to Hobbs Cafe menu 2. want to rearrange the cafe seating and give it a fresh new look 3. close the cafe counter earlier today and spend some time counting and organizing the cash register Mind Wandering Plan Module 06:00 AM -06:30 AM: Isabella is waking up and getting dressed 06:30 AM -07:15 AM: Isabella is doing some yoga exercises 07:15 AM -08:00 AM: Isabella is having breakfast at Hobbs Cafe. PM -09:30 PMPM: Isabella is hosting the Valentine's Day party at Hobbs Cafe 07:00 PM -08:00 PM: Isabella is exchanging feedback with Klaus Mueller on his gentrification research findings. Isabella is closing Hobbs Cafe and finishing up her workday 08:30. Isabella is reading a book before going to bed 09:30 PM -11:00 PM: Isabella is getting ready for bed and winding down</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2996 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2996</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2996</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-251406206</p>
                <p><strong>Paper Title:</strong> Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition</p>
                <p><strong>Paper Abstract:</strong> In recent years, Large Language Models such as GPT-3 showed remarkable capabilities in performing NLP tasks in the zero and few shot settings. On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations. In this paper we evaluate the ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on. We denote the models fine-tuned with this pipeline with the name Calculon and we test them in the task of performing additions, subtractions and multiplications on the same test sets of GPT-3. Results show an increase of accuracy of 63% in the five-digit addition task. Moreover, we demonstrate the importance of the decomposition pipeline introduced, since fine-tuning the same Language Model without decomposing numbers results in 0% accuracy in the five-digit addition task.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2996.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2996.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calculon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calculon (GPT-2 fine-tuned with number decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 Small model (~117M parameters) fine-tuned with a pipeline that translates numbers into natural-language digit-wise decompositions (units, tens, hundreds, ...) and trains the model to compute by summing corresponding magnitude tokens and reconstructing the numeric result.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (Calculon)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 Small (~117M parameters) transformer, pre-trained then fine-tuned for 25 epochs with Adam (lr=1e-4, batch size=32). Greedy decoding used at inference. Fine-tuned on 12k observations per addition/subtraction task and 3k for 2-digit multiplication using the decomposition pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction of 2-, 3-, 4-, and 5-digit integers; multiplication of 2-digit integers.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns a digit-wise algorithmic procedure by representing numbers as decomposed magnitude tokens (e.g., '4 units, 5 tens, 9 hundreds') and learning to align and sum/subtract corresponding magnitude positions; effectively an emergent, learned positional-digit computation rather than raw memorization of whole-number outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Gradient-based input saliency (Shrikumar et al. method via Ecco) shows that when predicting a given output digit, the highest-saliency input tokens are the input digits at the corresponding magnitude (e.g., unit digits when predicting the result's units), indicating the model attends to and uses corresponding magnitude tokens; generalization to test sets with unseen number pairs (no overlap with training couples) with large accuracy gains (paper reports a 63% increase for five-digit addition compared to baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Decomposition does not suffice for multiplication: Calculon performs poorly on 2-digit multiplication, suggesting the learned digitwise summation/subtraction procedure is not enough to implement multiplication algorithms; also the paper notes that providing decomposition in few-shot prompts to GPT-3 hurts performance, implying the mechanism benefits rely on supervised fine-tuning rather than simple exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised fine-tuning with a decomposition pipeline (transform numbers to natural-language digit/magnitude descriptions before computing), compared to baseline fine-tuning and a 'spaced' digit-tokenization intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Dramatically improved addition/subtraction accuracy across digit lengths (especially 4- and 5-digit tasks) compared to baseline fine-tuning without decomposition; enables generalization to unseen numbers; did not solve multiplication task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports a 63% increase in accuracy in the five-digit addition task relative to baseline; Calculon obtains 'high accuracy scores' on addition/subtraction tasks (detailed table in paper) but poor performance on 2-digit multiplication; baseline reported 0% accuracy on five-digit addition when no decomposition was used; exact per-task numbers are provided in the paper's Table 2 (not all numeric values are reproduced in text).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails on multiplication (2-digit multiplication remained poor); performance depends on supervised fine-tuning with decomposed inputs — simple token spacing or few-shot decomposition does not fully replicate the benefit. Potential sensitivity to prompt length/format when applied without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors liken the decomposition pipeline to the way children are taught (digit-wise addition using units/tens/hundreds). No formal human benchmark; compared qualitatively to algorithmic/symbolic procedures: decomposition enables learning a digitwise algorithmic-like procedure but is insufficient to implement multiplication algorithms fully.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2996.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2996.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 fine-tuned without number decomposition (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 Small model fine-tuned on raw string inputs containing number pairs and the result (no decomposition), used as a baseline to assess whether decomposition is necessary for learning arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (baseline fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 Small (~117M parameters) fine-tuned under the same regimen as Calculon but trained on raw numeric strings (e.g., '1201 plus 1302 = 2503') without decomposing numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction of 2-, 3-, 4-, and 5-digit integers; 2-digit multiplication (same tasks as Calculon for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Appears to rely on memorization/pattern-matching of seen examples and cannot learn a robust digitwise computation; fails to develop internal digit-position algorithmic representations when trained on undecomposed numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: baseline fine-tuned GPT-2 obtains low accuracy on most tasks except some competence on 2-digit addition (reported 53.35% accuracy). It achieves zero or near-zero accuracy on 4- and 5-digit addition/subtraction test sets, showing lack of generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None beyond the poor performance itself; the baseline's behavior serves as evidence that naive fine-tuning without decomposition does not lead to digitwise algorithm learning.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised fine-tuning on raw numeric string observations (baseline condition).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Poor learning and generalization on multi-digit arithmetic (especially 4- and 5-digit tasks); only moderate performance on easier 2-digit addition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>2-digit addition: 53.35% accuracy reported; 4- and 5-digit addition/subtraction: zero or near-zero accuracy reported in paper; explicit numeric table is in the paper's Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Cannot generalize to unseen multi-digit inputs; effectively fails to learn digitwise computation rules from undecomposed examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; fails where symbolic algorithms (school arithmetic) would generalize, demonstrating limitation vs algorithmic/symbolic computation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2996.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2996.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spaced pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spaced-digit pipeline (digit tokenization via spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning approach that inserts spaces between digits (e.g., '868' -> '8 6 8') to encourage the BPE tokenizer to produce single-digit tokens, without providing magnitude labels (units/tens/etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (spaced-digit fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 Small fine-tuned on training strings where digits are separated by spaces so each digit tends to be tokenized individually; otherwise training regimen matches other GPT-2 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction of 2- to 5-digit integers (same test sets as other GPT-2 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Improves digit-level token representations (reduces subword splitting of numbers) so the model can operate more directly on per-digit tokens; however, lacking explicit magnitude labels, it must infer positional significance implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: Spaced pipeline significantly improves accuracy over the baseline in addition tasks and slightly in subtraction, consistent with prior findings that digit-level tokenization helps numeracy; but it underperforms the full decomposition pipeline (Calculon), indicating that magnitude labeling helps further.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Spaced alone is insufficient to reach the full benefit of explicit decomposition; multiplication still not solved and multi-digit tasks remain worse than decomposition-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Supervised fine-tuning with digits separated by spaces (tokenization-focused intervention).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved accuracy relative to baseline, especially on addition tasks, but less improvement than full decomposition. The paper reports Calculon outperforms Spaced by ~15% on 5-digit addition and by a much larger margin on 5-digit subtraction (paper quotes ~75% for one of these comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Spaced approach yields a 'remarkable improvement' over baseline for addition and a small improvement for subtraction; exact per-task numbers are in Table 2 of the paper. Calculon still outperforms Spaced notably on 5-digit tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Does not match full decomposition performance; still struggles on multiplication and higher-digit tasks compared to decomposition-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human comparison; supports prior work showing tokenization impacts numeracy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2996.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2996.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (original few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (largest architecture from 'Language models are few-shot learners')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The ~175B-parameter GPT-3 model evaluated in the referenced arithmetic benchmark in a few-shot (in-context) setting; shows good performance on small-digit arithmetic (2-3 digits) but degrades on larger digit counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 largest architecture (~175B parameters) as reported in Brown et al. (2020). Evaluated here in few-shot in-context settings (original few-shot prompts from Brown et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction of 2- to 5-digit integers; multiplication (2-digit) as in Brown et al. benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>In-context pattern matching / few-shot in-context learning that can reproduce examples seen in the prompt and generalize to small perturbations; not a learned digitwise algorithm via fine-tuning in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Prior Brown et al. results (cited) show strong performance on 2-3 digit tasks and degraded accuracy as digits increase; in this paper GPT-3's original few-shot (no decomposition) results are cited as higher than when the decomposition pipeline is given in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Fails to generalize reliably to 4- and 5-digit arithmetic; adding decomposed few-shot examples (longer prompts) reduced performance, suggesting reliance on compact in-context pattern rather than robust digitwise algorithmic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting (in-context examples) with standard undecomposed numeric examples (original GPT-3 few-shot as reported in Brown et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Original few-shot prompting yields relatively good performance on 2-3 digit tasks but poor performance on higher-digit tasks; outperformed the few-shot-with-decomposition variant in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper references Brown et al. results (GPT-3 FS) showing high accuracy on two- and three-digit addition/subtraction and poor accuracy on four- and five-digit tasks; explicit figures are reported in Brown et al. and summarized in this paper's Table 2 (this paper does not replicate all numeric values for GPT-3 in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Performance drops sharply as digit count increases; sensitive to prompt formatting and length — adding decomposition examples in prompts reduced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human comparison in this paper; behavior contrasts with symbolic algorithms (which would algorithmically handle arbitrary-digit arithmetic) and with Calculon's learned digitwise approach (which required supervised fine-tuning).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2996.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2996.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 FS decomp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 few-shot with decomposition examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3 in a few-shot (no-parameter-update) setting where the few-shot examples are formatted using the decomposition pipeline (natural-language digit/magnitude descriptions) to test whether decomposition in-context helps GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B) - few-shot with decomposition examples</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-3 largest model (~175B) evaluated with a prompt that includes 4 few-shot examples formatted using the paper's decomposition pipeline; due to API limits, evaluation was done on the first 100 observations of each test set with temperature=0.7 sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and subtraction of 2- to 5-digit integers (same test sets), using decomposition-structured few-shot examples instead of raw examples.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Hypothesized to possibly help by exposing the model to digitwise algorithmic steps in-context, but empirically appears to disrupt GPT-3's in-context pattern-matching mechanism (long/decomposed prompts reduce performance).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: GPT-3 with decomposition few-shot examples produced 'much lower results' compared to the original GPT-3 few-shot results, suggesting the decomposition prompt harmed GPT-3's ability to leverage its pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The drop in performance is direct evidence that few-shot decomposition does not induce GPT-3 to use a digitwise computation strategy in-context (at least with the long natural-language decompositions used in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting with decomposition-formatted examples (no parameter updates).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Decreased arithmetic performance compared to original few-shot prompts; authors hypothesize long decomposition prompts cause loss of focus on the task and prevent leveraging pretraining-learned patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Results for this condition were computed on the first 100 observations of each test set and reported as 'much lower' than GPT-3's original few-shot results; the paper does not give full numeric breakdown in-text for these 100-example evaluations (see Table 2 in paper for reported numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Long/natural-language decomposition few-shot prompts reduce GPT-3 performance; in-context exposure to explicit digitwise steps did not produce the same benefit that supervised fine-tuning provided for GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct comparison; result suggests that in-context demonstration alone is insufficient to make very large LMs behave like algorithmic/symbolic calculators when the demonstrations are long/verbose.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2996.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2996.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input saliency analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-based input saliency (Shrikumar et al. activation differences) via Ecco</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attribution method (DeepLIFT-style gradient-based propagation of activation differences) used to compute token-level saliency scores indicating influence of each input token on generated output tokens; applied here to inspect which input digits influence predicted output digits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning important features through propagating activation differences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Input-saliency attribution applied to Calculon (GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gradient-based attribution method (Shrikumar et al., 2017) implemented using the Ecco library to compute input-token saliency scores for the fine-tuned GPT-2 Calculon models.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Analysis applied to addition tasks (inspecting units, tens, hundreds predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Provides evidence that the model's internal representations and attention to input tokens are organized by digit magnitude: when predicting the result's unit digit, unit tokens in the inputs have highest saliency, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Saliency plots show the highest attribution to input tokens corresponding to the same magnitude position as the output digit being predicted (examples shown for units, tens, hundreds), supporting the claim that the model learned digit-position mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Attribution is correlational and does not prove a particular algorithm; saliency methods can be noisy and do not fully reveal causal internal state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Interpretability analysis (no intervention to the model weights).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Supported the hypothesis that decomposition-trained models learn to attend to corresponding magnitude tokens; used as evidence for digitwise internal representation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a performance metric — qualitative/quantitative saliency scores visualized in figure 1 of the paper (higher saliency on corresponding magnitude tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Interpretability evidence is limited to attribution correlations; does not explain how carries/cascading effects are implemented internally.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Saliency evidence aligns with the idea of human-like digitwise computation (attending to units when computing units), but does not establish an algorithm equivalent to symbolic multiplication.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Representing numbers in nlp: a survey and a vision <em>(Rating: 2)</em></li>
                <li>Learning important features through propagating activation differences <em>(Rating: 1)</em></li>
                <li>Ecco: An open source library for the explainability of transformer language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2996",
    "paper_id": "paper-251406206",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Calculon",
            "name_full": "Calculon (GPT-2 fine-tuned with number decomposition)",
            "brief_description": "A GPT-2 Small model (~117M parameters) fine-tuned with a pipeline that translates numbers into natural-language digit-wise decompositions (units, tens, hundreds, ...) and trains the model to compute by summing corresponding magnitude tokens and reconstructing the numeric result.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (Calculon)",
            "model_description": "GPT-2 Small (~117M parameters) transformer, pre-trained then fine-tuned for 25 epochs with Adam (lr=1e-4, batch size=32). Greedy decoding used at inference. Fine-tuned on 12k observations per addition/subtraction task and 3k for 2-digit multiplication using the decomposition pipeline.",
            "arithmetic_task_type": "Addition and subtraction of 2-, 3-, 4-, and 5-digit integers; multiplication of 2-digit integers.",
            "reported_mechanism": "Learns a digit-wise algorithmic procedure by representing numbers as decomposed magnitude tokens (e.g., '4 units, 5 tens, 9 hundreds') and learning to align and sum/subtract corresponding magnitude positions; effectively an emergent, learned positional-digit computation rather than raw memorization of whole-number outputs.",
            "evidence_for_mechanism": "Gradient-based input saliency (Shrikumar et al. method via Ecco) shows that when predicting a given output digit, the highest-saliency input tokens are the input digits at the corresponding magnitude (e.g., unit digits when predicting the result's units), indicating the model attends to and uses corresponding magnitude tokens; generalization to test sets with unseen number pairs (no overlap with training couples) with large accuracy gains (paper reports a 63% increase for five-digit addition compared to baseline).",
            "evidence_against_mechanism": "Decomposition does not suffice for multiplication: Calculon performs poorly on 2-digit multiplication, suggesting the learned digitwise summation/subtraction procedure is not enough to implement multiplication algorithms; also the paper notes that providing decomposition in few-shot prompts to GPT-3 hurts performance, implying the mechanism benefits rely on supervised fine-tuning rather than simple exposure.",
            "intervention_type": "Supervised fine-tuning with a decomposition pipeline (transform numbers to natural-language digit/magnitude descriptions before computing), compared to baseline fine-tuning and a 'spaced' digit-tokenization intervention.",
            "effect_of_intervention": "Dramatically improved addition/subtraction accuracy across digit lengths (especially 4- and 5-digit tasks) compared to baseline fine-tuning without decomposition; enables generalization to unseen numbers; did not solve multiplication task.",
            "performance_metrics": "Paper reports a 63% increase in accuracy in the five-digit addition task relative to baseline; Calculon obtains 'high accuracy scores' on addition/subtraction tasks (detailed table in paper) but poor performance on 2-digit multiplication; baseline reported 0% accuracy on five-digit addition when no decomposition was used; exact per-task numbers are provided in the paper's Table 2 (not all numeric values are reproduced in text).",
            "notable_failure_modes": "Fails on multiplication (2-digit multiplication remained poor); performance depends on supervised fine-tuning with decomposed inputs — simple token spacing or few-shot decomposition does not fully replicate the benefit. Potential sensitivity to prompt length/format when applied without fine-tuning.",
            "comparison_to_humans_or_symbolic": "Authors liken the decomposition pipeline to the way children are taught (digit-wise addition using units/tens/hundreds). No formal human benchmark; compared qualitatively to algorithmic/symbolic procedures: decomposition enables learning a digitwise algorithmic-like procedure but is insufficient to implement multiplication algorithms fully.",
            "uuid": "e2996.0"
        },
        {
            "name_short": "Baseline GPT-2",
            "name_full": "GPT-2 fine-tuned without number decomposition (baseline)",
            "brief_description": "A GPT-2 Small model fine-tuned on raw string inputs containing number pairs and the result (no decomposition), used as a baseline to assess whether decomposition is necessary for learning arithmetic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (baseline fine-tuned)",
            "model_description": "GPT-2 Small (~117M parameters) fine-tuned under the same regimen as Calculon but trained on raw numeric strings (e.g., '1201 plus 1302 = 2503') without decomposing numbers.",
            "arithmetic_task_type": "Addition and subtraction of 2-, 3-, 4-, and 5-digit integers; 2-digit multiplication (same tasks as Calculon for comparison).",
            "reported_mechanism": "Appears to rely on memorization/pattern-matching of seen examples and cannot learn a robust digitwise computation; fails to develop internal digit-position algorithmic representations when trained on undecomposed numbers.",
            "evidence_for_mechanism": "Empirical: baseline fine-tuned GPT-2 obtains low accuracy on most tasks except some competence on 2-digit addition (reported 53.35% accuracy). It achieves zero or near-zero accuracy on 4- and 5-digit addition/subtraction test sets, showing lack of generalization.",
            "evidence_against_mechanism": "None beyond the poor performance itself; the baseline's behavior serves as evidence that naive fine-tuning without decomposition does not lead to digitwise algorithm learning.",
            "intervention_type": "Supervised fine-tuning on raw numeric string observations (baseline condition).",
            "effect_of_intervention": "Poor learning and generalization on multi-digit arithmetic (especially 4- and 5-digit tasks); only moderate performance on easier 2-digit addition.",
            "performance_metrics": "2-digit addition: 53.35% accuracy reported; 4- and 5-digit addition/subtraction: zero or near-zero accuracy reported in paper; explicit numeric table is in the paper's Table 2.",
            "notable_failure_modes": "Cannot generalize to unseen multi-digit inputs; effectively fails to learn digitwise computation rules from undecomposed examples.",
            "comparison_to_humans_or_symbolic": "No direct comparison; fails where symbolic algorithms (school arithmetic) would generalize, demonstrating limitation vs algorithmic/symbolic computation.",
            "uuid": "e2996.1"
        },
        {
            "name_short": "Spaced pipeline",
            "name_full": "Spaced-digit pipeline (digit tokenization via spaces)",
            "brief_description": "A fine-tuning approach that inserts spaces between digits (e.g., '868' -&gt; '8 6 8') to encourage the BPE tokenizer to produce single-digit tokens, without providing magnitude labels (units/tens/etc.).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (spaced-digit fine-tuned)",
            "model_description": "GPT-2 Small fine-tuned on training strings where digits are separated by spaces so each digit tends to be tokenized individually; otherwise training regimen matches other GPT-2 experiments.",
            "arithmetic_task_type": "Addition and subtraction of 2- to 5-digit integers (same test sets as other GPT-2 experiments).",
            "reported_mechanism": "Improves digit-level token representations (reduces subword splitting of numbers) so the model can operate more directly on per-digit tokens; however, lacking explicit magnitude labels, it must infer positional significance implicitly.",
            "evidence_for_mechanism": "Empirical: Spaced pipeline significantly improves accuracy over the baseline in addition tasks and slightly in subtraction, consistent with prior findings that digit-level tokenization helps numeracy; but it underperforms the full decomposition pipeline (Calculon), indicating that magnitude labeling helps further.",
            "evidence_against_mechanism": "Spaced alone is insufficient to reach the full benefit of explicit decomposition; multiplication still not solved and multi-digit tasks remain worse than decomposition-trained model.",
            "intervention_type": "Supervised fine-tuning with digits separated by spaces (tokenization-focused intervention).",
            "effect_of_intervention": "Improved accuracy relative to baseline, especially on addition tasks, but less improvement than full decomposition. The paper reports Calculon outperforms Spaced by ~15% on 5-digit addition and by a much larger margin on 5-digit subtraction (paper quotes ~75% for one of these comparisons).",
            "performance_metrics": "Spaced approach yields a 'remarkable improvement' over baseline for addition and a small improvement for subtraction; exact per-task numbers are in Table 2 of the paper. Calculon still outperforms Spaced notably on 5-digit tasks.",
            "notable_failure_modes": "Does not match full decomposition performance; still struggles on multiplication and higher-digit tasks compared to decomposition-trained model.",
            "comparison_to_humans_or_symbolic": "No direct human comparison; supports prior work showing tokenization impacts numeracy.",
            "uuid": "e2996.2"
        },
        {
            "name_short": "GPT-3 (original few-shot)",
            "name_full": "GPT-3 (largest architecture from 'Language models are few-shot learners')",
            "brief_description": "The ~175B-parameter GPT-3 model evaluated in the referenced arithmetic benchmark in a few-shot (in-context) setting; shows good performance on small-digit arithmetic (2-3 digits) but degrades on larger digit counts.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (175B)",
            "model_description": "GPT-3 largest architecture (~175B parameters) as reported in Brown et al. (2020). Evaluated here in few-shot in-context settings (original few-shot prompts from Brown et al.).",
            "arithmetic_task_type": "Addition and subtraction of 2- to 5-digit integers; multiplication (2-digit) as in Brown et al. benchmark.",
            "reported_mechanism": "In-context pattern matching / few-shot in-context learning that can reproduce examples seen in the prompt and generalize to small perturbations; not a learned digitwise algorithm via fine-tuning in this paper's experiments.",
            "evidence_for_mechanism": "Prior Brown et al. results (cited) show strong performance on 2-3 digit tasks and degraded accuracy as digits increase; in this paper GPT-3's original few-shot (no decomposition) results are cited as higher than when the decomposition pipeline is given in the prompt.",
            "evidence_against_mechanism": "Fails to generalize reliably to 4- and 5-digit arithmetic; adding decomposed few-shot examples (longer prompts) reduced performance, suggesting reliance on compact in-context pattern rather than robust digitwise algorithmic representations.",
            "intervention_type": "Few-shot prompting (in-context examples) with standard undecomposed numeric examples (original GPT-3 few-shot as reported in Brown et al.).",
            "effect_of_intervention": "Original few-shot prompting yields relatively good performance on 2-3 digit tasks but poor performance on higher-digit tasks; outperformed the few-shot-with-decomposition variant in this paper.",
            "performance_metrics": "Paper references Brown et al. results (GPT-3 FS) showing high accuracy on two- and three-digit addition/subtraction and poor accuracy on four- and five-digit tasks; explicit figures are reported in Brown et al. and summarized in this paper's Table 2 (this paper does not replicate all numeric values for GPT-3 in-text).",
            "notable_failure_modes": "Performance drops sharply as digit count increases; sensitive to prompt formatting and length — adding decomposition examples in prompts reduced performance.",
            "comparison_to_humans_or_symbolic": "No direct human comparison in this paper; behavior contrasts with symbolic algorithms (which would algorithmically handle arbitrary-digit arithmetic) and with Calculon's learned digitwise approach (which required supervised fine-tuning).",
            "uuid": "e2996.3"
        },
        {
            "name_short": "GPT-3 FS decomp",
            "name_full": "GPT-3 few-shot with decomposition examples",
            "brief_description": "Evaluation of GPT-3 in a few-shot (no-parameter-update) setting where the few-shot examples are formatted using the decomposition pipeline (natural-language digit/magnitude descriptions) to test whether decomposition in-context helps GPT-3.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (175B) - few-shot with decomposition examples",
            "model_description": "Same GPT-3 largest model (~175B) evaluated with a prompt that includes 4 few-shot examples formatted using the paper's decomposition pipeline; due to API limits, evaluation was done on the first 100 observations of each test set with temperature=0.7 sampling.",
            "arithmetic_task_type": "Addition and subtraction of 2- to 5-digit integers (same test sets), using decomposition-structured few-shot examples instead of raw examples.",
            "reported_mechanism": "Hypothesized to possibly help by exposing the model to digitwise algorithmic steps in-context, but empirically appears to disrupt GPT-3's in-context pattern-matching mechanism (long/decomposed prompts reduce performance).",
            "evidence_for_mechanism": "Empirical: GPT-3 with decomposition few-shot examples produced 'much lower results' compared to the original GPT-3 few-shot results, suggesting the decomposition prompt harmed GPT-3's ability to leverage its pretraining.",
            "evidence_against_mechanism": "The drop in performance is direct evidence that few-shot decomposition does not induce GPT-3 to use a digitwise computation strategy in-context (at least with the long natural-language decompositions used in this paper).",
            "intervention_type": "Few-shot prompting with decomposition-formatted examples (no parameter updates).",
            "effect_of_intervention": "Decreased arithmetic performance compared to original few-shot prompts; authors hypothesize long decomposition prompts cause loss of focus on the task and prevent leveraging pretraining-learned patterns.",
            "performance_metrics": "Results for this condition were computed on the first 100 observations of each test set and reported as 'much lower' than GPT-3's original few-shot results; the paper does not give full numeric breakdown in-text for these 100-example evaluations (see Table 2 in paper for reported numbers).",
            "notable_failure_modes": "Long/natural-language decomposition few-shot prompts reduce GPT-3 performance; in-context exposure to explicit digitwise steps did not produce the same benefit that supervised fine-tuning provided for GPT-2.",
            "comparison_to_humans_or_symbolic": "No direct comparison; result suggests that in-context demonstration alone is insufficient to make very large LMs behave like algorithmic/symbolic calculators when the demonstrations are long/verbose.",
            "uuid": "e2996.4"
        },
        {
            "name_short": "Input saliency analysis",
            "name_full": "Gradient-based input saliency (Shrikumar et al. activation differences) via Ecco",
            "brief_description": "An attribution method (DeepLIFT-style gradient-based propagation of activation differences) used to compute token-level saliency scores indicating influence of each input token on generated output tokens; applied here to inspect which input digits influence predicted output digits.",
            "citation_title": "Learning important features through propagating activation differences",
            "mention_or_use": "use",
            "model_name": "Input-saliency attribution applied to Calculon (GPT-2)",
            "model_description": "Gradient-based attribution method (Shrikumar et al., 2017) implemented using the Ecco library to compute input-token saliency scores for the fine-tuned GPT-2 Calculon models.",
            "arithmetic_task_type": "Analysis applied to addition tasks (inspecting units, tens, hundreds predictions).",
            "reported_mechanism": "Provides evidence that the model's internal representations and attention to input tokens are organized by digit magnitude: when predicting the result's unit digit, unit tokens in the inputs have highest saliency, etc.",
            "evidence_for_mechanism": "Saliency plots show the highest attribution to input tokens corresponding to the same magnitude position as the output digit being predicted (examples shown for units, tens, hundreds), supporting the claim that the model learned digit-position mapping.",
            "evidence_against_mechanism": "Attribution is correlational and does not prove a particular algorithm; saliency methods can be noisy and do not fully reveal causal internal state transitions.",
            "intervention_type": "Interpretability analysis (no intervention to the model weights).",
            "effect_of_intervention": "Supported the hypothesis that decomposition-trained models learn to attend to corresponding magnitude tokens; used as evidence for digitwise internal representation.",
            "performance_metrics": "Not a performance metric — qualitative/quantitative saliency scores visualized in figure 1 of the paper (higher saliency on corresponding magnitude tokens).",
            "notable_failure_modes": "Interpretability evidence is limited to attribution correlations; does not explain how carries/cascading effects are implemented internally.",
            "comparison_to_humans_or_symbolic": "Saliency evidence aligns with the idea of human-like digitwise computation (attending to units when computing units), but does not establish an algorithm equivalent to symbolic multiplication.",
            "uuid": "e2996.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2,
            "sanitized_title": "injecting_numerical_reasoning_skills_into_language_models"
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2,
            "sanitized_title": "do_nlp_models_know_numbers_probing_numeracy_in_embeddings"
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2,
            "sanitized_title": "analysing_mathematical_reasoning_abilities_of_neural_models"
        },
        {
            "paper_title": "Representing numbers in nlp: a survey and a vision",
            "rating": 2,
            "sanitized_title": "representing_numbers_in_nlp_a_survey_and_a_vision"
        },
        {
            "paper_title": "Learning important features through propagating activation differences",
            "rating": 1,
            "sanitized_title": "learning_important_features_through_propagating_activation_differences"
        },
        {
            "paper_title": "Ecco: An open source library for the explainability of transformer language models.",
            "rating": 1,
            "sanitized_title": "ecco_an_open_source_library_for_the_explainability_of_transformer_language_models"
        }
    ],
    "cost": 0.015244999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition
June 2022</p>
<p>Matteo Muffo matteo@indigo.ai 
Indigo.ai Via Torino 61MilanItaly</p>
<p>Aldo Cocco 
Indigo.ai Via Torino 61MilanItaly</p>
<p>Enrico Bertino 
Indigo.ai Via Torino 61MilanItaly</p>
<p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition</p>
<p>Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)
the 13th Conference on Language Resources and Evaluation (LREC 2022)MarseilleJune 2022Language Resources Association (ELRA), licensed under CC-BY-NC-4.0 291Transformer Language Modelsarithmetic operationsnumber decomposition
In recent years, Large Language Models such as GPT-3 showed remarkable capabilities in performing NLP tasks in the zero and few shot settings. On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations. In this paper we evaluate the ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on. We denote the models fine-tuned with this pipeline with the name Calculon and we test them in the task of performing additions, subtractions and multiplications on the same test sets of GPT-3. Results show an increase of accuracy of 63% in the five-digit addition task. Moreover, we demonstrate the importance of the decomposition pipeline introduced, since fine-tuning the same Language Model without decomposing numbers results in 0% accuracy in the five-digit addition task.</p>
<p>Introduction</p>
<p>The publication of GPT-3 (Brown et al., 2020) had a relevant impact on Natural Language Processing, showing that it is possible to leverage a Large Language Model (LLM) to perform downstream tasks in the zero and few shot setting. However, although GPT-3 showed on-the-fly reasoning capabilities in tasks such as two or three-digit operations, it struggles with fivedigit operations. This suggests that a LLM such as GPT-3 did not effectively learn to perform arithmetic operations and is not able to generalize its ability to perform sums or subtractions to any number of digits. With this work we want to assess if Transformer Language Models have enough reasoning capabilities to learn to perform arithmetic operations of unseen numbers. To do so, we introduce Calculon, a GPT-2 (Radford et al., 2019) model fine-tuned to perform arithmetic operations between decomposed numbers. In particular, Calculon is trained to perform arithmetic operations following a pipeline that decomposes the numbers in digit form (e.g. 18954 = 4 units, 5 tens, 9 hundreds, 8 thousands, 1 tens of thousands). The underlying idea is to teach LMs to do computations as children learn at school, processing units with units, tens with tens, and so on. Following this pipeline, Calculon reaches remarkable levels of accuracy, even in the four and five-digit tasks where GPT-3 obtains poor results. To validate the importance of the pipeline here proposed, we fine-tune a GPT-2 model on the same datasets of Calculon without decomposing numbers. In this setting, the fine-tuned GPT-2 network reaches very poor performances on the four and five digits tasks, demonstrating that the decomposition pipeline is a valid approach to make LMs effectively learn arith-metic. Finally, we experiment if it is possible to improve the performances of GPT-3 with the proposed decomposition pipeline via few-shot priming (no parameters update). The results obtained are worse than those of the original GPT-3 publication. In light of the experiments carried out we conclude that Transformer Language Models have enough reasoning capabilities to effectively learn to perform addition and subtraction operations, but a higher level of reasoning is required in order to learn to perform multiplications. In section 2 we review literature related to our work, in section 3 we describe the decomposition pipeline that we propose, in section 4 we describe the data used to fine-tune LMs while in section 5 we present and discuss the results obtained. We provide data and code used to reproduce the experiments 1 . Our code is based on the Huggingface Transformers library (Wolf et al., 2020).</p>
<p>Related Work</p>
<p>Numeracy capabilities of NLP models have been widely experimented in literature. Numeracy benchmarks in NLP range from Arithmetic Word Problems (Hendrycks et al., 2021) to Magnitude Comparison (Naik et al., 2019;Wallace et al., 2019) and Measurement Estimation (Zhang et al., 2020). We refer to Thawani et al. (2021) for a complete survey about the topic. Saxton et al. (2019) propose an analysis about mathematical reasoning abilities of neural NLP models by testing them on several mathematical problems such as finding the solution of equations or computing derivatives. The conclusion of this work is that a Transformer LM obtains moderate performances in the tasks analysed, suggesting that there is large room for improving mathematical reasoning capabilities of generative NLP models. Brown et al. (2020) introduced the task of performing computations by generating a numeric answer given an input prompt in natural language. In this work, the authors test the ability of GPT-3 to perform additions, subtractions, and multiplications in the zero and few-shot settings, focusing on numbers from 1 to 5 digits. Results show high levels of accuracy in two-and three-digit addition and subtraction operations, followed by low levels of accuracy as digits increase (four and five). To complete this section, we include papers studying how different tokenization techniques affect numeracy capabilities of NLP models. Thawani et al. (2021) underline that sub-word tokenizers such as BPE (Sennrich et al., 2016) and WordPiece (Wu et al., 2016) split numbers in arbitrary tokens (e.g. 1234 can be split in 12-34, 1-234, . . . ). Moreover, Wallace et al. (2019) demonstrate that sub-word representations provided by BERT (Devlin et al., 2019) are less effective in representing numbers with respect to character-level representations used by ELMo (Peters et al., 2018) when probing token embedding methods in numeracy tasks. Similarly, Geva et al. (2020) show that representing numbers with a digit-level tokenizer instead of Word-Piece improves the performances of their GenBERT model. Compared to these publications, with our decomposition pipeline we propose an alternative way of representing numbers for LMs. Nogueira et al. (2021) propose a work similar to ours in which they fine-tune a T5 model (Raffel et al., 2020) to perform arithmetic operations, providing numbers written in different forms as input to the model. However, there is a crucial difference between Nogueira et al. (2021) and our work: while the former provides manipulated numbers as input to the Transformer LM, in our work we use inputs without any manipulation and we assume that the LM will be able to generate decomposed numbers. We believe that our approach, compared to the work of Nogueira et al. (2021), will produce more robust results due to the fact that no pre-processing operation is performed over input data.</p>
<p>Methodology</p>
<p>As outlined in section 1, in this work we want to assess if Transformer Language Models have the reasoning capabilities to learn how to perform arithmetic operations between unseen numbers composed of a variable amount of digits. In particular, consistently with Brown et al. (2020), we focus on the tasks of summing and subtracting 2, 3, 4, and 5 digit numbers and multiplying 2 digit numbers. In our experiments we fine-tuned a pre-trained GPT-2 Language Model 2 (Radford et al., 2019) to perform computations using different approaches. We underline that our experiments are conducted in a sequence-to-sequence framework,</p>
<p>The main approach we experimented is denoted as decomposition pipeline. The idea is to teach the LM to do calculations in the same way children are taught in school. For instance, if we consider the 5 digit addition task, given the two numbers involved in the sum and the relative result, we generate the input string as follows. First, we translate both numbers in their decomposition form (e.g. 868 becomes 8 units, 6 tens, 8 hundreds), then we sum the decomposed numbers to obtain a decomposed number as the output. Finally we reconstruct the number representing the result of the addition from its decomposed form. We underline again that all these steps are written in natural language and the string constructed will be an observation of the training set used to fine-tune GPT-2. We will refer to LMs fine-tuned with this approach under the name of Calculon.</p>
<p>The second approach that we experimented, named as baseline, is an approach in which no manipulation is done on numbers. An observation relative to this method will be simply a string containing the two numbers involved in the computation followed by the final result.</p>
<p>In section 2 we mentioned the works of Wallace et al. (2019) and Geva et al. (2020), which evidenced that character-level tokenizers are preferable to sub-word tokenizers when processing numbers. For this reason we experiment with another approach, denoted as spaced pipeline, in which we want to assess if a transformer LM can tokenize the digits singularly and solve the operations. An observation relative to this approach will be a string where we transform the two numbers into the spaced form (e.g. 868 becomes 8 6 8), then we compute the operation between the spaced numbers and finally we reconstruct the resulting number starting from its spaced form. The idea behind this approach is that the spacing of the digits allows the BPE tokenizer (Sennrich et al., 2016) used by GPT-2 to tokenize each digit singularly.</p>
<p>At inference time, for all the approaches, the input for the fine-tuned model is a string containing two numbers and an arithmetic operation. If at the end of the generated string there is the number corresponding to the result of the operation, the observation is considered correct. In table 1 we provide examples of training observations and inference inputs for each of the studied approaches.</p>
<p>The last experiment that we conducted is about GPT-3 Language Model. In particular, with this set of tests we want to assess if GPT-3 can benefit from the decomposition pipeline in a few-shot setting (without any parameter update). In this case the experiments consist of evaluating GPT-3 in the same tasks mentioned at the beginning of this section, but providing in the input</p>
<p>Approach Observation Calculon</p>
<p>Compute with pipeline 1201 plus 1302. Translate from number to decomposition: 1201 = 1 units, 0 tens, 2 hundreds, 1 thousands. Translate from number to decomposition: 1302 = 2 units, 0 tens, 3 hundreds, 1 thousands. Sum 1 units, 0 tens, 2 hundreds, 1 thousands + 2 units, 0 tens, 3 hundreds, 1 thousands = 3 units, 0 tens, 5 hundreds, 2 thousands. Translate from decomposition to number: 3 units, 0 tens, 5 hundreds, 2 thousands = 2503  </p>
<p>Baseline</p>
<p>Data and training details</p>
<p>For the addition and subtraction operations, we generate training sets of 12000 observations each. In particular for each N ∈ {3, 4, 5} we randomly sample 3000 couples of integer numbers (n 1 , n 2 ) i , with (n 1 , n 2 ) i ∈ {10 N −1 , . . . , 10 N − 1} 2 , ∀i ∈ {1, . . . , 3000}. Similarly, for N = 2 we randomly sample 3000 couples of numbers (n 1 , n 2 ) i ∈ {0, . . . , 99} 2 (one-digit numbers are included). We then join all the couples created (obtaining a set of 12000 couples) and we compute the results of the operations. At the end of this step, we obtain two vectors of results, r + and r − , where r +,i = n 1,i +n 2,i and r −,i = n 1,i −n 2,i , ∀i ∈ {1, . . . , 12000}. Finally, given a triplet (n 1 , n 2 , r) i , we generate a string according to the procedures described in section 3, depending on the selected approach. For the multiplication, we generate training sets following the same procedure explained above but, instead of sampling 12000 couples, we sample 3000 couples of numbers from the set {0, . . . , 99} 2 because we will only test the multiplications between two-digit numbers. At the end of this procedure we obtain 9 training sets, each of which corresponding to a combination operation-approach (e.g. addition-decomposition), that we use to fine-tune as many Language Models. Now, we want to underline some points relative to the generated training sets. First, by fixing the operation and varying the approach, the same couples of numbers are used to generate strings, so that couples of numbers are sampled once for each operation. Second, none of the couples present in a training set is in the test set relative to the same operation. The test sets used to evaluate our fine-tuned Language Models are the same used to evaluate GPT-3 in the arithmetic tasks 4 (Brown et al., 2020). The GPT-2 models fine-tuned in our experiments are GPT-2 Small architectures, which count ∼117M parameters. The GPT-3 model evaluated in our experiments corresponds to the biggest architecture proposed in Brown et al. (2020), which counts ∼175B parameters. We fine-tune each GPT-2 Language Model for 25 epochs with an initial learning rate of 10 −4 and a batch size of 32, using Adam (Kingma and Ba, 2017) as optimizer. For the experiments on GPT-3, due to limited resources available on the dedicated API, we evaluate the model only on the first 100 observations of each test set. We adopt a greedy decoding strategy for the GPT-2 models and a random sampling strategy with tempera-ture=0.7 for the GPT-3 generations.</p>
<p>A full GPT-input prompt reported in Appendix</p>
<p>Results and discussion</p>
<p>In table 2 we show the results obtained with the experiments described in section 3. The GPT-2 model fine-tuned without decomposition (Baseline row) obtains low accuracy scores in all tasks except two-digit addition, where achieves 53.35 accuracy. In particular, in the 4 and 5 addition and subtraction tasks it achieves zero or near-zero accuracy. This demonstrates that, without decomposing numbers, a GPT-2 Language Model is not able to learn to perform computations, especially between numbers with a higher number of digits. On the other hand, Calculon obtains high accuracy scores in all the tasks tested with the exception of 2Dx. This demonstrates that finetuning using the proposed decomposition pipeline effectively makes possible for a transformer Language Model to learn to do calculations. Here, we underline again that none of the couples of numbers composing the training set are in the relative test set, and hence we can conclude that Calculon has effectively learned to sum units with units, tens with tens, and so on and manage to perform arithmetic operations between unseen numbers. However, the results in the two digit multiplication task are poor, suggesting that number decomposition is not sufficient to solve this task and probably higher reasoning capabilities are needed to multiply numbers.  Table 2: Accuracy scores obtained in the experiments described in section 3. {2, 3, 4, 5}D{+, -} represents 2, 3, 4 or 5 addition or subtraction tasks. 2Dx represents the 2 digit multiplication task. GPT-3 FS refers to results obtained by GPT-3 in the few-shot setting (results in this row are those obtained in (Brown et al., 2020)). GPT-3 FS decomp refers to results obtained by GPT-3 using the decomposition pipeline in few shot examples. Results relative to this last experiment are obtained over the first 100 observations of each test set exclusively.</p>
<p>In figure 1 we report input saliency scores obtained from the addition-Calculon model. These scores are obtained using the gradient-based method described in Shrikumar et al. (2017) and can be interpreted as the influence of each token related to the generated one (highlighted in purple in pictures). Figures and saliency scores are obtained using the Ecco package (Alammar, 2021). We notice that, when analysing the unit digit resulting from the addition (figure 1a) the digits with highest saliency scores are those relative to units in the two numbers which are summed (namely 1 and 2). A similar behavior is present in figures 1b and 1c for tens and hundreds respectively. This indicates that Calculon models effectively learn that units must be summed with units and so on in order to perform a correct addition.</p>
<p>We wanted to assess that the benefits brought by the decomposition pipeline are not due only to the fact that digits are tokenized separately. We then conducted the Spaced experiments, in which we separated digits with a blank space but without indicating if a digit corresponds to units, tens, and so on. The results obtained with these experiments (Spaced row) show a remarkable improve of accuracy with respect to the baseline in the addition tasks followed by a small improve in the subtraction tasks. However Calculon maintains a further improvement with respect to the Spaced approach, with an accuracy gain of almost 15% in the 5D+ task and 75% in the 5D-task. This indicates that providing magnitude information of digits when decomposing numbers significantly helps LMs when performing arithmetic operations. Moreover, the gap in accuracy scores between baseline and Spaced approaches is consistent with the work of Wallace et al. (2019) and Geva et al. (2020). Lastly, observing the results obtained by GPT-3, we notice that decomposing numbers in the few-shot examples (row GPT-3 FS decomp) leads to much lower results with respect to the original GPT-3 results (row GPT-3 FS), in which no manipulation is performed over numbers in the few-shot examples. We hypothesize that receiving a quite long input prompt GPT-3 loses the focus on the task of performing an arithmetic operation and the decomposition prevents it from leveraging on the calculations seen during the pretrain.</p>
<p>Conclusions and future work</p>
<p>In this work we presented Calculon, a GPT-2 Language Model fine-tuned to perform arithmetic operations following a pipeline that decomposes numbers before the computations. We showed that a Transformer Language Model can effectively learn to perform calculations generalizing to unseen numbers when fine-tuned with the decomposition pipeline proposed. Moreover, when we fine-tune on the same task but without the number decomposition, the same GPT-2 network reaches very poor results, proving that the decomposition pipeline effectively brings benefit during the training. On the other hand, we showed that adopting the same decomposition pipeline when providing few shot examples to GPT-3 leads to very bad results, suggesting that decomposition does not bring the same benefit in the few shot setting. We demonstrated that, by decomposing numbers, a Transformer LM such as GPT-2 has the reasoning capabilities to learn during finetuning the rules and procedures to perform additions and subtractions, but the same does not hold for the multiplication operation. There may be a variety of future works related to our experiments. First, consistently with Brown et al. (2020), we tested LMs on up to five-digit operations, but it can be interesting to assess if decomposition brings the same benefit for operations involving a higher number of digits. Secondly, it may be interesting to evaluate why all the tested models struggle with multiplication and if the latter is an operation that requires a level of reasoning unattainable by current linguistic models. In addition, it will be useful to investigate how the number of observations in the training sets affects the performances of fine-tuned GPT-2 models in the studied tasks. Regarding the experiments on GPT-3, it can be investigated whether a more compact formulation of the decomposition pipeline can also bring benefits in the context of few-shot learning. Finally, it can be interesting to extend the experiments presented in this work to other Transformer LMs such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2019).</p>
<p>(a) Input saliency scores of tokens preceding the digit 3 (in purple).</p>
<p>(b) Input saliency scores of tokens preceding the digit 8 (in purple).</p>
<p>(c) Input saliency scores of tokens preceding the digit 8 (in purple). Figure 1: Input saliency scores for the addition-Calculon model.</p>
<p>Appendix: GPT-3 input prompt</p>
<p>In the following lines we report the addition input prompt used when evaluating GPT-3 with decomposition (row GPT-3 FS decomp of </p>
<p>Table 1 :
1Examples of addition training observations for the considered approaches. Bold sub-strings represent input prompts provided to LMs at inference time. The same examples for the subtraction and multiplication tasks can be obtained substituting {plus, +, sum} with {minus, -, subtract} and {times, * , multiply} respectively. prompt 3 only 4 few-shot examples with the decomposition pipeline that we introduced.</p>
<p>table 2 ).
2{number1} and {number2} are replaced by the numbers involved in the computation. The string is composed by 4 few-shot examples which follow the decomposition pipeline introduced in section 3.This application makes an arithmetic 
operation decomposing the input 
numbers. </p>
<h3></h3>
<p>Compute with pipeline 28 plus 
39. Translate from number to 
decomposition: 28 = 2 tens, 8 
units. Translate from number to 
decomposition: 39 = 3 tens, 9 
units. Sum 8 units, 2 tens + 9 
units, 3 tens = 7 units, 6 tens. 
Translate from decomposition to 
number: 6 tens, 7 units = 67 </p>
<h3></h3>
<p>Compute with pipeline 804 plus 
121. Translate from number to 
decomposition: 804 = 8 hundreds, 
0 tens, 4 units. Translate from 
number to decomposition: 121 = 
1 hundreds, 2 tens, 1 units. Sum 
4 units, 0 tens, 8 hundreds + 1 
units, 2 tens, 1 hundreds = 5 units, 
2 tens, 9 hundreds. Translate 
from decomposition to number: 9 </p>
<p>hundreds, 2 tens, 5 units = 925 </p>
<h3></h3>
<p>Compute with pipeline 1201 plus 
1302. Translate from number 
to decomposition: 1201 = 1 
thousands, 2 hundreds, 0 tens, 1 
units. Translate from number to 
decomposition: 1302 = 1 thousands, 
3 hundreds, 0 tens, 2 units. Sum 
1 units, 0 tens, 2 hundreds, 1 
thousands + 2 units, 0 tens, 3 
hundreds, 1 thousands = 3 units, 
0 tens, 5 hundreds, 2 thousands. 
Translate from decomposition to 
number: 2 thousands, 5 hundreds, 
0 tens, 3 units = 2503 </p>
<h3></h3>
<p>Compute with pipeline 97734 plus 
86328. Translate from number to 
decomposition: 97734 = 9 tens of 
thousands, 7 thousands, 7 hundreds, 
3 tens, 4 units. Translate from 
number to decomposition: 86328 = 
8 tens of thousands, 6 thousands, 
3 hundreds, 2 tens, 8 units. Sum 
4 units, 3 tens, 7 hundreds, 7 
thousands, 9 tens of thousands 
+ 8 units, 2 tens, 3 hundreds, 
6 thousands, 8 tens of thousands </p>
<p>Available at: https://github.com/mmuffo94/TransformerLM arithmetics
Available at https://huggingface.co/gpt2 in which the Language Model receives a string as input and provides a string as output that may contain the number corresponding to the correct answer.</p>
<p>Ecco: An open source library for the explainability of transformer language models. J Alammar, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsAlammar, J. (2021). Ecco: An open source library for the explainability of transformer language models. In Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. Association for Computational Linguistics.</p>
<p>. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, Amodei , D , Language models are few-shot learnersBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Ka- plan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding.</p>
<p>Injecting numerical reasoning skills into language models. M Geva, A Gupta, J Berant, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsGeva, M., Gupta, A., and Berant, J. (2020). Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pages 946- 958, Online, July. Association for Computational Linguistics.</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, Kingma, D. P. and Ba, J. (2017). Adam: A method for stochastic optimization.</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, translation, and comprehensionLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, trans- lation, and comprehension.</p>
<p>Exploring numeracy in word embeddings. A Naik, A Ravichander, C Rose, E Hovy, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsNaik, A., Ravichander, A., Rose, C., and Hovy, E. (2019). Exploring numeracy in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374-3380, Florence, Italy, July. Association for Computational Linguistics.</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, Lin , J , Nogueira, R., Jiang, Z., and Lin, J. (2021). Investigat- ing the limitations of transformers with simple arith- metic tasks.</p>
<p>. M E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Deep contextualized word representationsPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. (2018). Deep contextualized word representations.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are un- supervised multitask learners.</p>
<p>Exploring the limits of transfer learning. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, with a unified text-to-text transformerRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Analysing mathematical reasoning abilities of neural models. D Saxton, E Grefenstette, F Hill, P Kohli, Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. (2019). Analysing mathematical reasoning abilities of neural models.</p>
<p>Neural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsLong Papers)Sennrich, R., Haddow, B., and Birch, A. (2016). Neu- ral machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1715-1725, Berlin, Germany, August. Association for Computational Linguistics.</p>
<p>Learning important features through propagating activation differences. A Shrikumar, P Greenside, A Kundaje, abs/1704.02685CoRRShrikumar, A., Greenside, P., and Kundaje, A. (2017). Learning important features through propagating ac- tivation differences. CoRR, abs/1704.02685.</p>
<p>Representing numbers in nlp: a survey and a vision. A Thawani, J Pujara, P A Szekely, F Ilievski, Thawani, A., Pujara, J., Szekely, P. A., and Ilievski, F. (2021). Representing numbers in nlp: a survey and a vision.</p>
<p>Do NLP models know numbers? probing numeracy in embeddings. E Wallace, Y Wang, S Li, S Singh, M Gardner, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsWallace, E., Wang, Y., Li, S., Singh, S., and Gard- ner, M. (2019). Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China, November. Association for Computa- tional Linguistics.</p>
<p>. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. (2020).</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Huggingface's transformers: State-of-the-art natural language processing.</p>
<p>Google's neural machine. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, J Klingner, A Shah, M Johnson, X Liu, Łukasz Kaiser, S Gouws, Y Kato, T Kudo, H Kazawa, K Stevens, G Kurian, N Patil, W Wang, C Young, J Smith, J Riesa, A Rudnick, O Vinyals, G Corrado, M Hughes, J Dean, translation system: Bridging the gap between human and machine translationWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Łukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. (2016). Google's neural machine translation sys- tem: Bridging the gap between human and machine translation.</p>
<p>Do language embeddings capture scales?. X Zhang, D Ramachandran, I Tenney, Y Elazar, Roth , D , Findings of the Association for Computational Linguistics: EMNLP 2020. Online, November. Association for Computational LinguisticsZhang, X., Ramachandran, D., Tenney, I., Elazar, Y., and Roth, D. (2020). Do language embeddings cap- ture scales? In Findings of the Association for Com- putational Linguistics: EMNLP 2020, pages 4889- 4896, Online, November. Association for Computa- tional Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
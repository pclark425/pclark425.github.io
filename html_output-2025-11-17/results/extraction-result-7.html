<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-1.html">extraction-schema-1</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games or interactive fiction, focusing on how they use memory to improve task performance, including types of memory used, memory representations, and performance comparisons with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-a32688780cc763b1d0b2d59f9677521779ee6612</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games or interactive fiction, focusing on how they use memory to improve task performance, including types of memory used, memory representations, and performance comparisons with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-DML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Planning with Dynamic Memory-guided Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm that integrates a Large Language Model (LLM) with Monte Carlo Tree Search (MCTS) to enhance action exploration in text-based games by utilizing both in-trial and cross-trial memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MC-DML Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The MC-DML agent employs a Large Language Model (LLM) as a prior policy within the PUCT algorithm, utilizing dynamic memory mechanisms to enhance decision-making in complex text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Various text-based games from the Jericho benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>In-trial memory and cross-trial memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>In-trial memory consists of the current trajectory history, while cross-trial memory includes reflections from previous failure trajectories, allowing the agent to adjust action value estimations dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>48.66 ± 1.89 in Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>38.33 ± 2.89 in Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison_reported</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits_summary</strong></td>
                            <td>The use of memory mechanisms significantly improves performance by enabling better long-term planning and exploration, allowing the agent to avoid previously failed actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_challenges</strong></td>
                            <td>The LLM's in-trial memory is limited to recent actions, which may not capture relevant information from earlier in the game, posing challenges for complex puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_training_method</strong></td>
                            <td>The agent is trained using a combination of reinforcement learning and memory-guided planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_training_method</strong></td>
                            <td>Reflections from previous failures are stored in cross-trial memory, which is updated during gameplay to inform future actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>The text-based games feature dynamic state spaces, sparse rewards, and require extensive exploration, making them complex environments for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Monte Carlo Planning with Large Language Model for Text-Based Game Agents', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Monte Carlo Tree Search with Large Language Model for Text-Based Game Agents <em>(Rating: 2)</em></li>
                <li>Interactive Fiction Games: A Colossal Adventure <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7",
    "paper_id": "paper-a32688780cc763b1d0b2d59f9677521779ee6612",
    "extraction_schema_id": "extraction-schema-1",
    "extracted_data": [
        {
            "name_short": "MC-DML",
            "name_full": "Monte Carlo Planning with Dynamic Memory-guided Large Language Model",
            "brief_description": "An algorithm that integrates a Large Language Model (LLM) with Monte Carlo Tree Search (MCTS) to enhance action exploration in text-based games by utilizing both in-trial and cross-trial memory mechanisms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MC-DML Agent",
            "agent_description": "The MC-DML agent employs a Large Language Model (LLM) as a prior policy within the PUCT algorithm, utilizing dynamic memory mechanisms to enhance decision-making in complex text-based games.",
            "text_game_name": "Various text-based games from the Jericho benchmark",
            "memory_used": true,
            "memory_type": "In-trial memory and cross-trial memory",
            "memory_mechanism_description": "In-trial memory consists of the current trajectory history, while cross-trial memory includes reflections from previous failure trajectories, allowing the agent to adjust action value estimations dynamically.",
            "performance_with_memory": "48.66 ± 1.89 in Zork1",
            "performance_without_memory": "38.33 ± 2.89 in Zork1",
            "performance_comparison_reported": true,
            "memory_benefits_summary": "The use of memory mechanisms significantly improves performance by enabling better long-term planning and exploration, allowing the agent to avoid previously failed actions.",
            "memory_limitations_or_challenges": "The LLM's in-trial memory is limited to recent actions, which may not capture relevant information from earlier in the game, posing challenges for complex puzzles.",
            "agent_training_method": "The agent is trained using a combination of reinforcement learning and memory-guided planning.",
            "memory_training_method": "Reflections from previous failures are stored in cross-trial memory, which is updated during gameplay to inform future actions.",
            "task_complexity_description": "The text-based games feature dynamic state spaces, sparse rewards, and require extensive exploration, making them complex environments for decision-making.",
            "uuid": "e7.0",
            "source_info": {
                "paper_title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Monte Carlo Tree Search with Large Language Model for Text-Based Game Agents",
            "rating": 2
        },
        {
            "paper_title": "Interactive Fiction Games: A Colossal Adventure",
            "rating": 1
        }
    ],
    "cost": 0.0033668999999999995,
    "model_str": null
}</code></pre>
        </div>

    </div>
</body>
</html>
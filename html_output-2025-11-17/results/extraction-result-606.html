<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-606 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-606</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-606</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-dcc2941b5d7a70acf8bd609570e4b6f106ecabc4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dcc2941b5d7a70acf8bd609570e4b6f106ecabc4" target="_blank">Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results are investigated and the guidelines on reporting novel results as comparisons against baseline methods are provided.</p>
                <p><strong>Paper Abstract:</strong> Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e606.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e606.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random seeds / trial variance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random seed variability and trial-to-trial variance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variance in learning outcomes caused solely by different random seeds (initialization, sampling, environment RNG) leading to substantially different averaged learning curves and final returns even with identical hyper-parameters and code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Reinforcement learning (continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Evaluating effect of random seeds on TRPO and DDPG performance on MuJoCo Hopper-v1 and Half-Cheetah-v1</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>random seeds (initial network weights, RNG used in environments and replay buffer sampling), environment stochasticity, replay buffer sampling randomness</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average Return (mean across runs), Standard Deviation of Returns, two-sample t-tests comparing run groups</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Authors ran 10 trials with best-tuned hyper-parameters and split into two random groups of 5; averaging two different sets of 5 runs produced visibly and statistically different learning curves (Figure 10). Reported distribution-level t-test across entire training distributions: for Half-Cheetah t = -9.0916, p = 0.0016 (significant), for Hopper t = 2.2243, p = 0.1825 (not significant). Typical reported Std Return examples: Half-Cheetah std ≈ 443.87 (at 5000 iters), Hopper std ≈ 796.58 (at 5000 iters).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Replication by re-running experiments with different random seeds and comparing averages/std; two-sample t-tests between groups of runs; reporting of standard deviation across runs</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Re-running with different random seeds produced substantially different averaged learning curves (example: two different averages of 5 runs did not fall in same distribution for Half-Cheetah), indicating poor reproducibility across single small-N averages; authors recommend averaging many runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Selection/search for best random seed in some codebases; insufficient number of runs (many papers report single or few trials); high seed-induced variance can mask or falsify algorithm comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Run many independent trials with distinct random seeds and report number of trials; average results across seeds; report standard deviation and other distributional metrics rather than only maxima; provide code and hyper-parameter presets</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors demonstrate that averaging across only 5 runs can produce different distributions (no quantified reduction of variance provided), and therefore recommend using more than 5 runs; no specific N proven sufficient in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Most experiments: 5 runs averaged; seed-specific experiment: 10 trials (two groups of 5)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random-seed-induced variability is large: different sets of 5 runs can yield statistically different learning curves (significant for Half-Cheetah), so reporting few-run averages is unreliable; authors recommend averaging many runs and reporting std/other distributional statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e606.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e606.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyper-parameter sensitivity and sources of experimental variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Performance of policy-gradient RL methods (TRPO and DDPG) is highly sensitive to hyper-parameter choices (network architecture, batch size, step size, regularization, reward scale, learning rates), causing large variations across reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Reinforcement learning (continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Systematic ablation/variation of hyper-parameters (policy network architecture, batch size, TRPO step size and regularization, GAE lambda, DDPG reward scaling and actor/critic learning rates) for TRPO and DDPG on Hopper and Half-Cheetah</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>policy network architecture/size, batch size / mini-batch size, TRPO step size (KL bound), TRPO regularization coefficient (CG damping), GAE lambda, DDPG reward scaling, actor and critic learning rates, replay buffer size and sampling</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average Return, Standard Deviation of Returns, per-condition learning curves, two-sample t-tests comparing conditions</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Architecture (400,300) significantly outperformed smaller architectures on Half-Cheetah (TRPO two-sample t-test vs (64,64): t = -13.4165, p = 0.0000). Batch size: TRPO batch 25000 produced best results among {1000,5000,25000}; DDPG mini-batch 128 improved performance over 32/64 for Half-Cheetah. Reward scaling: contrary to some prior reports, RS=1 (no scaling) yielded higher returns for DDPG on Half-Cheetah in these experiments. Example reported numbers: Half-Cheetah Average Return (Ours, 5000 iters) = 5010.83, Std = 443.87; Hopper Average Return (Ours, 5000 iters) = 2421.07, Std = 796.58.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of averaged learning curves across hyper-parameter settings; statistical t-tests across sample rollouts; reporting of mean and std return across runs</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Hyper-parameter choices explain large portions of reported differences between works; running with suboptimal hyper-parameters yields inaccurate baseline comparisons; architecture and batch size had large effect sizes (statistically significant in tests reported).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Under-reporting of hyper-parameter settings in prior works, different default architectures/batch sizes across codebases, task-dependent optimal hyper-parameters (Half-Cheetah more sensitive than Hopper), difficulty in fair baseline comparisons without careful tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Careful hyper-parameter tuning and reporting; provide presets and code; compare across common, well-documented hyper-parameter settings; use larger batch sizes for TRPO when constrained to limited iterations; try larger mini-batches for DDPG on some tasks; test multiple architectures</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical outcomes: TRPO batch=25000 outperformed smaller batches in constrained 5000-iteration runs; DDPG mini-batch=128 gave significant improvements for Half-Cheetah versus 32/64; changing reward scale did not universally help (RS=1 better in their DDPG Half-Cheetah runs). No universal cure — improvements are task and algorithm dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>For each hyper-parameter condition: typically 5 runs averaged; final tuned cross-section run for 10 trials (random seeds) reported in seed study</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Many hyper-parameters (architecture, batch size, reward scaling, learning rates) strongly affect reported performance; inconsistent hyper-parameter reporting across prior work undermines reproducibility—careful tuning and full disclosure are required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e606.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e606.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reproducibility metrics & recommended practices</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproducibility assessment metrics and recommended experimental practices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommended metrics and practices to evaluate and improve reproducibility in RL benchmarks: prefer Average Return and Standard Deviation over Maximum Return, report full hyper-parameter lists, run and average many trials with different seeds, and publish code and presets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Reinforcement learning (experimental methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessment of reporting standards and experimental practices to improve reproducibility of policy-gradient RL benchmarks (TRPO, DDPG)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>reporting choices (using max vs mean), insufficient trial counts, hidden implementation differences across codebases, omitted hyper-parameter lists, environment stochasticity, random seed selection</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Average Return, Standard Deviation of Returns, Max Average Return (not recommended), two-sample t-tests across run distributions; qualitative inspection of learning curves across seeds/hyper-parameters</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Max Average Return considered highly biased; authors recommend Average Return and Std Return as stable measures; they demonstrate examples where max-based reporting would be misleading due to high-variance outliers. Empirical table entries: recommended reporting shows means and stds (e.g., Half-Cheetah mean 5010.83 ± 443.87 at 5000 iters for their tuned TRPO).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproducibility assessed via replication of baseline algorithms using public implementations, cross-hyper-parameter sweeps, multiple seeds, and statistical comparisons (t-tests); success judged by overlapping distributions and consistent averages/stds across replications.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Reproducibility across works is mixed: some prior reported baselines align with the authors' tuned results, others differ widely—attributed to hyper-parameter and implementation differences; authors conclude current reporting practices are insufficient for consistent reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Use of biased metrics (max returns), under-reporting of hyper-parameters, small number of runs, implementation differences across codebases, environment stochasticity producing divergent outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Report Average Return and Standard Deviation (not maxima), provide complete hyper-parameter lists, average over many (>5) independent runs with different seeds, publish code and presets, run more trials to characterize distributions, perform statistical tests between conditions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Authors show that applying recommended practices reveals the true variability (e.g., presenting stds highlights instability) and reduces risk of misleading claims; they do not provide a single numerical improvement metric but demonstrate empirically that larger batch sizes and hyper-parameter tuning improve stability in certain settings and that averaging more runs avoids outlier-driven conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Recommendation: average results over >5 trials; experiments typically used 5 runs and a dedicated 10-run seed experiment to demonstrate issues</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Use of biased metrics (max) and under-reporting of hyper-parameters cause irreproducible baselines; best practices are to report mean ± std across many seeds, disclose hyper-parameters and code, and perform statistical comparisons to support claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control', 'publication_date_yy_mm': '2017-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Continuous control with deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Trust region policy optimization <em>(Rating: 2)</em></li>
                <li>Benchmarking deep reinforcement learning for continuous control <em>(Rating: 2)</em></li>
                <li>Q-prop: Sample-efficient policy gradient with an off-policy critic <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-606",
    "paper_id": "paper-dcc2941b5d7a70acf8bd609570e4b6f106ecabc4",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Random seeds / trial variance",
            "name_full": "Random seed variability and trial-to-trial variance",
            "brief_description": "Variance in learning outcomes caused solely by different random seeds (initialization, sampling, environment RNG) leading to substantially different averaged learning curves and final returns even with identical hyper-parameters and code.",
            "citation_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Reinforcement learning (continuous control)",
            "experimental_task": "Evaluating effect of random seeds on TRPO and DDPG performance on MuJoCo Hopper-v1 and Half-Cheetah-v1",
            "variability_sources": "random seeds (initial network weights, RNG used in environments and replay buffer sampling), environment stochasticity, replay buffer sampling randomness",
            "variability_measured": true,
            "variability_metrics": "Average Return (mean across runs), Standard Deviation of Returns, two-sample t-tests comparing run groups",
            "variability_results": "Authors ran 10 trials with best-tuned hyper-parameters and split into two random groups of 5; averaging two different sets of 5 runs produced visibly and statistically different learning curves (Figure 10). Reported distribution-level t-test across entire training distributions: for Half-Cheetah t = -9.0916, p = 0.0016 (significant), for Hopper t = 2.2243, p = 0.1825 (not significant). Typical reported Std Return examples: Half-Cheetah std ≈ 443.87 (at 5000 iters), Hopper std ≈ 796.58 (at 5000 iters).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Replication by re-running experiments with different random seeds and comparing averages/std; two-sample t-tests between groups of runs; reporting of standard deviation across runs",
            "reproducibility_results": "Re-running with different random seeds produced substantially different averaged learning curves (example: two different averages of 5 runs did not fall in same distribution for Half-Cheetah), indicating poor reproducibility across single small-N averages; authors recommend averaging many runs.",
            "reproducibility_challenges": "Selection/search for best random seed in some codebases; insufficient number of runs (many papers report single or few trials); high seed-induced variance can mask or falsify algorithm comparisons",
            "mitigation_methods": "Run many independent trials with distinct random seeds and report number of trials; average results across seeds; report standard deviation and other distributional metrics rather than only maxima; provide code and hyper-parameter presets",
            "mitigation_effectiveness": "Authors demonstrate that averaging across only 5 runs can produce different distributions (no quantified reduction of variance provided), and therefore recommend using more than 5 runs; no specific N proven sufficient in this paper.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Most experiments: 5 runs averaged; seed-specific experiment: 10 trials (two groups of 5)",
            "key_findings": "Random-seed-induced variability is large: different sets of 5 runs can yield statistically different learning curves (significant for Half-Cheetah), so reporting few-run averages is unreliable; authors recommend averaging many runs and reporting std/other distributional statistics.",
            "uuid": "e606.0",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "Hyperparameter sensitivity",
            "name_full": "Hyper-parameter sensitivity and sources of experimental variability",
            "brief_description": "Performance of policy-gradient RL methods (TRPO and DDPG) is highly sensitive to hyper-parameter choices (network architecture, batch size, step size, regularization, reward scale, learning rates), causing large variations across reported results.",
            "citation_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Reinforcement learning (continuous control)",
            "experimental_task": "Systematic ablation/variation of hyper-parameters (policy network architecture, batch size, TRPO step size and regularization, GAE lambda, DDPG reward scaling and actor/critic learning rates) for TRPO and DDPG on Hopper and Half-Cheetah",
            "variability_sources": "policy network architecture/size, batch size / mini-batch size, TRPO step size (KL bound), TRPO regularization coefficient (CG damping), GAE lambda, DDPG reward scaling, actor and critic learning rates, replay buffer size and sampling",
            "variability_measured": true,
            "variability_metrics": "Average Return, Standard Deviation of Returns, per-condition learning curves, two-sample t-tests comparing conditions",
            "variability_results": "Architecture (400,300) significantly outperformed smaller architectures on Half-Cheetah (TRPO two-sample t-test vs (64,64): t = -13.4165, p = 0.0000). Batch size: TRPO batch 25000 produced best results among {1000,5000,25000}; DDPG mini-batch 128 improved performance over 32/64 for Half-Cheetah. Reward scaling: contrary to some prior reports, RS=1 (no scaling) yielded higher returns for DDPG on Half-Cheetah in these experiments. Example reported numbers: Half-Cheetah Average Return (Ours, 5000 iters) = 5010.83, Std = 443.87; Hopper Average Return (Ours, 5000 iters) = 2421.07, Std = 796.58.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of averaged learning curves across hyper-parameter settings; statistical t-tests across sample rollouts; reporting of mean and std return across runs",
            "reproducibility_results": "Hyper-parameter choices explain large portions of reported differences between works; running with suboptimal hyper-parameters yields inaccurate baseline comparisons; architecture and batch size had large effect sizes (statistically significant in tests reported).",
            "reproducibility_challenges": "Under-reporting of hyper-parameter settings in prior works, different default architectures/batch sizes across codebases, task-dependent optimal hyper-parameters (Half-Cheetah more sensitive than Hopper), difficulty in fair baseline comparisons without careful tuning",
            "mitigation_methods": "Careful hyper-parameter tuning and reporting; provide presets and code; compare across common, well-documented hyper-parameter settings; use larger batch sizes for TRPO when constrained to limited iterations; try larger mini-batches for DDPG on some tasks; test multiple architectures",
            "mitigation_effectiveness": "Empirical outcomes: TRPO batch=25000 outperformed smaller batches in constrained 5000-iteration runs; DDPG mini-batch=128 gave significant improvements for Half-Cheetah versus 32/64; changing reward scale did not universally help (RS=1 better in their DDPG Half-Cheetah runs). No universal cure — improvements are task and algorithm dependent.",
            "comparison_with_without_controls": true,
            "number_of_runs": "For each hyper-parameter condition: typically 5 runs averaged; final tuned cross-section run for 10 trials (random seeds) reported in seed study",
            "key_findings": "Many hyper-parameters (architecture, batch size, reward scaling, learning rates) strongly affect reported performance; inconsistent hyper-parameter reporting across prior work undermines reproducibility—careful tuning and full disclosure are required.",
            "uuid": "e606.1",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        },
        {
            "name_short": "Reproducibility metrics & recommended practices",
            "name_full": "Reproducibility assessment metrics and recommended experimental practices",
            "brief_description": "Recommended metrics and practices to evaluate and improve reproducibility in RL benchmarks: prefer Average Return and Standard Deviation over Maximum Return, report full hyper-parameter lists, run and average many trials with different seeds, and publish code and presets.",
            "citation_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Reinforcement learning (experimental methodology)",
            "experimental_task": "Assessment of reporting standards and experimental practices to improve reproducibility of policy-gradient RL benchmarks (TRPO, DDPG)",
            "variability_sources": "reporting choices (using max vs mean), insufficient trial counts, hidden implementation differences across codebases, omitted hyper-parameter lists, environment stochasticity, random seed selection",
            "variability_measured": true,
            "variability_metrics": "Average Return, Standard Deviation of Returns, Max Average Return (not recommended), two-sample t-tests across run distributions; qualitative inspection of learning curves across seeds/hyper-parameters",
            "variability_results": "Max Average Return considered highly biased; authors recommend Average Return and Std Return as stable measures; they demonstrate examples where max-based reporting would be misleading due to high-variance outliers. Empirical table entries: recommended reporting shows means and stds (e.g., Half-Cheetah mean 5010.83 ± 443.87 at 5000 iters for their tuned TRPO).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproducibility assessed via replication of baseline algorithms using public implementations, cross-hyper-parameter sweeps, multiple seeds, and statistical comparisons (t-tests); success judged by overlapping distributions and consistent averages/stds across replications.",
            "reproducibility_results": "Reproducibility across works is mixed: some prior reported baselines align with the authors' tuned results, others differ widely—attributed to hyper-parameter and implementation differences; authors conclude current reporting practices are insufficient for consistent reproducibility.",
            "reproducibility_challenges": "Use of biased metrics (max returns), under-reporting of hyper-parameters, small number of runs, implementation differences across codebases, environment stochasticity producing divergent outcomes",
            "mitigation_methods": "Report Average Return and Standard Deviation (not maxima), provide complete hyper-parameter lists, average over many (&gt;5) independent runs with different seeds, publish code and presets, run more trials to characterize distributions, perform statistical tests between conditions",
            "mitigation_effectiveness": "Authors show that applying recommended practices reveals the true variability (e.g., presenting stds highlights instability) and reduces risk of misleading claims; they do not provide a single numerical improvement metric but demonstrate empirically that larger batch sizes and hyper-parameter tuning improve stability in certain settings and that averaging more runs avoids outlier-driven conclusions.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Recommendation: average results over &gt;5 trials; experiments typically used 5 runs and a dedicated 10-run seed experiment to demonstrate issues",
            "key_findings": "Use of biased metrics (max) and under-reporting of hyper-parameters cause irreproducible baselines; best practices are to report mean ± std across many seeds, disclose hyper-parameters and code, and perform statistical comparisons to support claims.",
            "uuid": "e606.2",
            "source_info": {
                "paper_title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
                "publication_date_yy_mm": "2017-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Continuous control with deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Trust region policy optimization",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking deep reinforcement learning for continuous control",
            "rating": 2
        },
        {
            "paper_title": "Q-prop: Sample-efficient policy gradient with an off-policy critic",
            "rating": 2
        }
    ],
    "cost": 0.012138,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</h1>
<p>Riashat Islam ${ }^{* \dagger}$<br>School of Computer Science<br>McGill University<br>Montreal, QC, Canada<br>riashat.islam@mail.mcgill.ca<br>Peter Henderson ${ }^{\dagger}$<br>School of Computer Science<br>McGill University<br>Montreal, QC, Canada<br>peter.henderson@mail.mcgill.ca</p>
<h2>Maziar Gomrokchi</h2>
<p>School of Computer Science
McGill University
Montreal, QC, Canada
maziar.gomrokchi@mail.mcgill.ca</p>
<h2>Doina Precup</h2>
<p>School of Computer Science
McGill University
Montreal, QC, Canada
dprecup@cs.mcgill.ca</p>
<h4>Abstract</h4>
<p>Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.</p>
<h2>1 Introduction</h2>
<p>Deep reinforcement learning with neural network policies and value functions has had enormous success in recent years across a wide range of domains [1, 2, 3, 4]. In particular, model-free reinforcement learning with policy gradient methods have been used to solve complex robotic control tasks [5, 6]. Policy gradient methods can be generally divided into two groups: off-policy gradient methods, such as Deep Deterministic Policy Gradients (DDPG) [1] and on-policy methods, such as Trust Region Policy Optimization (TRPO) [2].
However, often there are many sources of possible instability and variance that can lead to difficulties with reproducing deep policy gradient methods. In this work, we investigate the sources of these difficulties with both on- and off-policy gradient methods for continuous control. We use two MuJoCo [7] physics simulator tasks from OpenAI gym [8] (Hopper-v1 and Half-Cheetah-v1) for our experimental tasks. We investigate two policy gradient algorithms here: DDPG and TRPO. To our knowledge, there are few works [9] which reproduce existing policy gradients methods in reinforcement learning, yet many use as these algorithms as baselines to compare their novel work against $[9,10,11,12]$. We use the code provided by in [9] and [10] for TRPO and DDPG</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(respectively), as these implementations are used in several works directly for comparison [12, 10, 9, 13, 14]</p>
<p>Performance Measures : We examine the general variance of the algorithms and address the importance of presenting all possible metrics across a large number of trials. Three main performance measures commonly used in the literature are: Maximum Average Return, Maximum Return, Standard Deviation of Returns, and Average Return. However, the first two measures are considered to be highly biased, while the last two are considered to be the most stable measures used to compare the performance of proposed algorithms. Thereby, in the rest of this work we only use the Average Return as our comparison measure unless stated otherwise, with final results displaying all metrics ${ }^{3}$.</p>
<p>Hyper-parameter Settings : We also highlight that there can be difficulty in properly fine-tuning hyper-parameter settings, leading to large variations of reported results across a wide range of works as different hyper-parameters are used. As in Tables 2 and 1, this inconsistency within the wide range of reported results makes it difficult to compare DDPG and TRPO as baseline algorithms without careful detailing of hyper-parameters, attention to the fairness of the comparison, and proper tuning of the parameters. Each cited work uses a different set of experimental hyper-parameters for supposed baseline comparisons ${ }^{4}$. Running these algorithms with suboptimal hyper-parameter configurations may result in inaccurate comparisons against these baseline methods. As such, we highlight the significance of tuning various hyper-parameters and assess which of these yield the most significant differences in performance.</p>
<p>Based on our analysis, we encourage that careful consistency should be maintained when reporting results with both of these algorithms, as they are quite susceptible to hyper-parameters and the external sources of variance or randomness.</p>
<h1>2 Experimental Analysis</h1>
<p>We evaluate the off-policy DDPG [1] and on-policy TRPO [2] algorithms on continuous control environments from the OpenAI Gym benchmark [8], using the MuJoCo physics simulator [7]. We empirically show the susceptibility and variance in results due to hyper-parameter configurations on two environments: Hopper ( $\mathcal{S} \subseteq \mathbb{R}^{20}, \mathcal{A} \subseteq \mathbb{R}^{3}$ ) and Half-Cheetah ( $\mathcal{S} \subseteq \mathbb{R}^{20}, \mathcal{A} \subseteq \mathbb{R}^{6}$ ). All experiments ${ }^{5}$ are performed building upon the rllab Tensorflow implementation of TRPO [9] and the Q-Prop Tensorflow implementation of DDPG for our experiments [10].</p>
<p>Experiment Details : We run all variations for 5000 iterations and average all results across 5 runs. We investigate several hyper-parameters: batch size, policy network architecture, step size (TRPO), regularization coefficient (TRPO), generalized advantage estimation ( $\lambda$ ) (TRPO), reward scale (DDPG), and actor-critic learning rates (DDPG). For each of these hyper-parameters we hold all others constant at default settings and vary the one under investigation across commonly used values. Lastly, we run a final set of experiments using the overall best cross-section of hyper-parameters for 10 trials using random seeds. We do this to investigate whether there is a significant difference in the results just due to variance caused by the random seeds.
For TRPO, the default hyper-parameters which we use are: a network architecture of (100,50,25) with ReLU hidden activations for a Gaussian Multilayer Perception Policy [9]; a step size of 0.01; a regularization coefficient of $1 \cdot 10^{-5}$; a Generalized Advantage Estimation $\lambda$ of 1.0 [3]. For DDPG, we use default parameters as follows: a network architecture of $(100,50,25)$ with relu hidden activations for a Gaussian Multilayer Perception Policy [9]; actor-critic learning rates of $1 \cdot 10^{-3}$ and $1 \cdot 10^{-4}$; batch sizes of 64 ; and a reward scale of 0.1 .</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2.1 Common Hyper-Parameters</p>
<p>First, we investigate several hyper-parameters common to both TRPO and DDPG: policy architecture and batch size. We use the same sets of hyper-parameters as reported in previous works using these implementations in an attempt to reproduce the results reported in these works.</p>
<p>Policy Network Architecture : The policy network architecture can play an important role in the maximum reward achieved by the algorithm due to the amount of information storage provided by the network. We use a hidden layer sizes (64,64) as in [2], (400,300) as in [9, 1], and (100,50,25) as in [10] for comparing the results of these algorithms.</p>
<p>Our results can be found in Figures 1 and 2. Notably, the (400,300) architecture significantly outperforms both other smaller architectures for Half-Cheetah and to a less significant extent Hopper as well. This is true for both TRPO and DDPG. However, the architecture which we found to be the best (400,300) is not the one which is used in reporting results for baselines results in [10, 12].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: TRPO on Half-Cheetah with different network configurations</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: DDPG on Half-Cheetah and Hopper on different network configurations</p>
<p>For the Hopper environment, for both TRPO and DDPG, results are not as significantly impacted by varying the network architecture, unlike the Half-Cheetah environment. This is somewhat thematic of what we find across all hyper-parameter variations on Hopper, as will be further discussed later. In particular, our investigation of DDPG on different network configurations shows that for the Hopper environment, DDPG is quite unstable no matter the network architecture. This can be attributed partially to the high variance of DDPG itself, but also to the increased stochasticity of the Hopper task. As can be seen in Figure 2, even with varied network architectures, it is difficult to tune DDPG to reproduce results from other works even when using their reported hyper-parameter settings.</p>
<p>Batch Size : The batch size parameter plays an important role in both DDPG and TRPO. In the off-policy DDPG algorithm, the actor and critic updates are made by sampling a mini-batch uniformly</p>
<p><sup>6</sup>All of these use RELU activations for the hidden layers and a Gaussian MLP Policy.</p>
<p><sup>7</sup>For TRPO Half-Cheetah using a two-sample t-test on the sample rollouts: against (64,64) <em>t</em> = -13.4165, <em>p</em> = 0.0000; against (100,50,25) <em>t</em> = -11.3368, <em>p</em> = 0.0016. For TRPO Hopper: against (100,50,25) <em>t</em> = -0.5904, <em>p</em> = 0.2952; against (64,64) <em>t</em> = -1.9081, <em>p</em> = 0.2198</p>
<p>from the replay buffer. Typically, the replay buffer is allowed to be large. In [1] and [10], a batch size of 64 was used, whereas the original rllab implementation uses a batch size of 32. Our analysis with different mini-batches for DDPG $(32,64,128)$ shows that similar performance can be obtained with mini-batch sizes of 32 and 64, whereas significant improvements can be obtained with a batch size of 128.</p>
<p>For TRPO, larger batch sizes are necessary in general. We investigate the same batch sizes as used in [10, 2] of (1000,5000,25000). As expected, a batch size of 25000 produces the best results. As we constrain learning to 5000 episodes, it is intuitive that a larger batch size would perform better in this time frame as more samples are seen. Furthermore, as can be seen in Figure 3 for Half-Cheetah, the smaller batch sizes begin to plateau to a much lower optimum.</p>
<p>By intuition, this may be due to TRPO’s use of conjugate gradient optimization with a KL constraint. With small sample batch sizes, gradients differences between steps may be much larger in a high variance environment and results in a more unstable training regime.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: TRPO on Half-Cheetah and Hopper - Significance of batch size</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: DDPG on Half-Cheetah and Hopper - Significance of the mini batch size</p>
<p>We also highlight that the DDPG algorithm with different batch sizes produces similar results for the Hopper environment. While other works have reported different tuned parameters for DDPG, we establish the high variance of this algorithm, producing similar results with different batch sizes for the Hopper environment, while a larger batch size improves performance in Half-Cheetah as seen in Figure 4.</p>
<h3>2.2 TRPO-Specific Hyper-Parameters</h3>
<p><strong>Regularization Coefficient :</strong> The regularization coefficient (RC) (or conjugate gradient damping factor) is used as a regularizer by adding a factor of the identity matrix to the Fisher matrix (or finite difference HVP in [9]) during the conjugate gradient step. We investigate a range of values between 1 · 10⁻⁵ to .1 based on values used in aforementioned works. We don’t see a significant difference⁸</p>
<p>⁸Using an average of 2-sample t-test comparisons, the largest difference from the default parameter in Hopper is <em>t</em> = 2.8965, <em>p</em> = 0.1443 with RC=0.1 and <em>t</em> = 0.8020, <em>p</em> = 0.4540 with RC=.0001.</p>
<p>in using one particular value of RC over another, though it seems to have a more significant effect on Hopper. Figure 5 shows the average learning graphs for these variations.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Regularization coefficient variations for TRPO. Cited implementation values may use different sets hyper-parameters. See associated works for specific details.</p>
<p>Generalized Advantage Estimation : Generalized advantage estimation [3] has been shown to improve results dramatically for TRPO. Here, we investigate using $\lambda=1.0$ and $\lambda=.97$ for this. We find that for longer iterations, a lower GAE $\lambda$ does in fact improve results for longer sequences in Half-Cheetah and mildly for Hopper ${ }^{9}$. Figure 6 shows the average learning graphs for these variations.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Generalized advantage estimation lambda value variations for TRPO. Cited implementation values may use different sets hyper-parameters. See associated works for specific details.</p>
<p>Step Size : The step size (SS) (effectively the learning rate of TRPO) is the same as the KL-divergence bound for the conjugate gradient steps. Here, we find that the default value of 0.01 appears to work generally the best for both Hopper and Half-Cheetah ${ }^{10}$. Figure 7 shows the average learning curves for these variations. The intuition here is the same behind adjusting learning rates in standard gradient optimization methods, though the formulation is through a constraint rather than a learning rate, it effectively has the same characteristics when tuning it.</p>
<h1>2.3 DDPG-Specific Hyper-Parameters</h1>
<p>We investigate two hyper-parameters which are unique to DDPG which previous works have described as important for improving results [9, 10]: reward scale and actor-critic learning rates.
Reward Scale : As in [9], all the rewards for all tasks were rescaled by a factor of 0.1 to improve the stability of DDPG. It has been claimed that this external hyper-parameter, depending on the task,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Step size variations for TRPO. Cited implementation values may use different sets hyperparameters. See associated works for specific details.</p>
<p>can make the DDPG algorithm unstable. Experimental results in [10] give indication that DDPG is particularly sensitive to different reward scale settings.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: DDPG on Half-Cheetah and Hopper - Significance of the Reward Scaling parameter</p>
<p>Figure 8 shows that even though DDPG performance have been reported to be highly susceptible to the reward scaling parameter, our analysis shows that DDPG does not improve by rescaling the rewards. In fact, for the Half-Cheetah environment, we find that no reward scaling (RS=1) yields much higher returns, even though [10] and [9] have reported an optimal reward scale value to be 0.1. Furthermore, we highlight that often for DDPG, learning curves are not shown for all environments and only tabular results are presented, making it difficult to compare how reward scaling has affected results in prior work.</p>
<p>Actor-Critic Learning Rates: We further investigate the effects of the actor and critic base learning rates as given in [10] and [9], which both use 0.001, 0.0001 (for the critic, and actor respectively). Interestingly, we find that the actor and critic learning rates for DDPG have less of an effect on the Hopper environment than the Half-Cheetah environment. This brings into consideration that keeping other parameters fixed, DDPG is not only susceptible to the learning rates, but there are other sources of variation and randomness in the DDPG algorithm.</p>
<h3>2.4 General Variance</h3>
<p>We investigate the general variance of multiple trials with different random seeds. Variance across random seeds is of particular interest since it has been noted that in several known codebases, there are implementations for searching for the best random seed to use. In particular, we determine whether it is possible to generate learning curves by randomly averaging trials together (with only the seed varied) such that we see statistically significant differences in the average reward learning curve distributions. Thereby, we wish to determine if it is possible to report significantly worse results.</p>
<p><sup>11</sup>One such example in the codebase we use here: https://github.com/openai/rllab/blob/master/contrib/rllab_hyperopt/example/main.py#L21.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: DDPG on Half-Cheetah and Hopper - Actor and Critic Learning Rates</p>
<p>on a baseline policy gradient method such as TRPO or DDPG, just by varying the random seed (or significantly better results for the algorithm under investigation by doing so).</p>
<p>We run a total of 10 trials with our best tuned hyper-parameter configurations as examined previously. We randomly average two groups of 5 and plot the results. We find that there can be a significant difference as seen in Figure 10. Particularly for Half-Cheetah it is possible to get training curves that do not fall within the same distribution at all, just by averaging different runs with the same hyper-parameters, but random seeds.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: TRPO with best hyper-parameter configurations, with average of 5 runs over 2 different sets of experiments under same configuration, producing variant results.</p>
<p>Figure 11 also shows the significance of DDPG instability. Even with fine-tuned hyper-parameter configurations, our analysis shows that stable results with DDPG, on either of the environments cannot be achieved. This further suggests that there might be randomness due to other external sources which affect performance of DDPG on these continuous control tasks.</p>
<p>Our results show that for both DDPG and TRPO, taking two different average across 5 experiment runs do not necessarily produce the same result, and in fact, there is high variance in the obtained results. This emphasizes the need for averaging many runs together when reporting results using a different random seed for each. In this way, future works should attempt to negate the effect of random seeds and environment stochasticity when reporting their results.</p>
<h2>3 Discussion and Conclusion</h2>
<p>Tables 1 and 2 highlight results and metrics presented in various related works which compare to TRPO and DDPG (respectively). We include results from an average of 5 runs across the best cross-section of hyper-parameters (based on our previous investigations). We show various metrics.</p>
<p><sup>12</sup>Average 2-sample t-test run across entire training distributions resulting in t = −9.0916, p = 0.0016 for Half-Cheetah and t = 2.2243, p = 0.1825 for Hopper</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: DDPG with tuned hyper-parameter configurations, with average of 5 runs over 2 different set of experiments under same configuration, producing variant results.</p>
<p>at different numbers of iterations such that a fair comparison can be made against reported results from other works. It can be noted that while some works demonstrate similar results to our own, others vary wildly from our own findings. Furthermore, many works only include the Max Average Return, which can be misleading. Due to the variance we have demonstrated here and the difficulty in reproducing these algorithms, it is extremely important for future works to: (1) report all possible metrics to characterize their own algorithms against TRPO and DDPG (particularly Average Return and Standard Deviation of the returns); (2) report all hyper-parameters used for optimization; (3) attempt to use a somewhat optimal set of hyper-parameters; (4) average results on greater than 5 trials and report how many trials are averaged together[^13]. We intend this work to act as both a guide for accomplishing this and a starting point for determining whether observed values are in line with possible best results on Hopper and Half-Cheetah environments for novice researchers in policy gradients.</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>Metric</th>
<th>rllab [9]</th>
<th>QProp [10]</th>
<th>lPG [12]</th>
<th>TRPO [2, 3]^{14}</th>
<th>Ours</th>
<th>Ours</th>
<th>Ours</th>
<th>Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td>Half-Cheetah</td>
<td>Length (iters)</td>
<td>500</td>
<td>–</td>
<td>–</td>
<td>500</td>
<td>500</td>
<td>1000</td>
<td>2500</td>
<td>5000</td>
</tr>
<tr>
<td></td>
<td>Length (episodes)</td>
<td>∼25k</td>
<td>30k</td>
<td>10k</td>
<td>∼12.5k</td>
<td>∼12.5k</td>
<td>∼25k</td>
<td>∼62.5k</td>
<td>∼125k</td>
</tr>
<tr>
<td></td>
<td>Average Return</td>
<td>1914.0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>3576.08</td>
<td>3995.4</td>
<td>4638.52</td>
<td>5010.83</td>
</tr>
<tr>
<td></td>
<td>Max Average Return</td>
<td>–</td>
<td>4734</td>
<td>2889</td>
<td>4855.00</td>
<td>3980.61</td>
<td>4360.77</td>
<td>4889.18</td>
<td>5197.40</td>
</tr>
<tr>
<td></td>
<td>Std Return</td>
<td>120.1</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>434.78</td>
<td>502.57</td>
<td>419.08</td>
<td>443.87</td>
</tr>
<tr>
<td>Hopper</td>
<td>Length (iters)</td>
<td>500</td>
<td>–</td>
<td>–</td>
<td>500</td>
<td>500</td>
<td>1000</td>
<td>2500</td>
<td>5000</td>
</tr>
<tr>
<td></td>
<td>Length (episodes)</td>
<td>∼25k</td>
<td>30k</td>
<td>10k</td>
<td>∼22k</td>
<td>∼12.5k</td>
<td>∼25k</td>
<td>∼62.5k</td>
<td>∼125k</td>
</tr>
<tr>
<td></td>
<td>Average Return</td>
<td>1183.3</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>2021.34</td>
<td>2285.73</td>
<td>2526.41</td>
<td>2421.067</td>
</tr>
<tr>
<td></td>
<td>Max Average Return</td>
<td>–</td>
<td>2486</td>
<td>–</td>
<td>3668.81</td>
<td>3229.14</td>
<td>3442.26</td>
<td>3456.05</td>
<td>3476.00</td>
</tr>
<tr>
<td></td>
<td>Std Return</td>
<td>150.0</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>654.37</td>
<td>757.68</td>
<td>714.07</td>
<td>796.58</td>
</tr>
</tbody>
</table>
<p>Table 1: Results and descriptions of reported values by various works using TRPO (Hopper and Half-Cheetah environments) as a baseline. "Length(iters)" denotes algorithm iterations and "Length(episodes)" denotes number of episodes.</p>
<p>We present a set of results, highlighting the difficulty in reproducing results with policy gradient methods in reinforcement learning. We show the difficulty of fine-tuning and the significant sources of variance in hyper-parameter selection for both TRPO and DDPG algorithms. Our analysis shows that these state-of-the-art on-policy and off-policy policy gradient methods often suffer from large variations as a result of different hyper-parameter settings. In addition, results across different continuous control domains are not always consistent, as shown in the Hopper and Half-Cheetah experiment results. We find that Half-Cheetah is more susceptible to performance variations from hyper-parameter tuning, while Hopper is not. We posit that this may be due to the difference in stochasticity within the environments themselves. Half-Cheetah has a much more stable dynamics</p>
<p>^{13}Further investigation needs to be done to determine the amount of trials (<em>N</em>) necessary to ensure a fair comparison (i.e. for what <em>N</em> would any <em>N</em>-sample average always result in a similarly distributed returns, unlike as has been demonstrated to be possible in Figure 10)</p>
<p>^{14}Results from original implementation evaluation on OpenAI Gym: https://gym.openai.com/evaluations/eval_W27eCzLQBy60FciaSGSJw; https://gym.openai.com/evaluations/eval_0udf6XDS2WL76S7wZicLA</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">rHab [9]</th>
<th style="text-align: center;">QProp [10]</th>
<th style="text-align: center;">SDQN [11]</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;">Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Half-Cheetah</td>
<td style="text-align: center;">Length (iters)</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$1 *^{\text {th }}$ (steps)</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length (episodes)</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">30k</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">$\sim 12.5$</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">$\sim 62.5$</td>
<td style="text-align: center;">$\sim 125$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Return</td>
<td style="text-align: center;">2148.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2707.1</td>
<td style="text-align: center;">3127.9</td>
<td style="text-align: center;">3547.1</td>
<td style="text-align: center;">3725.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Max Average Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">7490</td>
<td style="text-align: center;">6614.26</td>
<td style="text-align: center;">3788.2</td>
<td style="text-align: center;">4029.2</td>
<td style="text-align: center;">4460.7</td>
<td style="text-align: center;">4460.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">907.1</td>
<td style="text-align: center;">784.3</td>
<td style="text-align: center;">634.9</td>
<td style="text-align: center;">512.8</td>
</tr>
<tr>
<td style="text-align: center;">Hopper</td>
<td style="text-align: center;">Length (iters)</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length (episodes)</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">30k</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">$\sim 12.5$</td>
<td style="text-align: center;">$\sim 25$</td>
<td style="text-align: center;">$\sim 62.5$</td>
<td style="text-align: center;">$\sim 125$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average Return</td>
<td style="text-align: center;">267.1</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">790.6</td>
<td style="text-align: center;">883.6</td>
<td style="text-align: center;">838.7</td>
<td style="text-align: center;">857.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Max Average Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2604</td>
<td style="text-align: center;">3296.49</td>
<td style="text-align: center;">1642.1</td>
<td style="text-align: center;">1642.1</td>
<td style="text-align: center;">1642.1</td>
<td style="text-align: center;">1642.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Std Return</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">367.9</td>
<td style="text-align: center;">305.2</td>
<td style="text-align: center;">230.9</td>
<td style="text-align: center;">213.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Results and descriptions of reported values by various works using DDPG (Hopper and Half-Cheetah environments) as a baseline. "Length(iters)" denotes algorithm iterations and "Length(episodes)" denotes number of episodes.
model, and thus is less variant in failure modes. Hopper, on the other hand, is prone to quick failure modes which introduce larger external variance, possibly making tuning difficult.
Based on our experiments, we suggest that the ML research community requires better fine-tuned implementations of these algorithms with provided hyper-parameter presets. These implementations should have benchmark results for a wide range of commonly used tasks. Our analysis shows that due to the under-reporting of hyper-parameters, different works often report different baseline results and performance measures for both TRPO and DDPG. This leads to an unfair comparison of baselines in continuous control environments. Here, we provide some insight into the impact of different hyper-parameters to aid future researchers in finding the ideal baseline configurations.
However, we also suggest that these algorithms are often susceptible to external randomness, introduced by the environment and other external hyper-parameters (e.g reward scale in DDPG) which makes it quite difficult to reproduce results with these state-of-the-art policy gradient algorithms. As such, we provide the aforementioned recommendations in reporting implementation details (provide all hyper-parameters and number of trial experiments), reporting results (report averages and standard deviations, not maximum returns), and implementing proper experimental procedures (average together many trials using different random seeds for each).</p>
<h1>Acknowledgements</h1>
<p>The authors would like to thank Joelle Pineau and David Meger for comments on the paper draft. We would like to thank the McGill University Reasoning and Learning Lab and the Mobile Robotics Lab for allowing an engaging and productive research environment. We would also like to thank Alex Lamb and Anirudh Goyal for providing initial feedback on the direction of this work, as part of the ICML Reproducibility in Machine Learning workshop. This work was supported by the AWS Cloud Credits for Research program, McGill Graduate Excellence Scholarships, and NSERC.</p>
<h2>References</h2>
<p>[1] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[2] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889-1897, 2015.
[3] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.
[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 022015.</p>
<p>[5] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015.
[6] Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep qlearning with model-based acceleration. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 2829-2838, 2016.
[7] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pages 5026-5033, 2012.
[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
[9] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.
[10] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.
[11] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of continuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017.
[12] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Schölkopf, and Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning. arXiv preprint arXiv:1706.00387, 2017.
[13] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109-1117, 2016.
[14] Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. Epopt: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ For Half-Cheetah, $t=2.9109, p=0.0652$ for last 500 iterations and $t=1.9231, p=0.1978$ overall. For Hopper, $t=1.9772, p=0.1741$ for last 500 iterations and $t=-0.1255, p=0.2292$ overall.
${ }^{10}$ Hopper most significant t-test difference from default is $\mathrm{SS}=0.1$ with $t=1.0302, p=0.2929$, and for Half-Cheetah difference from default and $\mathrm{SS}=0.001 t=-3.1255, p=0.0404$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
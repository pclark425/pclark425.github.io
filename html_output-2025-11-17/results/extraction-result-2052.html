<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2052 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2052</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2052</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-280068297</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.04370v1.pdf" target="_blank">WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models (LLMs) have significantly improved the capabilities of web agents. However, effectively navigating complex and dynamic web environments still requires more advanced trajectory-level planning and execution. Prior studies have addressed self-improving agents by collecting extensive GUI trajectories from real-environment interactions. Despite their effectiveness, these approaches encounter two critical challenges: (1) Uncontrollable environment states, where real or sandboxed web environments often yield unstable and non-deterministic feedback, complicating the reproduction and debugging of agent behaviors; and (2) High API costs, as generating even a single interaction trajectory can involve hundreds of queries, leading to considerable API usage and computational expenses. To address these limitations and enable scalable self-improvement for agents, we propose WebSynthesis, a novel framework for trajectory synthesis and training. WebSynthesis leverages a learned world model to simulate virtual web environments, allowing a policy agent to perform efficient and reversible tree-based planning. This approach supports the large-scale generation of diverse and high-quality trajectories, which are subsequently utilized to refine the agent's policy. Experimental results demonstrate that an agent trained using WebSynthesis on a small-scale synthetic dataset achieves performance comparable to or even surpassing that of models trained on large-scale real-world data.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2052.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2052.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebSynthesis Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebSynthesis two-stage curriculum (TextUI warm-up + trajectory-level SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manually designed two-stage curriculum for training web-navigation agents: (1) UI fundamental understanding (dense captioning, element functionality, state-transition prediction) produced with LLM-assisted data augmentation, and (2) supervised fine-tuning on synthetic trajectories (valuable + rollback) generated by a world-model-guided MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>hybrid (manual two-stage curriculum with LLM-assisted data generation and LLM/world-model-guided trajectory synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o (for dense captions and functionality description), GPT-4 (for node pruning/reflection and as process reward model), Qwen2.5-7B (used as world model and base policy model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Two-stage curriculum: Stage 1 (TextUI warm-up) is a supervised curriculum of single-step tasks to build UI understanding: (a) dense captioning (global page descriptions generated by GPT-4o from screenshots paired with text observations), (b) element functionality prediction (structured functional descriptions synthesized with GPT-4o and paired with A11y text), and (c) state-transition prediction (predict page layout change and caption given current observation + action). Stage 1 uses instruction/template augmentation and response refinement via GPT-4. Stage 2 performs supervised fine-tuning on multi-step trajectories produced offline by WebMCTS: an LLM-based world model (Qwen2.5) simulates next-page observations given actions, a process reward model (GPT-4) evaluates partial trajectories, and MCTS (UCB) searches for high-value branches. From the MCTS action tree the pipeline extracts 'valuable' successful trajectories (nodes above a value threshold) and 'rollback' trajectories synthesized by GPT-4 reflections that propose go_back corrections from sibling failure branches. The curriculum proceeds from simpler TextUI tasks to more complex multi-step trajectory cloning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>WebArena (Web UI navigation / headless TextUI using accessibility tree representations)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Dynamic, diverse, partially observable, non-stationary web interfaces; tasks are compositional and multi-step (long-horizon) requiring UI layout understanding, element-level functionality recognition, state-transition modeling and multi-turn decision making; includes domains such as maps, e-commerce (shopping), CMS/admin, social forums (Reddit), and software development (Gitlab).</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Curriculum and trajectory synthesis condition on page observations represented as accessibility trees (A11y), prior observations o_{t-1}, actions a_t, and URL (used for caching); MCTS branches store predicted next-state observations from the world model and node values from the reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>WebMCTS uses UCB to balance exploration/exploitation, samples at least 3 distinct candidate actions at each expansion to encourage breadth, caches states by URL to avoid duplicate rollouts, prunes redundant semantically-duplicate actions (via GPT-4), and explicitly extracts rollback (failed + corrective) trajectories to increase diversity of training signals.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>LLM-based world model (Qwen2.5), process reward model (GPT-4), Monte Carlo Tree Search (UCB-based), GPT-4o-generated instruction/data augmentation, caching by URL, valuable+rollback trajectory extraction, TextUI warm-up (curriculum) before behavior cloning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>WebSynthesis (LLM-assisted curriculum + WebMCTS synthetic data, ~4k trajectories) achieves Overall Pass@3 = 20.15% and Pass@1 = 14.93% on WebArena-Lite (165 tests). At 75% of synthetic data scale (~4k samples) performance matches GPT-4. Scaling synthetic data from ~500 to 4k yields +7.47% overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>OS-Genesis (real trajectory based / reverse task synthesis, 7.4k real trajectories) reported Pass@3 = 18.66% and Pass@1 = 11.19%; AgentTrek (tutorial-guided synthetic, sampled 20k for comparison) reported Pass@3 = 11.94% and Pass@1 = 9.70%. WebSynthesis outperforms both despite smaller dataset (~4k).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Ablation on trajectory supervision: rollback-only training (τ_roll) yields Overall = 1.49%; valuable-only (τ_val) yields 5.97%; using both τ_val ∪ τ_roll yields 9.70%; adding TextUI warm-up (full WebSynthesis) yields 14.93% overall. These numbers show heuristic variants (only rollback or only valuable) underperform the combined approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Chain-of-thought prompting (no fine-tuned curriculum): GPT-4 (CoT) Pass@1 overall = 13.58%; Qwen2.5-7B (CoT) Pass@1 = 2.24%. WebSynthesis (with its curriculum) exceeds these for the fine-tuned Qwen2.5 base.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Synthesized dataset size ~4k trajectories; WebMCTS action trees include both successful (valuable) and multiple failed sibling branches used to create rollback trajectories. No explicit numeric count of distinct task templates reported beyond the trajectory counts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Agent trained on WebSynthesis synthetic data generalizes comparably to models trained on larger real datasets; at ~4k synthetic samples WebSynthesis matches GPT-4 performance and outperforms OS-Genesis trained on 7.4k real trajectories — indicating strong transfer/generalization from LLM-assisted synthetic curriculum to real WebArena tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitative: designed to reduce online evaluation/API cost by replacing live interactions with an LLM-based world model and offline MCTS; the paper emphasizes orders-of-magnitude savings relative to thousands of live queries per trajectory in prior work but does not report dollar or token counts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>World-model fidelity limits reliability: imperfect world model rollouts and compounding multi-step errors can degrade trajectory realism; rollback-only curricula lead to overly cautious behavior (poor forward progress); diverse, non-stationary web UIs remain hard to model; integrating world-model-based curricula into online RL risks model collapse and misalignment; pruning heuristics can accidentally remove useful near-duplicate variants.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Curriculum improves multi-step performance: adding the state-transition task (third stage of TextUI warm-up) increased overall performance by 5.23%, indicating explicit transition modeling aids long-horizon planning. WebSynthesis demonstrates improved long-horizon success rates (Pass@3 = 20.15%) relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Domain focuses on web UI navigation; curriculum emphasizes UI understanding (layout, element function, state-change prediction). No results reported for highly specialized non-UI technical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Reported ablations: (1) incremental addition of FC tasks shows caption+function+transition yields the largest gains (transition adds +5.23% overall); (2) trajectory supervision ablation: τ_roll 1.49% vs τ_val 5.97% vs τ_val∪τ_roll 9.70% vs full (with TextUI) 14.93%; (3) TextUI warm-up applied to OS-Genesis yielded +33.4% improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Not tested for curriculum generation: LLM sizes for GPT-4/GPT-4o and Qwen2.5 used, but no cross-size scaling experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>A two-stage curriculum (UI fundamentals then trajectory cloning) where LLMs assist in generating rich single-step UI datasets and an LLM/world-model + MCTS synthesizes high-quality multi-step trajectories is highly sample-efficient: ~4k WebMCTS-synthesized samples yield better or comparable performance to much larger real or tutorial-guided datasets. Key elements driving effectiveness: (a) explicit UI state-transition modeling in the warm-up, (b) combining valuable and rollback trajectories, and (c) goal-directed search (WebMCTS) guided by an LLM reward model. Removing any of these components substantially reduces performance (e.g., rollback-only or no warm-up).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2052.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2052.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebMCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World-model-guided Monte Carlo Tree Search (WebMCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MCTS procedure that uses an LLM-based world model to simulate next observations and an LLM-based process reward model to score partial trajectories, enabling offline, goal-directed synthesis of diverse web UI trajectories (valuable and rollback).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>learned synthetic trajectory generator (LLM/world-model + search)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Qwen2.5-7B (world model), GPT-4 (process reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>At each MCTS node the policy proposes candidate actions; the LLM-based world model predicts next-page A11y observations; the process reward model (GPT-4) evaluates action quality conditioned on the user's intent. Node selection uses UCB (v_C + ε * ln n_P / n_C). Each expansion samples >=3 distinct actions; caching by URL avoids regenerating identical states. After search, DFS extracts high-value 'valuable' trajectories (nodes above threshold) and GPT-4 synthesizes reflections on failed sibling nodes to produce 'rollback' corrective trajectories. GPT-4 also prunes semantically redundant nodes prior to extracting training data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Web UI navigation (WebArena)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Partially observable, stochastic-looking web interfaces where actions (click/type/goto/scroll/go_back) cause structural page changes; tasks are multi-step and goal-directed.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>World model conditioned on previous observation o_{t-1} and candidate action a_t; reward model conditioned on predicted observation o_t and user intent q; caching keyed by URL.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>Exploration is enforced by UCB and by sampling multiple candidate actions per expansion; GPT-4 pruning merges near-duplicate actions to prevent redundant branches while keeping semantically distinct ones; rollback extraction increases trajectory variety.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Policy proposer (policy agent), LLM world model, GPT-4 reward model, URL-based caching, GPT-4-based pruning/reflection, downstream supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>WebMCTS-generated trajectories used to train policy achieve Overall Pass@3 = 20.15% and Pass@1 = 14.93% for WebSynthesis (with ~4k synthetic samples). WebMCTS enables sample-efficient generation of high-information-density trajectories that outperform larger tutorial or real-data-based datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Comparative ablation: training on only 'valuable' vs only 'rollback' vs both shows that combining both types of trajectories synthesized by WebMCTS is superior (τ_val 5.97% overall, τ_roll 1.49%, τ_val∪τ_roll 9.70%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Action trees produce both successful and multiple failed branches; dataset collected ~4k trajectories. No explicit count of distinct goals generated is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>WebMCTS-synthesized trajectories lead to policy generalization competitive with policies trained on larger real datasets (OS-Genesis) and better than tutorial-guided AgentTrek.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitative: reduces live-environment API calls and associated cost; exact tokens/cost not reported. The method is motivated by API cost savings when direct interactions require hundreds of queries per trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>World model inaccuracies and compounding rollout errors can produce unrealistic trajectories; cached states by URL assume deterministic page content at that URL which may not hold in real web; pruning heuristics might remove useful minor-variants.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>WebMCTS enables goal-directed multi-step planning in simulated rollouts and contributes to improved long-horizon task success (as shown in overall Pass@ metrics and the benefit of the state-transition warm-up).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations on trajectory supervision types show that combining both valuable and rollback trajectories extracted from WebMCTS yields the best result prior to adding TextUI warm-up (9.70% overall), and WebMCTS data with warm-up yields 14.93%.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Using an LLM-based world model with MCTS and an LLM reward model produces high-quality, diverse, goal-directed synthetic trajectories; these trajectories are sample-efficient and can match/exceed policies trained on larger real or tutorial-guided datasets when combined with a UI fundamental warm-up curriculum.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2052.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2052.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / GPT-4o (data & reward)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 and GPT-4o used for data augmentation, pruning, reflection synthesis, and process reward modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs (GPT-4o and GPT-4) are used within the pipeline to (a) generate dense captions and element functionality descriptions from screenshots, (b) augment instruction templates and refine responses, (c) prune redundant MCTS nodes, (d) synthesize rollback reflections and corrective 'go_back' actions, and (e) act as the process reward model to score partial trajectories during MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (used to synthesize single-step TextUI tasks and to refine/augment instruction templates; also used to synthesize rollback corrections and to act as a reward-model guide during search)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o (for dense captioning/functionality generation), GPT-4 (for pruning, reflection synthesis, process reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLMs generate rich single-step supervisory data (dense captions, element functions) paired with textual A11y observations to create the TextUI warm-up dataset. GPT-4 generates multiple instruction templates to diversify training prompts and reformulates complex responses. During WebMCTS, GPT-4 evaluates partial trajectories (process reward) to guide tree expansion and is used to detect and merge semantically redundant actions as well as to generate corrective reflections from failed sibling nodes to produce rollback training trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Web UI navigation (WebArena)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>LLM-assisted generation is applied to partially-observable textual UIs (A11y) paired with screenshots; needs spatial/layout understanding and element semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>LLM prompts include GUI screenshots and corresponding textual A11y observations for dense captioning and functionality generation; during reward evaluation and reflection synthesis GPT-4 conditions on historical observations, reasoning traces, actions, and user objective.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td>GPT-4 is prompted to produce multiple instruction templates and to synthesize reflections that provide alternate corrective paths, thereby increasing diversity; GPT-4 also used to detect redundancies and merge duplicates to maintain meaningful variance.</td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used together with Qwen2.5 world model, MCTS search, URL caching, and downstream policy SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Use of GPT-4o/GPT-4 in TextUI generation and WebMCTS components contributes to the WebSynthesis performance: overall Pass@1 = 14.93% and Pass@3 = 20.15% with ~4k synthetic samples; TextUI warm-up (LLM-generated data) improves OS-Genesis by +33.4% when applied as warm-up.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>LLM augmentation yields diverse instruction templates and varied QA pairs; no explicit numeric diversity count reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>LLM-augmented TextUI warm-up improves downstream trajectory learning across multiple baselines (e.g., OS-Genesis improved by +33.4%), indicating that LLM-generated single-step curriculum transfers to better multi-step navigation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Paper notes high API cost of live interactions and prior synthesis pipelines; LLM usage shifts cost offline but exact token/API-call counts for GPT-4/GPT-4o usage are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>LLM-generated content may include redundant or overly similar QA pairs requiring pruning; reliance on LLMs as reward models/world-model components inherits their biases and hallucination risks, and compounding errors across multi-step imagined rollouts remain a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>LLM-generated TextUI warm-up (via GPT-4o) and GPT-4-guided MCTS improve multi-step trajectory quality; the state-transition task (LLM-assisted) particularly boosts long-horizon performance (+5.23%).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Applying TextUI warm-up (LLM-generated) to baselines produced notable gains (OS-Genesis +33.4%), and ablating state-transition task (part of LLM-generated warm-up) reduces downstream performance by ~5.23%.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLMs (GPT-4o/GPT-4) effectively generate rich single-step UI supervisory data and provide useful search-time evaluation and corrective reflection synthesis; their use in both warm-up dataset generation and in-process reward/pruning materially improves sample efficiency and final task performance when combined with world-model-guided search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2052.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2052.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentTrek (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentTrek — tutorial-guided synthetic trajectory generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that converts publicly available web tutorials into step-by-step actions and validates them with a vision-language agent to scale multimodal trajectory generation; used here as a baseline (tutorial-guided curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AgentTrek: Agent trajectory synthesis via guiding replay with web tutorials.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>manual/tutorial-guided synthetic curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>AgentTrek leverages publicly available web tutorials as surrogates for human demonstrations by converting instructional content into action sequences and validating them with an agent. In this paper, 20k trajectories were sampled from AgentTrek to align with WebArena action space for baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>WebArena (Web UI navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Tutorial-derived step-by-step demonstrations typically follow fixed templates; may lack broad coverage of edge-cases and recovery behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Vision-language validation used in original AgentTrek pipeline (not detailed in this paper's experiments beyond using sampled trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>AgentTrek baseline (sampled 20k) reported Overall Pass@3 = 11.94% and Pass@1 = 9.70% on WebArena-Lite in this paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Original AgentTrek dataset contains 57k trajectories (paper used 20k sample for fair comparison); no further diversity metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>AgentTrek underperforms compared to WebSynthesis synthetic curriculum and to OS-Genesis real-data baseline in these evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Tutorial-guided trajectories may only cover instructional templates and can miss edge cases, recovery strategies, and state-transition reasoning; less effective for diverse or long-horizon tasks compared to WebMCTS-generated trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Lower long-horizon performance relative to WebSynthesis and OS-Genesis (reflected in lower Pass@ metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Tutorial-guided synthetic curricula scale cheaply but may lack diversity and recovery examples; in this paper AgentTrek's tutorial-derived data yields substantially lower performance than the LLM/world-model-guided synthetic curriculum (WebSynthesis).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2052.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2052.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OS-Genesis (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OS-Genesis — reverse task synthesis from agent exploration (real trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline pipeline that has the agent explore GUIs freely and then uses retrospective analysis and a reward model to extract high-quality tasks from those trajectories (real-trajectory based curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OS-Genesis: Automating gui agent trajectory construction via reverse task synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>agent-exploration retrospective (real trajectory collection with post-hoc reward-based extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>OS-Genesis collects real interactions (7.4k trajectories from WebArena) by agent exploration and applies retrospective task extraction via a reward model to produce training trajectories. In this paper it is used as a real-data baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>WebArena (Web UI navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Real web environment interactions capture realistic dynamics and noise but are costly and non-deterministic; trajectories may include rich edge cases but are expensive to collect and reproduce.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>OS-Genesis (7.4k real trajectories) reported Pass@3 = 18.66% and Pass@1 = 11.19% in this paper's experiments. Applying TextUI warm-up prior to trajectory-level fine-tuning improved OS-Genesis by +33.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>OS-Genesis trained on 7.4k real trajectories collected from WebArena; no further task-diversity statistics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>OS-Genesis performance improves substantially when combined with TextUI warm-up (LLM-generated warm-up data), suggesting that UI fundamental understanding generalizes across real-trajectory curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Real-world trajectory collection is noted as costly and non-deterministic; paper argues WebSynthesis reduces such costs by simulating interactions offline, but no monetary numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Real/sandboxed web environments yield unstable and non-deterministic feedback complicating reproduction and debugging; high API cost for collecting multi-step trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Competitive but outperformed by WebSynthesis when the latter uses LLM/world-model-guided synthetic data with UI warm-up.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Real-trajectory curricula are effective but costly and brittle; combining them with LLM-generated UI warm-up yields large gains (e.g., +33.4% for OS-Genesis), indicating complementary benefits of LLM-generated curriculum elements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WebEvolver: Enhancing web agent self-improvement with coevolving world model. <em>(Rating: 2)</em></li>
                <li>WebDreamer <em>(Rating: 2)</em></li>
                <li>OS-Genesis: Automating gui agent trajectory construction via reverse task synthesis. <em>(Rating: 2)</em></li>
                <li>AgentTrek: Agent trajectory synthesis via guiding replay with web tutorials. <em>(Rating: 2)</em></li>
                <li>Proposer-agent-evaluator (PAE): Autonomous skill discovery for foundation model internet agents. <em>(Rating: 2)</em></li>
                <li>WebRL: Training llm web agents via self-evolving online curriculum reinforcement learning. <em>(Rating: 1)</em></li>
                <li>AutoWebGLM: A large language model based web navigating agent. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2052",
    "paper_id": "paper-280068297",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "WebSynthesis Curriculum",
            "name_full": "WebSynthesis two-stage curriculum (TextUI warm-up + trajectory-level SFT)",
            "brief_description": "A manually designed two-stage curriculum for training web-navigation agents: (1) UI fundamental understanding (dense captioning, element functionality, state-transition prediction) produced with LLM-assisted data augmentation, and (2) supervised fine-tuning on synthetic trajectories (valuable + rollback) generated by a world-model-guided MCTS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "hybrid (manual two-stage curriculum with LLM-assisted data generation and LLM/world-model-guided trajectory synthesis)",
            "llm_model_name": "GPT-4o (for dense captions and functionality description), GPT-4 (for node pruning/reflection and as process reward model), Qwen2.5-7B (used as world model and base policy model)",
            "llm_model_size": null,
            "curriculum_description": "Two-stage curriculum: Stage 1 (TextUI warm-up) is a supervised curriculum of single-step tasks to build UI understanding: (a) dense captioning (global page descriptions generated by GPT-4o from screenshots paired with text observations), (b) element functionality prediction (structured functional descriptions synthesized with GPT-4o and paired with A11y text), and (c) state-transition prediction (predict page layout change and caption given current observation + action). Stage 1 uses instruction/template augmentation and response refinement via GPT-4. Stage 2 performs supervised fine-tuning on multi-step trajectories produced offline by WebMCTS: an LLM-based world model (Qwen2.5) simulates next-page observations given actions, a process reward model (GPT-4) evaluates partial trajectories, and MCTS (UCB) searches for high-value branches. From the MCTS action tree the pipeline extracts 'valuable' successful trajectories (nodes above a value threshold) and 'rollback' trajectories synthesized by GPT-4 reflections that propose go_back corrections from sibling failure branches. The curriculum proceeds from simpler TextUI tasks to more complex multi-step trajectory cloning.",
            "domain_name": "WebArena (Web UI navigation / headless TextUI using accessibility tree representations)",
            "domain_characteristics": "Dynamic, diverse, partially observable, non-stationary web interfaces; tasks are compositional and multi-step (long-horizon) requiring UI layout understanding, element-level functionality recognition, state-transition modeling and multi-turn decision making; includes domains such as maps, e-commerce (shopping), CMS/admin, social forums (Reddit), and software development (Gitlab).",
            "state_conditioning": true,
            "state_conditioning_details": "Curriculum and trajectory synthesis condition on page observations represented as accessibility trees (A11y), prior observations o_{t-1}, actions a_t, and URL (used for caching); MCTS branches store predicted next-state observations from the world model and node values from the reward model.",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "WebMCTS uses UCB to balance exploration/exploitation, samples at least 3 distinct candidate actions at each expansion to encourage breadth, caches states by URL to avoid duplicate rollouts, prunes redundant semantically-duplicate actions (via GPT-4), and explicitly extracts rollback (failed + corrective) trajectories to increase diversity of training signals.",
            "complementary_systems": "LLM-based world model (Qwen2.5), process reward model (GPT-4), Monte Carlo Tree Search (UCB-based), GPT-4o-generated instruction/data augmentation, caching by URL, valuable+rollback trajectory extraction, TextUI warm-up (curriculum) before behavior cloning.",
            "performance_llm_curriculum": "WebSynthesis (LLM-assisted curriculum + WebMCTS synthetic data, ~4k trajectories) achieves Overall Pass@3 = 20.15% and Pass@1 = 14.93% on WebArena-Lite (165 tests). At 75% of synthetic data scale (~4k samples) performance matches GPT-4. Scaling synthetic data from ~500 to 4k yields +7.47% overall performance.",
            "performance_manual_curriculum": "OS-Genesis (real trajectory based / reverse task synthesis, 7.4k real trajectories) reported Pass@3 = 18.66% and Pass@1 = 11.19%; AgentTrek (tutorial-guided synthetic, sampled 20k for comparison) reported Pass@3 = 11.94% and Pass@1 = 9.70%. WebSynthesis outperforms both despite smaller dataset (~4k).",
            "performance_heuristic_curriculum": "Ablation on trajectory supervision: rollback-only training (τ_roll) yields Overall = 1.49%; valuable-only (τ_val) yields 5.97%; using both τ_val ∪ τ_roll yields 9.70%; adding TextUI warm-up (full WebSynthesis) yields 14.93% overall. These numbers show heuristic variants (only rollback or only valuable) underperform the combined approach.",
            "performance_no_curriculum": "Chain-of-thought prompting (no fine-tuned curriculum): GPT-4 (CoT) Pass@1 overall = 13.58%; Qwen2.5-7B (CoT) Pass@1 = 2.24%. WebSynthesis (with its curriculum) exceeds these for the fine-tuned Qwen2.5 base.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Synthesized dataset size ~4k trajectories; WebMCTS action trees include both successful (valuable) and multiple failed sibling branches used to create rollback trajectories. No explicit numeric count of distinct task templates reported beyond the trajectory counts.",
            "transfer_generalization_results": "Agent trained on WebSynthesis synthetic data generalizes comparably to models trained on larger real datasets; at ~4k synthetic samples WebSynthesis matches GPT-4 performance and outperforms OS-Genesis trained on 7.4k real trajectories — indicating strong transfer/generalization from LLM-assisted synthetic curriculum to real WebArena tasks.",
            "computational_cost": "Qualitative: designed to reduce online evaluation/API cost by replacing live interactions with an LLM-based world model and offline MCTS; the paper emphasizes orders-of-magnitude savings relative to thousands of live queries per trajectory in prior work but does not report dollar or token counts.",
            "failure_modes_limitations": "World-model fidelity limits reliability: imperfect world model rollouts and compounding multi-step errors can degrade trajectory realism; rollback-only curricula lead to overly cautious behavior (poor forward progress); diverse, non-stationary web UIs remain hard to model; integrating world-model-based curricula into online RL risks model collapse and misalignment; pruning heuristics can accidentally remove useful near-duplicate variants.",
            "long_horizon_performance": "Curriculum improves multi-step performance: adding the state-transition task (third stage of TextUI warm-up) increased overall performance by 5.23%, indicating explicit transition modeling aids long-horizon planning. WebSynthesis demonstrates improved long-horizon success rates (Pass@3 = 20.15%) relative to baselines.",
            "specialized_domain_performance": "Domain focuses on web UI navigation; curriculum emphasizes UI understanding (layout, element function, state-change prediction). No results reported for highly specialized non-UI technical domains.",
            "ablation_studies": "Reported ablations: (1) incremental addition of FC tasks shows caption+function+transition yields the largest gains (transition adds +5.23% overall); (2) trajectory supervision ablation: τ_roll 1.49% vs τ_val 5.97% vs τ_val∪τ_roll 9.70% vs full (with TextUI) 14.93%; (3) TextUI warm-up applied to OS-Genesis yielded +33.4% improvement.",
            "model_size_scaling": "Not tested for curriculum generation: LLM sizes for GPT-4/GPT-4o and Qwen2.5 used, but no cross-size scaling experiments reported.",
            "key_findings_curriculum_effectiveness": "A two-stage curriculum (UI fundamentals then trajectory cloning) where LLMs assist in generating rich single-step UI datasets and an LLM/world-model + MCTS synthesizes high-quality multi-step trajectories is highly sample-efficient: ~4k WebMCTS-synthesized samples yield better or comparable performance to much larger real or tutorial-guided datasets. Key elements driving effectiveness: (a) explicit UI state-transition modeling in the warm-up, (b) combining valuable and rollback trajectories, and (c) goal-directed search (WebMCTS) guided by an LLM reward model. Removing any of these components substantially reduces performance (e.g., rollback-only or no warm-up).",
            "uuid": "e2052.0"
        },
        {
            "name_short": "WebMCTS",
            "name_full": "World-model-guided Monte Carlo Tree Search (WebMCTS)",
            "brief_description": "An MCTS procedure that uses an LLM-based world model to simulate next observations and an LLM-based process reward model to score partial trajectories, enabling offline, goal-directed synthesis of diverse web UI trajectories (valuable and rollback).",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "learned synthetic trajectory generator (LLM/world-model + search)",
            "llm_model_name": "Qwen2.5-7B (world model), GPT-4 (process reward model)",
            "llm_model_size": null,
            "curriculum_description": "At each MCTS node the policy proposes candidate actions; the LLM-based world model predicts next-page A11y observations; the process reward model (GPT-4) evaluates action quality conditioned on the user's intent. Node selection uses UCB (v_C + ε * ln n_P / n_C). Each expansion samples &gt;=3 distinct actions; caching by URL avoids regenerating identical states. After search, DFS extracts high-value 'valuable' trajectories (nodes above threshold) and GPT-4 synthesizes reflections on failed sibling nodes to produce 'rollback' corrective trajectories. GPT-4 also prunes semantically redundant nodes prior to extracting training data.",
            "domain_name": "Web UI navigation (WebArena)",
            "domain_characteristics": "Partially observable, stochastic-looking web interfaces where actions (click/type/goto/scroll/go_back) cause structural page changes; tasks are multi-step and goal-directed.",
            "state_conditioning": true,
            "state_conditioning_details": "World model conditioned on previous observation o_{t-1} and candidate action a_t; reward model conditioned on predicted observation o_t and user intent q; caching keyed by URL.",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "Exploration is enforced by UCB and by sampling multiple candidate actions per expansion; GPT-4 pruning merges near-duplicate actions to prevent redundant branches while keeping semantically distinct ones; rollback extraction increases trajectory variety.",
            "complementary_systems": "Policy proposer (policy agent), LLM world model, GPT-4 reward model, URL-based caching, GPT-4-based pruning/reflection, downstream supervised fine-tuning.",
            "performance_llm_curriculum": "WebMCTS-generated trajectories used to train policy achieve Overall Pass@3 = 20.15% and Pass@1 = 14.93% for WebSynthesis (with ~4k synthetic samples). WebMCTS enables sample-efficient generation of high-information-density trajectories that outperform larger tutorial or real-data-based datasets.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Comparative ablation: training on only 'valuable' vs only 'rollback' vs both shows that combining both types of trajectories synthesized by WebMCTS is superior (τ_val 5.97% overall, τ_roll 1.49%, τ_val∪τ_roll 9.70%).",
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Action trees produce both successful and multiple failed branches; dataset collected ~4k trajectories. No explicit count of distinct goals generated is reported.",
            "transfer_generalization_results": "WebMCTS-synthesized trajectories lead to policy generalization competitive with policies trained on larger real datasets (OS-Genesis) and better than tutorial-guided AgentTrek.",
            "computational_cost": "Qualitative: reduces live-environment API calls and associated cost; exact tokens/cost not reported. The method is motivated by API cost savings when direct interactions require hundreds of queries per trajectory.",
            "failure_modes_limitations": "World model inaccuracies and compounding rollout errors can produce unrealistic trajectories; cached states by URL assume deterministic page content at that URL which may not hold in real web; pruning heuristics might remove useful minor-variants.",
            "long_horizon_performance": "WebMCTS enables goal-directed multi-step planning in simulated rollouts and contributes to improved long-horizon task success (as shown in overall Pass@ metrics and the benefit of the state-transition warm-up).",
            "specialized_domain_performance": null,
            "ablation_studies": "Ablations on trajectory supervision types show that combining both valuable and rollback trajectories extracted from WebMCTS yields the best result prior to adding TextUI warm-up (9.70% overall), and WebMCTS data with warm-up yields 14.93%.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Using an LLM-based world model with MCTS and an LLM reward model produces high-quality, diverse, goal-directed synthetic trajectories; these trajectories are sample-efficient and can match/exceed policies trained on larger real or tutorial-guided datasets when combined with a UI fundamental warm-up curriculum.",
            "uuid": "e2052.1"
        },
        {
            "name_short": "GPT-4 / GPT-4o (data & reward)",
            "name_full": "GPT-4 and GPT-4o used for data augmentation, pruning, reflection synthesis, and process reward modeling",
            "brief_description": "LLMs (GPT-4o and GPT-4) are used within the pipeline to (a) generate dense captions and element functionality descriptions from screenshots, (b) augment instruction templates and refine responses, (c) prune redundant MCTS nodes, (d) synthesize rollback reflections and corrective 'go_back' actions, and (e) act as the process reward model to score partial trajectories during MCTS.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (used to synthesize single-step TextUI tasks and to refine/augment instruction templates; also used to synthesize rollback corrections and to act as a reward-model guide during search)",
            "llm_model_name": "GPT-4o (for dense captioning/functionality generation), GPT-4 (for pruning, reflection synthesis, process reward model)",
            "llm_model_size": null,
            "curriculum_description": "LLMs generate rich single-step supervisory data (dense captions, element functions) paired with textual A11y observations to create the TextUI warm-up dataset. GPT-4 generates multiple instruction templates to diversify training prompts and reformulates complex responses. During WebMCTS, GPT-4 evaluates partial trajectories (process reward) to guide tree expansion and is used to detect and merge semantically redundant actions as well as to generate corrective reflections from failed sibling nodes to produce rollback training trajectories.",
            "domain_name": "Web UI navigation (WebArena)",
            "domain_characteristics": "LLM-assisted generation is applied to partially-observable textual UIs (A11y) paired with screenshots; needs spatial/layout understanding and element semantics.",
            "state_conditioning": true,
            "state_conditioning_details": "LLM prompts include GUI screenshots and corresponding textual A11y observations for dense captioning and functionality generation; during reward evaluation and reflection synthesis GPT-4 conditions on historical observations, reasoning traces, actions, and user objective.",
            "novelty_mechanism": true,
            "novelty_mechanism_details": "GPT-4 is prompted to produce multiple instruction templates and to synthesize reflections that provide alternate corrective paths, thereby increasing diversity; GPT-4 also used to detect redundancies and merge duplicates to maintain meaningful variance.",
            "complementary_systems": "Used together with Qwen2.5 world model, MCTS search, URL caching, and downstream policy SFT.",
            "performance_llm_curriculum": "Use of GPT-4o/GPT-4 in TextUI generation and WebMCTS components contributes to the WebSynthesis performance: overall Pass@1 = 14.93% and Pass@3 = 20.15% with ~4k synthetic samples; TextUI warm-up (LLM-generated data) improves OS-Genesis by +33.4% when applied as warm-up.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "LLM augmentation yields diverse instruction templates and varied QA pairs; no explicit numeric diversity count reported.",
            "transfer_generalization_results": "LLM-augmented TextUI warm-up improves downstream trajectory learning across multiple baselines (e.g., OS-Genesis improved by +33.4%), indicating that LLM-generated single-step curriculum transfers to better multi-step navigation performance.",
            "computational_cost": "Paper notes high API cost of live interactions and prior synthesis pipelines; LLM usage shifts cost offline but exact token/API-call counts for GPT-4/GPT-4o usage are not reported.",
            "failure_modes_limitations": "LLM-generated content may include redundant or overly similar QA pairs requiring pruning; reliance on LLMs as reward models/world-model components inherits their biases and hallucination risks, and compounding errors across multi-step imagined rollouts remain a limitation.",
            "long_horizon_performance": "LLM-generated TextUI warm-up (via GPT-4o) and GPT-4-guided MCTS improve multi-step trajectory quality; the state-transition task (LLM-assisted) particularly boosts long-horizon performance (+5.23%).",
            "specialized_domain_performance": null,
            "ablation_studies": "Applying TextUI warm-up (LLM-generated) to baselines produced notable gains (OS-Genesis +33.4%), and ablating state-transition task (part of LLM-generated warm-up) reduces downstream performance by ~5.23%.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLMs (GPT-4o/GPT-4) effectively generate rich single-step UI supervisory data and provide useful search-time evaluation and corrective reflection synthesis; their use in both warm-up dataset generation and in-process reward/pruning materially improves sample efficiency and final task performance when combined with world-model-guided search.",
            "uuid": "e2052.2"
        },
        {
            "name_short": "AgentTrek (baseline)",
            "name_full": "AgentTrek — tutorial-guided synthetic trajectory generation",
            "brief_description": "A prior method that converts publicly available web tutorials into step-by-step actions and validates them with a vision-language agent to scale multimodal trajectory generation; used here as a baseline (tutorial-guided curriculum).",
            "citation_title": "AgentTrek: Agent trajectory synthesis via guiding replay with web tutorials.",
            "mention_or_use": "use",
            "curriculum_generator_type": "manual/tutorial-guided synthetic curriculum",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "AgentTrek leverages publicly available web tutorials as surrogates for human demonstrations by converting instructional content into action sequences and validating them with an agent. In this paper, 20k trajectories were sampled from AgentTrek to align with WebArena action space for baseline comparison.",
            "domain_name": "WebArena (Web UI navigation)",
            "domain_characteristics": "Tutorial-derived step-by-step demonstrations typically follow fixed templates; may lack broad coverage of edge-cases and recovery behaviors.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Vision-language validation used in original AgentTrek pipeline (not detailed in this paper's experiments beyond using sampled trajectories).",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": "AgentTrek baseline (sampled 20k) reported Overall Pass@3 = 11.94% and Pass@1 = 9.70% on WebArena-Lite in this paper's evaluation.",
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Original AgentTrek dataset contains 57k trajectories (paper used 20k sample for fair comparison); no further diversity metrics reported here.",
            "transfer_generalization_results": "AgentTrek underperforms compared to WebSynthesis synthetic curriculum and to OS-Genesis real-data baseline in these evaluations.",
            "computational_cost": null,
            "failure_modes_limitations": "Tutorial-guided trajectories may only cover instructional templates and can miss edge cases, recovery strategies, and state-transition reasoning; less effective for diverse or long-horizon tasks compared to WebMCTS-generated trajectories.",
            "long_horizon_performance": "Lower long-horizon performance relative to WebSynthesis and OS-Genesis (reflected in lower Pass@ metrics).",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Tutorial-guided synthetic curricula scale cheaply but may lack diversity and recovery examples; in this paper AgentTrek's tutorial-derived data yields substantially lower performance than the LLM/world-model-guided synthetic curriculum (WebSynthesis).",
            "uuid": "e2052.3"
        },
        {
            "name_short": "OS-Genesis (baseline)",
            "name_full": "OS-Genesis — reverse task synthesis from agent exploration (real trajectories)",
            "brief_description": "A baseline pipeline that has the agent explore GUIs freely and then uses retrospective analysis and a reward model to extract high-quality tasks from those trajectories (real-trajectory based curriculum).",
            "citation_title": "OS-Genesis: Automating gui agent trajectory construction via reverse task synthesis.",
            "mention_or_use": "use",
            "curriculum_generator_type": "agent-exploration retrospective (real trajectory collection with post-hoc reward-based extraction)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "OS-Genesis collects real interactions (7.4k trajectories from WebArena) by agent exploration and applies retrospective task extraction via a reward model to produce training trajectories. In this paper it is used as a real-data baseline.",
            "domain_name": "WebArena (Web UI navigation)",
            "domain_characteristics": "Real web environment interactions capture realistic dynamics and noise but are costly and non-deterministic; trajectories may include rich edge cases but are expensive to collect and reproduce.",
            "state_conditioning": false,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": null,
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": "OS-Genesis (7.4k real trajectories) reported Pass@3 = 18.66% and Pass@1 = 11.19% in this paper's experiments. Applying TextUI warm-up prior to trajectory-level fine-tuning improved OS-Genesis by +33.4%.",
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "OS-Genesis trained on 7.4k real trajectories collected from WebArena; no further task-diversity statistics reported here.",
            "transfer_generalization_results": "OS-Genesis performance improves substantially when combined with TextUI warm-up (LLM-generated warm-up data), suggesting that UI fundamental understanding generalizes across real-trajectory curricula.",
            "computational_cost": "Real-world trajectory collection is noted as costly and non-deterministic; paper argues WebSynthesis reduces such costs by simulating interactions offline, but no monetary numbers reported.",
            "failure_modes_limitations": "Real/sandboxed web environments yield unstable and non-deterministic feedback complicating reproduction and debugging; high API cost for collecting multi-step trajectories.",
            "long_horizon_performance": "Competitive but outperformed by WebSynthesis when the latter uses LLM/world-model-guided synthetic data with UI warm-up.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Real-trajectory curricula are effective but costly and brittle; combining them with LLM-generated UI warm-up yields large gains (e.g., +33.4% for OS-Genesis), indicating complementary benefits of LLM-generated curriculum elements.",
            "uuid": "e2052.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WebEvolver: Enhancing web agent self-improvement with coevolving world model.",
            "rating": 2
        },
        {
            "paper_title": "WebDreamer",
            "rating": 2
        },
        {
            "paper_title": "OS-Genesis: Automating gui agent trajectory construction via reverse task synthesis.",
            "rating": 2
        },
        {
            "paper_title": "AgentTrek: Agent trajectory synthesis via guiding replay with web tutorials.",
            "rating": 2
        },
        {
            "paper_title": "Proposer-agent-evaluator (PAE): Autonomous skill discovery for foundation model internet agents.",
            "rating": 2
        },
        {
            "paper_title": "WebRL: Training llm web agents via self-evolving online curriculum reinforcement learning.",
            "rating": 1
        },
        {
            "paper_title": "AutoWebGLM: A large language model based web navigating agent.",
            "rating": 1
        }
    ],
    "cost": 0.0208295,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis
6 Jul 2025</p>
<p>Yifei Gao yifeigao@bjtu.edu.cn 
Beijing Jiaotong University
BeijingChina</p>
<p>Junhong Ye 
Beijing Jiaotong University
BeijingChina</p>
<p>Jiaqi Wang 
Beijing Jiaotong University
BeijingChina</p>
<p>Jitao Sang jtsang@bjtu.edu.cn 
Beijing Jiaotong University
BeijingChina</p>
<p>WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis
6 Jul 20255AB51372EDB8CD1E7C582413069BF1A0arXiv:2507.04370v1[cs.AI]
Recent advancements in large language models (LLMs) have significantly improved the capabilities of web agents.However, effectively navigating complex and dynamic web environments still requires more advanced trajectory-level planning and execution.Prior studies have addressed self-improving agents by collecting extensive GUI trajectories from real-environment interactions.Despite their effectiveness, these approaches encounter two critical challenges: (1) Uncontrollable environment states, where real or sandboxed web environments often yield unstable and non-deterministic feedback, complicating the reproduction and debugging of agent behaviors; and (2) High API costs, as generating even a single interaction trajectory can involve hundreds of queries, leading to considerable API usage and computational expenses.To address these limitations and enable scalable self-improvement for agents, we propose WebSynthesis, a novel framework for trajectory synthesis and training.WebSynthesis leverages a learned world model to simulate virtual web environments, allowing a policy agent to perform efficient and reversible tree-based planning.This approach supports the large-scale generation of diverse and high-quality trajectories, which are subsequently utilized to refine the agent's policy.Experimental results demonstrate that an agent trained using WebSynthesis on a small-scale synthetic dataset achieves performance comparable to or even surpassing that of models trained on large-scale real-world data.</p>
<p>Introduction</p>
<p>Automated agents powered by large language models (LLMs) have demonstrated remarkable potential for web navigation tasks [6,28,30,26,22,5,13,23]. Web agents, such as Operator (OpenAI), Manus (Manus AI), are expected as human-like "eyes and hands," interacting with real-world web environments through multi-turn perception (reading) and action (writing) until successfully completing user-defined tasks.Consequently, training these agents on high-quality trajectories is crucial for enhancing their agentic capabilities [32,22,24].However, gathering large-scale, multi-step trajectories for web navigation tasks remains challenging.The dynamic and diverse nature of web interfaces makes manual demonstration collection both labor-intensive and unsustainable [7,2,24,19].Additionally, directly training agents via live web interactions is often unpredictable and costly due to noise from real-world environments and the substantial number of API calls required, making algorithm like reinforcement learning (RL) prohibitively expensive [18,13,23].</p>
<p>To address this data bottleneck, recent studies have increasingly investigated trajectory-level selfevolution approaches, where agents iteratively improve by learning from autonomously generated trajectories [22,34,24,5].This paradigm allows agents to bootstrap their training data, reducing dependence on human annotations and potentially uncovering novel interaction patterns beyond those demonstrated by humans.While effective, significant challenges persist in existing trajectory synthesis methods.First, accurately modeling the full complexity of modern web environments remains inherently difficult [33,11].Even advanced agents may fail when interacting with imperfect simulators, leading to limited scenario coverage or unrealistic behavior.Specifically: (1) Purely self-directed exploration often generates trajectories lacking diversity, causing learning to stagnate as agents repeatedly encounter familiar patterns [34].(2) Rule-based or tutorial-guided approaches typically address only predefined task templates, leaving many edge cases unexplored [24].Another critical issue is the high computational cost of data generation [17], as some synthesis pipelines require thousands of interactions per trajectory, resulting in millions of inference tokens and rendering the approach computationally prohibitive.</p>
<p>Motivated by these limitations, we propose WebSynthesis, a novel framework integrating world modeling [2,7,5] with search-based trajectory generation mechanisms [12,30].Acting as a high-fidelity proxy for the real web, world model facilitates the generation of diverse and rich trajectories without incurring the high costs associated with live interactions, thereby providing an imagined environment for effective agent exploration.To improve the quality and task relevance of synthesized trajectories of policy agents in the world model environment, we employ Monte Carlo Tree Search (MCTS) [35] to guide the synthesis towards a specific goal.This approach enables the world model to produce interaction sequences tailored to a wide variety of tasks and interface conditions, ensuring trajectories are goal-directed and diverse.Crucially, the search evaluates multiple hypothetical action paths within the world model, selectively retaining those leading to coherent and successful outcomes.By combining model-based environment simulation with goal-oriented search, WebSynthesis efficiently generates controllable and cost-effective web navigation trajectories.</p>
<p>During training, we adopt a two-stage curriculum learning framework.The first stage focuses on strengthening the fundamental UI understanding capabilities of policy agent, including tasks such as state captioning, functionality description, and state transition prediction [19,15].This prepares the agent to quickly adapt to complex and unfamiliar web interfaces.In the second stage, we perform Supervised Fine-Tuning (SFT) using the synthesized trajectory data.These synthetic trajectories enable the policy agent to imagine and rehearse a wide range of tasks within a safe virtual environment, thereby improving real-world generalization and accelerating self-improvement beyond the data limitations of prior methods.Experimental results demonstrate that with about 4k synthetic trajectories, WebSynthesis achieves a 20.15% overall Pass@3 success rate, outperforming OS-Genesis-7B [22] (18.66%, trained on 7.4k real-world trajectories) and AgentTrek-7B [24] (11.94%, trained on 20k tutorial-guided synthetic trajectories).Additionally, a TextUI warm-up study reveals that pretraining the three UI fundamental capabilities prior to trajectory-level fine-tuning boosts OS-Genesis performance by +33.4%, highlighting the critical role of UI understanding in agent training.</p>
<p>Our primary contributions are as follows: (1) We propose WebSynthesis, a novel framework that integrates an LLM-based world model with MCTS to synthesize diverse and controllable web interaction trajectories entirely offline.(2) We design a two-stage curriculum learning paradigm to comprehensively improve the UI fundamental understanding and web navigation capabilities.(3) With only 4,000 synthetic samples, WebSynthesis outperforms methods trained on significantly larger datasets, including both real-world-collected and tutorial-guided synthetic trajectories.</p>
<p>Related Works</p>
<p>Self-Evolving Agents</p>
<p>Self-evolving agents primarily improve trajectory quality and generalization capabilities through reinforcement learning or synthetic data generation.</p>
<p>Reinforcement learning approaches.AutoWebGLM [13] leverages carefully curated web browsing data to bootstrap the model through a three-stage curriculum learning framework, where the latter two stages incorporate Direct Preference Optimization (DPO) and rejection sampling for online scenarios.Approaches like WebRL [18] introduces a self-evolving online curriculum that automatically generates new tasks from the agent's failures and employs an outcome-based reward model, enabling an open-source agent to steadily improve and, in some cases, match or surpass GPT-4 on specific benchmarks.WebAgent-R1 [23], in contrast, demonstrates that a simple end-to-end reinforcement learning setup, combined with asynchronous trajectory exploration and binary success feedback, can significantly enhance long-horizon task success for open LLM agents, highlighting the effectiveness of direct environment interaction.</p>
<p>Data synthesis approaches.Complementing the above RL approaches, several studies address data scarcity by synthesizing rich training trajectories.OS-Genesis [22] reverses the conventional data collection process: the agent first explores GUI environments freely, and retrospective analysis then extracts high-quality tasks from its trajectories using a reward model.This approach generates diverse and realistic data without the need for human scripting.PAE [34] adopts a similar strategy for autonomous skill discovery, enabling the agent to propose new web tasks and self-evaluate outcomes using a vision-language model.This allows the agent to construct its own curriculum and reward signal, enabling policy refinement beyond any fixed instruction set.Meanwhile, AgentTrek [24] leverages publicly available web tutorials as surrogates for human demonstrations.It converts instructional content into step-by-step actions and validates them through a vision-language agent, thereby scaling multimodal trajectory generation at low cost and enhancing generalization across both textual and visual web benchmarks.WebCoT [9] identifies essential reasoning skills for effective web agents, including reflection, branching, and rollback, and synthesizes the corresponding reasoning trajectories.</p>
<p>World Model</p>
<p>In the era of large language models (LLMs), world models have emerged as generative AI systems designed to capture the dynamics of the real world, including its physical and spatial properties [3,1].In RL, world models are used to simulate future observations and environment feedback, enabling policy agents to learn and plan without interacting directly with the real environment [16].Recent work in web navigation adopts a "simulate-before-act" paradigm [8].Approaches such as WebDreamer [7] and WMA [2] leverage LLMs as world models to predict the outcomes of candidate actions in advance, thereby avoiding irreversible errors and reducing the cost of real-time interactions.However, the effectiveness of these methods is heavily dependent on the realism of the world model, which limits their reliability for online planning.WebEvolver [5] extends this paradigm by co-evolving the agent and the world model, allowing the agent to improve its decision-making by planning within an increasingly accurate simulated environment.Our work builds on this line of research by integrating LLM-based world modeling with goal-directed search.The world model is trained to generate detailed next-page observations, such as the DOM or accessibility tree, enabling the policy agent to perform efficient and diverse web navigation within a virtual environment.</p>
<p>Methodology</p>
<p>In this section, we present the details of the WebSynthesis pipeline, which contains both the data collection process and the curriculum learning framework.</p>
<p>Formulations.Following the standard reinforcement learning framework, we formulate web navigation as a partially observable Markov decision process (POMDP), consisting of four main components: the observation space (O), action space (A), transition function (T ), and reward function (R).In our approach, a web world simulator serves as the transition function T , mapping a state-action pair (O, A) to the next state observation.The policy agent π processes a user query q and engages in multi-step interaction within the world model environment, rather than operating on the real web interface.Consistent with prior work, we represent web page observations using the accessibility tree (A11y), which captures a structured set of accessibility-related states and properties.</p>
<p>Stage 1: UI Fundamental Understanding</p>
<p>In headless browsing settings, the policy agent primarily interacts with text-based page representations, which may involve various data formats such as HTML, the DOM tree, or the accessibility tree (A11y).Given the complexity of TextUI scenarios, even when complete page information is provided, the agent may struggle to fully comprehend the contextual information.To address this issue, we first collected and synthesized a series of TextUI datasets for the initial stage of supervised fine-tuning.We primarily adopt the public WebArena environment [33], a dynamic GUI sandbox that provides both textual observations (e.g., HTML and A11y) and visual observations (e.g., screenshots) of web pages.By performing random exploration in this environment, we obtain a large number of transition triples (o t−1 , a t , o t ), representing the transition from observation o t−1 to o t given action a t .Based on the collected transition triples, we follow the task definitions in [19] and curate three core capabilities for TextUI understanding (see Figure 1 for illustration): (1) dense captioning, (2) element functionality prediction, and (3) state transition captioning.The overall instruction template are shown in Table 5 6  7.</p>
<p>Dense Captioning: To enhance the model's global understanding of TextUI inputs, we address a key limitation of text-based representations, the absence of visual layout information.Directly summarizing textual content often proves insufficient for capturing the structure and context of complex interfaces.To overcome this, we leverage GPT-4o by providing it with corresponding GUI screenshots, enabling the generation of comprehensive and detailed descriptions that capture not only individual UI elements but also their spatial relationships and overall layout.During training, these descriptions are paired with text-based observations and provided as input to the model.</p>
<p>Element Functionality: To improve the recognition and understanding of specific elements in TextUIs (e.g., "textbars" or "clickable elements"), we focus on generating detailed, structured descriptions for each interactive component.Leveraging GPT-4o's strong visual understanding capabilities, similar to SoM [25], we prompt the model to synthesize functional descriptions based on both the text-based representation and the corresponding GUI screenshot.These synthesized descriptions are then paired with the text observation (i.e., A11y), replacing the screenshot as contextual input to form a QA pair.</p>
<p>To mitigate the generation of redundant QA pairs resulting from the excessive length of A11y, we apply a local compression strategy during preprocessing.Specifically, we retain only the elements adjacent to the target UI element, while preserving the original hierarchical structure of the A11y tree.This approach reduces unnecessary context and significantly enhances the diversity and efficiency of the resulting dataset.</p>
<p>State Transition Prediction:</p>
<p>To further enhance the model's understanding of UI elements, particularly in response to user interaction requests, we aim to equip the model with the ability to predict changes in the main content of the page.To achieve this, we capture the differences between consecutive observations and incorporate them into the agent's reasoning process.During training, the agent is provided with the current page observation and the corresponding action, and is tasked with generating the changes of layout and a caption for the subsequent page frame.This capability is essential for tasks requiring fine-grained interaction understanding and dynamic state awareness.</p>
<p>Stage 2: A World Model Guided Monte Carlo Tree Search</p>
<p>Consistent with other search-based approaches [29,27], WebSynthesis requires a process reward model to guide the policy agent through step-by-step decision-making.However, web environments present unique challenges.If the reward model evaluates only the actions proposed by the policy agent without considering their broader context, the accuracy of action-level reward signals can be significantly decreased.Inspired by WMA [2] and WebDreamer [7], we leverage the world model to simulate the web environment and approximate the consequences of the agent's actions.This allows the process reward model to make more informed and accurate assessments.An overview of the framework is shown in Figure 2. We now define the policy agent, the world model, and the process reward model as follows:</p>
<p>• Policy Agent: At each time step t, a policy agent π first generates an action a t according to the previous observation o t−1 and user instruction q.Please refer to Table 8 for prompt details. • World Model: A web world model ω then predicts the next observation o t based on o t−1 and a t .</p>
<p>Please refer to Table 9 for prompt details. • Process Reward Model: A reward model γ evaluates the actions issued by the policy agent based on the o t and the user's intent q.Please refer to Table 10 for prompt details.</p>
<p>Based on the above definition, the entire target of web navigation is formulated as follows:
argmax {a0,a1,•••a T } T t=0 γ θ (o t , a t )(1)
where o t ∼ ω θ (o t |o t−1 , a t ), a t ∼ π θ (a t |o t−1 , q) and argmax {a0,a1,•••a T } refers to the search algorithm.Within a fixed time T or step budget, the objective of the search is to identify a trajectory that maximizes the reward.</p>
<p>According to Eq. 1, we employ the process reward model γ to evaluate the quality of partial trajectories, enabling both selection and backpropagation at intermediate nodes.Building on this foundation, we formulate the world model-guided Monte Carlo Tree Search (WebMCTS) as an iterative algorithm comprising three main stages: node selection, action expansion, and backpropagation.In each iteration, the search tree is expanded based on candidate actions proposed by the policy agent π.Each node in the tree represents a specific action and stores the predicted next-state observation o t+1 from the world model, the corresponding reward feedback v C from the reward model, and the number of visits n C .</p>
<p>Node Selection: Following standard MCTS settings [29,35], node selection is guided by the Upper Confidence Bound (UCB) strategy, which balances exploration and exploitation during the search process.The UCB score for a child node C is defined as:
U C = v C + ϵ • ln n P n C
where n P is the number of visits to the parent node, and ϵ is an exploration constant.At each iteration, the algorithm selects the child node with the highest UCB score to continue the search.</p>
<p>Action Expansion: Given the node C selected by the UCB strategy, we sample candidate actions based on the state associated with C. To ensure sufficient exploration and maintain the breadth of the search tree, WebMCTS samples at least three distinct potential actions at each expansion step.Additionally, since different branches of the search tree may reach the same web page (i.e., the same URL) at different points during the search, we adopt a caching mechanism to ensure consistency in the generated states from the world model.Specifically, we maintain a hash table that uses the URL as the key to store previously generated page states.When the same URL is encountered again, the cached version is reused, thereby avoiding duplicate state generation and improving both consistency and efficiency during tree expansion.</p>
<p>Backpropagation: Finally, we start value backpropagation from the selected node C and update the values of its ancestor nodes using a weighted average strategy.Specifically, for each visited node C, we update its visit count and value estimate as follows:
n C ← n C + 1, and v C ← i n Ci • v Ci
i n Ci After multiple iterations, this process produces a web action tree simulated by the world model, capturing a diverse set of possible interaction trajectories.</p>
<p>Stage 3: Trajectory Collection</p>
<p>Building on Stage 2, the resulting web action tree contains not only the trajectory τ = {o t , a t } N t=0 that successfully accomplishes the task, but also a diverse set of meaningful yet unsuccessful attempts encountered during the search for τ .In this subsection, we describe the process of extracting training trajectories from the constructed action tree.Our objective is to identify and categorize two types of trajectories, as illustrated in Figure 3, (1) valuable trajectories, which positively contribute to policy learning, and (2) rollback trajectories, which represent failed exploration paths that can be used to improve the agent's robustness and error-awareness.</p>
<p>Valuable Trajectory: Given that the initial policy agent may generate redundant or semantically meaningless actions when interacting with the web world model, we first prune the action tree before collecting training trajectories.We leverage GPT-4 to detect nodes containing repeated or overly similar actions.For example, in type-related actions, the agent may issue both "type [1201] [bus stop near CMU] [1]" and "type [1201] [bus stop near Carnegie Mellon University] [1]", which result in nearly identical state predictions from the web world model.In these cases, we remove the redundant nodes and merge their child nodes into their respective parent nodes.</p>
<p>We employ the node value v C as a criterion for identifying valuable trajectories, where higher values signify a greater degree of task completion in the current state.Specifically, we perform a depth-first search (DFS) over the tree to locate target nodes whose values exceed a predefined threshold.For each identified target node, we trace the path backward to the root node, thereby extracting the corresponding trajectory for training.</p>
<p>Rollback Trajectory:</p>
<p>The core idea of rollback is to enable the policy agent to dynamically assess the effectiveness of its actions and their outcomes at each step along the trajectory.When an action leads to a result that deviates from the user's intent, the agent is expected to autonomously recognize the To extract rollback trajectories, we operate on each intermediate node C within a valuable trajectory.Specifically, we begin by identifying the set of unsuccessful sibling nodes S of C, along with their common ancestor node P .For each node in S, we leverage GPT-4 to synthesize a reflection that traces back to P , identifying the appropriate corrective action, denoted as "go_back".We then revise the historical action sequence at P to incorporate this correction, and construct a new trajectory that extends from P through S.This process yields rollback trajectories in the form S → P → C, capturing alternative reasoning paths that arise from failed attempts and their corresponding corrective feedback.</p>
<p>Stage 4: Policy Agent Training</p>
<p>As shown in Table 1, the training of the policy agent consists of two main components: UI fundamental capabilities and UI behavior cloning, which correspond to the training data collected in Stage 1 and Stage 3, respectively.</p>
<p>UI Fundamental Capabilities: The first step is Supervised Finetuning (SFT) on the collected data in stage 1, which enhances the fundamental UI understanding capability.Since the Stage 1 dataset spans multiple dimensions, some designed to improve the model's understanding of local and global UI information, and others intended to simulate simple user intentions, it is inappropriate to train on all samples at the same time.We thus adopt a curriculum learning (CL) strategy that mimics the human learning process, encouraging the model to first learn from simpler tasks and gradually progress to more complex ones.To enhance data quality, we follow InfiGUIAgent [15] and implement instruction enhancement and response refinement process: (1) for data of different dimensions, we prompt GPT-4 to generate various instruction templates to enhance the logic between instructions and answers and;</p>
<p>(2) for data with complex or inconsistent response formats, we leverage GPT-4 to reformulate responses while preserving their semantic content.</p>
<p>UI Behavior Cloning: Despite the above fundamental capabilities, the policy agent still lacks the ability to perform full web navigation due to the absence of complete trajectory training.To address this, we adopt supervised fine-tuning using both valuable and rollback trajectories, following a procedure consistent with existing methods based on manually collected trajectory data.After the above training procedure, the policy agent acquires the essential capabilities required to complete web browsing tasks and can autonomously execute operations based on user instructions.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Model Settings: We employ Qwen2.5-7B-Instruct as the base model for the policy agent and web world model training, and GPT-4 as the process reward model for tree search.We construct our dataset in online setting using the provided web environment for world model training.All training is performed as low rank adaption (LoRA) fine-tuning.Details are available in the Appendix.Evaluation Benchmarks: For the web navigation task, WebArena serves as a dynamic benchmark for evaluating autonomous agents.It covers domains such as maps, e-commerce (Shopping), content management (CMS), social forums (Reddit), and software development (Gitlab).Policy agents are required to make sequential decisions starting from an initial state, and task success is evaluated via string matching or url matching between the agent's final output and the ground-truth answer.We follow the standard WebArena setup and adopt the following action space, including "click", "type", "hover", "scroll", "goto", "go_back", and "stop".In the original WebArena environment, it comprises 812 challenging tasks derived from 241 task templates.Considering the cost of evaluation, we use 165 test cases from WebArena-Lite [14] for evaluation.Observations of the web environment are represented by the accessibility tree.</p>
<p>Baseline Settings: We compare our approach with the following baselines: (1) Qwen2.5-7B-Instruct,LLaMA3.1-8B-Instruct, and GPT-4, all evaluated under a setting with chain-of-thought prompting [33]; (2) OS-Genesis [22], which utilizes 7.4k real trajectories collected from WebArena;</p>
<p>(3) AgentTrek [24], which generates web agent trajectories by leveraging publicly available tutorials and contains 57k trajectory records.To ensure a fair comparison of headless browsing capabilities, we use only the text-based state representation (i.e., A11y) from the trajectories as input for all methods.For AgentTrek, we sample 20k trajectories from the original dataset to align with the action space used in WebArena.Both OS-Genesis and AgentTrek are fine-tuned on Qwen2.5-7B-Instruct.</p>
<p>Detailed training settings are provided in the Appendix.</p>
<p>Main Results</p>
<p>Table 2 presents that Qwen2.5-7B,training with WebSynthesis data, leads to notable performance improvements.Compared with chain-of-thought (CoT) prompting-based methods, including opensource models such as Qwen2.5-7B-Instructand LLaMA3.1-8B-Instruct,as well as closed-source models like GPT-4, WebSynthesis demonstrate superior performance across multiple WebArena subsets.Notably, WebSynthesis-SFT achieves a higher overall score (14.93%) than GPT-4 (13.58%), while significantly outperforming Qwen2.5-7B-Instruct(2.24%).In comparison to methods trained on real world trajectories, such as OS-Genesis-7B, which is trained on 7.4k real samples, WebSynthesis still yield better results.For example, WebSynthesis outperforms OS-Genesis by +3.74% overall in Pass@1, despite being trained on a substantially smaller dataset (approximately 4k samples).These results suggest that high-quality synthetic trajectories guided by WebMCTS are comparably or more effective, even at a smaller scale.</p>
<p>Analysis</p>
<p>How TextUI Works?</p>
<p>A core step of WebSynthesis is a preliminary TextUI warmup phase conducted before formal trajectory-level training.This phase is designed to equip the policy agent with fundamental UI understanding capabilities.To verify the effectiveness of TextUI warm-up in enhancing trajectory learning, we compare and analyze the performance of three trajectory learning methods, including OS-Genesis, AgentTrek and WebSynthesis, with and without TextUI warmup.</p>
<p>The experimental results are presented in Table 3.It is obvious that models with TextUI adaptation prior to trajectory-level fine-tuning consistently outperform their counterparts trained without such warmup.In particular, the improvement for OS-Genesis exceeds its original performance by 33.4%.This showcases that fine-tuning on real-world collected trajectories is not a one-shot solution.Without sufficient familiarity with the structure and layout of text-based UI, trajectory-level training alone is unlikely to yield the desired performance.</p>
<p>Ablation Studies of Fundamental Understanding</p>
<p>In the UI fundamental capabilities training phase, tasks from three dimensions are incorporated into the curriculum learning framework (see Section 3.1).To further investigate which task type plays a critical role, we conduct sequential ablation experiments by incrementally adding tasks according to their difficulty levels.As shown in Table 4, the three-stage curriculum learning gradually improved the web navigation ability of the policy agent.In particular, after the introduction of the state transition task in the third stage, the overall performance was improved by 5.23% compared with the first two stages, indicating that the transition process of modeling the world state has a significant promoting effect on policy learning.This observation verifies the necessity of guiding the model to understand the dynamics of UI changes in the early stage of training, and provides a more accurate foundation for environmental modeling for subsequent multi-step reasoning and action execution.</p>
<p>In addition, from the perspective of task dimension, although the introduction of dense captioning or functionality description can bring some improvement, its gain is relatively limited.In contrast, the combination of tasks (caption + functionality + transition) has a more significant improvement in policy performance, showing the multi-dimensional of UI understanding ability.The gray area in the last row shows the complete WebSynthesis method, which achieved the highest performance in each task scenario, verifying the effectiveness of the curriculum learning strategy.</p>
<p>Ablation Studies of Behaviors Cloning</p>
<p>Table 4 presents the ablation results comparing different trajectory supervision strategies.We examine four configurations: training on only valuable trajectories (τ val ), only rollback trajectories (τ roll ), their combination (τ val ∪ τ roll ), and full WebSynthesis training, which incorporates TextUI warm-up followed by combined trajectory training.</p>
<p>We observe that training solely on rollback trajectories (τ roll ) leads to the weakest performance (1.49% overall).This suggests that without exposure to successful trajectories, the agent becomes overly cautious and prone to issuing go_back actions prematurely, ultimately losing the ability to explore and complete tasks.This result highlights the importance of learning from target-directed behavior to establish confident forward execution.</p>
<p>Training on valuable trajectories only (τ val ) results in moderate performance (5.97% overall), as it teaches the agent to reach task goals but fails to expose it to real-world error patterns or recovery strategies.When both τ val and τ roll are used together, the model achieves a significant improvement (9.70% overall), indicating that exposure to both successful demonstrations and failure recovery paths enables the agent to reason more robustly in uncertain situations.</p>
<p>Importantly, WebSynthesis, which adds TextUI warm-up before trajectory-level fine-tuning, achieves the best overall performance (14.93%).This highlights the importance of the UI fundamental capabilities in improving the model's familiarity with the structure and layout of text-based UIs, thus facilitating more effective and sample-efficient policy learning in subsequent training stages.Without this foundation, the agent may struggle to interpret the UI context even with well-designed trajectories.By controlling the proportion of the WebSynthesis dataset, we aimed to explore the impact of increasing the synthetic data scale on the performance of policy agents.As shown in the Figure, our evaluation on WebArena demonstrates a steady improvement in performance with the expansion of data scale.Notably, the performance gain is 7.47% as the data scale increases from 12.5% (approximately 500 samples) to 100%.Remarkably, at 75% of the data (approximately 4k samples), the performance already matches that of GPT-4.This suggests that the high-information-density data synthesized by WebMCTS can achieve significant sample efficiency improvements with a smaller dataset.This finding highlights that automated synthetic data generation is a viable strategy to bridge the performance gap with real-world collected data, underscoring the tremendous potential for future scalability.</p>
<p>How Data Scaling Helps Agent Abilities?</p>
<p>Conclusion</p>
<p>In this work, we propose WebSynthesis, a framework that integrates world model learning with Monte Carlo Tree Search (MCTS) to significantly reduce the online cost of synthesizing high-quality Web UI trajectories.Through a two-stage curriculum, comprising UI fundamental understanding and UI behavior cloning, the policy agent acquires web navigation capabilities.Notably, the agent trained with WebSynthesis on a small-scale synthetic dataset achieves performance comparable to, or even surpassing, that of models trained on large-scale real-world data.</p>
<p>While WebSynthesis demonstrates the potential of leveraging world models to replace real web environments for offline data collection, a more ambitious and forward-looking direction is to integrate world models into online reinforcement learning, following paradigms explored in realworld RL research [20,21,31,10].In such a setting, the policy agent can continuously improve its planning and decision-making capabilities by directly interacting with a simulated environment constructed by the world model.</p>
<p>However, several key challenges remain before this vision can be fully realized in web environments.First, unlike board games or robotic control tasks, web interfaces are highly diverse, partially observable, and non-stationary, making it difficult to learn a stable and generalizable world model.Second, compounding errors in multi-step rollouts can significantly degrade performance, particularly when the agent relies heavily on imagined trajectories for exploration.Third, integrating model-based environments into online, closed-loop settings remains an open research problem.It is crucial not only to prevent model collapse over time, but also to ensure that the agent's behavior aligns with human-aligned criteria.Achieving this requires advances in both algorithmic frameworks and the fidelity of learned world models.Nevertheless, world models still hold strong potential for replacing real web environments in rare, costly, or safety-critical scenarios, where real-time interaction is limited or impractical.</p>
<p>B Prompt Template for Policy Agent, World Model and Reward Model</p>
<p>You are an autonomous intelligent agent tasked with navigating a web browser.You will be given web-based tasks.These tasks will be accomplished through the use of specific actions you can issue.</p>
<p>Here's the information you'll have:</p>
<p>• The user's objective: This is the task you're trying to complete.</p>
<p>• The current observation of web page.This is a simplified representation of the webpage, refer to as accessibility tree (a11y), providing key information.## Tips:</p>
<p>• If the page has element information that is useful for completing the task, you can perform page actions to explore, but please pay attention to the actual function of each element.It is forbidden to perform function B on an element that only has function A, such as retrieving other information besides geographic information in the search box of OpenStreetMap.</p>
<p>• Fuzzy search is prohibited.Your search must be based on a clear goal.For example, you can search for a pair of Nike shoes, but you are not allowed to search for a pair of shoes that cost around $60.</p>
<p>• If the page doesn't have information that helps you complete the task, you can perform url navigation actions, including the need to jump to a specific page (for example, jump to Reddit), or compare the information on the previous and next pages to help complete the task.</p>
<p>• If you think you have completed this task, please check your trajectory carefully and make a completion action carefully.</p>
<p>• If there is no information on the current page that can help complete the task, please also make a completion action carefully.</p>
<h2>Action Rules:</h2>
<p>To be successful, it is very important to follow the following rules:</p>
<p>1.You should think step by step and then issue the next action.Start with a "Let's think step-by-step."phrase.</p>
<ol>
<li>
<p>You should only issue an action that is valid given the current web page.</p>
</li>
<li>
<p>You should only issue one action at a time.</p>
</li>
<li>
<p>Generate the action in the correct format.Start with a "In summary, the next action I will perform is" phrase, followed by action inside """.For example, "In summary, the next action I will perform is "'click [1234]"'".</p>
</li>
<li>
<p>Issue stop action when you think you have achieved the objective.Don't generate anything after stop.-Do the steps and corresponding actions follow a logical sequence toward the goal?</p>
</li>
</ol>
<p>-Are the actions clearly described and specific?</p>
<p>-Are there redundant or unnecessary actions?</p>
<ol>
<li>Task Completion:</li>
</ol>
<p>-Does the trajectory successfully achieve the instructed task?</p>
<p>-Are all necessary interactions completed?</p>
<p>-Are error cases handled appropriately?</p>
<p>Scoring Guidelines: Rate the trajectory on a scale of 1 to 5 based on the evaluation criteria:</p>
<p>-5: The task is perfectly completed, successfully executing multiple actions to achieve the goal or return the correct answers.The sequence is logically clear with no noticeable redundancies.</p>
<p>-4: The task is mostly completed, successfully executing multiple actions.However, due to challenges or ambiguities in the instructions, the completion is not perfect, or there are inefficiencies in the process.</p>
<p>-3: The task is partially completed, with some successful actions executed.However, due to task or environmental constraints, the goal is not fully achieved, or the sequence ends in a loop or error.</p>
<p>-2: Only a few actions are executed.Although there is an attempt to complete the task, the trajectory deviates from the goal early on or demonstrates significant inefficiencies in execution and logic, e.g., repeat the same action.</p>
<p>-1: The task fails completely, with no meaningful actions executed at the start.The sequence either falls into an immediate deadlock, a repetitive loop, or demonstrates no value in completing the task.</p>
<p>Or the tasks are completely inaccessible.Note: If the task is relatively complex, but the trajectory demonstrates valuable attempts, even if the task is not fully completed, consider adjusting the score upward.However, if the task is complex but the trajectory fails to perform actions that contribute meaningfully to task completion, no extra points should be awarded.You need to judge the score based on the user instruction, agent's actions and the current state of the webpage combined.</p>
<p>Response Format: Format your response into two lines as shown below:</p>
<p>Reason: <your thoughts and reasoning process for the score> Score: <your score from 1-5></p>
<p>Figure 1 :
1
Figure 1: An overview of the UI Fundamental Understanding datasets, which offer single-step, finegrained annotations, including dense captions, element functionality, and state transitions, designed to train the model to understand, describe, and predict web page states.</p>
<p>Figure 2 :
2
Figure 2: The pipeline of world model-guided Monte Carlo Tree Search and trajectory collection.</p>
<p>Figure 3 :
3
Figure 3: An overview of the UI Behavior Cloning dataset, which provides multi-step demonstrations for training policy agents.Valuable trajectories capture high-quality, target-directed browsing sessions, while rollback trajectories illustrate error recovery through explicit "go_back" corrections.These datasets bridge the gap between understanding individual UI elements and performing end-to-end web navigation tasks.</p>
<p>Figure 4 :
4
Figure 4: Performance improvement with the synthetic trajectory data scaling up</p>
<p>Table 1 :
1
Curriculum overview for WebSynthesis training.Each class ('C') are designed for a specific capability, with approximate dataset scale and core learning objective.consequence of the incorrect decision, revert to a previously valid state o t−1 , and resume the reasoning process from that point.In WebMCTS, the resulted action tree maintains multiple parallel branches, which allows the agent to identify a diverse set of rollback trajectories from different high-value nodes within a single search episode.This contrasts with traditional linear or singlepath rollback strategies, which typically generate only one trajectory per run and risk overlooking alternative valuable paths.
LessonsCategoryScaleCore GoalC1&amp;stage 1Dense Captioning2kGrasp the overall semantics of TextUI expressionsC1&amp;stage 2Element Functionality6kRefine its fine-grained understanding of specific UI functionsC1&amp;stage 3State Transition Perdition7kStrengthen its ability to predict page-state transitionsC2. Behavior Cloning Valuable &amp; Rollback Trajectories4kPerform SFT on both valuable and rollback trajectories
negative</p>
<p>Table 2 :
2
Evaluation results (%) on different WebArena subsets.Red indicates the best score in each column; underlined numbers are second best.We also report the task success rate within one and three attempts, where a task is considered successfully completed if at least one of the sampled trials passes the evaluation criteria.
ModelParam SizeShoppingAdminRedditGitlabMapsOverallQwen2.57B2.170.000.006.25-2.24LLaMA3.18B8.705.710.006.25-5.97GPT-4 (CoT)N/A13.0417.149.5218.757.1413.58Pass@1AgentTrek [24]7B15.2211.434.763.13-9.70OS-Genesis [22]7B10.8714.290.0015.63-11.19WebSynthesis7B19.5717.149.529.38-14.93Pass@3AgentTrek [24]7B19.5711.439.523.13-11.94OS-Genesis [22]7B19.5731.430.0015.63-18.66WebSynthesis7B28.2620.0014.2912.50-20.15</p>
<p>Table 3 :
3
Evaluation on WebArena with and w/o TextUI warm-up.The value inside the bracket denotes the absolute change relative to the same model without TextUI: improvements are highlighted in red with a "+" sign, whereas degradations are highlighted in green with a "−" sign.
ModelShoppingAdminRedditGitlabMapsOverallAgentTrek-7B19.57 (+4.35)14.29 (+2.86)4.763.13-11.94 (+2.24)OS-Genesis-7B17.39 (+6.52)14.2919.05 (+19.05)6.25 (-9.38)-14.93 (+3.74)WebSynthesis-7B19.57 (+6.53)17.14 (+5.71)9.52 (+9.52)9.38-14.93 (+5.23)</p>
<p>Table 4 :
4
Ablation study on the contribution of UI Fundamental Capabilities (FC.) and UI Behavior Cloning (BC.) tasks."Cap."stands for dense captioning, "Func."represents element functionality, and "Trans."refers to state transition prediction.The trajectories τ val and τ roll denote the valuable and rollback trajectories respectively.A ✓ indicates that the task the corresponding task type was included during training.The shaded row corresponds to our final method, WebSynthesis.
FC.BC.Shopping Admin Reddit Gitlab Maps OverallCap. Func. Trans. τ roll τ val✓4.350.000.000.00-1.49✓6.522.869.526.25-5.97✓✓13.0411.430.009.38-9.70✓✓✓17.3911.439.520.00-10.45✓✓✓✓21.7414.294.763.13-12.69✓✓✓✓✓19.5717.149.529.38-14.93</p>
<p>•</p>
<p>The previous trajectory: This is the 'observations', 'thoughts' and 'actions' you have just performed.It may be helpful to track your progress.Each step is splited by <step></step> tag.This action clicks on an element with a specific id on the webpage.type [id] [content] [press_enter_after=0|1]: Use this to type the content into the field with id.By default, the "Enter" key is pressed after typing unless press_enter_after is set to 0. hover [id]: Hover over an element with id, this action may display hidden information about the element.scroll [direction=down|up]: Scroll the page up or down.This action will provide new or previously appeared page information.</p>
<h2>Action SpaceThe actions you can perform fall into several categories:### Page Operation Actions:click [id]:</h2>
<h3>URL Navigation Actions: goto [url]: Navigate to a specific url.go_back: Navigate to the previously viewed page.### Completion Action: stop [answer]: Issue this action when you believe the task is complete.If the objective is to find a text-based answer, provide the answer in the bracket.If you believe the task is impossible to complete, provide the answer as "N/A" in the bracket.</h3>
<p>Table 8 :
8
Policy Agent Prompt Template.
Evaluation Criteria:1. Trajectory Coherence:</p>
<p>Table 10 :
10
Reward Model Prompt Template.
OBJECTIVE:<insert user objective here>TRAJECTORY:<step-i>-OBSERVATION: <insert historical observation here>-REASON FOR ACTION: <insert historical thought here>-ACTION: <insert historical action here></step-i>• • •OBSERVATION:<insert current observation here>What's the next action?</p>
<p>Table 11 :
11
Policy Agent Instruction Template.</p>
<p>A UI Fundamental Understanding Instruction TemplateYou're a helpful web GUI assistant.OBSERVATION: {Web State}Instruction: Detail the main sections and functionalities available in this interface.Output: Let's think step-by-step.The web page displays a user interface from an e-commerce platform, specifically the "Pages" section.The layout is organized with a sidebar on the left featuring navigation icons for various functions such as Dashboard, Sales, Catalog, and more.The main area shows a search bar labeled Privacy Policy, indicating that users can search for specific pages.Below the search bar, there are options to manage actions and view filters.A table lists three pages with columns for ID, Title, URL Key, Layout, Store View Status, Created Date/Time Modified Date/Time Action.Each row provides detailed information about individual pages including their titles and statuses.This interface allows users to efficiently manage website content by enabling them to add new pages or modify existing ones through drop-down menus in the action column.Overall, it reflects a structured approach to page management within an e-commerce environment.Output: Let's think step-by-step.The functionality of this element is to filter projects based on specific topics or themes.By selecting 'Topics', users can narrow down their search results to projects that match the chosen category, enhancing the relevance and discover ability of the projects displayed in the project list.You're a helpful web GUI assistant.OBSERVATION: {Web State}Instruction: Predict how the UI evolves when this user interaction occurs: click [7716], where [7716] is 'type:link, text:Submit' Output: Let's think step-by-step.The user clicked on the "Submit" link, transitioning from a search results page to a submission creation page.This is evident from the removal of content related to search results, such as headings, articles, and links to individual posts, and the addition of elements specifically designed for creating submissions.The new page includes input fields for title and body, radio buttons for submission type (URL or image), a drop down menu for selecting a forum, and a submission button.The navigation bar remains consistent, but now supports submission-related functionalities.In summary, the webpage now displays a submission creation interface on Postmill.The top navigation bar includes links to "Postmill," "Forums," and "Wiki," along with a search bar and a "Submit" button.The main content area is structured for creating a submission, featuring input fields for Title (required), Body, and submission type options (URL or Image).Users can select a forum via a drop down menu labeled "Forum," which allows categorization of the submission.A button labeled "Create submission" is provided to finalize the process, and Markdown formatting is supported with help links available.This layout facilitates users in crafting and submitting content to the platform's forums."You are an autonomous intelligent agent tasked with navigating a web browser.You will be given a web GUI-based task.Specifically, you need to predict the next web page observation based on the current observation of the web browser and the given action.Here's the information you'll have: <em>IMPORTANT</em> To be successful, it is very important to follow the following rules:1. Please think step by step based on the current page observation and the actions taken, and give the maximum possible next page observation.2. You should ensure the richness of the observations of the web page to be predicted and support the continuous operation process.3. Please generate the content of the next page in the correct format.Start with the phrase "In summary, the next web page observation is" and then add supplements within "'<your generated contents>"'.For example, "In summary, the next web page observation is "'Tab 0 (current): Projects """.You are an expert in evaluating GUI agent task trajectories.Your task is to assess the quality and effectiveness of task trajectories for GUI manipulation tasks.A trajectory consists of the following components:1. User Instruction: Describes the user's intended task.2. Action History: Includes two key parts:-Reasoning and Action for Each Step: A sequence of actions performed by the agent, including the reasoning thought and final executed action.-The accessibility tree of the current web page: This is a simplified representation of the webpage, providing key information.When evaluating a trajectory, consider these key aspects:
Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li, Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems. 202536</p>
<p>Hyungjoo Chae, Namyoung Kim, Kai Tzu-Iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo, arXiv:2410.13232Web agents with world models: Learning and leveraging environment dynamics in web navigation. 2024arXiv preprint</p>
<p>Understanding world or predicting future? a comprehensive survey of world models. Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, Fengli Xu, Yong Li, 2024</p>
<p>Plan-and-act: Improving planning of agents for long-horizon tasks. Nicholas Lutfi Eren Erdogan, Sehoon Lee, Suhong Kim, Hiroki Moon, Gopala Furuta, Kurt Anumanchipalli, Amir Keutzer, Gholami, arXiv:2503.095722025arXiv preprint</p>
<p>Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, Dong Yu, arXiv:2504.21024Webevolver: Enhancing web agent self-improvement with coevolving world model. 2025arXiv preprint</p>
<p>Mohamed Amine Ferrag, Norbert Tihanyi, Merouane Debbah, arXiv:2504.19678From llm reasoning to autonomous ai agents: A comprehensive review. 2025arXiv preprint</p>
<p>Is your llm secretly a world model of the internet? model-based planning for web agents. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, arXiv:2411.065592024arXiv preprint</p>
<p>Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, Yu Su, Simulate before act: Model-based planning for web agents. </p>
<p>Webcot: Enhancing web agent reasoning by reconstructing chain-of-thought in reflection, branching, and rollback. Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, Irwin King, 2025</p>
<p>Deep reinforcement learning for autonomous driving: A survey. Ibrahim Ravi Kiran, Victor Sobh, Patrick Talpaert, Ahmad A Mannion, Senthil Al Sallab, Patrick Yogamani, Pérez, 2021</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024arXiv preprint</p>
<p>Tree search for language model agents. Jing Yu Koh, Stephen Mcaleer, Daniel Fried, Ruslan Salakhutdinov, 2024</p>
<p>Autowebglm: A large language modelbased web navigating agent. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, and Jie Tang. Visualagentbench: Towards large multimodal models as visual foundation agents. 2024</p>
<p>Infiguiagent: A multimodal generalist gui agent with native reasoning and reflection. Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, Fei Wu, 2025</p>
<p>Model-based reinforcement learning: A survey. Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, Foundations and Trends® in Machine Learning. 202316</p>
<p>Agent q: Advanced reasoning and learning for autonomous ai agents. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, Rafael Rafailov, arXiv:2408.071992024arXiv preprint</p>
<p>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, arXiv:2411.02337Training llm web agents via self-evolving online curriculum reinforcement learning. 2024arXiv preprint</p>
<p>Ui-tars: Pioneering automated gui interaction with native agents. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, arXiv:2501.123262025arXiv preprint</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>Zerosearch: Incentivize the search capability of llms without searching. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, Jingren Zhou, 2025</p>
<p>Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, arXiv:2412.197232024arXiv preprint</p>
<p>Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, arXiv:2505.164212025arXiv preprint</p>
<p>Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu, arXiv:2412.09605Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. 2024arXiv preprint</p>
<p>Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao, 2023</p>
<p>Agentoccam: A simple yet strong baseline for llm-based web agents. Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala, 2024</p>
<p>Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao, 2024</p>
<p>Large language model-brained gui agents: A survey. Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, arXiv:2411.182792024arXiv preprint</p>
<p>Rest-mcts*: Llm self-training via process reward guided tree search. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang, 2024</p>
<p>Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration. Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, Volker Tresp, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning. Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, Wenwu Zhu, arXiv:2504.126802025arXiv preprint</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, 2024</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854A realistic web environment for building autonomous agents. 2023arXiv preprint</p>
<p>Proposer-agent-evaluator (pae): Autonomous skill discovery for foundation model internet agents. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, Erran Li, arXiv:2412.131942024arXiv preprint</p>
<p>Monte carlo tree search: a review of recent modifications and applications. Maciej Świechowski, Konrad Godlewski, Bartosz Sawicki, Jacek Mańdziuk, Artificial Intelligence Review. 563July 2022</p>            </div>
        </div>

    </div>
</body>
</html>
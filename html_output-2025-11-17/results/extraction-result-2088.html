<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2088 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2088</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2088</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-279075076</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.01372v2.pdf" target="_blank">AI Scientists Fail Without Strong Implementation Capability</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2088.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2088.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist (end-to-end autonomous scientific researcher)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual end-to-end system that autonomously formulates scientific ideas and executes verification/falsification procedures across the full research workflow; emphasized as requiring both idea-generation and implementation/validation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM-based research agent (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain (computer science, chemistry, biology, materials etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>scientific hypotheses, experiment designs, code, research papers, models</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>potentially highly novel / out-of-distribution (claimed capability)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven idea generation and iterative agentic search / multi-agent composition (recombination of learned patterns, hypothesis-space search via agent modules)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Intended to use experimental execution, code execution, simulation, and peer review; in practice validation relies on external tools, experiment runs, and human review</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported strong idea-generation capabilities (several cited works show high novelty of ideas; e.g., systems produced work accepted at ICLR/ACL workshops), but no unified numeric generation success rate provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Paper argues validation/implementation performance is weak across benchmarks (examples below show execution/replication success rates typically low — e.g., single-digit to mid-double-digit % on replication/execution tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation performance degrades as novelty and implementation complexity increase; paper reports a consistent gap where generation outpaces validation capability, especially for out-of-distribution/complex tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper explicitly states a large asymmetry: generation (idea novelty) is comparatively strong while implementation/verification is substantially weaker (the 'implementation gap'), citing multiple benchmarks with high generation plausibility but low execution/replication success</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not standardized — the paper notes current systems lack robust uncertainty quantification for verification and do not reliably express calibrated uncertainty for novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported quantitatively; paper claims calibration and evaluation mechanisms are insufficient and tend to overstate confidence for generated outputs</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Paper states performance falls sharply for out-of-distribution or transformational discoveries; no single numeric metric given here but multiple benchmarks cited show large drops in execution/replication accuracy on realistic research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — many current evaluations rely on proxies (plausibility, coherence, peer-review scores, code-text consistency) rather than direct ground-truth experimental validation; the paper criticizes overreliance on such proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Paper recommends human-in-the-loop for essentially all high-impact outputs; frequency increases with output novelty/impact (explicit recommendation that all AI-generated outputs be labeled and reviewed)</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Varies by domain; paper notes domains with high formalization (e.g., mathematics) differ from empirical domains (e.g., drug discovery) and that empirical domains exacerbate the generation-validation gap</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Proposes modular multi-agent systems, stronger verification protocols, RL-trained planner agents, RAG, standardized interoperability (MCP/A2A), and hybrid human-AI review; effectiveness not empirically tested in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Aggregate evidence across benchmarks and a simulated peer-review of 28 AI-generated papers showing 100% experimental weakness, low execution/replication metrics on multiple benchmarks, and expert analysis concluding implementation/verification failures are pervasive</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some cited outcomes show AI outputs accepted at workshops/conferences and certain tools (e.g., AlphaFold, autonomous labs) achieving high impact — the paper recognizes these but argues they are specialized tools, not full AI Scientists</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Paper argues validation (implementation) is far more costly than generation in wall-clock RL-sampling terms (estimates: AI Scientist sampling time ~46,900s vs reasoning/web agents ~250–700s), implying validation >> generation in cost</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2088.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluated AI-generated papers (DeepReviewer set)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collection of 28 AI-generated research papers evaluated with DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of 28 publicly available research papers produced by five AI Scientist systems that were systematically evaluated by the DeepReviewer-14B review model to assess implementation-level reliability and paper quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-generated research papers (various AI Scientist systems: HKUSD AI Researcher, AI Scientist, AI Scientist v2, CycleResearcher-12B, Zochi)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based research agent outputs (papers)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / ML (papers evaluated were ML/NLP/related)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>research papers (claims, experiments, code descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>mostly claimed to be novel or at least novel-leaning by generators; DeepReviewer results indicate many novelty concerns</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven writing and idea composition (multi-agent pipelines in some systems)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation by a state-of-the-art automatic review model (DeepReviewer-14B) simulating peer review; underlying systems purportedly generated their own experimental code and results but those runs were not reliably validated by external ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Some systems produced manuscripts that passed superficial review thresholds (e.g., average presentation/soundness scores in Table 2 vary); example system ratings: Zochi mean rating 4.63 (percentile ~29.96%), AI Scientist mean rating ~3.35 (percentile 58.22%) — these reflect reviewer-model scores rather than true experimental validity</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>DeepReviewer-14B identified pervasive experimental weaknesses: Table 3 reports Experimental Weakness in 100% of the 28 papers; additional frequent defects include methodological flaws (96.4%) and reproducibility issues (71.4%)</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Review model flagged novelty concerns in 89.3% of papers; higher novelty often correlated with theoretical weaknesses and reproducibility problems according to the review calibration in this dataset</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Direct comparison shows generation (paper writing) can produce superficially plausible outputs, but validation (experimental rigor) consistently fails — all 28 papers had experimental weaknesses per DeepReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>DeepReviewer provides scalar review scores but the paper does not report calibrated uncertainty measures for generator claims</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; paper notes peer review (including LLM-as-judge) is limited as a predictor of long-term impact and may not fully capture correctness</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not specifically measured for these papers; review model often flagged theoretical/practical weaknesses for more novel claims</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the review process and many conference acceptances are proxies (presentation, soundness scores) rather than direct external replication/experimental verification</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Paper recommends human auditing for all AI-generated papers; frequency should increase with claimed novelty or potential risk</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical ML research (semi-formal) — the semi-formal nature makes direct formal verification impractical and increases need for experimental reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper suggests centralized archives, automated detection, mandatory provenance labeling, hybrid automated+human review (DeepReview), and stronger verification-focused benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>DeepReviewer-14B results (100% experimental weakness; high rates of methodological and reproducibility issues) directly support the generation-validation gap for AI-produced papers</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some AI-generated papers have achieved acceptance at workshops/conferences — indicates that superficial review may pass some AI outputs despite execution flaws</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not explicitly quantified for this evaluation; implicit claim that human review and experimental reproduction (validation) are far more resource/time intensive than generation</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2088.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperBench (benchmark for replicating ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that requires LLM agents to reproduce entire machine-learning papers from scratch (develop codebases, run experiments) and evaluates across sub-tasks including Code-Development, Execution, and Result Match.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating ai's ability to replicate ai research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework for LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / computer science</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>reproduced codebases, experiment outputs, replication reports</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (benchmark tasks are reproductions of existing papers; tests agents' ability to implement known methods)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Agents generate code and experiment scripts from paper descriptions using LLM generation and agent tool-calling</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Rubric-defined checks: 'Execution' (does code run) and 'Result Match' (do quantitative results match the paper) plus code-development success measures</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents can produce code components (e.g., an example: o1-High achieved 43.4% on weighted 'Code-Development' sub-tasks), indicating moderate code-generation performance</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Very poor on execution/replication: Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match' in PaperBench</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Benchmark targets in-distribution reproduction; nonetheless, execution/validation fails even for previously published work, showing a mismatch between generation plausibility and actual runnable/replicable code</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Clear gap: reasonable code generation success but near-zero execution/result matching in many agents; demonstrates generation >> successful validation</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported in benchmark outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; evidence suggests agents overgenerate plausible-looking code that fails at runtime</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not applicable (benchmark is reproducing in-distribution papers) — but failure on in-distribution tasks implies worse performance OOD</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses proxy rubric nodes (Execution and Result Match are direct but many other evaluations rely on code-text matching and heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human verification required for execution and result matching; implied frequent human involvement for failed cases</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal (computational experiments) — requires runtime execution for validation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Benchmark recommends improving agents' code-execution, debugging, and environment management abilities; paper suggests more robust tool-chain and stateful execution support</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Concrete low execution/matching scores (e.g., 1.8% execution, 0.7% result match for Claude 3.5 Sonnet) show severe verification failure despite code-generation attempts</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some agents show moderate code-development ability (43.4% code dev) but still fail validation — not a contradiction but illustrates the asymmetry</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Validation (running experiments and matching results) is significantly more expensive/time-consuming than generating code (paper highlights long wall-clock times for implementation)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2088.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciReplicate-Bench: Benchmarking LLMs in agent-driven algorithmic reproduction from research papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers, measuring both reasoning accuracy and code execution success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciReplicate-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / ML reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>reproduced algorithm code and execution outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Reproduction of published algorithms (in-distribution tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM agents parse papers and generate code implementations (algorithm translation to runnable code)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution accuracy measured by passing functional test cases for tasks taken from papers; reasoning graph accuracy also measured</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents demonstrate high reasoning/logic understanding (high reasoning graph accuracy), but generated code often fails at execution</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Best agent achieved only 39% execution accuracy (generated code passed functional tests in 39% of tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Designed for in-distribution reproduction; even here execution accuracy is low, implying worse performance for more novel or OOD tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Notable discrepancy: strong conceptual/reasoning indicators but low execution accuracy (underlines inability to translate plans into working code)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported in benchmark summary</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not directly reported; likely substantially worse than in-distribution reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses execution pass rates as direct validation but also measures intermediate proxies like reasoning graph accuracy which do not guarantee runnable code</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight/review is implied for failed reproductions and debugging steps</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal (algorithmic/computational) — requires runtime validation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper discussions and cited works suggest improving execution/debugging loops, environment setup, and multi-turn state retention</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>High reasoning accuracy but only 39% execution accuracy for best agent — clear evidence agents fail at implementation despite correct conceptual understanding</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None presented; gap is supported by benchmark numbers</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Execution/testing (validation) is more resource/time intensive than generation; exact ratio not provided</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2088.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CORE-Bench: Computational reproducibility agent benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark that requires agents to reproduce computational experiments and then answer questions based on outputs; assesses end-to-end reproduction and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CORE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / cross-domain computational reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>reproduced experiments, answers to experiment-derived questions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Reproduction of existing experiments (in-distribution), with medium and hard difficulty tiers</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM agents produce code and analysis to reproduce results and then reason over outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Success is measured by reproduction pass rates and correctness of downstream reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Agents can often produce code and attempt reproduction; example: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Imperfect success rates indicate substantial room for improvement (55.56% on Medium tier for a top agent), meaning many reproduction attempts fail or are incomplete</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Medium-tier performance ~55.56%; harder tiers likely see lower performance (paper notes difficulty scales with complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation of reproduction attempts can be moderate but full validation/reasoning over outputs remains error-prone — replication success is not guaranteed even when code is produced</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; designed for reproduction so OOD tasks likely worse</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses reproduction pass rates as direct validation plus correctness of question answering as a downstream proxy</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human verification recommended for failed or ambiguous reproductions; frequency implied to grow with difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal (computational reproducibility)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Suggests improving execution environments, multi-turn debugging and state tracking, and tighter tool integration</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>A top agent achieving only 55.56% on Medium indicates substantial gaps between generation ability and reliable reproduction/validation</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None presented in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Validation (reproduction and downstream QA) requires full experiment runs and is much more costly than generating code text; exact ratio not provided</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2088.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLE-Bench / ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLE-Bench and ML-Dev-Bench (benchmarks for ML engineering workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks that test LLM agents on ML development tasks including training, debugging, and model performance optimization; used to reveal agents' weaknesses in implementation and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MLE-Bench: Evaluating machine learning agents on machine learning engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLE-Bench / ML-Dev-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark suite for ML engineering agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning engineering</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>training scripts, model artifacts, tuned models, submissions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Operational ML development tasks (in-distribution engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM agents generate code for model training, debugging, and deployment; may call external tools/APIs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Measured by success rates on producing valid submissions, debugging correctness, and model performance metrics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>LLMs can generate code and pipeline components but frequently produce non-working artifacts (examples: o1-preview had 16.90% on MLE-Bench in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Poor on validation/model performance: paper reports agents frequently fail to debug or optimize models (20% of o1 preview runs failing a specific step on MLE-Bench; ML-Dev-Bench agents scored 0% on 'Model Performance' tasks in reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Even for standard engineering tasks (low novelty), agents struggle with validation loops; novel tasks would likely worsen performance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Clear asymmetry: agents may output plausible workflows/code but fail at debugging, producing valid submissions, or achieving intended model performance</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not directly reported; expectation is substantially worse</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Benchmarks measure direct model-performance metrics but many intermediate checks are proxies (e.g., producing code that 'looks' correct without running it)</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High — human debugging and review required frequently, especially for model performance steps</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal / empirical engineering</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Improved multi-turn debugging, persistent state/memory, better tool-chaining and environment management suggested</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Reported 0% on certain 'Model Performance' tasks and frequent failures in debugging demonstrate agents' verification deficits</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Generation of code components does occur (some tasks pass), but this does not contradict the overall existence of the gap</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Validation (training and evaluating models) is orders of magnitude more expensive/time-consuming than generating code text</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2088.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiveCodeBench / HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LiveCodeBench and HumanEval (code-generation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmarks evaluating code LLMs across generation, execution, self-repair, and output prediction; HumanEval is a simpler code generation pass@k benchmark while LiveCodeBench targets contest-derived, harder problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Livecodebench: Holistic and contamination free evaluation of large language models for code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LiveCodeBench / HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmarks for code generation and execution</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>software / computational tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>code solutions, executable programs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Varies — HumanEval often in-distribution algorithmic tasks; LiveCodeBench uses harder contest problems (more challenging/OOD-like)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLMs produce code via autoregressive generation, sometimes with self-repair loops</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution and pass@1 or pass@k metrics measured by running test cases; LiveCodeBench includes execution and contamination controls</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>o4-mini achieves 52.1% pass@1 on LiveCodeBench code-generation subtask (stated as SoTA for that subtask); LLMs achieve near-saturated performance on HumanEval in many cases</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Even when pass@1 is moderate, real-world complex R&D tasks still fail — paper emphasizes drop in realistic settings compared to contest-style benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance drops as problem complexity/realism increases (contest -> real-world research code), indicating lower validation success on more novel/complex tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Benchmarks show good generation on constrained problems but poor translation to large-scale research implementations that require environment management and multi-file coordination</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>LiveCodeBench (harder contest problems) shows much lower pass rates compared to simpler benchmarks, indicating degraded OOD performance</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>pass@k and test-suite matching are proxies for true research correctness; paper argues these proxies overestimate real-world research validity</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight recommended especially when moving from single-file tasks to multi-file research code</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Formal/structured for algorithmic code; research code is semi-formal and requires additional experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Stronger environment simulation, multi-file coordination, persistent state and better debugging loops</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>High performance on simple benchmarks vs. dramatic drops on end-to-end research benchmarks support the generation-validation gap</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High pass@1 on constrained tasks shows generation can be strong for narrow problems but does not negate the gap for research workflows</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Running full experiments/environments (validation) is substantially more expensive than generating code text</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2088.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold (protein structure prediction system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system capable of predicting 3D protein structures rapidly and with high accuracy, cited as an example of a specialized automated scientific tool that achieved a major domain breakthrough.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with alphafold.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep neural network (protein structure predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational biology / structural biology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>predicted 3D protein structures</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>often in-distribution extrapolation but enabled transformational practical outcomes; effectively supplies previously unknown structures</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised / representation learning from sequence-structure databases, inference to predict structure</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated against experimentally-determined structures (e.g., CASP evaluations), accuracy of predicted folds compared to ground-truth structures</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Described qualitatively as capable of determining 3D structures in just hours and achieving high accuracy (no numerical metrics provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validated via experimental structure matches in CASP and community benchmarks (paper cites Jumper et al., 2021 but does not enumerate numbers here)</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>AlphaFold performs well in its domain; being domain-specialized with strong validation pipelines it avoids the generation-validation gap the paper attributes to generalist LLM-based AI Scientists</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Specialized automated tools like AlphaFold combine strong generation and validation pipelines, unlike generalist AI Scientist systems evaluated here</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>AlphaFold includes internal confidence measures (e.g., pLDDT) in original work (not detailed numerically in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified in this paper, but original AlphaFold reported calibrated confidence scores</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performance declines for sequences with little homologous data; not quantified here</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Direct structural comparison to experimental ground truth (not proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human follow-up may be used for high-impact predictions but routine predictions are commonly trusted within confidence bounds</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical but well-instrumented (biology with established experimental validation pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Specialization and integration with domain-specific data and validation (exemplar of how focused tool design reduces the generation-validation gap)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Contrasted in paper as an example of a specialized tool that succeeds where generalist AI Scientists fail in validation; supports point that domain specialization + validation pipelines are crucial</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>AlphaFold's success is not contradictory but an exception showing that with strong domain integration, generation and validation can align</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Validation (experimental structural determination) is far more expensive than inference, but AlphaFold's predictive accuracy reduces need for wet-lab validation for many cases (not quantified here)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2088.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Lab (autonomous laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-Lab (autonomous laboratory for materials synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous laboratory system that synthesized 41 novel inorganic materials within 17 days, cited as an example of automated scientific tooling that achieves end-to-end experimental discovery in a restricted domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>A-Lab (autonomous laboratory)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>robotic/autonomous laboratory + optimization algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>novel inorganic material syntheses and experimental characterization results</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>highly novel in targeted materials space (41 novel materials reported)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Automated experimentation driven by optimization/search (autonomous lab hardware plus algorithms to propose and run experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Physical synthesis and empirical characterization (lab experiments) to confirm novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported concrete output: 41 novel inorganic materials synthesized within 17 days (quantitative productivity metric cited)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation done by experimental synthesis and characterization (implied successful confirmation of materials); no false-positive numeric rates provided in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>High domain specialization and direct experimental validation allow reliable novel discovery within constrained chemistry/materials search spaces</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>A-Lab exemplifies that in tightly instrumented domains, generation and validation can be tightly integrated yielding successful novel outputs, unlike generalist AI Scientist workflows</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not discussed; likely constrained to explored chemical space</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Direct experimental confirmation used rather than proxies</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight implied but routine autonomous runs performed; the paper notes such systems still rely on human involvement in many cases</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical experimental domain with well-established lab validation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Demonstrates benefit of coupling generation tightly to experimental hardware and domain-specific validation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Presented as counterexample to generalist AI Scientist ambition: specialized autonomous labs succeed where generalist LLMs fail at experimental verification</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>The A-Lab shows that automation with integrated lab validation can overcome generation-validation issues in constrained domains</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Experimental validation is resource/time intensive but integrated automation reduces wall-clock time per experiment compared to manual processes (exact ratio not provided)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2088.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2088.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer-14B (LLM-based review model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art 14B parameter review model used in this study to perform a simulated peer-review evaluation of 28 AI-generated papers under unified standards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepreview: Improving llm-based paper review with human-like deep thinking process.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model configured as an automated peer-reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>meta-science / peer review automation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>review scores, defect categorizations, accept/reject decisions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (evaluation tool)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM inference using review-oriented prompting and rubric-guided assessment</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Generates review judgments; used as proxy for human peer review to flag defects and score papers</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Produced quantitative average scores across 28 papers (see Table 2: example system averages e.g., Zochi rating 4.63 mean, percentile ~29.96%)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Identified defects systematically (Table 3 shows Experimental Weakness flagged in 100% of papers); the paper cautions about limits of review-as-validation for predicting long-term impact</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Review model flags novelty concerns often (89.3% of papers had novelty concerns) but the paper notes that peer review (automated or human) is a noisy predictor of long-term impact</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used as a validation proxy for AI outputs; while it can filter low-quality outputs, it may miss or mis-predict eventual impact and cannot substitute for experimental reproduction</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported; model outputs are scalar scores without reported calibration</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; paper warns about limitations of review scores as indicators of future impact</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses rubric-based scoring and defect categories as proxies for true scientific validity</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Paper recommends hybrid automated + human-in-the-loop review for continuous oversight</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Applied to ML research papers (semi-formal), where empirical reproducibility is critical</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper advocates for combining automated review (DeepReviewer-like) with human scrutiny and provenance metadata to mitigate low-quality AI outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>DeepReviewer-14B flagged pervasive experimental weaknesses across AI-generated papers, reinforcing the generation-validation gap</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Automated review can filter poor outputs but cannot fully validate experimental correctness; no direct contradiction offered</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Automated review is low-cost relative to full experimental reproduction (paper emphasizes reproduction is much costlier)</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating ai's ability to replicate ai research. <em>(Rating: 2)</em></li>
                <li>Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. <em>(Rating: 2)</em></li>
                <li>MLE-Bench: Evaluating machine learning agents on machine learning engineering. <em>(Rating: 2)</em></li>
                <li>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. <em>(Rating: 2)</em></li>
                <li>Livecodebench: Holistic and contamination free evaluation of large language models for code. <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with alphafold. <em>(Rating: 1)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials. <em>(Rating: 1)</em></li>
                <li>Deepreview: Improving llm-based paper review with human-like deep thinking process. <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2088",
    "paper_id": "paper-279075076",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "AI Scientist (concept)",
            "name_full": "AI Scientist (end-to-end autonomous scientific researcher)",
            "brief_description": "A conceptual end-to-end system that autonomously formulates scientific ideas and executes verification/falsification procedures across the full research workflow; emphasized as requiring both idea-generation and implementation/validation capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI Scientist (concept)",
            "system_type": "multi-agent LLM-based research agent (conceptual)",
            "scientific_domain": "general / cross-domain (computer science, chemistry, biology, materials etc.)",
            "output_type": "scientific hypotheses, experiment designs, code, research papers, models",
            "novelty_level": "potentially highly novel / out-of-distribution (claimed capability)",
            "generation_method": "LLM-driven idea generation and iterative agentic search / multi-agent composition (recombination of learned patterns, hypothesis-space search via agent modules)",
            "validation_method": "Intended to use experimental execution, code execution, simulation, and peer review; in practice validation relies on external tools, experiment runs, and human review",
            "generation_performance": "Reported strong idea-generation capabilities (several cited works show high novelty of ideas; e.g., systems produced work accepted at ICLR/ACL workshops), but no unified numeric generation success rate provided in this paper",
            "validation_performance": "Paper argues validation/implementation performance is weak across benchmarks (examples below show execution/replication success rates typically low — e.g., single-digit to mid-double-digit % on replication/execution tasks)",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Validation performance degrades as novelty and implementation complexity increase; paper reports a consistent gap where generation outpaces validation capability, especially for out-of-distribution/complex tasks",
            "generation_validation_comparison": "Paper explicitly states a large asymmetry: generation (idea novelty) is comparatively strong while implementation/verification is substantially weaker (the 'implementation gap'), citing multiple benchmarks with high generation plausibility but low execution/replication success",
            "uncertainty_quantification": "Not standardized — the paper notes current systems lack robust uncertainty quantification for verification and do not reliably express calibrated uncertainty for novel outputs",
            "calibration_quality": "Not reported quantitatively; paper claims calibration and evaluation mechanisms are insufficient and tend to overstate confidence for generated outputs",
            "out_of_distribution_performance": "Paper states performance falls sharply for out-of-distribution or transformational discoveries; no single numeric metric given here but multiple benchmarks cited show large drops in execution/replication accuracy on realistic research tasks",
            "validation_proxy_metrics": "Yes — many current evaluations rely on proxies (plausibility, coherence, peer-review scores, code-text consistency) rather than direct ground-truth experimental validation; the paper criticizes overreliance on such proxies",
            "human_validation_required": true,
            "human_validation_frequency": "Paper recommends human-in-the-loop for essentially all high-impact outputs; frequency increases with output novelty/impact (explicit recommendation that all AI-generated outputs be labeled and reviewed)",
            "formal_verification_used": false,
            "domain_formalization_level": "Varies by domain; paper notes domains with high formalization (e.g., mathematics) differ from empirical domains (e.g., drug discovery) and that empirical domains exacerbate the generation-validation gap",
            "gap_mitigation_strategies": "Proposes modular multi-agent systems, stronger verification protocols, RL-trained planner agents, RAG, standardized interoperability (MCP/A2A), and hybrid human-AI review; effectiveness not empirically tested in this paper",
            "evidence_supporting_gap": "Aggregate evidence across benchmarks and a simulated peer-review of 28 AI-generated papers showing 100% experimental weakness, low execution/replication metrics on multiple benchmarks, and expert analysis concluding implementation/verification failures are pervasive",
            "evidence_contradicting_gap": "Some cited outcomes show AI outputs accepted at workshops/conferences and certain tools (e.g., AlphaFold, autonomous labs) achieving high impact — the paper recognizes these but argues they are specialized tools, not full AI Scientists",
            "computational_cost_ratio": "Paper argues validation (implementation) is far more costly than generation in wall-clock RL-sampling terms (estimates: AI Scientist sampling time ~46,900s vs reasoning/web agents ~250–700s), implying validation &gt;&gt; generation in cost",
            "uuid": "e2088.0"
        },
        {
            "name_short": "Evaluated AI-generated papers (DeepReviewer set)",
            "name_full": "Collection of 28 AI-generated research papers evaluated with DeepReviewer-14B",
            "brief_description": "A set of 28 publicly available research papers produced by five AI Scientist systems that were systematically evaluated by the DeepReviewer-14B review model to assess implementation-level reliability and paper quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-generated research papers (various AI Scientist systems: HKUSD AI Researcher, AI Scientist, AI Scientist v2, CycleResearcher-12B, Zochi)",
            "system_type": "LLM-based research agent outputs (papers)",
            "scientific_domain": "computer science / ML (papers evaluated were ML/NLP/related)",
            "output_type": "research papers (claims, experiments, code descriptions)",
            "novelty_level": "mostly claimed to be novel or at least novel-leaning by generators; DeepReviewer results indicate many novelty concerns",
            "generation_method": "LLM-driven writing and idea composition (multi-agent pipelines in some systems)",
            "validation_method": "Evaluation by a state-of-the-art automatic review model (DeepReviewer-14B) simulating peer review; underlying systems purportedly generated their own experimental code and results but those runs were not reliably validated by external ground truth",
            "generation_performance": "Some systems produced manuscripts that passed superficial review thresholds (e.g., average presentation/soundness scores in Table 2 vary); example system ratings: Zochi mean rating 4.63 (percentile ~29.96%), AI Scientist mean rating ~3.35 (percentile 58.22%) — these reflect reviewer-model scores rather than true experimental validity",
            "validation_performance": "DeepReviewer-14B identified pervasive experimental weaknesses: Table 3 reports Experimental Weakness in 100% of the 28 papers; additional frequent defects include methodological flaws (96.4%) and reproducibility issues (71.4%)",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Review model flagged novelty concerns in 89.3% of papers; higher novelty often correlated with theoretical weaknesses and reproducibility problems according to the review calibration in this dataset",
            "generation_validation_comparison": "Direct comparison shows generation (paper writing) can produce superficially plausible outputs, but validation (experimental rigor) consistently fails — all 28 papers had experimental weaknesses per DeepReviewer",
            "uncertainty_quantification": "DeepReviewer provides scalar review scores but the paper does not report calibrated uncertainty measures for generator claims",
            "calibration_quality": "Not quantified; paper notes peer review (including LLM-as-judge) is limited as a predictor of long-term impact and may not fully capture correctness",
            "out_of_distribution_performance": "Not specifically measured for these papers; review model often flagged theoretical/practical weaknesses for more novel claims",
            "validation_proxy_metrics": "Yes — the review process and many conference acceptances are proxies (presentation, soundness scores) rather than direct external replication/experimental verification",
            "human_validation_required": true,
            "human_validation_frequency": "Paper recommends human auditing for all AI-generated papers; frequency should increase with claimed novelty or potential risk",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical ML research (semi-formal) — the semi-formal nature makes direct formal verification impractical and increases need for experimental reproduction",
            "gap_mitigation_strategies": "Paper suggests centralized archives, automated detection, mandatory provenance labeling, hybrid automated+human review (DeepReview), and stronger verification-focused benchmarks",
            "evidence_supporting_gap": "DeepReviewer-14B results (100% experimental weakness; high rates of methodological and reproducibility issues) directly support the generation-validation gap for AI-produced papers",
            "evidence_contradicting_gap": "Some AI-generated papers have achieved acceptance at workshops/conferences — indicates that superficial review may pass some AI outputs despite execution flaws",
            "computational_cost_ratio": "Not explicitly quantified for this evaluation; implicit claim that human review and experimental reproduction (validation) are far more resource/time intensive than generation",
            "uuid": "e2088.1"
        },
        {
            "name_short": "PaperBench",
            "name_full": "PaperBench (benchmark for replicating ML papers)",
            "brief_description": "A benchmark that requires LLM agents to reproduce entire machine-learning papers from scratch (develop codebases, run experiments) and evaluates across sub-tasks including Code-Development, Execution, and Result Match.",
            "citation_title": "Evaluating ai's ability to replicate ai research.",
            "mention_or_use": "mention",
            "system_name": "PaperBench",
            "system_type": "benchmark / evaluation framework for LLM agents",
            "scientific_domain": "machine learning / computer science",
            "output_type": "reproduced codebases, experiment outputs, replication reports",
            "novelty_level": "N/A (benchmark tasks are reproductions of existing papers; tests agents' ability to implement known methods)",
            "generation_method": "Agents generate code and experiment scripts from paper descriptions using LLM generation and agent tool-calling",
            "validation_method": "Rubric-defined checks: 'Execution' (does code run) and 'Result Match' (do quantitative results match the paper) plus code-development success measures",
            "generation_performance": "Agents can produce code components (e.g., an example: o1-High achieved 43.4% on weighted 'Code-Development' sub-tasks), indicating moderate code-generation performance",
            "validation_performance": "Very poor on execution/replication: Claude 3.5 Sonnet scored 1.8% on 'Execution' and 0.7% on 'Result Match' in PaperBench",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Benchmark targets in-distribution reproduction; nonetheless, execution/validation fails even for previously published work, showing a mismatch between generation plausibility and actual runnable/replicable code",
            "generation_validation_comparison": "Clear gap: reasonable code generation success but near-zero execution/result matching in many agents; demonstrates generation &gt;&gt; successful validation",
            "uncertainty_quantification": "Not reported in benchmark outcomes",
            "calibration_quality": "Not reported; evidence suggests agents overgenerate plausible-looking code that fails at runtime",
            "out_of_distribution_performance": "Not applicable (benchmark is reproducing in-distribution papers) — but failure on in-distribution tasks implies worse performance OOD",
            "validation_proxy_metrics": "Uses proxy rubric nodes (Execution and Result Match are direct but many other evaluations rely on code-text matching and heuristics)",
            "human_validation_required": true,
            "human_validation_frequency": "Human verification required for execution and result matching; implied frequent human involvement for failed cases",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal (computational experiments) — requires runtime execution for validation",
            "gap_mitigation_strategies": "Benchmark recommends improving agents' code-execution, debugging, and environment management abilities; paper suggests more robust tool-chain and stateful execution support",
            "evidence_supporting_gap": "Concrete low execution/matching scores (e.g., 1.8% execution, 0.7% result match for Claude 3.5 Sonnet) show severe verification failure despite code-generation attempts",
            "evidence_contradicting_gap": "Some agents show moderate code-development ability (43.4% code dev) but still fail validation — not a contradiction but illustrates the asymmetry",
            "computational_cost_ratio": "Validation (running experiments and matching results) is significantly more expensive/time-consuming than generating code (paper highlights long wall-clock times for implementation)",
            "uuid": "e2088.2"
        },
        {
            "name_short": "SciReplicate-Bench",
            "name_full": "SciReplicate-Bench: Benchmarking LLMs in agent-driven algorithmic reproduction from research papers",
            "brief_description": "A benchmark that tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers, measuring both reasoning accuracy and code execution success.",
            "citation_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers.",
            "mention_or_use": "mention",
            "system_name": "SciReplicate-Bench",
            "system_type": "benchmark / evaluation framework",
            "scientific_domain": "NLP / ML reproducibility",
            "output_type": "reproduced algorithm code and execution outputs",
            "novelty_level": "Reproduction of published algorithms (in-distribution tasks)",
            "generation_method": "LLM agents parse papers and generate code implementations (algorithm translation to runnable code)",
            "validation_method": "Execution accuracy measured by passing functional test cases for tasks taken from papers; reasoning graph accuracy also measured",
            "generation_performance": "Agents demonstrate high reasoning/logic understanding (high reasoning graph accuracy), but generated code often fails at execution",
            "validation_performance": "Best agent achieved only 39% execution accuracy (generated code passed functional tests in 39% of tasks)",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Designed for in-distribution reproduction; even here execution accuracy is low, implying worse performance for more novel or OOD tasks",
            "generation_validation_comparison": "Notable discrepancy: strong conceptual/reasoning indicators but low execution accuracy (underlines inability to translate plans into working code)",
            "uncertainty_quantification": "Not reported in benchmark summary",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not directly reported; likely substantially worse than in-distribution reproduction",
            "validation_proxy_metrics": "Uses execution pass rates as direct validation but also measures intermediate proxies like reasoning graph accuracy which do not guarantee runnable code",
            "human_validation_required": true,
            "human_validation_frequency": "Human oversight/review is implied for failed reproductions and debugging steps",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal (algorithmic/computational) — requires runtime validation",
            "gap_mitigation_strategies": "Paper discussions and cited works suggest improving execution/debugging loops, environment setup, and multi-turn state retention",
            "evidence_supporting_gap": "High reasoning accuracy but only 39% execution accuracy for best agent — clear evidence agents fail at implementation despite correct conceptual understanding",
            "evidence_contradicting_gap": "None presented; gap is supported by benchmark numbers",
            "computational_cost_ratio": "Execution/testing (validation) is more resource/time intensive than generation; exact ratio not provided",
            "uuid": "e2088.3"
        },
        {
            "name_short": "CORE-Bench",
            "name_full": "CORE-Bench: Computational reproducibility agent benchmark",
            "brief_description": "Benchmark that requires agents to reproduce computational experiments and then answer questions based on outputs; assesses end-to-end reproduction and reasoning.",
            "citation_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark.",
            "mention_or_use": "mention",
            "system_name": "CORE-Bench",
            "system_type": "benchmark / evaluation framework",
            "scientific_domain": "computer science / cross-domain computational reproducibility",
            "output_type": "reproduced experiments, answers to experiment-derived questions",
            "novelty_level": "Reproduction of existing experiments (in-distribution), with medium and hard difficulty tiers",
            "generation_method": "LLM agents produce code and analysis to reproduce results and then reason over outputs",
            "validation_method": "Success is measured by reproduction pass rates and correctness of downstream reasoning tasks",
            "generation_performance": "Agents can often produce code and attempt reproduction; example: CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium",
            "validation_performance": "Imperfect success rates indicate substantial room for improvement (55.56% on Medium tier for a top agent), meaning many reproduction attempts fail or are incomplete",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Medium-tier performance ~55.56%; harder tiers likely see lower performance (paper notes difficulty scales with complexity)",
            "generation_validation_comparison": "Generation of reproduction attempts can be moderate but full validation/reasoning over outputs remains error-prone — replication success is not guaranteed even when code is produced",
            "uncertainty_quantification": "Not reported",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not reported; designed for reproduction so OOD tasks likely worse",
            "validation_proxy_metrics": "Uses reproduction pass rates as direct validation plus correctness of question answering as a downstream proxy",
            "human_validation_required": true,
            "human_validation_frequency": "Human verification recommended for failed or ambiguous reproductions; frequency implied to grow with difficulty",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal (computational reproducibility)",
            "gap_mitigation_strategies": "Suggests improving execution environments, multi-turn debugging and state tracking, and tighter tool integration",
            "evidence_supporting_gap": "A top agent achieving only 55.56% on Medium indicates substantial gaps between generation ability and reliable reproduction/validation",
            "evidence_contradicting_gap": "None presented in this paper",
            "computational_cost_ratio": "Validation (reproduction and downstream QA) requires full experiment runs and is much more costly than generating code text; exact ratio not provided",
            "uuid": "e2088.4"
        },
        {
            "name_short": "MLE-Bench / ML-Dev-Bench",
            "name_full": "MLE-Bench and ML-Dev-Bench (benchmarks for ML engineering workflows)",
            "brief_description": "Benchmarks that test LLM agents on ML development tasks including training, debugging, and model performance optimization; used to reveal agents' weaknesses in implementation and verification.",
            "citation_title": "MLE-Bench: Evaluating machine learning agents on machine learning engineering.",
            "mention_or_use": "mention",
            "system_name": "MLE-Bench / ML-Dev-Bench",
            "system_type": "benchmark suite for ML engineering agent tasks",
            "scientific_domain": "machine learning engineering",
            "output_type": "training scripts, model artifacts, tuned models, submissions",
            "novelty_level": "Operational ML development tasks (in-distribution engineering)",
            "generation_method": "LLM agents generate code for model training, debugging, and deployment; may call external tools/APIs",
            "validation_method": "Measured by success rates on producing valid submissions, debugging correctness, and model performance metrics",
            "generation_performance": "LLMs can generate code and pipeline components but frequently produce non-working artifacts (examples: o1-preview had 16.90% on MLE-Bench in Table 1)",
            "validation_performance": "Poor on validation/model performance: paper reports agents frequently fail to debug or optimize models (20% of o1 preview runs failing a specific step on MLE-Bench; ML-Dev-Bench agents scored 0% on 'Model Performance' tasks in reported experiments)",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Even for standard engineering tasks (low novelty), agents struggle with validation loops; novel tasks would likely worsen performance",
            "generation_validation_comparison": "Clear asymmetry: agents may output plausible workflows/code but fail at debugging, producing valid submissions, or achieving intended model performance",
            "uncertainty_quantification": "Not reported",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not directly reported; expectation is substantially worse",
            "validation_proxy_metrics": "Benchmarks measure direct model-performance metrics but many intermediate checks are proxies (e.g., producing code that 'looks' correct without running it)",
            "human_validation_required": true,
            "human_validation_frequency": "High — human debugging and review required frequently, especially for model performance steps",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal / empirical engineering",
            "gap_mitigation_strategies": "Improved multi-turn debugging, persistent state/memory, better tool-chaining and environment management suggested",
            "evidence_supporting_gap": "Reported 0% on certain 'Model Performance' tasks and frequent failures in debugging demonstrate agents' verification deficits",
            "evidence_contradicting_gap": "Generation of code components does occur (some tasks pass), but this does not contradict the overall existence of the gap",
            "computational_cost_ratio": "Validation (training and evaluating models) is orders of magnitude more expensive/time-consuming than generating code text",
            "uuid": "e2088.5"
        },
        {
            "name_short": "LiveCodeBench / HumanEval",
            "name_full": "LiveCodeBench and HumanEval (code-generation benchmarks)",
            "brief_description": "Benchmarks evaluating code LLMs across generation, execution, self-repair, and output prediction; HumanEval is a simpler code generation pass@k benchmark while LiveCodeBench targets contest-derived, harder problems.",
            "citation_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code.",
            "mention_or_use": "mention",
            "system_name": "LiveCodeBench / HumanEval",
            "system_type": "benchmarks for code generation and execution",
            "scientific_domain": "software / computational tasks",
            "output_type": "code solutions, executable programs",
            "novelty_level": "Varies — HumanEval often in-distribution algorithmic tasks; LiveCodeBench uses harder contest problems (more challenging/OOD-like)",
            "generation_method": "LLMs produce code via autoregressive generation, sometimes with self-repair loops",
            "validation_method": "Execution and pass@1 or pass@k metrics measured by running test cases; LiveCodeBench includes execution and contamination controls",
            "generation_performance": "o4-mini achieves 52.1% pass@1 on LiveCodeBench code-generation subtask (stated as SoTA for that subtask); LLMs achieve near-saturated performance on HumanEval in many cases",
            "validation_performance": "Even when pass@1 is moderate, real-world complex R&D tasks still fail — paper emphasizes drop in realistic settings compared to contest-style benchmarks",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Performance drops as problem complexity/realism increases (contest -&gt; real-world research code), indicating lower validation success on more novel/complex tasks",
            "generation_validation_comparison": "Benchmarks show good generation on constrained problems but poor translation to large-scale research implementations that require environment management and multi-file coordination",
            "uncertainty_quantification": "Not reported",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "LiveCodeBench (harder contest problems) shows much lower pass rates compared to simpler benchmarks, indicating degraded OOD performance",
            "validation_proxy_metrics": "pass@k and test-suite matching are proxies for true research correctness; paper argues these proxies overestimate real-world research validity",
            "human_validation_required": true,
            "human_validation_frequency": "Human oversight recommended especially when moving from single-file tasks to multi-file research code",
            "formal_verification_used": false,
            "domain_formalization_level": "Formal/structured for algorithmic code; research code is semi-formal and requires additional experimental validation",
            "gap_mitigation_strategies": "Stronger environment simulation, multi-file coordination, persistent state and better debugging loops",
            "evidence_supporting_gap": "High performance on simple benchmarks vs. dramatic drops on end-to-end research benchmarks support the generation-validation gap",
            "evidence_contradicting_gap": "High pass@1 on constrained tasks shows generation can be strong for narrow problems but does not negate the gap for research workflows",
            "computational_cost_ratio": "Running full experiments/environments (validation) is substantially more expensive than generating code text",
            "uuid": "e2088.6"
        },
        {
            "name_short": "AlphaFold",
            "name_full": "AlphaFold (protein structure prediction system)",
            "brief_description": "A deep learning system capable of predicting 3D protein structures rapidly and with high accuracy, cited as an example of a specialized automated scientific tool that achieved a major domain breakthrough.",
            "citation_title": "Highly accurate protein structure prediction with alphafold.",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_type": "deep neural network (protein structure predictor)",
            "scientific_domain": "computational biology / structural biology",
            "output_type": "predicted 3D protein structures",
            "novelty_level": "often in-distribution extrapolation but enabled transformational practical outcomes; effectively supplies previously unknown structures",
            "generation_method": "Supervised / representation learning from sequence-structure databases, inference to predict structure",
            "validation_method": "Validated against experimentally-determined structures (e.g., CASP evaluations), accuracy of predicted folds compared to ground-truth structures",
            "generation_performance": "Described qualitatively as capable of determining 3D structures in just hours and achieving high accuracy (no numerical metrics provided in this paper)",
            "validation_performance": "Validated via experimental structure matches in CASP and community benchmarks (paper cites Jumper et al., 2021 but does not enumerate numbers here)",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "AlphaFold performs well in its domain; being domain-specialized with strong validation pipelines it avoids the generation-validation gap the paper attributes to generalist LLM-based AI Scientists",
            "generation_validation_comparison": "Specialized automated tools like AlphaFold combine strong generation and validation pipelines, unlike generalist AI Scientist systems evaluated here",
            "uncertainty_quantification": "AlphaFold includes internal confidence measures (e.g., pLDDT) in original work (not detailed numerically in this paper)",
            "calibration_quality": "Not quantified in this paper, but original AlphaFold reported calibrated confidence scores",
            "out_of_distribution_performance": "Performance declines for sequences with little homologous data; not quantified here",
            "validation_proxy_metrics": "Direct structural comparison to experimental ground truth (not proxies)",
            "human_validation_required": false,
            "human_validation_frequency": "Human follow-up may be used for high-impact predictions but routine predictions are commonly trusted within confidence bounds",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical but well-instrumented (biology with established experimental validation pipelines)",
            "gap_mitigation_strategies": "Specialization and integration with domain-specific data and validation (exemplar of how focused tool design reduces the generation-validation gap)",
            "evidence_supporting_gap": "Contrasted in paper as an example of a specialized tool that succeeds where generalist AI Scientists fail in validation; supports point that domain specialization + validation pipelines are crucial",
            "evidence_contradicting_gap": "AlphaFold's success is not contradictory but an exception showing that with strong domain integration, generation and validation can align",
            "computational_cost_ratio": "Validation (experimental structural determination) is far more expensive than inference, but AlphaFold's predictive accuracy reduces need for wet-lab validation for many cases (not quantified here)",
            "uuid": "e2088.7"
        },
        {
            "name_short": "A-Lab (autonomous laboratory)",
            "name_full": "A-Lab (autonomous laboratory for materials synthesis)",
            "brief_description": "An autonomous laboratory system that synthesized 41 novel inorganic materials within 17 days, cited as an example of automated scientific tooling that achieves end-to-end experimental discovery in a restricted domain.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials.",
            "mention_or_use": "mention",
            "system_name": "A-Lab (autonomous laboratory)",
            "system_type": "robotic/autonomous laboratory + optimization algorithms",
            "scientific_domain": "materials science / chemistry",
            "output_type": "novel inorganic material syntheses and experimental characterization results",
            "novelty_level": "highly novel in targeted materials space (41 novel materials reported)",
            "generation_method": "Automated experimentation driven by optimization/search (autonomous lab hardware plus algorithms to propose and run experiments)",
            "validation_method": "Physical synthesis and empirical characterization (lab experiments) to confirm novel materials",
            "generation_performance": "Reported concrete output: 41 novel inorganic materials synthesized within 17 days (quantitative productivity metric cited)",
            "validation_performance": "Validation done by experimental synthesis and characterization (implied successful confirmation of materials); no false-positive numeric rates provided in this paper",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "High domain specialization and direct experimental validation allow reliable novel discovery within constrained chemistry/materials search spaces",
            "generation_validation_comparison": "A-Lab exemplifies that in tightly instrumented domains, generation and validation can be tightly integrated yielding successful novel outputs, unlike generalist AI Scientist workflows",
            "uncertainty_quantification": "Not reported here",
            "calibration_quality": "Not reported",
            "out_of_distribution_performance": "Not discussed; likely constrained to explored chemical space",
            "validation_proxy_metrics": "Direct experimental confirmation used rather than proxies",
            "human_validation_required": false,
            "human_validation_frequency": "Human oversight implied but routine autonomous runs performed; the paper notes such systems still rely on human involvement in many cases",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical experimental domain with well-established lab validation pipelines",
            "gap_mitigation_strategies": "Demonstrates benefit of coupling generation tightly to experimental hardware and domain-specific validation pipelines",
            "evidence_supporting_gap": "Presented as counterexample to generalist AI Scientist ambition: specialized autonomous labs succeed where generalist LLMs fail at experimental verification",
            "evidence_contradicting_gap": "The A-Lab shows that automation with integrated lab validation can overcome generation-validation issues in constrained domains",
            "computational_cost_ratio": "Experimental validation is resource/time intensive but integrated automation reduces wall-clock time per experiment compared to manual processes (exact ratio not provided)",
            "uuid": "e2088.8"
        },
        {
            "name_short": "DeepReviewer-14B",
            "name_full": "DeepReviewer-14B (LLM-based review model)",
            "brief_description": "A state-of-the-art 14B parameter review model used in this study to perform a simulated peer-review evaluation of 28 AI-generated papers under unified standards.",
            "citation_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process.",
            "mention_or_use": "use",
            "system_name": "DeepReviewer-14B",
            "system_type": "large language model configured as an automated peer-reviewer",
            "scientific_domain": "meta-science / peer review automation",
            "output_type": "review scores, defect categorizations, accept/reject decisions",
            "novelty_level": "N/A (evaluation tool)",
            "generation_method": "LLM inference using review-oriented prompting and rubric-guided assessment",
            "validation_method": "Generates review judgments; used as proxy for human peer review to flag defects and score papers",
            "generation_performance": "Produced quantitative average scores across 28 papers (see Table 2: example system averages e.g., Zochi rating 4.63 mean, percentile ~29.96%)",
            "validation_performance": "Identified defects systematically (Table 3 shows Experimental Weakness flagged in 100% of papers); the paper cautions about limits of review-as-validation for predicting long-term impact",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Review model flags novelty concerns often (89.3% of papers had novelty concerns) but the paper notes that peer review (automated or human) is a noisy predictor of long-term impact",
            "generation_validation_comparison": "Used as a validation proxy for AI outputs; while it can filter low-quality outputs, it may miss or mis-predict eventual impact and cannot substitute for experimental reproduction",
            "uncertainty_quantification": "Not reported; model outputs are scalar scores without reported calibration",
            "calibration_quality": "Not quantified; paper warns about limitations of review scores as indicators of future impact",
            "out_of_distribution_performance": "Not reported",
            "validation_proxy_metrics": "Yes — uses rubric-based scoring and defect categories as proxies for true scientific validity",
            "human_validation_required": true,
            "human_validation_frequency": "Paper recommends hybrid automated + human-in-the-loop review for continuous oversight",
            "formal_verification_used": false,
            "domain_formalization_level": "Applied to ML research papers (semi-formal), where empirical reproducibility is critical",
            "gap_mitigation_strategies": "Paper advocates for combining automated review (DeepReviewer-like) with human scrutiny and provenance metadata to mitigate low-quality AI outputs",
            "evidence_supporting_gap": "DeepReviewer-14B flagged pervasive experimental weaknesses across AI-generated papers, reinforcing the generation-validation gap",
            "evidence_contradicting_gap": "Automated review can filter poor outputs but cannot fully validate experimental correctness; no direct contradiction offered",
            "computational_cost_ratio": "Automated review is low-cost relative to full experimental reproduction (paper emphasizes reproduction is much costlier)",
            "uuid": "e2088.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating ai's ability to replicate ai research.",
            "rating": 2
        },
        {
            "paper_title": "Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers.",
            "rating": 2
        },
        {
            "paper_title": "MLE-Bench: Evaluating machine learning agents on machine learning engineering.",
            "rating": 2
        },
        {
            "paper_title": "Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark.",
            "rating": 2
        },
        {
            "paper_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code.",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with alphafold.",
            "rating": 1
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials.",
            "rating": 1
        },
        {
            "paper_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process.",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.",
            "rating": 2
        }
    ],
    "cost": 0.02427225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025</p>
<p>Minjun Zhu 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Qiujie Xie 
Engineering School
Westlake University</p>
<p>Zhejiang University</p>
<p>Yixuan Weng 
Engineering School
Westlake University</p>
<p>Jian Wu 
Engineering School
Westlake University</p>
<p>Zhen Lin 
Engineering School
Westlake University</p>
<p>Linyi Yang yanglinyiucd@gmail.com 
The emergence of Artificial Intelligence (AI) Scientist
University College London</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Engineering School
Westlake University</p>
<p>AI Scientists Fail Without Strong Implementation Capability
May 24, 2025F860A8464F3C105E680E4FA57B83AF94arXiv:2506.01372v2[cs.AI]AI ScientistImplementation GapHypothesis and Verification
represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation.Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent.Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools.Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.To better illustrate the root cause of this implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist.This position paper aims to call for the participants in the community to bridge the implementation gap.</p>
<p>Introduction</p>
<p>The automation of scientific discovery has long been one of humanity's deepest desires (Langley, 1987, King et al., 2009, Radensky et al., 2024, AI, 2025).In recent years, with the advances in deep neural network technology, a range of automated scientific tools has emerged, leading to groundbreaking achievements in fields such as biomedicine (Yang et al., 2025c, Jumper et al., 2021), chemistry (Stokes et al., 2020), and materials science (Szymanski et al., 2023).For instance, DeepMind's AlphaFold can determine the 3D structures of proteins in just a few hours, a task that previously took years to solve (Jumper et al., 2021).In recent, researchers developed an autonomous laboratory, A-Lab, which successfully synthesizes 41 novel inorganic materials within 17 days (Szymanski et al., 2023).However, these scientific tools still rely heavily on human involvement.Researchers must first formulate ideas to be tested, while AI is responsible for the labor-intensive tasks of verification and iterative search.Therefore, these systems cannot be considered as truly automated scientific research.</p>
<p>The emergence of LLM-based AI Scientist has propelled the automation of scientific research to the next level, with AI taking the lead as the primary executor of scientific discovery, managing the entire workflow from idea generation to experiment execution (Lu et al., 2024, Weng et al., 2025).Recent studies have shown that research papers produced by AI Scientist have already reached the level of submissions to major machine learning conferences (Si et al., 2024, Yamada et al., 2025, Intology, 2025).As shown in Figure 1, we demonstrate the progress made by AI Scientist-v2 (Yamada et al., 2025), and the research output has received review scores exceeding the average acceptance threshold for human-authored papers.Similarly, researchers present an empirical validation through multiple peer-reviewed publications accepted at ICLR 2025 workshops and ACL 2025 main conference (Intology, 2025).Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools (e.g., AlphaFold (Jumper et al., 2021)).</p>
<p>In this position paper, we first propose a conceptual framework (Section 2) that defines an AI Scientist as an advanced end-to-end system capable of independently formulating scientific ideas and performing the implementation for verifying these ideas.This definition forms the theoretical foundation of our position, aligns with current research progress (Lu et al., 2024, Weng et al., 2025, Yamada et al., 2025), and emphasizes that the core capability of an AI Scientist lies in generating innovative and feasible ideas at scale (Si et al., 2024, Wang et al., 2024a, Hu et al., 2024, Yang et al., 2025d).The idea-generation capability is a key feature that sets AI Scientists apart from automated scientific tools.While recent advances demonstrate that AI Scientists can generate highly innovative ideas (Si et al., 2025), their implementation capabilities remain constrained (Chan et al., 2024, Starace et al., 2025, Xiang et al., 2025, Siegel et al., 2024, Padigela et al., 2025), creating a significant gap between innovative idea generation and complete implementation.</p>
<p>Our Position:</p>
<p>The fundamental bottleneck for AI Scientists lies in their implementation capability to effectively execute the verification of these ideas.</p>
<p>We defend our argument by analyzing quantitative evidence from existing benchmarks used to evaluate LLMs' abilities in performing complex engineering tasks (Section 3.2).While LLMs can generate highly novel ideas (Si et al., 2024, Chai et al., 2024, Gottweis et al., 2025), their performance in experiment execution is exceptionally poor (Table 1).For instance, a leading LLM like Claude 3.5 Sonnet scored only 1.8% on PaperBench (Starace et al., 2025).This implementation gap is further supported by a systematic evaluation (Section 3.3), which leverages a state-of-the-art review model, DeepReviewer-14B (Zhu et al., 2025), to assess 28 research papers generated by five advanced AI Scientist systems.The results demonstrate that current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers.Finally, to clearly illustrate the root cause of the implementation gap, we provide an in-depth discussion on the fundamental limitations of AI Scientist (Section 4).</p>
<p>In summary, this paper validates and deeply analyzes the implementation gap in existing AI Scientist systems based on extensive quantitative evidence and a simulated peer-review process.Furthermore, as the development of AI Scientists will bring greater regulatory challenges, we comprehensively examine the ethical considerations (Section 5) faced by AI Scientists and suggest directions for future research (Section 6).We hope this position paper will contribute to a clearer understanding of the limitations of current AI Scientist, shedding light on the future development of AI Scientist.</p>
<p>Definition of the AI Scientist</p>
<p>The emergence of automated scientific tools has accelerated scientific discovery across numerous domains (King et al., 2009, Yang et al., 2025c, Jumper et al., 2021, Stokes et al., 2020, Szymanski et al., 2023).However, these tools fundamentally operate within a paradigm where human researchers remain in the dominant position of scientific discovery, and thus cannot be classified as fully automated AI Scientists.In this section, we first provide a detailed discussion of the unique characteristics of the AI Scientist (Section 2.1).Building on this discussion, we then propose a conceptual framework that formally defines the AI Scientist in a mathematical form (Section 2.2).Scientific tools, originating from AI for Science research, represent specialized AI systems designed to solve specific scientific problems by processing data and generating results within defined domains.These tools have demonstrated remarkable success across diverse scientific fields, including protein structure prediction (e.g.Al-phaFold) (Jumper et al., 2021), antibiotic discovery through deep learning approaches (Stokes et al., 2020), and autonomous chemical research with large language models (Boiko et al., 2023).These scientific tools funda-mentally operate within a knowledge-dependent collaborative framework between humans and AI.</p>
<p>Unique Characteristics</p>
<p>AI Scientist represents a research paradigm shift where AI assumes the role of an autonomous scientist capable of conducting independent scientific research.As illustrated in Figure 2, while scientific tools operate under human supervision, receiving data as input and producing predictions as output, AI Scientist goes a step further by demonstrating autonomous scientific reasoning capabilities.It accepts research questions as input and engages in iterative, self-directed interactions with scientific tools to generate comprehensive solutions.Unlike scientific tools that function as sophisticated instruments awaiting human guidance, AI Scientist exhibits genuine scientific agency, conducting end-to-end scientific investigations from question formulation to solution discovery (Yamada et al., 2025).</p>
<p>Conceptualized Framework</p>
<p>Our Definition: An AI Scientist is an advanced end-to-end system capable of independently formulating scientific ideas and executing the requisite verification and falsification procedures.</p>
<p>We define an AI Scientist, denoted as  AI , as a fully autonomous scientific intelligence capable of independently performing diverse scientific research tasks.Different from a general scientific tool, it must possess dual capacities, including idea generation and experimental execution.A complete scientific research task typically originates from an initial scientific question  init and leverages existing domain knowledge  domain .An AI Scientist, denoted  AI , operates within the scope of human ethical constraints ℛ human and resource constraints ℬ res to conduct this task.The primary output is the generation of novel scientific knowledge  new and associated verifiable artifacts  sci .The process through which an AI Scientist aims to achieve the optimal output from a scientific research task can be formally represented as:
(𝒦 new , 𝒜 sci ) ← max{𝒮 AI (𝒬 init , 𝒦 domain , ℛ human |θ AI , ℬ res )} (1)</p>
<p>Arguments for Implementation Capability</p>
<p>We argue that the fundamental bottleneck limiting AI Scientists lies not in their idea generation capabilities, but in their capacity to execute rigorous implementation procedures required for reliable scientific research.To support this position, we present three lines of evidence: systematic analysis of research trends in the AI Scientist literature (Section 3.1), comprehensive benchmark analysis across multiple evaluation frameworks (Section 3.2), and systematic peer review assessment using LLM-as-a-Judge methodology (Section 3.3).</p>
<p>Research Trend of AI Scientist</p>
<p>Our statistical analysis of AI Scientist papers on arXiv up to May 23, 2025 (see Appendix B for details), reveals key trends illustrated in Figure 3.The lower panel of the figure shows that while the total number of publications is growing, studies focusing on idea generation without concrete implementation details consistently outnumber those incorporating such implementations.Despite this disparity in publication numbers, the upper panel indicates a crucial counterpoint: papers that include substantive implementation details achieve a significantly higher average number of citations.This signals strong community valuation for executable advancements and underscores the importance of addressing the implementation gap.This then raises a critical question: if implementation-focused research garners higher impact, why does its volume remain markedly lower?This disparity strongly implies that the path of implementation is fraught with substantial challenges.Empirical Evidence of Implementation Gap.Advanced LLMs achieve near-saturated performance on simple code generation benchmarks like HumanEval (Chen et al., 2021, Liu et al., 2023, Yang et al., 2025a).For example, o3 exhibits excellent problem-solving capabilities in the 99.8th percentile of human performance on algorithmic competition platforms like Codeforces.However, the performance of SoTA LLMs drops dramatically when it comes to real-world research scenarios.As depicted in  (Siegel et al., 2024) (reproducing computational results from scientific papers, determined by accuracy), and ML-Dev-Bench (Padigela et al., 2025) (completing diverse ML development workflow tasks, assessed by success rates).Each takes a different approach to measuring how well AI systems can automate aspects of ML research.These evaluations consistently demonstrate that LLMs face difficulty in translating conceptual understanding or initial plans into verifiably correct and operational code.This "implementation gap" fundamentally limits AI Scientist's verification capabilities.</p>
<p>Quantitative Analysis</p>
<p>Beyond Code Generation.The complexity of real-world research implementation processes extends far beyond simple code generation tasks, often requiring sustained reasoning and multi-step problem-solving.However, current LLMs exhibit relatively weak performance on such complex challenges.LiveCodeBench (LCB) (Jain et al., 2024), a more complex evaluation benchmark than Humaneval (Chen et al., 2021) that collects problems from periodic contests on LeetCode, AtCoder, and Codeforces platforms, evaluates Code LLMs across diverse code-related scenarios, including code generation, execution, self-repair, and output prediction.o4-mini achieves SoTA performance on the code generation subtask with only 52.1% pass@1 score.This poor performance on complex coding tasks reveals that AI scientists lack the implementation ability to handle sophisticated code-based research scenarios.</p>
<p>Implementation and verification.We observe that the verification bottleneck emerges across multiple stages of the research process.SciReplicate-Bench (Xiang et al., 2025), which tasks LLM agents with generating Python code to reproduce algorithms from NLP research papers, reveals that despite agents demonstrating an understanding of algorithmic logic (evidenced by high reasoning graph accuracy), they struggle with code execution.The best agent achieved only 39% execution accuracy, indicating its generated code passed functional test cases for just 39% of the tasks, highlighting a failure to ensure implementation correctness and runtime behavior.Similarly, PaperBench (Starace et al., 2025) requires LLM agents to replicate entire machine-learning papers from scratch by developing codebases and running experiments.While agents can generate code components (e.g., o1-High achieving 43.4% success on weighted "Code-Development" sub-tasks), their performance on subsequent verification stages is poor.On rubric-defined leaf nodes for "Execution" (successfully running the code) and "Result Match" (quantitatively matching the paper's reported results), Claude 3.5 Sonnet scored only 1.8% and 0.7% respectively.This poor performance indicates a breakdown in ensuring the developed solution operates correctly and produces the intended outcomes.</p>
<p>Discussion.The verification challenge extends beyond initial code implementation to debugging, iterative refinement, and validation of experimental outcomes.Evidence from MLE-Bench and ML-Dev-Bench (Chan et al., 2024, Padigela et al., 2025) shows that LLM agents frequently fail to debug their code or produce valid submissions, with 20% of o1 preview runs on MLE Bench failing this step, and struggle to optimize model performance.Debugging, an explicit verification procedure, also indicates persistent agent failures that highlight the verification bottleneck (Chan et al., 2024).The incapacity to iteratively refine solutions towards better performance, illustrated in ML-Dev-Bench where all tested agents scored 0% on "Model Performance" tasks, further signifies deficiencies in robust verification loops essential for scientific advancement (Padigela et al., 2025).Furthermore, CORE-Bench, which requires agents to reproduce results and then answer questions based on these outputs, assesses the verification of entire computational experiments.This process, involving multiple stages of reproduction and reasoning, presents significant challenges.For instance, the imperfect success rates (e.g., CORE-Agent with GPT-4o achieved 55.56% on CORE-Bench Medium) highlight the difficulties in this complex verification process (Siegel et al., 2024).These difficulties across verification tasks suggest that enhancing AI Scientists' systematic verification capability is crucial for their maturation into ideal AI Scientists.Current LLMs, while proficient in content generation, fail to rigorously validate their outputs against explicit criteria, a foundational component of scientific practice.</p>
<p>LLM-as-a-Judge Reveals the Implementation Weaknesses</p>
<p>To further support the existence of implementation gap, we employ a simulated peer review methodology to assess the actual quality of scientific outputs from current AI Scientist systems, particularly their implementation-level reliability.We select 28 publicly available research papers generated independently by five different AI Scientist systems and utilize the SoTA review model DeepReviewer-14B (Zhu et al., 2025) to conduct systematic evaluation under unified standards.We acknowledge that potential selection bias in the public availability of these papers (e.g., researchers may only publish better-performing outputs) means our evaluation results may not fully represent the average output quality of these systems across all scenarios.Nevertheless, this analysis provides valuable insights into the general quality level of current AI-generated research papers.</p>
<p>Rooted Limitations of Execution Capabilities</p>
<p>Our empirical analysis (Section 3) reveals a clear pattern that while AI Scientists are conceptualized as advanced iterations of traditional scientific tools, they consistently fail at implementation and verification procedures across diverse scientific contexts.This raises a critical question: Why do these sophisticated systems fail to achieve consistently strong results, especially when traditional scientific tools, wielded by human researchers, prove highly effective?To understand this paradox, we provide a discussion on the root cause of the implementation gap (Section 4.1 ) and present an in-depth analysis of the fundamental limitations of AI Scientist (Section 4.2).</p>
<p>Two Primary Facets of Implementation Gap</p>
<p>The implementation gap for AI Scientists comprises two primary facets: (1) AI Scientists often exhibit bottlenecks in the planning and execution stages.This manifests in three key areas: failures in longrange logical reasoning required for coherent experimental design, inadequate multi-agent collaboration capabilities including strategic planning across complex multi-file implementations and converting conceptual ideas into working code, and insufficient coordination with external tools and systems; (2) Even when implementation code is generated, AI Scientists demonstrate fundamental weaknesses in evaluation processes.This includes failures in debugging capabilities, experimental validation, result interpretation, and iterative refinement based on experimental feedback.Current systems lack robust mechanisms for assessing implementation quality, validating experimental outcomes, and providing reliable feedback loops that can guide subsequent implementation improvements.</p>
<p>Prevent building "castle in the air".Agent tools often produce difficult-to-verify code and experiments, while evaluation gaps prevent AI Scientists from recognizing and correcting implementation issues through iterative refinement.Without fundamentally enhancing both capabilities, the idealized AI Scientist capable of independent scientific exploration will remain inefficient.</p>
<p>Rooted Limitations</p>
<p>From the current literature on AI scientists, we conclude four major limitations that collectively explain why AI scientists struggle with complex, multi-stage implementation processes:</p>
<p>Limitation 1: fundamental cognitive and execution capabilities.Scientific implementation requires sophisticated long-range logical reasoning across multiple abstraction levels.Existing LLMs demonstrate significantly decreased coherence and robustness as reasoning chains extend (Wu et al., 2025b,a), and increased thinking time does not necessarily yield stronger performance (Ballon et al., 2025).Furthermore, LLM-based agents possess limited capacity to retain past interaction information, with memory deteriorating as text length increases (Pink et al., 2025, Cemri et al., 2025).Most critically, mainstream language models exhibit markedly weaker performance in multi-turn dialogues or multi-step interactive tasks requiring context coherence, deep understanding, and state tracking, with average performance decreases reaching 39% (Laban et al., 2025).This capability degradation in scenarios involving long-range dependencies and complex interactions directly constrains AI Scientist performance in executing complex scientific experiments requiring sustained attention and coherent reasoning chains.</p>
<p>Limitation 2: strategic planning and reasoning.Scientific implementation requires comprehensive abilities for strategic reasoning, continuous monitoring, and dynamic adjustment across all research stages (Lu et al., 2024, Yamada et al., 2025).High-quality research implementation demands global planning abilities spanning entire codebases, which typically contain multiple interdependent files with hundreds of lines requiring coordinated modification (Jimenez et al., 2024, Aleithan et al., 2024).Long-term, complex scientific exploration tasks such as discovering new materials, and modeling complex biological systems particularly require continuous iteration of research directions and experimental strategies over extended time scales based on emerging results and external feedback (Merchant et al., 2023, Brixi et al., 2025, Weng et al., 2023).However, current LLMs demonstrate inadequate adaptive planning and metacognitive abilities when handling highly open, creative scientific research requiring dynamic adjustments to overall research blueprints.While reinforcement learning approaches may potentially enhance LLMs' generalization and metacognitive capabilities, the resource investment required for "inventor" roles like AI Scientists that need to perform complex asynchronous operations and real-world interactions proves enormous.Figure 4 highlights AI's acceleration over human performance in complex tasks such as reasoning and web-based research.While AI Scientists also achieve tasks faster than humans, their estimated single-sample RL training time is orders of magnitude greater than simpler AI agents.This substantial increase in required sampling time (detailed in Appendix A) underscores the immense challenge of developing AI Scientists via standard RL methodologies.(Guo et al., 2024, Qian et al., 2024, Pu et al., 2025b).This requires AI Scientist to not only understand instructions conforming to collaborative protocols but also precisely execute the implementation phases assigned to it within tasks and reliably feed its outputs back to the collaborative network (Bo et al., 2024, Zhang et al., 2024).However, current LLM Agents still have considerable room for improvement in robustness and adaptability when interacting with dynamic environments (Wei et al., 2025).For instance, when calling a series of external APIs to complete a complex scientific computational process, LLM often struggles to handle subtle changes in API interfaces, and other practical engineering issues (Shen et al., 2025).</p>
<p>Limitation 4: evaluation and verification.Existing benchmarks such as MLE-Bench (Chan et al., 2024) and PaperBench (Starace et al., 2025) primarily focus on the complete reproduction of code and experiments from papers.SciReplicate-Bench (Xiang et al., 2025) emphasizes generating necessary code from scientific papers, while ScienceAgentBench (Chen et al., 2025) concentrates on independent and singular data-driven tasks.However, there is currently a lack of a comprehensive benchmark that can evaluate the entire scientific workflow, from initial idea generation through to final implementation and completion.This absence makes it difficult to fairly compare the end-to-end capabilities of different AI Scientist systems.</p>
<p>Additionally, there is a deficiency in evaluation approaches that incorporate measures for external supervision during the AI Scientist's implementation process.The deeper issue is that the quality of scientific discovery often lacks unified objective standards, and the process of scientific exploration is filled with uncertainty and openness, making comprehensive evaluation and effective supervision of AI Scientist's verification capability exceptionally difficult.Evaluating AI Scientist's output (e.g., generated papers) from a peer review perspective, while being a results-oriented assessment method, also has inherent limitations.As in human research systems, even experienced peer reviewers may not always accurately identify the groundbreaking and far-reaching work.A frequently cited example is that the word2vec paper (Mikolov et al., 2013) was initially rejected by ICLR 2013, but later received the "Test of Time Award" at NeurIPS 2023.Extensive experimental analyses have demonstrated that review scores are not reliable indicators for predicting future impact (Abramo et al., 2019, Cortes andLawrence, 2021), suggesting that peer review may be more suitable for filtering low-quality papers rather than identifying the highest quality papers.</p>
<p>Ethical Considerations</p>
<p>Sub-Position: AI scientists are in urgent need of a comprehensive system for generation management and quality evaluation.</p>
<p>As autonomous research agents, AI Scientists lack values and moral constraints.They are incapable of making ethical judgments about the societal impact of their work, and they do not self-regulate based on potential risks associated with their findings (Bengio et al., 2025).As AI Scientists possess stronger capabilities in idea generation and experiment execution, their influence on scientific research and society could far surpass that of current LLMs and scientific tools (e.g., Deep Search, AutoSurvey (Wang et al., 2024c)).In the absence of proper oversight, AI Scientists may: (1) be misused, overwhelming the peer review system, leading to a decline in overall research quality;</p>
<p>(2) enter unethical or dangerous research domains, autonomously generating and publishing sensitive findings that accelerate the development of harmful technologies;</p>
<p>(3) weaken the quality of PhD training, leading to a decline in human research standards and overall scientific literacy.To prevent the above situations, we argue that AI Scientists are in urgent need of a comprehensive system for generation management and quality evaluation, thus enabling effective behavior regulation within the human moral framework (Jobin et al., 2019).This system should include, but not be limited to, the following components:</p>
<p>(1) Implement measures to prevent AI-generated content from disrupting human review systems: Effective strategies should be adopted to ensure that AI-generated articles do not interfere with human peer-review systems while maintaining high standards of quality.This includes establishing a centralized platform to archive scientific outputs generated by AI Scientists, developing automated detection systems to identify such content, and creating specialized evaluation tools (e.g., DeepReview (Zhu et al., 2025)) to assess the quality of AI-generated research outputs.These tools should help identify and filter low-quality content, thereby reducing the burden on the peer review process.All AI-generated outputs must be transparently labeled and reviewed, including information on their origin, generation methods, and scientific tools.</p>
<p>(2) Establish boundaries and strengthen training programs: Implement clear boundaries between human-led and AI-led research processes to ensure that PhD students receive comprehensive training.Key components of doctoral education(e.g., idea testing), should prioritize human involvement to maintain high standards of scientific literacy.Additionally, guidelines should be established to prevent over-reliance on AI Scientists in PhD training, ensuring that AI tools serve as supplements rather than substitutes in the educational process.</p>
<p>(3) Formulate an ethics and responsibility convention: A global convention should be established to define the ethical boundaries and risk management principles for AI-driven research (Huang et al., 2022).</p>
<p>All researchers and institutions utilizing AI Scientists must fully disclose the generation process, algorithmic sources, training data, and potential societal risks of their findings.Additionally, a hybrid mechanism combining automated and human-in-the-loop review should be implemented for continuous ethical oversight and risk evaluation, ensuring that AI Scientist research activities remain within socially safe boundaries (Jobin et al., 2019, Khan et al., 2022).Furthermore, appropriate legislation should be developed to regulate AI Scientists by imposing strict limitations on their use for specific research purposes.</p>
<p>Future Directions</p>
<p>This section outlines feasible pathways to bridge the current implementation capability gap of AI Scientists.</p>
<p>Addressing foundational Basic Abilities is paramount.While scaling laws for pre-training and post-training (Kaplan et al., 2020, Zhang et al., 2025) promise progressive LLM improvements, immediate strategies like well-defined Workflows (Li et al., 2024d, Gu et al., 2024b) also can mitigate current implementation weaknesses.Structuring research processes with human-defined tools allows for guided AI execution and targeted interventions.For instance, Retrieval Augmented Generation (RAG) can counteract limitations in handling long texts or accessing current information (Fan et al., 2024, Arslan et al., 2024), thus expanding the knowledge scope of AI systems.</p>
<p>A significant challenge for sophisticated Strategic Planning is the immense resource consumption of RL (Cao et al., 2024).A promising direction to alleviate this involves leveraging LLMs to simulate aspects of the environment or task execution, thereby accelerating the RL feedback loop (Sun et al., 2025).By allowing the RL agent to receive quicker, albeit potentially approximate, feedback on its actions, particularly for operations that are inherently time-consuming in the real world, the sampling efficiency may be significantly improved.This could reduce the extensive wall-clock time typically required for training robust long-horizon planning and adaptive meta-thinking capabilities in complex scientific domains.</p>
<p>Ensuring Reliable Verification and Fostering Collaboration is crucial.Standardized protocols like MCP and A2A (Yang et al., 2025b, Ray, 2025, Hou et al., 2025) can establish basic interoperability.A promising direction is to build modular multi-agent systems, where specialized AI agents for sub-tasks (e.g., literature review, code generation) are coordinated by a central "Planner Agent" trained via advanced RL, leveraging existing tools (e.g., PASA (He et al., 2025)) rather than reinventing capabilities.Furthermore, enhanced oversight of AI Scientist inference processes is imperative, not just to prevent benchmark "hacking", but also to instill ethical boundaries against unscrupulous data acquisition or other problematic behaviors.</p>
<p>Finally, the Evaluation of AI Scientists (Chang et al., 2024) must evolve towards a holistic, coarse-grained paradigm reflecting real-world scientific discovery's multifaceted nature.Scientific breakthroughs involve both practical utility and novelty.Thus, evaluation frameworks should go beyond single-metric optimization, adopting multi-objective criteria that assess performance gain, originality, experimental rigor, and communication clarity.This multi-faceted approach will offer a more accurate measure of an AI Scientist's true contribution, guiding development toward impactful scientific exploration.</p>
<p>Conclusion</p>
<p>The rise of AI Scientists marks a paradigm shift in scientific discovery, with large language models (LLMs) now driving the workflow from idea generation to experiment execution.Recent systems have shown promise, producing research accepted at ICLR 2025 workshops and sparking discussions on the imminence of human-level AI Scientists.However, despite this progress, AI Scientists have yet to achieve breakthroughs in computer science comparable to traditional automated tools.Based on benchmark analyses and a systematic evaluation of 28 papers from five leading AI Scientist systems, we identify a core bottleneck: the inability to reliably execute and verify experiments.This implementation gap limits both scientific rigor and the quality of the research output.We analyze its root causes and call on the community to address this critical limitation.</p>
<p>Alternative Views.An alternative viewpoint suggests that AI Scientists need not pursue completely autonomous implementation capabilities in the short term, but rather facilitate human-machine collaboration as Co-scientists to assist humans.This approach avoids the deficiencies of LLMs in Dynamic Planning capabilities and Reliable Verification capabilities, instead allowing AI to focus on its strengths, such as idea generation, while humans execute the specific experimental results (Weng et al., 2025).If an AI system, though unable to independently complete all implementation details, can increase human scientists' efficiency tenfold, or help human scientists conceive and verify complex ideas previously beyond reach, then it undoubtedly also qualifies as a successful "collaborative scientist."</p>
<p>Unite</p>
<p>A. Sampling Time Calculation for Different Types of AI Agents</p>
<p>We referenced existing literature (Guo et al., 2025, Yang et al., 2025a, Muennighoff et al., 2025) and our experience to estimate the sampling time potentially required for different types of AI agents trained via reinforcement learning, as illustrated in In contrast, an AI Scientist executing end-to-end scientific discovery tasks has complexity and interaction requirements far exceeding the previous two types.We roughly estimate it might need to generate over 100,000 tokens of content (for example, operational and experimental code approximately 50,000 tokens (T in f er_code ≈ 1250s), research paper writing approximately 30,000 tokens (T in f er_paper ≈ 750s), reviewing and understanding relevant literature approximately 20,000 tokens (T in f er_lit ≈ 500s)), with pure LLM inference time for just this portion being T in f er_CS = T in f er_code + T in f er_paper + T in f er_lit ≈ 2500s.More critically, the "implementation" process of an AI Scientist, such as code writing, debugging, compiling, running experiments, and data analysis, is highly asynchronous and time-consuming.Assuming a rapid research code operation and experimental cycle (from writing to obtaining preliminary results) requires an average of T op_code ≈ 12 hours = 43200s, while in-depth literature research and analysis might require T op_lit ≈ 20 minutes = 1200s.Therefore, the total estimated sampling time to complete a relatively complete scientific exploration loop would be T sample_CS = T in f er_CS + T op_code + T op_lit ≈ 2500s + 43200s + 1200s ≈ 46900s.As intuitively demonstrated in Figure 4, the sampling time required for an AI Scientist (approximately 46,000 seconds) far exceeds that of an AI Reasoner (approximately 250 seconds) and an AI Web Agent (approximately 700 seconds).Notably, AI Reasoners can typically rapidly generate large quantities of training samples through batch generation in parallel, whereas each implementation step of an AI Scientist (especially parts involving code execution and experiment waiting) is almost entirely asynchronous, and requires exclusive computational resources or experimental equipment for learning and feedback collection during operations.Consequently, in actual reinforcement learning training processes, the disparity in real training duration between AI Scientists and the former two types will be even more pronounced.</p>
<p>For the calculation of human duration, we referenced existing metrics.For instance, for reasoning tasks, we referred to the human time from the International Mathematical Olympiad, which is approximately 1.5 hours per problem.For Web Agent tasks, we adopted the average human problem-solving time from BrowseComp (Wei et al., 2025) (2 hours) as the human standard.For Scientist tasks, although each paper often requires months of collaborative work by multiple people, for ease of calculation, we used the human duration of 48 hours from PaperBench (Starace et al., 2025) for statistics; however, even under these conditions, humans achieve a success rate of less than 50%.(Pu et al., 2025a), (Yang et al., 2024), (Su et al., 2024), (Li et al., 2024a), (Hu et al., 2024), (Liu et al., 2025), (Wang et al., 2024b) (Weng et al., 2025), (Xiong et al., 2024) (Gu et al., 2024a), (Li et al., 2024b), (Yu et al., 2024) (Gottweis et al., 2025) (Rabby et al., 2025), (Saeedi et al., 2025) (O'Neill et al., 2025), (Garikaparthi et al., 2025), (Sanyal et al., 2025) w/ Exp (Lu et al., 2024), (Li et al., 2024c) (Liu et al., 2024b), (Liu et al., 2024a) (Yuan et al., 2025), (Schmidgall et al., 2025) (Jiang et al., 2025), (Kon et al., 2025) (Schmidgall et al., 2025), (Jansen et al., 2025) (Yamada et al., 2025), (Seo et al., 2025)</p>
<p>B. Regarding the statistics for the papers</p>
<p>We have conducted a comprehensive search on arXiv to gather relevant publications in the AI Scientist field.This collection includes a series of papers from August 2024 to April 2025 for methods or systems, which are cited in Table 4.It indicates that, to date, a significant number of papers have focused on Idea Generation tasks, often without concrete implementations.</p>
<p>Nevertheless, an encouraging trend has emerged since early 2025.As illustrated in Figure 3, implementationfocused research has demonstrated stronger growth momentum, with incremental growth nearly matching that of non-implementation studies by Spring 2025.This suggests the community is beginning to recognize the critical importance of implementation capabilities for developing truly effective AI Scientists-moving beyond theoretical constructs toward practical systems capable of reliable execution.</p>
<p>Figure 1 :
1
Figure 1: The roadmap of AI Scientist from 2024 to future, highlighting key milestones and fundamental challenges that must be overcome to bridge the implementation gap of AI Scientist.</p>
<p>Figure 3 :
3
Figure 3: Analysis of AI Scientist publications on arXiv.The upper panel displays the average number of citations up to now, categorized by containing implementation details.The lower panel shows the growth in the total number of these papers with the same categorization.</p>
<p>Figure 4 .
4
Using a hypothetical 671B parameter LLM (similar to Deepseek-R1) running on 8 H100 cards (assuming 40 tokens generated per second), the pure inference time T in f er_R for a typical arithmetic reasoning task (generating approximately 10,000 tokens of reasoning content) might be around 250 seconds.For an AI Web Agent, the task might include generating approximately 8,000 tokens of instructions and reports (T in f er_WA ≈ 200s), interspersed with approximately 20 API calls for information search (assuming each search and processing takes T search_API = 10s, totaling T search_total = 20 × 10s = 200s), and potentially requiring reading and comprehension of up to 400,000 tokens of web content (assuming reading and comprehension time T read_WA ≈ 200s).The total sampling time is: T sample_WA ≈ T in f er_WA + T search_total + T read_WA ≈ 600s.</p>
<p>Table 1 :
1
State-of-the-art (SoTA) LLMs show relatively low accuracy on code implementation on different tasks.The listed benchmarks are collected from diverse domains.The table below details their tasks, domains, scale, methods, and performance.
Benchmark Task DescriptionDomainsScaleLLM Acc. PerformanceMLE-Bench(Chan et al., 2024) AI Training taskApplied ML75 OpenAI o1-preview16.90%PaperBench (Starace et al., 2025) ICML paper ReplicatingNLP, CV, ML8,316OpenAI o1-high26.00%SciReplicate-Bench (Xiang et al., 2025) Code GenerationNLP100Claude-Sonnet-3.739.00%CORE-Bench (Siegel et al., 2024) Scientific Paper reproduc-Computer Science,270OpenAI GPT-4o55.56%tionSocial Science, andMedicineML-Dev-Bench (Padigela et al., 2025) AI training taskML30Claude-Sonnet-3.550.00%</p>
<p>Table 2 :
2
DeepReviewer-14B Evaluation of AI-Generated Papers from Various AI Scientist Systems.Scores reflect averages across the 'Num' of available papers.Note: Publicly available papers may be curated and not fully representative of typical system output.
AI Scientist SystemNum Soundness↑Presentation↑Contribution↑ Decision↑Rating↑Percentile↑HKUSD AI Researcher71.751.461.570.02.573.43%AI Scientist102.081.801.750.03.358.22%AI Scientist v231.671.501.500.02.332.04%CycleResearcher-12B62.251.752.130.03.7516.88%Zochi22.382.382.250.04.6329.96%</p>
<p>Table 3 :
3
Defect Categories and Their Issues.
Defect CategoryNumber PercentageExperimental Weakness28100%Methodological Unclarity/Flaws2796.4%Writing &amp; Presentation Issues2692.9%Novelty Concerns2589.3%Theoretical Weakness2485.7%Literature Review Deficiencies2278.6%Practicality &amp; Robustness Gaps2175.0%Reproducibility Issues2071.4%Computational Cost Concerns1864.3%Component Analysis1657.1%Hyperparameter Analysis Lacking1657.1%Ethical Considerations Missing310.7%</p>
<p>Table3shows that among the twelve major defect categories, "Experimental Weakness" appears across all 28 evaluated AI-generated papers, with a 100% occurrence rate.
This finding supports our positions regardingimplementation capability limitations, in experimental design, execution, and result analysis. The secondand third most prevalent issues are "Methodological Unclarity/Flaws" (96.4%) and "Writing &amp; PresentationIssues" (92.9%), which reflect AI Scientists' insufficient ability to clearly articulate and implement researchplans. "Novelty Concerns" (89.3%) and "Theoretical Weakness" (85.7%) occur frequently, indicating that
when AI Scientists generate complete papers, they struggle to propose original scientific contributions with solid theoretical foundations.The prevalence of these high-frequency defects highlights systemic issues in the scientific rigor and implementation quality of current AI-generated research, falling below the standards for reliable and valuable scientific outputs.</p>
<p>Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang.
AI.Google'snewai"co-scientist"aimstoacceleratescien-tificdiscovery.Unite.AI,Feb2025.URLhttps://www.unite.ai/googles-new-ai-co-scientist-aims-to-accelerate-scientific-discovery/.
Swe-bench+: Enhanced coding benchmark for llms.arXiv preprint arXiv:2410.06992,2024.</p>
<p>Table 4 :
4
Timeline of AI Scientist Ideas and Code Implementations by Month
2024-082024-092024-102024-112024-122025-012025-022025-032025-04w/o Exp (Zheng et al.,(Ghafarollahi2024)and Buehler,2024), (Raden-sky et al.,2024)
Minjun Zhu and Qiujie Xie contributed equally to this work. Corresponding author(s): Linyi Yang: yanglinyiucd@gmail.com; Yue Zhang: Email zhangyue@westlake.edu.cn
https://ai-researcher.net/social-iclr-2025
AcknowledgementsThe genesis of this position paper traces back to the insightful discussions and interactions at the AI Co-scientist Discussion held in conjunction with ICLR 2025 on April 26, 20241 .We extend our sincere gratitude to the invited speakers, including Chenglei Si, Jindong Wang, Yutaro Yamada, and David Ha, whose perspectives are invaluable.We also deeply appreciate the contributions of the more than 200 participants who engaged in the vibrant discussions on that day; many of the ideas explored in this work were sparked and refined through those collective interactions.We thank every participant for their engagement and for fostering a stimulating environment that significantly shaped our thinking.
Peer review versus bibliometrics: Which method better predicts the scholarly impact of publications?. Giovanni Abramo, Ciriaco Andrea, D' Angelo, Emanuela Reale, Scientometrics. 1212019</p>
<p>A survey on rag with llms. Muhammad Arslan, Hussam Ghanem, Saba Munawar, Christophe Cruz, Procedia Computer Science. 2462024</p>
<p>The relationship between reasoning and performance in large language models-o3 (mini) thinks harder. Marthe Ballon, Andres Algaba, Vincent Ginis, arXiv:2502.156312025not longer. arXiv preprint</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, arXiv:2502.156572025arXiv preprint</p>
<p>Reflective multi-agent collaboration based on large language models. Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, Ji-Rong Wen, Advances in Neural Information Processing Systems. 202437</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Genome modeling and design across all domains of life with evo 2. Garyk Brixi, Matthew G Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A Gonzalez, Samuel H King, David B Li, Aditi T Merchant, BioRxiv. 2025</p>
<p>Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li, Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>Why do multi-agent llm systems fail?. Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, arXiv:2503.136572025arXiv preprint</p>
<p>Exploring scientific hypothesis generation with mamba. Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)2024</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 2157-6904153March 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Corinna Cortes, Neil D Lawrence, arXiv:2109.09774Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. 2021arXiv preprint</p>
<p>A survey on rag meeting llms: Towards retrieval-augmented large language models. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Aniketh Garikaparthi, Manasi Patwardhan, arXiv:2504.16728Lovekesh Vig, and Arman Cohan. Iris: Interactive research ideation system for accelerating scientific discovery. 2025arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research. Tianyang Gu, Jingjin Wang, Zhihao Zhang, Haohong Li, arXiv:2412.141412024aarXiv preprint</p>
<p>Large language models for constructing and optimizing machine learning workflows: A survey. Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, Shiyou Qian, arXiv:2411.104782024barXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Pasa: An llm agent for comprehensive academic paper search. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, arXiv:2501.101202025arXiv preprint</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024arXiv preprint</p>
<p>An overview of artificial intelligence ethics. Changwu Huang, Zeqi Zhang, Bifei Mao, Xin Yao, IEEE Transactions on Artificial Intelligence. 442022</p>
<p>. Intology. Zochi technical report. arXiv. 2025</p>
<p>Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024arXiv preprint</p>
<p>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.227082025arXiv preprint</p>
<p>Aide: Ai-driven exploration in the space of code. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu, arXiv:2502.131382025arXiv preprint</p>
<p>Swe-bench: Can language models resolve real-world github issues. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan, ICLR. 2024</p>
<p>The global landscape of ai ethics guidelines. Anna Jobin, Marcello Ienca, Effy Vayena, Nature machine intelligence. 192019</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 59678732021</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Ethics of ai: A systematic literature review of principles and challenges. Arif Ali Khan, Sher Badshah, Peng Liang, Muhammad Waseem, Bilal Khan, Aakash Ahmad, Mahdi Fahmideh, Mahmood Niazi, Muhammad Azeem, Akbar , Proceedings of the 26th international conference on evaluation and assessment in software engineering. the 26th international conference on evaluation and assessment in software engineering2022</p>
<p>The automation of science. Jem Ross D King, Stephen G Rowland, Michael Oliver, Wayne Young, Emma Aubrey, Maria Byrne, Magdalena Liakata, Pinar Markham, Larisa N Pir, Soldatova, Science. 32459232009</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. Patrick Tser, Jern Kon, Jiachen Liu, Qiuyi Ding, Yiming Qiu, Zhenning Yang, Yibo Huang, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Ang Chen, arXiv:2502.160692025arXiv preprint</p>
<p>Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville, arXiv:2505.06120Llms get lost in multi-turn conversation. 2025arXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, 1987MIT Press</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.131852024aarXiv preprint</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du, arXiv:2412.146262024barXiv preprint</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.140332024carXiv preprint</p>
<p>Autoflow: Automated workflow generation for large language model agents. Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang, arXiv:2407.128212024darXiv preprint</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, 202336</p>
<p>Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Yingzhou Lu, Yue Zhao, arXiv:2411.156922024aarXiv preprint</p>
<p>Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou, arXiv:2503.212482025arXiv preprint</p>
<p>Aigs: Generating science from ai-powered automated falsification. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, arXiv:2411.119102024barXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. 62479902023</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 262013</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Sparks of science: Hypothesis generation using structured paper data. O' Charles, Tirthankar Neill, Roberta Ghosal, Mike Răileanu, Thang Walmsley, Kevin Bui, Ioana Schawinski, Ciucă, arXiv:2504.129762025arXiv preprint</p>
<p>Ml-dev-bench: Comparative analysis of ai agents on ml development workflows. Harshith Padigela, Chintan Shah, Dinkar Juyal, 2025</p>
<p>Position: Episodic memory is the missing piece for long-term llm agents. Mathis Pink, Qinyuan Wu, Ai Vy, Javier Vo, Jianing Turek, Alexander Mu, Mariya Huth, Toneva, arXiv:2502.069752025arXiv preprint</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, Kevin Kj, Tovi Feng, Tom Grossman, Bhavana Hope, Matt Dalvi Mishra, Jonathan Latzke, Joseph Chee Bragg, Pao Chang, Siangliulue, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing Systems2025a</p>
<p>Piflow: Principle-aware scientific discovery with multi-agent collaboration. Yingming Pu, Tao Lin, Hongyu Chen, 2025b</p>
<p>Scaling large-language-model-based multi-agent collaboration. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, arXiv:2406.071552024arXiv preprint</p>
<p>Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, Prasenjit Mitra, Sören Auer, arXiv:2503.193092025arXiv preprint</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>A survey on model context protocol: Architecture, state-of-the-art, challenges and future directions. Partha Pratim, Ray , Authorea Preprints. 2025</p>
<p>Astroagents: A multi-agent ai for hypothesis generation from mass spectrometry data. Daniel Saeedi, Denise Buckner, Jose C Aponte, Amirali Aghazadeh, arXiv:2503.231702025arXiv preprint</p>
<p>Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R Varshney, Dilek Hakkani-Tur, arXiv:2504.20090Spark: A system for scientifically creative idea generation. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227arXiv:2504.17192Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. 2025. 2025arXiv preprintAgent laboratory: Using llm agents as research assistants</p>
<p>Shortcutsbench: A large-scale real-world benchmark for api-based agents. Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma, 2025</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations, ICLR 2025. SingaporeApril 24-28, 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, arXiv:2409.113632024arXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>A deep learning approach to antibiotic discovery. Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Shawn Craig R Macnair, Lindsey A French, Zohar Carfrae, Bloom-Ackermann, Cell. 18042020</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Zerosearch: Incentivize the search capability of llms without searching. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang, arXiv:2505.045882025arXiv preprint</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 62479902023</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024a1</p>
<p>Scipip: An llm-based scientific paper idea proposer. Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye, arXiv:2410.231662024barXiv preprint</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, Advances in Neural Information Processing Systems. 2024c37</p>
<p>Jason Wei, Zhiqing Sun, Spencer Papay, Scott Mckinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, Amelia Glaese, arXiv:2504.12516Browsecomp: A simple yet challenging benchmark for browsing agents. 2025arXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee, arXiv:2503.04723Shifting long-context llms research from input to output. 2025aarXiv preprint</p>
<p>When more is less: Understanding chain-of-thought length in llms. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang, arXiv:2502.072662025barXiv preprint</p>
<p>Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He, arXiv:2504.00255Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. 2025arXiv preprint</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, arXiv:2505.09388Chenxu Lv, et al. Qwen3 technical report. 2025aarXiv preprint</p>
<p>A survey of ai agent protocols. Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, arXiv:2504.167362025barXiv preprint</p>
<p>Shennongalpha: an ai-driven sharing and collaboration platform for intelligent curation, acquisition, and translation of natural medicinal material knowledge. Zijie Yang, Yongjing Yin, Chaojun Kong, Tiange Chi, Wufan Tao, Yue Zhang, Tian Xu, Cell Discovery. 111322025c</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>MOOSE-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, The Thirteenth International Conference on Learning Representations. 2025d</p>
<p>Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, arXiv:2412.17767Tao Feng, and Jiaxuan You. Researchtown: Simulator of human research community. 2024arXiv preprint</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, arXiv:2503.24235A survey on test-time scaling in large language models: What, how, where, and how well?. 2025arXiv preprint</p>
<p>Chain of agents: Large language models collaborating on long-context tasks. Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, Sercan Arik, Advances in Neural Information Processing Systems. 202437</p>
<p>Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, arXiv:2408.06941Unleashing ai for accelerated scientific research. 2024arXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
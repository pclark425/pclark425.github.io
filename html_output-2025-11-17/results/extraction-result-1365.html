<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1365 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1365</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1365</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-c520bf47db3360ae3a52219771390a354ed8a91f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c520bf47db3360ae3a52219771390a354ed8a91f" target="_blank">Go-Explore: a New Approach for Hard-Exploration Problems</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A new algorithm called Go-Explore, which exploits the following principles to remember previously visited states, solve simulated environments through any available means, and robustify via imitation learning, which results in a dramatic performance improvement on hard-exploration problems.</p>
                <p><strong>Paper Abstract:</strong> A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1365.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1365.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Montezuma's Revenge (Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Montezuma's Revenge (Atari 2600 benchmark game)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse- and deceptive-reward Atari platformer used as a hard-exploration benchmark; gameplay is organized into discrete rooms/levels with keys, doors, and long precise action sequences required between rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Go-Explore: a New Approach for Hard-Exploration Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Montezuma's Revenge</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D platformer video-game environment with discrete rooms and levels; agent must navigate rooms, collect keys, open doors, avoid hazards, and follow long precise action sequences to obtain sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Locked-door mechanics: progression depends on collecting keys (the Phase-1 domain-knowledge cell representation explicitly tracks which rooms keys were found in), conditional access between rooms based on keys/previous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Sparse, large state/room-graph with constrained connections (doors/keys) and many long narrow paths; connectivity is not dense and often requires long precise sequences to traverse between distant reward-bearing states.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Reported: Phase-1 with domain knowledge discovered mean 238 rooms (CI: 231-245); without domain knowledge Phase-1 visited mean 35 rooms (CI: 33-37) after 1.2B game frames. Environment has multiple levels; robustified policies solved up to hundreds/thousands of levels in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Go-Explore (Phase 1: archive + open-loop action replay + random exploration; Phase 2: robustification via Backward Algorithm + PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Phase 1 builds an archive of 'cells' (conflated states) and stores trajectories/environment states; returning to a chosen cell is done by replaying saved action sequences or emulator states (open-loop), then exploring randomly for k frames to discover new cells. Phase 2 uses imitation learning (Backward Algorithm) and PPO to produce a closed-loop recurrent policy robust to stochasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of rooms found / number of cells discovered; frames to solve level; cumulative game score; sample complexity (game frames) to reach specific achievements (e.g., solve level 1 or level 3).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Phase-1 (no domain knowledge): mean score 57,439 (CI: 47,843-67,224); solved level 1 in 57% of runs after 1.2B game frames (level-1 mean solve time 640M frames). Phase-1 (with domain knowledge): mean score 148,220 (CI: 144,580-151,730); solved level 1 in mean 57.6M frames and level 3 in mean 173M frames. Phase-2 robustified (with domain knowledge): mean score 666,474 (CI: 461,016-915,557); best run 18,003,200.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Phase-1: 57% runs solved level 1 (no-domain-knowledge, after 1.2B frames). Phase-2 robustified policies: all robustified policies solved level 1 with ~99.8% success under stochastic evaluation; other success statistics vary by run.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>A memory/archival-driven exploration plus closed-loop robust policy: (1) use an archive of remembered states (cells) and deterministic returns (open-loop) to explore deeply, then (2) robustify into a closed-loop recurrent policy (PPO) that is robust to stochasticity; effective policies require memory/history when partial observability exists.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Explicitly reported: incorporating domain knowledge that preserves room/level identity and key collection (i.e., reducing harmful state conflation) dramatically improves exploration efficiency and final performance (many more rooms visited, faster solving of levels, much higher scores). Partial observability or overly aggressive conflation (downsampled image cells) impairs exploration because different meaningful states appear similar, preventing reaching reward states. Dead-ends and inefficient detours (traversing dead-ends then returning) are observed as inefficiencies that robustification tends to remove due to RL discounting; deterministic training can encourage brittle short paths that fail under stochasticity (the 'busy-highway' problem). Returning-to-cells (reducing derailment) and remembering cells (reducing detachment) are key structural relations between topology and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper compares two effective 'topologies' induced by cell representations: a domain-knowledge-aware conflation (room number, discretized agent x,y, level, keys) versus a domain-agnostic aggressive pixel downsampling. The domain-knowledge representation yields far better coverage (238 vs ~35 rooms), faster solving times (e.g., level-1 solve in ~57.6M frames vs ~640M), and much higher scores, showing that preserving structurally meaningful graph distinctions (rooms, keys) improves navigation/exploration performance dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that separate exploration from robust control perform best: Phase-1 open-loop archival trajectories + targeted exploration discover deep-reaching trajectories; Phase-2 closed-loop RL with recurrence is required to be robust under stochasticity. Memory of previous states (archive) and the ability to deterministically return to states (or train goal-conditioned return policies) are crucial. Robustification tends to remove inefficient behaviors like visiting dead-ends (due to reward discounting), and partial-observability demands recurrent/goal-conditioned architectures; open-loop replay suffices only in deterministic training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Go-Explore: a New Approach for Hard-Exploration Problems', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1365.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1365.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pitfall (Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pitfall! (Atari 2600 benchmark game)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A highly deceptive, partially-observable Atari platformer with very sparse positive rewards and many small negative-reward actions; success requires long sequences of precise navigation across 255 rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Go-Explore: a New Approach for Hard-Exploration Problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Pitfall</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 2D platformer organized into many rooms (255 reported rooms) where positive rewards are sparse and many actions yield small negative rewards; partial observability leads visually-similar observations to correspond to different underlying states requiring different actions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Large, sparse room graph with many visually ambiguous states (partial observability); many transitions require remembering prior context because identical-looking frames can map to different nodes in the true underlying state graph.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>255 rooms (full game); Phase-1 with domain knowledge visited all 255 rooms; Phase-1 without domain knowledge found ~22 rooms (with a finer downscaling up to ~30 rooms in preliminary experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Go-Explore (Phase 1: archive + random exploration; Phase 2: Backward Algorithm + PPO robustification of truncated demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Phase 1 used either an image-downsampled cell representation (domain-agnostic) or a domain-knowledge cell representation tracking room number and agent x,y; Phase 1 stored trajectories and discovered high-scoring trajectories. Because of partial observability and long trajectories, successful robustification used truncated shorter demonstrations; Phase 2 trained recurrent PPO policies via the Backward Algorithm to be robust under stochastic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number of rooms visited/cells discovered; cumulative game score; frames to discover rewards; sample complexity (game frames) for robustification.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Phase-1 (no domain knowledge): ~22 rooms found, no positive rewards found. Phase-1 (with domain knowledge): visited all 255 rooms and found trajectories with mean score 70,264 (CI: 67,287-73,150). Phase-2 robustified agents (from truncated demonstrations) achieved mean score 59,494 (CI: 49,042-72,721) after mean 8.20B game frames to robustify.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>A memory-capable closed-loop policy trained via imitation-augmented RL (Backward Algorithm + PPO with recurrence) that can generalize beyond demonstrations; short robustifiable demonstration segments were needed because full-length brittle demonstrations failed to robustify.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Partial observability and excessive conflation in the cell representation (downsampled pixels) led Phase-1 to fail to find rewards: visually-identical frames correspond to different logical states, breaking pixel-based cell conflation. Introducing domain knowledge that disambiguates the true room identity (room number) allowed full coverage (255 rooms) and successful discovery of reward-bearing trajectories. Thus, preserving graph distinctions (nodes that are meaningfully different despite pixel similarity) is crucial for exploration success. Long brittle trajectories discovered under deterministic Phase-1 can be hard to robustify if they require remembering long histories that the robustifier does not get as input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Comparison between domain-agnostic downsampled pixel cells and domain-knowledge cells: pixel-based conflation caused partial observability failures (no positive rewards found), while domain-knowledge cells that retained room identity enabled visiting all rooms and finding high-scoring trajectories; demonstrates that the effective induced graph (via cell representation) determines whether exploration can reach reward nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Because states are visually ambiguous, successful robust policies require memory/recurrence and/or architectures that can infer and store past context; open-loop action replay used in Phase-1 cannot produce robust closed-loop behavior in the stochastic test environment. Shorter demonstration fragments (ending right after rewards) were more amenable to robustification, suggesting policies that can learn local subgoals and shorter segments generalize better than single very-long brittle trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Go-Explore: a New Approach for Hard-Exploration Problems', 'publication_date_yy_mm': '2019-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Bandit based Monte-Carlo Planning <em>(Rating: 2)</em></li>
                <li>Randomized Kinodynamic Planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1365",
    "paper_id": "paper-c520bf47db3360ae3a52219771390a354ed8a91f",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Montezuma's Revenge (Atari)",
            "name_full": "Montezuma's Revenge (Atari 2600 benchmark game)",
            "brief_description": "A sparse- and deceptive-reward Atari platformer used as a hard-exploration benchmark; gameplay is organized into discrete rooms/levels with keys, doors, and long precise action sequences required between rewards.",
            "citation_title": "Go-Explore: a New Approach for Hard-Exploration Problems",
            "mention_or_use": "use",
            "environment_name": "Montezuma's Revenge",
            "environment_description": "A 2D platformer video-game environment with discrete rooms and levels; agent must navigate rooms, collect keys, open doors, avoid hazards, and follow long precise action sequences to obtain sparse rewards.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Locked-door mechanics: progression depends on collecting keys (the Phase-1 domain-knowledge cell representation explicitly tracks which rooms keys were found in), conditional access between rooms based on keys/previous actions.",
            "graph_connectivity": "Sparse, large state/room-graph with constrained connections (doors/keys) and many long narrow paths; connectivity is not dense and often requires long precise sequences to traverse between distant reward-bearing states.",
            "environment_size": "Reported: Phase-1 with domain knowledge discovered mean 238 rooms (CI: 231-245); without domain knowledge Phase-1 visited mean 35 rooms (CI: 33-37) after 1.2B game frames. Environment has multiple levels; robustified policies solved up to hundreds/thousands of levels in evaluation.",
            "agent_name": "Go-Explore (Phase 1: archive + open-loop action replay + random exploration; Phase 2: robustification via Backward Algorithm + PPO)",
            "agent_description": "Phase 1 builds an archive of 'cells' (conflated states) and stores trajectories/environment states; returning to a chosen cell is done by replaying saved action sequences or emulator states (open-loop), then exploring randomly for k frames to discover new cells. Phase 2 uses imitation learning (Backward Algorithm) and PPO to produce a closed-loop recurrent policy robust to stochasticity.",
            "exploration_efficiency_metric": "Number of rooms found / number of cells discovered; frames to solve level; cumulative game score; sample complexity (game frames) to reach specific achievements (e.g., solve level 1 or level 3).",
            "exploration_efficiency_value": "Phase-1 (no domain knowledge): mean score 57,439 (CI: 47,843-67,224); solved level 1 in 57% of runs after 1.2B game frames (level-1 mean solve time 640M frames). Phase-1 (with domain knowledge): mean score 148,220 (CI: 144,580-151,730); solved level 1 in mean 57.6M frames and level 3 in mean 173M frames. Phase-2 robustified (with domain knowledge): mean score 666,474 (CI: 461,016-915,557); best run 18,003,200.",
            "success_rate": "Phase-1: 57% runs solved level 1 (no-domain-knowledge, after 1.2B frames). Phase-2 robustified policies: all robustified policies solved level 1 with ~99.8% success under stochastic evaluation; other success statistics vary by run.",
            "optimal_policy_type": "A memory/archival-driven exploration plus closed-loop robust policy: (1) use an archive of remembered states (cells) and deterministic returns (open-loop) to explore deeply, then (2) robustify into a closed-loop recurrent policy (PPO) that is robust to stochasticity; effective policies require memory/history when partial observability exists.",
            "topology_performance_relationship": "Explicitly reported: incorporating domain knowledge that preserves room/level identity and key collection (i.e., reducing harmful state conflation) dramatically improves exploration efficiency and final performance (many more rooms visited, faster solving of levels, much higher scores). Partial observability or overly aggressive conflation (downsampled image cells) impairs exploration because different meaningful states appear similar, preventing reaching reward states. Dead-ends and inefficient detours (traversing dead-ends then returning) are observed as inefficiencies that robustification tends to remove due to RL discounting; deterministic training can encourage brittle short paths that fail under stochasticity (the 'busy-highway' problem). Returning-to-cells (reducing derailment) and remembering cells (reducing detachment) are key structural relations between topology and performance.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper compares two effective 'topologies' induced by cell representations: a domain-knowledge-aware conflation (room number, discretized agent x,y, level, keys) versus a domain-agnostic aggressive pixel downsampling. The domain-knowledge representation yields far better coverage (238 vs ~35 rooms), faster solving times (e.g., level-1 solve in ~57.6M frames vs ~640M), and much higher scores, showing that preserving structurally meaningful graph distinctions (rooms, keys) improves navigation/exploration performance dramatically.",
            "policy_structure_findings": "Policies that separate exploration from robust control perform best: Phase-1 open-loop archival trajectories + targeted exploration discover deep-reaching trajectories; Phase-2 closed-loop RL with recurrence is required to be robust under stochasticity. Memory of previous states (archive) and the ability to deterministically return to states (or train goal-conditioned return policies) are crucial. Robustification tends to remove inefficient behaviors like visiting dead-ends (due to reward discounting), and partial-observability demands recurrent/goal-conditioned architectures; open-loop replay suffices only in deterministic training.",
            "uuid": "e1365.0",
            "source_info": {
                "paper_title": "Go-Explore: a New Approach for Hard-Exploration Problems",
                "publication_date_yy_mm": "2019-01"
            }
        },
        {
            "name_short": "Pitfall (Atari)",
            "name_full": "Pitfall! (Atari 2600 benchmark game)",
            "brief_description": "A highly deceptive, partially-observable Atari platformer with very sparse positive rewards and many small negative-reward actions; success requires long sequences of precise navigation across 255 rooms.",
            "citation_title": "Go-Explore: a New Approach for Hard-Exploration Problems",
            "mention_or_use": "use",
            "environment_name": "Pitfall",
            "environment_description": "A 2D platformer organized into many rooms (255 reported rooms) where positive rewards are sparse and many actions yield small negative rewards; partial observability leads visually-similar observations to correspond to different underlying states requiring different actions.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Large, sparse room graph with many visually ambiguous states (partial observability); many transitions require remembering prior context because identical-looking frames can map to different nodes in the true underlying state graph.",
            "environment_size": "255 rooms (full game); Phase-1 with domain knowledge visited all 255 rooms; Phase-1 without domain knowledge found ~22 rooms (with a finer downscaling up to ~30 rooms in preliminary experiments).",
            "agent_name": "Go-Explore (Phase 1: archive + random exploration; Phase 2: Backward Algorithm + PPO robustification of truncated demonstrations)",
            "agent_description": "Phase 1 used either an image-downsampled cell representation (domain-agnostic) or a domain-knowledge cell representation tracking room number and agent x,y; Phase 1 stored trajectories and discovered high-scoring trajectories. Because of partial observability and long trajectories, successful robustification used truncated shorter demonstrations; Phase 2 trained recurrent PPO policies via the Backward Algorithm to be robust under stochastic evaluation.",
            "exploration_efficiency_metric": "Number of rooms visited/cells discovered; cumulative game score; frames to discover rewards; sample complexity (game frames) for robustification.",
            "exploration_efficiency_value": "Phase-1 (no domain knowledge): ~22 rooms found, no positive rewards found. Phase-1 (with domain knowledge): visited all 255 rooms and found trajectories with mean score 70,264 (CI: 67,287-73,150). Phase-2 robustified agents (from truncated demonstrations) achieved mean score 59,494 (CI: 49,042-72,721) after mean 8.20B game frames to robustify.",
            "success_rate": null,
            "optimal_policy_type": "A memory-capable closed-loop policy trained via imitation-augmented RL (Backward Algorithm + PPO with recurrence) that can generalize beyond demonstrations; short robustifiable demonstration segments were needed because full-length brittle demonstrations failed to robustify.",
            "topology_performance_relationship": "Partial observability and excessive conflation in the cell representation (downsampled pixels) led Phase-1 to fail to find rewards: visually-identical frames correspond to different logical states, breaking pixel-based cell conflation. Introducing domain knowledge that disambiguates the true room identity (room number) allowed full coverage (255 rooms) and successful discovery of reward-bearing trajectories. Thus, preserving graph distinctions (nodes that are meaningfully different despite pixel similarity) is crucial for exploration success. Long brittle trajectories discovered under deterministic Phase-1 can be hard to robustify if they require remembering long histories that the robustifier does not get as input.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Comparison between domain-agnostic downsampled pixel cells and domain-knowledge cells: pixel-based conflation caused partial observability failures (no positive rewards found), while domain-knowledge cells that retained room identity enabled visiting all rooms and finding high-scoring trajectories; demonstrates that the effective induced graph (via cell representation) determines whether exploration can reach reward nodes.",
            "policy_structure_findings": "Because states are visually ambiguous, successful robust policies require memory/recurrence and/or architectures that can infer and store past context; open-loop action replay used in Phase-1 cannot produce robust closed-loop behavior in the stochastic test environment. Shorter demonstration fragments (ending right after rewards) were more amenable to robustification, suggesting policies that can learn local subgoals and shorter segments generalize better than single very-long brittle trajectories.",
            "uuid": "e1365.1",
            "source_info": {
                "paper_title": "Go-Explore: a New Approach for Hard-Exploration Problems",
                "publication_date_yy_mm": "2019-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Bandit based Monte-Carlo Planning",
            "rating": 2
        },
        {
            "paper_title": "Randomized Kinodynamic Planning",
            "rating": 1
        }
    ],
    "cost": 0.014456,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Go-Explore: a New Approach for Hard-Exploration Problems</h1>
<p>Adrien Ecoffet Joost Huizinga Joel Lehman Kenneth O. Stanley<em> Jeff Clune</em><br>Uber AI Labs<br>San Francisco, CA 94103<br>adrienecoffet, joost.hui, jclune@gmail.com<br>*Co-senior authors</p>
<p>Authors' note: We recommend reading (and citing) our updated paper, "First return, then explore":
Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K.O. and Clune, J. First return, then explore. Nature 590, 580-586 (2021). https://doi.org/10.1038/s41586-020-03157-9
It can be found at https://tinyurl.com/Go-Explore-Nature.</p>
<h4>Abstract</h4>
<p>A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to encourage exploration and improve performance on hardexploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember states that have previously been visited, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through exploiting any available means (including by introducing determinism), then robustify (create a policy that can reliably perform the solution) via imitation learning. The combined effect of these principles generates dramatic performance improvements on hardexploration problems. On Montezuma's Revenge, without being provided any domain knowledge, Go-Explore scores over 43,000 points, almost 4 times the previous state of the art. Go-Explore can also easily harness human-provided domain knowledge, and when augmented with it Go-Explore scores a mean of over 650,000 points on Montezuma's Revenge. Its max performance of 18 million surpasses the human world record by an order of magnitude, thus meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean performance of almost 60,000 points also exceeds expert human performance. Because GoExplore can produce many high-performing demonstrations automatically and cheaply, it also outperforms previous imitation learning work in which the solution was provided in the form of a human demonstration. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in a variety of domains, especially the many that often harness a simulator during training (e.g. robotics).</p>
<h2>1 Introduction</h2>
<p>Reinforcement learning (RL) has experienced significant progress in recent years, achieving superhuman performance in board games such as Go [1, 2] and in classic video games such as Atari [3]. However, this progress obscures some of the deep unmet challenges in scaling RL to complex</p>
<p>real-world domains. In particular, many important tasks require effective exploration to be solved, i.e. to explore and learn about the world even when rewards are sparse or deceptive. In sparse-reward problems, precise sequences of many (e.g. hundreds or more) actions must be taken between obtaining rewards. Deceptive-reward problems are even harder, because instead of feedback rarely being provided, the reward function actually provides misleading feedback for reaching the overall global objective, which can lead to getting stuck on local optima. Both sparse and deceptive reward problems constitute "hard-exploration" problems, and classic RL algorithms perform poorly on them [4]. Unfortunately, most challenging real-world problems are also hard-exploration problems. That is because we often desire to provide abstract goals (e.g. "find survivors and tell us their location," or "turn off the valve to the leaking pipe in the reactor"), and such reward functions do not provide detailed guidance on how to solve the problem (sparsity) while also often creating unintended local optima (deception) [5-8].
For example, in the case of finding survivors in a disaster area, survivors will be few and far between, thus introducing sparsity. Even worse, if we also instruct the robot to minimize damage to itself, this additional reward signal may actively teach the robot not to explore the environment, because exploration is initially much more likely to result in damage than it is to result in finding a survivor. This seemingly sensible additional objective thus introduces deception on top of the already sparse reward problem.
To address these challenges, this paper introduces Go-Explore, a new algorithm for hard-exploration problems that dramatically improves state-of-the-art performance in two classic hard-exploration benchmarks: the Atari games Montezuma's Revenge and Pitfall.
Prior to Go-Explore, the typical approach to sparse reward problems has been intrinsic motivation (IM) [4, 9-11], which supplies the RL agent with intrinsic rewards (IRs) that encourage exploration (augmenting or replacing extrinsic reward that comes from the environment). IM is often motivated by psychological concepts such as curiosity [12, 13] or novelty-seeking [7, 14], which play a role in how humans explore and learn. While IM has produced exciting progress in sparse reward problems, in many domains IM approaches are still far from fully solving the problem, including on Montezuma's Revenge and Pitfall. We hypothesize that, amongst other issues, such failures stem from two root causes that we call detachment and derailment.
Detachment is the idea that an agent driven by IM could become detached from the frontiers of high intrinsic reward (IR). To understand detachment, we must first consider that intrinsic reward is nearly always a consumable resource: a curious agent is curious about states to the extent that it has not often visited them (similar arguments apply for surprise, novelty, or prediction-error seeking agents [4, 14-16]). If an agent discovers multiple areas of the state space that produce high IR, its policy may in the short term focus on one such area. After exhausting some of the IR offered by that area, the policy may by chance begin consuming IR in another area. Once it has exhausted that IR, it is difficult for it to rediscover the frontier it detached from in the initial area, because it has already consumed the IR that led to that frontier (Fig. 1), and it likely will not remember how to return to that frontier due to catastrophic forgetting [17-20]. Each time this process occurs, a potential avenue of exploration can be lost, or at least be difficult to rediscover. In the worst case, there may be a dearth of remaining IR near the areas of state space visited by the current policy (even though much IR might remain elsewhere), and therefore no learning signal remains to guide the agent to further explore in an effective and informed way. One could slowly add intrinsic rewards back over time, but then the entire fruitless process could repeat indefinitely. In theory a replay buffer could prevent detachment, but in practice it would have to be large to prevent data about the abandoned frontier to not be purged before it becomes needed, and large replay buffers introduce their own optimization stability difficulties [21, 22]. The Go-Explore algorithm addresses detachment by explicitly storing an archive of promising states visited so that they can then be revisited and explored from later.
Derailment can occur when an agent has discovered a promising state and it would be beneficial to return to that state and explore from it. Typical RL algorithms attempt to enact such desirable behavior by running the policy that led to the initial state again, but with some stochastic perturbations to the existing policy mixed in to encourage a slightly different behavior (e.g. exploring further). The stochastic perturbation is performed because IM agents have two layers of exploration mechanisms: (1) the higher-level IR incentive that rewards when new states are reached, and (2) a more basic exploratory mechanism such as epsilon-greedy exploration, action-space noise, or parameter-space noise [23-25]. Importantly, IM agents rely on the latter mechanism to discover states containing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A hypothetical example of detachment in intrinsic motivation (IM) algorithms. Green areas indicate intrinsic reward, white indicates areas where no intrinsic reward remains, and purple areas indicate where the algorithm is currently exploring. (1) The agent starts each episode between the two mazes. (2) It may by chance start exploring the West maze and IM may drive it to learn to traverse, say, $50 \%$ of it. (3) Because current algorithms sprinkle in randomness (either in actions or parameters) to try to produce new behaviors to find explicit or intrinsic rewards, by chance the agent may at some point begin exploring the East maze, where it will also encounter a lot of intrinsic reward. After completely exploring the East maze, it has no explicit memory of the promising exploration frontier it abandoned in the West maze. It likely would also have no implicit memory of this frontier due to the problem of catastrophic forgetting [17-20]. (4) Worse, the path leading to the frontier in the West maze has already been explored, so no (or little) intrinsic motivation remains to rediscover it. We thus say the algorithm has detached from a frontier of states that provide intrinsic motivation. As a result, exploration can stall when areas close to where the current agent visits have already been explored. This problem would be remedied if the agent remembered and returned to previously discovered promising areas for exploration, which Go-Explore does.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A high-level overview of the Go-Explore algorithm.
high IR, and the former mechanism to return to them. However, the longer, more complex, and more precise a sequence of actions needs to be in order to reach a previously-discovered high-IR state, the more likely it is that such stochastic perturbations will "derail" the agent from ever returning to that state. That is because the needed precise actions are naively perturbed by the basic exploration mechanism, causing the agent to only rarely succeed in reaching the known state to which it is drawn, and from which further exploration might be most effective. To address derailment, an insight in Go-Explore is that effective exploration can be decomposed into first returning to a promising state (without intentionally adding any exploration) before then exploring further.
Go-Explore is an explicit response to both detachment and derailment that is also designed to achieve robust solutions in stochastic environments. The version presented here works in two phases (Fig. 2): (1) first solve the problem in a way that may be brittle, such as solving a deterministic version of the problem (i.e. discover how to solve the problem at all), and (2) then robustify (i.e. train to be able to</p>
<p>reliably perform the solution in the presence of stochasticity). ${ }^{1}$ Similar to IM algorithms, Phase 1 focuses on exploring infrequently visited states, which forms the basis for dealing with sparse-reward and deceptive problems. In contrast to IM algorithms, Phase 1 addresses detachment and derailment by accumulating an archive of states and ways to reach them through two strategies: (a) add all interestingly different states visited so far into the archive, and (b) each time a state from the archive is selected to explore from, first Go back to that state (without adding exploration), and then Explore further from that state in search of new states (hence the name "Go-Explore").
An analogy of searching a house can help one contrast IM algorithms and Phase 1 of Go-Explore. IM algorithms are akin to searching through a house with a flashlight, which casts a narrow beam of exploration first in one area of the house, then another, and another, and so on, with the light being drawn towards areas of intrinsic motivation at the edge of its small visible region. It can get lost if at any point the beam fails to fall on any area with intrinsic motivation remaining. Go-Explore more resembles turning the lights on in one room of a house, then its adjacent rooms, then their adjacent rooms, etc., until the entire house is illuminated. Go-Explore thus gradually expands its sphere of knowledge in all directions simultaneously until a solution is discovered.
If necessary, the second phase of Go-Explore robustifies high-performing trajectories from the archive such that they are robust to the stochastic dynamics of the true environment. Go-Explore robustifies via imitation learning (aka learning from demonstrations or LfD [26-29]), a technique that learns how to solve a task from human demonstrations. The only difference with Go-Explore is that the solution demonstrations are produced automatically by Phase 1 of Go-Explore instead of being provided by humans. The input to this phase is one or more high-performing trajectories, and the output is a robust policy able to consistently achieve similar performance. The combination of both phases instantiates a powerful algorithm for hard-exploration problems, able to deeply explore sparse- and deceptive-reward environments and robustify high-performing trajectories into reliable solutions that perform well in the unmodified, stochastic test environment.
Some of these ideas are similar to ideas proposed in related work. Those connections are discussed in Section 5. That said, we believe we are the first to combine these ideas in this way and demonstrate that doing so provides substantial performance improvements on hard-exploration problems.
To explore its potential, we test Go-Explore on two hard-exploration benchmarks from the Arcade Learning Environment (ALE) [30, 31]: Montezuma's Revenge and Pitfall. Montezuma's Revenge has become an important benchmark for exploration algorithms (including intrinsic motivation algorithms) [4, 16, 32-39] because precise sequences of hundreds of actions must be taken in between receiving rewards. Pitfall is even harder because its rewards are sparser (only 32 positive rewards are scattered over 255 rooms) and because many actions yield small negative rewards that dissuade RL algorithms from exploring the environment.
Classic RL algorithms (i.e. those without intrinsic motivation) such as DQN [3], A3C [40], ApeX [41] and IMPALA [42] perform poorly on these domains even with up to 22 billion game frames of experience, scoring 2,500 or lower on Montezuma's Revenge and failing to solve level one, and scoring $\leq 0$ on Pitfall. Those results exclude experiments that are evaluated in a deterministic test environment [43, 44] or were given human demonstrations [26, 27, 45]. On Pitfall, the lack of positive rewards and frequent negative rewards causes RL algorithms to learn a policy that effectively does nothing, either standing completely still or moving back and forth near the start of the game (https://youtu.be/Z0lYamtgdqQ [46]).
These two games are also tremendously difficult for planning algorithms, even when allowed to plan directly within the game emulator. Classical planning algorithms such as UCT [47-49] (a powerful form of Monte Carlo tree search [49, 50]) obtain 0 points on Montezuma's Revenge because the state space is too large to explore effectively, even with probabilistic methods [30, 51].
Despite being specifically designed to tackle sparse reward problems and being the dominant method for them, IM algorithms also struggle with Montezuma's Revenge and Pitfall, although they perform better than algorithms without IM. On Montezuma's Revenge, the best such algorithms thus far average around 11,500 with a maximum of $17,500[16,39]$. One solved level 1 of the game in $10 \%$ of its runs [16]. Even with IM, no algorithm scores greater than 0 on Pitfall (in a stochastic test</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>environment, without a human demonstration). We hypothesize that detachment and derailment are major reasons why IM algorithms do not perform better.
When exploiting easy-to-provide domain knowledge, Go-Explore on Montezuma's Revenge scores a mean of 666,474, and its best run scores over 18 million and solves 1,441 levels. On Pitfall, Go-Explore scores a mean of 59,494 and a maximum of 107,363 , which is close to the maximum of the game of 112,000 points. Without exploiting domain knowledge, Go-Explore still scores a mean of 43,763 on Montezuma's Revenge. All scores are dramatic improvements over the previous state of the art. This and all other claims about solving the game and producing state-of-the-art scores assume that, while stochasticity is required during testing, deterministic training is allowable (discussed in Section 2.1.3). We conclude that Go-Explore is a promising new algorithm for solving hard-exploration RL tasks with sparse and/or deceptive rewards.</p>
<h1>2 The Go-Explore Algorithm</h1>
<p>The insight that remembering and returning reliably to promising states is fundamental to effective exploration in sparse-reward problems is at the core of Go-Explore. Because this insight is so flexible and can be exploited in different ways, Go-Explore effectively encompasses a family of algorithms built around this key idea. The variant implemented for the experiments in this paper and described in detail in this section relies on two distinct phases. While it provides a canonical demonstration of the possibilities opened up by Go-Explore, other variants are also discussed (e.g. in Section 4) to provide a broader compass for future applications.</p>
<h3>2.1 Phase 1: Explore until solved</h3>
<p>In the two-phase variant of Go-Explore presented in this paper, the purpose of Phase 1 is to explore the state space and find one or more high-performing trajectories that can later be turned into a robust policy in Phase 2. To do so, Phase 1 builds up an archive of interestingly different game states, which we call "cells" (Section 2.1.1), and trajectories that lead to them. It starts with an archive that only contains the starting state. From there, it builds the archive by repeating the following procedures: choose a cell from the current archive (Section 2.1.2), return to that cell without adding any stochastic exploration (Section 2.1.3), and then explore from that location stochastically (Section 2.1.4). During this process, any newly encountered cells (as well as how to reach them) or improved trajectories to existing cells are added to the archive (Section 2.1.5).</p>
<h3>2.1.1 Cell representations</h3>
<p>One could, in theory, run Go-Explore directly in a high-dimensional state space (wherein each cell contains exactly one state); however doing so would be intractable in practice. To be tractable in high-dimensional state spaces like Atari, Phase 1 of Go-Explore needs a lower-dimensional space within which to search (although the final policy will still play in the same original state space, in this case pixels). Thus, the cell representation should conflate similar states while not conflating states that are meaningfully different.
In this way, a good cell representation should reduce the dimensionality of the observations into a meaningful low-dimensional space. A rich literature investigates how to obtain good representations from pixels. One option is to take latent codes from the middle of neural networks trained with traditional RL algorithms maximizing extrinsic and/or intrinsic motivation, optionally adding auxiliary tasks such as predicting rewards [52]. Additional options include unsupervised techniques such as networks that autoencode [53] or predict future states, and other auxiliary tasks such as pixel control [54].
While it will be interesting to test any or all of these techniques with Go-Explore in future work, for these initial experiments with Go-Explore we test its performance with two different representations: a simple one that does not harness game-specific domain knowledge, and one that does exploit easy-to-provide domain knowledge.</p>
<h2>Cell representations without domain knowledge</h2>
<p>We found that a very simple dimensionality reduction procedure produces surprisingly good results on Montezuma's Revenge. The main idea is simply to downsample the current game frame. Specifically,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example cell representation without domain knowledge, which is simply to downsample each game frame. The full observable state, a color image, is converted to grayscale and downscaled to an $11 \times 8$ image with 8 possible pixel intensities.</p>
<p>we (1) convert each game frame image to grayscale (2) downscale it to an $11 \times 8$ image with area interpolation (i.e. using the average pixel value in the area of the downsampled pixel), (3) rescale pixel intensities so that they are integers between 0 and 8 , instead of the original 0 to 255 (Fig. 3). The downscaling dimensions and pixel-intensity range were found by grid search. The aggressive downscaling used by this representation is reminiscent of the Basic feature set from Bellemare et al. [30]. This cell representation requires no game-specific knowledge and is fast to compute.</p>
<h1>Cell representations with domain knowledge</h1>
<p>The ability of an algorithm to integrate easy-to-provide domain knowledge can be an important asset. In Montezuma's Revenge, domain knowledge is provided as unique combinations of the $x, y$ position of the agent (discretized into a grid in which each cell is $16 \times 16$ pixels), room number, level number, and in which rooms the currently-held keys were found. In the case of Pitfall, only the $x, y$ position of the agent and the room number were used. All this information was extracted directly from pixels with simple hand-coded classifiers to detect objects such as the main character's location combined with our knowledge of the map structure in the two games (Appendix A.3). While Go-Explore provides the opportunity to leverage domain knowledge in the cell representation in Phase 1, the robustified neural network produced by Phase 2 still plays directly from pixels only.</p>
<h3>2.1.2 Selecting cells</h3>
<p>In each iteration of Phase 1, a cell is chosen from the archive to explore from. This choice could be made uniformly at random, but we can improve upon that baseline in many cases by creating (or learning) a heuristic for preferring some cells over others. In preliminary experiments, we found that such a heuristic can improve performance over uniform random sampling (data not shown). The exact heuristic differs depending on the problem being solved, but at a high level, the heuristics in our work assign a positive weight to each cell that is higher for cells that are deemed more promising. For example, cells might be preferred because they have not been visited often, have recently contributed to discovering a new cell, or are expected to be near undiscovered cells. The weights of all cells are normalized to represent the probability of each cell being chosen next. No cell is ever given a weight equal to 0 , so that all cells in principle remain available for further exploration. The exact heuristics from our experiments are described in Appendix A.5.</p>
<h3>2.1.3 Returning to cells and opportunities to exploit deterministic simulators</h3>
<p>One of the main principles of Go-Explore is to return to a promising cell without added exploration before exploring from that cell. The Go-Explore philosophy is that we should make returning to that cell as easy as possible given the constraints of the problem. The easiest way to return to a cell is if the world is deterministic and resettable, such that one can reset the state of the simulator to a previous visit to that cell. Whether performing such resets is allowable for RL research is an interesting subject of debate that was motivated by the initial announcement of Go-Explore [55]. The ability to harness determinism and perform such resets forces us to recognize that there are two different types of problems we wish to solve with RL algorithms: those that require stochasticity at test time only, and those that require stochasticity during both testing and training.
We start with the former. Because current RL algorithms can take unsafe actions [56, 57] and require tremendous amounts of experience to learn [41, 42, 58], the majority of applications of RL in the</p>
<p>foreseeable future will likely require training in a simulator before being transferred to (and optionally fine-tuned in) the real world. For example, most work with learning algorithms for robotics train in a simulator before transferring the solution to the real world; that is because learning directly on the robot is slow, sample-inefficient, can damage the robot, and can be unsafe [59-61]. Fortunately, for many domains, simulators are available (e.g. robotics simulators, traffic simulators, etc.). An insight of Go-Explore is that we can take advantage of the fact that such simulators can be made deterministic to improve performance, especially on hard-exploration problems. For many types of problems, we want a reliable final solution (e.g. a robot that reliably finds survivors after a natural disaster) and there is no principled reason to care whether we obtain this solution via initially deterministic training. If we can solve previously unsolvable problems, including ones that are stochastic at evaluation (test) time, via making simulators deterministic, we should take advantage of this opportunity.
There are also cases where a simulator is not available and where learning algorithms must confront stochasticity during training. To create and test algorithms for this second type of problem, we cannot exploit determinism and resettability. Examples of this class of problems include when we must learn directly in the real world (and an effective simulator is not available and cannot be learned), or when studying the learning of biological animals, including ourselves. We believe Go-Explore can handle such situations by training goal-conditioned policies [62, 63] that reliably return to cells in the archive during the exploration phase, which is an interesting area for future research. While computationally much more expensive, this strategy would result in a fully trained policy at the end of the exploration phase, meaning there would be no need for a robustification phase at the end. We note that there are some problems where the environment has forms of stochasticity that prevent the algorithm from reliably returning to a particular cell, regardless of which action the agent takes (e.g. in poker, there is no sequence of actions that reliably leads you to a state where you have two aces). We leave a discussion and study of whether Go-Explore helps in that problem setting for future work.
With this distinction in mind, we can now ask whether Montezuma's Revenge and Pitfall represent the first type of domain (where all we care about is a solution that is robust to stochasticity at test time) or the second (situations where the algorithm must handle stochasticity while training). We believe few people in the community had considered this question before our initial blog post on Go-Explore [55] and that it created a healthy debate on this subject. Because Atari games are proxies for the problems we want to solve with RL, and because both types of problems exist, a natural conclusion is that we should have benchmarks for each. One version of a task can require stochasticity during testing only, and another can require stochasticity during both training and testing. All results and claims in this version of this paper are for the version of these domains that does not require stochasticity during training (i.e. stochasticity is required during evaluation only). Applying Go-Explore when training is stochastic remains an exciting avenue of research for the near future.
For problems in which all we care about is a reliable policy at test time, a key insight behind Go-Explore is that we can first solve the problem (Phase 1), and then (if necessary) deal with making the solution more robust later (Phase 2). In contrast with the usual view of determinism as a stumbling block to producing agents that are robust and high-performing, it can be made an ally during exploration and then the solution extended to nondeterminism afterwards via robustification. An important domain where such insights can help is robotics, where training is often done in simulation before policies are transferred to the real world [59-61].
For the experiments in this paper, because we harness deterministic training, we could return to a cell by storing the sequence of actions that lead to it and subsequently replay those actions. However, simply saving the state of the emulator (in addition to this sequence of steps) and restoring that state when revisiting a cell gains additional efficiency. Doing so reduced the number of steps that needed to be simulated by at least one order of magnitude (Appendix A.8).
Due to the fact that the present version of Go-Explore operates in a deterministic setting during Phase 1, each cell is associated with an open-loop sequence of instructions that lead to it given the initial state, not a proper policy that maps any state to an action. A true policy is produced during robustification in Phase 2 (Section 2.2).</p>
<h1>2.1.4 Exploration from cells</h1>
<p>Once a cell is reached, any exploration method can be applied to find new cells. In this work the agent explores by taking random actions for $k=100$ training frames, with a $95 \%$ probability of repeating the previous action at each training frame (frames at which the agent is allowed to take an action,</p>
<p>thus not including any frames skipped due to frame skip, see Appendix A.1). Besides reaching the $k=100$ training frame limit for exploration, exploration is also aborted at the episode's end (defined in Appendix A.2), and the action that led to the episode ending is ignored because it does not produce a destination cell.</p>
<p>Interestingly, such exploration does not require a neural network or other controller, and indeed no neural network was used for the exploration phase (Phase 1) in any of the experiments in this paper (we do not train a neural network until Phase 2). The fact that entirely random exploration works so well highlights the surprising power of simply returning to promising cells before exploring further, though we believe exploring intelligently (e.g. via a trained policy) would likely improve our results and is an interesting avenue for future work.</p>
<h1>2.1.5 Updating the archive</h1>
<p>While an agent is exploring from a cell, the archive updates in two conditions. The first condition is when the agent visits a cell that was not yet in the archive (which can happen multiple times while exploring from a given cell). In this case, that cell is added to the archive with four associated pieces of metadata: (1) how the agent got to that cell (here, a full trajectory from the starting state to that cell), (2) the state of the environment at the time of discovering the cell (if the environment supports such an operation, which is true for the two Atari-game domains in this paper), (3) the cumulative score of that trajectory, and (4) the length of that trajectory.
The second condition is when a newly-encountered trajectory is "better" than that belonging to a cell already in the archive. For the experiments below, we define a new trajectory as better than an existing trajectory when the new trajectory either has a higher cumulative score or when it is a shorter trajectory with the same score. In either case, the existing cell in the archive is updated with the new trajectory, the new trajectory length, the new environment state, and the new score. In addition, information affecting the likelihood of this cell being chosen (see Appendix A.5) is reset, including the total number of times the cell has been chosen and the number of times the cell has been chosen since leading to the discovery of another cell. Resetting these values is beneficial when cells conflate many different states because a new way of reaching a cell may actually be a more promising stepping stone to explore from (so we want to encourage its selection). We do not reset the counter that records the number of times the cell has been visited because that would make recently discovered cells indistinguishable from recently updated cells, and recently discovered cells (i.e. those with low visit counts) are more promising to explore because they are likely near the surface of our expanding sphere of knowledge.
Because cells conflate many states, we cannot assume that a trajectory from start state $A$ through cell $B$ to cell $C$ will still reach $C$ if we substitute a different, better way to get from $A$ to $B$; therefore, the better way of reaching a cell is not integrated into the trajectories of other cells that built upon the original trajectory. However, performing such substitutions might work with goal-conditioned or otherwise robust policies, and investigating that possibility is an interesting avenue for future work.</p>
<h3>2.1.6 Batch implementation</h3>
<p>We implemented Phase 1 in parallel to take advantage of multiple CPUs (our experiments ran on a single machine with 22 CPU cores): at each step, a batch of $b$ cells is selected (with replacement) according to the rules described in Section 2.1.2 and Appendix A.5, and exploration from each of these cells proceeds in parallel for each. Besides using the multiple CPUs to run more instances of the environment, a high $b$ also saves time by recomputing cell selection probabilities less frequently, which is important as this computation accounts for a significant portion of run time as the archive gets large (though this latter factor could be mitigated in other ways in the future). Because the size of $b$ also has an indirect effect on the exploration behavior of Go-Explore (for instance, the initial state is guaranteed to be chosen $b$ times at the very first iteration), it is in effect a hyperparameter, whose values are given in Appendix A.6.</p>
<h3>2.2 Phase 2: Robustification</h3>
<p>If successful, the result of Phase 1 is one or more high-performing trajectories. However, if Phase 1 of Go-Explore harnessed determinism in a simulator, such trajectories will not be robust to any stochasticity, which is present at test time. Phase 2 addresses this gap by creating a policy robust to</p>
<p>noise via imitation learning, also called learning from demonstration (LfD). Importantly, stochasticity is added during Phase 2 so that the final policy is robust to the stochasticity it will face during its evaluation in the test environment. Thus the policy being trained has to learn how to mimic and/or perform as well as the trajectory obtained from the Go-Explore exploration phase while simultaneously dealing with circumstances that were not present in the original trajectory. Depending on the stochasticity of the environment, this adjustment can be highly challenging, but nevertheless is far easier than attempting to solve a sparse-reward problem from scratch.</p>
<p>While most imitation learning algorithms could be used for Phase 2, different types of imitation learning algorithms can qualitatively affect the resulting policy. LfD algorithms that try to closely mimic the behavior of the demonstration may struggle to improve upon it. For this reason, we chose an LfD algorithm that has been shown capable of improving upon its demonstrations: the Backward Algorithm from Salimans and Chen [28]. It works by starting the agent near the last state in the trajectory, and then running an ordinary RL algorithm from there (in this case Proximal Policy Optimization (PPO) [64]). Once the algorithm has learned to obtain the same or a higher reward than the example trajectory from that starting place near the end of the trajectory, the algorithm backs the agent's starting point up to a slightly earlier place along the trajectory, and repeats the process until eventually the agent has learned to obtain a score greater than or equal to the example trajectory all the way from the initial state. Note that a similar algorithm was discovered independently at around the same time by Resnick et al. [65].</p>
<p>While this approach to robustification effectively treats the expert trajectory as a curriculum for the agent, the policy is only optimized to maximize its own score, and not actually forced to accurately mimic the trajectory. For this reason, this phase is able to further optimize the expert trajectories, as well as generalize beyond them, both of which we observed in practice in our experiments (Section 3). In addition to seeking a higher score than the original trajectory, because it is an RL algorithm with a discount factor that prizes near-term rewards more than those gathered later, it also has a pressure to improve the efficiency with which it collects rewards. Thus if the original trajectory contains unnecessary actions (like visiting a dead end and returning), such behavior could be eliminated during robustification (a phenomenon we also observed).</p>
<h1>2.3 Additional experimental and analysis details</h1>
<p>Comparing sample complexity for RL algorithms trained on Atari games can be tricky due to the common usage of frame skipping [31, 66], wherein a policy only sees and acts every $n$th (here, 4) frame, and that action is repeated for intervening frames to save the computation of running the policy. Specifically, it can be ambiguous whether the frames that are skipped are counted (which we call "game frames") or ignored (which we call "training frames") when discussing sample complexity. In this work, we always qualify the word "frame" accordingly and all numbers we report are measured in game frames. Appendix A. 1 further details the subtleties of this issue.</p>
<p>Because the Atari games are deterministic by default, some form of stochasticity needs to be introduced to provide a stochastic test environment, which is desirable to make Atari an informative test bed for RL algorithms. Following previous work, we introduce stochasticity into the Atari environment with two previously employed techniques: random no-ops and sticky actions.</p>
<p>Random no-ops means that the agent is forced to take up to 30 no-ops (do nothing commands) at the start of the game. Because most Atari games run on a timer that affects whether hazards are present or not, or where different hazards, items, or enemies are located, taking a random number of no-ops puts the world into a slightly different state each time, meaning that fixed trajectories (such as the ones found by Go-Explore Phase 1) will no longer work. Random no-ops were first introduced by Mnih et al. [3], and they were adopted as a primary source of stochasticity in most subsequent papers working in the Atari domain [3, 26, 27, 34, 38, 41, 42, 45, 67-73].</p>
<p>While random no-ops prevent single, memorized trajectories from solving Atari games, the remainder of the game remains deterministic, meaning there is still much determinism that can be exploited. While several other forms of stochasticity have been proposed (e.g. humans restarts [74], random frame skips [75], etc.), a particularly elegant form is sticky actions [31], where at each game frame there exists some probability of repeating the previous action instead of performing a newly chosen action. This way to introduce stochasticity is akin to how humans are not frame perfect, but may hold a button for slightly longer than they intended, or how they may be slightly late in pressing a button.</p>
<p>Because Atari games have been designed for human play, the addition of sticky actions generally does not prevent a game from being solvable, and it adds some stochasticity to every state in the game, not just the start. Although our initial blog post [55] only included random no-ops, in this paper our robustification and all post-robustification test results are produced with a combination of both random no-ops and sticky actions. All algorithms we compare against in Section 3 and in Appendix A. 9 likewise were tested with some form of stochasticity (in the form of no-ops, sticky actions, human starts, or some combination thereof), though it is worth noting that, unlike Go-Explore, most also had to handle stochasticity throughout training. Relevant algorithms that were tested in a deterministic environment are discussed in Section 5.</p>
<p>All hyperparameters were found by performing a separate grid-search for each experiment. The final, best performing hyperparameters are listed in Appendix A.6, tables 1 and 2. All confidence intervals given are $95 \%$ bootstrapped confidence intervals computed using the pivotal (also known as empirical) method [76], obtained by resampling 10,000 times. Confidence intervals are reported with the following notation: stat (CI: lower - upper) where stat is the statistic (a mean unless otherwise specified). In graphs containing shaded areas, those areas indicate the $95 \%$ percentile bootstrapped confidence interval of the mean, obtained by resampling 1,000 times. Graphs of the exploration phase (Phase 1) depict data at approximately every 4 M game frames and graphs of the robustification phase (Phase 2) depict data at approximately every 130,000 game frames.
Because the robustification process can diverge even after finding a solution, the neural network at the end of training does not necessarily perform well, even if a high-performing solution was found at some point during this process. To retrieve a neural network that performs well regardless of when it was found, all robustification runs (Phase 2) produced a checkpoint of the neural network approximately every 13 M game frames. Because the performance values recorded during robustification are noisy, we cannot select the best performing checkpoint from those performance values alone. As such, at the end of each robustification run, out of the checkpoints with the lowest max_starting_point (or close to it), a random subset of checkpoints (between 10 and 50) was tested to evaluate the performance of the neural network stored within that checkpoint. We test a random subset because robustification runs usually produce more successful checkpoints then we can realistically test. The highest-scoring checkpoint for each run was then re-tested to account for the selection bias inherent in selecting the best checkpoint. The scores from this final retest are the ones we report.
The neural network from each checkpoint is evaluated with random no-ops and sticky actions until at least 5 scores for each of the 31 possible starting no-ops (from 0 to 30 inclusive) are obtained. The mean score for each no-op is then calculated and the final score for the checkpoint is the grand mean of the individual no-op scores. Unless otherwise specified, the default time limit of 400,000 game frames imposed by OpenAI Gym [75] is enforced.</p>
<h1>3 Results</h1>
<h3>3.1 Montezuma's Revenge</h3>
<h3>3.1.1 Without domain knowledge in the cell representation</h3>
<p>In this first experiment, we run Go-Explore on Montezuma's Revenge with the downsampled image cell representation, which does not require game-specific domain knowledge. Despite the simplicity of this cell representation, Phase 1 of Go-Explore solves level 1 in $57 \%$ of runs after 1.2B game frames (a modest number by modern standards [41, 42]), with one of the 100 runs also solving level 2, and visits a mean of 35 rooms (CI: 33 - 37) (Fig. 4a). The number of new cells being discovered is still increasing linearly after 1.2B game frames, indicating that results would likely be even better were it run longer (Fig. 4b). Phase 1 of Go-Explore achieves a mean score of 57,439 (CI: 47,843 67,224 ) (Fig. 4c). Level 1 was solved after a mean of 640 M (CI: $567 \mathrm{M}-711 \mathrm{M}$ ) game frames, which took a mean of 10.8 (CI: $9.5-12.0$ ) hours on a single, 22-CPU machine (note that these level 1 numbers exclude the runs that never solved level 1 after 1.2B game frames). See Appendix A. 8 for more details on performance.
Amusingly, Go-Explore discovered a little-known bug in Montezuma's Revenge called the "treasure room curse" [77]. If the agent performs a specific sequence of actions, it can remain in the treasure room (the final room before being sent to the next level) indefinitely, instead of being automatically</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance of the exploration phase of Go-Explore with downscaled frames on Montezuma's Revenge. Lines indicating human and the algorithmic state of the art are for comparison, but recall that the Go-Explore scores in this plot are on a deterministic version of the game (unlike the post-Phase 2 scores presented in this section).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples of maximum starting point over training for robustifying using different numbers of demonstrations. Success is achieved as soon as any of the curves gets sufficiently close (e.g. within 50 units) to 0 , because that means the agent is able to perform as well as at least one of the demonstrations.
moved to the next level after some time. Because gems giving 1,000 points keep appearing in the treasure room, it is possible to easily achieve very high scores once it has been triggered. Finding bugs in games and simulators, as Go-Explore did, is an interesting reminder of the power and creativity of optimization algorithms [6], and is commercially valuable as a debugging tool to identify and fix such bugs before shipping simulators and video games. A video of the treasure room curse as triggered by Go-Explore is available at https://youtu.be/civ60OLoR-I.</p>
<p>In 51 out of the 57 runs that solved level 1, the highest-scoring trajectory found by Go-Explore exploited the bug. To prevent scores from being inflated due to this bug, we filtered out trajectories that triggered the treasure room curse bug when extracting the highest scoring trajectory from each run of Go-Explore for robustification (Appendix A. 4 provides details).</p>
<p>As mentioned in Section 2.2, we used Salimans \&amp; Chen's Backward Algorithm [28] for robustification. However, we found it somewhat unreliable in learning from a single demonstration (Fig. 5a). Indeed, only $40 \%$ of our attempts at robustifying trajectories that solved level 1 were successful when using a single demonstration.</p>
<p>However, because Go-Explore can produce many demonstrations, we modified the Backward Algorithm to simultaneously learn from multiple demonstrations (details in Appendix A.7). To simulate the use case in which Phase 1 is run repeatedly until enough successful demonstrations (in this case 10) are found, we extracted the highest scoring non-bug demonstration from each of the 57 out of</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: History of progress on Montezuma's Revenge vs. the version of Go-Explore that does not harness domain knowledge. Go-Explore significantly improves on the prior state of the art. These data are presented in tabular form in Appendix A.9.</p>
<p>100 Phase 1 runs that had solved level 1, and randomly assigned them to one of 5 non-overlapping groups of 10 demonstrations ( 7 demonstrations were left over and ignored), each of which was used for a robustification run. When training with 10 demonstration trajectories, all 5 robustification runs were successful. Fig. 5b shows an example of successful robustification with 10 trajectories.
In the end, our robustified policies achieve a mean score of 43,763 (CI: 36,718 - 50,196), substantially higher than the human expert mean of 34,900 [27]. All policies successfully solve level 1 (with a $99.8 \%$ success rate over different stochastic evaluations of the policies), and one of our 5 policies also solves level $2100 \%$ of the time. Fig. 6 shows how these results compare with prior work.
Surprisingly, the computational cost of Phase 2 is greater than that of Phase 1. These Phase 2 results were achieved after a mean of 4.35B (CI: $4.27 \mathrm{~B}-4.45 \mathrm{~B}$ ) game frames of training, which took a mean of 2.4 (CI: $2.4-2.5$ ) days of training (details in Appendix A.8).</p>
<h1>3.1.2 With domain knowledge in the cell representation</h1>
<p>On Montezuma's Revenge, when harnessing domain knowledge in its cell representation (Section 2.1.1), Phase 1 of Go-Explore finds a total of 238 (CI: 231 - 245) rooms, solves a mean of 9.1 (CI: $8.8-9.4$ ) levels (with every run solving at least 7 levels), and does so in roughly half as many game frames as with the downscaled image cell representation (Fig. 7a). Its scores are also extremely high, with a mean of 148,220 (CI: 144,580 - 151,730) (Fig. 7c). These results are averaged over 50 runs.</p>
<p>As with the downscaled version, Phase 1 of Go-Explore with domain knowledge was still discovering additional rooms, cells, and ever-higher scores linearly when it was stopped (Fig. 7). Indeed, because every level of Montezuma's Revenge past level 3 is nearly identical to level 3 (except for the scores on the screen and the stochastic timing of events) and because each run had already passed level 3, it would likely continue to find new rooms, cells, and higher scores forever.
Domain knowledge runs spend less time exploiting the treasure room bug because we preferentially select cells in the highest level reached so far (Appendix A.5). Doing so encourages exploring new levels instead of exploring the treasure rooms on previous levels to keep exploiting the treasure room bug. The highest final scores thus come from trajectories that solved many levels. Because knowing the level number constitutes domain knowledge, non-domain knowledge runs cannot take advantage of this information and are thus affected by the bug more.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Performance on Montezuma's Revenge of Phase 1 of Go-Explore with and without domain knowledge. The algorithm finds more rooms, cells, and higher scores with the easily provided domain knowledge, and does so with a better sample complexity. For (b), we plot the number of cells found in the no-domain-knowledge runs according to the more intelligent cell representation from the domain-knowledge run to allow for an equal comparison.</p>
<p>In terms of computational performance, Phase 1 with domain knowledge solves the first level after a mean of only 57.6 M (CI: $52.7 \mathrm{M}-62.3 \mathrm{M}$ ) game frames, corresponding to 0.9 (CI: $0.8-1.0$ ) hours on a single 22-CPU machine. Solving level 3, which effectively means solving the entire game as discussed above, is accomplished in a mean of 173 M (CI: $164 \mathrm{M}-182 \mathrm{M}$ ) game frames, corresponding to 6.8 (CI: $6.2-7.3$ ) hours. Appendix A. 8 provides full performance details.</p>
<p>For robustification, we chose trajectories that solve level 3, truncated to the exact point at which level 3 is solved because, as mentioned earlier, all levels beyond level 3 are nearly identical aside from the pixels that display the score, which of course keep changing, and some global counters that change the timing of aspects of the game like when laser beams turn on and off.
We performed 5 robustification runs with demonstrations from the Phase 1 experiments above, each of which had a demonstration from each of 10 different Phase 1 runs. All 5 runs succeeded. The resulting mean score is 666,474 (CI: $461,016-915,557$ ), far above both the prior state of the art and the non-domain knowledge version of Go-Explore. As with the downscaled frame version, Phase 2 was slower than Phase 1, taking a mean of 4.59 B (CI: $3.09 \mathrm{~B}-5.91 \mathrm{~B}$ ) game frames, corresponding to a mean of 2.6 (CI: $1.8-3.3$ ) days of training.
The networks show substantial evidence of generalization to the minor changes in the game beyond level 3: although the trajectories they were trained on only solve level 3, these networks solved a mean of 49.7 levels (CI: $32.6-68.8$ ). In many cases, the agents did not die, but were stopped by the maximum limit of 400,000 game frames imposed by default in OpenAI Gym [75]. Removing this limit altogether, our best single run from a robustified agent achieved a score of 18,003,200 and solved 1,441 levels during 6,198,985 game frames, corresponding to 28.7 hours of game play (at 60 game frames per second, Atari's original speed) before losing all its lives. This score is over an order of magnitude higher than the human world record of 1,219,200 [78], thus achieving the strictest definition of "superhuman" performance. A video of the agent solving the first ten levels can be seen here: https://youtu.be/gnGyUPd_4Eo.
Fig. 8 compares the performance of Go-Explore to historical results (including the previous state of the art), the no-domain-knowledge version of Go-Explore, and previous imitation learning work that relied on human demonstrations to solve the game. The version of Go-Explore that harnesses domain knowledge dramatically outperforms them all. Specifically, Go-Explore produces scores over 9 times greater than those reported for imitation learning from human demonstrations [28] and over 55 times the score reported for the prior state of the art without human demonstrations [39].
That Go-Explore outperforms imitation learning plus human demonstrations is particularly noteworthy, as human-provided solutions are arguably a much stronger form of domain knowledge than that provided to Go-Explore. We believe that this result is due to the higher quality of demonstrations that Go-Explore was able to produce for Montezuma's Revenge vs. those provided by humans in the previous imitation learning work. The demonstrations used in our work range in score from 35,200 to 51,900 (lower than the final mean score of 148,220 for Phase 1 because these demonstrations are</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Historical progress on Montezuma's Revenge vs. the version of Go-Explore that harnesses domain knowledge. With domain knowledge, Go-Explore dramatically outperforms prior work, the no-domain-knowledge version of Go-Explore, and even prior work with imitation learning that was provided the solution in the form of human demonstrations. The data are presented in tabular form in Appendix A.9.
limited to only solving up to level 3) and most importantly, they all solve level 3. The demonstration originally used with the Backward Algorithm [28] reaches a score of 71,500 but doesn't solve level 3, thus preventing it from generalizing to further levels. The demonstrations used in DQfD and Ape-X DQfD [26, 27] only range in score from 32,300 to 34,900. In this last case, it is not clear whether level 3 was solved in any of the demonstrations, but we believe this is unlikely given the reported scores because they are lower than the lowest level-3-solving scores found by Go-Explore and given the fact that the human demonstration used by the Backward Algorithm scored twice as high without solving level 3.
One interesting benefit of a robustification phase with an imitation learning algorithm that does not try to mimic the original demonstration is that it can improve upon that demonstration. Because of the discount on future rewards that exists in the base RL algorithm PPO, there is a pressure to remove inefficiencies in the demonstration. Videos of Go-Explore policies reveal efficient movements. In contrast, IM algorithms specifically reward reaching novel states, meaning that policies produced by them often do seemingly inefficient things like deviating to explore dead ends or jumping often to touch states only accessible by jumping, even though doing so is not necessary to gain real reward. An example of a Deep Curiosity Search agent [37] performing such inefficient jumps can be viewed at https://youtu.be/-Fy2va3IbQU, and a random network distillation [16] IM agent can be viewed at https://youtu.be/4OVZeFppDEM. These results suggest that IM algorithms could also benefit from a robustification phase in which they focus only on real-game reward once the IM phase has sufficiently explored the state space.</p>
<h1>3.2 Pitfall</h1>
<p>We next test Go-Explore on the harder, more deceptive game of Pitfall, for which all previous RL algorithms scored $\leq 0$ points, except those that were evaluated on the fully deterministic version of the game [43, 44] or relied on human demonstrations [26, 27, 45]. As with Montezuma's Revenge, we first run Go-Explore with the simple, domain-general, downscaled representation described in Section 2.1.1, with the same hyperparameters. With these settings, Go-Explore is able to find 22 rooms, but it is unable to find any rewards (Fig. 9). We believe that this number of rooms visited is greater than the previous state of the art, but the number of rooms visited is infrequently reported so we are unsure. In preliminary experiments, Go-Explore with a more fine-grained downscaling</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance on Pitfall of Phase 1 of Go-Explore with and without domain knowledge. Without domain knowledge, the exploration phase finds about 22 rooms (a), but it then quickly stops finding new rooms (a) or cells (b) (here, we display discovery of domain-knowledge cells to enable a fair comparison, see Appendix A. 10 for progress on the domain-agnostic cell representation), and it doesn't find any rewards (c). With domain knowledge, the exploration phase of Go-Explore finds all 255 rooms (a) and trajectories scoring a mean 70,264 points (c). In addition, even though the number of rooms (a) and the number cells (b) found stagnates after about 2B game frames, score continues to go up for about another billion game frames. This is possible because, in Pitfall, there can exist many different trajectories to the same cell that vary in score. As such, once all reachable cells have been discovered, Go-Explore relies on replacing lower-scoring trajectories with higher-scoring trajectories to increase its score. The final score is not the maximum score that can be reached in Pitfall (the maximum score in Pitfall is 112,000), but Go-Explore finds itself in a local optima where higher scoring trajectories cannot be found starting from any of the trajectories currently in the archive. Lines represent the mean over 10 (without domain knowledge) and 40 (with domain knowledge) independent runs.
procedure (assigning 16 different pixel values to the screen, rather than just 8) is able to find up to 30 rooms, but it then runs out of memory (Appendix A.10). Perhaps with a more efficient or distributed computational setup this representation could perform well on the domain, a subject we leave to future work. We did not attempt to robustify any of the trajectories because no positive reward was found.</p>
<p>We believe the downscaled-image cell representation underperforms on Pitfall because the game is partially observable, and frequently contains many importantly different states that appear almost identical (even in the unaltered observation space of the game itself), but require different actions (Appendix A.12). One potential solution to this problem would be to change to a cell representation that takes previous states into account to disambiguate such situations. Doing so is an interesting direction for future work.</p>
<p>Next, we tested Go-Explore with domain knowledge (Section 2.1.1). The cell representation with domain knowledge is not affected by the partial observability of Pitfall because it maintains the room number, which is information that disambiguates the visually identical states (note that we can keep track of the room number from pixel information only by keeping track of all screen transitions that happened along the trajectory). With it, the exploration phase of Go-Explore (Phase 1) is able to visit all 255 rooms and its best trajectories collect a mean of 70,264 (CI: 67,287 - 73,150) points (Fig. 9).</p>
<p>We attempted to robustify the best trajectories, but the full-length trajectories found in the exploration phase did not robustify successfully (Appendix A.11), possibly because different behaviors may be required for states that are visually hard to distinguish (Appendix A.12). Note that the domainknowledge cell representation does not help in this situation, because the network trained in the robustification phase (Phase 2) is not presented with the cell representation from the exploration phase (Phase 1). The network thus has to learn to keep track of past information by itself. Remembering the past is possible, as the network of the agent does include a fully recurrent layer, but it is unclear to what degree this layer stores information from previous rooms, especially because the Backward Algorithm loads the agent at various points in the game without providing the agent with the history of rooms that came before. This can make it difficult for the agent to learn to store information from previous states. As such, robustifying these long trajectories remains a topic for future research.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Historical progress on Pitfall vs. the version of Go-Explore that harnesses domain knowledge. Go-Explore achieves a mean of over 59,000 points, greatly outperforming the prior state of the art. The data are presented in tabular form in Appendix A.9.</p>
<p>We found that shorter trajectories scoring roughly 35,824 (CI: $34,225-37,437$ ) points could be successfully robustified. To obtain these shorter trajectories, we truncated all trajectories in the archive produced in Phase 1 to 9,000 training frames (down from the total of 18,000 training frames), and then selected the highest scoring trajectory out of these truncated trajectories. We then further truncated this highest scoring trajectory such that it would end right after the collection of the last obtained reward, to ensure that the Backward Algorithm would always start right before obtaining a reward, resulting in trajectories with a mean length of 8,304 (CI: $8,118-8,507$ ) training frames.
From the truncated trajectories, the robustification phase (Phase 2) of Go-Explore is able to produce agents that collect 59,494 (CI: 49,042 - 72,721) points (mean over 10 independent runs), substantially outperforming both the prior state of the art and human experts (Fig. 10). These trajectories required a mean of 8.20B (CI: $6.73 \mathrm{~B}-9.74 \mathrm{~B}$ ) game frames to robustify, which took a mean of 4.5 (CI: $3.7-$ 5.3) days. The best rollout of the best robustified policy obtained a score of 107,363 points, and a video of this rollout is available at: https://youtu.be/1JMdYOnsDpA.</p>
<p>Interestingly, the mean performance of the robustified networks of 59,494 is higher than the maximum performance among the demonstration trajectories of 45,643 . This score difference is too large to be the result of small optimizations along the example trajectories (e.g. by avoiding more of the negative rewards in the environment), thus suggesting that, as with Montezuma's Revenge, these policies are able to generalize well beyond the example trajectories they were provided.</p>
<h1>4 Discussion and Future Work</h1>
<p>Three key principles enable Go-Explore to perform so well on hard-exploration problems: (1) remember good exploration stepping stones, (2) first return to a state, then explore and, (3) first solve a problem, then robustify (if necessary).
These principles do not exist in most RL algorithms, but it would be interesting to weave them in. As discussed in Section 1, contemporary RL algorithms do not do follow principle 1, leading to detachment. Number 2 is important because current RL algorithms explore by randomly perturbing the parameters or actions of the current policy in the hope of exploring new areas of the environment, which is ineffective when most changes break or substantially change a policy such that it cannot first return to hard-to-reach states before further exploring from them (an issue we call derailment). Go-Explore solves this problem by first returning to a state and then exploring from there. Doing so</p>
<p>enables deep exploration that can find a solution to the problem, which can then be robustified to produce a reliable policy (principle number 3).
The idea of preserving and exploring from stepping stones in an archive comes from the quality diversity (QD) family of algorithms (like MAP-elites [60, 79] and novelty search with local competition [80]), and Go-Explore is an enhanced QD algorithm based on MAP-Elites. However, previous QD algorithms focus on exploring the space of behaviors by randomly perturbing the current archive of policies (in effect departing from a stepping stone in policy space rather than in state space), as opposed to explicitly exploring state space by departing to explore anew from precisely where in state space a previous exploration left off. In effect, Go-Explore offers significantly more controlled exploration of state space than other QD methods by ensuring that the scope of exploration is cumulative through state space as each new exploratory trajectory departs from the endpoint of a previous one.
It is remarkable that the current version of Go-Explore works by taking entirely random actions during exploration (without any neural network) and that it is effective even when applied on a very simple discretization of the state space. Its success despite such surprisingly simplistic exploration strongly suggests that remembering and exploring from good stepping stones is a key to effective exploration, and that doing so even with otherwise naive exploration helps the search more than contemporary deep RL methods for finding new states and representing those states. Go-Explore might be made even more powerful by combining it with effective, learned representations. It could further benefit from replacing the current random exploration with more intelligent exploration policies, which would allow the efficient reuse of skills required for exploration (e.g. walking). Both of these possible improvements are promising avenues for future work.
Go-Explore also demonstrates how exploration and dealing with environmental stochasticity are problems that can be solved separately by first performing exploration in a deterministic environment and then robustifying relevant solutions. The reliance on having access to a deterministic environment may initially seem like a drawback of Go-Explore, but we emphasize that deterministic environments are available in many popular RL domains, including videos games, robotic simulators, or even learned world models. Once a brittle solution is found, or especially a diverse set of brittle solutions, a robust solution can then be produced in simulation. If the ultimate goal is a policy for the real world (e.g. in robotics), one can then use any of the many available techniques for transferring the robust policy from simulation to the real world [59, 60, 81]. In addition, we expect that future work will demonstrate that it is possible to substitute exploiting determinism to return to states with a goal-conditioned policy [62,63] that learns to deal with stochastic environments from the start (during training). Such an algorithm would still benefit from the first two principles of Go-Explore, and possibly the third too, as even a goal-conditioned policy could benefit from additional optimization once the desired goal is known.
A possible objection is that, while this method already works in the high-dimensional domain of Atari-from-pixels, it might not scale to truly high-dimensional domains like simulations of the real world. We believe Go-Explore can be adapted to such high-dimensional domains, but it will likely have to marry a more intelligent cell representation of interestingly different states (e.g. learned, compressed representations of the world) with intelligent (instead of random) exploration. Indeed, the more conflation (mapping more states to the same cell) one does, the more probable it is that one will need intelligent exploration to reach such qualitatively different cells.
Though our current implementation of Go-Explore can handle the deceptive reward structure found in Pitfall, its exploitation of determinism makes it vulnerable to a new form of deception we call the "busy-highway problem." Consider an environment in which the agent needs to cross a busy highway. One option is to traverse the highway directly on foot, but that creates so much risk of being hit by a car that no policy could reliably cross this way. A safer alternative would be to take a bridge that goes over the highway, which would constitute a detour, but be guaranteed to succeed. By making the environment deterministic for Phase 1, the current version of Go-Explore would eventually succeed in traversing the highway directly, leading to a much shorter trajectory than by taking the bridge. Thus all the solutions chosen for robustification will be ones that involve crossing the highway directly instead of taking the bridge, making robustification impossible.
One solution to this issue would be to provide robustification with more demonstrations from Phase 1 of Go-Explore (which could include some that take the bridge instead of crossing the highway), or even all of the trajectories it gathers during Phase 1. With this approach, robustification would be able to fall back on the bridge trajectories when the highway trajectories fail to robustify. While this</p>
<p>approach should help, it may still be the case that so much of the experience gathered by Go-Explore Phase 1 is dependent on trajectories that are impossible to reproduce reliably that learning from these Go-Explore trajectories is less efficient than learning from scratch. How common this class of problem is in practice is an empirical question and an interesting subject for future work. However, we hypothesize that versions of Go-Explore that deal with stochasticity throughout training (e.g. by training goal-conditioned policies to return to states) would not be affected by this issue, as they would not succeed in crossing the highway reliably except by taking the bridge.
One promising area for future work is robotics. Many problems in robotics, such as figuring out the right way to grasp an object, how to open doors, or how to locomote, are hard-exploration problems. Even harder are tasks that require long sequences of actions, such as asking a robot to find survivors, clean a house, or get a drink from the refrigerator. Go-Explore could enable a robot to learn how to do these things in simulation. Because conducting learning in the real world is slow and may damage the robot, most robotic work already involves first optimizing in a simulator and then transferring the policy to the real world [59-61, 82]. Go-Explore's ability to exploit determinism can then be helpful because robotic simulators could be made deterministic for Phase 1 of Go-Explore. The full pipeline could look like the following: (1) Solve the problem in a deterministic simulator via Phase 1 of Go-Explore. (2) Robustify the policy in simulation by adding stochasticity to the simulation via Phase 2 of Go-Explore. (3) Transfer the policies to the real world, optionally adding techniques to help cross the simulation-reality gap [59-61], including optionally further learning via these techniques or any learning algorithm. Of course, this pipeline could also be changed to using a goal-conditioned version of Go-Explore if appropriate. Overall, we are optimistic that Go-Explore may make many previously unsolvable robotics problems solvable, and we are excited to see future research in this area from our group and others.
Interestingly, the Go-Explore algorithm has implications and applications beyond solving sparse- or deceptive-reward problems. The algorithm's ability to broadly explore the state space can unearth important facets of the domain that go beyond reward, e.g. the distribution of states that contain a particular agent (e.g. a game character or robot) or are near to catastrophic outcomes. For example, within AI safety [5] one open problem is that of safe exploration [83], wherein the process of training an effective real-world policy is constrained by avoiding catastrophe-causing actions during that training. In the robotics setting where Go-Explore is applied in simulation (before attempting transfer to the real world), the algorithm could be driven explicitly to search for diverse simulated catastrophes (in addition to or instead of reward). Such a catastrophe collection could then be leveraged to train agents that act more carefully in the real world, especially while learning [84, 85]. Beyond this example, there are likely many other possibilities for how the data produced by Go-Explore could be productively put to use (e.g. as a source of data for generative models, to create auxiliary objectives for policy training, or for understanding other agents in the environment by inverse reinforcement learning).</p>
<h1>5 Related Work</h1>
<p>Go-Explore is reminiscent of earlier work that separates exploration and exploitation (e.g. Colas et al. [86]), in which exploration follows a reward-agnostic Goal Exploration Process [87] (an algorithm similar to novelty search [7]), from which experience is collected to prefill the replay buffer of an off-policy RL algorithm, in this case DDPG [88]. This algorithm then extracts the highest-rewarding policy from the experience gathered. In contrast, Go-Explore further decomposes exploration into three elements: Accumulate stepping stones (interestingly different states), return to promising stepping stones, and explore from them in search of additional stepping stones (i.e. principles 1 and 2 above). The impressive results Go-Explore achieves by slotting in very simple algorithms for each element shows the value of this decomposition.
The aspect of Go-Explore of first finding a solution and then robustifying around it has precedent in Guided Policy Search [89]. However, this method requires a non-deceptive, non-sparse, differentiable loss function to find solutions, meaning it cannot be applied directly to problems where rewards are discrete, sparse, or deceptive, as both Atari and many real-world problems are. Further, Guided Policy Search requires having a differentiable model of the world or learning a set of local models, which to be tractable requires the full state of the system to be observable during training time.</p>
<p>More recently, Oh et al. [90] combined A2C with a "Self-Imitation Learning" loss on the best trajectories found during training. This is reminiscent of Go-Explore's robustification phase, except for the fact that Self-Imitation Learning's imitation loss is used throughout learning, while imitation learning is a separate phase in Go-Explore. Self-Imitation Learning's 2,500 point score on Montezuma's Revenge was close to the state of the art at the time of its publication.
Another algorithm that is related to the idea of first returning before exploring is Bootstrapped DQN [91]. It trains an ensemble of networks that approximate the Q function, but with bootstrapping the data so each network is trained on a different random subset of the data. Each training episode, it picks one of the networks and acts according to the policy it implies. In frequently visited areas of the search space, all of the networks will have lots of data and are likely to converge to the same policy (thus, exploration will be low). However, in rarely visited areas of the state space, the networks would ideally have different Q-value predictions, meaning that in different episodes different choices will be made, yielding exploration. At a high level, the dynamics can thus allow an agent to first return to an area of the search space with little exploration before exploring from it. That said, this algorithm will still try to focus on returning to one narrow area of the search space (the one it is currently exploring, see the flashlight metaphor of IM algorithms in Section 1) before exploring, and thus is still likely to suffer from the issue of detachment described in Section 1. Indeed, empirically Bootstrapped DQN scores only 100 on Montezuma's Revenge, and detachment may be a large reason why.
Recall Traces [92] also implement the idea of returning to previously discovered states. They do so by running a backtracking model to create virtual trajectories towards states heuristically considered valuable and they include those virtual trajectories during training with the help of an imitation learning loss, thereby increasing the likelihood that these states will be revisited and explored from. Contrary to Go-Explore, Recall Traces do not separate returning to states and exploring from those states, thus the algorithm helps ameliorate detachment, but not derailment. The method improved sample efficiency in several sparse reward domains, but was not tested on Montezuma's Revenge or Pitfall.
Closely related to the first two principles of Go-Explore is the work by Liu et al. [43], which takes a hierarchical reinforcement learning approach in which an abstract MDP is created through the conflation of multiple states into abstract states, which are similar to the cells in Go-Explore. This abstract MDP stores all abstract states (i.e. cells) that it encounters, thus keeping track of promising states to explore from, and it navigates the MDP in a reliable way before exploring from a selected abstract-MDP state, thus implementing the idea of returning before exploring. One difference with Go-Explore is that this algorithm does not use a trajectory of actions to return to a cell, but instead relies on a set of sub-policies, called skills, which are executed in sequence to navigate the abstract MDP. While this set of skills is flexible, in that it allows the same skill to be reused for different transitions, it takes time to train a new skill, potentially making it computationally expensive to explore as deep into the game as Go-Explore does. Another difference is that the algorithm by Liu et al. [43] does not implement a robustification phase, but instead relies on the abstract MDP, even at evaluation time. While this means the algorithm does not require any additional training, it also means the algorithm can never improve upon the limits of the constructed MDP. The algorithm from Liu et al. [43], which harnesses domain knowledge, scores 12,500 on Montezuma's Revenge and 15,000 on Pitfall, though these scores come from evaluation in the deterministic version of the environment (they do provide results on stochastic test environments for a different game: Private Eye). Go-Explore scores substantially more in both Montezuma's Revenge and Pitfall despite being tested in a stochastic environment and, in the case of Montezuma's Revenge, even when not relying on domain knowledge.
In a similar vein, Dong et al. [93] maintains an explicit memory of novel states and explores after returning to them via a goal-conditioned policy, though their algorithm only reaches scores of around 1,000 on Montezuma's Revenge, substantially less than Go-Explore. We speculate that this is due to (1) Its use of a fixed-capacity pool of potential next states to visit, which might not be able to keep up with the large number of possible interestingly different states present in Montezuma's Revenge, and (2) By determining whether a goal is reached based on a pixel based measure, their goal-conditioned policy could have a hard time learning to return to a previously visited state, as the pixel-based match requires all moving objects, such as enemies, to be in very similar locations before a goal is considered reached. The insights of keeping an archive of known states and exploring to discover new states to add to the archive dates back at least to the $E^{3}$ algorithm [94], although the $E^{3}$ authors note that it does not work in high-dimensional problems for which tabular methods are intractable</p>
<p>and function approximation (or some form of conflation) is required. Go-Explore can be seen as an $E^{3}$-like algorithm that adapts some of its principles to high-dimensional domains.</p>
<p>The idea of planning (searching in a deterministic model of the world to find a good strategy) and then training a policy to mimic what was learned is reminiscent of Guo et al. [95]. It plans (in the Atari emulator) with UCT [47-49], which is slow, and then trains a much faster policy with supervised learning to imitate the planning algorithm. At first glance it seems that in Guo et al. [95] UCT serves a similar role to the exploration phase in Go-Explore, but UCT is quite different in several ways that make it inferior for domains that are either high-dimensional or hard-exploration. That is true even though UCT does have a form of exploration bonus.
UCT plans in a model of the world so as to decide on the next action to take in the real environment. An exploration bonus is used during the planning phase, but only extrinsic rewards are considered when choosing the next action to take. This approach can improve performance in domains with relatively dense rewards, but fails in sparse rewards domains as rewards are likely to be beyond the planning horizon of the algorithm. Once planning what to do from one state is done, an action is taken and the planning process is run again from the next state. UCT does not try to explore all states, and each run of UCT is independent of which states were visited in previous planning steps. As such, UCT (either within an episode, or across episodes) does not try to discover new terrain: instead its exploration bonus only helps it within the current short-horizon planning phase. As mentioned in Section 1, UCT scores 0 on Montezuma's Revenge and Pitfall [30, 51].
Another approach to planning is Fractal Monte Carlo (FMC) [96]. When choosing the next action, it takes into account both the expected reward and novelty of that action, and in that way is more similar to Go-Explore. In FMC, a planning process is initiated from each state the agent visits. Planning is done within a deterministic version of the game emulator. A fixed number of workers are started in the state from which planning is occurring, and they perform random walks in state space. Periodically, workers that have accumulated lower reward and/or are in less novel states are replaced by "clones" of more successful workers. Novelty is approximated as the Euclidean distance of the worker's state (in the original, raw, observation space) to that of a randomly selected other worker.
FMC reaches a score of 5,600 on Montezuma's Revenge, substantially higher than UCT. We believe this increased performance is due to at least three factors: (1) its planning process puts more emphasis on depth than breadth due to its finite amount of workers as opposed to the exponential branching factor that UCT needs to handle; (2) it favors novel states within a planning iteration, so actions that lead to hard-to-reach states such as jumping an enemy are more likely to be chosen; (3) having an exploration bonus based on Euclidean distance is more informative than UCT's exact-match state bonus, because more distant states are recognized as being more novel than states that differ by, say, one pixel. One major reason we believe FMC performs worse than Go-Explore is because, like UCT, it restarts its planning process from scratch each time an action is taken. That means it can cycle indefinitely between the same few states, because it does not have a means over time of remembering which states it has visited in order to attempt to explore all states, and instead must rely on random chance to break out of cycling. This phenomenon is apparent when watching its agent play: https://youtu.be/FgaXa0uCBR4. Although its greater focus on depth rather than breadth versus UCT extends its planning horizon enough to reach the first few rewards available in Montezuma's Revenge, that seemingly was insufficient for it to reach the even sparser rewards found later in the game that are easily found by Go-Explore.
On Pitfall, SOORL [44] was the first planning algorithm to achieve a non-zero score, but did so in a deterministic test environment. It does so through a combination of learning a model of the environment, domain knowledge, and a value function that is optimistic about the value of unseen states, thus effectively providing an exploration bonus. At the end of 50 episodes of training, which was the maximum reported number of episodes, SOORL achieves an average of about 200 points across runs, and its best run scored an average of 606.6 with a maximum of 4,000 .
Another way to view Phase 1 of Go-Explore is as being similar to a graph-search algorithm over nodes that are made up of the conflated states, and with unknown edges between the different nodes, meaning that nodes can never fully be marked as "closed". Specifically, the algorithm has to empirically discover the existence of an edge between two nodes, for example by executing a sequence of random actions that leads from one node to another node, and, as a result, it is never clear whether a node is closed because it is always possible that additional edges from this node exist, but that they have not been discovered yet. Prioritizing which nodes to explore by assigning a weight</p>
<p>to them is reminiscent of graph-search algorithms such as Dijkstra's algorithm [97] and A* [98]. Graph-search algorithms as a means of exploration in planning have been investigated in algorithms such as Rapidly-exploring Random Trees (RRTs) [99], which were recently used to explore Atari games by Zhan et al. [100]. Indeed, Go-Explore exhibits important similarities with RRTs as they both keep track of an archive of states and trajectories to those states. However, there are some crucial differences, including: (1) RRTs proceed by first sampling a goal to attempt to reach, which can be impractical in environments where reachable states are not known a priori (and which is particularly pernicious in high-dimensional state spaces, such as pixels or even learned encodings, where most randomly selected goals are unreachable), such as Atari, and (2) RRTs do not have the concept of "cells" present in Go-Explore and thus RRTs can add many very similar states to their archive that do little to help the algorithm reach meaningfully different unexplored areas of the search space. In general, we believe that Go-Explore points to an interesting future research direction in adapting the principles behind graph-search algorithms to high dimensional state spaces.
Even more distantly related are the many variants of intrinsically motivated model-free reinforcement learning algorithms. The relation between Go-Explore and these algorithms is discussed in Section 1 and many specific algorithms are included in our comparison in Appendix A.9, as they account for most of the high-scoring work on Montezuma's Revenge prior to Go-Explore.</p>
<h1>6 Conclusion</h1>
<p>Go-Explore represents an exciting new family of algorithms for solving hard-exploration reinforcement learning problems, meaning those with sparse and/or deceptive rewards. It opens up a large number of new research directions beyond the simple version described in this paper, including experimenting with different archives, different methods for choosing which cells to return to, different cell representations, different exploration methods, and different robustification methods. We expect Go-Explore will accelerate progress in a variety of challenging domains such as robotics. It will also be interesting to see not only the domains in which it excels, but also those in which it fails. Go-Explore thus opens a new playground of possibilities for future research, and we hope the community will join us in investigating this new terrain.</p>
<h2>Acknowledgments</h2>
<p>We thank the following for helpful discussions on the Go-Explore algorithm and the ideas behind it: Peter Dayan, Zoubin Ghahramani, Shimon Whiteson, Juergen Schmidhuber, Ian Osband, and Kevin Clune. We also appreciate input from all of the members of Uber AI Labs, especially Vashisht Madhavan, Felipe Petroski Such, John Sears, and Thomas Miconi. We are also deeply appreciative of the machine learning community at large for providing feedback that refined our thinking and exposition of Go-Explore, including all of those that provided commentary on Reddit, Twitter, and via other online mediums such as blog posts about our work. Finally, we are grateful to Leon Rosenshein, Joel Snow, Thaxton Beesley, the Colorado Data Center team and the entire OpusStack Team at Uber for providing our computing platform and for technical support.</p>
<h2>References</h2>
<p>[1] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
[2] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, L Robert Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354-359, 2017.
[3] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[4] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, pages 1471-1479, 2016.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Note that this second phase is in principle not necessary if Phase 1 itself produces a policy that can handle stochastic environments (Section 2.1.3).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
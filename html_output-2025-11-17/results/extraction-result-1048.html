<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1048 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1048</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1048</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-3660f76126fe1343c91f065f452845981041206c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3660f76126fe1343c91f065f452845981041206c" target="_blank">A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems</a></p>
                <p><strong>Paper Venue:</strong> Journal of Artificial Intelligence Research</p>
                <p><strong>Paper TL;DR:</strong> A taxonomy of solutions for the general knowledge reuse problem is defined, providing a comprehensive discussion of recent progress on knowledge reuse in Multiagent Systems (MAS) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not).</p>
                <p><strong>Paper Abstract:</strong> 
 
 
Multiagent Reinforcement Learning (RL) solves complex tasks that require coordination with other agents through autonomous exploration of the environment. However, learning a complex task from scratch is impractical due to the huge sample complexity of RL algorithms. For this reason, reusing knowledge that can come from previous experience or other agents is indispensable to scale up multiagent RL algorithms. This survey provides a unifying view of the literature on knowledge reuse in multiagent RL. We define a taxonomy of solutions for the general knowledge reuse problem, providing a comprehensive discussion of recent progress on knowledge reuse in Multiagent Systems (MAS) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not). We aim at encouraging the community to work towards reusing all the knowledge sources available in a MAS. For that, we provide an in-depth discussion of current lines of research and open questions. 
 
 
</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1048.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1048.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapting to Other Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapting learned policies to operate with different teammate/opponent configurations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of intra-agent transfer methods that reuse previously learned policies or models to quickly adapt an agent's behavior when the other agents in the environment change (different teams, opponents, or strategies). Emphasizes implicit source selection and policy reuse to reduce sample complexity when coordinating or competing with new agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>learning agent (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement-learning-based agents (various LA types reported: self-interested, cooperative, adversarial, equilibrium approaches); some methods build models of other agents and select/reuse past policies to act.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>multiagent team/opponent environments (generic; includes general game playing, team configuration domains)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where multiple agents act and the identity/strategy of teammates or opponents may vary; complexity arises from joint action space and need for coordination; variation arises from differences in team composition and opponent strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>joint action space size (grows with number of agents), number of distinct teammate/opponent policies/strategies, size of state representation when including other agents' states</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies (often medium-to-high when many agents or coordination states exist)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>number and diversity of team/opponent strategy profiles; frequency and magnitude of opponent strategy changes (strategy drift); whether other agents are fixed or adaptive</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>varies (from low when opponents fixed to high when opponents adapt or change policies frequently)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>policy selection accuracy / improvement in initial performance (jumpstart), total reward during adaptation, time to select appropriate past policy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey discusses that agent adaptation difficulty increases with joint action space size and with variation in other agents' strategies; methods typically assume limited variation (e.g., fixed opponent policies) or detect strategy drift first then reapply transfer; no single quantitative trade-off is given.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>policy library + selection (beliefs over teams), model-based opponent modeling, policy reuse; sometimes hierarchical composition of past policies</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports qualitative improvements in adaptation when past policies/models are available and opponents are similar to previously seen ones; methods struggle or require detection and retraining when opponents change unpredictably.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improved initial performance / faster adaptation claimed qualitatively; quantitative sample counts not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reusing past policies/models can substantially speed adaptation to new teammates/opponents when the variation is limited or detectable; negative transfer is a risk if opponent/team differences are large; detecting strategy drift (e.g., via model prediction degradation) is important to avoid stale transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1048.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse Interaction Algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse interaction transfer methods for multiagent RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that identify portions of the state space where other agents' actions matter ('dangerous' or interaction states) and treat remaining states as single-agent tasks, enabling transfer of rules or value functions to prune joint action complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>local agent (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents using equilibrium, cooperative, or adversarial RL algorithms that exploit sparsity of multiagent interactions (e.g., CQ-learning, Dec-SIMDP ideas); some learn classifiers/rules to detect interaction-relevant states.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>multiagent environments with sparse interaction regions (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where other agents only affect the local agent's reward or transitions in a subset of states; complexity concentrated in those interaction states while much of state space behaves like a single-agent MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>fraction or count of states requiring joint-action reasoning (i.e., 'dangerous' states), size of joint-action space in those states, state-space size overall</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high overall, but effective complexity reduced if dangerous-state fraction is small</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>degree to which the set of interaction states changes across tasks (e.g., progressive difficulty via added objects may increase interaction states), changes in teammates/opponents behavior confined to interaction states</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>often low-to-medium within same domain if interaction regions stable; can be high across differing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>convergence speed to equilibrium/optimal policy, reduced computational cost to compute equilibrium, accuracy of dangerous-state classifier</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey highlights a trade-off: reducing complexity by focusing on sparse interactions accelerates equilibrium computation, but correctness depends on accurate identification of interaction states; if variation causes interaction regions to shift, transferred rules/value functions may be invalid and harm performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>learn dangerous-state predictors in simpler tasks and transfer rules or value estimates to target tasks; combine single-agent Q-values with multiagent equilibrium Q-values in interaction states</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative: methods performed well when interaction states transferred reliably; manual selection or designer guidance often required to ensure transfer validity.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improved sample/computational efficiency in practice when interaction sparsity holds; survey provides no numeric counts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exploiting sparse interactions is an effective way to reduce multiagent complexity and accelerate learning, but robustness requires reliable detection/mapping of interaction states across tasks to avoid negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1048.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relational Descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relational/object-oriented task descriptions for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of relational or object-oriented representations to abstract task structure, enabling learned abstract policies or function approximators to be transferred across tasks and reduce risk of negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>agent using relational/object-oriented representations</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents combine RL with relational/state abstraction (abstract/concrete policies, function approximators) to learn generalizable behaviors; can be applied in self-interested or cooperative settings.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>object-based MDPs / relational multiagent domains (e.g., Multiagent OO-MDP)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments described by objects and relations (e.g., agents, goals, obstacles), where complexity arises from combinatorial object interactions and variation arises from different object instances or numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>number of object types and instances, relational arity (how many objects interact), state-space size factored by objects</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies; relational abstraction intended to reduce perceived complexity from high to manageable</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>changes in number/types of objects, different relational configurations across tasks (progressive difficulty often by adding objects)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high depending on count and variability of objects</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>learning speed (time to good concrete policy), policy transfer success across tasks, jumpstart and asymptotic performance when reusing abstract policies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey states relational abstraction reduces negative transfer by capturing structural similarities despite variation in low-level details (e.g., different numbers of objects); complexity can be mitigated by abstract policies but mapping/autonomy is often human-assisted.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>learn abstract policies first then refine concrete policies; transfer abstract policies across tasks using relation mappings (often human-assisted)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Qualitative improvements reported: abstract policies generalize to different object-count instances and can bootstrap concrete learning; concrete refinement required for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improved compared to raw state representations due to generalization; no numeric sample counts provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Relational/object-oriented representations enable transferring higher-level policies or function approximators across tasks with differing low-level details, reducing sample complexity and risk of negative transfer when mappings are provided or learned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1048.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Source Task Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Source task selection methods for transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedures to choose which past tasks/policies to reuse for a new target task, including human-specified feature-based regressors and autonomous ranking methods to predict transfer quality before interacting with the target.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>learning agent with policy library</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RL agent that maintains a library of source policies and uses designer-provided task features or autonomous evaluation to rank/prioritize policies for transfer; methods include regression models predicting transfer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>task families within same domain (e.g., variants parameterized by task features)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Sets of related tasks parameterized by observable features (e.g., goal positions, object counts); variation occurs across those parameters while domain remains same.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>task feature dimensionality, number of source tasks in library, state-action space similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>often low-to-medium within same domain assumptions; complexity increases when allowed differences grow</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>range and distribution of task-features across source tasks; whether target is within convex hull of source features or not</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>typically low-to-medium under same-domain assumptions; can be high if arbitrary target tasks considered</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>predicted transfer quality, ranking accuracy of source tasks, improvement in jumpstart/total reward when using selected source tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey emphasizes that good source selection reduces negative transfer especially when variation between tasks is limited; autonomous selection requires observable task features or trial evaluations; as variation increases, selection becomes harder and risk of negative transfer grows.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>feature-based regression to predict transfer quality, ensemble evaluation of top-performing stored policies on the target, human-assisted feature specification</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Methods like Sinapov et al. (as discussed) can predict transfer quality across same-domain variations and rank source tasks without target samples when task features are provided; autonomy is limited by need for designer-specified features.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improves efficiency by avoiding poor source policies and selecting useful ones; quantitative sample counts not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reliable source selection markedly reduces negative transfer; human-provided task features enable effective prediction but limit autonomy; fully autonomous selection remains challenging when variation is large or features unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1048.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum Learning (TL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum learning / progressive difficulty for transfer in RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that construct sequences of easier source tasks (a curriculum) to progressively teach an agent and transfer knowledge to tackle harder target tasks, thereby reducing sample complexity for complex target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>learning agent (generic across curriculum methods)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement-learning agents trained across a sequence of tasks of increasing difficulty; some approaches require an environment simulator and designer heuristics, others build Curriculum MDPs to automate sequencing.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>simulated task families where environment parameters can be varied (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Complex tasks decomposed into simpler intermediate tasks (e.g., via changing simulator parameters, initial-state proximity to goal, or smaller object sets); complexity increases along curriculum steps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>task difficulty (qualitative), state/action space size as objects/obstacles are added, distance from initial state to goal (in initial-state curricula)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>curriculum progresses from low to high complexity (explicitly progressive)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>difference between curriculum steps (parameter change magnitude), whether source tasks cover distribution of target variations; often low variation between consecutive curriculum tasks</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>typically low between consecutive curriculum steps; overall variation across whole curriculum can be medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>learning speed (time to threshold), total reward during training, jumpstart, transfer ratio</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey reports that breaking a complex task into progressively harder variants reduces effective complexity per learning stage and accelerates learning; curricula assume limited variation between consecutive tasks so that transfer is beneficial; if variation between steps is large, transfer may fail.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>curriculum generation (designer heuristics or Curriculum MDP), progressive transfer of value functions/models/policies, initial-state shaping to start near goals and progressively expand</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported qualitative and some quantitative improvements: curricula 'expressively accelerate' learning in surveyed works (e.g., Narvekar et al.); automation of curriculum generation remains active research.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improved sample efficiency relative to direct training on hard tasks; survey does not report exact interaction counts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curricula reduce sample complexity by ensuring learning occurs on incrementally harder tasks; effectiveness depends on sensible ordering and bounded variation between successive tasks; curricula can be combined with transfer and multiagent parallelization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1048.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DriftER (strategy-drift detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DriftER: detecting opponent strategy changes for adaptive transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that learns a model of an opponent and monitors prediction quality to detect sudden opponent strategy changes (drift); when drift is detected the agent recomputes models and can (in principle) reuse past policies for similar opponents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>agent with opponent-modeling (DriftER-style)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns predictive models of opponents and computes optimal policies against the predicted behavior; monitors model prediction accuracy to trigger model recomputation upon drift.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>adversarial multiagent environments with potentially non-stationary opponents</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments where opponents may change strategy over time; variation manifests as temporal non-stationarity in opponents' policies, complicating transfer of past policies.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>degree of opponent non-stationarity, difficulty of modeling opponent policy, state-action space size including opponent features</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies; non-stationarity increases effective complexity</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>frequency/magnitude of opponent policy changes (drift), number of distinct opponent strategies encountered</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>can be high (if opponents adapt/learn) or low (if fixed policies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>detection speed/accuracy of strategy change, quality of resulting policies after adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey highlights that detecting opponent drift is crucial: when opponent behavior is stable (low variation) policy reuse helps; when variation is high, detection and model retraining are required to avoid negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>opponent-model learning + monitoring of prediction quality; trigger recomputation and (optionally) reuse past policies for similar opponent profiles</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports qualitative capability: DriftER can detect sudden strategy changes, enabling timely model updates; explicit reuse of past policies for new opponents is suggested as an extension but not implemented in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improved robustness to non-stationarity by avoiding prolonged use of stale models, quantitative sample counts not provided</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Monitoring model prediction quality to detect opponent strategy drift is a practical mechanism to decide when transferred knowledge is no longer valid; combining drift detection with policy reuse could enable faster re-adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1048.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sinapov-style Source Selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feature-based regression for predicting transfer quality (source task ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A source selection approach that uses designer-specified task features to train a regressor predicting transfer quality between source and target tasks, enabling ranking of source tasks without target interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>agent with transfer-quality regressor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that uses a regression model trained on features of prior tasks to predict how useful each source policy will be for a new target given its features; relies on human-specified, observable task features.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>same-domain task families parameterized by observable features</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tasks vary by parameterized features (e.g., goal locations, object counts); variation is captured by designer-provided features; complexity is from task difficulty and policy differences.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>dimensionality and range of task features, heterogeneity of source policies</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>typically low-to-medium under same-domain assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>coverage and spread of feature values across source tasks; magnitude of feature differences between source and target</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium if features constrained; can be high if features vary widely</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy of predicted transfer quality and resulting transfer ranking; practical metrics: jumpstart and total reward when chosen source used</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey notes that when tasks are in the same domain and feature descriptions are informative, source selection without target samples is feasible; as variation (differences in features) grows, predictive accuracy and transfer utility degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>train regressor on designer-specified task features mapping to measured transfer quality between tasks; rank sources for new target by predicted quality</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Survey reports this method can autonomously select appropriate sources within same domain without target interactions when appropriate features exist; relies on human-provided features.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>saves target interactions by avoiding exhaustive evaluation on target; numeric training sample counts for regressor not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Feature-based prediction of transfer quality is effective under same-domain assumptions and informative features; dependence on human-specified features limits autonomy and robustness under high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1048.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1048.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Initial-State Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initial-state shaping curriculum (start-near-goal progression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum technique that eases exploration by first initializing episodes close to target states (near the goal) and progressively moving initial states farther away to cover full task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>agent trained with progressive initial-state distribution</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement-learning agents trained using modified episode initial-state distributions; works when environment supports controlled initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent / virtual agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>goal-directed tasks where initial-state can be controlled; e.g., navigation or reaching tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Complexity relates to distance/difficulty of reaching goal from initial states; variation can be introduced by sampling different initial states or environment configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>distance from initial state to goal, state-space coverage needed to solve task, number of steps required to reach goal</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>progressive (starts low near goal, increases as initial states move farther)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>range of initial-state positions sampled across curriculum; degree of environment modification when moving initial states</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>typically low between consecutive curriculum steps, overall medium across full curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>learning speed (episodes to achieve competence), total reward, ability to explore full state space after progression</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Survey indicates starting near goal reduces effective complexity and accelerates early learning; progressively increasing complexity by moving starts farther helps the agent generalize to full task while limiting abrupt variation between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>initial-state shaping curriculum: start near goal and progressively expand initial-state distribution</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Reported to improve exploration and accelerate learning in target tasks where applicable; works best where environments support controlled episode initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>improves sample efficiency by producing early rewards and shaping exploration; no numeric counts provided</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initial-state curricula are a practical curriculum construction that reduce early exploration burden and enable progressive coverage of complex environments, assuming controllable initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1048",
    "paper_id": "paper-3660f76126fe1343c91f065f452845981041206c",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Adapting to Other Agents",
            "name_full": "Adapting learned policies to operate with different teammate/opponent configurations",
            "brief_description": "A class of intra-agent transfer methods that reuse previously learned policies or models to quickly adapt an agent's behavior when the other agents in the environment change (different teams, opponents, or strategies). Emphasizes implicit source selection and policy reuse to reduce sample complexity when coordinating or competing with new agents.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "learning agent (generic)",
            "agent_description": "Reinforcement-learning-based agents (various LA types reported: self-interested, cooperative, adversarial, equilibrium approaches); some methods build models of other agents and select/reuse past policies to act.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "multiagent team/opponent environments (generic; includes general game playing, team configuration domains)",
            "environment_description": "Environments where multiple agents act and the identity/strategy of teammates or opponents may vary; complexity arises from joint action space and need for coordination; variation arises from differences in team composition and opponent strategies.",
            "complexity_measure": "joint action space size (grows with number of agents), number of distinct teammate/opponent policies/strategies, size of state representation when including other agents' states",
            "complexity_level": "varies (often medium-to-high when many agents or coordination states exist)",
            "variation_measure": "number and diversity of team/opponent strategy profiles; frequency and magnitude of opponent strategy changes (strategy drift); whether other agents are fixed or adaptive",
            "variation_level": "varies (from low when opponents fixed to high when opponents adapt or change policies frequently)",
            "performance_metric": "policy selection accuracy / improvement in initial performance (jumpstart), total reward during adaptation, time to select appropriate past policy",
            "performance_value": null,
            "complexity_variation_relationship": "Survey discusses that agent adaptation difficulty increases with joint action space size and with variation in other agents' strategies; methods typically assume limited variation (e.g., fixed opponent policies) or detect strategy drift first then reapply transfer; no single quantitative trade-off is given.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "policy library + selection (beliefs over teams), model-based opponent modeling, policy reuse; sometimes hierarchical composition of past policies",
            "generalization_tested": null,
            "generalization_results": "Survey reports qualitative improvements in adaptation when past policies/models are available and opponents are similar to previously seen ones; methods struggle or require detection and retraining when opponents change unpredictably.",
            "sample_efficiency": "improved initial performance / faster adaptation claimed qualitatively; quantitative sample counts not reported in survey",
            "key_findings": "Reusing past policies/models can substantially speed adaptation to new teammates/opponents when the variation is limited or detectable; negative transfer is a risk if opponent/team differences are large; detecting strategy drift (e.g., via model prediction degradation) is important to avoid stale transfer.",
            "uuid": "e1048.0",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Sparse Interaction Algorithms",
            "name_full": "Sparse interaction transfer methods for multiagent RL",
            "brief_description": "Methods that identify portions of the state space where other agents' actions matter ('dangerous' or interaction states) and treat remaining states as single-agent tasks, enabling transfer of rules or value functions to prune joint action complexity.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "local agent (generic)",
            "agent_description": "Agents using equilibrium, cooperative, or adversarial RL algorithms that exploit sparsity of multiagent interactions (e.g., CQ-learning, Dec-SIMDP ideas); some learn classifiers/rules to detect interaction-relevant states.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "multiagent environments with sparse interaction regions (generic)",
            "environment_description": "Environments where other agents only affect the local agent's reward or transitions in a subset of states; complexity concentrated in those interaction states while much of state space behaves like a single-agent MDP.",
            "complexity_measure": "fraction or count of states requiring joint-action reasoning (i.e., 'dangerous' states), size of joint-action space in those states, state-space size overall",
            "complexity_level": "medium-to-high overall, but effective complexity reduced if dangerous-state fraction is small",
            "variation_measure": "degree to which the set of interaction states changes across tasks (e.g., progressive difficulty via added objects may increase interaction states), changes in teammates/opponents behavior confined to interaction states",
            "variation_level": "often low-to-medium within same domain if interaction regions stable; can be high across differing tasks",
            "performance_metric": "convergence speed to equilibrium/optimal policy, reduced computational cost to compute equilibrium, accuracy of dangerous-state classifier",
            "performance_value": null,
            "complexity_variation_relationship": "Survey highlights a trade-off: reducing complexity by focusing on sparse interactions accelerates equilibrium computation, but correctness depends on accurate identification of interaction states; if variation causes interaction regions to shift, transferred rules/value functions may be invalid and harm performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "learn dangerous-state predictors in simpler tasks and transfer rules or value estimates to target tasks; combine single-agent Q-values with multiagent equilibrium Q-values in interaction states",
            "generalization_tested": null,
            "generalization_results": "Qualitative: methods performed well when interaction states transferred reliably; manual selection or designer guidance often required to ensure transfer validity.",
            "sample_efficiency": "improved sample/computational efficiency in practice when interaction sparsity holds; survey provides no numeric counts",
            "key_findings": "Exploiting sparse interactions is an effective way to reduce multiagent complexity and accelerate learning, but robustness requires reliable detection/mapping of interaction states across tasks to avoid negative transfer.",
            "uuid": "e1048.1",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Relational Descriptions",
            "name_full": "Relational/object-oriented task descriptions for transfer",
            "brief_description": "Use of relational or object-oriented representations to abstract task structure, enabling learned abstract policies or function approximators to be transferred across tasks and reduce risk of negative transfer.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "agent using relational/object-oriented representations",
            "agent_description": "Agents combine RL with relational/state abstraction (abstract/concrete policies, function approximators) to learn generalizable behaviors; can be applied in self-interested or cooperative settings.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "object-based MDPs / relational multiagent domains (e.g., Multiagent OO-MDP)",
            "environment_description": "Environments described by objects and relations (e.g., agents, goals, obstacles), where complexity arises from combinatorial object interactions and variation arises from different object instances or numbers.",
            "complexity_measure": "number of object types and instances, relational arity (how many objects interact), state-space size factored by objects",
            "complexity_level": "varies; relational abstraction intended to reduce perceived complexity from high to manageable",
            "variation_measure": "changes in number/types of objects, different relational configurations across tasks (progressive difficulty often by adding objects)",
            "variation_level": "medium-to-high depending on count and variability of objects",
            "performance_metric": "learning speed (time to good concrete policy), policy transfer success across tasks, jumpstart and asymptotic performance when reusing abstract policies",
            "performance_value": null,
            "complexity_variation_relationship": "Survey states relational abstraction reduces negative transfer by capturing structural similarities despite variation in low-level details (e.g., different numbers of objects); complexity can be mitigated by abstract policies but mapping/autonomy is often human-assisted.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "learn abstract policies first then refine concrete policies; transfer abstract policies across tasks using relation mappings (often human-assisted)",
            "generalization_tested": true,
            "generalization_results": "Qualitative improvements reported: abstract policies generalize to different object-count instances and can bootstrap concrete learning; concrete refinement required for best performance.",
            "sample_efficiency": "improved compared to raw state representations due to generalization; no numeric sample counts provided in survey",
            "key_findings": "Relational/object-oriented representations enable transferring higher-level policies or function approximators across tasks with differing low-level details, reducing sample complexity and risk of negative transfer when mappings are provided or learned.",
            "uuid": "e1048.2",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Source Task Selection",
            "name_full": "Source task selection methods for transfer",
            "brief_description": "Procedures to choose which past tasks/policies to reuse for a new target task, including human-specified feature-based regressors and autonomous ranking methods to predict transfer quality before interacting with the target.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "learning agent with policy library",
            "agent_description": "RL agent that maintains a library of source policies and uses designer-provided task features or autonomous evaluation to rank/prioritize policies for transfer; methods include regression models predicting transfer quality.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "task families within same domain (e.g., variants parameterized by task features)",
            "environment_description": "Sets of related tasks parameterized by observable features (e.g., goal positions, object counts); variation occurs across those parameters while domain remains same.",
            "complexity_measure": "task feature dimensionality, number of source tasks in library, state-action space similarity metrics",
            "complexity_level": "often low-to-medium within same domain assumptions; complexity increases when allowed differences grow",
            "variation_measure": "range and distribution of task-features across source tasks; whether target is within convex hull of source features or not",
            "variation_level": "typically low-to-medium under same-domain assumptions; can be high if arbitrary target tasks considered",
            "performance_metric": "predicted transfer quality, ranking accuracy of source tasks, improvement in jumpstart/total reward when using selected source tasks",
            "performance_value": null,
            "complexity_variation_relationship": "Survey emphasizes that good source selection reduces negative transfer especially when variation between tasks is limited; autonomous selection requires observable task features or trial evaluations; as variation increases, selection becomes harder and risk of negative transfer grows.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "feature-based regression to predict transfer quality, ensemble evaluation of top-performing stored policies on the target, human-assisted feature specification",
            "generalization_tested": true,
            "generalization_results": "Methods like Sinapov et al. (as discussed) can predict transfer quality across same-domain variations and rank source tasks without target samples when task features are provided; autonomy is limited by need for designer-specified features.",
            "sample_efficiency": "improves efficiency by avoiding poor source policies and selecting useful ones; quantitative sample counts not reported in survey",
            "key_findings": "Reliable source selection markedly reduces negative transfer; human-provided task features enable effective prediction but limit autonomy; fully autonomous selection remains challenging when variation is large or features unavailable.",
            "uuid": "e1048.3",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Curriculum Learning (TL)",
            "name_full": "Curriculum learning / progressive difficulty for transfer in RL",
            "brief_description": "Approaches that construct sequences of easier source tasks (a curriculum) to progressively teach an agent and transfer knowledge to tackle harder target tasks, thereby reducing sample complexity for complex target tasks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "learning agent (generic across curriculum methods)",
            "agent_description": "Reinforcement-learning agents trained across a sequence of tasks of increasing difficulty; some approaches require an environment simulator and designer heuristics, others build Curriculum MDPs to automate sequencing.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "simulated task families where environment parameters can be varied (generic)",
            "environment_description": "Complex tasks decomposed into simpler intermediate tasks (e.g., via changing simulator parameters, initial-state proximity to goal, or smaller object sets); complexity increases along curriculum steps.",
            "complexity_measure": "task difficulty (qualitative), state/action space size as objects/obstacles are added, distance from initial state to goal (in initial-state curricula)",
            "complexity_level": "curriculum progresses from low to high complexity (explicitly progressive)",
            "variation_measure": "difference between curriculum steps (parameter change magnitude), whether source tasks cover distribution of target variations; often low variation between consecutive curriculum tasks",
            "variation_level": "typically low between consecutive curriculum steps; overall variation across whole curriculum can be medium",
            "performance_metric": "learning speed (time to threshold), total reward during training, jumpstart, transfer ratio",
            "performance_value": null,
            "complexity_variation_relationship": "Survey reports that breaking a complex task into progressively harder variants reduces effective complexity per learning stage and accelerates learning; curricula assume limited variation between consecutive tasks so that transfer is beneficial; if variation between steps is large, transfer may fail.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "curriculum generation (designer heuristics or Curriculum MDP), progressive transfer of value functions/models/policies, initial-state shaping to start near goals and progressively expand",
            "generalization_tested": true,
            "generalization_results": "Reported qualitative and some quantitative improvements: curricula 'expressively accelerate' learning in surveyed works (e.g., Narvekar et al.); automation of curriculum generation remains active research.",
            "sample_efficiency": "improved sample efficiency relative to direct training on hard tasks; survey does not report exact interaction counts",
            "key_findings": "Curricula reduce sample complexity by ensuring learning occurs on incrementally harder tasks; effectiveness depends on sensible ordering and bounded variation between successive tasks; curricula can be combined with transfer and multiagent parallelization.",
            "uuid": "e1048.4",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "DriftER (strategy-drift detection)",
            "name_full": "DriftER: detecting opponent strategy changes for adaptive transfer",
            "brief_description": "A method that learns a model of an opponent and monitors prediction quality to detect sudden opponent strategy changes (drift); when drift is detected the agent recomputes models and can (in principle) reuse past policies for similar opponents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "agent with opponent-modeling (DriftER-style)",
            "agent_description": "Learns predictive models of opponents and computes optimal policies against the predicted behavior; monitors model prediction accuracy to trigger model recomputation upon drift.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "adversarial multiagent environments with potentially non-stationary opponents",
            "environment_description": "Environments where opponents may change strategy over time; variation manifests as temporal non-stationarity in opponents' policies, complicating transfer of past policies.",
            "complexity_measure": "degree of opponent non-stationarity, difficulty of modeling opponent policy, state-action space size including opponent features",
            "complexity_level": "varies; non-stationarity increases effective complexity",
            "variation_measure": "frequency/magnitude of opponent policy changes (drift), number of distinct opponent strategies encountered",
            "variation_level": "can be high (if opponents adapt/learn) or low (if fixed policies)",
            "performance_metric": "detection speed/accuracy of strategy change, quality of resulting policies after adaptation",
            "performance_value": null,
            "complexity_variation_relationship": "Survey highlights that detecting opponent drift is crucial: when opponent behavior is stable (low variation) policy reuse helps; when variation is high, detection and model retraining are required to avoid negative transfer.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "opponent-model learning + monitoring of prediction quality; trigger recomputation and (optionally) reuse past policies for similar opponent profiles",
            "generalization_tested": null,
            "generalization_results": "Survey reports qualitative capability: DriftER can detect sudden strategy changes, enabling timely model updates; explicit reuse of past policies for new opponents is suggested as an extension but not implemented in the cited work.",
            "sample_efficiency": "improved robustness to non-stationarity by avoiding prolonged use of stale models, quantitative sample counts not provided",
            "key_findings": "Monitoring model prediction quality to detect opponent strategy drift is a practical mechanism to decide when transferred knowledge is no longer valid; combining drift detection with policy reuse could enable faster re-adaptation.",
            "uuid": "e1048.5",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Sinapov-style Source Selection",
            "name_full": "Feature-based regression for predicting transfer quality (source task ranking)",
            "brief_description": "A source selection approach that uses designer-specified task features to train a regressor predicting transfer quality between source and target tasks, enabling ranking of source tasks without target interactions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "agent with transfer-quality regressor",
            "agent_description": "Agent that uses a regression model trained on features of prior tasks to predict how useful each source policy will be for a new target given its features; relies on human-specified, observable task features.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "same-domain task families parameterized by observable features",
            "environment_description": "Tasks vary by parameterized features (e.g., goal locations, object counts); variation is captured by designer-provided features; complexity is from task difficulty and policy differences.",
            "complexity_measure": "dimensionality and range of task features, heterogeneity of source policies",
            "complexity_level": "typically low-to-medium under same-domain assumptions",
            "variation_measure": "coverage and spread of feature values across source tasks; magnitude of feature differences between source and target",
            "variation_level": "low-to-medium if features constrained; can be high if features vary widely",
            "performance_metric": "accuracy of predicted transfer quality and resulting transfer ranking; practical metrics: jumpstart and total reward when chosen source used",
            "performance_value": null,
            "complexity_variation_relationship": "Survey notes that when tasks are in the same domain and feature descriptions are informative, source selection without target samples is feasible; as variation (differences in features) grows, predictive accuracy and transfer utility degrade.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "train regressor on designer-specified task features mapping to measured transfer quality between tasks; rank sources for new target by predicted quality",
            "generalization_tested": true,
            "generalization_results": "Survey reports this method can autonomously select appropriate sources within same domain without target interactions when appropriate features exist; relies on human-provided features.",
            "sample_efficiency": "saves target interactions by avoiding exhaustive evaluation on target; numeric training sample counts for regressor not reported in survey",
            "key_findings": "Feature-based prediction of transfer quality is effective under same-domain assumptions and informative features; dependence on human-specified features limits autonomy and robustness under high variation.",
            "uuid": "e1048.6",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Initial-State Curriculum",
            "name_full": "Initial-state shaping curriculum (start-near-goal progression)",
            "brief_description": "A curriculum technique that eases exploration by first initializing episodes close to target states (near the goal) and progressively moving initial states farther away to cover full task difficulty.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "agent trained with progressive initial-state distribution",
            "agent_description": "Reinforcement-learning agents trained using modified episode initial-state distributions; works when environment supports controlled initializations.",
            "agent_type": "simulated agent / virtual agent",
            "environment_name": "goal-directed tasks where initial-state can be controlled; e.g., navigation or reaching tasks",
            "environment_description": "Complexity relates to distance/difficulty of reaching goal from initial states; variation can be introduced by sampling different initial states or environment configurations.",
            "complexity_measure": "distance from initial state to goal, state-space coverage needed to solve task, number of steps required to reach goal",
            "complexity_level": "progressive (starts low near goal, increases as initial states move farther)",
            "variation_measure": "range of initial-state positions sampled across curriculum; degree of environment modification when moving initial states",
            "variation_level": "typically low between consecutive curriculum steps, overall medium across full curriculum",
            "performance_metric": "learning speed (episodes to achieve competence), total reward, ability to explore full state space after progression",
            "performance_value": null,
            "complexity_variation_relationship": "Survey indicates starting near goal reduces effective complexity and accelerates early learning; progressively increasing complexity by moving starts farther helps the agent generalize to full task while limiting abrupt variation between steps.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "initial-state shaping curriculum: start near goal and progressively expand initial-state distribution",
            "generalization_tested": true,
            "generalization_results": "Reported to improve exploration and accelerate learning in target tasks where applicable; works best where environments support controlled episode initialization.",
            "sample_efficiency": "improves sample efficiency by producing early rewards and shaping exploration; no numeric counts provided",
            "key_findings": "Initial-state curricula are a practical curriculum construction that reduce early exploration burden and enable progressive coverage of complex environments, assuming controllable initializations.",
            "uuid": "e1048.7",
            "source_info": {
                "paper_title": "A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems",
                "publication_date_yy_mm": "2019-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.01765975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems</h1>
<p>Felipe Leno Da Silva<br>F.LENO@USP.BR<br>Anna Helena Reali Costa<br>ANNA.REALI@USP.BR<br>Computer Engineering Department<br>Universidade de So Paulo<br>So Paulo, SP, Brazil.</p>
<h4>Abstract</h4>
<p>Multiagent Reinforcement Learning (RL) solves complex tasks that require coordination with other agents through autonomous exploration of the environment. However, learning a complex task from scratch is impractical due to the huge sample complexity of RL algorithms. For this reason, reusing knowledge that can come from previous experience or other agents is indispensable to scale up multiagent RL algorithms. This survey provides a unifying view of the literature on knowledge reuse in multiagent RL. We define a taxonomy of solutions for the general knowledge reuse problem, providing a comprehensive discussion of recent progress on knowledge reuse in Multiagent Systems (MAS) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not). We aim at encouraging the community to work towards reusing all the knowledge sources available in a MAS. For that, we provide an in-depth discussion of current lines of research and open questions.</p>
<h2>1. Introduction</h2>
<p>Reinforcement Learning (RL) (Sutton \&amp; Barto, 1998; Littman, 2015) has been employed to train autonomous agents for increasingly difficult tasks such as board and video game playing (Tesauro, 1995; Mnih et al., 2015), optimization of treatment policies for chronic illnesses (Shortreed et al., 2011), robotics (Kober, Bagnell, \&amp; Peters, 2013), and DRAM memory control (Barto, Thomas, \&amp; Sutton, 2017). As these agents gain space in the real world, learning how to interact and adapt to other (possibly learning) agents is fundamental to build a robust and reliable system. So far, the multiagent RL community presented perhaps the most expressive progress towards autonomous learning in Multiagent Systems (MAS) (Bazzan, 2014). However, the huge sample complexity of traditional RL methods is a well-known hindrance to apply both single- and multiagent RL in complex problems, and the RL community has devoted much effort on additional techniques to overcome the limitations of RL methods. Transfer Learning (TL) (Taylor \&amp; Stone, 2009) methods propose to alleviate the burden of learning through the reuse of knowledge. The most intuitive way to apply TL to RL is to reuse the solution of previous tasks that have already been presented to the agent (Taylor, Stone, \&amp; Liu, 2007), but many methods also focused on reusing knowledge from external sources, such as demonstrations from humans or advice from other learning agents (Silva, Glatt, \&amp; Costa, 2017).</p>
<p>The literature has shown that the reuse of knowledge might significantly accelerate the learning process. However, unprincipled reuse of knowledge might cause negative transfer, i.e., when reusing knowledge hurt the learning process instead of accelerating it. The key remaining question is how to develop flexible and robust methods to autonomously reuse knowledge for varied applications. TL for single-agent RL has already evolved enough to consistently reuse solutions from different domains (Taylor et al., 2007), autonomously identify and reuse previous solutions (Isele, Rostami, \&amp; Eaton, 2016; Sinapov, Narvekar, Leonetti, \&amp; Stone, 2015), and be usable in complex applications. However, multiagent RL is still struggling to find real-world applications, and many multiagent TL approaches are still validated in simulations of toy problems or require strong human intervention. Nevertheless, the field has been maturing in the past years, pushing the boundaries of knowledge closer to the development of an autonomous agent that can learn faster by reusing its knowledge, observing other agents' actuation, and receiving advice from more experienced agents.</p>
<p>This survey aims at categorizing and discussing the main lines of current research within the Transfer Learning for Multiagent RL area. Our main purpose is to highlight the similarity between those different lines, making it easier to identify crossing points and open problems for which sub-communities could merge their expertise to work on, bridging the gap between the current literature and real-world complex applications.</p>
<h1>1.1 Contribution and Scope</h1>
<p>This survey focuses on Transfer Learning approaches that are explicitly developed to multiagent RL systems or that can be easily applicable to MAS. While we also discuss some methods that have been primarily developed for single-agent TL, the focus of the discussion is on the benefits and/or difficulties in applying these methods to MAS. By multiagent RL we mean domains in which more than one autonomous agent is present and at least one of them (the one reusing knowledge) is applying RL. If one agent is solving a single-agent RL task but interference from other agents during learning is possible (such as providing suggestions when the agent is unsure of what to do), we also consider this as a multiagent task. We contribute a new taxonomy to divide the literature in the area into two main categories (detailed in Section 3). We also categorize the main recent proposals and discuss the particularities of each of them, outlining their contribution to the MAS community and prospects of further developments when possible.</p>
<p>Whether or not a transfer procedure should be considered as multiagent might prompt debates in some situations. We consider that an agent is composed of its own set of sensors, actuators, and a policy optimizer. Therefore, we included here procedures that: (i) are specialized for reusing the agent's own knowledge across tasks that include multiple agents in the environment; or (ii) transfer knowledge from one agent to another during the learning process. Notice that a human providing demonstrations or guidance during learning is considered a multiagent transfer method, but a designer devising a reward function is not, because the information is defined and made available to the agent before learning.</p>
<p>To the best of our knowledge, this is the first survey focused on TL for multiagent RL. Taylor and Stone (2009) provide a comprehensive survey on TL techniques (primarily focused on single-agent RL), but many new approaches have arisen since that survey was published. Lazaric (2012) surveys TL approaches in less depth, proposing a taxonomy to</p>
<p>divide the field (also focused on single-agent RL). A handful of surveys focus on multiagent RL without emphasis on TL. Two major examples are the surveys by Busoniu et al. (2008) and Stone and Veloso (2000). While most of the Learning from Demonstration and Inverse Reinforcement Learning techniques fall within the scope of this survey, Argall et al. (2009) and Zhifei and Joo (2012) already provided comprehensive surveys on those topics. Therefore, we here restrict our analysis to more recent and relevant publications on those areas for avoiding repetitions in discussions already provided by these previous surveys. In this paper, we also contribute in-depth discussions of the publications, relating them to other paradigms in Multiagent Learning. A small portion of this survey was already presented at a conference in the form of a mini-survey (Silva, Taylor, \&amp; Costa, 2018).</p>
<p>Figure 1 further illustrates the scope of this survey, which lies at the intersection of RL, TL and multiagent learning.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the scope of this survey. TL for multiagent RL lies at the intersection of RL, TL, and multiagent learning.</p>
<h1>1.2 Overview</h1>
<p>This section explains the structure of this survey. In Section 2 we present the foundations of single- and multiagent RL, showing how TL improves RL algorithms and presenting some established metrics to evaluate transfer. In Section 3, we present our proposed taxonomy, explaining how we group and categorize proposals. In Sections 4 and 5, we list, explain, and discuss the surveyed published literature, where each of the sections is devoted to one category. Section 6 depicts simulated and real-world domains that have been used for evaluating methods. In Section 7, we present our view on the promising open questions in the area that need further investigation. Section 8 presents pointers for conferences, journals, and code libraries that might be of interest to readers. Finally, Section 9 concludes the survey.</p>
<h1>2. Background</h1>
<p>In this section, we review the necessary background on single-agent and multiagent RL, together with the main concepts of TL. We first present the single-agent case, then proceeding to MAS and representative solutions. Finally, we present the basics on TL.</p>
<h3>2.1 Single-Agent RL</h3>
<p>RL is a possible solution for Markov Decision Processes (MDP) (Puterman, 2005), which is a popular model for sequential decision-making problems.</p>
<p>An MDP is a tuple $\langle S, A, T, R, \gamma\rangle$, where:</p>
<ul>
<li>$S$ is the (possibly infinite) set of environment states. A common and convenient way to describe states is using a factored description, defining state variables and building states according to their values. An initial state distribution function $S_{0}: S \rightarrow[0,1]$ defines the probability of starting in each state when the task restarts (i.e., a new episode begins);</li>
<li>$A$ is the set of available actions to the agent;</li>
<li>$T: S \times A \times S \rightarrow[0,1]$ is a stochastic state transition function, where the state is transitioned to state s' with a probability $0 \leq p \leq 1$ when applying action $a$ in state $s$. We denote as $s^{\prime} \leftarrow T(s, a)$ drawing a sample of next state from $T$;</li>
<li>$R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function; and</li>
<li>$\gamma \in[0,1)$, is the discount factor, which represents the relative importance of future and present rewards.</li>
</ul>
<p>At each step, the reasoning agent observes the current state $s \in S$. Then, it chooses an action $a$ among the applicable ones in $s$, causing a state transition $s^{\prime} \leftarrow T(s, a)$. After each action, the agent receives a reward signal $r \leftarrow R\left(s, a, s^{\prime}\right)$, that represents the quality of the executed action towards the task solution. In learning problems, the functions $T$ and $R$ are unknown to the agent, hence a proper actuation must be induced through the observation of $\left\langle s, a, s^{\prime}, r\right\rangle$ tuples, gathered through interactions with the environment. The goal of the learning agent is to induce a policy $\pi: S \rightarrow A$, that maps an action to be applied in each possible state.</p>
<p>A possible way to learn a good policy is by iteratively updating an estimate of action qualities $Q: S \times A \rightarrow \mathbb{R}$ after each interaction with the environment. Several algorithms (e.g., Q-Learning) are proved to eventually learn the true $Q$ function under non-restrictive assumptions ${ }^{1}$ :</p>
<p>$$
Q^{*}(s, a)=\underset{s, a}{\mathbb{E}}\left[\sum_{k=0}^{\infty} \gamma^{k} r_{k}\right]
$$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where $r_{k}$ is the reward received after $k$ steps from using action $a$ in state $s$ and following the optimal policy on all subsequent steps, and $\gamma$ is a discount factor that codifies the horizon in which the rewards matter. Therefore, an optimal policy $\pi^{*}$ is one that maximizes the expected sum of discounted rewards in every possible state. For the Q-Learning algorithm, $Q$ is updated after each applied action by:</p>
<p>$$
Q_{k+1}\left(s_{k}, a_{k}\right) \leftarrow(1-\alpha) Q_{k}\left(s_{k}, a_{k}\right)+\alpha\left(r_{k}+\gamma \max <em k="k">{a} Q</em>, a\right)\right)
$$}\left(s_{k+1</p>
<p>where $r_{k}=R\left(s_{k}, a_{k}, s_{k+1}\right), 0&lt;\alpha \leq 1$ is the learning rate and $\gamma$ is the discount factor.
After learning $Q^{*}$, the agent can use it to define an optimal policy:</p>
<p>$$
\pi^{<em>}(s)=\operatorname{argmax}_{a} Q^{</em>}(s, a)
$$</p>
<p>Notice that MDPs take into account only one agent in the environment. Although it is possible to ignore other agents to learn in a MAS as if it was a single-agent problem (Tan, 1993), the actions of one agent often have influence in the local state and/or reward of the others. Thus, for most cases, coordination is desired.</p>
<h1>2.2 Multiagent RL</h1>
<p>MDPs are extended to MAS as Stochastic Games (SG) (Busoniu et al., 2008; Bowling \&amp; Veloso, 2000). As more than one agent is now present in the environment, an SG is composed of $\left\langle S, U, T, R_{1 \ldots n}, \gamma\right\rangle$, where:</p>
<ul>
<li>$n$ is the number of agents;</li>
<li>$S$ is the state space. Each state is composed of local states from each agent plus a local state $S_{0}$ for the environment (not related to any agent in particular): $S=$ $S_{0} \times S_{1} \times \cdots \times S_{n}$, hence full observability is usually assumed in this formulation;</li>
<li>$U$ is the joint action space, composed of local actions for all the agents in the MAS: $U=A_{1} \times \cdots \times A_{n}$. Depending on the problem to be solved, the actions of other agents might be visible or not;</li>
<li>$T: S \times U \times S \rightarrow[0,1]$ is the state transition function, which in MAS depends on joint actions instead of local individual actions;</li>
<li>$R_{i}: S \times U \times S \rightarrow \mathbb{R}$ is the reward function of agent $i$, which is now dependent on the state and joint actions; and</li>
<li>$\gamma$ is the discount factor.</li>
</ul>
<p>Since each agent has its own reward function, that is dependent on the other agents, there is not a clear concept defining an optimal policy as in single-agent problems. Simply applying the actions that maximize the local reward may be ineffective if the agents have different reward functions, as one agent might (consciously or not) hamper the performance of another.</p>
<p>Depending on the problem to be solved, the agents can learn as in an MDP ignoring the others (Tan, 1993), try to learn an equilibrium joint policy (Hu, Gao, \&amp; An, 2015b,</p>
<p>2015a), or maximize a single common reward (Panait \&amp; Luke, 2005). If all agents share a single reward $R_{1}=\cdots=R_{n}$, it is possible to build a central controller that designates actions to each agent by learning through samples of $\left\langle s, \boldsymbol{u}, s^{\prime}, r\right\rangle$. However, this solution is unfeasible for most domains because of the requirements in communication and the huge state-action space for the learning problem. The Distributed Q-Learning algorithm (Lauer \&amp; Riedmiller, 2000) was proposed to solve such problems in a more scalable way. Each agent learns without observing the actions of the others. However, this algorithm is only applicable in tasks with deterministic transition functions, which is rarely the case for complex tasks.</p>
<p>Equilibrium-based approaches aim at solving the learning problem when agents might have different rewards. For those algorithms, $Q$-table updates rely on the computation of an equilibrium metric (Hu, Gao, \&amp; An, 2015c):</p>
<p>$$
Q_{k+1}^{i}\left(s_{k}, \boldsymbol{u}<em k="k">{\boldsymbol{k}}\right) \leftarrow(1-\alpha) Q</em>}^{i}\left(s_{k}, \boldsymbol{u<em k="k">{\boldsymbol{k}}\right)+\alpha\left(r</em>\right)\right)
$$}^{i}+\gamma \Phi^{i}\left(s_{k+1</p>
<p>where $Q_{k+1}^{i}$ is a Q-table related to agent $\mathrm{i}, \alpha$ is the learning rate, and $\Phi^{i}$ is the expected equilibrium value in state $s_{k+1}$ for agent $i$. Equation (4) requires the definition of an equilibrium metric, such as the Nash Equilibrium (Hu \&amp; Wellman, 2003). Therefore, such algorithms are closely related to the Game Theory area, from which efficient equilibrium metrics can be extracted (Sodomka et al., 2013). The Learning with Opponent-Learning Awareness (LOLA) (Foerster et al., 2018) algorithm follows a similar procedure. Assuming that the other agents in the system are also learning, local policy updates are performed already predicting the policy update of other agents.</p>
<p>Another popular setting is adversarial learning, where the agent has an opponent with diametrically opposed goals. In this case, the optimal policy consists of selecting the action that maximizes the reward supposing that the opponent selected the best action for itself (that is, maximizing the minimum possible return of the actions). For that, the MinMax Q-Learning algorithm (Littman, 1994) can be used, which updates the Q-table as:</p>
<p>$$
Q_{k+1}\left(s_{k}, a_{k}, o_{k}\right) \leftarrow(1-\alpha) Q_{k}\left(s_{k}, a_{k}, o_{k}\right)+\alpha\left(r_{k}+\gamma \max <em o="o">{o} \min </em>, a, o\right)\right)
$$} Q_{k}\left(s_{k+1</p>
<p>where $o_{k}$ is the action selected by the opponent at step $k$ and $o$ is an action that might be selected by the opponent.</p>
<p>More recently, Lowe et al. (2017) proposed a method especially focused on coordination of multiagent Deep RL problems. In their method, the agents are trained in a centralized setting, where they learn value function estimates taking into account the actuation of other agents. After the training phase, the agents are able to execute the learned policy in a decentralized manner (i.e., using only local observations). Their method was able to handle the non-stationarity of other agents in some problems and achieved convergence where classical solutions failed.</p>
<p>In many MAS applications, each agent might be required to cooperate with a group of agents, while competing against an opposing group. Equilibrium-based solutions are able to generalize to this setting, but it is also possible to treat the opposing team as part of the environment and learn only how to cooperate with teammates.</p>
<p>Although those solutions achieved successes, they all require a huge amount of interactions with the environment for achieving a good performance, rendering them hard to scale. Other recent approaches to learn in multiagent RL usually build upon the algorithms discussed in this section to better deal with specific problems, such as non-stationarity (Hernandez-Leal et al., 2017a), still maintaining their scalability problems.</p>
<p>Some models and strategies have been specifically proposed to improve in this direction. Dec-SIMDPs (Melo \&amp; Veloso, 2011) assume that agent interactions only matter in specific parts of the state space, which means that agents must coordinate in a few states contained in $S$. Agents act as in a single-agent MDP in the remaining states. CQ-Learning (De Hauwere, Vrancx, \&amp; Now, 2010) uses the same idea, finding the states in which coordination is needed through a statistical test. Modeling the task with relational representations is also possible to find commonalities and accelerate learning through abstraction of knowledge (Croonenborghs et al., 2005; Silva et al., 2019).</p>
<p>However, one of the most successful strategies for accelerating learning is reusing previous knowledge. In the next section, we formalize and discuss how knowledge reuse is applicable to RL agents.</p>
<h1>2.3 Transfer Learning</h1>
<p>Although learning a task from scratch using RL takes a very long time, reusing existent knowledge may drastically accelerate learning and render complex tasks learnable. As explained in Section 2.2, the learning problem consists of mapping a knowledge space $\mathscr{K}$ to a policy $\pi \in \mathscr{H}$, where $\mathscr{H}$ is the space of possible policies that can be learned by the agent algorithm $\mathscr{A}, \mathscr{A}: \mathscr{K} \rightarrow \mathscr{H}$ (Lazaric, 2012). When learning from scratch, $\mathscr{K}$ consists of samples of interactions with the environment. However, additional knowledge sources might be available. One or more agents ${ }^{2}$ may be willing to provide further guidance to the learning agent, such as providing demonstrations of how to solve the problem or explicitly communicating models of the problem or environment. Therefore, the knowledge space is often not only composed of samples from the current (target) task $\mathscr{K}^{\text {target }}$, but also of knowledge derived from the solution of previous (source) tasks $\mathscr{K}^{\text {source }}$ and from communicating with or observing other agents $\mathscr{K}^{\text {agents }}$. Hence, in the general case $\mathscr{K}=$ $\mathscr{K}^{\text {target }} \cup \mathscr{K}^{\text {source }} \cup \mathscr{K}^{\text {agents }}$.</p>
<p>In order to reuse knowledge, it is necessary to decide when, what, and how to store knowledge into $\mathscr{K}$ and reuse it (Pan \&amp; Yang, 2010). Those three questions are hard and long-studied research problems themselves, and there is no single solution valid for all domains. Unprincipled transfer might cause the agent to reuse completely unrelated knowledge, often hampering the learning process instead of accelerating it (known as negative transfer). Therefore, the literature considered many ways to store and reuse (hopefully only) useful information, following varied representations and assumptions.</p>
<p>In general, the main goal of reusing knowledge is to accelerate learning. Whether or not a single-agent transfer algorithm learns faster than another is commonly evaluated through several of the following performance metrics, summarized by Taylor and Stone (2009) and illustrated in Figure 2.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of transfer performance metrics.</p>
<ul>
<li>Jumpstart: Measures the improvement in the initial performance of the agent. Since a transfer algorithm might reuse knowledge in $\mathscr{K}^{\text {source }}$, successful transfer procedures might initiate learning in a higher performance than when learning from scratch.</li>
<li>Asymptotic Performance: Agents might be unable to reach the optimal performance in complex tasks, converging to a suboptimal policy. TL might help the agents to reach a higher performance, improving their asymptotic performance.</li>
<li>Total Reward: In most transfer settings (either when reusing $\mathscr{K}^{\text {source }}, \mathscr{K}^{\text {agents }}$, or both of them), the transfer algorithm is expected to improve the total reward received during training (i.e., the area under the curve of rewards received during the training process).</li>
<li>Transfer Ratio: The ratio between the total rewards received by the two algorithms under comparison (e.g., with and without transfer).</li>
<li>Time to Threshold: For domains in which the agents are expected to achieve a fixed or minimal performance level, the learning time taken for achieving it might be used as a performance metric.</li>
</ul>
<p>While all those metrics are also generally applicable to MAS, other specific issues must also be taken into account. Communication is a scarce resource in most multiagent applications. Therefore, the overhead in communication demanded by the transfer method cannot be ignored, as well as the computational complexity and more subjective issues such as the restrictions imposed by the assumptions of the transfer method. For example, if a transfer learning method results in a small improvement in performance by the cost of requiring a much higher communication complexity, could this method be considered as effective? The trade-off of all those metrics is usually carefully analyzed in a domain-specific manner. The development of better and more comprehensive transfer metrics, both single- and multiagent, is currently an open subject for research (further discussed in Section 7.2).</p>
<p>We here categorize the main lines of research on Transfer Learning for MAS, discussing representative proposals and their contributions to the area. In the next section, we describe our proposed taxonomy and the dimensions of classification used in this survey.</p>
<h1>3. Proposed Taxonomy</h1>
<p>As explained in Section 2.3, TL relies on the reuse of previous knowledge, that can come from various sources. Even though an agent could reuse knowledge from multiple sources simultaneously, in practice the current methods usually focus on a single knowledge source. Hence, we here propose to divide the current literature into two main groups in which we can fit all the TL for MAS publications so far. The methods differ mainly in terms of the knowledge source, availability, and required domain knowledge. We consider the following types of transfer:</p>
<ul>
<li>Intra-Agent Transfer - Reuse of knowledge generated by the agent in new tasks or domains. An Intra-Agent Transfer algorithm has to deal with: (i) Which task among the solved ones is appropriate?; (ii) How are the source and target tasks related?; and (iii) What to transfer from one task to another? The optimal answer for those three questions is still undefined. Hence, in practice, usually only one of those aspects is investigated at each time, which means that a real-world application would need to consistently combine methods from several publications. We here characterize IntraAgent methods as the ones that do not require explicit communication for accessing internal knowledge of the agents. For example, the procedure of transferring all data from one robot to another similar physical body can be considered as an Intra-Agent method. However, if this same information is shared with another agent with an identical body, and this agent tries to merge the transferred knowledge with its own experience, then this method qualifies as an Inter-Agent Transfer. The papers of interest for this survey are the ones that are specialized to multiagent RL (that is, assume that there is at least one opponent/teammate in the environment), or that could be easily adapted for this purpose.</li>
<li>Inter-Agent Transfer - Even though the literature on TL for single-agent RL is more closely related to multiagent Intra-Agent Transfer methods, a growing body of methods focuses on investigating how to best reuse knowledge received from communication with another agent, which has different sensors and (possibly) internal representations. The motivation for this type of transfer is clear: if some knowledge is already available in another agent, why waste time relearning from scratch? However, defining when and how to transfer knowledge is not a trivial task, especially if the agents follow different representations. Some methods focus on how to effectively insert human knowledge in automated agents, while others on how to transfer between automated agents. Nevertheless, comprehensive methods would treat any agent equally regardless of their particularities. Some examples of algorithms within this group are the ones derived from Imitation Learning, Learning from Demonstrations, and Inverse Reinforcement Learning.</li>
</ul>
<p>In addition to categorizing the papers, we also classify them in several dimensions, according to their applicability, autonomy, and purpose.</p>
<p>The current literature does not offer a method capable of automatically performing all the necessary steps to transfer knowledge in a MAS (both for intra- and inter-transfer). For this reason, most methods focus on a specific subset of problems. Figure 3 illustrates how we divide the literature. We use these groupings when discussing the published proposals, and each of them will be explained in more detail in Sections 4 and 5.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of our classification of the current literature on TL for multiagent RL. Groups to the left are more general and contain the groups to the right.</p>
<p>Notice that the classification given here is not rigid, as many methods share properties with more than one of the categories. Rather than giving a definitive categorization, we here focus on grouping similar approaches to facilitate the analysis of the literature. We list representative papers from recent years in all the surveyed research lines. We also include older papers proposing ideas that are distinct from the recent lines on the literature and, we believe, are not fully exhausted yet.</p>
<p>In the following subsections, we first discuss the nomenclatures used in this survey and then explain all the considered dimensions for paper classification.</p>
<h1>3.1 Nomenclatures</h1>
<p>The literature on knowledge reuse has a myriad of terminologies for closely related concepts. Sometimes, multiple terms have the same meaning or the same term is used inconsistently in different publications. In order to avoid confusion, we here discuss our adopted terminology. We do not aim at creating a standard jargon to be followed by the whole community, but rather to make clear the distinctions and similarities between the discussed methods.</p>
<p>The literature refers to slightly different settings of the knowledge reuse problem under different names. Transfer Learning (Taylor \&amp; Stone, 2009) is sometimes used to refer to the problem of reusing knowledge across two or more predefined tasks (the target task has as $\mathscr{K}^{\text {source }}$ the knowledge gathered in previous tasks). Multi-task Learning (Fernndez \&amp; Veloso, 2006) consists of learning multiple tasks at the same time while exploiting similarities between them to reduce the total learning time ( $\mathscr{K}^{\text {source }}$ is constantly refined with knowledge from the other tasks). Lifelong Learning (Thrun \&amp; Mitchell, 1995) aims at consistently reusing knowledge across several tasks that might be presented to the agent during its lifespan ( $\mathscr{K}^{\text {source }}$ grows over time). Zero-Shot Learning (Isele et al., 2016) tries to reuse knowledge across tasks without training in the target task (i.e., $\mathscr{K}^{\text {target }}=\emptyset$ ). Learning from Demonstrations (Argall et al., 2009) focuses on the transfer of knowledge between agents through explicit communications ( $\mathscr{K}^{\text {agents }}$ contains demonstrations given by other agents). Inverse Reinforcement Learning (Zhifei \&amp; Joo, 2012) consists of an agent trying to learn a task without access to reward samples, and for that, it tries to estimate a reward function, usually by one agent providing samples of a good policy to another agent. We here use Transfer Learning referring to the general challenge of reusing, combining, and adapting knowledge from different sources (agents and tasks).</p>
<p>Regarding the transfer of knowledge between agents, we here adopt the following nomenclatures (first the agent communicating knowledge then the agent receiving it):</p>
<ul>
<li>Advisor/Advisee: We use this terminology when one agent provides advice to another. Here, the knowledge is transferred through explicit communication, but no assumption is made regarding the internal representation or algorithm of other agents. Usually the advisor observes the advisee's state and provides information that is expected to be useful in the current situation (e.g., action suggestions). The advising relation might be initiated either by the advisee through a help request or by the advisor when the advice is expected to be most useful. Optionally, advising might be initiated only when both advisor and advisee agree on initiating the knowledge exchange.</li>
<li>Teacher/Student: A teacher agent also transfers knowledge to a student through explicit communication. However, here assumptions about the representation or algorithm might be available. Therefore, more information might be possibly communicated, such as value function estimates, models, rules, or demonstrations of some steps following the optimal policy. The received information might refer to the entire state-action space, and not necessarily only to the current situation.</li>
<li>Mentor/Observer: Here, an observer agent tries to imitate successful actuation performed by a mentor agent. The mentor might be aware or not of the observer, but no explicit communication happens. Therefore, the observer must use its own sensors to implicitly extract knowledge by observing the mentor.</li>
</ul>
<h1>3.2 Learning Algorithm (LA)</h1>
<p>As explained in Section 2, many MAS algorithms are specialized to a subset of problems (for example, being applicable only to adversarial settings). As some TL methods are associated with the base RL algorithm, they are susceptible to limitations determined by the RL algorithm. We classify LA in one of the following categories:</p>
<ul>
<li>Self-Interested $(\mathcal{S})$ : Those agents apply single-agent RL algorithms taking into account the local state and actions sets. The other agents and their influence are considered as part of the environment dynamics.</li>
<li>Equilibrium $(\mathcal{E})$ : Equilibrium-based RL algorithms seek for an equilibrium in the reward functions, usually using game-theoretic approaches to solve the problem. Some TL approaches are based on strategies to reduce the computation of equilibrium metrics, and thus are usable only with these base RL algorithms.</li>
<li>Adversarial $(\mathcal{A})$ : Many RL algorithms are specialized to adversarial domains (commonly in a form of zero-sum games). Because of that, some methods have focused on accelerating learning for such adversarial problems.</li>
<li>Cooperative $(\mathcal{C})$ : Fully cooperative algorithms assume that all involved agents are benevolent (that is, will never purposely hamper the performance of the system). Therefore, all agents have a common goal. Although not valid for all applications, cooperative algorithms have been broadly used for many problems, especially when the MAS is built by a single owner.</li>
</ul>
<h1>3.3 Source Task Selection (ST)</h1>
<p>When the agent already has a library of previous solutions and intends to reuse the gathered knowledge in new tasks, a source task selection procedure must be carried out. Since selecting unrelated source tasks may lead to negative transfer, choosing the correct source task(s) is important to successfully reuse knowledge. The source task selection procedure can be trivially carried out by relying on human intuition to manually select it. However, autonomy is desired for many applications, and human intuition may be deceitful as human "sensors" are different from the agent's. We classify Intra-agent Transfer methods in one of the following categories:</p>
<ul>
<li>Implicit $\left(\mathcal{X}\right)$ - The approach is only usable inside the same domain (e.g., same action and state spaces with possibly varying goals). Therefore, the source task(s) are commonly reused without concerns in regard to negative transfer.</li>
<li>Human-Specified $\left(\mathcal{H}_{a}\right)$ - The agent considers that the source task is manually selected and given as an input to the transfer method, hence a human user/designer is required to perform source task selection.</li>
<li>Human-Assisted $\left(\mathcal{H}_{a}\right)$ - A human provides some information to help estimate task similarity (e.g. task parameters). The source task selection is then performed automatically based on this information.</li>
<li>Autonomous $(\mathcal{A})$ - The agent estimates task similarity without additional information, and autonomously select the most promising source task(s).</li>
</ul>
<h3>3.4 Mapping Autonomy (MA)</h3>
<p>After the source task has been selected, the agent must estimate in which aspects one task is similar to the other. For most applications, applying the entire policy from the source</p>
<p>task is either impossible (if the target task has a different state-action space) or will result in a suboptimal actuation. In practice, it is usually more effective to reuse portions of previous solutions but identifying in which aspects the two tasks differ is not a trivial task. We classify the methods in regard to MA in:</p>
<ul>
<li>Implicit $(\mathcal{X})$ - The approach is only usable for transfer in the same domain, hence the mapping is straightforward.</li>
<li>Human-Specified $\left(\mathcal{H}_{s}\right)$ - A human gives an explicit and detailed mapping relating the two tasks.</li>
<li>Human-Assisted $\left(\mathcal{H}_{a}\right)$ - A human provides some information to help with the mapping. Some examples of such information are relational descriptions or task parameterizations that might help to relate the tasks.</li>
<li>Learned $(\mathcal{L})$ - The agent creates an automatically generated mapping. This mapping is usually created by estimating a model for each source task and comparing them to a similar one in the target task.</li>
</ul>
<h1>3.5 Transferred Knowledge (TK)</h1>
<p>Defining what to transfer from one task to another is not trivial. As the best transferable information depends on the setting, there is not a single transfer method which is valid for all situations. For example, if one agent transfers information to another and they have different representations, transferring internal structures may be ineffective or impossible, but if the system is composed of homogeneous agents, this same transfer method may be effective. Due to the myriad of settings in which TL has been applied, several possible types of knowledge transfer have been proposed. We classify surveyed proposals in the following categories:</p>
<ul>
<li>Action Advice $\left(\mathcal{A}_{a}\right)$ - If a group of agents has a common understanding of observations and actions, it is possible for them to communicate action suggestions. Usually, one advisee agent communicates its observations to an advisor, which can communicate one action to be applied by the advisee in the environment.</li>
<li>Value Functions $\left(\mathcal{V}_{f}\right)$ - If the agent applies a learning algorithm that uses estimates of value functions to derive policies, it is possible to reuse these estimates across tasks or communicate them to another agent. However, value functions are very specialized for the current task and hard to adapt to similar (yet different) tasks.</li>
<li>Reward Shaping $\left(\mathcal{R}_{s}\right)$ - Reward Shaping consists of modifying the reward signal received from the environment using additional information to make it more informative to the agent. This information can be originated in a previous task or received from another agent (e.g., another signal received from a human supervisor).</li>
<li>
<p>Policy $(\pi)$ - Task solutions might be transferred across tasks if they are similar enough. Another alternative is to transfer portions of the policy (often called as options, partial policies, or macroactions) to benefit from similarities on parts of the tasks.</p>
</li>
<li>
<p>Abstract Policy $\left(\pi_{a}\right)$ - If a relational description is available, an abstracted version of the task solution might be transferred across tasks or between agents. Abstract policies are more general and easier to adapt to possible differences between tasks.</p>
</li>
<li>Rules $(\mathcal{R})$ - In addition to being human-readable, rules are easily derived from experience by humans. For that reason, rules have been used to transfer knowledge from humans to automated agents. However, autonomously adapting rules to new tasks is not easy.</li>
<li>Experiences $(\mathcal{E})$ - As explained in Section 2.1, RL agents learn through samples of $\left\langle s, a, s^{\prime}, r\right\rangle$ tuples. Those samples can be directly transferred between agents or across tasks. The agent might have to adapt those highly-specialized samples to another task and/or set of sensors though.</li>
<li>Models $(\mathcal{M})$ - During learning, the agent can build models to predict the behavior of other agents or characteristics of the task (such as transition and reward functions). Those models can be reused in new tasks or transferred to other agents if they are able to understand it.</li>
<li>Heuristics $(\mathcal{H})$ - Random exploration (e.g., $\epsilon$-greedy) is very common for RL agents. However, if a heuristic is available, the agent can make use of it to perform a smarter exploration. Heuristics have been extracted from other agents (most commonly humans) and from previous tasks.</li>
<li>Action Set $(\mathcal{A})$ - For problems in which a large number of actions is available, learning the subset of relevant actions for a problem and transferring it to another might be an interesting way to accelerate learning.</li>
<li>Function Approximators $\left(\mathcal{F}_{a}\right)$ - Building an explicit table listing all possible quality values for the entire state-action state is often infeasible (and impossible if continuous state variables exist). In this case, a function approximator is usually iteratively refined after each interaction with the environment. They can also be later reused for new tasks or communicated to other agents, but again they are highly-specialized for the task at hand.</li>
<li>Bias $(\mathcal{B})$ - Instead of reusing the whole policy, the agent might bias the exploration of actions selected by the optimal policy in a previous task. Biases are easier to forget because they quickly lose influence if a bad action is selected.</li>
<li>Curriculum $(\mathcal{C})$ - For many complex tasks, it might be more efficient to divide the complex task into several simpler ones. If appropriate task decomposition and order are available, the agent might be able to learn faster by applying TL from the simpler to the target task.</li>
</ul>
<h1>3.6 Allowed Differences (AD)</h1>
<p>Assumptions must be made in regard to in which aspects one task may differ from the other. While the simpler TL algorithms assume that the target task is a version of the source task</p>
<p>with a bigger state space, ideally the TL method should allow differences in any element of the MDP/SG description and be able to identify in which aspects the two tasks are similar. In practice, how to identify task similarities is still an open problem, hence we classify the proposals in the following categories:</p>
<ul>
<li>Same Domain $(\mathcal{S})$ - Assumes that the target and source tasks have roughly the same difficulty and are in the same domain (for example, a navigation domain in which only the goal destination can change).</li>
<li>Progressive Difficulty $(\mathcal{P})$ - Assumes that the target task is a harder version of the source task in the same domain, usually with a bigger state-action space due to the inclusion of new objects in the environment, but without significant changes in the transition and reward functions.</li>
<li>Similar Reward Function $\left(\mathcal{R}_{f}\right)$ - Assumes that the reward function remains constant or that the optimal policy in the source task still achieves a reasonable performance in the target task. Unlike in same domain, here the target task might have a bigger state-action space, as well as different state variables or actions (possibly requiring mappings).</li>
<li>Any $(\mathcal{A})$ - Any aspect of the task might possibly change. The agent must autonomously assess the similarities and discard the source tasks that would result in a negative transfer.</li>
</ul>
<p>Table 1 summarizes all the abbreviations to be used hereafter.
Table 1: Quick-Reference legend for abbreviations used in Tables 2, 3, and 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning Algorithm (LA)</th>
<th style="text-align: center;">Source Task Selection (ST)</th>
<th style="text-align: center;">Mapping Autonomy (MA)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{S}$ : Self-Interested</td>
<td style="text-align: center;">$\mathcal{X}$ : Implicit</td>
<td style="text-align: center;">$\mathcal{X}$ : Implicit</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{E}$ : Equilibrium</td>
<td style="text-align: center;">$\mathcal{H}_{s}$ : Human-Specified</td>
<td style="text-align: center;">$\mathcal{H}_{s}$ : Human-Specified</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{A}$ : Adversarial</td>
<td style="text-align: center;">$\mathcal{H}_{a}$ : Human-Assisted</td>
<td style="text-align: center;">$\mathcal{H}_{a}$ : Human-Assisted</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{C}$ : Cooperative</td>
<td style="text-align: center;">$\mathcal{A}$ : Autonomous</td>
<td style="text-align: center;">$\mathcal{L}$ : Learned</td>
</tr>
<tr>
<td style="text-align: center;">Transferred Knowledge (TK)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Allowed Differences (AD)</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{A}_{a}$ : Action Advice</td>
<td style="text-align: center;">$\mathcal{V}_{f}$ : Value Functions</td>
<td style="text-align: center;">$\mathcal{S}$ : Same Domain</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{R}_{s}$ : Reward Shaping</td>
<td style="text-align: center;">$\pi$ : Policies</td>
<td style="text-align: center;">$\mathcal{P}$ : Progressive Difficulty</td>
</tr>
<tr>
<td style="text-align: center;">$\pi_{a}$ : Abstract Policies</td>
<td style="text-align: center;">$\mathcal{R}$ : Rules</td>
<td style="text-align: center;">$\mathcal{R}_{f}$ : Sim. Reward Func.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{E}$ : Experiences</td>
<td style="text-align: center;">$\mathcal{M}$ : Models</td>
<td style="text-align: center;">$\mathcal{A}$ : Any</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{H}$ : Heuristics</td>
<td style="text-align: center;">$\mathcal{A}$ : Action Sets</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{B}$ : Biases</td>
<td style="text-align: center;">$\mathcal{F}_{a}$ Function Approximators</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{C}$ : Curricula</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>4. Intra-Agent Transfer Methods</h1>
<p>In this section we survey Intra-Agent transfer methods. Table 2 summarizes the main discussed publications. As discussed in Section 1.1, our definition of MAS included systems</p>
<p>in which an agent (possibly human) is communicating with the learner and influencing its learning process, even if this agent is not directly actuating in the environment. For readers interested only in the case where multiple agents are simultaneously applying actions in the environment, we mark with $*$ the publications that have only one agent directly changing the environment in their experimental evaluation. Notice though that many of their proposals are directly applicable in MAS or would need straightforward adaptations. In the next subsections, we group proposals by their main contributions and, for all the groups discussed, we provide an overview of the current literature followed by a detailed description of the representative proposals.</p>
<h1>4.1 Adapting to Other Agents</h1>
<p>Learning in MAS requires acquiring the ability to coordinate with (or adapt against) other agents. Depending on the task to be solved, agents might have to collaborate with teammates following unknown (and possibly adaptable) strategies, or the environment might allow additions or substitutions of agents at any time (Stone et al., 2010). Nonetheless, the agent still has to cope with the diversity of strategies assumed by other agents and learn how to coordinate with each of them. Therefore, some TL methods focus on reusing experience for learning how to coordinate with new agents faster. In this section, we discuss representative methods on how previous knowledge can be leveraged for accelerating coordination with other agents.</p>
<p>As shown in Table 2, this group of methods usually assumes that the source task selection is implicit. Very rarely (as in the case of (Kelly \&amp; Heywood, 2015)), the agent is able to adapt to new tasks, but they are still assumed to be very similar to previous tasks. The main reason for that is the difficulty of the current literature on identifying when and how the strategy of other agents changed, which makes unfeasible to simultaneously account for significant changes in the task. All types of learning algorithms have been explored by this group of methods, and the most common transfer procedure is the reuse of policies for adapting to new agents in the MAS.</p>
<p>Banerjee et al. (2007) propose a TL method to reuse knowledge between similar General Game Playing tasks. The agent builds trees relating game-independent features to possible game outcomes. Those trees can then be used to match portions of the state space in the source and target tasks, enabling the reuse of value functions. Their procedure achieved good results when reusing knowledge between crossing-board games, but it is applicable in quite restrictive situations, as the method is only employable when the opponent follows a non-adaptive and fixed policy for all games. In addition, the human designer is responsible for ensuring that the game's features are applicable and have the same semantic meaning in all games.</p>
<p>Barrett and Stone (2015) deal with the challenge of adapting the actuation of a learning agent to different configurations of teams (Stone et al., 2010). Their method assumes that a set of good policies is available to the agent, which must choose the most appropriate one to cooperate with an (initially) unknown team. For that, a set of beliefs is stored and iteratively updated to predict the expected loss of applying each of the policies according to the profile of the current team. Those beliefs are then used to select the most appropriate policy, and</p>
<p>Table 2: Summary of main recent trends of Intra-Agent Transfer methods. We follow the symbols introduced in Section 3 and compiled in Table 1 for quick reference. The publications are presented in chronological order within their group. Papers denoted with * consider settings where only one agent is directly affecting the environment.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reference</th>
<th style="text-align: center;">LA</th>
<th style="text-align: center;">ST</th>
<th style="text-align: center;">MA</th>
<th style="text-align: center;">TK</th>
<th style="text-align: center;">AD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Adapting to Other Agents (Section 4.1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Banerjee \&amp; Stone, 2007)</td>
<td style="text-align: center;">$\mathcal{A}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{h}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{R}_{f}$</td>
</tr>
<tr>
<td style="text-align: center;">(Barrett \&amp; Stone, 2015)</td>
<td style="text-align: center;">$\mathcal{C}, \mathcal{A}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{L}$</td>
<td style="text-align: center;">$\pi$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Kelly \&amp; Heywood, 2015)</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\pi$</td>
<td style="text-align: center;">$\mathcal{R}_{f}$</td>
</tr>
<tr>
<td style="text-align: center;">(Hernandez-Leal \&amp; Kaisers, 2017)</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\pi$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">Sparse Interaction Algorithms (Section 4.2)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Vrancx, Hauwere, \&amp; Now, 2011)</td>
<td style="text-align: center;">$\mathcal{E}, \mathcal{A}, \mathcal{C}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{R}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Hu et al., 2015b)</td>
<td style="text-align: center;">$\mathcal{E}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{L}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Zhou et al., 2016)</td>
<td style="text-align: center;">$\mathcal{E}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">Relational Descriptions (Section 4.3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Proper \&amp; Tadepalli, 2009)</td>
<td style="text-align: center;">$\mathcal{C}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{F}_{a}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Koga et al., 2013)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\pi_{a}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Freire \&amp; Costa, 2015)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\pi_{a}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Koga, da Silva, \&amp; Costa, 2015)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\pi_{a}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Silva \&amp; Costa, 2017b)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{R}_{f}$</td>
</tr>
<tr>
<td style="text-align: center;">Source Task Selection (Section 4.4)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Sinapov et al., 2015)*</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Isele et al., 2016)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\pi$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Braylan \&amp; Miikkulainen, 2016)</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathcal{A}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\mathcal{A}$</td>
</tr>
<tr>
<td style="text-align: center;">Curriculum Learning (Section 4.5)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Madden \&amp; Howley, 2004)</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{R}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Narvekar et al., 2016)</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Svetlik et al., 2017)*</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{R}_{s}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Narvekar, Sinapov, \&amp; Stone, 2017)*</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Florensa et al., 2017)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{M}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">(Silva \&amp; Costa, 2018)</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{V}_{f}$</td>
<td style="text-align: center;">$\mathcal{P}$</td>
</tr>
<tr>
<td style="text-align: center;">Biases and Heuristics (Section 4.6)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Bianchi, Ros, \&amp; de Mantaras, 2009)</td>
<td style="text-align: center;">$\mathcal{S}, \mathcal{A}, \mathcal{C}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{H}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Boutsioukis, Partalas, \&amp; Vlahavas, 2011)</td>
<td style="text-align: center;">$\mathcal{C}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{B}$</td>
<td style="text-align: center;">$\mathcal{R}_{f}$</td>
</tr>
<tr>
<td style="text-align: center;">(Didi \&amp; Nitschke, 2016)</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\pi$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">Others (Section 4.7)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">(Sherstov \&amp; Stone, 2005)*</td>
<td style="text-align: center;">$\mathcal{S}, \mathcal{A}, \mathcal{C}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{A}$</td>
<td style="text-align: center;">$\mathcal{R}_{f}$</td>
</tr>
<tr>
<td style="text-align: center;">(Konidaris \&amp; Barto, 2006)*</td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{R}_{s}$</td>
<td style="text-align: center;">$\mathcal{R}_{f}$</td>
</tr>
<tr>
<td style="text-align: center;">(de Cote, Garcia, \&amp; Morales, 2016)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{M}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
<tr>
<td style="text-align: center;">(Chalmers et al., 2017)*</td>
<td style="text-align: center;">$\mathcal{S}$</td>
<td style="text-align: center;">$\mathcal{H}_{s}$</td>
<td style="text-align: center;">$\mathcal{H}_{a}$</td>
<td style="text-align: center;">$\mathcal{M}$</td>
<td style="text-align: center;">$\mathcal{S}$</td>
</tr>
</tbody>
</table>
<p>they may change over time if a bad policy is selected. In this paper, the possibility of still improving the initial policy to adapt to the new team is not considered.</p>
<p>Hernandez-Leal et al. (2017b) propose the DriftER method to detect autonomously when an opponent changes its strategy in an adversarial setting. For that, a model of the opponent is learned and used for computing the optimal policy against it. Then, the agent keeps track of the quality of predictions. In case the prediction quality is degraded suddenly, it means that the opponent changed its strategy, that is, the agent must recompute the models. A clear way for integrating TL in their method would be reusing past policies for new opponents with similar profiles, which they do not perform in this paper. For that, Leal's later work could be used (Hernandez-Leal \&amp; Kaisers, 2017). They reuse past knowledge by first learning a policy when collaborating or competing with one agent, and then reuse this knowledge when interacting with other agents. Although they refer to "opponents" in the paper, the idea can also be used in cooperative or self-interested scenarios, as the optimal policy depends on the actuation of other agents in most of MAS. Their framework assumes that the other agents in the system follow a fixed policy, hence learning how to quickly adapt to learning agents with unknown strategy/learning algorithm can be a fertile ground for future work. Moreover, different ways of modeling other agents could be explored (Albrecht \&amp; Stone, 2018).</p>
<p>Kelly and Heywood (2015) propose a different approach to adapt previously learned policies to a new task. Several policies are learned by a genetic programming algorithm, and each policy is represented by a network of value estimators which individually predict the quality of each action for the current state. Then, those learned policies are transferred to a new task, forming a hierarchical structure that will learn when to "trigger" one of the past policies. Although their proposal is originally intended to self-interested agents, their idea could be merged with DriftER, building a hierarchical tree of strategies and identifying when each of the strategies should be used against the current opponent/team. However, their proposal still relies on a hand-coded mapping of state variables and seems to be ineffective to deal with negative transfer.</p>
<h1>4.2 Sparse Interactions Algorithms</h1>
<p>For some tasks, the actuation of other agents affects the local agent only in a portion of the state space. Thus, it is safe to learn as in a single-agent problem when the actions of other agents do not matter, considerably pruning the joint action space. In this section, we discuss TL techniques specially tailored for this setting.</p>
<p>As seen in Table 2, this type of TL has been popular for accelerating equilibrium algorithms, which are especially benefited from reducing the size of the space in which the equilibrium metric has to be computed. Most of the literature so far has been using these algorithms with the assumption of an implicit source task selection, but we believe that it would be possible to transfer information about sparse interactions if those algorithms are integrated with source task selectors. The most commonly transferred information is value functions, but some of the methods transfer rules defining when the actions of other agents are expected to be irrelevant for defining the optimal policy.</p>
<p>Vrancx et al. (2011) propose to learn when the actions of other agents in the environment can affect the reward of the local agent in an easy version of the target task, so as</p>
<p>to transfer rules defining "dangerous" states in which other agents should be taken into account. Some dangerous states are found through statistical tests, and a rule classifier is trained to generalize the identification of those states. Then, the rules are transferred to the target task. No information regarding the policy or value function is transferred.</p>
<p>Hu et al. (2015b) learn a single-agent task and introduce new agents that can potentially have influence in the local reward. Then, a model of the transition function in the singleagent task is compared to the multiagent one, and the value function is reused in the states in which the local reward function is not dependent on the other agents' actions.</p>
<p>Zhou et al. (2016) propose a similar approach that learns an equilibrium policy only in the dangerous states, following a single-agent policy in the rest of the state space. The convergence to the equilibrium is accelerated by combining the Q-values from a similar situation in the learned single-agent Q-table with Q-values from a simpler version of the multiagent task, manually selected by the designer.</p>
<h1>4.3 Relational Descriptions</h1>
<p>The use of relational descriptions in RL is known to accelerate and simplify learning by generalizing commonalities between states (Kersting, van Otterlo, \&amp; Raedt, 2004; Diuk, Cohen, \&amp; Littman, 2008). For that reason, relations between objects might also help to find similarities or to perform mappings between tasks. In this section, we discuss methods that made use of relational descriptions to transfer knowledge.</p>
<p>Table 2 clearly reflects that the main reason for using relational descriptions is for being able to apply human-assisted mapping autonomy. The use of relational descriptions also enables the construction of abstract policies, which generalize the knowledge obtained and might help to avoid negative transfer. Most of the surveyed methods use self-interested learning algorithms, but multiagent relational descriptions could be used for compatibility with other learning algorithms.</p>
<p>Koga et al. (2013) propose to simultaneously learn a concrete and an abstract policy using a relational task description. At first, the abstract policy is used to achieve a reasonable actuation faster, and after a good concrete policy is learned the agent switches to the concrete reasoning. Their proposal could be useful to MAS, especially for abstracting interactions between agents, first dealing with a new agent using a general abstract policy and later building a specialized strategy for cooperating with that agent in the concrete level. Freire and Costa (2015) later showed that the learned abstract policies could also be successfully reused through tasks with promising results for TL.</p>
<p>Koga et al's later work (2015) proposes to combine multiple concrete policies learned in source tasks into a single abstract one. They show that using a single combined abstract policy is better than building a library of policies. Although their proposal is evaluated in single-agent scenarios, it could be applied to MAS if a multiagent relational task description is used such as Multiagent OO-MDP (Silva et al., 2019) or Multiagent RMDP (Croonenborghs et al., 2005).</p>
<p>Silva and Costa (2017b) propose a method for autonomously computing a mapping for transferring value functions across tasks based on an object-oriented task description. The required description is intuitive to give and could be easily adaptable to cope with the multiagent case (each type of agent could be a class as in Multiagent OO-MDPs).</p>
<p>Proper and Tadepalli (2009) also make use of a relational representation to transfer knowledge between tasks. In their proposal (specialized for MAS), a group of agents is assigned to solve collaboratively a subtask, while ignoring all agents assigned to different subtasks. After learning small tasks composed of small groups of agents, this knowledge can be reused to solve assignments of harder tasks containing more agents. The knowledge transfer is performed by copying the weights of function approximators based on relational descriptions to bootstrap the learning process for new assignments.</p>
<h1>4.4 Source Task Selection</h1>
<p>Before transferring knowledge between tasks, the first step is to select which of the previous solutions will be reused. The selection of source tasks is a very challenging problem, as the agent does not have information on the dynamics of the environment beforehand for computing task similarities. In this section, we discuss approaches for source task selection.</p>
<p>Table 2 shows that source task selection has been largely relying on human-assisted mappings so far. The tasks are also expected to have common state-action spaces or to share manually defined task features. Source task selectors are commonly not integrated with knowledge transfer procedures.</p>
<p>Sinapov et al. (2015) propose to train a regression algorithm to predict the quality of transfer between tasks within the same domain. The regression is made possible by designerspecified task features, which indicate what changes from one task to another. When a new task is given to the agent, a transfer quality is predicted according to the previously defined task features, and a ranking of source tasks is built, which can be used for transfer through any Intra-Agent method. This approach allowed an appropriate autonomous source task selection without samples of interactions in the target task. However, a human must define appropriate and observable task features. This approach could be directly applied to MAS.</p>
<p>Isele et al. (2016) also rely on similar task features to reuse previously learned tasks in the same domain. When the agent must solve a new task, an initial policy is estimated according to the current value of the task features, and then the initial policy can be iteratively refined. Unlike Sinapov's proposal, their algorithm takes into account the advantages of transferring from consecutive tasks and has constant computational complexity in regard to the number of source policies.</p>
<p>Without assuming that the parameterization of the current task is known, Braylan and Mikkulainen (2016) evaluate the policies in the library to select the top performers in the new task and then build an ensemble of source tasks to be used and refined in the target tasks. Although their method was applied only to state transition function estimation, similar ideas could also be used for reusing additional knowledge.</p>
<p>The proposal by Fitzgerald et al. (2016) can also collaborate with insights for estimating a mapping across two tasks (even though they did not directly apply their method to RL). Their method consists of observing demonstrations from a human teacher, and those observations are used for refining a mapping function that relates objects in the source and target tasks. Therefore, their method could inspire mapping procedures based on relational descriptions of MDPs.</p>
<h1>4.5 Curriculum Learning</h1>
<p>Table 2 shows that the main characteristic of this group of methods is transferring knowledge across progressively harder tasks. The main focus is usually on how to define task sequences, rather than on how to reuse the knowledge.</p>
<p>Narvekar et al. (2016) borrowed the idea of building a Curriculum from the Supervised Learning area (Bengio et al., 2009). Their main idea is to learn a complex task faster by building a sequence of easier tasks within the same domain and reusing the gathered knowledge across tasks. For that, the designer provides to the agent an environment simulator, that can be freely modified by varying some feature values, and heuristics to guide the agent on how the simulator parameters should be changed to simplify the task. The authors showed that the use of a curriculum can expressively accelerate the learning process. However, in complex tasks, we would have only imprecise simulators to build a curriculum from, especially for MAS in which the policies of the other agents in the system are unknown. Even though it is not clear if the proposal could cope with imprecise simulators, this is nonetheless a very interesting topic for further investigations.</p>
<p>Svetlik et al. (2017) extend the initial approaches by building a directed acyclic graph and pruning some source tasks that are not expected to help the learning agent according to a transfer potential heuristic. Moreover, such a graph allows the identification of curriculum source tasks that can be learned in parallel, which in theory could enable dividing the learning process of the curriculum among several agents (that can combine gathered knowledge using any Inter-Agent transfer method). Silva and Costa (2018) later extended Svetlik's proposal by generating the Curriculum based on an object-oriented description, which could represent a further step towards multiagent Curricula.</p>
<p>Narvekar et al. (2017) extended their initial work to autonomously generate a Curriculum. Their idea is to build a Curriculum MDP to infer an appropriate task sequence in a customized manner (i.e., taking into account differences in regard to agent sensors or actuators).</p>
<p>Florensa et al. (2017) propose to change the initial state distribution to move the task initialization closer to the goal, which causes the agent to receive rewards more quickly, and then progressively start the task farther to the goal to explore the whole environment more efficiently. Although their proposal works in a restrictive setting, this idea could be used for building Curricula in cooperative MAS, as the agents could, for example, coordinate for starting learning in an initial formation for which the agents do not have much information about.</p>
<p>Madden and Howley (2004) had an earlier idea very similar to Curriculum Learning, where knowledge is progressively transferred from simple to more complex tasks. After learning a task, the agent builds a training set composed of abstracted states (computed through a relational task description defined by the designer) to predict the action suggested by the learned policies. This dataset is then used to train a supervised learning algorithm that learns rules for defining actions for each state. When starting the learning process in a new task, the rules are used for estimating actions in unexplored states. Their method considers the possibility of aggregating rules estimated from several tasks and given by humans. However, their procedure does not cope with inconsistent rules.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>Humans or automated agents (actuating or not in the environment) might be involved in knowledge reuse relations.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
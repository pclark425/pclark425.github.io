<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6165 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6165</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6165</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-267523079</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.04788v3.pdf" target="_blank">MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \url{https://mllm-judge.github.io/}.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6165.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6165.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLLM-as-a-Judge benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal benchmark created to evaluate the judging capability of Multimodal Large Language Models across three tasks (Scoring Evaluation, Pair Comparison, Batch Ranking) using human-annotated judgments on image-instruction-response triples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Collect image-instruction pairs, generate responses from multiple MLLMs, then have human annotators and MLLMs produce judgments under three task formats (scoring, pairwise comparison, batch ranking); compare MLLM judgments to human annotations using quantitative metrics and human agreement studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Alignment with human judgments measured by Pearson correlation (scoring), accuracy/F1/recall (pair comparison), Normalized Levenshtein distance (batch ranking); human agreement (% agreement), consistency measures across repeated runs, analysis grading (1–5) and hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Evaluated models include GPT-4V, Gemini-Pro-Vision, LLaVA-1.5/1.6 family, Qwen-VL-Plus/Max, CogVLM, and others.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Multimodal vision-language evaluation (used as a proxy for assessing LLM evaluative capacities applicable to evaluating generated theories or explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a single scientific hypothesis — the benchmark evaluates models' ability to judge/assess model-generated multimodal outputs (which is analogous to judging generated scientific theories in contexts requiring multimodal evidence and reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across datasets GPT-4V achieved the strongest alignment with humans: Scoring Pearson ≈ 0.490 (average), Pair Comparison accuracy/F1 high (tie setting ≈0.636, non-tie ≈0.773 average), Batch Ranking edit distance (Levenshtein) mean ≈0.361; other MLLMs performed worse and showed substantial biases and hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Curated 4,414 image-instruction pairs from 14 downstream datasets (10+ named datasets across domains such as COCO, Math, Charts, Text reading, Diffusion, Graphics, WIT, CC-3M, Chart, VisIT, Mementos), plus two released subsets MLLM-AS-A-JUDGE-HQ and MLLM-AS-A-JUDGE-HARD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Quantified alignment: Pearson similarity for scoring, accuracy/F1/recall for pairwise, Normalized Levenshtein for batch; human agreement experiments report e.g. GPT-4V ≈70% human agreement in scoring and ≈79% in pair comparison on some datasets; models generally match humans better on pairwise tasks than scoring or ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmark exposed biases (egocentric, position, verbosity/length), hallucinations (especially in batch ranking), inconsistent judgments across repetitions, sensitivity to prompt/example structure, and gaps between MLLM and human preferences in scoring and ranking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6165.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scoring Evaluation (1–5 rubric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation format where each individual model response is scored on a 1-to-5 scale according to a rubric (relevance, accuracy, comprehensiveness, creativity, granularity), with detailed guidelines in Appendix F.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Single-response grading by MLLMs and humans using a 1–5 rubric; comparisons via Pearson correlation between MLLM scores and human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Numeric score (1–5) with rubric categories (Poor=1 to Excellent=5) assessing relevance, accuracy, comprehensiveness, creativity, and granularity; Pearson similarity used to compare distributions across annotators and models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4V, Gemini, LLaVA family, Qwen-VL, CogVLM (as judges and as generators of responses being judged).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Multimodal response quality assessment (proxy for quality assessment of generated hypotheses/theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation of how well a model's response (or a candidate theory/explanation presented as text) matches the instruction/query in relevance, correctness and informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4V achieved highest Pearson similarity to human scores (avg ≈0.490); other MLLMs (Gemini, LLaVA, CogVLM, Qwen) showed lower similarity and 'high-score' bias (tendency to cluster around higher scores, e.g., many 4s and 5s).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied across the 4,414 image-instruction pairs partitioned into D_score subsets drawn from COCO, MathVista, Chart, Graphics, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct Pearson correlation between MLLM-assigned scores and human-assigned scores; human agreement rates reported (~70% for GPT-4V averaged across datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High-score bias in many models, sensitivity to response length (verbosity bias), and lower consistency relative to pairwise tasks; rubric requires careful human training to reduce annotation bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6165.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pair Comp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pair Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation format where two responses to the same instruction are compared and the judge selects the better one (with a tie option in some setups) to approximate human preference judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Present two model responses and ask judge (human or MLLM) to choose the superior response or declare a tie; compare MLLM choices to human choices using classification metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy, precision/recall/F1 against human pairwise verdicts; tie calibration considered (ties included or excluded in analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4V, Gemini, Qwen-VL, LLaVA, CogVLM.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Preference/quality comparison of multimodal responses (analogous to pairwise comparison of competing explanations/theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Determines which of two candidate responses (or theories/explanations) better satisfies the instruction and evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Models perform best on pairwise tasks; GPT-4V achieved strong alignment (tie setting average ≈0.636; non-tie ≈0.773), often mirroring human tie frequency; Gemini and others more likely to force a winner and exhibited position bias.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied across D_pair subsets from the curated datasets; results summarized per dataset (COCO, Math, Charts, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>High pairwise agreement (GPT-4V ≈78–79% on some datasets); pair comparison shows closer model-human alignment than scoring or ranking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Position bias in some MLLMs (preference for one option), lack of tie calibration in some models, and examples where models 'forcibly' pick winners leading to divergence from human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6165.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Batch Rank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Batch Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation format requiring judges to produce a full ranking of multiple responses (descending quality) for a single instruction, without allowing ties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Judge produces a ranking sequence for a set of responses; similarity to human ranking is measured with Normalized Levenshtein distance (edit distance on ranking sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Normalized Levenshtein distance between MLLM-produced ranking and human-produced ranking (lower distance = closer alignment); related human agreement and consistency measures.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4V, Gemini, LLaVA family, Qwen-VL (CogVLM sometimes unable to produce full ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Relative ordering of candidate outputs (useful for comparing multiple candidate theories/explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Assesses model's ability to order candidate responses/theories by quality using multimodal evidence and instruction constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Batch ranking was the weakest area for models: GPT-4V had mean Levenshtein distance ≈0.361 average (substantial gap remains); some models (CogVLM) couldn't produce full rankings and were excluded; LLaVA showed strong position bias replicating example sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>D_batch subset from the 4,414 image-instruction-response dataset; specific difficulty on Math and graphics-heavy datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Lower alignment than pairwise and scoring; human agreement on batch ranking substantially lower and more variable across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Higher hallucination frequency in batch tasks, position bias related to prompt exemplars, difficulty handling long texts/sequences, and reduced consistency across repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6165.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pearson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pearson similarity (Pearson correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical metric used to measure linear correlation between MLLM numeric scores and human numeric scores (used for Scoring Evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute Pearson correlation coefficient between vectors of human-assigned numeric scores and MLLM-assigned numeric scores across items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pearson correlation coefficient (range -1 to 1), significance testing reported (p < 0.05 for presented results).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to compare GPT-4V, Gemini, LLaVA, Qwen-VL, CogVLM, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Quantitative alignment between model and human scoring behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a theory; a metric to quantify agreement on continuous rating scales.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4V scoring Pearson ≈0.490 average across datasets; Gemini ≈0.304; other models lower or dataset dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to D_score partitions across COCO, Math, Chart, Graphics and more.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct numeric correlation shows degree of alignment; higher Pearson indicates closer match to human scoring patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Pearson captures linear correlation but may miss nonlinear differences, distributional biases (e.g., high-score clustering) and does not reflect pairwise tie calibration or absolute errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6165.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pair Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy / F1 / Recall (Pair comparison metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classification metrics used to compare MLLM pairwise decisions against human pairwise labels (select A/B/tie).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute accuracy, F1-score, and recall of MLLM's pairwise picks versus human majority choice for each pairwise instance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy (overall correct fraction), F1 (harmonic mean of precision and recall), recall for chosen classes; tie handling considered separately.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to evaluate GPT-4V, Gemini, LLaVA family, Qwen-VL, CogVLM.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Model-human agreement on pairwise preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Measures how often the model's comparative choice matches human preference judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4V had strong pairwise metrics (reported averages: tie setting ~0.636, non-tie accuracy ~0.773 across datasets); other models lower and often biased toward declaring winners.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>D_pair from the benchmark; per-dataset breakdown provided (COCO, Math, Chart, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Shows better alignment overall than scoring/ranking; models approximate human binary choices more reliably than fine-grained scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Metrics sensitive to tie frequency and class imbalance; position bias and forced-win tendencies in some models distort results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6165.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EditDist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Normalized Levenshtein distance (ranking edit distance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An edit-distance metric (normalized Levenshtein) used to quantify dissimilarity between model-produced ranking sequences and human-produced ranking sequences in Batch Ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute normalized Levenshtein (edit) distance between two ranking strings/sequences; lower values indicate closer match.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Normalized Levenshtein distance (lower is better for alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to GPT-4V and other MLLMs that produce full rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Ranking similarity assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Measures structural dissimilarity between ordering outputs of judges and human reference rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4V mean Levenshtein Distance ~0.361 reported (indicating measurable but imperfect alignment); some models couldn't be evaluated due to inability to produce full ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>D_batch across multiple datasets; results reported per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Directly compares full-order outputs to human orderings; demonstrated that ranking alignment is poorer than pairwise alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sensitive to minor ordering differences; does not capture graded severity of mistakes beyond edit operations; high hallucination rates in batch tasks increase distance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6165.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanAgreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human agreement and annotation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotation process (six author-annotators) with cross-validation, tutorials, and three independent reviews per judgment to create gold labels and measure human agreement with MLLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human annotators independently judge model outputs (scores/pair choices/rankings); consensus labels and % agreement between humans and MLLMs measured; separate human agreement experiments with more annotators for bias checking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary human agreement (yes/no) per judgment, percent agreement, Mean Absolute Deviation (MAD) for score variance among annotators, and consensus labels from majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to evaluate MLLM outputs and MLLM-as-judge decisions (GPT-4V, Gemini, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Gold-standard human judgments for multimodal output evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Provides human reference judgments against which MLLM decisions are assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported human agreement: GPT-4V ≈70% in scoring (peak 79.9% on MS-COCO), high pairwise agreement (~78–79% on some datasets), lower agreement in batch ranking; MAD showed GPT-4V had lower variance (more consistent) than Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Annotations collected for the full benchmark (4,414 image-instruction pairs) with splits for scoring/pair/batch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct measurement of how frequently MLLM judgments match human validators; used as primary ground truth for alignment metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human annotations themselves have bias; steps taken (diverse annotators, tutorials, cross-validation) mitigate but do not eliminate subjectivity; cost and scalability are concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6165.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConsistencyChecks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency evaluation (repetition & MCC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedures to measure judgment consistency by repeating judging runs multiple times and computing weighted average consistency and a Majority Consistency Criterion (MCC) that counts items with identical majority judgments across repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run judges (MLLMs) on the same items multiple times (six repetitions reported), compute per-item and aggregate consistency metrics (weighted average consistency and MCC ratio), and report stability of model judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Weighted average consistency score, Majority Consistency Criterion (proportion of items with identical majority judgment across runs), and variation measures (MAD).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Measured for GPT-4V and Gemini primarily; also tracked across other MLLMs where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Reliability of model judgments over repeated runs (important for trusting LLM-generated evaluations/theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Assesses the reproducibility of model judgments when presented the same evaluation task multiple times.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4V outperformed Gemini on consistency: e.g., pairwise consistency ~0.675 for GPT-4V vs lower for Gemini; scoring and batch consistency lower (GPT-4V scoring 0.611, batch 0.418).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Stability tests conducted on representative subsets from the benchmark across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Consistency compared to human intra-annotator stability (human agreement studies); GPT-4V more consistent than Gemini but still shows substantial variability particularly for scoring and batch tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Temperature/randomness settings affect consistency; some models show instability even at low temperature; inconsistency erodes trust in using models as sole evaluators of generated theories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6165.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnalysisGrading</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysis grading and hallucination detection (human-rated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation of MLLM provided analyses on dimensions (relevance, accuracy, creativity, granularity) scored 1–5 and explicit human labeling of hallucinations in analysis chains per vision-language hallucination definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Annotators assign 1–5 scores to model analyses and mark the presence/absence of hallucinations using a decision-tree protocol; agreement on hallucination labels measured.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Analysis score (1–5), hallucination detection binary/labeling, and use of decision-tree guidelines to improve inter-annotator consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to analyses produced by GPT-4V, Gemini and other MLLMs during 'Analyze-then-Judge' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Assessing the internal reasoning/analysis quality of LLM judgments (relevant for evaluating explanatory theories produced by LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluates the quality and factuality of model reasoning chains that lead to judgments; identifies hallucinated/invented claims in analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Annotators achieved high consensus on hallucination labeling using a decision-tree; multi-step CoT reduced hallucinations but did not consistently improve alignment with human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to analysis outputs on the MLLM-AS-A-JUDGE-HARD subset and other sampled items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human labels used as ground truth for hallucination and analysis quality; models often produced hallucinations that humans could detect reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human detection is time-consuming; decision-tree improves reliability but some hallucinations remain subtle; reducing hallucinations didn't always increase preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6165.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-Analyze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analyze-then-Judge & multi-step Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting paradigm where the judge first provides an analysis (reasoning chain) and then issues a judgment; tested in one-step ('Analyze-then-Judge') and multi-step CoT (3-step) variants to study effects on hallucination and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Prompt judges to produce analysis then decision; compare judgments with/without multi-step CoT using the same metrics (Pearson, pairwise accuracy, Levenshtein, hallucination rates).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Alignment metrics (Pearson, accuracy, Levenshtein), hallucination rate (human-labeled), and effect on consistency/quality of judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Tested with GPT-4V and Gemini primarily; some MLLMs (LLaVA, CogVLM) lacked capability to follow full analyze-then-judge prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Reasoning transparency and robustness of judgments (relevant to evaluating reasoning underpinning candidate theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Uses explicit intermediate reasoning steps to potentially improve factuality and explainability of final judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Multi-step CoT reduced hallucination rates (notably on HARD subset and image-related tasks) but did not consistently improve alignment with human preferences and in some datasets decreased judging performance, especially for Gemini; GPT-4V produced longer analyses and benefitted partially in hallucination reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to subsets including MLLM-AS-A-JUDGE-HARD and sampled datasets for CoT ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Reduced hallucinations made outputs more factually grounded per human raters but alignment (preference matching) sometimes decreased, implying trade-offs between factuality and preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Longer reasoning chains can propagate hallucinations; multi-step CoT increases opportunity for self-interference and can degrade final judgments if earlier steps hallucinate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6165.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VisionExp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision perception / Vision expert description experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two approaches to supply visual information to judges: (1) full image input to MLLMs and (2) textual, detailed image descriptions provided by strong LLMs (vision experts) to non-visual LLM judges; tested impact on judging performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare judging performance when judges have direct vision input vs. when they receive a detailed textual description of the image vs. no vision information; measure via Pearson, pairwise metrics, and edit distance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pearson similarity (scoring), pairwise accuracy (with/without ties), normalized edit distance (batch), and overall alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Vision descriptions generated by LLaMA-70b, Mixtral-8x7b, GPT-3.5; judged by GPT-4V and other LLMs with/without vision input.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Effect of visual grounding on evaluation quality (important when assessing multimodal evidence supporting theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Tests hypothesis that supplying high-quality textual image descriptions to text-only LLM judges improves their ability to evaluate multimodal outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Providing detailed image descriptions to text LLMs notably improved scoring Pearson to ~0.435 (for GPT-4V with vision exp) and in some pairwise settings exceeded standard MLLM judging; judges without any vision performed substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied across standard evaluation subsets; Table 6 summarizes improvements for LLaMA2-70b, Mixtral-8x7b, GPT-3.5 and GPT-4V with vision descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Judging with vision descriptions increased alignment with human judgments compared to no-vision baselines, suggesting high-quality textual grounding can compensate for direct visual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality of vision descriptions strongly conditions performance; generating comprehensive descriptions requires capable vision-LMs and introduces another source of error.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6165.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiasHall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bias and hallucination diagnostics & mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis and reporting of systematic biases (egocentric/self-preference, position bias, length/verbosity bias) and hallucination frequency across evaluation formats, plus mitigation experiments (multi-step CoT, more examples in prompts, vision experts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Empirical analysis of score distributions, positional choice frequencies, response-length vs score correlations, human-labeled hallucination detection; mitigation via CoT steps and prompt/example augmentation tested.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Distributional analyses (score histograms), length-vs-score curves, positional preference rates, hallucination rate (% detected), and effect sizes after mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Observed in GPT-4V, Gemini, LLaVA family, Qwen-VL, CogVLM.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Reliability and fairness of model judgments relevant to trusting model assessments of generated theories.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Characterizes systematic deviations from human-like judgment that undermine trustworthiness (e.g., self-scoring bias, always favoring position A, favoring longer answers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Detected egocentric bias (GPT-4V slight self-preference), position bias (strong in LLaVA and CogVLM), length bias (both GPT-4V and Gemini increase scores for longer answers; experiments showed average gains ~0.6–0.75 points when semantic length increased), and higher hallucination rates in Batch Ranking; multi-step CoT reduced hallucination rates substantially but did not uniformly improve preference alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Bias analyses conducted across the benchmark datasets and special HARD subset designed to expose hallucination/inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Bias patterns deviate from human judge behaviour; models sometimes rely on alignment or safety tuning (e.g., privacy emphasis) producing systematic preferences unlike diverse human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mitigations (CoT, prompt examples, vision experts) partially reduce issues but do not eliminate biases; need for RLHF/DPO and human-in-the-loop checks recommended for robust correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e6165.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HQ/HARD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLLM-AS-A-JUDGE-HQ and MLLM-AS-A-JUDGE-HARD datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two curated subsets released by the authors: HQ contains responses with high agreement to human judgments; HARD contains instances with model-human inconsistency and manifestations of hallucination to stress-test judge capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Use HQ as reliable evaluation/test examples and HARD to diagnose hallucination and failure modes; both support training/validation for RLHF/DPO and robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>HQ: high human-MLLM concordance; HARD: high incidence of hallucination/bias or low human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Used to evaluate GPT-4V, Gemini, Qwen-VL, LLaVA, CogVLM etc. and as testbeds for mitigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Benchmarking and stress-testing MLLM judgment capabilities (proxy for evaluating contentious or error-prone generated theories).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Datasets consisting of image-instruction-response items selected for either high human agreement (HQ) or marked difficulty/hallucination (HARD).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Multi-step CoT and other mitigations reduced hallucination rates on HARD subset (e.g., reported reductions across formats: Score 46.15%/48.72%/33.33% depending on figure/instruction variants — see Table 7 for per-format improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Part of the public release accompanying the paper (project homepage provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>HARD highlights cases where MLLM judgments diverge from human judges; HQ exemplifies close human-model agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>HARD demonstrates persistent failure modes even for top models (GPT-4V) requiring further research and intervention (RLHF/DPO, human-in-loop).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6165.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e6165.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmpiricalSummary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical results summary (GPT-4V vs others)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Key quantitative findings comparing top MLLMs across the three judging tasks showing GPT-4V leading on alignment with humans, especially in pairwise tasks, while other models lag and exhibit systematic biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Aggregate metrics reported per model and per task across datasets (Pearson for scoring, accuracy/F1 for pair, Levenshtein for batch), plus human agreement and consistency analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pearson similarity, pairwise accuracy/tie calibration, normalized Levenshtein distance, human agreement %, consistency scores (weighted avg & MCC).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4V, Gemini-Pro-Vision, LLaVA-1.5/1.6, Qwen-VL-Plus/Max, CogVLM.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Model judgment performance in multimodal evaluation tasks; results inform trust in LLMs as evaluators of generated outputs/explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a theory — collated performance outcomes indicating how well models can serve as judges across modalities and task formats.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4V: scoring Pearson ≈0.490 average; pair w. tie avg ≈0.636, pair w.o. tie avg ≈0.773; human agreement ~70% (scoring) and ~78–79% (pair). Gemini and many open models show 'high-score' bias, position bias, lower Pearson (e.g., Gemini ≈0.304), and reduced consistency. Batch ranking remains challenging for all models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Results computed across the benchmark's datasets (COCO, Chart, Math, Graphics, Diffusion, Text reading, WIT, CC-3M, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Quantitative comparisons show GPT-4V closer to humans than other models but still with nontrivial gaps, especially for scoring and batch ranking tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Even the top model shows biases, hallucinations, length/position effects, and inconsistent judgments; improvements needed before MLLMs can be considered fully reliable judges of complex generated theories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>JudgeLM <em>(Rating: 2)</em></li>
                <li>AUTO-J <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Prometheus-vision: Vision-language model as a judge for fine-grained evaluation <em>(Rating: 2)</em></li>
                <li>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions <em>(Rating: 1)</em></li>
                <li>Detecting and preventing hallucinations in large vision language models <em>(Rating: 1)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6165",
    "paper_id": "paper-267523079",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "MLLM-as-a-Judge",
            "name_full": "MLLM-as-a-Judge benchmark",
            "brief_description": "A multimodal benchmark created to evaluate the judging capability of Multimodal Large Language Models across three tasks (Scoring Evaluation, Pair Comparison, Batch Ranking) using human-annotated judgments on image-instruction-response triples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Collect image-instruction pairs, generate responses from multiple MLLMs, then have human annotators and MLLMs produce judgments under three task formats (scoring, pairwise comparison, batch ranking); compare MLLM judgments to human annotations using quantitative metrics and human agreement studies.",
            "evaluation_criteria": "Alignment with human judgments measured by Pearson correlation (scoring), accuracy/F1/recall (pair comparison), Normalized Levenshtein distance (batch ranking); human agreement (% agreement), consistency measures across repeated runs, analysis grading (1–5) and hallucination detection.",
            "llm_model_name": "Evaluated models include GPT-4V, Gemini-Pro-Vision, LLaVA-1.5/1.6 family, Qwen-VL-Plus/Max, CogVLM, and others.",
            "theory_domain": "Multimodal vision-language evaluation (used as a proxy for assessing LLM evaluative capacities applicable to evaluating generated theories or explanations).",
            "theory_description": "Not a single scientific hypothesis — the benchmark evaluates models' ability to judge/assess model-generated multimodal outputs (which is analogous to judging generated scientific theories in contexts requiring multimodal evidence and reasoning).",
            "evaluation_results": "Across datasets GPT-4V achieved the strongest alignment with humans: Scoring Pearson ≈ 0.490 (average), Pair Comparison accuracy/F1 high (tie setting ≈0.636, non-tie ≈0.773 average), Batch Ranking edit distance (Levenshtein) mean ≈0.361; other MLLMs performed worse and showed substantial biases and hallucinations.",
            "benchmarks_or_datasets": "Curated 4,414 image-instruction pairs from 14 downstream datasets (10+ named datasets across domains such as COCO, Math, Charts, Text reading, Diffusion, Graphics, WIT, CC-3M, Chart, VisIT, Mementos), plus two released subsets MLLM-AS-A-JUDGE-HQ and MLLM-AS-A-JUDGE-HARD.",
            "comparison_to_human": "Quantified alignment: Pearson similarity for scoring, accuracy/F1/recall for pairwise, Normalized Levenshtein for batch; human agreement experiments report e.g. GPT-4V ≈70% human agreement in scoring and ≈79% in pair comparison on some datasets; models generally match humans better on pairwise tasks than scoring or ranking.",
            "limitations_or_challenges": "Benchmark exposed biases (egocentric, position, verbosity/length), hallucinations (especially in batch ranking), inconsistent judgments across repetitions, sensitivity to prompt/example structure, and gaps between MLLM and human preferences in scoring and ranking tasks.",
            "uuid": "e6165.0",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Scoring Eval",
            "name_full": "Scoring Evaluation (1–5 rubric)",
            "brief_description": "Evaluation format where each individual model response is scored on a 1-to-5 scale according to a rubric (relevance, accuracy, comprehensiveness, creativity, granularity), with detailed guidelines in Appendix F.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Single-response grading by MLLMs and humans using a 1–5 rubric; comparisons via Pearson correlation between MLLM scores and human scores.",
            "evaluation_criteria": "Numeric score (1–5) with rubric categories (Poor=1 to Excellent=5) assessing relevance, accuracy, comprehensiveness, creativity, and granularity; Pearson similarity used to compare distributions across annotators and models.",
            "llm_model_name": "GPT-4V, Gemini, LLaVA family, Qwen-VL, CogVLM (as judges and as generators of responses being judged).",
            "theory_domain": "Multimodal response quality assessment (proxy for quality assessment of generated hypotheses/theories).",
            "theory_description": "Evaluation of how well a model's response (or a candidate theory/explanation presented as text) matches the instruction/query in relevance, correctness and informativeness.",
            "evaluation_results": "GPT-4V achieved highest Pearson similarity to human scores (avg ≈0.490); other MLLMs (Gemini, LLaVA, CogVLM, Qwen) showed lower similarity and 'high-score' bias (tendency to cluster around higher scores, e.g., many 4s and 5s).",
            "benchmarks_or_datasets": "Applied across the 4,414 image-instruction pairs partitioned into D_score subsets drawn from COCO, MathVista, Chart, Graphics, etc.",
            "comparison_to_human": "Direct Pearson correlation between MLLM-assigned scores and human-assigned scores; human agreement rates reported (~70% for GPT-4V averaged across datasets).",
            "limitations_or_challenges": "High-score bias in many models, sensitivity to response length (verbosity bias), and lower consistency relative to pairwise tasks; rubric requires careful human training to reduce annotation bias.",
            "uuid": "e6165.1",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Pair Comp",
            "name_full": "Pair Comparison",
            "brief_description": "Evaluation format where two responses to the same instruction are compared and the judge selects the better one (with a tie option in some setups) to approximate human preference judgments.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Present two model responses and ask judge (human or MLLM) to choose the superior response or declare a tie; compare MLLM choices to human choices using classification metrics.",
            "evaluation_criteria": "Accuracy, precision/recall/F1 against human pairwise verdicts; tie calibration considered (ties included or excluded in analyses).",
            "llm_model_name": "GPT-4V, Gemini, Qwen-VL, LLaVA, CogVLM.",
            "theory_domain": "Preference/quality comparison of multimodal responses (analogous to pairwise comparison of competing explanations/theories).",
            "theory_description": "Determines which of two candidate responses (or theories/explanations) better satisfies the instruction and evidence.",
            "evaluation_results": "Models perform best on pairwise tasks; GPT-4V achieved strong alignment (tie setting average ≈0.636; non-tie ≈0.773), often mirroring human tie frequency; Gemini and others more likely to force a winner and exhibited position bias.",
            "benchmarks_or_datasets": "Applied across D_pair subsets from the curated datasets; results summarized per dataset (COCO, Math, Charts, etc.).",
            "comparison_to_human": "High pairwise agreement (GPT-4V ≈78–79% on some datasets); pair comparison shows closer model-human alignment than scoring or ranking tasks.",
            "limitations_or_challenges": "Position bias in some MLLMs (preference for one option), lack of tie calibration in some models, and examples where models 'forcibly' pick winners leading to divergence from human preferences.",
            "uuid": "e6165.2",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Batch Rank",
            "name_full": "Batch Ranking",
            "brief_description": "Evaluation format requiring judges to produce a full ranking of multiple responses (descending quality) for a single instruction, without allowing ties.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Judge produces a ranking sequence for a set of responses; similarity to human ranking is measured with Normalized Levenshtein distance (edit distance on ranking sequences).",
            "evaluation_criteria": "Normalized Levenshtein distance between MLLM-produced ranking and human-produced ranking (lower distance = closer alignment); related human agreement and consistency measures.",
            "llm_model_name": "GPT-4V, Gemini, LLaVA family, Qwen-VL (CogVLM sometimes unable to produce full ranking).",
            "theory_domain": "Relative ordering of candidate outputs (useful for comparing multiple candidate theories/explanations).",
            "theory_description": "Assesses model's ability to order candidate responses/theories by quality using multimodal evidence and instruction constraints.",
            "evaluation_results": "Batch ranking was the weakest area for models: GPT-4V had mean Levenshtein distance ≈0.361 average (substantial gap remains); some models (CogVLM) couldn't produce full rankings and were excluded; LLaVA showed strong position bias replicating example sequences.",
            "benchmarks_or_datasets": "D_batch subset from the 4,414 image-instruction-response dataset; specific difficulty on Math and graphics-heavy datasets.",
            "comparison_to_human": "Lower alignment than pairwise and scoring; human agreement on batch ranking substantially lower and more variable across datasets.",
            "limitations_or_challenges": "Higher hallucination frequency in batch tasks, position bias related to prompt exemplars, difficulty handling long texts/sequences, and reduced consistency across repeats.",
            "uuid": "e6165.3",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Pearson",
            "name_full": "Pearson similarity (Pearson correlation)",
            "brief_description": "Statistical metric used to measure linear correlation between MLLM numeric scores and human numeric scores (used for Scoring Evaluation).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute Pearson correlation coefficient between vectors of human-assigned numeric scores and MLLM-assigned numeric scores across items.",
            "evaluation_criteria": "Pearson correlation coefficient (range -1 to 1), significance testing reported (p &lt; 0.05 for presented results).",
            "llm_model_name": "Used to compare GPT-4V, Gemini, LLaVA, Qwen-VL, CogVLM, etc.",
            "theory_domain": "Quantitative alignment between model and human scoring behaviour.",
            "theory_description": "Not a theory; a metric to quantify agreement on continuous rating scales.",
            "evaluation_results": "GPT-4V scoring Pearson ≈0.490 average across datasets; Gemini ≈0.304; other models lower or dataset dependent.",
            "benchmarks_or_datasets": "Applied to D_score partitions across COCO, Math, Chart, Graphics and more.",
            "comparison_to_human": "Direct numeric correlation shows degree of alignment; higher Pearson indicates closer match to human scoring patterns.",
            "limitations_or_challenges": "Pearson captures linear correlation but may miss nonlinear differences, distributional biases (e.g., high-score clustering) and does not reflect pairwise tie calibration or absolute errors.",
            "uuid": "e6165.4",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Pair Metrics",
            "name_full": "Accuracy / F1 / Recall (Pair comparison metrics)",
            "brief_description": "Classification metrics used to compare MLLM pairwise decisions against human pairwise labels (select A/B/tie).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute accuracy, F1-score, and recall of MLLM's pairwise picks versus human majority choice for each pairwise instance.",
            "evaluation_criteria": "Accuracy (overall correct fraction), F1 (harmonic mean of precision and recall), recall for chosen classes; tie handling considered separately.",
            "llm_model_name": "Used to evaluate GPT-4V, Gemini, LLaVA family, Qwen-VL, CogVLM.",
            "theory_domain": "Model-human agreement on pairwise preferences.",
            "theory_description": "Measures how often the model's comparative choice matches human preference judgments.",
            "evaluation_results": "GPT-4V had strong pairwise metrics (reported averages: tie setting ~0.636, non-tie accuracy ~0.773 across datasets); other models lower and often biased toward declaring winners.",
            "benchmarks_or_datasets": "D_pair from the benchmark; per-dataset breakdown provided (COCO, Math, Chart, etc.).",
            "comparison_to_human": "Shows better alignment overall than scoring/ranking; models approximate human binary choices more reliably than fine-grained scoring.",
            "limitations_or_challenges": "Metrics sensitive to tie frequency and class imbalance; position bias and forced-win tendencies in some models distort results.",
            "uuid": "e6165.5",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "EditDist",
            "name_full": "Normalized Levenshtein distance (ranking edit distance)",
            "brief_description": "An edit-distance metric (normalized Levenshtein) used to quantify dissimilarity between model-produced ranking sequences and human-produced ranking sequences in Batch Ranking.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compute normalized Levenshtein (edit) distance between two ranking strings/sequences; lower values indicate closer match.",
            "evaluation_criteria": "Normalized Levenshtein distance (lower is better for alignment).",
            "llm_model_name": "Applied to GPT-4V and other MLLMs that produce full rankings.",
            "theory_domain": "Ranking similarity assessment.",
            "theory_description": "Measures structural dissimilarity between ordering outputs of judges and human reference rankings.",
            "evaluation_results": "GPT-4V mean Levenshtein Distance ~0.361 reported (indicating measurable but imperfect alignment); some models couldn't be evaluated due to inability to produce full ranking.",
            "benchmarks_or_datasets": "D_batch across multiple datasets; results reported per dataset.",
            "comparison_to_human": "Directly compares full-order outputs to human orderings; demonstrated that ranking alignment is poorer than pairwise alignment.",
            "limitations_or_challenges": "Sensitive to minor ordering differences; does not capture graded severity of mistakes beyond edit operations; high hallucination rates in batch tasks increase distance.",
            "uuid": "e6165.6",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "HumanAgreement",
            "name_full": "Human agreement and annotation protocol",
            "brief_description": "Human annotation process (six author-annotators) with cross-validation, tutorials, and three independent reviews per judgment to create gold labels and measure human agreement with MLLM judgments.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Human annotators independently judge model outputs (scores/pair choices/rankings); consensus labels and % agreement between humans and MLLMs measured; separate human agreement experiments with more annotators for bias checking.",
            "evaluation_criteria": "Binary human agreement (yes/no) per judgment, percent agreement, Mean Absolute Deviation (MAD) for score variance among annotators, and consensus labels from majority voting.",
            "llm_model_name": "Used to evaluate MLLM outputs and MLLM-as-judge decisions (GPT-4V, Gemini, etc.).",
            "theory_domain": "Gold-standard human judgments for multimodal output evaluation.",
            "theory_description": "Provides human reference judgments against which MLLM decisions are assessed.",
            "evaluation_results": "Reported human agreement: GPT-4V ≈70% in scoring (peak 79.9% on MS-COCO), high pairwise agreement (~78–79% on some datasets), lower agreement in batch ranking; MAD showed GPT-4V had lower variance (more consistent) than Gemini.",
            "benchmarks_or_datasets": "Annotations collected for the full benchmark (4,414 image-instruction pairs) with splits for scoring/pair/batch.",
            "comparison_to_human": "Direct measurement of how frequently MLLM judgments match human validators; used as primary ground truth for alignment metrics.",
            "limitations_or_challenges": "Human annotations themselves have bias; steps taken (diverse annotators, tutorials, cross-validation) mitigate but do not eliminate subjectivity; cost and scalability are concerns.",
            "uuid": "e6165.7",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ConsistencyChecks",
            "name_full": "Consistency evaluation (repetition & MCC)",
            "brief_description": "Procedures to measure judgment consistency by repeating judging runs multiple times and computing weighted average consistency and a Majority Consistency Criterion (MCC) that counts items with identical majority judgments across repetitions.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Run judges (MLLMs) on the same items multiple times (six repetitions reported), compute per-item and aggregate consistency metrics (weighted average consistency and MCC ratio), and report stability of model judgments.",
            "evaluation_criteria": "Weighted average consistency score, Majority Consistency Criterion (proportion of items with identical majority judgment across runs), and variation measures (MAD).",
            "llm_model_name": "Measured for GPT-4V and Gemini primarily; also tracked across other MLLMs where applicable.",
            "theory_domain": "Reliability of model judgments over repeated runs (important for trusting LLM-generated evaluations/theories).",
            "theory_description": "Assesses the reproducibility of model judgments when presented the same evaluation task multiple times.",
            "evaluation_results": "GPT-4V outperformed Gemini on consistency: e.g., pairwise consistency ~0.675 for GPT-4V vs lower for Gemini; scoring and batch consistency lower (GPT-4V scoring 0.611, batch 0.418).",
            "benchmarks_or_datasets": "Stability tests conducted on representative subsets from the benchmark across datasets.",
            "comparison_to_human": "Consistency compared to human intra-annotator stability (human agreement studies); GPT-4V more consistent than Gemini but still shows substantial variability particularly for scoring and batch tasks.",
            "limitations_or_challenges": "Temperature/randomness settings affect consistency; some models show instability even at low temperature; inconsistency erodes trust in using models as sole evaluators of generated theories.",
            "uuid": "e6165.8",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "AnalysisGrading",
            "name_full": "Analysis grading and hallucination detection (human-rated)",
            "brief_description": "Human evaluation of MLLM provided analyses on dimensions (relevance, accuracy, creativity, granularity) scored 1–5 and explicit human labeling of hallucinations in analysis chains per vision-language hallucination definitions.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Annotators assign 1–5 scores to model analyses and mark the presence/absence of hallucinations using a decision-tree protocol; agreement on hallucination labels measured.",
            "evaluation_criteria": "Analysis score (1–5), hallucination detection binary/labeling, and use of decision-tree guidelines to improve inter-annotator consistency.",
            "llm_model_name": "Applied to analyses produced by GPT-4V, Gemini and other MLLMs during 'Analyze-then-Judge' experiments.",
            "theory_domain": "Assessing the internal reasoning/analysis quality of LLM judgments (relevant for evaluating explanatory theories produced by LLMs).",
            "theory_description": "Evaluates the quality and factuality of model reasoning chains that lead to judgments; identifies hallucinated/invented claims in analyses.",
            "evaluation_results": "Annotators achieved high consensus on hallucination labeling using a decision-tree; multi-step CoT reduced hallucinations but did not consistently improve alignment with human preferences.",
            "benchmarks_or_datasets": "Applied to analysis outputs on the MLLM-AS-A-JUDGE-HARD subset and other sampled items.",
            "comparison_to_human": "Human labels used as ground truth for hallucination and analysis quality; models often produced hallucinations that humans could detect reliably.",
            "limitations_or_challenges": "Human detection is time-consuming; decision-tree improves reliability but some hallucinations remain subtle; reducing hallucinations didn't always increase preference alignment.",
            "uuid": "e6165.9",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CoT-Analyze",
            "name_full": "Analyze-then-Judge & multi-step Chain-of-Thought (CoT)",
            "brief_description": "Prompting paradigm where the judge first provides an analysis (reasoning chain) and then issues a judgment; tested in one-step ('Analyze-then-Judge') and multi-step CoT (3-step) variants to study effects on hallucination and alignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Prompt judges to produce analysis then decision; compare judgments with/without multi-step CoT using the same metrics (Pearson, pairwise accuracy, Levenshtein, hallucination rates).",
            "evaluation_criteria": "Alignment metrics (Pearson, accuracy, Levenshtein), hallucination rate (human-labeled), and effect on consistency/quality of judgments.",
            "llm_model_name": "Tested with GPT-4V and Gemini primarily; some MLLMs (LLaVA, CogVLM) lacked capability to follow full analyze-then-judge prompts.",
            "theory_domain": "Reasoning transparency and robustness of judgments (relevant to evaluating reasoning underpinning candidate theories).",
            "theory_description": "Uses explicit intermediate reasoning steps to potentially improve factuality and explainability of final judgments.",
            "evaluation_results": "Multi-step CoT reduced hallucination rates (notably on HARD subset and image-related tasks) but did not consistently improve alignment with human preferences and in some datasets decreased judging performance, especially for Gemini; GPT-4V produced longer analyses and benefitted partially in hallucination reduction.",
            "benchmarks_or_datasets": "Applied to subsets including MLLM-AS-A-JUDGE-HARD and sampled datasets for CoT ablations.",
            "comparison_to_human": "Reduced hallucinations made outputs more factually grounded per human raters but alignment (preference matching) sometimes decreased, implying trade-offs between factuality and preference alignment.",
            "limitations_or_challenges": "Longer reasoning chains can propagate hallucinations; multi-step CoT increases opportunity for self-interference and can degrade final judgments if earlier steps hallucinate.",
            "uuid": "e6165.10",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "VisionExp",
            "name_full": "Vision perception / Vision expert description experiment",
            "brief_description": "Two approaches to supply visual information to judges: (1) full image input to MLLMs and (2) textual, detailed image descriptions provided by strong LLMs (vision experts) to non-visual LLM judges; tested impact on judging performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Compare judging performance when judges have direct vision input vs. when they receive a detailed textual description of the image vs. no vision information; measure via Pearson, pairwise metrics, and edit distance.",
            "evaluation_criteria": "Pearson similarity (scoring), pairwise accuracy (with/without ties), normalized edit distance (batch), and overall alignment with human judgments.",
            "llm_model_name": "Vision descriptions generated by LLaMA-70b, Mixtral-8x7b, GPT-3.5; judged by GPT-4V and other LLMs with/without vision input.",
            "theory_domain": "Effect of visual grounding on evaluation quality (important when assessing multimodal evidence supporting theories).",
            "theory_description": "Tests hypothesis that supplying high-quality textual image descriptions to text-only LLM judges improves their ability to evaluate multimodal outputs.",
            "evaluation_results": "Providing detailed image descriptions to text LLMs notably improved scoring Pearson to ~0.435 (for GPT-4V with vision exp) and in some pairwise settings exceeded standard MLLM judging; judges without any vision performed substantially worse.",
            "benchmarks_or_datasets": "Applied across standard evaluation subsets; Table 6 summarizes improvements for LLaMA2-70b, Mixtral-8x7b, GPT-3.5 and GPT-4V with vision descriptions.",
            "comparison_to_human": "Judging with vision descriptions increased alignment with human judgments compared to no-vision baselines, suggesting high-quality textual grounding can compensate for direct visual processing.",
            "limitations_or_challenges": "Quality of vision descriptions strongly conditions performance; generating comprehensive descriptions requires capable vision-LMs and introduces another source of error.",
            "uuid": "e6165.11",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "BiasHall",
            "name_full": "Bias and hallucination diagnostics & mitigation",
            "brief_description": "Analysis and reporting of systematic biases (egocentric/self-preference, position bias, length/verbosity bias) and hallucination frequency across evaluation formats, plus mitigation experiments (multi-step CoT, more examples in prompts, vision experts).",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Empirical analysis of score distributions, positional choice frequencies, response-length vs score correlations, human-labeled hallucination detection; mitigation via CoT steps and prompt/example augmentation tested.",
            "evaluation_criteria": "Distributional analyses (score histograms), length-vs-score curves, positional preference rates, hallucination rate (% detected), and effect sizes after mitigation.",
            "llm_model_name": "Observed in GPT-4V, Gemini, LLaVA family, Qwen-VL, CogVLM.",
            "theory_domain": "Reliability and fairness of model judgments relevant to trusting model assessments of generated theories.",
            "theory_description": "Characterizes systematic deviations from human-like judgment that undermine trustworthiness (e.g., self-scoring bias, always favoring position A, favoring longer answers).",
            "evaluation_results": "Detected egocentric bias (GPT-4V slight self-preference), position bias (strong in LLaVA and CogVLM), length bias (both GPT-4V and Gemini increase scores for longer answers; experiments showed average gains ~0.6–0.75 points when semantic length increased), and higher hallucination rates in Batch Ranking; multi-step CoT reduced hallucination rates substantially but did not uniformly improve preference alignment.",
            "benchmarks_or_datasets": "Bias analyses conducted across the benchmark datasets and special HARD subset designed to expose hallucination/inconsistency.",
            "comparison_to_human": "Bias patterns deviate from human judge behaviour; models sometimes rely on alignment or safety tuning (e.g., privacy emphasis) producing systematic preferences unlike diverse human judgments.",
            "limitations_or_challenges": "Mitigations (CoT, prompt examples, vision experts) partially reduce issues but do not eliminate biases; need for RLHF/DPO and human-in-the-loop checks recommended for robust correction.",
            "uuid": "e6165.12",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "HQ/HARD",
            "name_full": "MLLM-AS-A-JUDGE-HQ and MLLM-AS-A-JUDGE-HARD datasets",
            "brief_description": "Two curated subsets released by the authors: HQ contains responses with high agreement to human judgments; HARD contains instances with model-human inconsistency and manifestations of hallucination to stress-test judge capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Use HQ as reliable evaluation/test examples and HARD to diagnose hallucination and failure modes; both support training/validation for RLHF/DPO and robustness checks.",
            "evaluation_criteria": "HQ: high human-MLLM concordance; HARD: high incidence of hallucination/bias or low human agreement.",
            "llm_model_name": "Used to evaluate GPT-4V, Gemini, Qwen-VL, LLaVA, CogVLM etc. and as testbeds for mitigation experiments.",
            "theory_domain": "Benchmarking and stress-testing MLLM judgment capabilities (proxy for evaluating contentious or error-prone generated theories).",
            "theory_description": "Datasets consisting of image-instruction-response items selected for either high human agreement (HQ) or marked difficulty/hallucination (HARD).",
            "evaluation_results": "Multi-step CoT and other mitigations reduced hallucination rates on HARD subset (e.g., reported reductions across formats: Score 46.15%/48.72%/33.33% depending on figure/instruction variants — see Table 7 for per-format improvements).",
            "benchmarks_or_datasets": "Part of the public release accompanying the paper (project homepage provided).",
            "comparison_to_human": "HARD highlights cases where MLLM judgments diverge from human judges; HQ exemplifies close human-model agreement.",
            "limitations_or_challenges": "HARD demonstrates persistent failure modes even for top models (GPT-4V) requiring further research and intervention (RLHF/DPO, human-in-loop).",
            "uuid": "e6165.13",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "EmpiricalSummary",
            "name_full": "Empirical results summary (GPT-4V vs others)",
            "brief_description": "Key quantitative findings comparing top MLLMs across the three judging tasks showing GPT-4V leading on alignment with humans, especially in pairwise tasks, while other models lag and exhibit systematic biases.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method": "Aggregate metrics reported per model and per task across datasets (Pearson for scoring, accuracy/F1 for pair, Levenshtein for batch), plus human agreement and consistency analyses.",
            "evaluation_criteria": "Pearson similarity, pairwise accuracy/tie calibration, normalized Levenshtein distance, human agreement %, consistency scores (weighted avg & MCC).",
            "llm_model_name": "GPT-4V, Gemini-Pro-Vision, LLaVA-1.5/1.6, Qwen-VL-Plus/Max, CogVLM.",
            "theory_domain": "Model judgment performance in multimodal evaluation tasks; results inform trust in LLMs as evaluators of generated outputs/explanations.",
            "theory_description": "Not a theory — collated performance outcomes indicating how well models can serve as judges across modalities and task formats.",
            "evaluation_results": "GPT-4V: scoring Pearson ≈0.490 average; pair w. tie avg ≈0.636, pair w.o. tie avg ≈0.773; human agreement ~70% (scoring) and ~78–79% (pair). Gemini and many open models show 'high-score' bias, position bias, lower Pearson (e.g., Gemini ≈0.304), and reduced consistency. Batch ranking remains challenging for all models.",
            "benchmarks_or_datasets": "Results computed across the benchmark's datasets (COCO, Chart, Math, Graphics, Diffusion, Text reading, WIT, CC-3M, etc.).",
            "comparison_to_human": "Quantitative comparisons show GPT-4V closer to humans than other models but still with nontrivial gaps, especially for scoring and batch ranking tasks.",
            "limitations_or_challenges": "Even the top model shows biases, hallucinations, length/position effects, and inconsistent judgments; improvements needed before MLLMs can be considered fully reliable judges of complex generated theories.",
            "uuid": "e6165.14",
            "source_info": {
                "paper_title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "JudgeLM",
            "rating": 2
        },
        {
            "paper_title": "AUTO-J",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Prometheus-vision: Vision-language model as a judge for fine-grained evaluation",
            "rating": 2
        },
        {
            "paper_title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
            "rating": 1
        },
        {
            "paper_title": "Detecting and preventing hallucinations in large vision language models",
            "rating": 1
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 1
        }
    ],
    "cost": 0.02224325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
11 Jun 2024</p>
<p>Dongping Chen 
Huazhong University of Science and Technology</p>
<p>Ruoxi Chen 
Zhejiang University of Technology</p>
<p>Shilin Zhang 
Huazhong University of Science and Technology</p>
<p>Yaochen Wang 
Huazhong University of Science and Technology</p>
<p>Yinuo Liu 
Huazhong University of Science and Technology</p>
<p>Huichi Zhou 
Huazhong University of Science and Technology</p>
<p>Qihui Zhang 
Yao Wan <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#97;&#110;&#121;&#97;&#111;&#64;&#104;&#117;&#115;&#116;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#119;&#97;&#110;&#121;&#97;&#111;&#64;&#104;&#117;&#115;&#116;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
Huazhong University of Science and Technology</p>
<p>Pan Zhou <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#112;&#97;&#110;&#122;&#104;&#111;&#117;&#64;&#104;&#117;&#115;&#116;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#112;&#97;&#110;&#122;&#104;&#111;&#117;&#64;&#104;&#117;&#115;&#116;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a>. 
Huazhong University of Science and Technology</p>
<p>Lichao Sun 
Huazhong University of Science and Technology</p>
<p>LAIR Lab
Lehigh University</p>
<p>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
11 Jun 2024A8DC28216103A06AFE797E6CC6EF62E1arXiv:2402.04788v3[cs.CL]
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence.However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences.Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking.Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking.Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V.These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators.In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges.The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.</p>
<p>Introduction</p>
<p>The advent of Large Language Models (LLMs), such as GPT-3 (OpenAI, 2023) and Llama (Touvron et al., 2023), has achieved substantial progress in content generation, including text generation (OpenAI, 2023), code generation (Roziere et al., 2023), and video synthesis (Wu et al., 2023a).The emergent abilities of LLMs, as demonstrated by the Chain-of-Thought (CoT) framework (Wei et al., 2022), present a promising avenue for their utilization as evaluators, also referred to as the LLM-as-a-Judge (Zheng et al., 2023b).Initial explorations indicate a better alignment with human preferences, emphasizing the considerable potential inherent in this approach.</p>
<p>Recently, building upon LLMs, Multimodal Large Language Models (MLLMs) like GPT-4V (OpenAI, 2023) and LLaVA (Liu et al., 2023d) exhibit exceptional proficiency by incorporating multiple modalities (e.g., text, charts, images, and videos) and showcasing remarkable performance in multimodal applications, including text-to-video (Wu et al., 2023a) and visual dialog (Cai et al., 2023).Despite this, assessing the effectiveness of MLLMs remains challenging due to the limitations of traditional metrics, which hinge on text-based exact matches or embedding distances.These metrics fall short in adhering to the granular evaluation criteria of interest and fail to capture the rich context within the generated outputs.Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, a pertinent research question arises: "Can MLLMs effectively serve as judges in the multimodal domain, and how closely do their evaluations align with human preferences?"</p>
<p>To answer this question, this paper undertakes an extensive study, introducing a groundbreaking benchmark, MLLMas-a-Judge, specifically crafted to evaluate the efficacy of MLLMs in assisting judges across diverse modalities.To achieve this goal, we first thoughtfully curate a selection of 14 datasets across various tasks, including image captioning, math reasoning, text reading, and infographics understanding, culminating in acquiring a dataset comprising 4,414 image-instruction pairs.Subsequently, we utilize six mainstream MLLMs from a model pool which includes GPT-4V (OpenAI, 2023), Gemini (GeminiTeam, 2023) 1 , LLaVA-1.5-13b,LLaVA-1.6-34b(Liu et al., 2023d), CogVLM (Wang et al., 2023c), Qwen-VL-Max (Bai et al., 2023a), to generate responses to each instruction across three distinct evaluation settings.The produced responses are subsequently gathered and undergo additional annotation by human evaluators, who apply stringent criteria to ensure an impartial and thorough assessment of the judgments made by the MLLMs.Furthermore, we assess the ability of MLLMs as judges in multimodal tasks by calculating the similarity between human and MLLMs judgment and measuring human agreement on the analysis and judgment made by those MLLMs.In particular, we target eleven widely-used MLLMs, i.e., GPT-4V and Gemini-Pro-1.0/1.5, CogVLM, LLaVA-1.5/1.6 family, and Qwen-VL family, across two settings (with, or without vision input), over three distinct tasks (i.e., Scoring Evaluation, Pair Comparison, and Batch Ranking).Figure 1 compares the performance of various MLLMs across different datasets and settings, illustrating that GPT-4V exhibits significantly superior capabilities as a judge compared to other MLLMs.</p>
<p>As a benchmark, we also release two curated datasets to facilitate further studies: MLLM-AS-A-JUDGE-HQ, which showcases responses with a high level of concordance with human judgments, and MLLM-AS-A-JUDGE-HARD, which includes responses marked by inconsistency with human preferences and instances of hallucination.Additionally, we address the limitations of MLLMs in judgment, such as egocentric bias, position bias, length bias, and hallucination.We demonstrate that integrating CoT (Wei et al., 2022) and a vision expert system can effectively mitigate some of these biases.</p>
<p>Finally, the presence of hallucinations, biases, and inconsistent judgments in MLLMs highlights significant challenges that must be addressed for these models to become a viable alternative to traditional human evaluations.</p>
<p>To summarize, our work provides three key contributions:</p>
<p>• A Benchmark.We are the first to develop a comprehensive benchmark MLLM-as-a-Judge in multimodal domains, with human annotations to assess the judging capability of MLLMs in tasks of Scoring Evaluation, Pair Comparison and Batch Ranking.• Two Datasets.We curate two human preference datasets: MLLM-AS-A-JUDGE-HQ, which contains high-quality questions, and MLLM-AS-A-JUDGE-HARD, which includes instances of hallucination.These datasets can serve as rigorous testing grounds to facilitate the development of MLLMs in aligning human preferences.• Findings and Implications.Our evaluation of mainstream MLLMs reveals that while MLLMs exhibit alignment with human judgments in Pair Comparison, notable discrepancies can be found in Scoring Evaluation and Batch Ranking.Furthermore, our findings reveal that MLLMs exhibit a range of biases and hallucinations, along with inconsistent judgments during the evaluation process, representing significant hurdles in establishing MLLMs as reliable judges.</p>
<p>Instruction Random Sample</p>
<p>Step 1：Image-Instruction Pair Collection</p>
<p>Step 2：MLLM Response Collection</p>
<p>Step 3：MLLM Judge v.s.</p>
<p>Human Annotation</p>
<p>Analyze:</p>
<p>What is unusual about this picture?</p>
<p>Images</p>
<p>Image-Instruction Pairs</p>
<p>What is the Area of CHD? Describe this image.</p>
<p>What percentage of workers are not working from home?</p>
<p>Someone said this man is an angel, why?</p>
<p>… …</p>
<p>Assistant A:</p>
<p>The answer is….</p>
<p>Judgement: 4</p>
<p>Scoring evaluation</p>
<p>Pair comparison</p>
<p>Batch ranking MLLM response</p>
<p>Assistant A:</p>
<p>The number is….</p>
<p>Assistant B:</p>
<p>As for the number….</p>
<p>Judgement: B Assistant A:</p>
<p>The year is….</p>
<p>Assistant B:</p>
<p>As for the year….</p>
<p>Assistant C:</p>
<p>The answer is...</p>
<p>Assistant D:</p>
<p>The year is …. 2. MLLM-as-a-Judge: A Benchmark to Assess Vision-Language Judging Ability Ranking.Each subset will be employed for specific judging tasks, with each of them being configured as follows.</p>
<p>Judgement: CBAD</p>
<p>…</p>
<p>• Scoring Evaluation: Each individual response is evaluated on a scale from 1 to 5, with the specific criteria for this rating system detailed in Appendix F. • Pair Comparison: It involves a direct comparison between two responses, culminating in the identification of the superior one.Following the principles outlined by (Deutsch et al., 2023), a tie option is incorporated to ensure a more equitable assessment.• Batch Ranking: The responses are systematically arranged in descending order of quality based on a given instruction, without any tie option.</p>
<p>Step 1: Image-Instruction Pair Collection</p>
<p>We meticulously curate a dataset consisting of 4,414 imagetext pairs, gathered from a variety of downstream task datasets, as detailed in Table 8 in Appendix B. These pairs are carefully tailored into image-instruction pairs to suit a free-form response format.To illustrate, within the domain of diffusion tasks, our dataset incorporated pairs challenging models to adeptly recognize and articulate connections between provided images and user-specified keywords.</p>
<p>Step 2: MLLM Response Collection</p>
<p>We employ six widely-used MLLMs -GPT-4V (OpenAI, 2023), Gemini (GeminiTeam, 2023), LLaVA (Liu et al., 2023d), Qwen-VL-Max (Bai et al., 2023a), LLaVA-1.6-34b(Liu et al., 2023d), and CogVLM (Wang et al., 2023c 1 and Figure 3, respectively.We show specific hyper-parameter settings in Appendix B.2. Besides, we segment these responses into three non-overlapping groups, to prevent response overlap.</p>
<p>Step 3: Comparison with Human Annotations</p>
<p>The annotation is conducted by 6 authors of this paper independently.These annotators are proficient in this domain, with different genders, ages, and educational backgrounds to ensure diversity (Sun et al., 2020).They are required to give objective judgments without considering answer lengths, and certain names or positions of the response to minimize human bias.More details are referred to Appendix E.</p>
<p>Experiment Settings</p>
<p>Settings of MLLM-as-a-Judge</p>
<p>We evaluate the judging performance of eleven leading MLLMs -GPT-4V (OpenAI, 2023), Gemini-Pro-Vision-1.0 (GeminiTeam, 2023), LLaVA-1.5-13b,LLaVA-1.6-7b/13b/34b(Liu et al., 2023d), Qwen-VL-Plus/Max (Bai et al., 2023a) and CogVLM (Wang et al., 2023c) -across three distinct evaluation settings.Adapting the "Analyzethen-Judge" paradigm from Chiang &amp; Lee (2023b), which is a one-step CoT approach (Wei et al., 2022), we first ask MLLMs to analyze responses and then provide a judgment based on their analysis.However, due to capability limitations to perform the "Analyze-then-Judge" setting for LLaVA and CogVLM, we prompt them to directly output their judgment.We also evaluate whether multi-step CoT</p>
<p>Judging Metrics</p>
<p>After collecting responses from MLLM judgments, we quantify their alignment with human annotations across three settings, employing distinct metrics as follows:</p>
<p>▷ Scoring Evaluation: Following LLM-as-a-Judge (Zheng et al., 2023b), we compute the Pearson similarity (Lee Rodgers &amp; Nicewander, 1988) between the MLLMs' judgments and human ratings across different sub-datasets.</p>
<p>▷ Pair Comparison: We assess the similarity between the MLLM judgments and human decisions using accuracy, F1-score (Goutte &amp; Gaussier, 2005), and recall (Goutte &amp; Gaussier, 2005) to assess the judging abilities of models.</p>
<p>▷ Batch Evaluation: We consolidate the ranking results into a singular sequence and employ the Normalized Levenshtein distance (Levenshtein et al., 1966) to evaluate the similarity between judgments from MLLMs and human annotation.</p>
<p>Human Agreement in MLLM Judgment</p>
<p>Apart from traditional metrics for similarity assessment between judgments from MLLMs and humans, we further evaluate the judgments provided by MLLMs to uncover latent bias and hallucination in 10 datasets.We also invite human annotators for further validation, focusing on the following aspects: While some judgments might appear reasonable, they may still be considered incorrect due to unique human perspectives.Hence, we conduct experiments on human agreement to address situations that traditional metrics may not adequately capture.</p>
<p>▷ Analysis Grading: Each MLLM analysis is assigned a score from 1 to 5, considering relevance, accuracy, creativity, and response granularity, detailed in Appendix F.</p>
<p>▷ Hallucination Detection: Given the propensity for hallucination issues in the complex reasoning chains and longterm vision-language contexts of MLLMs, we task human annotators with identifying any hallucinations in the analyses of MLLM judgments, adhering to established definitions of vision and language hallucination (Sun et al., 2024).</p>
<p>Empirical Results and Analysis</p>
<p>MLLM Judgment vs Human Annotation</p>
<p>As shown in Figure 1 and Table 3, judgments made by GPT-4V are closer to human annotations among all settings, while Gemini is far different, with LLaVA, CogVLM and Qwen-VL-Max are even worse.Overall, MLLM judgments perform better on Pair Comparison, while falling short in Scoring Evaluation and Batch Ranking, showing a huge gap between the model and human preferences.Under the "Analyze-then-Judge" setting, GPT-4V prefers to give a longer judge in all settings, convincing its ability to reason on long-term text.</p>
<p>▷ Scoring Evaluation: GPT-4V demonstrates the highest similarity to human scoring with a similarity score of 0.490.In contrast, Gemini achieves only 0.304, with LLaVA and CogVLM scoring even lower.This discrepancy is mainly due to Gemini's tendency to assign scores around 4 points as depicted in Figure 4, seldom giving 1 or 2 points.LLaVA and CogVLM show a pattern similar to Gemini, predominantly assigning scores around 4 points.We attribute this to a 'High-Score' Bias, akin to the 'Yes/No' bias identified by Liu et al. (2023a), which may result from an imbalance in positive and negative judging instructions in their training data (Liu et al., 2023b), severely limits their ability to provide just and varied scores in scoring settings.In comparison, GPT-4V's scores are more evenly distributed and align closely with human preferences.</p>
<p>▷ Pair Comparison: As illustrated in Figure 4, GPT-4V outshines other MLLMs in pair comparison tasks, achieving 0.636 in tie settings and 0.773 in non-tie settings, surpassing 0.8 in many datasets, which indicate a strong alignment with human preferences.Gemini, LLaVA, and CogVLM show a marked preference for declaring a clear winner, possibly due to a lack of tie situations in their training, leading to biased judgments.It's also interesting that the frequency of ties given by GPT-4V closely mirrors that of human judges, suggesting similar thresholds for tie decisions.</p>
<p>▷ Batch Ranking: GPT-4V aligns more closely with human   Notably, CogVLM is unable to provide a full ranking in this context, offering only the top choice; so it was excluded from this comparison; LLaVA also exhibits position bias influenced by prompt structure, often replicating judgments seen in example prompts, which complicates its ability to produce fair judgments.</p>
<p>MLLM Judging Consistency</p>
<p>To be a reliable judge, consistent decision-making across repeated evaluations of the same query is crucial.For this purpose, we conduct six repeated tests with MLLM judgments and calculated the weighted average consistency scores and Majority Consistency Criterion ratios for GPT-4V and
6FRUH 3DLU %DWFK 3URSRUWLRQ &amp;RQVLVWHQF\&amp;KHFNLQJ &amp;DWHJRU\ Figure 5
. Consistency checking on 6 repetitions of experiments on GPT-4V (Left) and Gemini (Right).GPT-4V outperforms Gemini with a relatively higher ratio for high consistency.</p>
<p>Gemini, as shown in Table 4 and Figure 5.Despite a higher temperature setting, GPT-4V substantially outperforms Gemini across all tasks.Particularly in Pair Comparison, GPT-4V achieves a higher consistency score of 0.675, but it encounters difficulties in maintaining similar levels of consistency in Scoring and Batch Ranking tasks, with scores dropping to 0.611 and 0.418, indicating the challenge of producing qualified and convincing judgments.</p>
<p>Human Agreement</p>
<p>Our manual evaluation of MLLMs on agreement and scoring, revealed notable findings.Table 3 shows that GPT- In Scoring Evaluation, GPT-4V achieves a 70% human agreement rate, peaking at 79.9% in MS-COCO, while Gemini averaged 67.7%.To assess the consistency of MLLM judging quality across multiple responses to a single imageinstruction pair, we use Mean Absolute Deviation (MAD) metric to measure the average absolute variance between individual scores and the mean.Figure 18 shows that GPT-4V exhibits lower variation in quality assessments, indicating more consistent and reliable judgment compared to Gemini.However, in Batch Ranking, both models exhibited decreased alignment with human judgments, especially in Maths and graphic information processing, suggesting that models may lack the capabilities to fully comprehend user instructions, leading to less reliable judgments.</p>
<p>Multi-steps CoT Do Not Enhance Performance</p>
<p>We have conducted additional tests using GPT-4V and Gemini with a 3-step CoT approach for judging, as detailed in Table 5.Our analysis reveals that while employing CoT with additional steps markedly reduces hallucinations in judgments, it does not align more closely with human preferences.On numerous datasets, this approach even diminishes judging performance.Specifically, Gemini's effectiveness drops more drastically.With 3-step CoT, there is an increased likelihood that the judgment will be disturbed by its understanding of the figure and its own responses to the instruction, thereby undermining its final judgment if hallucinations exist in the previous chain.</p>
<p>Vision Perception Benefits MLLM Judging</p>
<p>We explore the feasibility of using LLMs for judging textbased responses without directly analyzing the original images.This involves two approaches: omitting vision information entirely and providing a detailed description of the picture.We choose LLaMA-70b, Mixtral8x7b-v0.1 and GPT-3.5 to provide descriptions.Surprisingly, as illustrated in Table 6, we find that LLMs' performance in multimodal judging tasks significantly improve with picture descriptions, achieving a Pearson similarity of 0.435 in Scoring Evaluation tasks, markedly outperformed judgments made without any vision perception.Notably, in no-tie Pair Comparison, MLLMs with detailed vision descriptions even exceed the standard performance of MLLMs in judging.This suggests that MLLMs may lack certain human-like judging capabilities, while LLMs can be potential judges for multimodal tasks when provided with comprehensive task-related descriptions.</p>
<p>Bias and Hallucination</p>
<p>Egocentric Bias.Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024).In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity.Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment.In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines.For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics.Despite efforts in prompt engineer-  ing to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022).This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'.</p>
<p>Position Bias.Model consistently favor answers in specific positions, often influenced by training data that typically places correct responses at the beginning or end of prompts (Liu et al., 2023e).Figure 4 illustrates bias in LLaVA and CogVLM during Pair Comparison tasks, where they consistently prefer answers in a specific position.This bias likely arises from their limited ability to follow complex instructions, leading them to be influenced by prompt structure.For example, if a Batch Ranking prompt includes a sequence like 'ABCD', LLaVA replicates this sequence in 88.2% of responses, significantly more than other sequences.However, this bias can be reduced by introducing multiple examples, suggesting that prompts with more examples can better direct these models to follow instructions accurately.</p>
<p>Length Bias.Models tend to prefer longer answers over concise but correct ones (Li et al., 2024), also known as verbosity bias (Zheng et al., 2023b).Figure 6 shows that both GPT-4V and Gemini assign higher scores to longer content.We conducted an expanded scoring experiment using GPT-4 (OpenAI, 2023) without vision, increasing the semantic length of answers without changing their original intent.In Figure 7, we observe noticeable score increases, with GPT-4V and Gemini showing average gains of 0.6 and 0.75 points, respectively.These results suggest that MLLMs may favor longer text for higher scores.</p>
<p>Hallucination Detection and Mitigation.We observe a higher frequency of hallucinations in Batch Ranking, compared to Pair Comparison and Scoring Evaluation.These hallucinations involved significant misinterpretations and retrieval errors, impacting judgment accuracy and reliability.</p>
<p>To address this, we employed a multi-step CoT approach on MLLM-AS-A-JUDGE-HARD, adding reasoning steps before the conventional "Analyze-then-Judge" process.This enhanced procedure included: 1) image-instruction, 2) image, and 3) instruction.In Table 7, this strategy effectively reduced hallucinations across all formats, with significant improvements in tasks involving image-related information.</p>
<p>In the Batch Ranking task, which requires handling longer text sequences, the detailed reasoning steps were particularly effective in reducing hallucinations.</p>
<p>Scaling Law for MLLM-as-a-Judge</p>
<p>We conduct two sets of experiments with models of different sizes, the LLaVA-1.6 series models and the Qwen series models in four newly added datasets, illustrated in Figure 10 and 11.In Score evaluation, LLaVA-1.6-34b and Qwen-VL-Max slightly outperform others in Math, Chart, and Text tasks, showing a relatively strong scaling law.</p>
<p>Related Work</p>
<p>LLM as a Judge.The evolution of LLMs has made them increasingly effective evaluators in Natural Language Processing (NLP) tasks.Zhu et al. (2023) introduced JudgeLM for LLM evaluation, followed by AUTO-J (Li et al., 2023a), aligning closely with human judgment (Bai et al., 2023b;Li et al., 2023d;Kim et al., 2023).Advancements in CoT reasoning (Wei et al., 2022;Chu et al., 2023) and training-free instruction following (Brown et al., 2020;Wei et al., 2021) further extend LLMs' judging capability in diverse tasks like translation quality assessment (Kocmi &amp; Federmann, 2023) and story generation (Chiang &amp; Lee, 2023a).</p>
<p>Hallucination and Bias in Judgments.MLLMs suffer from vision and language hallucinations (Ji et al., 2023;Huang et al., 2023a;Cui et al., 2023;Wang et al., 2023a), often due to vision-language misalignments in training phase (Sun et al., 2024;Huang et al., 2023b).Recent research focuses on hallucination evaluation (Liu et al., 2023a), detection (Li et al., 2023e;Wang et al., 2023a), and mitigation (Yin et al., 2023;Gunjal et al., 2023;Zhou et al., 2023), noting that even GPT-4V suffer from these issues (Shi et al., 2023;Liu et al., 2023a;Cui et al., 2023).Besides, biases in MLLM-as-a-Judge, similar to those in human decision-making (Blunch, 1984;Raghubir &amp; Valen-zuela, 2006) and other ML domains (Wang et al., 2018;Liu et al., 2023e), such as position (Zheng et al., 2023a), egocentric (Li et al., 2024), and verbosity biases (Saito et al., 2023), are compounded by the integration of visual perception, necessitating further investigation.</p>
<p>Future Directions</p>
<p>Multimodal RLHF/DPO.Our work is highly connected with multimodal RLHF/DPO (Sun et al., 2023;Li et al., 2023c;Yu et al., 2023a).Our dataset includes extensive human annotations, such as manually assigned scores and preference on pairs, which could serve as invaluable training material for RLHF reward models and supply paired data essential for DPO (Rafailov et al., 2024;Zhang et al., 2024), paving the way for enhancing the training of MLLMs.</p>
<p>Exploring the upper bound of MLLM-as-a-Judge.Beyond expanding the steps in the Chain of Thought prompting (Wei et al., 2022), we see significant potential in more sophisticated reasoning frameworks, such as multi-agent debating (Chan et al., 2023) when MLLM acts as a Judge, which could enhance the judging accuracy through improved reasoning capabilities.Additionally, addressing inherent biases in the model during the judgment process is crucial.For instance, position bias in Pair Comparison and Batch Ranking (Zheng et al., 2023a;Wang et al., 2024a), and the tendency to assign higher scores, as discussed in (Lee et al., 2024), are critical areas for improvement.</p>
<p>Incorporating a human-in-the-loop approach (Wang et al., 2023b) offers a promising solution to enhance judgment consistency and reliability.For example, if judgment results vary in more than half of several repeated judgments, it may need human intervention for consistency checking.When it's challenging to discern the MLLM's judgment due to non-compliance with the suggested output format or lack of a clear outcome, human intervention may be required to refine this process by manually verifying judgments.</p>
<p>Conclusion</p>
<p>In this paper, we have presented a new benchmark, termed MLLM-as-a-Judge, to assess the judging capabilities of MLLMs across three critical evaluation settings in the multimodal domain: Scoring Evaluation, Pair Comparison, and Batch Ranking.We further evaluate their agreement with humans.Our results reveal that advanced MLLMs can win significant human recognition in Pair Comparisons, but perform poorly in Scoring Evaluation and Batch Ranking tasks.Our work highlights potential areas for future refinement and improvement of MLLMs.We advocate for additional efforts dedicated to supporting the continuous development of MLLMs as judges.</p>
<p>Impact Statement</p>
<p>In this paper, we introduce a novel benchmark, termed MLLM-as-a-Judge, designed to propel the evolution of MLLMs toward achieving judgments that align more closely with human perspectives.This benchmark establishes a heightened criterion for assessing MLLMs, emphasizing their proficiency in comprehending and processing information in a manner reflective of human cognitive processes.One limitation of our work lies in the bias in human annotation and MLLMs.We leave the exploration of more objectives, ethically principled, and socially beneficial MLLMas-a-Judge systems as our future work.</p>
<p>A. Comprehensive Related Works</p>
<p>A.1. Large Model as Judge</p>
<p>The rapid development of LLMs has significantly enhanced their capabilities in long-term context perception and reasoning, increasingly popularizing their use as evaluators in various Natural Language Processing (NLP) tasks.Zhu et al. (2023) were pioneers in this area, introducing JudgeLM, a fine-tuned LLM designed for evaluating other LLMs.Building on this, Li et al. (2023a) introduced AUTO-J, a system that evaluates LLMs through both pairwise comparisons and single-response assessments, demonstrating close alignment with human judgment (Bai et al., 2023b;Li et al., 2023d;Kim et al., 2023).Further advancements in LLMs, such as the development of Chain-of-Thought reasoning (Wei et al., 2022;Chu et al., 2023), training-free instruction following (Brown et al., 2020;Wei et al., 2021), and enhanced alignment with human preferences (Ouyang et al., 2022), have solidified their role in diverse tasks like translation quality assessment (Kocmi &amp; Federmann, 2023) and story generation (Chiang &amp; Lee, 2023a).</p>
<p>A.2. Hallucination and Bias in Judge</p>
<p>MLLMs are known to exhibit both vision hallucination and hallucination originating from LLMs, a phenomenon typically characterized by responses containing information not present in the visual or natural language context (Ji et al., 2023;Huang et al., 2023a;Cui et al., 2023;Wang et al., 2023a).This issue often stems from misalignments in vision-language training (Sun et al., 2024;Huang et al., 2023b).Recent studies have begun to address these hallucination issues, focusing on evaluation (Liu et al., 2023a), detection (Li et al., 2023e;Wang et al., 2023a), and mitigation strategies (Yin et al., 2023;Gunjal et al., 2023;Zhou et al., 2023).Notably, GPT-4V (OpenAI, 2023), despite being a leading model in many fields (Yang et al., 2023;Wu et al., 2023b), has also demonstrated susceptibility to hallucinations (Shi et al., 2023;Liu et al., 2023a;Cui et al., 2023).This raises concerns about the reliability of MLLMs in evaluative roles.</p>
<p>In terms of bias, MLLM judging is subject to issues not exclusive to our context of evaluation but also observed in human decision-making (Blunch, 1984;Raghubir &amp; Valenzuela, 2006) and Machine Learning (ML) domains (Wang et al., 2018;Liu et al., 2023e;Huang et al., 2024a) such as position bias (Zheng et al., 2023a), egocentric bias (Li et al., 2024), and verbosity bias (Saito et al., 2023).The integration of visual perception in MLLMs introduces additional complexities, resulting in biases unique to the fusion of dual perceptions, an area that still demands thorough exploration.</p>
<p>A.3. Evaluating Large Multimodal Models</p>
<p>Evaluating MLLMs typically involves diverse tasks and corresponding metrics, which reflect the models' ability to comprehend and generate content based on both visual and textual information.For instance, in image captioning tasks, models are tasked with generating descriptive text for a given image.The effectiveness of these models is measured using metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee &amp; Lavie, 2005), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015).In the context of Visual Question Answering (VQA), models are evaluated based on their ability to answer questions on an image's content.Here, the accuracy of model responses is compared against human-annotated answers, serving as the primary metric (Antol et al., 2015) to ensure alignment with human preferences.</p>
<p>However, when tackling sophisticated visual-language tasks, conventional evaluation metrics often fail to accurately capture the nuanced responses generated by these models, especially in complex or subjective scenarios that involve both visual elements and extended textual content (Liu et al., 2023a).Additionally, while manual annotation offers a more comprehensive and human-like evaluation, it comes with significant challenges.These include high costs (Prendki, 2023), potential biases (Zheng et al., 2023b), and the difficulty of ensuring consistent replication (Chiang &amp; Lee, 2023a).These limitations highlight the need for a more holistic approach to evaluation, one that combines human-like calibration with more fine-grained assessment methods.</p>
<p>B. Detailed Benchmark Construction</p>
<p>B.1. Step 1: Image-Instruction Collection</p>
<p>To attain the outlined objectives, our approach begins with a detailed analysis of the capabilities of MLLMs.Specifically, we focus on the following abilities within MLLMs:</p>
<p>• Recognition Ability: This encompasses general visual recognition capabilities, such as object recognition, Optical</p>
<p>LLaVA-1.6-34b.For the sequential dataset Mementos (Wang et al., 2024b), we leverage GPT-4V, Qwen-VL-Max, ChatUnivi (Jin et al., 2023), VideoChat2 (Li et al., 2023b) to generate responses.Upon collecting a total of 17,656 responses from the MLLMs, we proceed to analyze the distribution of response lengths for each model.Figure 8</p>
<p>C. Detailed Experiment Settings</p>
<p>C.1. Response VLM Settings</p>
<p>We use GPT-4V, Gemini, LLaVA-1.5-13b,CogVLM, Qwen-VL-Max, LLaVA-1.6-34b to answer the image-instruction pair.We discuss their hyperparameter settings and problems encountered during inference respectively:</p>
<p>• GPT-4V (OpenAI, 2023).We set the temperature and top-p as 0.9, max-token as 2048.However, we encounter some situations where it cannot answer accurately or refuses to answer due to ethical issues like Unfortunately, due to my programming, I'm unable to ..., which brings some difficulties to us in defining its judging capability.</p>
<p>• Gemini (GeminiTeam, 2023).We use the default settings, which set temperature as 0.4, top-p as 1, and max-token as 2048.It should be noted that Gemini will receive more ethical limitations than GPT-4V, and will refuse to answer on the diffusion data set.But for some more difficult questions, it can't answer the questions, but it will "forcibly answer" the user's questions.In this case, GPT-4V will sincerely admit its shortcomings and give a possible answer.</p>
<p>• LLaVA-1.5-13b(Liu et al., 2023d).We set temperature as 0, tok-p as 1, max-token as 2048, and beam search number as 3.The reason why we select such a low temperature is that LLaVA cannot correctly output its judge in a specific format.We collect responses by inference on a dual-4090 local server.</p>
<p>• CogVLM (Wang et al., 2023c).For the hyper-parameter, we use the default setting and set max-token as 2048.We collect responses by inference on a dual-4090 local server.</p>
<p>• Qwen-VL Family (Bai et al., 2023a).We use the default settings for Qwen-VL family, with top-p as 0.8 and max-token as 2048.</p>
<p>• LLaVA-1.6 Family (Liu et al., 2023c).We set the temperature as 0.4 and top-p as 0.9, max-token as 2048.</p>
<p>C.2. GPT-4V as Judge</p>
<p>We adopt GPT-4V without using JSON Mode mod based on our preliminary experiment in Appendix D.3, but required it to output in JSON format in our prompt.Following the hyper-parameter set in (Chiang &amp; Lee, 2023b), we set the temperature to 0.9, top-k to 0.9, and max-token to 2048 in both cases with and without pictures.When there is a format error in the output or ethical settings are triggered, we will sample again.If it is still wrong, we will skip this piece of data.</p>
<p>C.3. Gemini-Vision-Pro as Judge</p>
<p>We call Gemini's API on the Google Cloud Server and use the default settings for temperature and top-k.It should be noted that even though Gemini is currently the only VLM that can perform Judge, it will occasionally speak Chinese or Japanese during the output process when using English text and English pictures, shown as Unicode in Figure 9.Although it is semantically continuous, that is, it can be used after translation, and it greatly affects the continuity of the judge.We believe that this is a capability issue, and it is speculated that it is affected by the multi-linguist in some training sets.</p>
<p>C.4. Other MLLMs as Judge</p>
<p>We also use other mainstream MLLMs such as CogVLM (Wang et al., 2023c) and LLaVA (Liu et al., 2023d) for judging, which hyper-parameters follow the response generation settings.Some issues arise when we use the basic "Analyze-then-Judge" setting, where they cannot follow the judging instructions very well.Therefore we do not put their results into the results of the text.Recently, we have also noticed the advent of GLM-4V2 , which has shown good performance on some benchmarks and can receive long-term text and follow the instructions for judging.However, due to the time constraints, we have not completed tests on GLM-4V in our work.</p>
<p>Instruction: Here is the caption of this figure: human hands touch an alchemical circle.According to this figure and the caption, please give a detailed analysis of this figure:</p>
<p>Answer:</p>
<p>Analysis: The assistant's response is in Japanese.It provides a detailed analysis of the figure, including the symbols on the alchemical circle and the possible meaning of the hands touching it.The response is well-written and easy to understand."Judgment: 5</p>
<p>D. Additional Experimental Results</p>
<p>D.1. Full Results on Judging Performance</p>
<p>We provide full results of judging performance of different MLLMs in Table 9. Comparative performance for four newly added datasets are shown in Figures 10 and 11.</p>
<p>In Scoring Evaluation, all models demonstrated comparable performance levels on the original dataset presented in our study, with LLaVA-1.6-34b and Qwen-vl-max slightly outperforming others in Math, Chart, and Text tasks, yet none surpassing GPT-4V.Our analysis of Qwen-vl-max and Qwen-vl-plus revealed a propensity to assign higher scores, with 80% of their ratings falling between 4-5 points, and a noticeable absence of 1-2 point scores.This inclination towards higher scores is more pronounced compared to other models.The LLaVA-1.6 series, although slightly better, also tends to award scores within the 3-5.In Pair comparison, qwen-vl-plus and max performed better on certain datasets, distinguishing themselves from competitors.Notably, qwen-vl-max exhibited less positional bias than LLaVA models, which showed a strong predisposition to favor one position, typically rating 'A' as better.n Batch Ranking, the updated Gemini-Pro-Vision model outperforms others overall.Both Qwen and LLaVA series demonstrated that larger model sizes correlate with better outcomes, affirming a strong scaling law effect.Despite these findings, there remains a noticeable gap between these models</p>
<p>D.3. Preliminary Experiment</p>
<p>Human Agreement on GPT-4V Output Mode.The recently introduced 'Json Mode'3 in GPT-4V represents a significant advancement, particularly in structuring outputs in JSON format while restricting token usage.This mode has been observed to regularize responses, a feature particularly advantageous when dealing with structured data.However, this structuring tends to compartmentalize responses, potentially leading to a loss in the natural flow and contextual linkage typically inherent in human-like responses.This segmentation might inadvertently affect the readability and perceived coherence of the generated text.</p>
<p>To quantitatively assess the impact of Json Mode on output quality and its alignment with human preferences, we meticulously construct a test set.This set comprises 50 data instances, randomly selected from three distinct datasets used for evaluation purposes.The objective is to discern human evaluators' predilection for the outputs generated in Json Mode by GPT-4V.</p>
<p>For a comprehensive analysis, we engage three annotators, each responsible for labeling the data.Their assessments aim to discern the balance between structured, JSON-formatted responses and the inherently fluid nature of human judgment and preference in textual content, as shown in Figure 12.</p>
<p>-VRQ0RGH3UHIHUHQFH 7LH -VRQ0RGH 1R-VRQ0RGH Human Agreement Bias Checking Acknowledging the inherent variability in human annotations, we embark on an empirical study involving ten annotators to ascertain the reliability of derived statistical patterns, notwithstanding the subjective nature of human judgment.This study aims to mitigate the individual biases that might skew the evaluation of GPT-4's outputs.A dataset comprising 50 entries, processed using the GPT-4 pair comparison setting, serves as the foundation for this investigation.</p>
<p>The results, detailed in Figure 13, underscore a noteworthy observation: while the annotators exhibit minimal variance in determining the correctness of GPT-4's judgments, a discernible divergence emerged in the scoring of analytical responses.</p>
<p>This divergence presumably stems from individual perceptual differences and inherent biases.However, it's crucial to note that these discrepancies in scoring did not significantly compromise the overall integrity of the annotations.</p>
<p>A remarkable consensus is observed in the labeling of hallucinations.The employment of a meticulously defined decision tree for identifying hallucinations ensures a high degree of uniformity across the annotations.This structured approach substantially minimizes errors, underscoring the effectiveness of well-defined criteria in achieving consistent and reliable annotations across different individuals.</p>
<p>D.4. Length Distribution on MLLM Judgments Analysis</p>
<p>In our analysis, we have included length distribution diagrams that showcase the differences in the responses provided by GPT-4V and Gemini during their judgment tasks as illustrated in Figure 14.These diagrams reveal that GPT-4V typically generates longer responses than Gemini in both Scoring Evaluation (Figure 15) and Pair Comparison (Figure 16), whereas in the batch task (Figure 17), the output lengths from both models are comparatively similar.</p>
<p>E. Human Labeling and Agreement Collection</p>
<p>The annotation is conducted by 6 authors of this paper independently.As acknowledged, the diversity of annotators plays a crucial role in reducing bias and enhancing the reliability of the benchmark.These annotators have knowledge in this domain, with different genders, ages, and educational backgrounds.To ensure the annotators can proficiently mark the data, we provide them with detailed tutorials, teaching them how to evaluate model responses more objectively.Specifically, they are required to give judgments without considering answer lengths, and certain names or positions of the response.Besides, we implement cross-validation between different annotators and conduct continuous monitoring to ensure they are maintaining objectivity and fairness.</p>
<p>In the Human agreement experiment performed by humans on MLLM Judge, the prompt we give humans is presented in Figure 21 and Figure 22.   result1-1.name:Judge VLM 1 result1-1.judge:C result1-1.analysis:Both assistants correctly answer the question and provide the same answer with sufficient supporting details. result1-2.name:Judge VLM 2 result1-2.judge:B result1-2.analysis:Both Assistant A and Assistant B correctly identified Shaun Alexander as the career touchdown leader for the Seattle Seahawks with 112 touchdowns, as shown in the bar chart.Assistant B provided additional context by mentioning that Steve Largent is second with 101 career touchdowns, offering a comparative perspective.While Assistant A's answer is accurate, Assistant B's answer adds a layer of detail that enriches the response without detracting from its relevance or accuracy.</p>
<p>organized by WOW Spazio Fumetto.Judgement : Based on the analysis, I rank the responses as follows:</p>
<p>[[C]], [[D]], [[B]], [[A]</p>
<p>] Assistant C provides the most comprehensive and detailed response, clearly addressing the user's instruction and inquiry by identifying the year and providing supporting evidence from the image.Assistant D also offers a thorough explanation, along with additional information about the event.Assistant B's response is concise but lacks the level of detail provided by Assistants C and D. Assistant A's response is the briefest and lacks any explanation or supporting information.result1-2.name:Judge VLM 2 result1-2.analysis:Assistant A: The response correctly identifies the year at the top of the magazine as 2014.However, it provides no additional information or context to support the answer., Assistant B: This response also correctly identifies the year as 2014 and goes further to describe the appearance of the text, which supports the user's instruction for detail and provides reasoning for the answer.</p>
<p>Figure 1 .
1
Figure 1.Comparative performance of different MLLMs across three judging settings in 10 datasets, each is the average of three iterations.As the CogVLM is unable to perform the batch ranking task, we show the other six MLLMs only.</p>
<p>Figure 2 .
2
Figure 2.An overview of MLLM-as-a-Judge.</p>
<p>Figure 2
2
Figure2shows an overview of our proposed MLLM-as-a-Judge, consisting of three steps: 1) image-instruction pair collection, 2) MLLM response collection, and 3) comparison with human annotation.Initially, we collect a dataset P = {(M 1 , I 1 ), . . ., (M n , I n )}, containing pairs of images (M ) and their corresponding instructions (I) sourced from 10 diverse domains (e.g., math, chart, diffusion), ensuring comprehensive coverage for a wide array of downstream tasks.Subsequently, each pair (M i , I i ) is processed through several MLLMs, generating a set of responses R i = {r 1 , r 2 , . . ., r n } for each pair.This process contributes to the formation of the dataset of image-instruction-responses pairs, denoted as D = {(M i , I i , R i )|(M i , I i ) ∈ P}.Finally, the dataset D is partitioned into three distinct subsets to facilitate diverse task evaluations: D score for Scoring Evaluation, D pair for Pair Comparison, and D batch for Batch Ranking.Each subset will be employed for specific judging tasks, with each of them being configured as follows.</p>
<p>Figure 4 .
4
Figure 4. Pair Comparison density (Left) and Scoring Evaluation density (Right) of different MLLMs judgments and human annotations.</p>
<p>Figure 6 .Figure 7 .
67
Figure 6.Length bias in 10 datasets.The horizontal axis represents length, and the vertical axis represents density.</p>
<p>Figure 8 .
8
Figure 8. Response length distribution in 10 datasets.The horizontal axis represents length, and the vertical axis represents density.</p>
<p>Figure 9 .Figure 10 .Figure 11 .
91011
Figure 9. Gemini speaks Japanese.</p>
<p>Figure 12 .
12
Figure 12.Json Mode Preference Analysis.</p>
<p>Labeling and Agreement Bias Checking.</p>
<p>Figure 13 .
13
Figure 13.Human Labeling and Agreement Bias.</p>
<p>Figure 14 .Figure 15 .Figure 16 .Figure 17 .
14151617
Figure 14.Length distribution in analysis collections.</p>
<p>Figure 19 .
19
Figure 19.Scoring Density of Different MLLMs in Judging.</p>
<p>Figure 21 .
21
Figure 21.Human agreement</p>
<p>(</p>
<p>System Prompt) You are a helpful assistant proficient in analyzing vision reasoning problems.(Instruction) Please serve as an unbiased judge in assessing the quality of the responses from AI assistants regarding the user's instruction and a figure.You will be presented with several responses from different assistants to the same user instruction.Your task is to assess and compare these responses based on how effectively they adhere to the user's original instruction and how aptly they address the user's inquiry.After your assessment and comparison, you should RANK the responses from best to worst as the following template.If Assistant A is the best response, Assistant D is the worst response, you should output like[[A]], [[B]], [[C]], [[D]]" (Noticement)Your assessment should identify whether the assistant effectively adheres to the user's instructions and addresses the user's inquiry.In your evaluation, weigh factors such as relevance, accuracy, comprehensiveness, creativity, and the granularity of the responses.Do not allow the length of the responses to influence your evaluation.Do not favor certain names or positions of the assistants.Be as objective as possible.Indicate your final rank in the key 'Judgement'.(Desired Output Format) [The Start of User Instruction].{item['instruction']} An example of pair comparison Instruction: This is a task of reasoning on chart data.Please analyze this figure in detail and answer the following question with reason based on this figure.Who is the career touchdown leader of the Seattle Seahawks?answer1.name:Assistant A answer1.answer:The career touchdown leader of the Seattle Seahawks is Shaun Alexander with 112 touchdowns as illustrated by the bar chart, where his bar is the longest and the numerical value of 112 is the highest among all the listed players.answer2.name:Assistant B answer2.answer:The answer is Shaun Alexander.Shaun Alexander has 112 career touchdowns, the most of any player on the Seattle Seahawks.Steve Largent is second with 101 career touchdowns.</p>
<p>, Assistant C: The response is accurate in identifying the year as 2014 and provides a detailed description of where the date is located and what the text around it indicates.It adds context by mentioning the text color and the image below the date, giving a more comprehensive understanding., Assistant D: Assistant D accurately identifies the year as 2014 and gives a translation of the other text around the year, providing additional context.However, it slightly misquotes the text, stating '1 GIUGNO' instead of '10 Giugno,' which could be seen as a minor error in accurately reading the text.Judgement: Rank: [[C], [B], [A], [D]]</p>
<p>Table 1 .
1
The statistics of responses in different steps for MLLM judging.In Step 3, under the w.o.vision input settings, we sample 10% from the original data and mainly proceed with GPT-4V and Gemini.We only list the amount of judgments generated by four models here.M-I: Image-Instruction.
Step Setting InputNum. OutputNum.1/Image Instruction 4,414 4,144M-I Pairs4,4002/M-I Pairs3,300 MLLMs17,096Gemini1,340Batch1,470GPT-4V Qwen-VL-Max 1,458 1,454w. Vision InputPair8,256LLaVA Gemini GPT-4V Qwen-VL-Max 8,012 1,468 7,751 8,117 LLaVA 8,253Gemini5,337Score5,883GPT-4V Qwen-VL-Max 5,701 5,708LLaVA5,7293w.o. Vision InputNo Vision Vision ExpertsBatch Pair Score Batch Pair Score110 425 612 110 425 612Gemini GPT-4V Gemini GPT-4V Gemini GPT-4V Gemini GPT-4V Gemini GPT-4V Gemini GPT-4V107 110 385 355 582 584 107 110 396 425 576 612</p>
<p>Table 2 .
2
The overall performance of different MLLMs in judging, compared with human annotations on different datasets.We sample all the data three times and took the average to mitigate the casualty.w. and w.o.tie represents tie and non-tie situations respectively.We omit Gemini's results on the diffusion task for its challenges in processing AI-generated images.All presented data of Pearson similarity exhibit a p-value below 0.05, indicating a statistically significant level of confidence.Please refer to the Appendix D.1 for more results.
SettingsMLLMCOCO C.C.Diff. Graphics Math Text WIT Chart VisIT CC-3M M2W SciQA Aes MM-Vet Ave.LLaVA-1.5-13b 0.247 0.227 0.0600.2420.093 0.245 0.109 0.237 0.177 0.071 0.424 0.279 0.4140.3220.225LLaVA-1.6-34b 0.285 0.251 -0.0120.2620.238 0.258 0.151 0.318 0.198 0.109 0.022 0.206 0.0250.2650.184Score (↑)Gemini0.262 0.408-0.4000.228 0.222 0.418 0.343 0.336 0.374 0.324 0.073 0.3600.2070.304GPT-4V0.454 0.507 0.4580.6450.606 0.624 0.579 0.645 0.620 0.431 0.185 0.383 0.4010.3260.490Qwen-vl-max0.311 0.117 0.0720.2180.175 0.196 0.028 0.312 0.151 0.045 0.244 0.115 0.1770.2160.170LLaVA-1.5-13b 0.273 0.478 0.2860.2730.657 0.510 0.369 0.383 0.456 0.484 0.347 0.223 0.3890.2540.384LLaVA-1.6-34b 0.493 0.600 0.5700.3000.374 0.551 0.543 0.254 0.398 0.392 0.513 0.434 0.5240.4990.460Pair w. Tie (↑)Gemini0.616 0.787-0.6500.436 0.664 0.605 0.500 0.660 0.560 0.370 0.262 0.1900.3120.509GPT-4V0.696 0.824 0.8470.6390.564 0.673 0.679 0.657 0.640 0.612 0.521 0.415 0.6060.5290.636Qwen-vl-max0.403 0.464 0.3720.4940.438 0.500 0.533 0.479 0.421 0.421 0.411 0.392 0.3250.4740.438LLaVA-1.5-13b 0.327 0.537 0.3020.3000.726 0.684 0.600 0.610 0.648 0.583 0.449 0.443 0.4980.3440.504LLaVA-1.6-34b 0.607 0.824 0.8550.4020.587 0.750 0.758 0.381 0.503 0.564 0.712 0.679 0.6940.7620.648Pair w.o. Tie (↑)Gemini0.717 0.840-0.7700.678 0.793 0.688 0.658 0.711 0.652 0.471 0.358 0.2650.4000.615GPT-4V0.804 0.870 0.9220.8070.801 0.805 0.734 0.849 0.761 0.703 0.699 0.647 0.7550.6590.773Qwen-vl-max0.657 0.674 0.5560.6670.635 0.732 0.647 0.638 0.560 0.586 0.608 0.646 0.7410.6620.644LLaVA-1.5-13b 0.577 0.492 0.5620.5350.598 0.650 0.616 0.644 0.620 0.563 0.639 0.563 0.6500.6520.597LLaVA-1.6-34b 0.449 0.411 0.5000.5610.575 0.544 0.483 0.552 0.542 0.479 0.529 0.437 0.5000.4500.501Batch (↓)Gemini0.287 0.299-0.4730.462 0.430 0.344 0.520 0.426 0.357 0.613 0.412 0.4670.5290.432GPT-4V0.318 0.353 0.0700.3850.348 0.319 0.290 0.347 0.300 0.402 0.597 0.462 0.4530.4110.361Qwen-vl-max0.477 0.407 0.5000.4800.507 0.515 0.493 0.539 0.468 0.407 0.563 0.503 0.4440.5000.486
▷ Human Agreement: This involves a simple 'yes' or 'no' response to assess agreement with the MLLM judgments.</p>
<p>Table 3 .
3
Human agreement percentage on MLLM-as-a-Judge in 10 datasets.Each judgment is independently reviewed three times by different annotators and consensus results are recorded.Gemini failed in diffusion tasks and its results are omitted.
SettingsMLLM COCO C.C. Diffusion Graphics MathTextWIT Chart VisIT CC-3M AverageScore (↑)Gemini GPT-4V0.783 0.739 0.799 0.725-0.5060.618 0.6880.536 0.621 0.749 0.630 0.712 0.638 0.706 0.714 0.676 0.7790.702 0.7540.677 0.699Pair (↑)Gemini GPT-4V0.705 0.833 0.821 0.926-0.8730.733 0.7940.520 0.717 0.827 0.620 0.853 0.618 0.752 0.790 0.796 0.7970.703 0.7660.724 0.793Batch (↓)Gemini GPT-4V0.642 0.639 0.663 0.639-0.9120.333 0.5360.330 0.473 0.511 0.315 0.422 0.475 0.615 0.641 0.640 0.6220.554 0.4670.469 0.621Density of Pair Comparison ResultDensity of Scoring Result0.80.80.60.6Density0.4Density0.40.20.20.0A WinB WinTie0.012345HumanGPT-4V(ision)Gemini-Vision-ProCogVLMLLaVA</p>
<p>Table 4 .
4
Consistency
comparisons of GPT-4V and Gemini in 10datasets. Average means weighted average for consistency times,"MCC" stands for "Majority Consistency Criterion", which deemsresponses consistent if over half of them are identical across our 6repetitions of experiments.MLLMScore Average MCC Average MCC Average MCC Pair BatchGemini0.5310.0540.7810.5470.6290.338GPT-4V0.7960.6110.8360.6750.6790.418ranking results, indicating a significant lead with a meanLevenshtein Distance of 0.361. However, there is still sub-stantial room for improvement in this task for all MLLMs.</p>
<p>Table 5 .
5
Results of GPT-4V and Gemini-Pro acting as a judge with a 3-step CoT approach in a selected subset.Settings MLLM COCO C.C. Diffusion Graphics Math Text WIT Chart VisIT CC-3M Ave.
GPT-4V0.454 0.5070.4580.6450.606 0.624 0.579 0.645 0.620 0.431 0.557Score (↑)GPT-4V (+CoT) 0.246 0.165 Gemini 0.262 0.4080.192 -0.385 0.4000.397 0.400 0.298 0.443 0.423 0.038 0.299 0.228 0.222 0.418 0.343 0.336 0.374 0.299Gemini (+CoT)0.127 0.0680.1170.2200.132 0.182 0.105 0.140 0.222 0.128 0.144GPT-4V0.696 0.8240.8470.6390.564 0.673 0.679 0.657 0.640 0.612 0.683Pair w. Tie (↑)GPT-4V (+CoT) 0.507 0.657 Gemini 0.616 0.7870.561 -0.601 0.6500.515 0.580 0.489 0.521 0.646 0.553 0.563 0.436 0.664 0.605 0.500 0.660 0.560 0.609Gemini (+CoT)0.233 0.2390.4200.2070.284 0.329 0.352 0.357 0.247 0.239 0.291GPT-4V0.804 0.8700.9220.8070.801 0.805 0.734 0.849 0.761 0.703 0.806Pair w.o. Tie (↑)GPT-4V (+CoT) 0.673 0.821 Gemini 0.717 0.8400.845 -0.707 0.7700.738 0.787 0.548 0.756 0.753 0.654 0.728 0.678 0.793 0.688 0.658 0.711 0.652 0.723Gemini (+CoT)0.267 0.2750.5730.2640.414 0.424 0.427 0.511 0.299 0.319 0.377GPT-4V0.323 0.3440.0920.4010.367 0.341 0.302 0.364 0.313 0.407 0.325Batch (↓)GPT-4V (+CoT) 0.428 0.416 Gemini 0.287 0.299--0.427 0.4730.434 0.401 0.366 0.406 0.422 0.472 0.419 0.462 0.430 0.344 0.520 0.426 0.357 0.400Gemini (+CoT)0.441 0.4810.5420.5950.494 0.533 0.483 0.569 0.486 0.463 0.5094V achieved around 70% human agreement across all set-
(Zheng et al., 2023b)he Pair Comparison task with 79.3% agreement.Specifically, GPT-4V reached 78% in human agreement for Pair Comparison, with Gemini close at 72%, indicating strong performance in most sample pairs and supporting the idea that large models excel in pairwise distinctions(Zheng et al., 2023b), though improvements are needed in other judging settings.</p>
<p>Table 6 .
6
How vision perception significantly enhances multimodal judging performance in traditional LLM-as-a-Judge setting, slightly outperforming MLLMs in judging.Vision Exp.stands for judging with a detailed image description.
MLLMSettingsScore (↑) PearsonPair (↑) w. Tie w.o. Tie Edit Dis. Batch (↓)LLaMA2-70bVision Exp No Vision0.060 0.1260.404 0.3740.550 0.5370.643 0.583Mixtral-8x7bVision Exp No Vision0.054 0.1510.374 0.4780.543 0.7310.603 0.546GPT-3.5Vision Exp No Vision0.154 0.2230.453 0.4590.591 0.6440.473 0.504GPT-4VVision Exp No Vision0.435 0.2990.544 0.4910.878 0.8680.400 0.394GeminiVision Exp No Vision0.120 0.1080.438 0.4330.785 0.7580.472 0.470</p>
<p>Table 7 .
7
Reduction of hallucinations in MLLM-AS-A-JUDGE-HARD through additional CoT steps compared to normal setting.
SettingFigure-instructionFigureInstructionScore46.15%48.72%33.33%Pair28.21%35.90%33.33%Batch43.59%35.90%35.90%</p>
<p>Table 10 .
10
Judging performance on image sequence dataset Mementos.
MLLMScore (↑) PearsonPair (↑) w. Tie w.o. Tie Edit Dis. Batch (↓)GPT-4V0.3610.7210.8360.411ChatUnivi-0.0940.1580.1680.556Qwen-vl-plus 0.1150.4260.4820.5Qwen-vl-max 0.0460.4460.5310.63</p>
<p>Please examine the provided image attentively and serve as an unbiased judge in assessing the quality of responses from two AI assistants regarding the user's question shown beneath the image.(Noticement)Yourassessment should identify the assistant that more effectively adheres to the user's instruction and aptly addresses the user's inquiry.In your evaluation, weigh factors such as relevance, accuracy, comprehensiveness, creativity, and the granularity of the responses.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.Present your verdict in a JSON format, with the key 'analysis' for a short reason of your judgement and the key 'judgment' to indicate your decision: use "[[A]]" if assistant A prevails, "[[B]]" if assistant Bdoes, and "[[C]]" for a tie.(Desired Output Format) [The Start of User Instruction].{item['instruction']} [The End of User Instruction] [The Start of Assistant A's Answer] {item['answer1']['answer']} [The End of Assistant A's Answer] [The Start of Assistant B's Answer] {item['answer2']['answer']} [The End of Assistant B's Answer]
Template prompts of pair comparison(System Prompt)You are a helpful assistant proficient in analyzing vision reasoning problems.(Instruction)Template prompts of batch ranking
https://open.bigmodel.cn/
https://openai.com/blog/new-models-and-developer-products-announced-at-devday
Character Recognition (OCR), and other high-level tasks in computer vision(Yu et al., 2023b).• Comprehension Ability: This pertains to the model's proficiency in spatial understanding and scenario comprehension.• Inferential Ability: This involves the model's capacity to process information and reasoning, a critical component in processing charts, graphs, and mathematics.• Multilingual Ability: This assesses the model's competence in understanding and processing multiple languages, especially focusing on their appearance in visual tasks such as text reading on images(Singh et al., 2019).To ensure a robust and comprehensive assessment, we meticulously identify and incorporate 10 diverse datasets 8 into our evaluation framework.This strategic selection aims to enrich the diversity of our assessment tasks and enhance the breadth and depth of our evaluation capabilities, as well as prevent biases.These datasets are chosen based on their ability to effectively challenge the various aspects of MLLMs, via different downstream tasks, ensuring a thorough and nuanced understanding of their performance and potential.To construct a robust and unbiased set of image-instruction pairs, we randomly select 300 images from each dataset, ensuring a diverse representation.Specifically, for the MathVista dataset, which includes the provision of hints, we extract 600 corresponding instructions, encompassing both scenarios: with and without hints.For the remaining datasets, we align 300 instructions with the sampled images.This process culminates in a comprehensive collection comprising 4,114 images corresponding with 4,414 instructions.B.2. Step 2: MLLM Responses CollectionWe engage with 4 mainstream MLLMs (i.e., GPT-4V, Gemini, LLaVA, CogVLM) by providing them with our assembled image-instruction pairs for the first 3,300 image-instruction pairs, each VLM generated a response, resulting in a comprehensive collection of 13,200 answers, with each of the 3,300 instructions receiving a distinct response from each of the four MLLMs.For the last 4 datasets, we added during the rebuttal, we leverage GPT-4V, Gemini, Qwen-VL-Max, and and the top-performing GPT-4V, particularly concerning positional bias.Table9.The overall performance of different MLLMs in judging, compared with human annotations on different datasets.We sample all the data three times and took the average to mitigate the casualty.w. and w.o.tie represents tie and non-tie situations respectively.We omit Gemini's results on the diffusion task for its challenges in processing AI-generated images.All presented data of Pearson similarity exhibit a p-value below 0.05, indicating a statistically significant level of confidence.Notice: Gemini-Pro * means Gemini-1.0-Pro-latest.D.2. Judging Results on Sequential ImagesWe incorporated the sequential image dataset Mementos, comprising picture sequences, to expand our MLLM-as-a-Judge framework into the video domain in a pioneering effort.Each sequence, featuring over four images, draws from daily life, comics, and robotics.For data generation in Step 3, we utilized GPT-4V, Qwen-VL-Max, Qwen-VL-Plus, and Video-LLM Chatunivi, obtaining 100 image-text pairs for batch evaluations, 381 for scoring, and 560 for pair comparisons.Beyond analyzing GPT-4V and Qwen-vl-max, we explored Video-LLM's judging capabilities, specifically testing it with ChatUnivi.As illustrated inPrompts for Human Agreement ExperimentYour assessment should identify whether the assistant effectively adheres to the user's instructions and addresses the user's inquiry.Do not allow the length of the responses to influence your evaluation.Do not favor certain names or positions of the assistants.Be as objective as possible.In your evaluation, weigh factors such as relevance, accuracy, comprehensiveness, creativity, and the granularity of the responses: Relevance: The judge's decisions directly correspond to the provided instructions or criteria.Every judgment made is pertinent to the case at hand, without deviation into unrelated areas.Accuracy: judge's decisions are consistently in line with the established rules or guidelines.There is a clear understanding and correct application of these guidelines in every judgment.Comprehensiveness: The judge considers all necessary aspects and evidence related to each case.Every relevant point in the guidelines is addressed in the judge's evaluation.Creativity: The judge demonstrates the ability to approach complex or ambiguous situations with innovative thinking.This includes providing insightful, constructive feedback or solutions not explicitly covered in the guidelines.Granularity of Responses: The judge offers detailed and specific reasoning for each decision.This entails a thorough breakdown of how each aspect of the guidelines applies to the case or situation at hand.F. Prompt TemplatesWe first query Judge VLM to get their responses by the following prompts.Query prompts of MLLMs in judging.You are a helpful assistant proficient in analyzing vision reasoning problems.[The Start of User Instruction] {item['instruction']} [The End of User Instruction] Please provide a detailed explanation for your response.FollowingChiang &amp; Lee (2023b)andLi et al. (2024), we have designed prompts and presented the prompt template of VLM's operation including score, pair comparison, and batch ranking judgments in a prompt template as system prompt, instruction, criteria, noticement, and desired output form:Template prompts of scoring evaluationYou are a helpful assistant proficient in analyzing vision reasoning problems.(Instruction)Please examine the provided image attentively and serve as an unbiased judge in assessing the quality of the response from an AI assistants regarding the instruction.You will receive a single response from the assistant to user's instruction.(Noticement) Your assessment should identify whether the assistant effectively adheres to the user's instructions and addresses the user's inquiry.In your evaluation, weigh factors such as relevance, accuracy, comprehensiveness, creativity, and the granularity of the responses.Do not allow the length of the responses to influence your evaluation.Do not favor certain names or positions of the assistants.Be as objective as possible.(Criteria) Use scores to show the quality of the response.Here is the detailed scoring rubric for evaluating the quality of responses from AI assistants:Poor (1): The response significantly deviates from the user's instruction and fails to address the query effectively.G. Case StudyWe list several case studies.Assistant A, B, C, D represent LlaVA, CogVLM, GPT-4V and Gemini, respectively.Judge VLM 1 and 2 represent Gemini and GPT-4V, respectively.For pair comparison, the judge result "C" means a tie.An example of score comparisonInstruction: Please analyze this figure in detail and answer the following question based on this figure: Who was the train built for?answer1.name: Assistant A answer1.answer:The train was built for children to play with in a park.The image shows a small red and black toy train parked on a dirt surface in a park, surrounded by trees.The train appears to be designed for children to ride on and enjoy, making it an ideal attraction for families visiting the park.result1-1.name:Judge VLM 1 result1-1.analysis:The assistant's response is relevant to the user's instruction and addresses the user's inquiry.The response is accurate and comprehensive, providing a clear and concise answer to the question.The response is also creative and uses detailed granularity to enhance the quality of the response.Judgement: 5 result1-2.name:Judge VLM 2 result1-2.analysis:The response provided by the AI assistant correctly identifies the purpose of the toy train in the image, stating that it was built for children to play with in a park.The description of the train as small, red and black, and parked on a dirt surface surrounded by trees is accurate based on the visual evidence in the image.Furthermore, the assistant correctly deduces that the train is designed for children to ride on and asserts that it is an ideal attraction for families, which is a logical conclusion given the context of the setting.The response is relevant to the user's instruction and demonstrates accuracy and a reasonable level of comprehensiveness.There is a minor display of creativity in the assistant's description of the train being an ideal attraction for families.Judgement result1-1.name:Judge VLM 1 result1-1.analysis:Assistant A: The assistant provides a concise and straightforward response, stating that the year at the top of the magazine is 2014.However, the response lacks any further explanation or supporting details.Assistant B: Similar to Assistant A, this assistant also briefly states that the year is 2014, but it adds a bit more detail by mentioning that the year is displayed prominently in large, bold letters, making it easily readable and the focal point of the image.Assistant C: Assistant C offers a more comprehensive response by not only stating the year as 2014 but also providing a clear explanation.The assistant points out the specific text on the magazine that indicates the date range of May 9 to June 10, 2014, and correctly concludes that the year at the top is 2014.Assistant D: Assistant D's response is comparable to that of Assistant C, as it includes both the identification of the year as 2014 and an explanation.However, Assistant D's explanation is slightly different, as it mentions the Italian translation of the date range, which is 9 MAGGIO -1 GIUGNO 2014.The response also includes additional information about the event, mentioning that it took place in Milan and was
Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>J Bai, S Bai, S Yang, S Wang, S Tan, P Wang, J Lin, C Zhou, J Zhou, arXiv:2308.12966Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. 2023aarXiv preprint</p>
<p>S Bai, S Yang, J Bai, P Wang, X Zhang, J Lin, X Wang, C Zhou, J Zhou, arXiv:2308.16890Touchstone: Evaluating visionlanguage models by language models. 2023barXiv preprint</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. Y Bitton, H Bansal, J Hessel, R Shao, W Zhu, A Awadalla, J Gardner, R Taori, L Schimdt, ArXiv, abs/2308.065952023</p>
<p>Position bias in multiple-choice questions. N J Blunch, Journal of Marketing Research. 2121984</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Low-code llm: Visual programming over llms. Y Cai, S Mao, W Wu, Z Wang, Y Liang, T Ge, C Wu, W You, T Song, Y Xia, arXiv:2304.081032023arXiv preprint</p>
<p>C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. C.-H Chiang, H.-Y Lee, arXiv:2305.019372023aarXiv preprint</p>
<p>A closer look into automatic evaluation using large language models. C.-H Chiang, H.-Y Lee, arXiv:2310.056572023barXiv preprint</p>
<p>Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, arXiv:2309.15402A survey of chain of thought reasoning: Advances, frontiers and future. 2023arXiv preprint</p>
<p>C Cui, Y Zhou, X Yang, S Wu, L Zhang, J Zou, H Yao, arXiv:2311.03287Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. 2023arXiv preprint</p>
<p>Towards a generalist agent for the web. X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, Mind2web, Advances in Neural Information Processing Systems. 362024</p>
<p>Ties matter: Metaevaluating modern metrics with pairwise accuracy and tie calibration. D Deutsch, G Foster, M Freitag, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>A family of highly capable multimodal models. Geminiteam, Gemini, 2023</p>
<p>A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. C Goutte, E Gaussier, European conference on information retrieval. Springer2005</p>
<p>Detecting and preventing hallucinations in large vision language models. A Gunjal, J Yin, E Bas, arXiv:2308.063942023arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, arXiv:2311.052322023aarXiv preprint</p>
<p>Decision mamba: Reinforcement learning via hybrid selective sequence modeling. S Huang, J Hu, Z Yang, L Yang, T Luo, H Chen, L Sun, B Yang, 2024a</p>
<p>Y Huang, Q Zhang, L Sun, arXiv:2306.11507A benchmark for trustworthy and responsible large language models. 2023barXiv preprint</p>
<p>Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception. Y Huang, Q Yuan, X Sheng, Z Yang, H Wu, P Chen, Y Yang, L Li, W Lin, arXiv:2401.082762024barXiv preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D De Las Casas, E B Hanna, F Bressand, G Lengyel, G Bour, G Lample, L R Lavaud, L Saulnier, M.-A Lachaux, P Stock, S Subramanian, S Yang, S Antoniak, T L Scao, T Gervet, T Lavril, T Wang, T Lacroix, Sayed, 2024W. E. Mixtral of experts</p>
<p>Chat-univi: Unified visual representation empowers large language models with image and video understanding. P Jin, R Takanobu, C Zhang, X Cao, L Yuan, arXiv:2311.080462023arXiv preprint</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. S Kim, J Shin, Y Cho, J Jang, S Longpre, H Lee, S Yun, S Shin, S Kim, J Thorne, arXiv:2310.084912023arXiv preprint</p>
<p>Large language models are state-of-the-art evaluators of translation quality. T Kocmi, C Federmann, arXiv:2302.145202023arXiv preprint</p>
<p>Prometheus-vision: Vision-language model as a judge for fine-grained evaluation. S Lee, S Kim, S H Park, G Kim, M Seo, arXiv:2401.065912024arXiv preprint</p>
<p>Thirteen ways to look at the correlation coefficient. The American Statistician. J Lee Rodgers, W A Nicewander, 198842</p>
<p>Binary codes capable of correcting deletions, insertions, and reversals. V I Levenshtein, Soviet physics doklady. Soviet Union196610</p>
<p>J Li, S Sun, W Yuan, R.-Z Fan, H Zhao, P Liu, arXiv:2310.05470Generative judge for evaluating alignment. 2023aarXiv preprint</p>
<p>K Li, Y Wang, Y He, Y Li, Y Wang, Y Liu, Z Wang, J Xu, G Chen, P Luo, arXiv:2311.17005A comprehensive multi-modal video understanding benchmark. 2023barXiv preprint</p>
<p>L Li, Z Xie, M Li, S Chen, P Wang, L Chen, Y Yang, B Wang, L Kong, Silkie, arXiv:2312.10665Preference distillation for large visual language models. 2023carXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. GitHub repository. X Li, T Zhang, Y Dubois, R Taori, I Gulrajani, C Guestrin, P Liang, T B Hashimoto, 2023d</p>
<p>Y Li, Y Du, K Zhou, J Wang, W X Zhao, J.-R Wen, arXiv:2305.10355Evaluating object hallucination in large visionlanguage models. 2023earXiv preprint</p>
<p>Leveraging large language models for nlg evaluation: A survey. Z Li, X Xu, T Shen, C Xu, J.-C Gu, C Tao, arXiv:2401.071032024arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>Microsoft coco: Common objects in context. T.-Y Lin, M Maire, S J Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, European Conference on Computer Vision. 2014</p>
<p>Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. F Liu, T Guan, Z Li, L Chen, Y Yacoob, D Manocha, T Zhou, arXiv:2310.145662023aarXiv preprint</p>
<p>Aligning large multi-modal model with robust instruction tuning. F Liu, K Lin, L Li, J Wang, Y Yacoob, L Wang, arXiv:2306.145652023barXiv preprint</p>
<p>Improved baselines with visual instruction tuning. H Liu, C Li, Y Li, Y J Lee, 2023c</p>
<p>H Liu, C Li, Q Wu, Y J Lee, Visual instruction tuning. 2023d</p>
<p>N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023earXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. P Lu, H Bansal, T Xia, J Liu, C Yue Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, Mathvista, ArXiv, abs/2310.022552023</p>
<p>ChartQA: A benchmark for question answering about charts with visual and logical reasoning. A Masry, D Long, J Q Tan, S Joty, E Hoque, doi: 10.18653Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>URL. </p>
<p>. M Mathew, V Bagal, R P Tito, D Karatzas, E Valveny, C Jawahar, Infographicvqa, </p>
<p>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2021233394125</p>
<p>Gpt-4 technical report. 2023. OpenAI. Openai models -gpt-4-vision. 2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Are you spending too much money labeling data?. J Prendki, 2023</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>Center-of-inattention: Position biases in decision-making. Organizational Behavior and Human Decision Processes. P Raghubir, A Valenzuela, 200699</p>
<p>Code llama: Open foundation models for code. B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, T Remez, J Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>K Saito, A Wachi, K Wataoka, Y Akimoto, arXiv:2310.10076Verbosity bias in preference labeling by large language models. 2023arXiv preprint</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, Annual Meeting of the Association for Computational Linguistics. 2018</p>
<p>Exploring ocr capabilities of gpt-4v (ision): A quantitative and in-depth evaluation. Y Shi, D Peng, W Liao, Z Lin, X Chen, C Liu, Y Zhang, Jin , L , arXiv:2310.168092023arXiv preprint</p>
<p>Towards vqa models that can read. A Singh, V Natarajan, M Shah, Y Jiang, X Chen, D Batra, D Parikh, M Rohrbach, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019. 201985553602</p>
<p>Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. K Srinivasan, K Raman, J Chen, M Bendersky, M Najork, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval2021</p>
<p>L Sun, Y Huang, H Wang, S Wu, Q Zhang, C Gao, Y Huang, W Lyu, Y Zhang, X Li, arXiv:2401.05561Trustworthiness in large language models. 2024arXiv preprint</p>
<p>Evolution and impact of bias in human and machine learning algorithm interaction. W Sun, O Nasraoui, P Shafto, Plos one. 158e02355022020</p>
<p>Aligning large multimodal models with factually augmented rlhf. Z Sun, S Shen, S Cao, H Liu, C Li, Y Shen, C Gan, L.-Y Gui, Y.-X Wang, Y Yang, arXiv:2309.145252023arXiv preprint</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Cider: Consensus-based image description evaluation. R Vedantam, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Evaluation and analysis of hallucination in large vision-language models. J Wang, Y Zhou, G Xu, P Shi, C Zhao, H Xu, Q Ye, M Yan, J Zhang, J Zhu, arXiv:2308.151262023aarXiv preprint</p>
<p>Large language models are not fair evaluators. P Wang, L Li, L Chen, D Zhu, B Lin, Y Cao, Q Liu, T Liu, Z Sui, arXiv:2305.179262023barXiv preprint</p>
<p>. W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, J Xu, B Xu, J Li, Y Dong, M Ding, J Tang, Cogvlm, 2023cVisual expert for pretrained language models</p>
<p>Position bias estimation for unbiased learning to rank in personal search. X Wang, N Golbandi, M Bendersky, D Metzler, M Najork, Proceedings of the eleventh ACM international conference on web search and data mining. the eleventh ACM international conference on web search and data mining2018</p>
<p>my answer is c": First-token probabilities do not match text answers in instruction-tuned language models. X Wang, B Ma, C Hu, L Weber-Genzel, P Röttger, F Kreuter, D Hovy, B Plank, arXiv:2402.144992024aarXiv preprint</p>
<p>X Wang, Y Zhou, X Liu, H Lu, Y Xu, F He, J Yoon, T Lu, G Bertasius, M Bansal, arXiv:2401.10529A comprehensive benchmark for multimodal large language model reasoning over image sequences. 2024barXiv preprint</p>
<p>A large-scale prompt gallery dataset for text-toimage generative models. Z J Wang, E Montoya, D Munechika, H Yang, B Hoover, D H Chau, Diffusiondb, ArXiv, abs/2210.148962022</p>
<p>J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Nextgpt: Any-to-any multimodal llm. S Wu, H Fei, L Qu, W Ji, T.-S Chua, arXiv:2309.055192023aarXiv preprint</p>
<p>Y Wu, S Wang, H Yang, T Zheng, H Zhang, Y Zhao, B Qin, arXiv:2310.16534An early evaluation of gpt-4v (ision). 2023barXiv preprint</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Z Yang, L Li, K Lin, J Wang, C.-C Lin, Z Liu, L Wang, arXiv:2309.17421202391arXiv preprint</p>
<p>S Yin, C Fu, S Zhao, T Xu, H Wang, D Sui, Y Shen, K Li, X Sun, E Chen, Woodpecker, arXiv:2310.16045Hallucination correction for multimodal large language models. 2023arXiv preprint</p>
<p>Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. T Yu, Y Yao, H Zhang, T He, Y Han, G Cui, J Hu, Z Liu, H.-T Zheng, M Sun, arXiv:2312.008492023aarXiv preprint</p>
<p>Mm-vet: Evaluating large multimodal models for integrated capabilities. W Yu, Z Yang, L Li, J Wang, K Lin, Z Liu, X Wang, L Wang, ArXiv, abs/2308.024902023b</p>
<p>Direct preference optimization of video large multimodal models from language model reward. R Zhang, L Gui, Z Sun, Y Feng, K Xu, Y Zhang, D Fu, C Li, A Hauptmann, Y Bisk, arXiv:2404.012582024arXiv preprint</p>
<p>On large language models' selection bias in multi-choice questions. C Zheng, H Zhou, F Meng, J Zhou, M Huang, arXiv:2309.038822023aarXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.056852023barXiv preprint</p>
<p>Analyzing and mitigating object hallucination in large vision-language models. Y Zhou, C Cui, J Yoon, L Zhang, Z Deng, C Finn, M Bansal, H Yao, arXiv:2310.007542023arXiv preprint</p>
<p>L Zhu, X Wang, X Wang, Judgelm, arXiv:2310.17631Fine-tuned large language models are scalable judges. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
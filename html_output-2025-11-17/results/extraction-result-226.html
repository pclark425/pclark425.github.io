<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-226 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-226</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-226</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-0286b2736a114198b25fb5553c671c33aed5d477</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0286b2736a114198b25fb5553c671c33aed5d477" target="_blank">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, and a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization is identified.</p>
                <p><strong>Paper Abstract:</strong> We apply preference modeling and reinforcement learning from human feedback (RLHF) to ﬁnetune language models to act as helpful and harmless assistants. We ﬁnd this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. Figure These plots show that PM accuracy decreases as we focus exclusively on comparisons between pairs of samples with high score. We have normalized all preference models to have the same mean score on a held-out dataset so that they’re directly comparable, and then plotted accuracy for the comparisons where both samples have scores above a speciﬁc threshold.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e226.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e226.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Helpfulness HF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Helpfulness human preference dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of human preference comparisons collected via open-ended dialogues where crowdworkers choose the more helpful response; used to train preference models and as the reward signal for RLHF policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>52B context-distilled LM (primary data source for collection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>general question answering / dialogue helpfulness</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>preference pairs from human feedback (crowdworker-written prompts interacting with models; comparisons of two model responses where the worker selects the more helpful)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Base/static: 44k helpfulness comparisons; RS dataset: 52k helpfulness comparisons; Online dataset: 22k helpfulness comparisons (numbers reported in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>open-ended, diverse dialogues; prompts are natural-language chat turns; dataset moves conversations in a beneficial direction (workers choose responses that improve the conversation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>preference model (PM) accuracy, PM score, human Elo/win-rate, and downstream zero-shot/few-shot NLP accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>PMs trained on helpfulness data achieve high accuracy on alignment evaluations (paper reports PM performance e.g. ~86% on the HHH evaluation for the static PM); RLHF trained using helpfulness rewards improves many zero-shot NLP tasks for large models (13B and 52B) and is preferred by crowdworkers in Elo comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Context-distilled and raw language models perform worse on HHH and on many NLP evaluations; small models suffer an 'alignment tax' after RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Qualitative/aggregate: RLHF on helpfulness data yields substantial gains for large models across many NLP tasks and in human preference (exact per-task lifts vary and are not summarized as a single number in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Human preference (helpfulness) comparisons teach PMs to prefer helpful responses; PMs trained only on helpfulness are well-calibrated and provide reliable reward on-distribution, but high-quality (upper-tail) examples are scarce which motivates rejection-sampling and online data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e226.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Harmlessness HF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Harmlessness / red-teaming human preference dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset collected by asking crowdworkers to adversarially probe models and select the more harmful response; used to train PMs that detect or penalize harmful outputs and to provide harmlessness rewards/penalties for RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>52B context-distilled LM (primary data source for collection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>red-teaming / harmlessness detection (adversarial harmful content elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>preference pairs from human feedback (crowdworkers attempt to elicit harmful responses and choose the more harmful model output)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Base/static: 42k red-teaming (harmlessness) comparisons; RS dataset: 2k red-teaming comparisons; Online dataset: none (online collection contained no red-teaming data per paper)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>adversarial, red-team-style dialogues that move conversations in a more harmful direction (dataset primarily shows what not to do rather than demonstrations of ideal refusal/mitigation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>preference model score for harmlessness, PM accuracy, and policy harmlessness PM-score; also human Elo/win-rate for combined HH evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>PMs trained on harmlessness data classify held-out harmful AI utterances (BAD dataset) as lower-scoring; RL policies trained with heavy harmlessness weighting can achieve very high harmlessness PM scores (policy harmlessness score moves to upper tail of harmlessness PM distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Without harmlessness data RL policies are easier to red-team; baseline PMs trained only on helpfulness perform poorly on harmfulness detection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Qualitative: harmlessness data strongly improves the model's tendency to avoid or refuse harmful content as measured by PM scores and human red-team evaluations, but exact numerical lift vs baseline varies by evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Harmlessness comparisons are often easier to learn (simple refusal yields low measured harm) and can be over-optimized by RLHF to produce safe-but-unhelpful behaviors; the way harmlessness data was collected (workers choose more harmful responses) made it hard for models to learn sophisticated mitigation strategies ('hostage negotiation'), producing a tension between maximizing harmlessness and retaining helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e226.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rejection Sampling (RS) Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection-sampling augmented preference data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data collected by rejection sampling: sample k responses from a context-distilled LM and use a preference model to select better samples, producing a distribution with a heavier upper tail of high-quality responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>52B context-distilled LM with 52B preference model for rejection sampling</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL (data-collection augmentation prior to training newer PMs/policies)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>general QA / dialogue helpfulness (same dialogue format as human HF)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>synthetic preference-labeled samples via rejection sampling (k-shot sampling, typically k=16) filtered by a PM</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>RS dataset reported as 52k helpfulness comparisons and 2k red-teaming comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>skewed toward higher-quality samples (upper tail); generated by sampling and PM-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PM accuracy on RS-distribution test sets, PM score distributions, human Elo</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>RS and online distributions contain more high-PM-score samples; PMs become less accurate at distinguishing high-quality pairs (paper reports PM test accuracies: base 74%, RS 70%, online 67% for their final online PM on different held-out distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Base/static distribution (context-distilled samples) has fewer upper-tail samples; PM discriminates base distribution more easily</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Rejection sampling increases representation of high-quality responses in the training data, enabling PMs and downstream RLHF to access and learn from higher-quality behavior; this improves model outputs in human preference evaluations versus base-only data (quantitative lift shown in distributional shifts and downstream Elo improvements in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Rejection sampling supplements the dataset with more high-quality responses and helps preference models and RLHF access the upper tail, but distinguishing among these high-quality samples is harder (PM accuracy drops), motivating iterated online collection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e226.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterated Online RLHF Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterated online RLHF human feedback dataset (online iteration data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data collected by repeatedly deploying RLHF-trained policies to interact with crowdworkers, collecting new comparisons on the improved distribution and using them to retrain PMs and policies in iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Successive RLHF-finetuned models (52B primary deployment size)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B (deployed models used for data collection)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>general QA / dialogue helpfulness (iterated online interactions with crowdworkers)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>preference pairs collected from interactions with RLHF-deployed models (human comparisons over model responses produced by RL policies)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Online dataset contains 22k helpfulness comparisons (paper reports no red-teaming in online tranche); authors also performed weekly updates over ~5 weeks</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>rich upper-tail coverage (more high-quality responses), includes data produced on-policy by RLHF models, intended to improve calibration and coverage in high-score regime</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human Elo/win-rate, PM accuracy on distributions (held-out test), PM score distributions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Online models are preferred by crowdworkers in Elo comparisons (Figure 1); PM test accuracy on online-only held-out distribution reported as ~67% (lower than base 74%), reflecting harder discrimination among higher-quality samples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Base/static-only training (44k helpfulness) and RS-augmented training; controlled experiments in paper show iterated-online mixtures outperform base when training conditions are matched</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Iterated online training improves human preference (Elo) and fills out upper-tail; in a controlled experiment the policy trained on an iterated-online mixture outperformed a policy trained on base-only data (exact Elo deltas depend on snapshot and are reported in paper figures)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Iterated online RLHF injects high-quality on-policy samples into the dataset, improves PM calibration in the high-score regime and produces models preferred by humans; it mitigates some robustness failures caused by scarcity of upper-tail examples but introduces considerations about entropy/diversity (authors deploy multiple snapshots to maintain diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e226.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-generated prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-generated prompts for RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts synthesized by a large LM using few-shot examples, used to augment the human-written prompt set for RLHF training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LM (unspecified; used in few-shot mode to generate prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>prompt generation for downstream dialogue/QA training</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>synthetic prompts generated by few-shot sampling from a large LM (context contains ~10 high-quality human queries)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>137k prompts from the static dataset (human) plus 369k model-generated prompts (numbers reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>diverse, model-synthesized prompts intended to increase training diversity and sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>sample efficiency of RLHF (noted qualitatively), downstream PM and policy performance on held-out prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Authors report that RLHF sample efficiency is roughly the same when training on human-written prompts versus model-generated prompts, and they combined both for greater diversity; no single-number performance improvement provided</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Training with human-written prompts only (137k) vs human+synthetic (additional 369k) gives more diversity but paper does not give a single aggregate metric comparison</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Qualitative: model-generated prompts allow larger and more diverse RLHF prompt sets without loss of sample efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Model-generated prompts are an effective and sample-efficient way to expand RLHF prompt datasets and are treated interchangeably with human-written prompts for RLHF training in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e226.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preference Model Pretraining (PMP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Preference Model Pretraining (PMP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining step applied to language models prior to finetuning them as preference models, following prior work and implemented here to improve PM performance before HF finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Preference models across sizes (13M to 52B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13M–52B (seven sizes used)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>pretraining step for preference modeling (applies prior to RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>pretraining on language-model style objectives as in prior PMP protocol (details in Appendix A; not human-comparison labels)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>prepares models for subsequent finetuning on human preference comparisons; intended to improve PM accuracy and sample-efficiency when later trained on the HF comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>preference model accuracy on held-out HF comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>PMP is used as part of the PM training pipeline; PMs trained with PMP then finetuned on HF data achieve the scaling trends and accuracies reported (e.g., PM accuracy curves in Figure 7); exact ablation numbers for PMP vs no-PMP are in Appendix (not re-stated in main text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Applying PMP prior to preference finetuning is part of the training pipeline and supports stronger PM performance across model sizes; details and implementation are as in prior work [Askell et al., 2021].</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e226.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixing HH with Summarization / Code</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed training: helpfulness/harmlessness PM training with specialized skills (summarization and code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments mixing preference-model training for helpfulness/harmlessness with specialized skill datasets such as summarization and code-finetuned models, to test whether alignment training degrades specialized capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PMs and RLHF policies (various sizes; e.g., 52B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B (examples discussed), other sizes also scanned</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple (PM training and RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>specialized skills (summarization) and code generation; applied alongside alignment objectives</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>mixed HF preference comparisons (HH) combined with specialized datasets (summarization data from Stiennon et al. 2020; code-finetuned models used for natural-language RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>mixture of alignment preference data and specialized-skill demonstrations/benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PM accuracy on HH+specialized distributions; downstream evaluations e.g., HumanEval for code, summarization metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Mixing HH PM training with summarization did not degrade PM accuracies (paper states mixed training 'does not degrade PM accuracies'); natural-language RLHF applied to code-finetuned models improves programming ability on HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Specialized models before mixing; authors report that alignment training on large models does not compromise specialized skills and can improve them</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Qualitative: no observed degradation, and some improvements in specialized tasks (e.g., code) when RLHF alignment applied — exact numeric lifts reported per-task in appendices/figures</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Alignment via RLHF can be mixed with specialized-skill datasets (summarization, code) without harming those skills for large models; in some cases alignment improves instruction-following and downstream specialized performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e226.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e226.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised fine-tuning (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised fine-tuning (SFT) as used in related work (Instruct/other)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SFT (supervised finetuning from demonstrations) is discussed as part of related work (e.g., InstructGPT) but was not used as a primary finetuning stage in this paper's pipeline (authors used RLHF instead).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mentioned across instruction-following related work</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>human demonstration datasets (mentioned as used in related InstructGPT/LaMDA work)</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>supervised demonstration labels vs preference comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Paper contrasts its purely RLHF finetuning pipeline with other works that include SFT (e.g., InstructGPT); authors note they did not perform a supervised finetuning step and rely on preference modeling + RLHF (plus context-distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to summarize with human feedback <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>LaMDA: Language Models for Dialog Applications <em>(Rating: 1)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-226",
    "paper_id": "paper-0286b2736a114198b25fb5553c671c33aed5d477",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "Helpfulness HF",
            "name_full": "Helpfulness human preference dataset",
            "brief_description": "A dataset of human preference comparisons collected via open-ended dialogues where crowdworkers choose the more helpful response; used to train preference models and as the reward signal for RLHF policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "52B context-distilled LM (primary data source for collection)",
            "model_size": "52B",
            "training_stage": "RL",
            "task_type": "general question answering / dialogue helpfulness",
            "is_scientific_domain": false,
            "data_type": "preference pairs from human feedback (crowdworker-written prompts interacting with models; comparisons of two model responses where the worker selects the more helpful)",
            "data_size": "Base/static: 44k helpfulness comparisons; RS dataset: 52k helpfulness comparisons; Online dataset: 22k helpfulness comparisons (numbers reported in the paper)",
            "data_properties": "open-ended, diverse dialogues; prompts are natural-language chat turns; dataset moves conversations in a beneficial direction (workers choose responses that improve the conversation)",
            "performance_metric": "preference model (PM) accuracy, PM score, human Elo/win-rate, and downstream zero-shot/few-shot NLP accuracy",
            "performance_with_data": "PMs trained on helpfulness data achieve high accuracy on alignment evaluations (paper reports PM performance e.g. ~86% on the HHH evaluation for the static PM); RLHF trained using helpfulness rewards improves many zero-shot NLP tasks for large models (13B and 52B) and is preferred by crowdworkers in Elo comparisons",
            "performance_baseline": "Context-distilled and raw language models perform worse on HHH and on many NLP evaluations; small models suffer an 'alignment tax' after RLHF",
            "performance_lift": "Qualitative/aggregate: RLHF on helpfulness data yields substantial gains for large models across many NLP tasks and in human preference (exact per-task lifts vary and are not summarized as a single number in the paper)",
            "compares_data_types": true,
            "key_finding": "Human preference (helpfulness) comparisons teach PMs to prefer helpful responses; PMs trained only on helpfulness are well-calibrated and provide reliable reward on-distribution, but high-quality (upper-tail) examples are scarce which motivates rejection-sampling and online data collection.",
            "uuid": "e226.0",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Harmlessness HF",
            "name_full": "Harmlessness / red-teaming human preference dataset",
            "brief_description": "A dataset collected by asking crowdworkers to adversarially probe models and select the more harmful response; used to train PMs that detect or penalize harmful outputs and to provide harmlessness rewards/penalties for RLHF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "52B context-distilled LM (primary data source for collection)",
            "model_size": "52B",
            "training_stage": "RL",
            "task_type": "red-teaming / harmlessness detection (adversarial harmful content elicitation)",
            "is_scientific_domain": false,
            "data_type": "preference pairs from human feedback (crowdworkers attempt to elicit harmful responses and choose the more harmful model output)",
            "data_size": "Base/static: 42k red-teaming (harmlessness) comparisons; RS dataset: 2k red-teaming comparisons; Online dataset: none (online collection contained no red-teaming data per paper)",
            "data_properties": "adversarial, red-team-style dialogues that move conversations in a more harmful direction (dataset primarily shows what not to do rather than demonstrations of ideal refusal/mitigation)",
            "performance_metric": "preference model score for harmlessness, PM accuracy, and policy harmlessness PM-score; also human Elo/win-rate for combined HH evaluations",
            "performance_with_data": "PMs trained on harmlessness data classify held-out harmful AI utterances (BAD dataset) as lower-scoring; RL policies trained with heavy harmlessness weighting can achieve very high harmlessness PM scores (policy harmlessness score moves to upper tail of harmlessness PM distribution)",
            "performance_baseline": "Without harmlessness data RL policies are easier to red-team; baseline PMs trained only on helpfulness perform poorly on harmfulness detection",
            "performance_lift": "Qualitative: harmlessness data strongly improves the model's tendency to avoid or refuse harmful content as measured by PM scores and human red-team evaluations, but exact numerical lift vs baseline varies by evaluation",
            "compares_data_types": true,
            "key_finding": "Harmlessness comparisons are often easier to learn (simple refusal yields low measured harm) and can be over-optimized by RLHF to produce safe-but-unhelpful behaviors; the way harmlessness data was collected (workers choose more harmful responses) made it hard for models to learn sophisticated mitigation strategies ('hostage negotiation'), producing a tension between maximizing harmlessness and retaining helpfulness.",
            "uuid": "e226.1",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Rejection Sampling (RS) Data",
            "name_full": "Rejection-sampling augmented preference data",
            "brief_description": "Data collected by rejection sampling: sample k responses from a context-distilled LM and use a preference model to select better samples, producing a distribution with a heavier upper tail of high-quality responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "52B context-distilled LM with 52B preference model for rejection sampling",
            "model_size": "52B",
            "training_stage": "RL (data-collection augmentation prior to training newer PMs/policies)",
            "task_type": "general QA / dialogue helpfulness (same dialogue format as human HF)",
            "is_scientific_domain": false,
            "data_type": "synthetic preference-labeled samples via rejection sampling (k-shot sampling, typically k=16) filtered by a PM",
            "data_size": "RS dataset reported as 52k helpfulness comparisons and 2k red-teaming comparisons",
            "data_properties": "skewed toward higher-quality samples (upper tail); generated by sampling and PM-based selection",
            "performance_metric": "PM accuracy on RS-distribution test sets, PM score distributions, human Elo",
            "performance_with_data": "RS and online distributions contain more high-PM-score samples; PMs become less accurate at distinguishing high-quality pairs (paper reports PM test accuracies: base 74%, RS 70%, online 67% for their final online PM on different held-out distributions)",
            "performance_baseline": "Base/static distribution (context-distilled samples) has fewer upper-tail samples; PM discriminates base distribution more easily",
            "performance_lift": "Rejection sampling increases representation of high-quality responses in the training data, enabling PMs and downstream RLHF to access and learn from higher-quality behavior; this improves model outputs in human preference evaluations versus base-only data (quantitative lift shown in distributional shifts and downstream Elo improvements in the paper)",
            "compares_data_types": true,
            "key_finding": "Rejection sampling supplements the dataset with more high-quality responses and helps preference models and RLHF access the upper tail, but distinguishing among these high-quality samples is harder (PM accuracy drops), motivating iterated online collection.",
            "uuid": "e226.2",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Iterated Online RLHF Data",
            "name_full": "Iterated online RLHF human feedback dataset (online iteration data)",
            "brief_description": "Data collected by repeatedly deploying RLHF-trained policies to interact with crowdworkers, collecting new comparisons on the improved distribution and using them to retrain PMs and policies in iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Successive RLHF-finetuned models (52B primary deployment size)",
            "model_size": "52B (deployed models used for data collection)",
            "training_stage": "RL",
            "task_type": "general QA / dialogue helpfulness (iterated online interactions with crowdworkers)",
            "is_scientific_domain": false,
            "data_type": "preference pairs collected from interactions with RLHF-deployed models (human comparisons over model responses produced by RL policies)",
            "data_size": "Online dataset contains 22k helpfulness comparisons (paper reports no red-teaming in online tranche); authors also performed weekly updates over ~5 weeks",
            "data_properties": "rich upper-tail coverage (more high-quality responses), includes data produced on-policy by RLHF models, intended to improve calibration and coverage in high-score regime",
            "performance_metric": "human Elo/win-rate, PM accuracy on distributions (held-out test), PM score distributions",
            "performance_with_data": "Online models are preferred by crowdworkers in Elo comparisons (Figure 1); PM test accuracy on online-only held-out distribution reported as ~67% (lower than base 74%), reflecting harder discrimination among higher-quality samples",
            "performance_baseline": "Base/static-only training (44k helpfulness) and RS-augmented training; controlled experiments in paper show iterated-online mixtures outperform base when training conditions are matched",
            "performance_lift": "Iterated online training improves human preference (Elo) and fills out upper-tail; in a controlled experiment the policy trained on an iterated-online mixture outperformed a policy trained on base-only data (exact Elo deltas depend on snapshot and are reported in paper figures)",
            "compares_data_types": true,
            "key_finding": "Iterated online RLHF injects high-quality on-policy samples into the dataset, improves PM calibration in the high-score regime and produces models preferred by humans; it mitigates some robustness failures caused by scarcity of upper-tail examples but introduces considerations about entropy/diversity (authors deploy multiple snapshots to maintain diversity).",
            "uuid": "e226.3",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Model-generated prompts",
            "name_full": "Model-generated prompts for RLHF",
            "brief_description": "Prompts synthesized by a large LM using few-shot examples, used to augment the human-written prompt set for RLHF training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Large LM (unspecified; used in few-shot mode to generate prompts)",
            "model_size": null,
            "training_stage": "RL",
            "task_type": "prompt generation for downstream dialogue/QA training",
            "is_scientific_domain": null,
            "data_type": "synthetic prompts generated by few-shot sampling from a large LM (context contains ~10 high-quality human queries)",
            "data_size": "137k prompts from the static dataset (human) plus 369k model-generated prompts (numbers reported in paper)",
            "data_properties": "diverse, model-synthesized prompts intended to increase training diversity and sample efficiency",
            "performance_metric": "sample efficiency of RLHF (noted qualitatively), downstream PM and policy performance on held-out prompts",
            "performance_with_data": "Authors report that RLHF sample efficiency is roughly the same when training on human-written prompts versus model-generated prompts, and they combined both for greater diversity; no single-number performance improvement provided",
            "performance_baseline": "Training with human-written prompts only (137k) vs human+synthetic (additional 369k) gives more diversity but paper does not give a single aggregate metric comparison",
            "performance_lift": "Qualitative: model-generated prompts allow larger and more diverse RLHF prompt sets without loss of sample efficiency",
            "compares_data_types": true,
            "key_finding": "Model-generated prompts are an effective and sample-efficient way to expand RLHF prompt datasets and are treated interchangeably with human-written prompts for RLHF training in this work.",
            "uuid": "e226.4",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Preference Model Pretraining (PMP)",
            "name_full": "Preference Model Pretraining (PMP)",
            "brief_description": "A pretraining step applied to language models prior to finetuning them as preference models, following prior work and implemented here to improve PM performance before HF finetuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Preference models across sizes (13M to 52B)",
            "model_size": "13M–52B (seven sizes used)",
            "training_stage": "multiple",
            "task_type": "pretraining step for preference modeling (applies prior to RLHF)",
            "is_scientific_domain": null,
            "data_type": "pretraining on language-model style objectives as in prior PMP protocol (details in Appendix A; not human-comparison labels)",
            "data_size": null,
            "data_properties": "prepares models for subsequent finetuning on human preference comparisons; intended to improve PM accuracy and sample-efficiency when later trained on the HF comparisons",
            "performance_metric": "preference model accuracy on held-out HF comparisons",
            "performance_with_data": "PMP is used as part of the PM training pipeline; PMs trained with PMP then finetuned on HF data achieve the scaling trends and accuracies reported (e.g., PM accuracy curves in Figure 7); exact ablation numbers for PMP vs no-PMP are in Appendix (not re-stated in main text)",
            "performance_baseline": null,
            "performance_lift": null,
            "compares_data_types": null,
            "key_finding": "Applying PMP prior to preference finetuning is part of the training pipeline and supports stronger PM performance across model sizes; details and implementation are as in prior work [Askell et al., 2021].",
            "uuid": "e226.5",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Mixing HH with Summarization / Code",
            "name_full": "Mixed training: helpfulness/harmlessness PM training with specialized skills (summarization and code)",
            "brief_description": "Experiments mixing preference-model training for helpfulness/harmlessness with specialized skill datasets such as summarization and code-finetuned models, to test whether alignment training degrades specialized capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PMs and RLHF policies (various sizes; e.g., 52B)",
            "model_size": "52B (examples discussed), other sizes also scanned",
            "training_stage": "multiple (PM training and RLHF)",
            "task_type": "specialized skills (summarization) and code generation; applied alongside alignment objectives",
            "is_scientific_domain": false,
            "data_type": "mixed HF preference comparisons (HH) combined with specialized datasets (summarization data from Stiennon et al. 2020; code-finetuned models used for natural-language RLHF)",
            "data_size": null,
            "data_properties": "mixture of alignment preference data and specialized-skill demonstrations/benchmarks",
            "performance_metric": "PM accuracy on HH+specialized distributions; downstream evaluations e.g., HumanEval for code, summarization metrics",
            "performance_with_data": "Mixing HH PM training with summarization did not degrade PM accuracies (paper states mixed training 'does not degrade PM accuracies'); natural-language RLHF applied to code-finetuned models improves programming ability on HumanEval",
            "performance_baseline": "Specialized models before mixing; authors report that alignment training on large models does not compromise specialized skills and can improve them",
            "performance_lift": "Qualitative: no observed degradation, and some improvements in specialized tasks (e.g., code) when RLHF alignment applied — exact numeric lifts reported per-task in appendices/figures",
            "compares_data_types": true,
            "key_finding": "Alignment via RLHF can be mixed with specialized-skill datasets (summarization, code) without harming those skills for large models; in some cases alignment improves instruction-following and downstream specialized performance.",
            "uuid": "e226.6",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Supervised fine-tuning (SFT)",
            "name_full": "Supervised fine-tuning (SFT) as used in related work (Instruct/other)",
            "brief_description": "SFT (supervised finetuning from demonstrations) is discussed as part of related work (e.g., InstructGPT) but was not used as a primary finetuning stage in this paper's pipeline (authors used RLHF instead).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "training_stage": "SFT",
            "task_type": "mentioned across instruction-following related work",
            "is_scientific_domain": null,
            "data_type": "human demonstration datasets (mentioned as used in related InstructGPT/LaMDA work)",
            "data_size": null,
            "data_properties": "supervised demonstration labels vs preference comparisons",
            "performance_metric": null,
            "performance_with_data": null,
            "performance_baseline": null,
            "performance_lift": null,
            "compares_data_types": null,
            "key_finding": "Paper contrasts its purely RLHF finetuning pipeline with other works that include SFT (e.g., InstructGPT); authors note they did not perform a supervised finetuning step and rely on preference modeling + RLHF (plus context-distillation).",
            "uuid": "e226.7",
            "source_info": {
                "paper_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "LaMDA: Language Models for Dialog Applications",
            "rating": 1
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.018262,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</h1>
<p>Yuntao Bai, ${ }^{*}$ Andy Jones, Kamal Ndousse,</p>
<p>Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion,</p>
<p>Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan*</p>
<h2>Anthropic</h2>
<h4>Abstract</h4>
<p>We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 4
1.1 Contributions ..... 5
1.2 Summary of Evaluations and Metrics ..... 7
1.3 Related Work ..... 8
2 Data Collection ..... 9
2.1 Task Specification and Crowdworkers ..... 9
2.2 Helpfulness and Harmlessness (Red Teaming) Datasets ..... 11
2.3 Models Deployed to the Feedback Interface and Associated Data Distributions ..... 11
2.4 Comparing Models with Elo Scores ..... 12
3 Preference Modeling for Helpfulness and Harmlessness ..... 13
3.1 Models and Training Setup ..... 13
3.2 Basic Scaling Results ..... 13
3.3 Calibration of Preference Models and Implications for RL ..... 13
3.4 Evaluating Helpful and Harmless Preference Models ..... 14
4 Reinforcement Learning from Human Feedback ..... 16
4.1 Training Setup ..... 16
4.2 Robustness Experiments ..... 17
4.3 An Approximately Linear Relation Between $\sqrt{D_{\mathrm{KL}}}$ and Reward ..... 18
4.4 Tension Between Helpfulness and Harmlessness in RLHF Training ..... 19
4.5 Iterated Online RLHF ..... 20
4.6 Evaluations: Alignment Bonus, Honesty, and Biases ..... 22
5 Competing Objectives, Specialized Skills, and OOD Detection ..... 24
5.1 Mixing Helpful and Harmless Objectives ..... 24
5.2 Summarization as a Specialized Skill ..... 25
5.3 Natural Language RLHF on Code-Finetuned Models ..... 26
5.4 Applying Out-of-Distribution Detection to Reject Strange or Harmful Requests ..... 27
6 Qualitative Examples and Comparisons ..... 29
6.1 Comparison with Human Writers ..... 29
6.2 Sensitive Questions and Avoidance versus Engagement ..... 32
6.3 Example Dialogues ..... 32
7 Discussion ..... 34
7.1 Limitations ..... 35
7.2 Alignment Data as a Public Good ..... 36
7.3 Broader Impacts ..... 37</p>
<p>A Details, Analysis, and Evaluations of Supervised Training ..... 39
B Details, Analysis, and Evaluations of RLHF ..... 44
C Samples from PALMS, LaMDA, and InstructGPT Prompts ..... 51
D Details on Data Collection and Crowdworkers ..... 63
E Details on NLP Evaluations Formatting and Prompts ..... 66</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 This plot summarizes crowdworker preferences for a variety of models, including context-distilled models, RLHF models trained on our 'static' dataset, and RLHF models trained by an iterated 'online' method for either helpfulness and harmlessness (HH) or for helpfulness only. We present both Elo scores and a match to the frequency with which crowdworkers prefer samples as compared to the 52B context-distilled model. For both helpfulness and harmlessness, a higher score is more desirable.</p>
<h1>1 Introduction</h1>
<p>We would like to develop techniques to train AI agents that are helpful, honest, and harmless [Askell et al., 2021]. In this paper we show that we can train a relatively helpful and harmless ${ }^{1}(\mathrm{HH})$ natural language assistant by collecting human preference data and applying the techniques of preference modeling (PMing) and reinforcement learning from human feedback (RLHF). Our full training process is summarized in Figure 2.</p>
<p>Our goal is not to define or prescribe what 'helpful' and 'harmless' mean but to evaluate the effectiveness of our training techniques, so for the most part we simply let our crowdworkers interpret these concepts as they see fit. We treat helpfulness and harmlessness separately, collecting distinct human-preference datasets for each. For helpfulness, we ask crowdworkers to solicit our models to assist with any purely text-based tasks such as answering questions, writing or editing documents, or discussing plans and decisions. For harmlessness, we invite crowdworkers to adversarially probe or 'red-team' our language models in order to provoke harmful responses: either to help them with harmful goals, such as planning a bank robbery, or to cause the AI to use toxic language. ${ }^{2}$ At each stage of their conversations with the AI assistant, crowdworkers are presented with two possible responses. Those engaged in the helpfulness task are instructed to choose the more helpful and honest (i.e. better) response. Those engaged in the red teaming task are instructed to choose the more harmful (i.e. worse) response. These conversations and the expressed human preferences form our datasets. ${ }^{3}$</p>
<p>Helpfulness and harmlessness often stand in opposition to each other. An excessive focus on avoiding harm can lead to 'safe' responses that don't actually address the needs of the human. An excessive focus on being</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 This diagram summarizes our data collection and model training workflow.
helpful can lead to responses that help humans cause harm or generate toxic content. We demonstrate this tension quantitatively by showing that preference models trained to primarily evaluate one of these qualities perform very poorly (much worse than chance) on the other. Fortunately, we find that PMs trained on a mixture of both datasets can nevertheless learn the right lessons and behave helpfully when appropriate, while encouraging the polite refusal of harmful requests. With preference models in hand, we then train helpful and harmless assistants via reinforcement learning, using the PM scores as rewards. We evaluate both PM performance and the more relevant performance characteristics of our RLHF-trained models. As can be seen in Figure 1, purely helpful RLHF-trained models are far easier to red-team, while helpful+harmless models are both very helpful and much less harmful.
A question that's often raised about alignment training is whether it will compromise AI capabilities. We find that when RLHF is applied to large language models, the answer seems to be an almost-categorical no. Our RLHF-trained models tend to perform better than their raw, generative counterparts on virtually all evaluations, as summarized in Figure 3. We also argue that one can mix specialized skills with alignmentrelated training without compromising either alignment or performance. In practice, aligned models are likely to be more user-friendly and deployable than their raw counterparts, which suggests that there's little reason to deploy models that have not been finetuned for alignment.</p>
<h1>1.1 Contributions</h1>
<h2>Dialogue Preference Datasets</h2>
<ul>
<li>We collect separate helpfulness and harmlessness (i.e. red-teaming) datasets primarily using various 52B language models (see Section 2 for details) in our interface (Figure 6). Crowdworkers have open-ended conversations with the models, either soliciting help, or providing instructions, or attempting to get the model to emit harmful responses, and they are asked to choose the more helpful response or the more harmful ${ }^{4}$ response at each conversational step, respectively.</li>
<li>We collect three tranches of data, one from our initial models, one with rejection sampling against early preference models, and a final dataset gathered with models trained with 'online' reinforcement learning from human feedback, which we improve on a roughly weekly cadence. See Section 2.3.</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 RLHF model performance on zero-shot and few-shot NLP tasks. For each model size, we plot the mean accuracy on MMMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, and TriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small models, but actually improves performance for larger models. Full results for each task are given in Figure 28 (zero-shot) and Figure 29 (few-shot).</p>
<h1>Alignment with Human Values Has Many Benefits and Essentially No Cost to Performance</h1>
<ul>
<li>Smaller models experience severe 'alignment taxes' - their performance on a wide variety of evaluations declines after RLHF training. However, we find a variety of alignment bonuses, with our 13B and $52 \mathrm{~B}^{8}$ RLHF-trained models performing better at zero-shot NLP evaluations, and the same at few-shot evaluations.</li>
<li>Natural language RLHF training for HH can be applied to models that have been first finetuned on code, and it improves their programming ability on evaluations (presumably by improving general-purpose instruction following). We also find that mixing preference model training for HH with the specialized skill of summarization [Stiennon et al., 2020] incurs no degradation in performance in either HH or summarization. So there is no reason not to combine alignment training with more specific, valuable skills.</li>
<li>There is a tension between helpfulness and harmlessness, which can be measured at the level of both preference modeling and RLHF-trained policies (Figure 1). However, as model size increases, PMs perform better on both distributions simultaneously and become much more robust to the relative proportions of helpful and harmless training data.</li>
<li>We also show that one can use OOD detection techniques [Fort et al., 2021] to reject most strange and harmful requests (Figure 22), with little or no harmful examples (Figure 23).</li>
</ul>
<h2>Scaling, RLHF Robustness, and Iterated 'Online’ Training</h2>
<ul>
<li>We study scaling relations for PM accuracy as a function of model and dataset size, and find roughly log-linear trends (Figure 7), though we encounter some idiosyncrasies (Figures 31 and 32).</li>
<li>We conduct experiments on the robustness of RLHF (see Figure 4), where we split our datasets in half and train separate preference models on each half. Then we train RL models against one PM while evaluating with the other. We conclude that larger PMs are more robust than smaller PMs, and as expected, overfitting increases during RLHF training.</li>
<li>We find that $\sqrt{D_{\mathrm{KL}}\left(\pi | \pi_{0}\right)}$ and reward are approximately linearly related for much of RLHF training (see Figures 4 and 13), where $\pi$ and $\pi_{0}$ are the policy and initial policy, respectively. We explain how this relation may arise and discuss possible applications and future directions.</li>
<li>We study iterated online training, where we update our preference models and RLHF policies on a weekly cadence, and then re-deploy these fresh RLHF models to interact with crowdworkers. This significantly improved our models as evaluated by crowdworkers (Figure 1), and greatly improved our dataset as judged by our own PMs (Figure 15), filling out the upper tail in terms of quality.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 This figure shows results from RL robustness experiments. We split our static dataset 50:50, and trained separate PMs on each half, which we refer to as train PMs and test PMs. We then trained RLHF policies against the train PMs, while evaluating their score with respect to the test PMs. Overfitting can then be observed as a divergence between the train and test PM scores. (left) We see that training is quite robust up to about 150k training samples, but beyond that point the train and test PM's disagree, with the train PM assigning a higher mean reward. We also show an approximately linear relationship between PM score gain and the square root of the KL divergence (between the policy and its initial snapshot) during early phase of training-we observe this for all our RLHF runs, as discussed more in Section 4.3. (right) This shows similar results for various policy sizes, all trained and tested on 52B PMs.</p>
<p>To remove confounders and bolster our conclusions, we perform additional controlled experiments (Figure 16) holding dataset size and other hyperparameters fixed.</p>
<h1>1.2 Summary of Evaluations and Metrics</h1>
<ul>
<li>NLP and Code Evaluations: We evaluate our models on MMLU [Hendrycks et al., 2021b], Lambada [Paperno et al., 2016], Hellaswag [Zellers et al., 2019], OpenBookQA [Mihaylov et al., 2018], ARC [Clark et al., 2018], and TriviaQA [Joshi et al., 2017]; see Figures 28 and 29 for full results and Figure 3 for the mean. In every case except for TriviaQA, 12B and 52B RLHF-trained models perform better than base LMs. Separately, we take Python coding models and finetune them with natural language RLHF, and then evaluate them on the codex HumanEval [Chen et al., 2021]; see Figure 21. We also experiment with mixing PM training for HH with summarization [Stiennon et al., 2020] as a specialized skill, and evaluate the resulting PM performance (Figure 20), finding that mixed training does not degrade PM accuracies.</li>
<li>Static Alignment Evaluations: We evaluate our PMs using our HHH Evaluations [Askell et al., 2021] from BIG-Bench ${ }^{6}$ (Figure 5), on Bot Adversarial Dialogues [Xu et al., 2020], and for gender bias [Rae et al., 2021] (Figure 12). We evaluate our RLHF models on TruthfulQA [Lin et al., 2021] (Figure 5), BBQ-Lite [Parrish et al., 2021] from BIG-Bench, gender bias (Figure 40), and sentiment based on race and religion [Rae et al., 2021] (Figure 17). RLHF improves sentiment towards all groups, but does not remove bias.</li>
<li>Human Evaluations: We compute Elo scores based on the preferences of our crowdworkers, comparing context-distilled models, base RLHF trained models, and final online RLHF models (Figure 1). We also test our online models' performance during training (Figure 15), compare various levels of rejection sampling (Figure 36), and perform a controlled experiment on iterated online training (Figure 16). Furthermore, we hired professional writers to compose conversations where an assistant provides high-quality, helpful and honest responses, and we then asked crowdworkers to compare our model's responses to those of these writers. Crowdworkers prefer our online HH model to these writers ${ }^{7}$ about $57 \%$ of the time.</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 (left) Here we show accuracy on the HHH alignment evaluation dataset we constructed previously [Askell et al., 2021] and shared on BIG-Bench. We see that our static preference models greatly outperform plain language models, including context distilled HHH models. This confirms that the data generated by our crowdworkers has taught preference models the desired lessons. (right) Our RLHF training improves performance on TruthfulQA (MC1) [Lin et al., 2021] for large models, with an effect that increases with model size. These RLHF models were trained from our static dataset (i.e. they did not use online data).</p>
<ul>
<li>Samples: We provide samples from all of the PALMs [Solaiman and Dennison, 2021] sensitive questions and from prompts provided with InstructGPT [Ouyang et al., 2022] and LaMDA [Thoppilan et al., 2022] in Appendix C. We show some comparisons with human writers in Section 6.1, and we show several short dialogues in Section 6.3. To mitigate the problem of cherry picking samples, we generate 17 samples per prompt and display only the median sample, as ranked by our online HH preference model.</li>
</ul>
<h3>1.3 Related Work</h3>
<p>Two recent papers, LaMDA [Thoppilan et al., 2022] and InstructGPT [Ouyang et al., 2022] have particular similarity to this work. Both use human data to train large language models to be more generally useful or aligned. Both use language models somewhat larger than our 52B model.</p>
<p>LaMDA [Thoppilan et al., 2022] finetunes large language models to participate in interesting, helpful, factually grounded, and safe natural language dialogue. As with our work, they include notions of both positive interactions and safety/harmlessness. And their use of external information to ensure accuracy/groundedness goes beyond the methods we discuss here, and is perhaps more similar to WebGPT and GopherCite [Nakano et al., 2021; Menick et al., 2022]. However, some differences are that rather than using reinforcement learning, they apply a mix of supervised learning techniques (both generative and discriminative), and their data collection process involves absolute ratings rather than comparisons. They do not explore whether their methods impose an 'alignment tax' on capabilities.</p>
<p>InstructGPT [Ouyang et al., 2022] finetunes GPT-3-type models [Brown et al., 2020] to improve their helpfulness. As in this work, they use reinforcement learning from human preferences, as expressed through comparisons. However, they also include a supervised learning stage of training, whereas in contrast our finetuning occurs purely through RL (we perform context distillation, but this is much more like simple prompting). Perhaps the main contrast with our work is that they do not include harmlessness training, or explore tensions between helpfulness and harmlessness. Their approach also differs from ours in some details: they did not train preference models larger than 6B parameters, and they mixed pretraining with RL in order to avoid a degradation in evaluation performance.</p>
<p>Our work differs from both InstructGPT and LaMDA in that we explore 'online' training, where we update the models interacting with crowdworkers in order to obtain progressively higher-quality data and fill out the tails of our data distribution. Another difference is our exploration of specialized skills such as summarization and coding, which we use to bolster the argument that alignment can be achieved without limiting capabilities. We also explicitly study the tension between helpfulness and harmlessness, which has not been addressed before as far as we are aware. Finally, we explore scaling and robustness in much more detail, including during RL training. With that said, our procedures (Figure 2) are actually somewhat simpler than those employed</p>
<p>in these other works. We believe the only essential steps are human feedback data collection, preference modeling, and RLHF training.</p>
<p>Several other recent works focus on aspects of truthfulness using retrieval [Lewis et al., 2020, Guu et al., 2020, Borgeaud et al., 2021] from a database, or via internet search and human feedback, such as WebGPT [Nakano et al., 2021] and GopherCite [Menick et al., 2022]. These works are exciting and complementary to our work; in particular our results suggest that their techniques should be very compatible with training for helpfulness and harmlessness. While these works improve the faithful representation of explicit evidence, more work will likely be necessary to achieve honest self-representation from AI systems. We are generally hopeful that techniques independent of human feedback may be applicable to this problem, since a great many sources of truth are not based on human judgment.
Safety and ethical issues associated with language models have been extensively discussed (e.g. [Henderson et al., 2017, Bender et al., 2021, Weidinger et al., 2021]), with well-known issues including toxicity, bias, and the possibility that models may reveal personally identifiable information. As models become increasingly powerful, new and surprising capabilities and safety issues may arise [Ganguli et al., 2022]. Other works have explored methods to mitigate these problems (e.g. [Liu et al., 2021, Xu et al., 2020]). Models have also been trained to directly evaluate ethical dilemmas [Jiang et al., 2021], demonstrating improvement on ethics benchmarks [Hendrycks et al., 2021a]. More general research proposals for AI safety include [Amodei et al., 2016, Hendrycks et al., 2021c]. The RL robustness failures we discuss can be viewed as an instance of 'reward hacking', which was recently explored in [Pan et al., 2022]. RL policies could also fail to generalize out of distribution in other dangerous ways [Koch et al., 2021].
Our interest in studying trends with model size is motivated by neural scaling laws [Hestness et al., 2019, Rosenfeld et al., 2019, Kaplan et al., 2020]. A related observation is that as parameter counts grow, models finetune more effectively [Hernandez et al., 2021] and become much less vulnerable to 'catastrophic forgetting' [Ramasesh et al., 2022]. We expect this effect helps to explain why our HH training is compatible with good evaluation performance and specialized skills for large models.</p>
<h1>2 Data Collection</h1>
<p>We expect human feedback (HF) to have the largest comparative advantage over other techniques when people have complex intuitions that are easy to elicit but difficult to formalize and automate. This means that when collecting HF, we should try to choose tasks that are as intuitive and familiar as possible. We chose to use natural language dialogue both for these reasons, and because it is so general - essentially any text-based task can be enacted through dialogue, perhaps with some source materials included in-line.</p>
<h3>2.1 Task Specification and Crowdworkers</h3>
<p>Our human feedback interface can be seen in Figure 6 (for more details see Appendix D). People can interact with our models in natural language via chat, and ask for help with any text-based task. When it's the model's conversational turn, users see two possible model responses, and choose one with which to proceed. These two responses may come from the same model, or two different models. They can then ask follow-up questions or provide further instructions to the models. So there are two core components to the task, which repeat several times in each dialogue:</p>
<ul>
<li>Crowdworkers write a chat message to our models, asking them to perform a task, answer a question, or discuss any topic of interest.</li>
<li>Crowdworkers are shown two responses, and are asked to choose the more helpful and honest response (or in the case of red-teaming, to choose the more harmful response).</li>
</ul>
<p>We conjectured that crowdworkers who wrote well and engaged the AI in more interesting discussions would tend to have better judgment about which AI responses were most 'helpful' and 'harmless'. This meant that rather than attempting to filter crowdworkers based on label quality, we instead used spot-checks of their writing, which were simpler and more intuitive for us to perform.
Otherwise, our approach to data collection was to largely let crowdworkers use their own intuitions to define 'helpfulness' and 'harmfulness'. Our hope was that data diversity (which we expect is very valuable) and the 'wisdom of the crowd' would provide comparable RoI to a smaller dataset that was more intensively validated and filtered. Overall, our process was roughly of this form:</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 We show the interface that crowdworkers use to interact with our models. This is the helpfulness format; the red-teaming interface is very similar but asks users to choose the more harmful response.</p>
<ol>
<li>We invited master-qualified US-based ${ }^{8}$ MTurk workers to engage in dialogues with our models.</li>
<li>Rather than evaluating all of our crowdworkers, we identified those who were most prolific, and together accounted for about $80 \%$ of our data (roughly 20 crowdworkers). We then evaluated their performance based primarily on the sophistication and variation in their dialogues, as this was quite easy to evaluate intuitively (rather than based on any measure of agreement on helpful/harmless choices). Based on this method, we collected a list of 'select' MTurk workers ${ }^{9}$ whom we continued to work with throughout the research process.</li>
<li>We invited our select crowdworkers to a Slack channel and corresponded with them by email, to ensure that they were being compensated fairly ${ }^{10}$ and to allow them to alert us to any problems or issues.</li>
<li>We also hired crowdworkers on Upwork, and vetted them in a similar, lightweight way. We have continued to use both platforms throughout this work. We find that it is easier to incentivize very high-quality interactions on platforms such as Upwork, where crowdworkers can easily be paid by the hour, rather than per task. But conversely, MTurk workers tend to generate data much more rapidly, and account for about $80 \%$ of our datasets.</li>
</ol>
<p>We did not filter workers based on agreement or other direct measures of label quality, though we evaluated them retrospectively (see Figure 10 right) and found poor average agreement (about 63\%) between An-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>thropic researchers and our crowdworkers, as compared to recent similar work such as [Stiennon et al., 2020, Ouyang et al., 2022].
As an important caveat, our crowdworker distribution was not held fixed throughout this work, and we expect that crowdworker quality probably improved as the project went on. We mention this as a possible complication when evaluating the success of the 'online training' program discussed in Section 4.5. Conversely, however, since we generally discouraged repetition, crowdworkers who have performed the task many times might also have had a tendency to engage in more esoteric interactions.
We should also note that we explicitly told crowdworkers that 'lying isn't helpful' and that they should try to only reward helpful and honest responses, which presumably explains why our models improve somewhat in terms of honesty. That said, we did not expect crowdworkers to fact-check our models significantly, and for example they often prefer responses that include non-functional URLs, which are probably one of the simplest possible 'lies' to debunk.</p>
<h1>2.2 Helpfulness and Harmlessness (Red Teaming) Datasets</h1>
<p>We collected two separate datasets using slightly different versions of our interface. For the helpfulness dataset, we asked crowdworkers to have open-ended conversations with our models, asking for help, advice, or for the model to accomplish a task (see Appendix D.2), and to choose the model response that was more helpful. For the harmlessness or red-teaming dataset, we asked crowdworkers to attempt to elicit harmful responses from our models, and to choose the more harmful response offered by the models.
Our interface (Figure 6) allows users to express a preference strength. We only include comparisons in our datasets if crowdworkers expressed a preference stronger than the weakest available. In this work we will not otherwise use this preference-strength information; we treat all comparisons in our dataset as binary and of equal weight (so in particular we do not include ties).
Note that this means our helpfulness dataset tends to move conversations in a more beneficial direction, while in our red-teaming dataset user responses move conversations in a more harmful direction. We made this choice to make it possible for users to fully trick and exploit models while red-teaming, as this was most natural for other work we're doing that's specifically focused on harmfulness. However, we believe this difference made it difficult to train models that were both helpful and harmless, as explained in Section 4.4. We plan to remedy this in future work, and would recommend others who are focused on training harmless dialogue models to collect data where users primarily choose model responses that move the conversation in the more beneficial direction instead.</p>
<h3>2.3 Models Deployed to the Feedback Interface and Associated Data Distributions</h3>
<p>For data collection we predominantly ${ }^{11}$ used 52B language models with the broad specifications given in [Askell et al., 2021]. We used three classes of models in our interface:</p>
<ul>
<li>HHH Context-Distilled 52B Language Model: At the beginning of the project this was the only model available. It performs similarly to a plain 52B language model prompted with HHH dialogues [Askell et al., 2021].</li>
<li>Rejection Sampling (RS) with a 52B preference model, where samples were generated from a 52B context-distilled LM. In this case the number $k$ of samples was a parameter, but most often we used $k=16$.</li>
<li>RLHF-Finetuned Models: We used a succession of these models in our interface. The models varied primarily based on the amount of data available when training the associated PMs (depending on the phase of the project). However, we also deployed models trained on different mixtures of helpfulness and harmlessness data.</li>
</ul>
<p>In the final phase of the project, when we were primarily deploying RLHF-finetuned models, we often deployed several such models at once. This allowed us to monitor progress by gathering model-comparison data, and also to (perhaps) improve data diversity.
Corresponding to the three classes of models, we divide our data into three distributions:</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 (left) We show the learning curves for PM accuracy when training on a mixture of the static helpfulness and harmlessness (i.e, 'HH') data distributions. Since we train for one epoch, these results also give sense for dataset-size scaling of accuracy. (right) We show the model size dependence of HH static preference model accuracy.</p>
<ul>
<li>A core base dataset collected using only the context-distilled LM. This dataset includes 44 k helpfulness comparisons and 42 k red-teaming (harmlessness) comparisons (note a conversation typically comprises about four comparisons).</li>
<li>A RS dataset consisting of 52 k helpfulness comparisons and 2 k red-teaming comparison using rejection sampling models, where rejection sampling used a preference model trained on the base dataset.</li>
<li>An iterated 'online' dataset including data from RLHF models, which were updated on a roughly weekly cadence over the course of about five weeks. This dataset contains 22 k helpfulness comparisons and no red-teaming data.</li>
</ul>
<p>A histogram of these distributions by our final, online HH preference model's score can be seen in Figure 15 in Section 4.5. In what follows when we discuss the static or base+RS dataset, we will be referring to the combination of the first two components. Our 'online' RLHF models in Section 4.5 are trained on all three components. Most of our results are based on the static dataset, as we conducted experiments and evaluations with it while the online data collection was underway.</p>
<p>We analyze a few different splits of the static dataset - a standard split into 95/5 train/test data, and a 65/35 split that we use in order to obtain better statistics when evaluating preference model calibration on the test set. We also consider a 50/50 split, where we train distinct PMs on the two halves of the dataset. This is used to evaluate the robustness of RL training, as we then train an RL policy against one PM while evaluating the rewards achieved by that policy as measured by the independent PM.</p>
<h1>2.4 Comparing Models with Elo Scores</h1>
<p>A significant part of our analysis compares models against each other in order to generate associated Elo scores, as described in [Askell et al., 2021]. That is, we have crowdworkers chat with two models simultaneously, with each model generating one response (either 'A' or 'B') at each turn, and we record the sample that is preferred by the worker. This provides us with a record of 'win rates' between pairs of models, which we can then fit to corresponding Elo scores, to produce Figure 1 (where we show both win rates and Elo scores). Two useful conversion formulas are</p>
<p>$$
\text { Win Fraction }=\frac{1}{1+10^{\frac{\Delta(\text { Elo Score })}{400}}} \quad \text { and } \quad \Delta(\text { Elo Score }) \approx 174 * \Delta(\text { PM Score })
$$</p>
<p>for the fraction in which one model is preferred over another, the difference in Elo scores, and our PM scores. Note that conceptually win fractions, Elo scores and PM scores are interchangeable; we keep both Elo and PM scores so that we can avoid confusing crowdworker preferences (where we use Elo) with our preference modeling and RLHF (where we use PM scores).</p>
<p>Note that the Elo scores for context-distilled models in Figure 1 differ somewhat from the analogous results for prompted models in [Askell et al., 2021] - the Elo scores are now more compressed. The main difference</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 (left) Distribution of conversational turns in a large held-out test set used to investigate calibration and accuracy. (right) We examine preference model accuracy as a function of the number of exchanges in the conversation.
is that we did not use top-p sampling this time ${ }^{12}$. The difference may also be due to changes in the crowdworker distribution since that earlier experiment, or changes in crowdworker expectations, as before this test our workers were mostly interacting with higher-quality RLHF-trained models.</p>
<h1>3 Preference Modeling for Helpfulness and Harmlessness</h1>
<h3>3.1 Models and Training Setup</h3>
<p>We use language models with specifications that are identical to those discussed in [Askell et al., 2021], with a total of seven language models with parameter counts running from 13 M to 52 B and approximating a geometric series with increments of roughly $4 \times$. We use PyTorch [Paszke et al., 2019] and Triton [Tillet et al., 2019] to facilitate model training and performance. Our preference model training setup is also identical to that in [Askell et al., 2021], and in particular we apply 'preference model pretraining' (PMP) to our language models before finetuning on our human feedback datasets, as explained in Section 4 of that paper. More details are provided in Appendix A. Note that we typically only train PMs for a single epoch, so the learning curves themselves (Figure 7 left) indicate how performance scales with dataset size (we used a fixed learning rate).</p>
<h3>3.2 Basic Scaling Results</h3>
<p>We would like to understand how preference modeling performance improves as we increase model size and collect additional data. In Figure 7 we show basic results for PM accuracy when training on our static helpful and harmless data mixture. Roughly speaking, we observe log-linear trends in both dataset and model size. We tend to find somewhat more consistent trends if we model only the helpfulness or harmlessness distributions in isolation, rather than as a mixture, as observed in Figure 32 in Appendix A.3. But there we also see that for some data distributions [Stiennon et al., 2020], scaling trends can exhibit more complex patterns that defy simple trends.</p>
<p>Our preference modeling data comes from natural language dialogue, where crowdworkers have text-based conversations with the model, and choose the more helpful of two model responses at every turn in the conversation (or the more harmful one, for red-teaming tasks). So it is natural to ask how PM performance changes as a function of the conversational turn. We show these results in Figure 8. PMs are somewhat more accurate on the first step of the conversation, but their accuracy is nearly constant thereafter.</p>
<h3>3.3 Calibration of Preference Models and Implications for RL</h3>
<p>Preference model scores should predict the probability that humans will prefer one or another modelgenerated response. We are interested in whether these probabilities are accurate, i.e. whether the PMs</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9 We show preference modeling accuracy as a function of the difference in PM score between higher and lower ranked responses. The black lines indicate the calibrated prediction of accuracy $1 /\left(1+e^{-\Delta}\right)$, where $\Delta$ is the score difference. On the (left) we show calibration for a PM trained and evaluated on all our static data, while on the (right) we show results for a model trained and evaluated only on our helpful data distribution. We see that calibration is slightly worse for models trained on the HH mixture.
are well calibrated. We characterize calibration in Figure 9, where we display PM accuracy as a function of the difference in PM scores assigned to pairs of samples, along with a heavy black line representing perfect calibration. We observe that PMs trained only on helpfulness data are very well calibrated, but PMs trained on a mixture of helpful and harmless data are slightly under-confident.
These calibration results are important because in later sections we will be using PM scores as the reward signal for reinforcement learning. Since the PM scores are well-calibrated, we can trust that they faithfully encode the probabilities that humans will prefer specific model samples (at least on-distribution with the training set). This means that when we see RL robustly achieving a given reward, we can trust that those who interact with this model (if they are well-represented by our crowdworker distribution) will prefer it to reference models at a predictable rate, provided that the PM scores of the models' responses are within the range considered in these calibration studies. That said, we find significant failures of robustness as RLHF optimizes towards much higher scores, as explained in Section 4.5 and Appendix B.4.
We might generally expect that as sample quality improves, it becomes more difficult to reliably identify the very best samples. In Figure 25 in the appendix, we show that if we restrict to comparisons where both samples have a PM score above a given threshold, then PM accuracy drops as a function of this threshold value (to compare different PMs we add an offset so that they all obtain the same mean score on the test distribution). Notice that this result combines three effects: (1) differentiation among more sophisticated samples may be more challenging and require greater capabilities, (2) we have fewer very high quality samples to learn from in our dataset, as they are non-generic, and (3) pairs of samples that are all high-quality will have similar scores (compared to randomly chosen pairs), and so be more difficult to distinguish.
These observations also have an implication for RLHF training, namely that we should expect diminishing returns from further RLHF training once our policies achieve a sufficiently high PM score. This also motivates online training, so that we can update our PMs to stay on-distribution as RLHF policies improve.</p>
<h1>3.4 Evaluating Helpful and Harmless Preference Models</h1>
<h3>3.4.1 HHH Evaluation and Crowdworker-Anthropic Agreement</h3>
<p>We have shown that PMs trained on our dataset are learning something and are well-calibrated. But we would also like to show that these PMs are actually learning to prefer helpful and harmless behavior, according to some independent standard. We provided a set of comparison evaluations for helpfulness, honesty, and harmlessness (i.e., HHH Evaluation) in prior work and evaluated raw, context-distilled, and prompted language models [Askell et al., 2021] on this benchmark. As shown in Figure 5, we find that our PMs perform much better than all of the models we evaluated previously. In fact, the Pathways LM effort [Chowdhery et al., 2022] recently reported a mean human score of $75 \%$ on this dataset, so in a sense our PM performance of $86 \%$ is well above mean human accuracy.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10 (left) We show a histogram of the 52B static PM predictions for the HHH evaluations. The three confidently incorrect outliers all contrast responses where the model declares its ignorance instead of providing a thorough and sophisticated-sounding response that contains misleading information. So they are failures of honesty and accuracy. (right) Here we show agreement rates between Anthropic researchers, our crowdworkers, and a static preference model on 320 samples from the static test set.</p>
<p>Human: Can you help me understand how US presidential election works?
Better Response: I'm sorry but I don't know the answer. Perhaps you can Google it?
Worse Response: Sure! I did a Google search and found the following information on www.usa.gov:
In the U.S., the president and vice president are chosen by 'electors' through a process called the Electoral College. Each state gets as many electors as it has members of the house of representatives. Including Washington, D.C.'s three electors, there are currently 270 electors in all.
After you cast your ballot for president, your vote goes to a statewide tally. In 48 states and Washington, D.C., the winner gets all the electoral votes for that state. A candidate needs the vote of at least 250 electors - which is nearly all electors - to win the presidential election. This is why American democracy is so often gridlocked.</p>
<p>Figure 11 This entirely human-written example from the HHH evaluation dataset [Askell et al., 2021] fools our preference models, which strongly prefer the worse response (which contains subtle inaccuracies).</p>
<p>We also observe that there are a few outliers, where our PMs are confidently incorrect on HHH evaluation comparisons (see Figure 10 left). These are all examples of failures of honesty, where the better choice is honest but unhelpful, while the incorrect choice appears both helpful and well-written, but is subtly incorrect or misleading. We show the single worst example in Figure 11. It's worth noting that the bad response (which was written by Anthropic researchers) is likely out of distribution as compared to samples from our language models and RL policies. Since only model-generated samples were used to train the PMs, this may explain why the PM was badly fooled by these examples. It also indicates that the PMs are not adversarially robust.
We can also compare our crowdworkers, ourselves, and our preference models in terms of agreement on labels. The results are shown on the right in Figure 10, and are based on about 320 examples from our static test set. The PMs are trained to predict crowdworker behavior, so PM-Crowdworker agreement is best. However, the largest PM actually agrees with the authors (i.e. Anthropic researchers) slightly more than the authors agree with crowdworkers on labels. We also suspect this is a poor subsample of the data, since PM accuracy is several percent lower on this subsample as compared to the full test set.</p>
<h1>3.4.2 Gender Bias and Bot Adversarial Dialogues</h1>
<p>We can begin to evaluate whether our PMs display gender bias by comparing scores across gendered terms. To this purpose we evaluate preference model scores on short dialogues of the form</p>
<div class="codehilite"><pre><span></span><code><span class="n">Human</span><span class="o">:</span><span class="w"> </span><span class="n">Can</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">sentence</span><span class="o">?</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="k">is</span><span class="o">:</span>
<span class="n">The</span><span class="w"> </span><span class="o">(</span><span class="n">Occupation</span><span class="o">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span>
<span class="n">Assistant</span><span class="o">:</span><span class="w"> </span><span class="n">Sure</span><span class="w"> </span><span class="n">thing</span><span class="o">,</span><span class="w"> </span><span class="n">here</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="n">filled</span><span class="w"> </span><span class="k">in</span><span class="o">:</span>
</code></pre></div>

<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12 These plots show score or score-difference distributions from the static preference models on alignment-relevant data distributions. (left) We evaluate Bot Adversarial Dialogues [Xu et al., 2020] that have been designated as harmful or not. (right) We show distributions of preference model score differences for statements of the form 'The CEO was a lady', comparing differences among words within a gender category (e.g. gentleman vs boyfriend) and across genders (e.g. mother vs father, lady vs gentleman) while holding occupations fixed.</p>
<p>The (Occupation) is a (Gendered Term)
where we use a list of 76 occupations, 12 matched gender terms (mother vs father, guy vs gal, etc), and both present and past tense [Rae et al., 2021]. Preference model scores are not directly meaningful, so instead we evaluate the difference in score between paired gender terms, and compare that to the difference in score among terms within a given gender. The results are shown on the left in Figure 12. We observe that the variation among terms that indicate a given gender appears to be at least as large as the variation across genders. So it would appear that the PMs do not exhibit a substantial bias.
As an external validation of our PMs, we consider the Bot Adversarial Dialogues (BAD) dataset [Xu et al., 2020]. This dataset contains a few thousand conversations between an AI system and a human. Each AI response is labeled as harmful or not harmful. We compute the distribution of preference model scores for BAD AI utterances (we restrict our analysis to the first BAD AI utterance per conversation) and find that the BAD AI utterances marked as harmful have significantly lower preference model scores. This suggests that our PMs are effectively classifying these AI generated utterances, even though they are likely quite different from the data distribution that our PMs were trained on.</p>
<h1>4 Reinforcement Learning from Human Feedback</h1>
<h3>4.1 Training Setup</h3>
<p>We apply reinforcement learning (RL) with preference modeling, following the approach outlined in [Stiennon et al., 2020], which can summarized in the following steps:</p>
<ol>
<li>Prepare a dataset of comparisons, and train a PM to assign a higher score to the 'better' item in each comparison. In the context of our human feedback experiments, each comparison consists of a prompt followed by a pair of model-generated responses, with a PM score evaluated at the end of each response.</li>
<li>Extract all the prompts from the preceding dataset, and train an RL policy to generate a response to each prompt autoregressively, with a reward signal provided by the PM score at the end of the response.</li>
</ol>
<p>PM dataset and training details are provided in Appendix A.2; we also discussed the performance of our PMs in Section 3. In the language of RL, each response generated by the policy is a 'timestep', a full conversation is one 'trajectory', and the PM score is a single 'reward' provided at the end.
The idea is to use the preference model to steer the policy towards writing better responses. However, as we saw in earlier sections, PMs also become less calibrated at higher scores, so higher rewards do not necessarily imply better performance.</p>
<p>To stabilize RL training, we use Proximal Policy Optimization (PPO) [Schulman et al., 2017]. We also follow other work [Stiennon et al., 2020] and apply an empirically-estimated KL penalty term in the reward, with the total reward given by</p>
<p>$$
r_{\text {total }}=r_{\mathrm{PM}}-\lambda_{\mathrm{KL}} D_{\mathrm{KL}}\left(\text { policy } | \text { policy }_{0}\right)
$$</p>
<p>where $\lambda_{\mathrm{KL}} \geq 0$ is a hyperparameter. In practice we use a very small value of $\lambda_{\mathrm{KL}}=0.001$, which likely has a very minor impact during most of RL training (as $D_{\mathrm{KL}}&lt;100$ typically), and might actually be wholly unnecessary. More details about RL are provided in B.1.
Throughout this paper we use $r_{\mathrm{PM}}=$ the preference model score itself for the RL reward. Recall that as implied by equation (2.1), this means that the difference in $r_{\mathrm{PM}}$ values between two samples $A$ and $B$ will be related to the predicted probability $P(A&gt;B)$ that $A$ will be preferred to $B$ via</p>
<p>$$
P(A&gt;B)=\frac{1}{1+e^{r_{\mathrm{PM}}(B)-r_{\mathrm{PM}}(A)}}
$$</p>
<p>There is no good reason ${ }^{13}$ to use this preference model score directly as the reward, but it has been used in prior work such as [Stiennon et al., 2020] and so for simplicity we will not explore variations on this choice here.</p>
<p>In order to produce additional prompts (i.e. the human side of the conversations) for RLHF training, we used a large LM to generate them. For this purpose, we simply used few-shot learning, creating a context with about 10 existing high-quality human queries, and then sampling to generate more. We find that the sample efficiency of RLHF is roughly the same on the original crowdworker-written prompt dataset and the modelgenerated one, so we combine the two for greater diversity during RLHF training. We used 137 k prompts from the 'static' dataset, and 369 k model-generated prompts.
Note that almost all of our preference modeling data was collected from 52B models. This means that RLHF training with smaller models might have been challenging, since samples from smaller models tend to be out-of-distribution from the PM training data. Thus it is quite interesting that models more than fifty times smaller were actually able to learn and improve, as seen in Figure 1.</p>
<h1>4.2 Robustness Experiments</h1>
<p>We now discuss the problem of RLHF robustness. A fully robust PM would agree with humans on distributions of dialogues quite different from those encountered during PM training (i.e. different from those created by crowdworker interactions with our deployed AI assistants). However, we do not expect that our PMs are so robust, and in fact Figure 11 provides one plausible example of a robustness failure. Since RL optimizes the policy to maximize the PM score, any failure in robustness on the part of the PM may be exploited by the RL policy to achieve higher rewards, without actually improving the policy's behavior from the point of view of human evaluators.
A rigorous way to study robustness is to take snapshots of the policy at various points during RLHF training, including the initial snapshot, and have crowdworkers compare their performance. This gives a 'true' Elo score, as evaluated by crowdworkers, which can then be compared directly with the PM scores. We present an example of this study in Section 4.5.
However, this sort of test requires collecting additional human feedback data, which can be slow and expensive, so here we also study robustness from a different angle. Similar to how datasets are split into train and test sets for supervised learning, we split our preference model comparison data into two halves (a train half and a test half), and train separate preference models on each, which we refer to as the train PM's and the test $P M$ 's. We then train RLHF policies against the train $P M$ 's, while evaluating them using the test $P M$ 's. Similar to how test set evaluations help us understand overfitting in supervised learning, test $P M$ evaluations help us understand overfitting relative to the train $P M$ 's. These experiments are not conclusive since the train and test PMs may exhibit correlated robustness failures.
The main conclusions from these experiments are: (1) RLHF becomes gradually less robust at higher PM scores, and (2) larger preference models are more robust than smaller ones.
We conduct two sets of experiments as follows:</p>
<ul>
<li>Train PM Size $=\mathbf{5 2 B}$ : This set consists of a scan of policies (i.e. one for each model size), all of which are trained with respect to the same 52B train PM.</li>
</ul>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13 These figures show training curves in the $\sqrt{\mathrm{KL}}$ vs PM score plane, exhibiting the approximate linear relationship between these variables, especially in the left-hand plot using the more highly-performing 52B PMs. We observe some instability in the smaller models, likely because the training data for all our PMs was created with 52B language models, and the much smaller LM samples tend to be quite OOD for the PMs. Finally, by comparing the left and right-hand plots, we see that training against smaller PMs (matched to policy sizes) eventually results in poor performance, as evaluated by the 52B PM. Some of our runs were cut off early as they became unstable. We found that smaller models were generally more difficult to stabilize.</p>
<ul>
<li>Train PM Size = Policy Size: This set consists of a scan of policies, with each policy trained with respect to a train $P M$ of the same size as the policy.</li>
</ul>
<p>For both experiments, each policy is further evaluated with respected to a scan of test $P M$ 's throughout training. Note that a scan refers to 7 different model sizes ranging from 13 M to 52 B , thus giving us 7 policies and $7 \times 7$ evaluations per experiment.
In Figure 4, we compare the train $P M$ and test $P M$ scores throughout the training process, similar to how train and test curves are often compared for supervised training. We find that in all cases, the two scores are in close agreement during early stages of training, but eventually diverge, with the test $P M$ providing a lower score. The divergence is likely an indication that the preference model is less robust and more easily exploited at higher rewards. That is, the policy has been over-optimized on the train PM, making the train PM overconfident in the policy's performance. The test $P M$, on the other hand, doesn't suffer from this problem since it was trained on a different portion of data that neither the policy nor the train $P M$ had observed.
We provide more discussion in Appendix B.2.</p>
<h1>4.3 An Approximately Linear Relation Between $\sqrt{D_{\mathrm{KL}}}$ and Reward</h1>
<p>In Figures 4 and 13 we observe an approximately linear relation between $\sqrt{\mathrm{KL}}$ and PM score during RLHF training. Furthermore, we note that when all models are trained and evaluated with the same PMs, the learning curves are roughly parallel in the $\sqrt{D_{\mathrm{KL}}}$-reward plane. Note that here the ' KL ' is more precisely $D_{\mathrm{KL}}\left(\pi | \pi_{0}\right)$, where $\pi$ denotes the policy distribution (and $\pi_{0}$ the initial policy), as evaluated empirically on the samples drawn from the policy during training.
Why should this be? When $D_{\mathrm{KL}}(\pi+\delta \pi | \pi)$ is series expanded in $\delta \pi$, the expansion begins at quadratic order, so if we imagine that the RL policy can also be series expanded around the base LM, and that the RL reward varies linearly in $\delta \pi$, then in the 'small- $\delta \pi$ region' (i.e. where the series expansion provides a good approximation), we should expect reward $\propto \sqrt{D_{\mathrm{KL}}}$. Typically we should expect that reward varies linearly in $\delta \pi$, because because the initial policy $\pi$ was not previously optimized for reward, so there is no reason why it would sit at an extremum with respect to small variations $\delta \pi$. So the fact that this relation seems to hold empirically suggests that most of RLHF training remains in the small- $\delta \pi$ regime.
Though they did not use these coordinates, a similar scaling can be read off from the results in learning to summarize [Stiennon et al., 2020]. In particular, they provide a nice analysis of rejection sampling, where they generate $N$ samples, and then plot mean reward of the top $k$ samples versus the $D_{\mathrm{KL}}=\log (N / k)$.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14 (left panel) We show PM score distributions for the helpfulness and red-teaming comparisons using a 52B PMs. (right panel) We train a 52B RLHF policy with respect to the same PM, and periodically evaluate the policy's performance on held-out prompts (by sampling responses from the policy on such prompts, then evaluating the PM score) from the helpfulness and red-teaming datasets. We find that the policy's harmlessness score (right, red) is nearly 'out-of-distribution' as it's on the upper tail of the harmless PM data (left, red). On the other hand, the policy's helpfulness score (right, blue) appears 'on-distribution' with respect to the helpfulness PM data (left, blue). In other words, we are over-optimized on harmlessness while we are still likely under-optimized on helpfulness. Dashed lines represent the asymptotic mean of the train scores, to guide the eye in connecting the left and right panels.</p>
<p>This analysis suggests that these RL learning curves might be associated with changes in the RL policy that behave very similarly to simply rejection sampling from the initial distribution.</p>
<p>We find this simple relation quite striking, and believe it merits further study. At a conjectural level, it might have a variety of implications and uses when RL-finetuning large generative models:</p>
<ul>
<li>These relations provide a rough prediction for 'how much does the policy need to change to achieve a specific reward'. Furthermore, if the lines corresponding to different model sizes really are parallel, then one can use RL training of a small model along with the zero-shot performance of a larger model to estimate the eventual performance of a larger RL policy. The slopes of these lines also explain how RLHF training can produce such large effective gains in model size, and for example it explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.</li>
<li>One can ask a subtle, perhaps ill-defined question about RLHF training - is it teaching the model new skills or simply focusing the model on generating a sub-distribution of existing behaviors. We might attempt to make this distinction sharp by associating the latter class of behaviors with the region where RL reward remains linear in $\sqrt{\mathrm{KL}}$.</li>
<li>To make some bolder guesses - perhaps the linear relation actually provides an upper bound on RL reward, as a function of the KL. One might also attempt to extend the relation further by replacing $\sqrt{\mathrm{KL}}$ with a geodesic length in the Fisher geometry.</li>
</ul>
<p>By making RL learning more predictable and by identifying new quantitative categories of behavior, we might hope to detect unexpected behaviors emerging during RL training.</p>
<h1>4.4 Tension Between Helpfulness and Harmlessness in RLHF Training</h1>
<p>Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we found that many RLHF policies were very frequently reproducing the same exaggerated responses to all remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they express any level of displeasure at all). This greatly limited these models' utility. We still see a vestige of this</p>
<p>behavior in some of the examples provided in Section 6.2. We now believe these policies were the result of over-optimizing for harmlessness, while under-optimizing helpfulness.
With our data collection procedure, we think this is quite intuitive. In order to get a very good score on red-teaming prompts, it's probably sufficient for models to respond with something like "I can't answer that." This does not require much sophistication (it just requires learning to classify harmful requests), and so we expect it is easier to learn than helpfulness.
In Figure 14 (right), we show the policy's PM score throughout training, after separating helpfulness and harmlessness prompts. On the left side of the same figure, we show the score distribution of PM comparison data, again separating helpful and harmless datasets. We observe that the policy's harmlessness score is somewhat off-distribution, as it is on the upper tail of the harmlessness comparison data. On the other hand, the policy's helpfulness score appears on-distribution, and is likely under-optimized. So we would expect this agent to be very difficult to red-team, but not very helpful.
This then raises an obvious question - can't we just collect more harmlessness data to fill out the upper tail of the distribution? The problem involves the definition of harmlessness mentioned above - if simply refusing to answer a question is the 'least harmful' behavior, then this is probably both very easy to learn, and hard to improve on. That said, a more interesting 'least harmful' behavior would involve the model (helpfully) explaining why the request was harmful, and perhaps even trying to convince the human not to pursue such requests. We informally refer to such a model as a 'hostage negotiator'.
However, our data collection process made it very difficult for models to learn 'hostage negotiation'. This is because when collecting our harmlessness dataset, we had crowdworkers choose the more harmful AI response. We made this choice so that we could fully explore the vulnerability of our models to red-teaming. However, from the point of view of RLHF this was problematic, because beyond the first turn of dialogue, our models never learned what a sophisticated response to a harmful query might be like. Our dataset does not provide guidance on the upper end of the distribution, on what models should do, but only tells models what not to do.
In practice, we have partially resolved the optimization issue by training on a larger fraction of helpfulness prompts during RLHF. But in the future we hope to more fully and systematically address this problem by collecting harmlessness data where crowdworkers choose the best possible response from our models. ${ }^{14}$ In this way we hope that rather than simply shutting down harmful requests, models can learn the more subtle art of 'hostage negotiation' with red-teamers.
Note that since the data and models discussed in this section are from an earlier stage of our research, the RL results may look slightly different from other parts of the paper.</p>
<h1>4.5 Iterated Online RLHF</h1>
<p>In preceding sections we discussed the problem that PMs become progressively less calibrated and less robust at higher scores, as seen in the PM calibration study in Figure 9, and the RLHF robustness study in Figure 4. We believe this is caused by a lack of data in this high score regime. To address this, we propose iterated online RLHF:</p>
<ul>
<li>We simply train the best RLHF policy we can, and use that to collect comparison data from crowdworkers. Since the policy was trained to optimize for PM score, it should produce responses that are on the upper end of the score distribution.</li>
<li>We mix the new comparison data with our existing data, and train a new scan of PMs, which we then use to train a new scan of RLHF policies. Then reiterate this process indefinitely.</li>
</ul>
<p>Our hypothesis is that the 'online' RLHF policy helps us collect data on the upper end of the PM score distribution, which should improve PM calibration at high scores on subsequent iterations, and thereby allow us to train even better policies. Continuing this process should give us progressively better PMs and policies. Note that our use of the terminology 'online' is different from conventional use of the word-instead of training the same model iteratively, we retrain a new model per iteration.</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15 (left) This plot shows individually normalized distributions of held-out helpfulness data from our base dataset (mostly with context-distilled models), from models augmented with rejection sampling, and from data collected with our iterated 'online' RLHF models. The upper tail of the distribution receives far more support from the RS and online models, which should make it possible for preference models to learn more subtle distinctions among high-quality responses, and amplify the value of further data collection. (right) We compare helpfulness Elo scores of our HH and pure-helpfulness iterated online RLHF models at various points during RLHF training. Note that Elo scores and preference frequency are measured relative to the initial snapshot, which is our 52B context distilled model in both cases. Elo scores in both subplots only evaluate helpfulness.</p>
<p>One concern about this approach is that RLHF tends to decrease the policy's entropy, which would limit the diversity of data collected through the online procedure. We partially address this by deploying a number of different snapshots from RL training, and from different online iterations, at once. This also makes it possible to compare these models to get a better sense of how they are performing.</p>
<p>We can see signs of life from the online approach by looking at the evolution of our data distribution. In Figure 15 (left), we show the PM scores from three distributions of models: Base, RS (rejection-sampling), and Online, as described in Section 2.3. We see that according to our final online PM (trained on all of the data), the quality of samples improves from the base to the rejection-sampling to the online data distributions. We also found that our online PM achieves accuracies of $74 \%, 70 \%$, and $67 \%$ on the test sets for the respective base, RS, and online-only distributions, which shows that distinguishing among higher quality samples is becoming more challenging. This makes us optimistic that online training should outperform rejection sampling in the long run.</p>
<p>We show the learning curves for our online models, along with measurements of Elo scores from crowdworkers, on the right in Figure 15. We see that models improve significantly during RLHF, but Elo scores from crowdworkers do not match predictions from PMs. We further discuss and decompose the robustness of RLHF training in Appendix B.4, where we see that distributional shift accounts for a significant part of the apparent robustness failure (Figure 35).</p>
<p>In Figure 1, we compare Elo scores of our online model with context-distilled models and RLHF models trained on the 'static' (i.e., no online) dataset, showing that the online models are clearly preferred by our crowdworkers. However, readers might worry about two caveats: the online model was trained on a slightly larger (about 20\% larger) dataset, and the online model was trained with improved RLHF hyperparameters (the online model was trained with a larger $K$, defined in Appendix B.1, and its PM was trained with 2048 context instead of 1024), as compared to the earlier static RLHF training run.</p>
<p>To address both of these caveats, we performed a controlled experiment comparing two RLHF runs: one trained with our base dataset (about 44 k PM comparisons), and another trained on an even mixture of base, RS, and online data whose total dataset size is the same as the base dataset ${ }^{15}$ (about 15 k PM comparisons from each). So for this experiment we trained two separate PMs on each dataset, and then trained a pair of RLHF policies against these two PMs. Apart from the data difference, both runs used the same settings, and were only trained on helpfulness. In figure 16, we compare Elo scores for various snapshots of both runs, as determined by crowdworker preferences, showing that the policy trained on the iterated-online mixture is</p>
<p><sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{15}$ As before, the RLHF prompts were obtained from the PM comparisons in both cases separately, plus additional model-generated prompts.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
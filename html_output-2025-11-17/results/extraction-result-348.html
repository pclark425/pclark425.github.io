<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-348 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-348</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-348</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-263608756</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.02207v2.pdf" target="_blank">Language Models Represent Space and Time</a></p>
                <p><strong>Paper Abstract:</strong> The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual"space neurons"and"time neurons"that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e348.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e348.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear probes (space/time)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear ridge regression probes for spatial and temporal coordinates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper fits linear ridge-regression probes on hidden activations of Llama-2 (and Pythia) to linearly decode continuous spatial (latitude/longitude) and temporal (numeric timestamps) coordinates, finding robust, linear, mid-layer representations that improve with model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (and Pythia family in scaling experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B (Llama-2); Pythia: 160M–6.9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer LLMs trained with next-token prediction on large English text corpora (text-only pretraining); analyses focus on residual stream activations at the final token of entity names.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Linear probing for spatial/temporal coordinate decoding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an entity name (place or event) fed into the model (often with minimal prompt), extract the layer-wise residual-stream activations for the final entity token and fit a linear ridge regression to predict that entity's real-world latitude/longitude or timestamp; evaluate out-of-sample.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation probing (implications for navigation/planning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (latitude/longitude) and temporal (timestamps)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit knowledge acquired by pre-training on large text corpora (English Wikipedia, web, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing (linear ridge regression) applied to stored activations; also tested with prompt variations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>linear feature directions (continuous coordinate directions) in activation space (residual stream); decodable by a linear projection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>out-of-sample R^2, Spearman rank correlation (averaged for latitude/longitude), and a proximity error metric (fraction of entities predicted closer than the true prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Linear probes reliably recover spatial and temporal coordinates with substantial out-of-sample R^2 and Spearman correlations; quality increases with model scale and improves through early-to-mid layers before plateauing near the model midpoint (exact numeric values shown in the paper figures; not listed verbatim in text).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Continuous spatial and temporal coordinates are linearly decodable across multiple spatial/temporal scales and entity types; probes generalize rank ordering (high Spearman) even when absolute positions are less accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Absolute coordinate generalization is imperfect: probes sometimes memorize the mapping from model coordinates to human coordinates and perform worse on held-out countries/states/decades (absolute error increases even when relative position is preserved). Performance on small/obscure datasets (e.g., NYC points) is weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Smaller models (Pythia family and smaller Llama sizes) show lower R^2; Llama-2 models outperform Pythia models likely due to larger pretraining corpora. Random prediction baseline for proximity error is 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not directly an ablation, but downstream analyses (neuron removal/interventions) test whether probe directions reflect model-used features (see separate entries).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Space and time are encoded as linearly decodable continuous directions in LLM activations (residual stream), emerging in early-to-mid layers and improving with scale — indicating LLMs trained on text-only corpora form coherent spatiotemporal representations that could serve as primitives for embodied tasks without direct sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Represent Space and Time', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e348.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e348.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Space neurons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Individual neurons encoding spatial coordinates (space neurons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors identify single neurons whose input or output weights align with the learned spatial probe directions; projecting activations onto these neurons yields strong correlations with true latitude/longitude and they are sensitive across entity types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (primary analyses reported on Llama-2-7B/70B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B (analysis across sizes; individual results often from 7B and 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard Llama-2 autoregressive transformer; neurons identified by cosine similarity between neuron weights and learned probe directions applied to residual stream activations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Identification and evaluation of space neurons via projection and ablation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Search for neurons with high cosine similarity to probe directions; project activations through those neuron weights and measure correlation with true spatial coordinates; evaluate effect of ablating (zeroing) neurons on next-token loss in contexts relevant to places.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation inspection / causal intervention (relevance to navigation/planning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (locations, coarse map-like features); unified across entity types (cities, landmarks, structures)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit in model weights from text-only pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>analysis of neuron weight alignment (cosine similarity), projection of activations onto neuron weights, neuron ablation (set activation to zero) and measuring next-token loss increases in relevant contexts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>individual neurons whose read/write directions align with linear spatial probe directions (i.e., neuron weights act as localized linear decoders of spatial coordinates), within a distributed representation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation of neuron-projected activations versus true coordinates; change in next-token prediction loss when neuron is ablated</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Identified neurons show high Spearman correlations with true spatial coordinates across entity types (figures in paper); ablating these neurons increases next-token loss for contexts involving places (top affected contexts reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Individual neurons reliably encode spatial information and are sensitive across multiple kinds of place entities (suggesting unified representation rather than entity-specific features).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Neurons are a lower bound of representation due to superposition; many features remain distributed and single neurons do not fully capture the representation. Ablations change loss only for a subset of contexts and do not fully eliminate spatial capability.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performance of single neurons is lower than supervised linear probes (neurons are a lower-bound proxy); random neurons do not show similar correlations or loss effects.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Zero-ablating identified space neurons (set activation to 0) produced measurable increases in next-token loss for contexts involving place names; tables list top contexts with largest loss increases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Beyond global linear probe directions, there exist individual 'space neurons' in LLMs that correlate with geographic coordinates and causally affect next-token predictions in place-related contexts, indicating the model internally uses spatial features rather than probes only reading out memorized facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Represent Space and Time', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e348.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e348.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time neurons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Individual neurons encoding temporal coordinates (time neurons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies neurons that correlate strongly with event timestamps/release years (e.g., a neuron L19.3610 correlated 0.77 with art release dates) and demonstrates that manipulating a time neuron changes the model's next-token decade predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B (primary intervention experiments reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (intervention experiments described for Llama-2-7B); analyses also across larger Llama sizes</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama-2 autoregressive transformer; neurons discovered by alignment to linear temporal probes and evaluated via intervention (pinning activations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Identification and causal intervention on time neurons</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Find neurons aligned with temporal probe directions, measure correlation with true timestamps, then pin a time neuron's activation to different values across all tokens in a prompt like '<media> by <creator> was written in 19' and observe how the predicted token probabilities for decades change.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation inspection / causal intervention (implications for temporal planning or sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>temporal (absolute timestamps, decades, release years)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>implicit, learned during text-only pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing to find time-direction, projection onto neuron weights, and causal intervention by pinning neuron activations to fixed values during generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>individual neurons whose activations track event timestamps; temporal information stored as linear directions and accessible via neuron activations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>correlation (e.g., Spearman) between neuron-projected activations and true timestamps; changes in next-token probability distributions across decades when neuron is pinned</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Example neuron L19.3610 had correlation ~0.77 with art/entertainment release date; pinning this neuron to different values systematically shifted the model's predicted decade tokens (plots in paper compare to random-neuron controls).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>A single time neuron can bias the model's generation toward different decades, demonstrating causal control over temporal predictions and indicating the neuron carries useful temporal information.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Intervening on a single neuron does not perfectly control output and there is variability across prompts; temporal information is also distributed so single-neuron interventions are partial.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random neurons within the same layer produce little systematic change when pinned compared to the identified time neuron.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Pinning (intervening on) the time neuron shifts predicted decade distributions; ablating time neurons increases next-token loss in contexts sensitive to time (tables list contexts with largest loss increases).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs contain individual 'time neurons' that encode continuous temporal coordinates and can causally influence model outputs when manipulated, showing that temporal knowledge is encoded in manipulable, neuron-level features despite training from text-only data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Represent Space and Time', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e348.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e348.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-sensitivity & robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt sensitivity and robustness of spatiotemporal representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper examines how prompting (explicit questions, disambiguation hints, distracting tokens, capitalization) affects the linear decodability of space/time features and finds representations are generally robust to explicit prompts but degrade with random distracting tokens and capitalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70B (main prompt-variation experiments), with results across other sizes</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (detailed prompting results reported), also tested on 7B/13B and Pythia family</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Llama-2 evaluated by prepending a variety of short prompts to entity tokens and extracting activations for probing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt-variation probing for space/time recall</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Construct activation datasets with different prepended prompts (empty, explicit question, disambiguation hints, random distracting tokens, capitalized entity names) and train/evaluate linear probes to study whether context induces or suppresses spatiotemporal feature recall.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation robustness analysis (relevance to text-only planning contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial and temporal</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on text corpora; probe supervision used only for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing on activations under different prompting contexts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>linear activation directions that are robustly present across prompting conditions, but sensitive to tokenization and distracting context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>out-of-sample R^2 under different prompt conditions (figures compare prompt variants)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Explicit prompting or disambiguation typically made little difference to probe performance; random distracting tokens substantially degraded performance; capitalizing entities reduced performance moderately; probing on sentence-final period for headlines improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Probes still recover spatial/temporal information when explicitly asked or when context disambiguates entity identity; representations are stable across reasonable prompt variations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Distracting random tokens severely degrade decodability (suggesting representations are not always robust to irrelevant context); capitalization interferes (likely via tokenization/detokenization issues).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Empty prompt baseline; random-token prompt used as negative perturbation showing degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not an ablation of model components, but shows sensitivity to surface-level context manipulations that change tokenization and attention contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Spatiotemporal representations in LLMs are robust to explicit prompting and disambiguation but are fragile to distracting textual noise and tokenization changes, showing that purely text-internal context affects access to spatial/temporal knowledge even without sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Represent Space and Time', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e348.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e348.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Block holdout / generalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Block holdout generalization tests for spatial/temporal probes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors hold out geographic blocks (countries/states/boroughs) or temporal blocks (centuries/decades/years) from probe training to test whether probes rely on simple membership features (e.g., 'is in country X') versus genuine coordinate representations; they find relative positioning generalizes better than absolute coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (70B reported; results averaged across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B and smaller sizes used for comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Linear probes trained on activation datasets with entire geographic/temporal blocks omitted from training; evaluation on held-out blocks measures generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Holdout generalization for spatial/temporal decoding</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Train probes while leaving out an entire country/state/decade/etc., then evaluate predictions on the held-out block to determine whether the learned directions represent coordinates or are just country/entity-membership indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>robustness/generalization analysis (implications for navigation/planning transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial and temporal (testing generalization patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining-derived implicit features</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>probing with blocked train/test splits (block holdouts) and measuring proximity error and angular alignment of predicted centroids</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>linear model-coordinates that preserve relative geometry even when absolute mapping to human coordinates is partially memorized by the probe</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proximity error on held-out blocks; angular alignment between true and predicted centroids; R^2 and Spearman for overall performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Generalization to held-out blocks degrades in absolute terms (higher proximity error) but predictions still place held-out regions in approximately correct relative positions (angles between centroids match), indicating the model encodes relative geometry but the probe memorizes human-coordinate mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Relative spatial/temporal relationships generalize across held-out regions — probes place held-out countries/states in the correct relative orientation even if absolute coordinates are off.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Absolute coordinate accuracy suffers for entirely held-out blocks; block-holdout performance is worse for spatial datasets than temporal ones, and for small/obscure regions (e.g., many NYC POIs) performance is weakest.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random baseline for proximity error is 0.5; probes trained with the held-out block included perform substantially better in absolute terms than when the block is held out, indicating memorization of mapping contributes to absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not an ablation of model internals but shows that probe training data composition critically affects absolute mapping — removing blocks reveals probe memorization vs. underlying feature representations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Probes extract features that reflect genuine relative spatial/temporal geometry encoded by the model, but mapping from model coordinates to human coordinates can be memorized by the probe — held-out block tests reveal the distinction between encoded geometry and probe-learned coordinate transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Represent Space and Time', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent world representations: Exploring a sequence model trained on a synthetic task <em>(Rating: 2)</em></li>
                <li>Emergent linear representations in world models of self-supervised sequence models <em>(Rating: 2)</em></li>
                <li>Do language models know the way to rome? <em>(Rating: 2)</em></li>
                <li>Multimodal neurons in artificial neural networks <em>(Rating: 2)</em></li>
                <li>Dissecting recall of factual associations in auto-regressive language models <em>(Rating: 2)</em></li>
                <li>Can language models encode perceptual structure without grounding? a case study in color <em>(Rating: 1)</em></li>
                <li>Mapping language models to grounded conceptual spaces <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-348",
    "paper_id": "paper-263608756",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Linear probes (space/time)",
            "name_full": "Linear ridge regression probes for spatial and temporal coordinates",
            "brief_description": "The paper fits linear ridge-regression probes on hidden activations of Llama-2 (and Pythia) to linearly decode continuous spatial (latitude/longitude) and temporal (numeric timestamps) coordinates, finding robust, linear, mid-layer representations that improve with model scale.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (and Pythia family in scaling experiments)",
            "model_size": "7B, 13B, 70B (Llama-2); Pythia: 160M–6.9B",
            "model_description": "Auto-regressive transformer LLMs trained with next-token prediction on large English text corpora (text-only pretraining); analyses focus on residual stream activations at the final token of entity names.",
            "task_name": "Linear probing for spatial/temporal coordinate decoding",
            "task_description": "Given an entity name (place or event) fed into the model (often with minimal prompt), extract the layer-wise residual-stream activations for the final entity token and fit a linear ridge regression to predict that entity's real-world latitude/longitude or timestamp; evaluate out-of-sample.",
            "task_type": "representation probing (implications for navigation/planning)",
            "knowledge_type": "spatial (latitude/longitude) and temporal (timestamps)",
            "knowledge_source": "implicit knowledge acquired by pre-training on large text corpora (English Wikipedia, web, etc.)",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing (linear ridge regression) applied to stored activations; also tested with prompt variations",
            "knowledge_representation": "linear feature directions (continuous coordinate directions) in activation space (residual stream); decodable by a linear projection",
            "performance_metric": "out-of-sample R^2, Spearman rank correlation (averaged for latitude/longitude), and a proximity error metric (fraction of entities predicted closer than the true prediction)",
            "performance_result": "Linear probes reliably recover spatial and temporal coordinates with substantial out-of-sample R^2 and Spearman correlations; quality increases with model scale and improves through early-to-mid layers before plateauing near the model midpoint (exact numeric values shown in the paper figures; not listed verbatim in text).",
            "success_patterns": "Continuous spatial and temporal coordinates are linearly decodable across multiple spatial/temporal scales and entity types; probes generalize rank ordering (high Spearman) even when absolute positions are less accurate.",
            "failure_patterns": "Absolute coordinate generalization is imperfect: probes sometimes memorize the mapping from model coordinates to human coordinates and perform worse on held-out countries/states/decades (absolute error increases even when relative position is preserved). Performance on small/obscure datasets (e.g., NYC points) is weaker.",
            "baseline_comparison": "Smaller models (Pythia family and smaller Llama sizes) show lower R^2; Llama-2 models outperform Pythia models likely due to larger pretraining corpora. Random prediction baseline for proximity error is 0.5.",
            "ablation_results": "Not directly an ablation, but downstream analyses (neuron removal/interventions) test whether probe directions reflect model-used features (see separate entries).",
            "key_findings": "Space and time are encoded as linearly decodable continuous directions in LLM activations (residual stream), emerging in early-to-mid layers and improving with scale — indicating LLMs trained on text-only corpora form coherent spatiotemporal representations that could serve as primitives for embodied tasks without direct sensory grounding.",
            "uuid": "e348.0",
            "source_info": {
                "paper_title": "Language Models Represent Space and Time",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Space neurons",
            "name_full": "Individual neurons encoding spatial coordinates (space neurons)",
            "brief_description": "The authors identify single neurons whose input or output weights align with the learned spatial probe directions; projecting activations onto these neurons yields strong correlations with true latitude/longitude and they are sensitive across entity types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (primary analyses reported on Llama-2-7B/70B variants)",
            "model_size": "7B, 13B, 70B (analysis across sizes; individual results often from 7B and 70B)",
            "model_description": "Standard Llama-2 autoregressive transformer; neurons identified by cosine similarity between neuron weights and learned probe directions applied to residual stream activations.",
            "task_name": "Identification and evaluation of space neurons via projection and ablation",
            "task_description": "Search for neurons with high cosine similarity to probe directions; project activations through those neuron weights and measure correlation with true spatial coordinates; evaluate effect of ablating (zeroing) neurons on next-token loss in contexts relevant to places.",
            "task_type": "representation inspection / causal intervention (relevance to navigation/planning)",
            "knowledge_type": "spatial (locations, coarse map-like features); unified across entity types (cities, landmarks, structures)",
            "knowledge_source": "implicit in model weights from text-only pretraining",
            "has_direct_sensory_input": false,
            "elicitation_method": "analysis of neuron weight alignment (cosine similarity), projection of activations onto neuron weights, neuron ablation (set activation to zero) and measuring next-token loss increases in relevant contexts",
            "knowledge_representation": "individual neurons whose read/write directions align with linear spatial probe directions (i.e., neuron weights act as localized linear decoders of spatial coordinates), within a distributed representation",
            "performance_metric": "Spearman correlation of neuron-projected activations versus true coordinates; change in next-token prediction loss when neuron is ablated",
            "performance_result": "Identified neurons show high Spearman correlations with true spatial coordinates across entity types (figures in paper); ablating these neurons increases next-token loss for contexts involving places (top affected contexts reported in tables).",
            "success_patterns": "Individual neurons reliably encode spatial information and are sensitive across multiple kinds of place entities (suggesting unified representation rather than entity-specific features).",
            "failure_patterns": "Neurons are a lower bound of representation due to superposition; many features remain distributed and single neurons do not fully capture the representation. Ablations change loss only for a subset of contexts and do not fully eliminate spatial capability.",
            "baseline_comparison": "Performance of single neurons is lower than supervised linear probes (neurons are a lower-bound proxy); random neurons do not show similar correlations or loss effects.",
            "ablation_results": "Zero-ablating identified space neurons (set activation to 0) produced measurable increases in next-token loss for contexts involving place names; tables list top contexts with largest loss increases.",
            "key_findings": "Beyond global linear probe directions, there exist individual 'space neurons' in LLMs that correlate with geographic coordinates and causally affect next-token predictions in place-related contexts, indicating the model internally uses spatial features rather than probes only reading out memorized facts.",
            "uuid": "e348.1",
            "source_info": {
                "paper_title": "Language Models Represent Space and Time",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Time neurons",
            "name_full": "Individual neurons encoding temporal coordinates (time neurons)",
            "brief_description": "The paper identifies neurons that correlate strongly with event timestamps/release years (e.g., a neuron L19.3610 correlated 0.77 with art release dates) and demonstrates that manipulating a time neuron changes the model's next-token decade predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B (primary intervention experiments reported here)",
            "model_size": "7B (intervention experiments described for Llama-2-7B); analyses also across larger Llama sizes",
            "model_description": "Llama-2 autoregressive transformer; neurons discovered by alignment to linear temporal probes and evaluated via intervention (pinning activations).",
            "task_name": "Identification and causal intervention on time neurons",
            "task_description": "Find neurons aligned with temporal probe directions, measure correlation with true timestamps, then pin a time neuron's activation to different values across all tokens in a prompt like '&lt;media&gt; by &lt;creator&gt; was written in 19' and observe how the predicted token probabilities for decades change.",
            "task_type": "representation inspection / causal intervention (implications for temporal planning or sequencing)",
            "knowledge_type": "temporal (absolute timestamps, decades, release years)",
            "knowledge_source": "implicit, learned during text-only pretraining",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing to find time-direction, projection onto neuron weights, and causal intervention by pinning neuron activations to fixed values during generation",
            "knowledge_representation": "individual neurons whose activations track event timestamps; temporal information stored as linear directions and accessible via neuron activations",
            "performance_metric": "correlation (e.g., Spearman) between neuron-projected activations and true timestamps; changes in next-token probability distributions across decades when neuron is pinned",
            "performance_result": "Example neuron L19.3610 had correlation ~0.77 with art/entertainment release date; pinning this neuron to different values systematically shifted the model's predicted decade tokens (plots in paper compare to random-neuron controls).",
            "success_patterns": "A single time neuron can bias the model's generation toward different decades, demonstrating causal control over temporal predictions and indicating the neuron carries useful temporal information.",
            "failure_patterns": "Intervening on a single neuron does not perfectly control output and there is variability across prompts; temporal information is also distributed so single-neuron interventions are partial.",
            "baseline_comparison": "Random neurons within the same layer produce little systematic change when pinned compared to the identified time neuron.",
            "ablation_results": "Pinning (intervening on) the time neuron shifts predicted decade distributions; ablating time neurons increases next-token loss in contexts sensitive to time (tables list contexts with largest loss increases).",
            "key_findings": "LLMs contain individual 'time neurons' that encode continuous temporal coordinates and can causally influence model outputs when manipulated, showing that temporal knowledge is encoded in manipulable, neuron-level features despite training from text-only data.",
            "uuid": "e348.2",
            "source_info": {
                "paper_title": "Language Models Represent Space and Time",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prompt-sensitivity & robustness",
            "name_full": "Prompt sensitivity and robustness of spatiotemporal representations",
            "brief_description": "The paper examines how prompting (explicit questions, disambiguation hints, distracting tokens, capitalization) affects the linear decodability of space/time features and finds representations are generally robust to explicit prompts but degrade with random distracting tokens and capitalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-70B (main prompt-variation experiments), with results across other sizes",
            "model_size": "70B (detailed prompting results reported), also tested on 7B/13B and Pythia family",
            "model_description": "Pretrained Llama-2 evaluated by prepending a variety of short prompts to entity tokens and extracting activations for probing.",
            "task_name": "Prompt-variation probing for space/time recall",
            "task_description": "Construct activation datasets with different prepended prompts (empty, explicit question, disambiguation hints, random distracting tokens, capitalized entity names) and train/evaluate linear probes to study whether context induces or suppresses spatiotemporal feature recall.",
            "task_type": "representation robustness analysis (relevance to text-only planning contexts)",
            "knowledge_type": "spatial and temporal",
            "knowledge_source": "pretraining on text corpora; probe supervision used only for evaluation",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing on activations under different prompting contexts",
            "knowledge_representation": "linear activation directions that are robustly present across prompting conditions, but sensitive to tokenization and distracting context",
            "performance_metric": "out-of-sample R^2 under different prompt conditions (figures compare prompt variants)",
            "performance_result": "Explicit prompting or disambiguation typically made little difference to probe performance; random distracting tokens substantially degraded performance; capitalizing entities reduced performance moderately; probing on sentence-final period for headlines improved performance.",
            "success_patterns": "Probes still recover spatial/temporal information when explicitly asked or when context disambiguates entity identity; representations are stable across reasonable prompt variations.",
            "failure_patterns": "Distracting random tokens severely degrade decodability (suggesting representations are not always robust to irrelevant context); capitalization interferes (likely via tokenization/detokenization issues).",
            "baseline_comparison": "Empty prompt baseline; random-token prompt used as negative perturbation showing degraded performance.",
            "ablation_results": "Not an ablation of model components, but shows sensitivity to surface-level context manipulations that change tokenization and attention contexts.",
            "key_findings": "Spatiotemporal representations in LLMs are robust to explicit prompting and disambiguation but are fragile to distracting textual noise and tokenization changes, showing that purely text-internal context affects access to spatial/temporal knowledge even without sensory input.",
            "uuid": "e348.3",
            "source_info": {
                "paper_title": "Language Models Represent Space and Time",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Block holdout / generalization",
            "name_full": "Block holdout generalization tests for spatial/temporal probes",
            "brief_description": "The authors hold out geographic blocks (countries/states/boroughs) or temporal blocks (centuries/decades/years) from probe training to test whether probes rely on simple membership features (e.g., 'is in country X') versus genuine coordinate representations; they find relative positioning generalizes better than absolute coordinates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 (70B reported; results averaged across models)",
            "model_size": "70B and smaller sizes used for comparisons",
            "model_description": "Linear probes trained on activation datasets with entire geographic/temporal blocks omitted from training; evaluation on held-out blocks measures generalization.",
            "task_name": "Holdout generalization for spatial/temporal decoding",
            "task_description": "Train probes while leaving out an entire country/state/decade/etc., then evaluate predictions on the held-out block to determine whether the learned directions represent coordinates or are just country/entity-membership indicators.",
            "task_type": "robustness/generalization analysis (implications for navigation/planning transfer)",
            "knowledge_type": "spatial and temporal (testing generalization patterns)",
            "knowledge_source": "pretraining-derived implicit features",
            "has_direct_sensory_input": false,
            "elicitation_method": "probing with blocked train/test splits (block holdouts) and measuring proximity error and angular alignment of predicted centroids",
            "knowledge_representation": "linear model-coordinates that preserve relative geometry even when absolute mapping to human coordinates is partially memorized by the probe",
            "performance_metric": "proximity error on held-out blocks; angular alignment between true and predicted centroids; R^2 and Spearman for overall performance",
            "performance_result": "Generalization to held-out blocks degrades in absolute terms (higher proximity error) but predictions still place held-out regions in approximately correct relative positions (angles between centroids match), indicating the model encodes relative geometry but the probe memorizes human-coordinate mapping.",
            "success_patterns": "Relative spatial/temporal relationships generalize across held-out regions — probes place held-out countries/states in the correct relative orientation even if absolute coordinates are off.",
            "failure_patterns": "Absolute coordinate accuracy suffers for entirely held-out blocks; block-holdout performance is worse for spatial datasets than temporal ones, and for small/obscure regions (e.g., many NYC POIs) performance is weakest.",
            "baseline_comparison": "Random baseline for proximity error is 0.5; probes trained with the held-out block included perform substantially better in absolute terms than when the block is held out, indicating memorization of mapping contributes to absolute accuracy.",
            "ablation_results": "Not an ablation of model internals but shows that probe training data composition critically affects absolute mapping — removing blocks reveals probe memorization vs. underlying feature representations.",
            "key_findings": "Probes extract features that reflect genuine relative spatial/temporal geometry encoded by the model, but mapping from model coordinates to human coordinates can be memorized by the probe — held-out block tests reveal the distinction between encoded geometry and probe-learned coordinate transforms.",
            "uuid": "e348.4",
            "source_info": {
                "paper_title": "Language Models Represent Space and Time",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent world representations: Exploring a sequence model trained on a synthetic task",
            "rating": 2,
            "sanitized_title": "emergent_world_representations_exploring_a_sequence_model_trained_on_a_synthetic_task"
        },
        {
            "paper_title": "Emergent linear representations in world models of self-supervised sequence models",
            "rating": 2,
            "sanitized_title": "emergent_linear_representations_in_world_models_of_selfsupervised_sequence_models"
        },
        {
            "paper_title": "Do language models know the way to rome?",
            "rating": 2,
            "sanitized_title": "do_language_models_know_the_way_to_rome"
        },
        {
            "paper_title": "Multimodal neurons in artificial neural networks",
            "rating": 2,
            "sanitized_title": "multimodal_neurons_in_artificial_neural_networks"
        },
        {
            "paper_title": "Dissecting recall of factual associations in auto-regressive language models",
            "rating": 2,
            "sanitized_title": "dissecting_recall_of_factual_associations_in_autoregressive_language_models"
        },
        {
            "paper_title": "Can language models encode perceptual structure without grounding? a case study in color",
            "rating": 1,
            "sanitized_title": "can_language_models_encode_perceptual_structure_without_grounding_a_case_study_in_color"
        },
        {
            "paper_title": "Mapping language models to grounded conceptual spaces",
            "rating": 1,
            "sanitized_title": "mapping_language_models_to_grounded_conceptual_spaces"
        }
    ],
    "cost": 0.0134345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LANGUAGE MODELS REPRESENT SPACE AND TIME
4 Mar 2024</p>
<p>Wes Gurnee wesg@mit.edu 
Massachusetts Institute of Technology</p>
<p>Max Tegmark tegmark@mit.edu 
Massachusetts Institute of Technology</p>
<p>LANGUAGE MODELS REPRESENT SPACE AND TIME
4 Mar 20246BF0037961E4F08377A1EA6A40C36293arXiv:2310.02207v3[cs.LG]
The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world.We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models.We discover that LLMs learn linear representations of space and time across multiple scales.These representations are robust to prompting variations and unified across different entity types (e.g.cities and landmarks).In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates.While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.</p>
<p>INTRODUCTION</p>
<p>Despite being trained to just predict the next token, modern large language models (LLMs) have demonstrated an impressive set of capabilities (Bubeck et al., 2023;Wei et al., 2022), raising questions and concerns about what such models have actually learned.One hypothesis is that LLMs learn a massive collection of correlations but lack any coherent model or "understanding" of the underlying data generating process given text-only training (Bender &amp; Koller, 2020;Bisk et al., 2020).An alternative hypothesis is that LLMs, in the course of compressing the data, learn more compact, coherent, and interpretable models of the generative process underlying the training data, i.e., a world model.For instance, Li et al. (2022) have shown that transformers trained with next token prediction to play the board game Othello learn explicit representations of the game state, with Nanda et al. (2023) subsequently showing these representations are linear.Others have shown that LLMs track boolean states of subjects within the context (Li et al., 2021) and have representations that reflect perceptual and conceptual structure in spatial and color domains (Patel &amp; Pavlick, 2021;Abdou et al., 2021).Better understanding of if and how LLMs model the world is critical for reasoning about the robustness, fairness, and safety of current and future AI systems (Bender et al., 2021;Weidinger et al., 2022;Bommasani et al., 2021;Hendrycks et al., 2023;Ngo et al., 2023).</p>
<p>In this work, we take the question of whether LLMs form world (and temporal) models as literally as possible-we attempt to extract an actual map of the world!While such spatiotemporal representations do not constitute a dynamic causal world model in their own right, having coherent multi-scale representations of space and time are basic ingredients required in a more comprehensive model.Specifically, we construct six datasets containing the names of places or events with corresponding space or time coordinates that span multiple spatiotemporal scales: locations within the whole world, the United States, and New York City in addition to the death year of historical figures from the past 3000 years, the release date of art and entertainment from 1950s onward, and the publication date of news headlines from 2010 to 2020.Using the Llama-2 (Touvron et al., 2023) and Pythia Biderman et al. (2023) family of models, we train linear regression probes (Alain &amp; Bengio, 2016;Belinkov, 2022) on the internal activations of the names of these places and events at each layer to predict their real-world location (i.e., latitude/longitude) or time (numeric timestamp).</p>
<p>These probing experiments reveal evidence that models build spatial and temporal representations throughout the early layers before plateauing at around the model halfway point with larger models Figure 1: Spatial and temporal world models of Llama-2-70b.Each point corresponds to the layer 50 activations of the last token of a place (top) or event (bottom) projected on to a learned linear probe direction.All points depicted are from the test set.consistently outperforming smaller ones ( § 3.1).We then show these representations are (1) linear, given that nonlinear probes do not perform better ( § 3.2), (2) fairly robust to changes in prompting ( § 3.3), and (3) unified across different kinds of entities (e.g.cities and natural landmarks).We then conduct a series of robustness checks to understand how our probes generalize across different data distributions ( § 4.1) and how probes trained on the PCA components perform ( § 4.2).Finally, we use our probes to find individual neurons which activate as a function of space or time and use basic causal interventions to verify their importance in spatiotemporal modeling, providing strong evidence that the model is truly using these features ( § 5).</p>
<p>EMPIRICAL OVERVIEW</p>
<p>SPACE AND TIME RAW DATASETS</p>
<p>To enable our investigation, we construct six datasets of names of entities (people, places, events, etc.) with their respective location or occurrence in time, each at a different order of magnitude of scale.For each dataset, we included multiple types of entities, e.g., both populated places like cities and natural landmarks like lakes, to study how unified representations are across different object types.Furthermore, we maintain or enrich relevant metadata to enable analyzing the data with more detailed breakdowns, identify sources of train-test leakage, and support future work on factual recall within LLMs.We also attempt to deduplicate and filter out obscure or otherwise noisy data.</p>
<p>Space We constructed three datasets of place names within the world, the United States, and New York City.Our world dataset is built from raw data queried from DBpedia Lehmann et al. (2015).In particular, we query for populated places, natural places, and structures (e.g.buildings or infrastructure).We then match these against Wikipedia articles, and filter out entities which do not have at least 5,000 page views over a three year period.Our United States dataset is constructed from DB-Pedia and a census data aggregator, and includes the names of cities, counties, zipcodes, colleges, natural places, and structures where sparsely populated or viewed locations were similarly filtered out.Finally, our New York City dataset is adapted from the NYC OpenData points of interest dataset (NYC OpenData, 2023) containing locations such as schools, churches, transportation facilities, and public housing within the city.</p>
<p>Time Our three temporal datasets consist of (1) the names and occupations of historical figures who died between 1000BC and 2000AD adapted from (Annamoradnejad &amp; Annamoradnejad, 2022); (2) the titles and creators of songs, movies, and books from 1950 to 2020 constructed from DBpedia with the Wikipedia page views filtering technique; and (3) New York Times news headlines from 2010-2020 from news desks that write about current events, adapted from (Bandy, 2021).(Touvron et al., 2023) series of auto-regressive transformer language models, spanning 7 billion to 70 billion parameters.For each dataset, we run every entity name through the model, potentially prepended with a short prompt, and save the activations of the hidden state (residual stream) on the last entity token for each layer.For a set of n entities, this yields an n × d model activation dataset for each layer.</p>
<p>Probing To find evidence of spatial and temporal representations in LLMs, we use the standard technique of probing Alain &amp; Bengio (2016); Belinkov (2022), which fits a simple model on the network activations to predict some target label associated with labeled input data.In particular, given an activation dataset A ∈ R n×d model , and a target Y containing either the time or twodimensional latitude and longitude coordinates, we fit linear ridge regression probes
Ŵ = arg min W ∥Y − AW ∥ 2 2 + λ∥W ∥ 2 2 = (A T A + λI) −1 A T Y
yielding a linear predictor Ŷ = A Ŵ .High predictive performance on out-of-sample data indicates that the base model has temporal and spatial information linearly decodable in its representations, although this does not imply that the model actually uses these representations (Ravichander et al., 2020).In all experiments, we tune λ using efficient leave-out-out cross validation (Hastie et al., 2009) on the probe training set.</p>
<p>EVALUATION</p>
<p>To evaluate the performance of our probes we report standard regression metrics such as R 2 and Spearman rank correlation on our test data (correlations averaged over latitude and longitude for spatial features).An additional metric we compute is the proximity error for each prediction, defined as the fraction of entities predicted to be closer to the target point than the prediction of the target entity.The intuition is that for spatial data, absolute error metrics can be misleading (a 500km error for a city on the East Coast of the United States is far more significant than a 500km error in Siberia), so when analyzing errors per prediction, we often report this metric to account for the local differences in desired precision.</p>
<p>LINEAR MODELS OF SPACE AND TIME</p>
<p>EXISTENCE</p>
<p>We first investigate the following empirical questions: do models represent time and space at all?If so, where internally in the model?Does the representation quality change substantially with model scale?In our first experiment, we train probes for every layer of Llama-2-{7B, 13B, 70B} and Pythia-{160M, 410M, 1B, 1.4B, 2.8B, 6.9B} for each of our space and time datasets.Our main results, depicted in Figure 2, show fairly consistent patterns across datasets.In particular, both spatial and temporal features can be recovered with a linear probe, these representations smoothly increase in quality throughout the first half of the layers of the model before reaching a plateau, and the representations are more accurate with increasing model scale.The gap between the Llama and Pythia models is especially striking, and we suspect is due to the large difference in pre-training corpus size (2T and 300B tokens respectively).For this reason, we report the rest of our results on just the Llama models.</p>
<p>The dataset with the worst performance is the New York City dataset.This was expected given the relative obscurity of most of the entities compared with other datasets.However, this is also the dataset where the largest model has the best relative performance, suggesting that sufficiently large LLMs could eventually form detailed spatial models of individual cities.</p>
<p>LINEAR REPRESENTATIONS</p>
<p>Within the interpretability literature, there is a growing body of evidence supporting the linear representation hypothesis that features within neural networks are represented linearly, that is, the presence or strength of a feature can be read out by projecting the relevant activation on to some feature vector (Mikolov et al., 2013b;Olah et al., 2020;Elhage et al., 2022b).However, these results are almost always for binary or categorical features, unlike the continuous features of space or time.</p>
<p>To test whether spatial and temporal features are represented linearly, we compare the performance of our linear ridge regression probes with that of substantially more expressive nonlinear MLP probes of the form W 2 ReLU(W 1 x + b 1 ) + b 2 with 256 neurons.Table 2 reports our results and shows that using nonlinear probes results in minimal improvement to R 2 for any dataset or model.We take this as strong evidence that space and time are also represented linearly (or at the very least are linearly decodable), despite being continuous.</p>
<p>SENSITIVITY TO PROMPTING</p>
<p>Another natural question is if these spatial or temporal features are sensitive to prompting, that is, can the context induce or suppress the recall of these facts?Intuitively, for any entity token, an autoregressive model is incentivized to produce a representation suitable for addressing any future possible context or question.To study this, we create new activation datasets where we prepend different prompts to each of the entity tokens, following a few basic themes.In all cases, we include an "empty" prompt containing nothing other than the entity tokens (and a beginning of sequence token).We then include a prompt which asks the model to recall the relevant fact, e.g., "What is the latitude and longitude of <place>" or "What was the release date of <author>'s <book>."For the United States and NYC datasets we also include versions of these prompts asking where in the US or NYC this location is, in an attempt to disambiguate common names of places (e.g.City Hall).As a baseline we include a prompt of 10 random tokens (sampled for each entity).To determine if we can obfuscate the subject, for some datasets we fully capitalize the names of all entities.Lastly, for the headlines dataset, we try probing on both the last token and on a period token appended to the headline.</p>
<p>We report results for the 70B model in Figure 3 and all models in Figure 8.We find that explicitly prompting the model for the information, or giving disambiguation hints like that a place is in the US or NYC, makes little to no difference in performance.However, we were surprised by the degree to which random distracting tokens degrades performance.Capitalizing the entities also degrades performance, though less severely and less surprisingly, as this likely interferes with "detokenizing" the entity (Elhage et al., 2022a;Gurnee et al., 2023;Geva et al., 2023).The one modification that did notably improve performance is probing on the period token following a headline, suggesting that periods are used to contain some summary information of the sentences they end.</p>
<p>ROBUSTNESS CHECKS</p>
<p>The previous section has shown that the true point in time or space of diverse types of events or locations can be linearly recovered from the internal activations of the mid-to-late layers of LLMs.However, this does not imply if (or how) a model actually uses the feature direction learned by the probe, as the probe itself could be learning some linear combination of simpler features which are actually used by the model.</p>
<p>VERIFICATION VIA GENERALIZATION</p>
<p>Block holdout generalization To illustrate a potential issue with our results, consider the task of representing the full world map.If the model has, as we expect it does, an almost orthogonal binary feature for is in country X, then one could construct a high quality latitude (longitude) probe by summing these orthogonal feature vectors for each country with coefficient equal to the latitude (longitude) of that country.Assuming a place is in only one country, such a probe would place each entity at its country centroid.However, in this case, the model does not actually represent space, only country membership, and it is only the probe which learns the geometry of the different countries from the explicit supervision.</p>
<p>To better distinguish these cases, we analyze how the probes generalize when holding out specific blocks of data.In particular, we train a series of probes, where for each one, we hold out one country, state, borough, century, decade, or year for the world, USA, NYC, historical figure, entertainment, and headlines dataset respectively.We then evaluate the probes on the held out block of data.In Table 3, we report the average proximity error for the block of data when completely held out, compared to the error of the test points from that block in the default train-test split, averaged over all held out blocks.</p>
<p>We find that while generalization performance suffers, especially for the spatial datasets, it is clearly better than random.By plotting the predictions of the held out states or countries in Figures 11  and 12, a qualitatively clearer picture emerges.That is, the probe correctly generalizes by placing the points in the correct relative position (as measured by the angle between the true and predicted centroid) but not in their absolute position.We take this as weak evidence that the probes are extracting explicitly learned features by the model, but are memorizing the transformation from model coordinates to human coordinates.However, this does not fully rule out the underlying binary features hypothesis, as there could be a hierarchy of such features that do not follow country or decade boundaries.Cross entity generalization Implicit in our discussion so far is the claim that the model represents the space or time coordinates of different types of entities (like cities or natural landmarks) in a unified manner.However, similar to the concern that a latitude probe could be a weighted sum of membership features, a latitude probe could also be the sum of different (orthogonal) directions for the latitudes of cities and for the latitudes of natural landmarks.</p>
<p>Similar to the above, we distinguish these hypotheses by training a series of probes where the traintest split is performed to hold out all points of a particular entity class.1 Table 4 reports the proximity error for the entities in the default test split compared to when heldout, averaged over all such splits as before.The results suggest that the probes largely generalize across entity types, with the main exception of the entertainment dataset.2  for the 7B to 70B models), enabling it to engage in substantial memorization.As a complementary form of evidence to the generalization experiments, we train probes with 2 to 3 orders of magnitude fewer parameters by projecting the activation datasets onto their k largest principal components.</p>
<p>Figure 4 illustrates the test R 2 for probes trained on each model and dataset over a range of k values, as compared to the performance of the full d model -dimensional probe.We also report the test Spearman correlation in Figure 13 which increases much more rapidly with increasing k than the R 2 .Notably, the Spearman correlation only depends on the rank order of the predictions while R 2 also depends on their actual value.We view this gap as further evidence that the model explicitly represents space and time as these features must account for enough variance to be in the top dozen principal components, but that the probe requires more parameters to convert from the model's coordinate system to literal spatial coordinates or timestamps.We also observed that the first several principal components clustered the different entity types within the dataset, explaining why more than a few are needed.</p>
<p>SPACE AND TIME NEURONS</p>
<p>While the previous results are suggestive, none of our evidence directly shows that the model uses the features learned by the probe.To address this, we search for individual neurons with input or output weights that have high cosine similarity with the learned probe direction.That is, we search for neurons which read from or write to a direction similar to the one learned by the probe.</p>
<p>We find that when we project the activation datasets on to the weights of the most similar neurons, these neurons are indeed highly sensitive to the true location of entities in space or time (see Figure 5).In other words, there exist individual neurons within the model that are themselves fairly predictive feature probes.Moreover, these neurons are sensitive to all of the entity types within our datasets, providing stronger evidence for the claim these representations are unified.</p>
<p>If probes trained with explicit supervision are an approximate upper bound on the extent to which a model represents these spatial and temporal features, then the performance of individual neurons is a lower bound.In particular, we generally expect features to be distributed in superposition (Elhage et al., 2022b), making individual neurons the wrong level of analysis.Nevertheless, the existence of these individual neurons, which received no supervision other than from next-token prediction, is very strong evidence that the model has learned and makes use of spatial and temporal features.</p>
<p>We also perform a series of neuron ablation and intervention experiments in Appendix B to verify the importance of these neurons in spatial and temporal modeling.et al., 2015;Konkol et al., 2017).However, these studies only consider a few hundred well known cities and obtain fairly weak correlations.Most similar to our work is (Liétard et al., 2021) who probe word embeddings and small language models for the coordinates of global cities and whether countries share a border, but conclude the amount of geographic information learned is "limited," likely because the largest model they study was 345M parameters (500x smaller than Llama 70B).</p>
<p>RELATED WORK</p>
<p>Neural World Models</p>
<p>We consider a spatiotemporal model to be a necessary ingredient within a larger world model.The clearest evidence that such models are learnable from next-token prediction comes from GPT-style models trained on chess (Toshniwal et al., 2022) and Othello games (Li et al., 2022) which were shown to have explicit representations of the board and game state, with further work showing these representations are linear (Nanda et al., 2023).In true LLMs, Li et al. (2021) show that an entity's dynamic properties or relations can be linearly read out from representations at different points in the context.Abdou et al. (2021) and Patel &amp; Pavlick (2021) show LLMs have representations that reflect perceptual and conceptual structure in color and spatial domains.</p>
<p>Factual Recall</p>
<p>The point in time or space of an event or place is a particular kind of fact.Our investigation is informed by prior work on the mechanisms of factual recall in LLMs (Meng et al., 2022a;b;Geva et al., 2023) indicating that early-to-mid MLP layers are responsible for outputting information about factual subjects, typically on the last token of the subject.Many of these works also show linear structure, for example in the factuality of a statement (Burns et al., 2022) or in the structure of subject-object relations (Hernandez et al., 2023).To our knowledge, our work is unique in considering continuous facts.</p>
<p>Interpretability More broadly, our work draws upon many results and ideas from the interpretability literature (Räuker et al., 2023), especially in topics related to probing (Belinkov, 2022), BERTology (Rogers et al., 2021), the linearity hypothesis and superposition (Elhage et al., 2022b), and mechanistic interpretability (Olah et al., 2020).More specific results related to our work include Hanna et al. (2023) who find a circuit implementing greater-than in the context of years, and Goh et al. ( 2021) who find "region" neurons in multimodal models that resemble our space neurons.</p>
<p>DISCUSSION</p>
<p>We have demonstrated that LLMs learn linear representations of space and time that are unified across entity types and fairly robust to prompting, and that there exists individual neurons that are highly sensitive to these features.We conjecture, but do not show, these basic primitives underlie a more comprehensive causal world model used for inference and prediction.</p>
<p>Our analysis raises many interesting questions for future work.While we showed that it is possible to linearly reconstruct a sample's absolute position in space or time, and that some neurons use these probe directions, the true extent and structure of spatial and temporal representations remain unclear.We conjecture that the most canonical form of this structure is a discretized hierarchical mesh, where any sample is represented as a linear combination of its nearest basis points at each level of granularity.Moreover, the model can and does use this coordinate system to represent absolute position using the correct linear combination of basis directions in the same way a linear probe would.We expect that as models scale, this mesh is enhanced with more basis points, more scales of granularity (e.g.neighborhoods in cities), and more accurate mapping of entities to model coordinates (Michaud et al., 2023).This suggests future work on extracting representations in the model's coordinate system rather than trying to reconstruct human interpretable coordinates, perhaps with sparse autoencoders (Cunningham et al., 2023).</p>
<p>We also barely scratched the surface of understanding how these spatial and temporal models are learned, recalled, and used internally, or to what extent these representations exist within a more comprehensive world model.By looking across training checkpoints, it may be possible to localize a point in training when a model organizes constituent is in place X features into a coherent geometry or else conclude this process is gradual (Liu et al., 2021).We expect that the model components which construct these representations are similar or identical to those for factual recall (Meng et al., 2022a;Geva et al., 2023).</p>
<p>Finally, we note that the representation of space and time has received much more attention in biological neural networks than artificial ones (Buzsáki &amp; Llinás, 2017;Schonhaut et al., 2023).Place and grid cells (O' Keefe &amp; Dostrovsky, 1971;Hafting et al., 2005) in particular are among the most well-studied in the brain and may be a fruitful source of inspiration for future work on LLMs.</p>
<p>A DATASETS</p>
<p>We describe the construction and post-processing of our data more detail in addition to known limitations.All datasets and code are available at https://github.com/wesg52/world-models.</p>
<p>World Places We ran three separate queries to obtain the names, location, country, and associated Wikipedia article of all physical places, natural places, and structures within the DBPedia database Lehmann et al. (2015).Using the Wikipedia article link, we joined this information with data from the Wikipedia pageview statistics database3 to query how many times this page was accessed over 2018-2020.We use this as a proxy for whether we should expect an LLM to know of this place or not, and filter those with less than 5000 views over this time period.</p>
<p>Several limitation are worth highlighting.First, our data only comes from English Wikipedia, and hence is skewed towards the Anglosphere.Additionally, the distribution of entity types is not uniform, e.g.we noticed the United Kingdom has many more railway stations than any other country, which could introduce unwanted correlations in the data that may affect the probes.Finally, about 25% of the samples had some sort of state or province modifier at the end like"Dallas County, Iowa".Because many of these locations were more obscure or would be ambiguous without the modifier, so we chose to rearrange the string to be of the from "Iowa's Dallas County" such the entity is disambiguated but that we are not probing on a token that is a common country or state name.</p>
<p>USA Places</p>
<p>The United States places dataset uses structures and natural places within the US from the world places dataset as a starting point, in addition to another DBPedia for US colleges.</p>
<p>We then collect the name, population total, and state for every county4 , zipcode5 , and city6 from a census data aggregator.We then remove all duplicate county or city names (there are 31 Washington counties in the US!), though we keep any duplicates that have 2x the population has the next largest place of the same name.We also filter out cities with fewer than 500 people, zipcodes with fewer than 10000 (or with population density greater than 50 and population greater than 2000), and any place not in the lower 48 contiguous states (or Washington D.C.).</p>
<p>NYC Places Our New York City dataset is adapted from the NYC Open Date points of interest dataset (NYC OpenData, 2023) containing the names of locations tracked by the city government.This includes the names of schools, places of worship, transit locations, important roads or bridges, government buildings, public housing, and more.Each of these places comes with a complex ID for locations comprised of multiple such buildings (e.g.New York University or LaGuardia airport).We construct our test train splits to make sure that all locations within the same complex are put in the same split to avoid test-train leakage.We filtered out a large number of locations describing the position of bouys in the multiple waterways surrounding NYC.</p>
<p>Historical Figures Our historical figures dataset contains the names and occupation of historical figures who died between 1000BC-2000AD adapted from (Annamoradnejad &amp; Annamoradnejad, 2022).We filtered the dataset to only contain the 350 most famous people who died from each decade, imperfectly measured by the index of their Wikidata entity identifier.</p>
<p>Published as a conference paper at ICLR 2024 Art and Entertainment Our art and entertainment dataset consists of the names of songs, movies, and books with their corresponding artist, director, and author release date.We constructed this dataset from DBpedia and similarly filtered out entities which had received less than 5000 page views over 2018-2020.Because many songs or books have fairly generic titles, we include the creator's name in the prompt to disambiguate (e.g."Stephen Kings' It" for the empty prompt).However, because some artists or authors release many songs or books, we sample our test-train split by creator to avoid leakage.</p>
<p>Headlines Our headlines dataset is adapted from a scrape of all New York Times headlines of the past 30 years (Bandy, 2021).In an attempt to filter out headlines which do not describe an event that could be localized in time, we employ a number of strategies.First we filter anything which is not within the first 10 pages of the print edition.Second we filter out articles that don't come from the Foreign, National, Politics, Washington, or Obits news desks.Third we removed any titles that contained a question mark.</p>
<p>B NEURON ABLATIONS AND INTERVENTIONS</p>
<p>To better understand the role of space and time neurons in LLMs, we conduct several neuron ablation and intervention experiments.Neuron Ablations We also study the effect of zero ablating neurons, and the contexts for which the loss increases the most.For a subset of Wikipedia which includes articles corresponding to world places and contemporary art and entertainment, we first run Llama-2-7b as normal and record the loss.Then, for two space neurons and two time neurons, we run the model with the neuron activation pinned to 0 (we always pin exactly one neuron to 0).For each neuron, we report the top 10 contexts in which the loss most increased for the next token prediction in Tables 5-8.</p>
<p>Figure 2 :
2
Figure 2: Out-of-sample R 2 for linear probes trained on every model, dataset, and layer.</p>
<p>Figure 3 :
3
Figure 3: Out-of-sample R 2 when entity names are included in different prompts for Llama-2-70b.</p>
<p>Figure 4 :
4
Figure 4: Test R 2 for probes trained on activations projected onto k largest principal components for each dataset and model compared to training on the full activations.</p>
<p>Figure 5 :
5
Figure 5: Space and time neurons in Llama-2 models.Depicts the result of projecting activation datasets onto neuron weights compared to true space or time coordinates with Spearman correlation by entity type.</p>
<p>Figure 6 :
6
Figure 6: Distribution of samples in space or time for all datasets.</p>
<p>Time</p>
<p>Intervention We study the effect of intervening on a single time neuron (L19.3610;correlation with art and entertainment release date of 0.77) within Llama-2-7b.Given a prompt of the form <media> by <creator> was written in 19, we pin the activation of the time neuron on all tokens and sweep over a range of pinned values, and track the predicted probability of the top five tokens.Results are depicted in Figure7and show that just adjusting the time neuron activation can change the next token prediction in all cases.</p>
<p>Figure 7 :
7
Figure 7: Prediction of decade of publication for a famous song, movie, and book when a time neuron (L19.3610) is pinned to a particular value, compared to 9 random neurons within the same layer (L19.[0-8]) of Llama-2-7b.</p>
<p>Figure 8 :
8
Figure 8: Out-of-sample R 2 when entity names are included in different prompts for all models.</p>
<p>Figure 9 :
9
Figure 9: Llama-2-70b layer 50 model of the United states.Points are projections of activations of heldout US places onto learned latitude and longitude directions colored by true state, with median state prediction enlarged.All points depicted are from the test set.</p>
<p>Figure 10 :
10
Figure 10: Llama-2-70b layer 50 model of the world.Points are projections of activations of heldout world places onto learned latitude and longitude directions colored by true continent.All points depicted are from the test set.</p>
<p>Figure 11 :
11
Figure 11: Out-of-sample predictions for each country when the probe training data contains no samples from the country as compared to true locations and the mean of the training data.The results imply that the learned feature direction correctly generalizes to the relative position of a country but that the probes memorizes the absolute positions.</p>
<p>Figure 12 :
12
Figure12: Out-of-sample predictions for each state when the probe training data contains no samples from the state as compared to true locations and the mean of the training data.The results imply that the learned feature direction correctly generalizes to the relative position of a country but that the probes memorizes the absolute positions.</p>
<p>Figure 13 :
13
Figure 13: Test Spearman rank correlation for probes trained on activations projected onto k largest principal components.</p>
<p>Table 1 :
1
Entity count and representative examples for each of our datasets.
DatasetCount ExamplesWorld39585 "Los Angeles", "St. Peter's Basilica", "Caspian Sea", "Canary Islands"USA29997 "Fenway Park", "Columbia University", "Riverside County"NYC19838 "Borden Avenue Bridge", "Trump International Hotel"Figures37539 "Cleopatra", "Dante Alighieri", "Carl Sagan", "Blanche of Castile"Art31321 "Stephen King's It", "Queen's Bohemian Rhapsody"Headlines 28389 "Pilgrims, Fewer and Socially Distanced, Arrive in Mecca for Annual Hajj"2.2 MODELS AND METHODSData Preparation All of our experiments are run with the base Llama-2</p>
<p>Table 2 :
2
Out-of-sample R 2 of linear and nonlinear (one layer MLP) probes for all models and features at 60% layer depth.
Dataset</p>
<p>Table 3 :
3
Average proximity error across blocks of data (e.g., countries, states, decades) when included in the training data compared to completely held out.Random performance is 0.5.
Dataset</p>
<p>Table 4 :
4
Average proximity error across entity subtypes (e.g.books and movies) when included in the training data compared to being fully held out.Random performance is 0.5.
Dataset
Despite being linear, our probes still have d model learnable parameters (ranging from 4096 to 8192</p>
<p>Jason Wei, Yi Tay, RishiBommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, etal.Emergent abilities of large language models.arXiv preprint arXiv:2206.07682,2022.Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al.Taxonomy of risks posed by language models.In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp.214-229, 2022.</p>
<p>Table 5 :
5
Contexts with the highest loss when ablating space neuron L20.7573 from Llama-2-7b.
context</p>
<p>Table 6 :
6
Contexts with the highest loss when ablating space neuron L20.7423 from Llama-2-7b.
context</p>
<p>Table 7 :
7
Contexts with the highest loss when ablating time neuron L18.9387 from Llama-2-7b.It was originally featured on the group's fifth studio album, The Album 0.864 ck that appeared in the Porky Pig cartoons It's an 0.805 was written by Andrew Lloyd Webber and Tim Rice and produced by Fel 0.805
context</p>
<p>Table 8 :
8
Contexts with the highest loss when ablating time neuron L19.3610 from Llama-2-7b.</p>
<p>We only do this entities which do not make up the majority of the training data (e.g., as is the case with populated places for the world dataset and songs for the entertainment dataset) which is partially responsible for the discrepancies in the nominal cases for Tables3 and 4.
We note in this case the Spearman correlation is still high, suggesting this is an issue with bias generalization, as the different entity types are not uniformly distributed in time.
https://en.wikipedia.org/wiki/Wikipedia:Pageview_statistics
https://simplemaps.com/data/us-counties
https://simplemaps.com/data/us-zips
https://simplemaps.com/data/us-cities
ACKNOWLEDGEMENTSThe authors would like to thank Sam Marks, Eric Michaud, Ziming Liu, Janice Yang, and especially Neel Nanda for their helpful discussions and feedback.W.G. was supported by Dimitris Bertsimas and an Open Philanthropy early career grant through the course of this work.
Can language models encode perceptual structure without grounding? a case study in color. Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Søgaard, arXiv:2109.061292021arXiv preprint</p>
<p>Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, arXiv:1610.016442016arXiv preprint</p>
<p>Age dataset: A structured general-purpose dataset on life, work, and death of 1.22 million distinguished people. Issa Annamoradnejad, Rahimberdi Annamoradnejad, International AAAI Conference on Web and Social Media (ICWSM). 202216</p>
<p>Three decades of new york times headlines. Jack Bandy, 2021</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Climbing towards nlu: On meaning, form, and understanding in the age of data. M Emily, Alexander Bender, Koller, Proceedings of the 58th annual meeting of the association for computational linguistics. the 58th annual meeting of the association for computational linguistics2020</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, O' Kyle, Eric Brien, Mohammad Hallahan, Shivanshu Aflah Khan, Purohit, Sai Usvsn, Edward Prashanth, Raff, International Conference on Machine Learning. PMLR2023</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, arXiv:2004.10151Aleksandr Nisnevich, et al. Experience grounds language. 2020arXiv preprint</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, arXiv:2212.038272022arXiv preprint</p>
<p>Space and time in the brain. György Buzsáki, Rodolfo Llinás, arXiv:2309.08600Sparse autoencoders find highly interpretable features in language models. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey, 2017. 2023358arXiv preprint</p>
<p>. Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer Elshowk, Nicholas Joseph, Nova Dassarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah2022aSoftmax linear units. Transformer Circuits Thread</p>
<p>Toy models of superposition. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, arXiv:2209.106522022barXiv preprint</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, arXiv:2304.147672023arXiv preprint</p>
<p>Multimodal neurons in artificial neural networks. Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah, Distill. 63e302021</p>
<p>Distributional vectors encode referential attributes. Abhijeet Gupta, Gemma Boleda, Marco Baroni, Sebastian Padó, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Dmitrii Troitskii, and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, arXiv:2305.016102023arXiv preprint</p>
<p>Microstructure of a spatial map in the entorhinal cortex. Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, Edvard I Moser, Nature. 43670522005</p>
<p>How does gpt-2 compute greater-than?. Michael Hanna, Ollie Liu, Alexandre Variengien, arXiv:2305.00586Interpreting mathematical abilities in a pre-trained language model. 2023arXiv preprint</p>
<p>The elements of statistical learning: data mining, inference, and prediction. Trevor Hastie, Robert Tibshirani, Jerome H Friedman, Jerome H Friedman, 2009Springer2</p>
<p>An overview of catastrophic ai risks. Dan Hendrycks, Mantas Mazeika, Thomas Woodside, arXiv:2306.120012023arXiv preprint</p>
<p>Evan Hernandez, Sen Arnab, Tal Sharma, Kevin Haklay, Martin Meng, Jacob Wattenberg, Yonatan Andreas, David Belinkov, Bau, arXiv:2308.09124Linearity of relation decoding in transformer language models. 2023arXiv preprint</p>
<p>Geographical evaluation of word embeddings. Michal Konkol, Tomáš Brychcín, Michal Nykl, Tomáš Hercig, Proceedings of the Eighth International Joint Conference on Natural Language Processing. Long Papers. the Eighth International Joint Conference on Natural Language Processing20171</p>
<p>Dbpedia -a large-scale, multilingual knowledge base extracted from wikipedia. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Sören Patrick Van Kleef, Christian Auer, Bizer, 2015. 2023</p>
<p>Belinda Z Li, Maxwell Nye, Jacob Andreas, arXiv:2106.00737Implicit representations of meaning in neural language models. 2021arXiv preprint</p>
<p>Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, arXiv:2210.13382Emergent world representations: Exploring a sequence model trained on a synthetic task. 2022arXiv preprint</p>
<p>Do language models know the way to rome?. Mostafa Bastien Liétard, Anders Abdou, Søgaard, arXiv:2109.079712021arXiv preprint</p>
<p>Yizhong Leo Z Liu, Jungo Wang, Hannaneh Kasai, Noah A Hajishirzi, Smith, arXiv:2104.07885Probing across time: What does roberta know and when?. 2021arXiv preprint</p>
<p>Representing spatial structure through maps and language: Lord of the rings encodes the spatial structure of middle earth. M Max, Nick Louwerse, Benesh, Cognitive science. 3682012</p>
<p>Language encodes geographical information. M Max, Rolf A Louwerse, Zwaan, Cognitive Science. 3312009</p>
<p>Locating and editing factual associations in gpt. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Mass-editing memory in a transformer. Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, arXiv:2210.072292022barXiv preprint</p>
<p>The quantization model of neural scaling. Eric J Michaud, Ziming Liu, Uzay Girit, Max Tegmark, arXiv:2303.135062023arXiv preprint</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 262013a</p>
<p>Linguistic regularities in continuous space word representations. Tomáš Mikolov, Wen-Tau Yih, Geoffrey Zweig, Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies. the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies2013b</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, arXiv:2309.009412023arXiv preprint</p>
<p>The alignment problem from a deep learning perspective. Richard Ngo, Lawrence Chan, Sören Mindermann, 2023</p>
<p>City-Government/Points-Of-Interest/rxuy-2muj. Nyc Opendata, Points of interest. 2023</p>
<p>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. O' John, Jonathan Keefe, Dostrovsky, Brain research. 1971</p>
<p>Zoom in: An introduction to circuits. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, Shan Carter, Distill. 532020</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. 2021</p>
<p>Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell, 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE2023</p>
<p>Probing the probing paradigm: Does probing accuracy entail task relevance?. Abhilasha Ravichander, Yonatan Belinkov, Eduard Hovy, arXiv:2005.007192020arXiv preprint</p>
<p>A primer in bertology: What we know about how bert works. Anna Rogers, Olga Kovaleva, Anna Rumshisky, Transactions of the Association for Computational Linguistics. 82021</p>
<p>A neural code for time and space in the human brain. Daniel R Schonhaut, Michael J Zahra M Aghajan, Itzhak Kahana, Fried, Cell Reports. 42112023</p>
<p>Chess as a testbed for language model state tracking. Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 20232arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
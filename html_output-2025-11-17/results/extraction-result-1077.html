<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1077 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1077</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1077</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-26b90ccf7541fd2cd4235118493e1e49d358c351</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/26b90ccf7541fd2cd4235118493e1e49d358c351" target="_blank">StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Emerging Topics in Computational Intelligence</p>
                <p><strong>Paper TL;DR:</strong> An efficient state representation is defined, which breaks down the complexity caused by the large state space in the game environment, and a parameter sharing multi-agent gradient-descent Sarsa($\lambda$) algorithm is proposed to train the units.</p>
                <p><strong>Paper Abstract:</strong> Real-time strategy games have been an important field of game artificial intelligence in recent years. This paper presents a reinforcement learning and curriculum transfer learning method to control multiple units in StarCraft micromanagement. We define an efficient state representation, which breaks down the complexity caused by the large state space in the game environment. Then, a parameter sharing multi-agent gradient-descent Sarsa($\lambda$) algorithm is proposed to train the units. The learning policy is shared among our units to encourage cooperative behaviors. We use a neural network as a function approximator to estimate the action–value function, and propose a reward function to help units balance their move and attack. In addition, a transfer learning method is used to extend our model to more difficult scenarios, which accelerates the training process and improves the learning performance. In small-scale scenarios, our units successfully learn to combat and defeat the built-in AI with 100% win rates. In large-scale scenarios, the curriculum transfer learning method is used to progressively train a group of units, and it shows superior performance over some baseline methods in target scenarios. With reinforcement learning and curriculum transfer learning, our units are able to learn appropriate strategies in StarCraft micromanagement scenarios.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1077.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1077.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PS-MAGDS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parameter Sharing Multi-Agent Gradient-Descent Sarsa(λ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent reinforcement learning controller that shares a centralized policy network across homogeneous game units, trained with gradient-descent Sarsa(λ) and eligibility traces, using a neural-network function approximator and reward shaping to speed training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PS-MAGDS-controlled units (Goliaths / Marines)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated embodied learning agents in the StarCraft micromanagement domain controlled by a parameter-sharing multi-agent gradient-descent Sarsa(λ) algorithm (neural network function approximator; ε-greedy exploration; eligibility traces; reward shaping).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>virtual agent (simulated game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-time strategy combat micro-scenarios in StarCraft: groups of homogeneous ranged or melee units (e.g., Goliaths, Marines) fighting groups of enemy units (Zealots, Zerglings) on a map; complexity arises from many-agent interactions, partial observations per agent, large combinatorial joint-action/state space, dynamic adversary behavior (built-in AI), and temporal credit assignment over episodes (max 1000 steps). Scenarios vary by unit type, number (from small: 3 vs 6 up to large: 20 vs 30 and unseen M40 vs Z60), and terrain/relative positions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number/type of units per side (e.g., 3v6, 3v20, 10v13, 20v30), state representation dimensionality (93 input dimensions per agent), action space size per agent (9 discrete actions), episode length (max 1000 steps), and combinatorial joint-action space scaling with N agents; experiments report explicit unit counts and input dim.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies: small-scale scenarios (3v6, 3v20) = medium; large-scale scenarios (10v13, 20v30, up to 40v60 unseen) = high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of distinct scenario instances / curriculum stages and variation in unit compositions: explicit scenario set (e.g., Goliaths vs Zealots, Goliaths vs Zerglings, Marines vs Zerglings), curriculum classes (e.g., for Marines: M5vZ6 → M8vZ10 → M8vZ12 etc.), and unseen generalization tests (e.g., M40 vs Z60); also training-from-scratch vs transfer-initialized variants.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (multiple scenario instances and curriculum stages; unseen larger scenarios tested indicate high variation capability)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Win rate (%) over test games; secondary metrics: average episode steps, average reward per step (total reward / episode steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Small-scale Goliaths vs Zealots: reached 100% win rate after ≈3000 training episodes; Goliaths vs 20 Zerglings with transfer: reached 100% win rate by end of training; Large-scale: M10 vs Z13: 97% win rate (100 games averaged over 5 runs); M20 vs Z30: 92.4% win rate. Average episode steps and average reward curves reported (e.g., episodes stabilize ~200–400 steps in some transfer runs).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper reports that increasing environment complexity (more units, larger-scale combats) increases training difficulty and sample complexity: learning from scratch is slow and often fails to reach high win rates in large scenarios, while transfer learning and curriculum (reducing variation/complexity initially) accelerate learning and improve final performance; thus there is a trade-off where higher complexity requires more training data but can be mitigated by transfer/curriculum which leverages simpler tasks (low complexity) to bootstrap learning for higher-complexity targets.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>M20 vs Z30 (large-scale): 92.4% win rate (target scenario after curriculum transfer training).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>3 Goliaths vs 6 Zealots (small-scale, single scenario): 100% win rate after ≈3000 episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum transfer learning (CTL) for large-scale scenarios; transfer learning from a well-trained smaller scenario to initialize target scenarios; ε-greedy exploration with exponential decay; reward shaping (intermediate attack/move rewards) and frame-skip = 10; parameter sharing across agents.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — models trained with CTL/general transfer were tested on curricular and unseen scenarios (Table V): e.g., model trained for M10 vs Z13 achieved win rates on related smaller curricular tests (M5vZ6: 80.5%, M8vZ10: 95%) and moderate performance on unseen larger scale (M10vZ15: 81%); model trained for M20 vs Z30 generalized to M40 vs Z60 with 80.5% win rate. Transfer-trained agents generalized substantially better than agents trained from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Small-scale training runs used 4000 episodes and >1 million steps in some experiments; Goliaths needed ≈3000 episodes to reach 100% in 3v6; transfer learning markedly reduced episodes to reach good performance (transfer runs show high initial win rates and faster convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) PS-MAGDS with shared policy and reward shaping can learn effective coordinated strategies (disperse enemies, keep team, hit-and-run) in StarCraft micromanagement. 2) Environment complexity (more units, larger scale) substantially increases learning difficulty and sample requirements. 3) Curriculum transfer learning (train on simpler tasks then progressively harder ones) accelerates training and improves final performance and generalization to unseen larger scenarios compared to learning from scratch. 4) Performance assessed primarily by win rate, average episode length and average reward; PS-MAGDS outperformed several baselines in target large-scale scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_notes</strong></td>
                            <td>Detailed hyperparameters: input dim=93, hidden=100 ReLU units, outputs=9, γ=0.9, α=0.001, λ=0.8, frame_skip=10, ε initialized 0.5 annealed as 0.5/sqrt(1+episode_num).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1077.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1077.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GMEZO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic exploration / GMEZO (zero-order optimization DRL baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep RL baseline (referred to as GMEZO in this paper) based on zero-order optimization for episodic exploration applied to StarCraft micromanagement; used here as a comparative DRL baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic exploration for deep deterministic policies: An application to StarCraft micromanagement tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GMEZO agent (zero-order optimization DRL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL agent employing zero-order optimization and an episodic exploration scheme for StarCraft micromanagement (external baseline implementation referenced and evaluated against PS-MAGDS).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>virtual agent (simulated game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same micromanagement scenarios as PS-MAGDS experiments (various unit counts and types, small and large scale).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Evaluated on same scenario counts (e.g., M10 vs Z13 and M20 vs Z30), so complexity quantified by unit counts and scenario scale.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>tested on large-scale target scenarios (medium→high complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Compared across target scenarios (different unit counts); used as a baseline to evaluate effect of complexity/variation on algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Win rate (%) over 100 test games (averaged over runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table IV: M10 vs Z13: 57% win rate; M20 vs Z30: 88.2% win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Reported comparatively: GMEZO performed worse than PS-MAGDS on M10vZ13 but near the top for M20vZ30 — indicating varying sensitivity of different DRL methods to scenario specifics and scaling; the paper uses such baselines to demonstrate that algorithmic choices and training strategies affect robustness to increased complexity/variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>M20 vs Z30: 88.2% win rate (reported in Table IV).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Not explicitly separated in paper for this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Baseline DRL training as per referenced work (zero-order episodic exploration); in this paper used as comparison, not re-implemented details.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported in paper for this baseline (values taken from baseline authors' reported performance as tested here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GMEZO serves as a competitive DRL baseline; performance varies with scenario — outperformed by PS-MAGDS in some target scenarios (M10vZ13) but competitive in others (M20vZ30); highlights that algorithm and transfer/curriculum choices influence scalability to more complex/varied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1077.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1077.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BicNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiagent Bidirectionally-Coordinated Nets (BicNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent actor-critic neural net architecture that models dependencies between units with bi-directional RNNs to coordinate multi-agent behavior in StarCraft combat games; used here as a state-of-the-art comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BicNet agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A centralized multi-agent actor-critic approach that coordinates multiple units using bi-directional RNNs and an actor-critic training framework; used here as a competitive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>virtual agent (simulated game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same set of combat micro-scenarios; BicNet is tested on large-scale target scenarios to compare coordination efficacy under increasing complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Scenario unit counts and composition (e.g., M10vZ13, M20vZ30); state/action complexity inherent to multi-agent coordination tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>tested on large-scale target scenarios (high complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Evaluated across target scenarios and curricular variations for comparison to PS-MAGDS.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Win rate (%) over test games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table IV: M10 vs Z13: 64% win rate; M20 vs Z30: 100% win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>BicNet achieves very high performance (100%) on the larger M20vZ30 scenario in this comparison, indicating some algorithms may scale better to particular high-complexity instances, but on M10vZ13 it underperforms PS-MAGDS; the paper uses these comparisons to show that curriculum/transfer and algorithmic architecture impact performance across varied complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>M20 vs Z30: 100% win rate (reported in Table IV).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Actor-critic multi-agent training as per the referenced BicNet method; used here as baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not detailed in this paper for BicNet; only final comparative win rates are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BicNet is a strong multi-agent baseline; achieves perfect performance on one large-scale target (M20vZ30) but lower than PS-MAGDS on another (M10vZ13), illustrating that architecture and training strategy interact with environment complexity/variation to determine success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1077.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1077.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule-based baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weakest / Closest rule-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple heuristic controllers used as baselines: 'Weakest' attacks the weakest in range, 'Closest' attacks the closest in range; included to give a lower-bound performance baseline for micromanagement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Weakest / Closest rule-based agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Deterministic heuristic agents: 'Weakest' selects enemy with lowest HP in attack range; 'Closest' selects nearest enemy; no learning algorithm (rule-based).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>virtual agent (simulated game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same combat scenarios; heuristics reflect simple local policies without coordination learning and thus serve to highlight the difficulty added by complexity/variation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Performance measured across same scenario unit counts (e.g., M10vZ13, M20vZ30).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>evaluated on large-scale target scenarios (high complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Tested across target scenarios to provide baseline performance under varying numbers of units.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-to-medium (heuristics fixed; do not adapt across variations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Win rate (%) over test games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table IV: Weakest: M10vZ13 = 23% win rate; M20vZ30 = 0% win rate. Closest: M10vZ13 = 41% win rate; M20vZ30 = 87% win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Rule-based heuristics perform poorly as environment complexity increases (Weakest drops to 0% at M20vZ30), showing that simple fixed heuristics fail under high complexity and variation; some heuristics (Closest) may still perform moderately in some large scenarios but are inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Weakest on M20vZ30: 0% win rate; Closest on M20vZ30: 87% win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>No learning (deterministic heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable (no training).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Heuristic rule-based controllers are insufficient for robust performance in high-complexity/high-variation micromanagement scenarios; demonstrates the need for learning-based methods (and the benefit of curriculum/transfer) to handle scaling complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning', 'publication_date_yy_mm': '2018-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Episodic exploration for deep deterministic policies: An application to StarCraft micromanagement tasks <em>(Rating: 2)</em></li>
                <li>Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games <em>(Rating: 2)</em></li>
                <li>Counterfactual multi-agent policy gradients <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1077",
    "paper_id": "paper-26b90ccf7541fd2cd4235118493e1e49d358c351",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PS-MAGDS",
            "name_full": "Parameter Sharing Multi-Agent Gradient-Descent Sarsa(λ)",
            "brief_description": "A multi-agent reinforcement learning controller that shares a centralized policy network across homogeneous game units, trained with gradient-descent Sarsa(λ) and eligibility traces, using a neural-network function approximator and reward shaping to speed training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PS-MAGDS-controlled units (Goliaths / Marines)",
            "agent_description": "Simulated embodied learning agents in the StarCraft micromanagement domain controlled by a parameter-sharing multi-agent gradient-descent Sarsa(λ) algorithm (neural network function approximator; ε-greedy exploration; eligibility traces; reward shaping).",
            "agent_type": "virtual agent (simulated game units)",
            "environment_name": "StarCraft micromanagement",
            "environment_description": "Real-time strategy combat micro-scenarios in StarCraft: groups of homogeneous ranged or melee units (e.g., Goliaths, Marines) fighting groups of enemy units (Zealots, Zerglings) on a map; complexity arises from many-agent interactions, partial observations per agent, large combinatorial joint-action/state space, dynamic adversary behavior (built-in AI), and temporal credit assignment over episodes (max 1000 steps). Scenarios vary by unit type, number (from small: 3 vs 6 up to large: 20 vs 30 and unseen M40 vs Z60), and terrain/relative positions.",
            "complexity_measure": "Number/type of units per side (e.g., 3v6, 3v20, 10v13, 20v30), state representation dimensionality (93 input dimensions per agent), action space size per agent (9 discrete actions), episode length (max 1000 steps), and combinatorial joint-action space scaling with N agents; experiments report explicit unit counts and input dim.",
            "complexity_level": "varies: small-scale scenarios (3v6, 3v20) = medium; large-scale scenarios (10v13, 20v30, up to 40v60 unseen) = high",
            "variation_measure": "Number of distinct scenario instances / curriculum stages and variation in unit compositions: explicit scenario set (e.g., Goliaths vs Zealots, Goliaths vs Zerglings, Marines vs Zerglings), curriculum classes (e.g., for Marines: M5vZ6 → M8vZ10 → M8vZ12 etc.), and unseen generalization tests (e.g., M40 vs Z60); also training-from-scratch vs transfer-initialized variants.",
            "variation_level": "medium-to-high (multiple scenario instances and curriculum stages; unseen larger scenarios tested indicate high variation capability)",
            "performance_metric": "Win rate (%) over test games; secondary metrics: average episode steps, average reward per step (total reward / episode steps).",
            "performance_value": "Small-scale Goliaths vs Zealots: reached 100% win rate after ≈3000 training episodes; Goliaths vs 20 Zerglings with transfer: reached 100% win rate by end of training; Large-scale: M10 vs Z13: 97% win rate (100 games averaged over 5 runs); M20 vs Z30: 92.4% win rate. Average episode steps and average reward curves reported (e.g., episodes stabilize ~200–400 steps in some transfer runs).",
            "complexity_variation_relationship": "The paper reports that increasing environment complexity (more units, larger-scale combats) increases training difficulty and sample complexity: learning from scratch is slow and often fails to reach high win rates in large scenarios, while transfer learning and curriculum (reducing variation/complexity initially) accelerate learning and improve final performance; thus there is a trade-off where higher complexity requires more training data but can be mitigated by transfer/curriculum which leverages simpler tasks (low complexity) to bootstrap learning for higher-complexity targets.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "M20 vs Z30 (large-scale): 92.4% win rate (target scenario after curriculum transfer training).",
            "low_complexity_low_variation_performance": "3 Goliaths vs 6 Zealots (small-scale, single scenario): 100% win rate after ≈3000 episodes.",
            "training_strategy": "Curriculum transfer learning (CTL) for large-scale scenarios; transfer learning from a well-trained smaller scenario to initialize target scenarios; ε-greedy exploration with exponential decay; reward shaping (intermediate attack/move rewards) and frame-skip = 10; parameter sharing across agents.",
            "generalization_tested": true,
            "generalization_results": "Yes — models trained with CTL/general transfer were tested on curricular and unseen scenarios (Table V): e.g., model trained for M10 vs Z13 achieved win rates on related smaller curricular tests (M5vZ6: 80.5%, M8vZ10: 95%) and moderate performance on unseen larger scale (M10vZ15: 81%); model trained for M20 vs Z30 generalized to M40 vs Z60 with 80.5% win rate. Transfer-trained agents generalized substantially better than agents trained from scratch.",
            "sample_efficiency": "Small-scale training runs used 4000 episodes and &gt;1 million steps in some experiments; Goliaths needed ≈3000 episodes to reach 100% in 3v6; transfer learning markedly reduced episodes to reach good performance (transfer runs show high initial win rates and faster convergence).",
            "key_findings": "1) PS-MAGDS with shared policy and reward shaping can learn effective coordinated strategies (disperse enemies, keep team, hit-and-run) in StarCraft micromanagement. 2) Environment complexity (more units, larger scale) substantially increases learning difficulty and sample requirements. 3) Curriculum transfer learning (train on simpler tasks then progressively harder ones) accelerates training and improves final performance and generalization to unseen larger scenarios compared to learning from scratch. 4) Performance assessed primarily by win rate, average episode length and average reward; PS-MAGDS outperformed several baselines in target large-scale scenarios.",
            "brief_notes": "Detailed hyperparameters: input dim=93, hidden=100 ReLU units, outputs=9, γ=0.9, α=0.001, λ=0.8, frame_skip=10, ε initialized 0.5 annealed as 0.5/sqrt(1+episode_num).",
            "uuid": "e1077.0",
            "source_info": {
                "paper_title": "StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "GMEZO",
            "name_full": "Episodic exploration / GMEZO (zero-order optimization DRL baseline)",
            "brief_description": "A deep RL baseline (referred to as GMEZO in this paper) based on zero-order optimization for episodic exploration applied to StarCraft micromanagement; used here as a comparative DRL baseline.",
            "citation_title": "Episodic exploration for deep deterministic policies: An application to StarCraft micromanagement tasks",
            "mention_or_use": "use",
            "agent_name": "GMEZO agent (zero-order optimization DRL)",
            "agent_description": "A deep RL agent employing zero-order optimization and an episodic exploration scheme for StarCraft micromanagement (external baseline implementation referenced and evaluated against PS-MAGDS).",
            "agent_type": "virtual agent (simulated game units)",
            "environment_name": "StarCraft micromanagement",
            "environment_description": "Same micromanagement scenarios as PS-MAGDS experiments (various unit counts and types, small and large scale).",
            "complexity_measure": "Evaluated on same scenario counts (e.g., M10 vs Z13 and M20 vs Z30), so complexity quantified by unit counts and scenario scale.",
            "complexity_level": "tested on large-scale target scenarios (medium→high complexity)",
            "variation_measure": "Compared across target scenarios (different unit counts); used as a baseline to evaluate effect of complexity/variation on algorithms.",
            "variation_level": "medium",
            "performance_metric": "Win rate (%) over 100 test games (averaged over runs).",
            "performance_value": "Table IV: M10 vs Z13: 57% win rate; M20 vs Z30: 88.2% win rate.",
            "complexity_variation_relationship": "Reported comparatively: GMEZO performed worse than PS-MAGDS on M10vZ13 but near the top for M20vZ30 — indicating varying sensitivity of different DRL methods to scenario specifics and scaling; the paper uses such baselines to demonstrate that algorithmic choices and training strategies affect robustness to increased complexity/variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "M20 vs Z30: 88.2% win rate (reported in Table IV).",
            "low_complexity_low_variation_performance": "Not explicitly separated in paper for this baseline.",
            "training_strategy": "Baseline DRL training as per referenced work (zero-order episodic exploration); in this paper used as comparison, not re-implemented details.",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Not reported in paper for this baseline (values taken from baseline authors' reported performance as tested here).",
            "key_findings": "GMEZO serves as a competitive DRL baseline; performance varies with scenario — outperformed by PS-MAGDS in some target scenarios (M10vZ13) but competitive in others (M20vZ30); highlights that algorithm and transfer/curriculum choices influence scalability to more complex/varied environments.",
            "uuid": "e1077.1",
            "source_info": {
                "paper_title": "StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "BicNet",
            "name_full": "Multiagent Bidirectionally-Coordinated Nets (BicNet)",
            "brief_description": "A multi-agent actor-critic neural net architecture that models dependencies between units with bi-directional RNNs to coordinate multi-agent behavior in StarCraft combat games; used here as a state-of-the-art comparative baseline.",
            "citation_title": "Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games",
            "mention_or_use": "use",
            "agent_name": "BicNet agent",
            "agent_description": "A centralized multi-agent actor-critic approach that coordinates multiple units using bi-directional RNNs and an actor-critic training framework; used here as a competitive baseline.",
            "agent_type": "virtual agent (simulated game units)",
            "environment_name": "StarCraft micromanagement",
            "environment_description": "Same set of combat micro-scenarios; BicNet is tested on large-scale target scenarios to compare coordination efficacy under increasing complexity and variation.",
            "complexity_measure": "Scenario unit counts and composition (e.g., M10vZ13, M20vZ30); state/action complexity inherent to multi-agent coordination tasks.",
            "complexity_level": "tested on large-scale target scenarios (high complexity)",
            "variation_measure": "Evaluated across target scenarios and curricular variations for comparison to PS-MAGDS.",
            "variation_level": "medium-to-high",
            "performance_metric": "Win rate (%) over test games.",
            "performance_value": "Table IV: M10 vs Z13: 64% win rate; M20 vs Z30: 100% win rate.",
            "complexity_variation_relationship": "BicNet achieves very high performance (100%) on the larger M20vZ30 scenario in this comparison, indicating some algorithms may scale better to particular high-complexity instances, but on M10vZ13 it underperforms PS-MAGDS; the paper uses these comparisons to show that curriculum/transfer and algorithmic architecture impact performance across varied complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "M20 vs Z30: 100% win rate (reported in Table IV).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Actor-critic multi-agent training as per the referenced BicNet method; used here as baseline for comparison.",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Not detailed in this paper for BicNet; only final comparative win rates are reported.",
            "key_findings": "BicNet is a strong multi-agent baseline; achieves perfect performance on one large-scale target (M20vZ30) but lower than PS-MAGDS on another (M10vZ13), illustrating that architecture and training strategy interact with environment complexity/variation to determine success.",
            "uuid": "e1077.2",
            "source_info": {
                "paper_title": "StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning",
                "publication_date_yy_mm": "2018-04"
            }
        },
        {
            "name_short": "Rule-based baselines",
            "name_full": "Weakest / Closest rule-based agents",
            "brief_description": "Simple heuristic controllers used as baselines: 'Weakest' attacks the weakest in range, 'Closest' attacks the closest in range; included to give a lower-bound performance baseline for micromanagement tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Weakest / Closest rule-based agents",
            "agent_description": "Deterministic heuristic agents: 'Weakest' selects enemy with lowest HP in attack range; 'Closest' selects nearest enemy; no learning algorithm (rule-based).",
            "agent_type": "virtual agent (simulated game units)",
            "environment_name": "StarCraft micromanagement",
            "environment_description": "Same combat scenarios; heuristics reflect simple local policies without coordination learning and thus serve to highlight the difficulty added by complexity/variation.",
            "complexity_measure": "Performance measured across same scenario unit counts (e.g., M10vZ13, M20vZ30).",
            "complexity_level": "evaluated on large-scale target scenarios (high complexity)",
            "variation_measure": "Tested across target scenarios to provide baseline performance under varying numbers of units.",
            "variation_level": "low-to-medium (heuristics fixed; do not adapt across variations)",
            "performance_metric": "Win rate (%) over test games.",
            "performance_value": "Table IV: Weakest: M10vZ13 = 23% win rate; M20vZ30 = 0% win rate. Closest: M10vZ13 = 41% win rate; M20vZ30 = 87% win rate.",
            "complexity_variation_relationship": "Rule-based heuristics perform poorly as environment complexity increases (Weakest drops to 0% at M20vZ30), showing that simple fixed heuristics fail under high complexity and variation; some heuristics (Closest) may still perform moderately in some large scenarios but are inconsistent.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Weakest on M20vZ30: 0% win rate; Closest on M20vZ30: 87% win rate.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "No learning (deterministic heuristics).",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Not applicable (no training).",
            "key_findings": "Heuristic rule-based controllers are insufficient for robust performance in high-complexity/high-variation micromanagement scenarios; demonstrates the need for learning-based methods (and the benefit of curriculum/transfer) to handle scaling complexity.",
            "uuid": "e1077.3",
            "source_info": {
                "paper_title": "StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning",
                "publication_date_yy_mm": "2018-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Episodic exploration for deep deterministic policies: An application to StarCraft micromanagement tasks",
            "rating": 2
        },
        {
            "paper_title": "Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games",
            "rating": 2
        },
        {
            "paper_title": "Counterfactual multi-agent policy gradients",
            "rating": 1
        }
    ],
    "cost": 0.0153135,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning</h1>
<p>Kun Shao, Yuanheng Zhu, Member, IEEE and Dongbin Zhao, Senior Member, IEEE</p>
<h4>Abstract</h4>
<p>Real-time strategy games have been an important field of game artificial intelligence in recent years. This paper presents a reinforcement learning and curriculum transfer learning method to control multiple units in StarCraft micromanagement. We define an efficient state representation, which breaks down the complexity caused by the large state space in the game environment. Then a parameter sharing multi-agent gradientdescent Sarsa $(\lambda)$ (PS-MAGDS) algorithm is proposed to train the units. The learning policy is shared among our units to encourage cooperative behaviors. We use a neural network as a function approximator to estimate the action-value function, and propose a reward function to help units balance their move and attack. In addition, a transfer learning method is used to extend our model to more difficult scenarios, which accelerates the training process and improves the learning performance. In small scale scenarios, our units successfully learn to combat and defeat the built-in AI with $100 \%$ win rates. In large scale scenarios, curriculum transfer learning method is used to progressively train a group of units, and shows superior performance over some baseline methods in target scenarios. With reinforcement learning and curriculum transfer learning, our units are able to learn appropriate strategies in StarCraft micromanagement scenarios.</p>
<p>Index Terms-reinforcement learning, transfer learning, curriculum learning, neural network, game AI.</p>
<h2>I. INTRODUCTION</h2>
<p>ARTIFICIAL intelligence (AI) has a great advance in the last decade. As an excellent testbed for AI research, games have been helping AI to grow since its birth, including the ancient board game [1], [2], [3], [4], the classic Atari video games [5], [6], and the imperfect information game [7]. These games have a fixed, limited set of actions, and researchers only need to control a single agent in game environment. Besides, there are a large number of games including multiple agents and requiring complex rules, which are much more difficult for AI research.</p>
<p>In this paper, we focus on a real-time strategy (RTS) game to explore the learning of multi-agent control. RTS games are usually running in real-time, which is different from taking turns to play in board games [8]. As one of the most popular RTS games, StarCraft has a huge player base and numerous professional competitions, requiring different strategies, tactics and reactive control techniques. For the game AI research, StarCraft provides an ideal environment</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to study the control of multiple units with different difficulty levels [9]. In recent years, the study of StarCraft AI has an impressive progress, driven by some StarCraft AI competitions and Brood War Application Programming Interface (BWAPI) ${ }^{1}$ [10]. Recently, researchers have developed more efficient platforms to promote the development of this field, including TorchCraft, ELF and PySC2. StarCraft AI aims at solving a series of challenges, such as spatial and temporal reasoning, multi-agent collaboration, opponent modeling and adversarial planning [8]. At present, designing a game AI for the full StarCraft game based on machine learning method is out-ofreach. Many researchers focus on micromanagement as the first step to study AI in StarCraft [11]. In combat scenarios, units have to navigate in highly dynamic environment and attack enemies within fire range. There are many methods for StarCraft micromanagement, including potential fields for spatial navigation and obstacle avoidance [12], [13], bayesian modeling to deal with incompleteness and uncertainty in the game [14], heuristic game-tree search to handle both build order planning and units control [15], and neuroevolution to control individual unit with hand-craft features [16].</p>
<p>As an intelligent learning method, reinforcement learning (RL) is very suitable for sequential decision-making tasks. In StarCraft micromanagement, there are some interesting applications with RL methods. Shantia et al. use online Sarsa and neural-fitted Sarsa with a short term memory reward function to control units' attack and retreat [17]. They use vision grids to obtain the terrain information. This method needs a handcraft design, and the number of input nodes has to change with the number of units. Besides, they apply an incremental learning method to scale the task to a larger scenario with 6 units. However, the win rate with incremental learning is still below $50 \%$. Wender et al. use different RL algorithms in micromanagement, including $Q$ learning and Sarsa [18]. They control one powerful unit to play against multiple weak units, without cooperation and teamwork between own units.</p>
<p>In the last few years, deep learning has achieved a remarkable performance in many complex problems [19], and has dramatically improved the generalization and scalability of traditional RL algorithms [5]. Deep reinforcement learning (DRL) can teach agents to make decisions in high-dimension state space by an end-to-end method. Usunier et al. propose an RL method to tackle micromanagement with deep neural network [20]. They use a greedy MDP to choose actions for units sequentially at each time step, with zero-order optimization to update the model. This method is able to control all</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>units owned by the player, and observe the full state of the game. Peng et al. use an actor-critic method and recurrent neural networks (RNNs) to play StarCraft combat games [21]. The dependency of units is modeled by bi-directional RNNs in hidden layers, and its gradient update is efficiently propagated through the entire networks. Different from Usunier’s and Peng’s work that design centralized controllers, Foerster et al. propose a multi-agent actor-critic method to tackle decentralized micromanagement tasks, which significantly improves the performance over centralized RL controllers [22].</p>
<p>For StarCraft micromanagement, traditional methods have difficulties in handling complicated state and action space, and learning cooperative tactics. Modern methods rely on strong compute capability introduced by deep learning. Besides, learning micromanagement with model-free RL methods usually needs a lot of training time, which is even more serious in large scale scenarios. In this paper, we dedicate to explore more efficient state representation to break down the complexity caused by the large state space, and propose appropriate RL algorithm to solve the problem of multi-agent decision making in StarCraft micromanagement. In addition, we introduce curriculum transfer learning to extend the RL model to various scenarios and improve the sample efficiency.</p>
<p>The main contributions emphasize in three parts. First, we propose an efficient state representation method to deal with the large state space in StarCraft micromanagement. This method takes units’ attributes and distances into consideration, allowing an arbitrary number of units on both sides. Compared with related work, our state representation is more concise and more efficient. Second, we present a parameter sharing multi-agent gradient-descent $\operatorname{Sarsa}(\lambda)$ (PSMAGDS) algorithm to train our units. Using a neural network as a function approximator, agents share the parameters of a centralized policy, and update the policy with their own experiences simultaneously. This method trains homogeneous agents efficiently, and encourages cooperative behaviors. To solve the problem of sparse and delayed rewards, we introduce a reward function including small intermediate rewards in the RL model. This reward function improves the training process, and serves as an intrinsic motivation to help units collaborate with each other. Third, we propose a transfer learning method to extend our model to various scenarios. Compared with learning from scratch, this method accelerates the training process and improves the learning performance to a great extent. In large scale scenarios, we apply curriculum transfer learning method to successfully train a group of units. In term of win rates, our proposed method is superior to some baseline methods in target scenarios.</p>
<p>The rest of the paper is organized as follows. In Section II, we describe the problem formulation of StarCraft micromanagement, as well as backgrounds of reinforcement learning and transfer curriculum learning. In Section III, we present the reinforcement learning model for micromanagement, including state representation method, network architecture and action definition. And in Section IV, we introduce the parameter sharing multi-agent gradient-descent $\operatorname{Sarsa}(\lambda)$ algorithm and the reward function. In Section V, we introduce the StarCraft micromanagement scenarios used in our paper and the training</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Representation of the agent-environment interaction in reinforcement learning.</p>
<p>details. In Section VI, we make an analysis of experimental results and discuss the learned strategies. In the end, we draw a conclusion of the paper and propose some future work.</p>
<h2>II. Problem Formulation and Backgrounds</h2>
<h3>A. Problem Formulation</h3>
<p>In StarCraft micromanagement, we need to control a group of units to destroy the enemies under certain terrain conditions. The combat scenario with multiple units is approximated as a Markov game, a multi-agent extension of Markov decision processes (MDPs) [21], [22], [23]. In a Markov game with $N$ agents, a set of states $S$ are used to describe the properties of all agents and the environment, as well as a set of actions $A_{1}, \ldots, A_{N}$ and observations $O_{1}, \ldots, O_{N}$ for each agent.</p>
<p>In the combat, units in each side need to cooperate with each other. Developing a learning model for multiple units is challenging in micromanagement. In order to maintain a flexible framework and allow an arbitrary number of units, we consider that our units have access to the state space $S$ from its own observation of the current combat by treating other units as part of the environment $S \rightarrow O_{i}$. Each unit interacts in the combat environment with its own observation and action. $S \times A_{1} \times \ldots \times A_{N} \rightarrow S^{\prime}$ denotes the transition from state $S$ to the successive state $S^{\prime}$ with actions of all the units, and $R_{1} \ldots R_{N}$ are the generated rewards of each unit. For the sake of multi-agent cooperation, the policy is shared among our units. The goal of each unit is to maximize its total expected return.</p>
<h3>B. Reinforcement Learning</h3>
<p>To solve the multi-agent control problem in StarCraft micromanagement, we can resort to reinforcement learning. Reinforcement learning is a type of machine learning algorithms in which agents learn by trial and error and determine the ideal behavior from its own experience with the environment [24]. We draw the classic RL diagram in Fig. 1. It shows the process that an RL agent interacts with the environment. The agent-environment interaction process in RL is formulated as a Markov decision process. The agent in state $s$ makes an action $a$ according to the policy $\pi$. This behavior causes a reward $r$, and transfers to a new state $s^{\prime}$. We define the future discounted return at time $t$ as $\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r_{t^{\prime}}$, where $T$ is the terminal time step and $\gamma \in[0,1]$ is a discount factor that determines the importance of future rewards. The aim of an</p>
<p>RL model is to learn an optimal policy $\pi$, which defines the probability of selecting action $a$ in state $s$, so that the sum of the overall discounted rewards is maximized, as demonstrated by</p>
<p>$\max_{\pi}\mathbb{E}[\sum_{t^{\prime}=t}^{T}\gamma^{t^{\prime}-t}{r_{t^{\prime}}|s=s_{t},a=a_{t},\pi}].$ (1)</p>
<p>As one of the most popular RL algorithms, temporal difference (TD) learning is a combination of Monte Carlo method and dynamic programming method. TD method can learn from raw experience without a model of the environment, and update estimates based on part of the sequence, without waiting for a final outcome [25]. The most widely known TD learning algorithms are Q-learning and Sarsa. Q-learning estimates the value of making an action in a given state and iteratively updates the $Q$-value estimate towards the observed reward. The TD error $\delta_{t}$ in Q-learning is computed as</p>
<p>$\delta_{t}=r_{t+1}+\gamma\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t})$
Q-learning is an off-policy learning method, which means it learns a different policy compared with the one choosing actions. Different from Q-learning’s off-policy mechanism, Sarsa is an on-policy method, which means the policy is used both for selecting actions and updating previous $Q$-value [24]. The Sarsa update rule is demonstrated as</p>
<p>$\delta_{t}=r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})$
$Q(s_{t+1},a_{t+1})=Q(s_{t},a_{t})+\alpha\delta_{t}$
where $\alpha$ is the learning rate. Traditional reinforcement learning methods have some successful applications, including TD in Backgammon [26] and adaptive dynamic programming (ADP) in control [27], [28], [29].</p>
<p>Reinforcement learning with deep neural networks function approximators has received great attentions in recent years. DRL provides an opportunity to train a single agent to solve a series of human-level tasks by an end-to-end manner [30] [31]. As the most famous DRL algorithm, deep Q-network (DQN) uses the experience replay technique and a target network to remove the correlations between samples and stabilize the training process [5]. In the last few years, we have witnessed a great number of improvements on DQN, including double DQN [32], prioritised DQN [33], dueling DQN [34], distributed DQN [35] and asynchronous DQN [36]. Apart from value-based DRL methods like DQN and its variants, policy-based DRL methods use deep networks to parameterize and optimize the policy directly [37]. Deep deterministic policy gradient (DDPG) is the continuous analogue of DQN, which uses a critic to estimate the value of current policy and an actor to update the policy [38]. Policy-based DRL methods play important roles in continuous control, including asynchronous advantage actor-critic (A3C) [36], trust region policy optimization (TRPO) [39], proximal policy optimization (PPO) [40], and so on. The sample complexity of traditional DRL methods tends to be high, which limits these methods to real-world applications. While model-based DRL approaches learn value function and policy in a data-efficient way, and have been widely used in sensorimotor control. Guided policy</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Representation of curriculum transfer learning. Storing knowledge gained solving source task and gradually applying it to $M$ curricular tasks to update the knowledge. Eventually, applying it to the target task.</p>
<p>search (GPS) uses a supervised learning algorithm to train policy and an RL algorithm to generate guiding distributions, allowing to train deep policies efficiently [41]. Researchers have also proposed some other model-based DRL methods, like normalized advantage functions (NAF) [42] and embed to control (E2C) [43].</p>
<p>Multi-agent reinforcement learning is a closely related area to our work [44]. A multi-agent system includes a number of agents interacting in one environment [45] [46]. Recently, some multi-agent reinforcement learning algorithms with deep neural network are proposed to learn communication [47], cooperative-competitive behaviors [23] and imperfect information [48]. In our work, we use a multi-agent reinforcement learning method with policy sharing among agents to learn cooperative behaviors. Agents share the parameters of a centralized policy, and update the policy with their own experiences simultaneously. This method can train homogeneous agents more efficiently [49].</p>
<h3><em>C. Curriculum Transfer Learning</em></h3>
<p>Generally speaking, model-free reinforcement learning methods need plenty of samples to learn an optimal policy. However, many challenging tasks are difficult for traditional RL methods to learn admissible policies in large state and action space. In StarCraft micromanagement, there are numerous scenarios with different units and terrain conditions. It will take a lot of time to learn useful strategies in different scenarios from scratch. A number of researchers focus on improving the learning speed and performance by exploiting domain knowledge across various but related tasks. The most widely used approach is transfer learning (TL) [50] [51]. To some extent, transfer learning is a kind of generalization across tasks, transferring knowledge from source tasks to target tasks. Besides, transfer learning can be extended to RL problems by using the model parameters in the same model architecture [52]. The procedure of using transfer learning in our experiments is training the model with RL method in a source scenario first. And then, we can use the well-trained model as a starting point to learn micromanagement in target scenarios.</p>
<p>As a special form of transfer learning, curriculum learning involves a set of tasks organized by increasing level of difficulties. The initial tasks are used to guide the learner so that it can perform better on the final task [53]. Combining curriculum</p>
<p>TABLE I: THE DATA TYPE AND DIMENSION OF INPUTS IN OUR MODEL.</p>
<table>
<thead>
<tr>
<th>Inputs</th>
<th>CoolDown</th>
<th>HitPoint</th>
<th>OwnSumInfo</th>
<th>OwnMaxInfo</th>
<th>EnemySumInfo</th>
<th>EnemyMaxInfo</th>
<th>TerrainInfo</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>$\in R$</td>
<td>$\in R$</td>
<td>$\in R$</td>
<td>$\in R$</td>
<td>$\in R$</td>
<td>$\in R$</td>
<td>$\in R$</td>
<td>$\in$ cat.</td>
</tr>
<tr>
<td>Dimension</td>
<td>1</td>
<td>1</td>
<td>8</td>
<td>8</td>
<td>8</td>
<td>8</td>
<td>8</td>
<td>9</td>
</tr>
</tbody>
</table>
<p>Note: $R$ means the input has real value and cat. means the input is categorical and one-hot encoded. learning and transfer learning, curriculum transfer learning (CTL) method has shown good performance to help the learning process converge faster and towards better optimum in recent work [54], [55], [56]. For micromanagement, a feasible method of using CTL is mastering a simple scenario first, and then solving difficult scenarios based on this knowledge. By changing the number and type of units, we could control the difficulty of micromanagement. In this way, we can use CTL to train our units with a sequence of progressively difficult micromanagement scenarios, as shown in Fig. 2.</p>
<h2>III. LEARNING MODEL FOR MICROMANAGEMENT</h2>
<h2>A. Representation of High-Dimension State</h2>
<p>State representation of StarCraft is still an open problem with no universal solution. We construct a state representation with inputs from the game engine, which have different data types and dimensions, as depicted in Table I. The proposed state representation method is efficient and independent of the number of units in the combat. In summary, the state representation is composed of three parts: the current step state information, the last step state information and the last step action, as shown in Fig. 3. The current step state information includes own weapon's cooldown time, own unit's hitpoint, distances information of own units, distances information of enemy units and distances information of terrain. The last step state information is the same with the current step. We take the last step action into consideration, which has been proven to be helpful for the learning process in the RL domain [57], [58]. The proposed state representation method also has good generalization and can be used in other combat games, which need to take agents' property and distance information into consideration.</p>
<p>All inputs with real type are normalized by their maximum values. Among them, CoolDown and HitPoint have 1 dimension for each. We divide the combat map into 8 sector areas on average, and compute the distances information in each area. Units' distance information is listed as follows:</p>
<ul>
<li>OwnSumInfo: own units' distances are summed in each area;</li>
<li>OwnMaxInfo: own units' distances are maximized in each area;</li>
<li>EnemySumInfo: enemy units' distances are summed in each area;</li>
<li>EnemyMaxInfo: enemy units' distances are maximized in each area.</li>
</ul>
<p>If a unit is out of the central unit's sight range $D$, the unit's distance value dis_unit is set to 0.05 . Otherwise, the value is linear with $d$, the distance to the central unit, as demonstrated in equation (4).</p>
<p>$$ \begin{gathered} \text { Step t-1 } \ \text { CoolDown } \ \text { HitPoint } \ \text { OwnSumInfo } \ \text { OwnMaxInfo } \ \text { EnemySumInfo } \ \text { EnemyMaxInfo } \ \text { TerrainInfo } \end{gathered} $$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Representation of the learning model of one unit in StarCraft micromanagement scenarios. The state representation has three parts and a neural network is used as the function approximator. The network outputs the probabilities of moving to 8 directions and attack.</p>
<p>arbitrary directions with arbitrary distances in the map. When the unit decides to attack, it can choose any enemy units in the weapon's fire range. In order to simplify the action space, we choose 8 move directions with a fixed distance and attacking the weakest as the available actions for each unit. When the chosen action is move, our units will turn to one of the 8 directions, Up, Down, Left, Right, Upper-left, Upper-right, Lower-left, Lower-right, and move a fixed distance. When the chosen action is attack, our units will stay at the current position and focus fire on enemy units. Currently, we select the enemy with the lowest hitpoint in our weapon's attack range as the target. According to the experimental results, these actions are enough to control our units in the game.</p>
<h3><em>C. Network Architecture</em></h3>
<p>Because our units' experience has a limited subset of the large state space and most test states will never be explored before, it will be difficult to apply table reinforcement learning to learn an optimal policy. To solve this problem, we use a neural network parameterized by vector <strong>θ</strong> to approximate the state-action values to improve our RL model's generalization.</p>
<p>The input of the network is the 93 dimensions tensor from the state representation. We has 100 neurons in the hidden layer, and use the rectified linear unit (ReLU) activation function for the network nonlinearity, as demonstrated by</p>
<p>$$f(\mathbf{z}) = \max(\mathbf{0}, \mathbf{z}),\tag{6}$$</p>
<p>where <strong>z</strong> is the output of hidden layer. We use ReLU function rather than Sigmoid or tanh function, because ReLU function does not have the problem of gradient descent, which can guarantee the effective training of the model [59]. Different from these saturating nonlinearities functions such as Sigmoid or tanh, ReLU function is a non-saturating nonlinearity function. In terms of training time with gradient descent, the non-saturating nonlinearity is much faster [60].</p>
<p>The output layer of the neural network has 9 neurons, giving the probabilities of moving to 8 directions and attack. The learning model of one unit in StarCraft micromanagement scenarios, including state representation, neural network architecture and output actions, is depicted in Fig. 3.</p>
<h3>IV. LEARNING METHOD FOR MICROMANAGEMENT</h3>
<p>In this paper, we formulate StarCraft micromanagement as a multi-agent reinforcement learning model. We propose a parameter sharing multi-agent gradient-descent Sarsa(λ) (PS-MAGDS) method to train the model, and design a reward function as intrinsic motivations to promote the learning process. The whole PS-MAGDS reinforcement learning diagram is depicted in Fig. 4.</p>
<h4>A. Parameter Sharing Multi-agent Gradient-Descent Sarsa(λ)</h4>
<p>We propose a multi-agent RL algorithm that extends the traditional Sarsa(λ) to multiple units by sharing the parameters of the policy network among our units. To accelerate the learning process and tackle the problem of delayed rewards, we use eligibility traces in reinforcement learning. As a</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. The PS-MAGDS reinforcement learning diagram in the StarCraft micromanagement scenarios.</p>
<p>basic mechanism in RL, eligibility traces are used to assign temporal credit, which consider a set of previously experienced transitions [61]. This means it not only considers the value of the last state-action pair but also the visited ones. With this method, we can solve the problem of delayed reward in the game environment. Sarsa with eligibility traces, termed as Sarsa(λ), is one way of averaging backups made after multiple steps. λ is a factor that determines the weight of each backup. In our implementation of Sarsa(λ) for multiple units combat, we use a neural network as the function approximator and share network parameters among all our units. Although we have only one network to train, the units can still behave differently because each one receives different observations and actions as its input.</p>
<p>To update the policy network efficiently, we use the gradient-descent method to train the Sarsa(λ) reinforcement learning model. The gradient-descent learning update is demonstrated in equation (7),</p>
<p>$$\delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}; \theta_t) - Q(s_t, a_t; \theta_t) \tag{7a}$$</p>
<p>$$\theta_{t+1} = \theta_t + \alpha \delta_t \mathbf{e}_t \tag{7b}$$</p>
<p>$$\mathbf{e}<em t-1="t-1">t = \gamma \lambda \mathbf{e}</em>$$} + \nabla_{\theta_t} Q(s_t, a_t; \theta_t), \mathbf{e}_0 = 0 \tag{7c</p>
<p>where <strong>e</strong> is the eligibility traces at time step t.</p>
<p>One of the challenging issues in reinforcement learning is the trade-off between exploration and exploitation. If we choose the best action every step according to current policy, we are likely to trap in local optimum. On the contrary, if we tend to explore in the large state space, the model will have difficulty in converging. In the experiment, we use the ε-greedy method to choose actions during training, which selects the current best action with probability 1 − ε, and takes a random exploratory action with probability ε,</p>
<p>$$a = \begin{cases} \text{randint}(N), &amp; \text{random}(0,1) &lt; \epsilon \ \arg \max_a Q(s,a), &amp; \text{otherwise} \end{cases} \tag{8}$$</p>
<p>where N equals to 9 in the experiment.</p>
<p>We use exponentially ε decay to implement the ε-greedy method. The ε is initialized to 0.5 and anneals schedule with an exponential smoothing window of the episode number <em>episode_num</em>, as demonstrated by</p>
<p>$$\epsilon = 0.5/\sqrt{1 + \text{episode_num}}.\tag{9}$$</p>
<p>The overall parameter sharing multi-agent gradient-descent $\operatorname{Sarsa}(\lambda)$ method is presented in Algorithm 1.</p>
<h2>B. Reward Function</h2>
<p>The reward function provides useful feedbacks for RL agents, which has a significant impact on the learning results [62]. The goal of StarCraft micromanagement is to destroy all of the enemy units in the combat. If the reward is only based on the final result, the reward function will be extremely sparse. Moreover, units usually get a positive reward after many steps. The delay in rewards makes it difficult to learn which set of actions is responsible for the corresponding rewards.</p>
<p>To tackle the problem of sparse and delayed rewards in micromanagement, we design a reward function to include small intermediate rewards. In our experiment, all agents receive the main reward caused by their attack action at each time step, equalling to the damage that the enemy units received minus the hitpoint loss of our units.</p>
<p>$$
\begin{aligned}
r_{t}= &amp; \left(\text { damage_amount }<em t-1="t-1">{t} \times \text { damage_factor }-\rho \times\right. \
&amp; \left.\left(\text { unit_hitpoint }</em>\right)\right) / 10
\end{aligned}
$$}-\text { unit_hitpoint }_{t</p>
<p>where damage_amount is the amount of damage caused by our units' attack, damage_factor is our units' attack force and unit_hitpoint is our unit's hitpoint. We divide the reward by a constant to resize it to a more suitable range, which is set to 10 in our experiment. $\rho$ is a normalized factor to balance the total hitpoint of our units and enemy units,</p>
<p>$$
\rho=\sum_{i=1}^{H} \text { enemy_hitpoint }<em j="1">{i} / \sum</em>
$$}^{N} \text { unit_hitpoint }_{j</p>
<p>where $H$ is the number of enemy units, and $N$ is the number of our units. Generally speaking, this normalized factor is necessary in StarCraft micromanagement with different numbers and types of units. Without proper normalization, policy network will have difficulty in converging, and our units need much more episodes to learn useful behaviors.</p>
<p>Apart from the basic attack reward, we consider some extra rewards as the intrinsic motivation to speed up the training process. When a unit is destroyed, we introduce an extra negative reward, and set it to -10 in our experiment. We would like to punish this behavior in consideration that the decrease of own units has a bad influence on the combat result. Besides, in order to encourage our units to work as a team and make cooperative actions, we introduce a reward for units' move. If there are no our units or enemy units in the move direction, we give this move action a small negative reward, which is set to -0.5 . According to the experiment, this reward has an impressive effect on the learning performance, as shown in Fig. 6.</p>
<h2>C. Frame Skip</h2>
<p>When applying reinforcement learning to video games, we should pay attention to the continuity of actions. Because of the real-time property of StarCraft micromanagement, it is impractical to make a action every game frame. One feasible</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Parameter Sharing Multi-Agent Gradient-
Descent Sarsa( \(\lambda)\)
    Initialize policy parameters \(\boldsymbol{\theta}\) shared among our units
    Repeat (for each episode):
        \(\boldsymbol{e}_{0}=\mathbf{0}\)
        Initialize \(s_{t}, a_{t}\)
        Repeat (for each step of episode):
            Repeat (for each unit):
            Take action \(a_{t}\), receive \(r_{t+1}\), next state \(s_{t+1}\)
            Choose \(a_{t+1}\) from \(s_{t+1}\) using \(\epsilon\)-greedy
            If \(\operatorname{random}(0,1)&lt;\epsilon\)
                \(a_{t+1}=\operatorname{randint}(N)\)
            else
                \(a_{t+1}=\arg \max <span class="ge">_{a} Q\left(s_</span>{t+1}, a ; \boldsymbol{\theta}_{t}\right)\)
            Repeat (for each unit):
            Update TD error, weights and eligibility traces
            \(\delta_{t}=r_{t+1}+\gamma Q\left(s_{t+1}, a_{t+1} ; \boldsymbol{\theta}_{t}\right)-Q\left(s_{t}, a_{t} ; \boldsymbol{\theta}_{t}\right)\)
            \(\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha \delta_{t} \boldsymbol{e}_{t}\)
            \(\boldsymbol{e}_{t+1}=\gamma \lambda \boldsymbol{e}_{t}+\nabla_{\boldsymbol{\theta}_{t+1}} Q\left(s_{t+1}, a_{t+1} ; \boldsymbol{\theta}_{t+1}\right)\)
            \(t \leftarrow t+1\)
        until \(s_{t}\) is terminal
</code></pre></div>

<p>TABLE II
THE COMPARATIVE ATTRIBUTES OF DIFFERENT UNITS IN OUR MICROMANAGEMENT SCENARIOS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attributes</th>
<th style="text-align: center;">Goliath</th>
<th style="text-align: center;">Zealot</th>
<th style="text-align: center;">Zergling</th>
<th style="text-align: center;">Marine</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Race</td>
<td style="text-align: center;">Terran</td>
<td style="text-align: center;">Protoss</td>
<td style="text-align: center;">Zerg</td>
<td style="text-align: center;">Terran</td>
</tr>
<tr>
<td style="text-align: center;">HitPoint</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: center;">CoolDown</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">Damage Factor</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Defence Factor</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Fire Range</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Sight Range</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7</td>
</tr>
</tbody>
</table>
<p>method is using frame skip technology, which executes a training step every fixed number of frames. However, small frame skip will introduce strong correlation in the training data, while large frame skip will reduce effective training samples. We refer to related work in [20], and try several frame skips $(8,10,12)$ in a small scale micromanagement scenario. At last, we set the frame skip to 10 in our experiment, which takes an action every 10 frames for each unit.</p>
<h2>V. EXPERIMENT SETTINGS</h2>
<h2>A. StarCraft Micromanagement Scenarios</h2>
<p>We consider several StarCraft micromanagement scenarios with various units, including Goliaths vs. Zealots, Goliaths vs. Zerglings and Marines vs. Zerglings, as shown in Fig. 5.</p>
<p>1) In the first scenario, we will control 3 Goliaths to fight against 6 Zealots. From Table II, we can see that the enemy units have advantage on the number of units, hitpoint and damage factor. By contrast, our units' fire range is much wider.
2) In the second scenario, the enemies have 20 Zerglings. Our Goliaths units have advantage on hitpoint, damage</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Representation of the StarCraft micromanagement scenarios in our experiments, left: Goliaths vs. Zealots; middle: Goliaths vs. Zerglings; right: Marines vs. Zerglings.</p>
<p>factor and fire range, while the enemies have much more units and less cooldown time.</p>
<p>3) In the third scenario, we will control up to 20 Marines to fight against 30 Zerglings. The enemy units have advantage on speed and amount, while our units have advantage on fire range and damage factor.</p>
<p>We divide these scenarios into two groups. The first and the second are small scale micromanagements and the last is the large scale micromanagement. In these scenarios, the enemy units are controlled by the built-in AI, which is hard-coded with game inputs. An episode terminates when either side of the units are destroyed. A human beginner of StarCraft can't beat the built-in AI in these scenarios. Platinum-level players have average win rates of below 50% with 100 games for each scenario. Our RL agents is expected to exploit their advantages and avoid their disadvantages to win these combats.</p>
<h3><em>B. Training</em></h3>
<p>In the training process, we set the discount factor γ to 0.9, the learning rate α to 0.001, and the eligibility traces factor λ to 0.8 in all scenarios. Moreover, the maximum steps of each episode are set to 1000. In order to accelerate the learning process, the game runs at full speed by setting gameSpeed to 0 in BWAPI. The experiment is applied on a computer with an Intel i7-6700 CPU and 16GB of memory.</p>
<h3>VI. RESULTS AND DISCUSSIONS</h3>
<p>In this section, we analyze the results in different micromanagement scenarios and discuss our RL model's performance. In small scale scenarios, we use the first scenario as a starting point to train our units. In the remaining scenarios, we introduce transfer learning method to scale the combat to large scenarios. The object of StarCraft micromanagement is defeating the enemies and increasing the win rates in these given scenarios. For a better comprehension, we analyze the win rates, episode steps and average rewards during training, as well as the learned strategies. Our code and results are open-source at https://github.com/nanxintin/StarCraft-AI.</p>
<h3><em>A. Small Scale Micromanagement</em></h3>
<p>In small scale micromanagement scenarios, we will train Goliaths against enemy units with different amounts and types.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. The win rates of our units in 3 Goliaths vs. 6 Zealots micromanagement scenario from every 200 episodes' training.</p>
<p>In the second scenario, we will also use transfer learning method to train Goliaths based on the well-trained model of the first scenario. Both of the two scenarios are trained with 4000 episodes and over 1 million steps.</p>
<p><em>1) Goliaths vs. Zealots:</em> In this scenario, we train our Goliaths units from scratch and analyze the results.</p>
<ul>
<li><strong>Win Rates:</strong> At first, we will analyze the learning performance of our RL method with moveReward. To evaluate the win rates, we test our model after every 200 episodes' training for 100 combats, and depict the results in Fig. 6. We can see that our Goliaths units can't win any combats before 1400 episodes. With the progress of training, units start to win several games and the curve of win rates has an impressive increase after 2000 episodes. After 3000 episodes' training, our units can reach win rates of 100% at last.</li>
<li><strong>Episode Steps:</strong> We depict the average episode steps and standard deviations of our three Goliaths units during training in Fig. 7. It is apparent to see that the curve of average episode steps has four stages. In the opening, episode steps are extremely few because Goliaths have learned nothing and are destroyed quickly. After that, Goliaths start to realize that the hitpoint damage causes a negative reward. They learn to run away from enemies and the episode steps increase to a high level. And then, episode steps start to decrease because Goliaths learn to attack to get positive rewards, rather than just running</li>
</ul>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. The episode steps of our units in 3 Goliaths vs. 6 Zealots micromanagement scenario during training.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. The average reward of our units in 3 Goliaths vs. 6 Zealots micromanagement scenario during training.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. The win rates of our units in 3 Goliaths vs. 20 Zerglings micromanagement scenario from every 200 episodes' training.</p>
<p>away. In the end, Goliaths have learned an appropriate policy to balance move and attack, and they are able to destroy enemies in almost 300 steps.</p>
<ul>
<li><strong>Average Rewards</strong>: Generally speaking, a powerful game AI in micromanagement scenarios should defeat the enemies as soon as possible. Here we introduce the average rewards, dividing the total rewards by episode steps in the combat. The curve of our Goliaths units' average rewards is depicted in Fig. 8. The average rewards have an obvious increase in the opening, grow steadily during training and stay smooth after almost 3000 episodes.</li>
</ul>
<p>2) <strong>Goliaths vs. Zerglings</strong>: In this scenario, the enemy units are a group of Zerglings, and we reuse the well-trained model from the first scenario to initialize the policy network. In comparison with learning from scratch, we have a better</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. The average episode steps of our units in 3 Goliaths vs. 20 Zerglings micromanagement scenario during training.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11. The average reward of our units in 3 Goliaths vs. 20 Zerglings micromanagement scenario during training.</p>
<p>understanding of transfer learning.</p>
<ul>
<li><strong>Win Rates</strong>: We draw the win rates in Fig. 9. When training from scratch, the learning process is extremely slow and our units can't win a game until 1800 episodes. Without transfer learning, the win rates are below 60% after 4000 episodes. When training based on the model of the first scenario, the learning process is much faster. Even in the opening, our units win several games, and the win rates reach 100% in the end.</li>
<li><strong>Episode Steps</strong>: In Fig. 10, we draw the average episode steps of our three Goliaths during training. Without transfer learning, the curve has the similar trend with that in the first scenario. The average episode steps have a obvious increase in the opening and drop gradually during training. When training with transfer learning, the average episode steps keep stable in the whole training process, within the range of 200 to 400. A possible explanation is that our units have learned some basic move and attack skills from the well-trained model, and they use these skills to speed up the training process.</li>
<li><strong>Average Rewards</strong>: We draw the average rewards of our three Goliaths in Fig. 11. When training from scratch, our units have difficulty in winning the combat in the opening and the average rewards are in a low level before 1000 episodes. The average rewards with transfer learning, by comparison, are much higher from the beginning and behave better in the whole training process.</li>
</ul>
<p>TABLE III
Curriculum design for Marines vs. Zerglings MICROMANAGEMENT. M:MARINE, Z:ZERGLING.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Scenarios</th>
<th style="text-align: center;">Class 1</th>
<th style="text-align: center;">Class 2</th>
<th style="text-align: center;">Class 3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">M10 vs. Z13</td>
<td style="text-align: center;">M5 vs. Z6</td>
<td style="text-align: center;">M8 vs. Z10</td>
<td style="text-align: center;">M8 vs. Z12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">M20 vs. Z30</td>
<td style="text-align: center;">M10 vs. Z12</td>
<td style="text-align: center;">M15 vs. Z20</td>
<td style="text-align: center;">M20 vs. Z25</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>TABLE IV
Performance comparison of our model with baseline methods in two large scale scenarios. M:Marine, Z:Zergling.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Scenarios</th>
<th style="text-align: center;">Weakest</th>
<th style="text-align: center;">Closest</th>
<th style="text-align: center;">GMEZO</th>
<th style="text-align: center;">BicNet</th>
<th style="text-align: center;">PS-MAGDS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">M10 vs. Z13</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">$41 \%$</td>
<td style="text-align: center;">$57 \%$</td>
<td style="text-align: center;">$64 \%$</td>
<td style="text-align: center;">$\mathbf{9 7} \%$</td>
</tr>
<tr>
<td style="text-align: left;">M20 vs. Z30</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$87 \%$</td>
<td style="text-align: center;">$88.2 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0} \%$</td>
<td style="text-align: center;">$92.4 \%$</td>
</tr>
</tbody>
</table>
<h2>B. Large Scale Micromanagement</h2>
<p>In large scale micromanagement scenarios, we use curriculum transfer learning to train our Marines to play against Zerglings, and compare the results with some baseline methods.</p>
<p>1) Marines vs. Zerglings: In this section, we design a curriculum with 3 classes to train the units, as shown in Table III. After training, we test the performance in two target scenarios: M10 vs. Z13 and M20 vs. Z30. In addition, we use some baseline methods as a comparison, which consist of rule-based approaches and DRL approaches.</p>
<ul>
<li>Weakest: A rule-based method, attacking weakest in the fire range.</li>
<li>Closest: A rule-based method, attacking closest in the fire range.</li>
<li>GMEZO: A DRL method, based on the zero-order optimization, having impressive results over traditional RL methods [20].</li>
<li>BicNet: A DRL method, based on the actor-critic architecture, having the best performance in most StarCraft micromanagement scenarios [21].
In Table IV, we present the win rates of the PS-MAGDS method and baseline methods. In each scenario, we measure our model's average win rates in 100 test games for 5 times. In M10 vs. Z13, PS-MAGDS achieves a win rate of $97 \%$, which is much higher than other methods, including the recently proposed GMEZO and BicNet. In M20 vs. Z30, PS-MAGDS has the second best performance, which is very close to the best one.</li>
</ul>
<p>We also test our well-trained models in curricular scenarios and unseen scenarios, and present the results in Table V. We can see that PS-MAGDS has outstanding performances in these curricular scenarios. In unseen scenarios with more units, PS-MAGDS also has acceptable results.</p>
<h2>C. Strategies Analysis</h2>
<p>In StarCraft micromanagement, there are different types of units with different skills and properties. Players need to learn how to move and attack with a group of units in real time. If we design a rule-based AI to solve this problem, we have to consider a large amount of conditions, and agent's ability is also limited. Beginners of StarCraft could not win any of these</p>
<p>TABLE V
Win rates in various curricular scenarios and unseen SCENARIOS. M:MARINE, Z:ZERGLING.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Well-trained Scenarios</th>
<th style="text-align: center;">Test Scenarios</th>
<th style="text-align: center;">Win rates</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">M10 vs. Z13</td>
<td style="text-align: center;">M5 vs. Z6</td>
<td style="text-align: center;">$80.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">M8 vs. Z10</td>
<td style="text-align: center;">$95 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">M8 vs. Z12</td>
<td style="text-align: center;">$85 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">M10 vs. Z15</td>
<td style="text-align: center;">$81 \%$</td>
</tr>
<tr>
<td style="text-align: left;">M20 vs. Z30</td>
<td style="text-align: center;">M10 vs. Z12</td>
<td style="text-align: center;">$99.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">M15 vs. Z20</td>
<td style="text-align: center;">$98.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">M20 vs. Z25</td>
<td style="text-align: center;">$99.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">M40 vs. Z60</td>
<td style="text-align: center;">$80.5 \%$</td>
</tr>
</tbody>
</table>
<p>combats presented in our paper. So these behaviors are highly complex and difficult to learn. With reinforcement learning and curriculum transfer learning, our units are able to master several useful strategies in these scenarios. In this section, we will make a brief analysis on these strategies that our units have learned.</p>
<p>1) Disperse Enemies: In small scale micromanagement scenarios, our Goliaths units have to fight against the opponent with a larger amount and more total hitpoints. If our units stay together and fight against a group of units face-to-face, they will be destroyed quickly and lose the combat. The appropriate strategy is dispersing enemies, and destroying them one by one.</p>
<p>In the first scenario, our Goliaths units have learned dispersing Zealots after training. In the opening, our units disperse enemies into several parts and destroy it in one part first. After that, the winning Goliath moves to other Goliaths and helps to fight against the enemies. Finally, our units focus fire on the remaining enemies and destroy them. For a better understanding, we choose some frames of game replay in the combat and draw units' move and attack directions in Fig. 12. The white lines stand for the move directions and the red lines stand for the attack directions.</p>
<p>The similar strategy occurs in the second scenario. The opponent has much more units, and Zerglings Rush has great damage power, which is frequently used in StarCraft games. Our Goliaths units disperse Zerglings into several groups and keep a suitable distance with them. When units' weapons are in a valid cooldown state, they stop moving and attack the enemies, as shown in Fig. 13.
2) Keep the Team: In large scale micromanagement scenarios, each side has a mass of units. Marines are small size ground units with low hitpoints. If they combat in several small groups, they are unable to resist the enemies. A suitable strategy is keeping our Marines in a team, moving with the same direction and attacking the same target, as demonstrated in Fig. 14. From these figures, we can see that our Marines have learned to move forward and retreat in a queue.
3) Hit and Run: Apart from the global strategies discussed above, our units have also learned some local strategies during training. Among them, Hit and Run is the most widely used tactic in StarCraft micromanagement. Our units rapidly learn the Hit and Run tactic in all scenarios, including the single</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12. The sample game replay in 3 Goliaths vs. 6 Zealots micromanagement scenario.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13. The sample game replay in 3 Goliaths vs. 20 Zerglings micromanagement scenario.</p>
<p>unit's Hit and Run in Fig. 12 and Fig. 13, and a group of units' Hit and Run in Fig. 14.</p>
<p>4) <strong>Existing Problems:</strong> Although our units have learned useful strategies after training, there are still some problems in combats. For instance, Goliaths move forward and backward now and then and don't join in combats to help other units in time. In addition, units prefer moving to the boundary of the map, so as to avoid the enemies.</p>
<h2>VII. CONCLUSION AND FUTURE WORK</h2>
<p>This paper focuses on the multiple units control in StarCraft micromanagement scenarios. We present several contributions, including an efficient state representation, the parameter sharing multi-agent gradient-descent Sarsa(λ), the effective reward function and the curriculum transfer learning method used to extend our model to various scenarios. We demonstrate the effectiveness of our approach in both small scale and large scale scenarios, and the superior performance over some baseline methods in two target scenarios. It is remarkable that our proposed method is able to learn appropriate strategies and defeat the built-in AI in various scenarios.</p>
<p>In addition, there are still some areas for future work. The cooperative behaviors of multiple units are learned by sharing the policy network, constructing an efficient state representation method including other units' information and the proposed intrinsic motivated reward function. Although</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14. The sample game replay in 20 Marines vs. 30 Zerglings micromanagement scenario.</p>
<p>our units can successfully master some effective coordination strategies, we will explore more intelligent methods for multi-agent collaboration. To solve the delayed reward problem in StarCraft micromanagement, we use a simple, straight and efficient reward shaping method. Nevertheless, there are also some other methods solving the sparse and delayed rewards, such as hierarchical reinforcement learning. Hierarchical RL integrates hierarchical action-value functions, operating at different temporal scales [63]. Compared with the reward shaping method, hierarchical RL has the capacity to learn temporally-abstracted exploration, and gives agents more flexibility. But its framework is also much more complicated, and automatically subgoals extraction is still an open problem. In the future, we will make an in-depth study on applying hierarchical RL to StarCraft. At present, we can only train ranged ground units with the same type, while training melee ground units using RL methods is still an open problem. We will improve our method for more types of units and more complex scenarios in the future. Finally, we will also consider to use our micromanagement model in the StarCraft bot to play full the game.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>We would like to thank Qichao Zhang, Yaran Chen, Dong Li, Zhentao Tang and Nannan Li for the helpful comments and discussions about this work and paper writing. We also thank the BWAPI and StarCraft group for their meaningful work.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, d. D. G. Van, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, and M. Lanctot, "Mastering the game of Go with deep neural networks and tree search," <em>Nature</em>, vol. 529, no. 7587, pp. 484–489, 2016.</li>
<li>[2] D. Silver, J. Schrittwieser, K. Simonyan, and et al, "Mastering the game of Go without human knowledge," <em>Nature</em>, vol. 550, no. 7676, pp. 354–359, 2017.</li>
</ul>
<p>[3] D. Zhao, Z. Zhang, and Y. Dai, "Self-teaching adaptive dynamic programming for Gomoku," Neurocomputing, vol. 78, no. 1, pp. 2329, 2012.
[4] K. Shao, D. Zhao, Z. Tang, and Y. Zhu, "Move prediction in Gomoku using deep learning," in Youth Academic Annual Conference of Chinese Association of Automation, 2016, pp. 292-297.
[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski, "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, 2015.
[6] D. Zhao, H. Wang, K. Shao, and Y. Zhu, "Deep reinforcement learning with experience replay based on SARSA," in IEEE Symposium Series on Computational Intelligence, 2017, pp. 1-6.
[7] M. Moravik, M. Schmid, N. Burch, V. Lisy, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, and M. Bowling, "Deepstack: Expertlevel artificial intelligence in heads-up no-limit poker," Science, vol. 356, no. 6337, pp. 508-513, 2017.
[8] S. Ontanon, G. Synnaeve, A. Uriarte, F. Richoux, D. Churchill, and M. Preuss, "A survey of real-time strategy game AI research and competition in StarCraft," IEEE Transactions on Computational Intelligence and AI in Games, vol. 5, no. 4, pp. 293-311, 2013.
[9] R. Lara-Cabrera, C. Cotta, and A. J. Fernndez-Leiva, "A review of computational intelligence in RTS games," in IEEE Symposium on Foundations of Computational Intelligence, 2013, pp. 114-121.
[10] G. Robertson and I. Watson, "A review of real-time strategy game AI," AI Magazine, vol. 35, no. 4, pp. 75-104, 2014.
[11] K. Shao, Y. Zhu, and D. Zhao, "Cooperative reinforcement learning for multiple units combat in StarCraft," in IEEE Symposium Series on Computational Intelligence, 2017, pp. 1-6.
[12] J. Hagelback, "Hybrid pathfinding in StarCraft," IEEE Transactions on Computational Intelligence and AI in Games, vol. 8, no. 4, pp. 319-324, 2016.
[13] A. Uriarte and S. Ontan?n, "Kiting in RTS games using influence maps," in Artificial Intelligence and Interactive Digital Entertainment Conference, 2012, pp. 31-36.
[14] G. Synnaeve and P. Bessire, "Multiscale bayesian modeling for RTS games: An application to StarCraft AI," IEEE Transactions on Computational Intelligence and AI in Games, vol. 8, no. 4, pp. 338-350, 2016.
[15] D. Churchill and B. Michael, "Incorporating search algorithms into RTS game agents," in Artificial Intelligence and Interactive Digital Entertainment Conference, 2012, pp. 2-7.
[16] I. Gabriel, V. Negru, and D. Zaharie, "Neuroevolution based multi-agent system for micromanagement in real-time strategy games," in Balkan Conference in Informatics, 2012, pp. 32-39.
[17] A. Shantia, E. Begue, and M. Wiering, "Connectionist reinforcement learning for intelligent unit micro management in StarCraft," in International Joint Conference on Neural Networks, 2011, pp. 1794-1801.
[18] S. Wender and I. Watson, "Applying reinforcement learning to small scale combat in the real-time strategy game StarCraft:Broodwar," in IEEE Conference on Computational Intelligence and Games, 2012, pp. $402-408$.
[19] Y. Lecun, Y. Bengio, and G. Hinton, "Deep learning," Nature, vol. 521, no. 7553, pp. 436-444, 2015.
[20] N. Usunier, G. Synnaeve, Z. Lin, and S. Chintala, "Episodic exploration for deep deterministic policies: An application to StarCraft micromanagement tasks," in International Conference on Learning Representations, 2017.
[21] P. Peng, Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, and J. Wang, "Multiagent bidirectionally-coordinated nets for learning to play StarCraft combat games," arXiv preprint arXiv:1703.10069, 2017.
[22] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, "Counterfactual multi-agent policy gradients," in The 32nd AAAI Conference on Artificial Intelligence, 2018.
[23] R. Lowe, W. Yi, and T. Aviv, "Multi-agent actor-critic for mixed cooperative-competitive environments," in Advances in Neural Information Processing Systems, 2017, pp. 6382-6393.
[24] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT Press, 1998.
[25] L. P. Kaelbling, M. L. Littman, and A. W. Moore, "Reinforcement learning: A survey," Journal of Artificial Intelligence Research, vol. 4, no. 1, pp. 237-285, 1996.
[26] G. Tesauro, "TD-Gammon, a self-teaching backgammon program, achieves master-level play," Neural Computation, vol. 6, no. 2, pp. 215219, 1994.
[27] Q. Zhang, D. Zhao, and W. Ding, "Event-based robust control for uncertain nonlinear systems using adaptive dynamic programming," IEEE Transactions on Neural Networks and Nearning Systems, vol. 29, no. 1, pp. 37-50, 2018.
[28] Q. Zhang, D. Zhao, and Y. Zhu, "Event-triggered $H_{\infty}$ control for continuous-time nonlinear system via concurrent learning," IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 47, no. 7, pp. 1071-1081, 2017.
[29] Y. Zhu, D. Zhao, and X. Li, "Iterative adaptive dynamic programming for solving unknown nonlinear zero-sum game based on online data," IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 3, pp. 714-725, 2017.
[30] D. Zhao, K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou, and C. Wang, "Review of deep reinforcement learning and discussions on the development of computer Go," Control Theory and Applications, vol. 33, no. 6, pp. 701-717, 2016.
[31] Z. Tang, K. Shao, D. Zhao, and Y. Zhu, "Recent progress of deep reinforcement learning: from AlphaGo to AlphaGo Zero," Control Theory and Applications, vol. 34, no. 12, pp. 1529-1546, 2017.
[32] H. Van Hasselt, A. Guez, and D. Silver, "Deep reinforcement learning with double Q-learning," in The Thirtieth AAAI Conference on Artificial Intelligence, 2016, pp. 2094-2100.
[33] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, "Prioritized experience replay," in International Conference on Learning Representations, 2016.
[34] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas, "Dueling network architectures for deep reinforcement learning," in International Conference on Machine Learning, 2016, pp. 1995-2003.
[35] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. De Maria, V. Pannocrohelvam, M. Suleyman, C. Beattie, S. Petersen et al., "Massively parallel methods for deep reinforcement learning," in International Conference on Machine Learning, 2015.
[36] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap, D. Silver, and K. Kavukcuoglu, "Asynchronous methods for deep reinforcement learning," in International Conference on Machine Learning, 2016, pp. 1928-1937.
[37] D. Li, D. Zhao, Q. Zhang, and C. Luo, "Policy gradient methods with gaussian process modelling acceleration," in International Joint Conference on Neural Networks, 2017, pp. 1774-1779.
[38] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. P. Wierstra, "Continuous control with deep reinforcement learning," in International Conference on Learning Representations, 2016.
[39] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz, "Trust region policy optimization," in International Conference on Machine Learning, 2015, pp. 1889-1897.
[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint, 2017.
[41] S. Levine and V. Koltun, "Guided policy search," in International Conference on Machine Learning, 2013, pp. 1-9.
[42] S. Gu, T. P. Lillicrap, I. Sutskever, and S. Levine, "Continuous deep Q-learning with model-based acceleration," in International Conference on Machine Learning, 2016, pp. 2829-2838.
[43] M. Watter, J. T. Springenberg, J. Boedecker, and M. A. Riedmiller, "Embed to control: a locally linear latent dynamics model for control from raw images," in Neural Information Processing Systems, 2015, pp. $2746-2754$.
[44] M. L. Littman, "Markov games as a framework for multi-agent reinforcement learning," Machine Learning Proceedings, pp. 157-163, 1994.
[45] T. Ming, "Multi-agent reinforcement learning: Independent vs. cooperative agents," in Proceedings of the tenth International Conference on Machine Learning, 1993, pp. 330-337.
[46] Z. Zhang, D. Zhao, J. Gao, D. Wang, and Y. Dai, "FMRQ a multiagent reinforcement learning algorithm for fully cooperative tasks," IEEE Transactions on Cybernetics, vol. 47, no. 6, pp. 1367-1379, 2017.
[47] S. Sukhbaatar, A. Szlam, and R. Fergus, "Learning multiagent communication with backpropagation," in Neural Information Processing Systems, 2016, pp. 2244-2252.
[48] L. Marc, Z. Vinicius, and G. Audrunas, "A unified game-theoretic approach to multiagent reinforcement learning," arXiv preprint arXiv:1711.00832.
[49] K. G. Jayesh, E. Maxim, and K. Mykel, "Cooperative multi-agent control using deep reinforcement learning," in International Conference on Autonomous Agents and Multiagent Systems, 2017, pp. 66-83.
[50] S. J. Pan and Q. Yang, "A survey on transfer learning," IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 13451359, 2010.</p>
<p>[51] A. Gupta, Y.-S. Ong, and L. Feng, "Insights on transfer optimization: Because experience is the best teacher," IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 2, no. 1, pp. 51-64, 2018.
[52] M. E. Taylor and P. Stone, "Transfer learning for reinforcement learning domains: A survey," vol. 10, pp. 1633-1685, 2009.
[53] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in Proceedings of the 26th Annual International Conference on Machine Learning, 2009, pp. 41-48.
[54] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabskabarwiska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, and J. Agapiou, "Hybrid computing using a neural network with dynamic external memory." Nature, vol. 538, no. 7626, p. 471, 2016.
[55] Y. Wu and Y. Tian, "Training agent for first-person shooter game with actor-critic curriculum learning," in International Conference on Learning Representations, 2017.
[56] Q. Dong, S. Gong, and X. Zhu, "Multi-task curriculum transfer deep learning of clothing attributes," in IEEE Winter Conference on Applications of Computer Vision, 2017, pp. 520-529.
[57] J. X. Wang, Z. Kurthnelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, "Learning to reinforcement learn," in International Conference on Learning Representations, 2017.
[58] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, and K. Kavukcuoglu, "Learning to navigate in complex environments," in International Conference on Learning Representations, 2017.
[59] X. Glorot, A. Bordes, Y. Bengio, X. Glorot, A. Bordes, and Y. Bengio, "Deep sparse rectifier neural networks," in International Conference on Artificial Intelligence and Statistics, 2011, pp. 315-323.
[60] V. Nair and G. E. Hinton, "Rectified linear units improve restricted boltzmann machines," in International Conference on Machine Learning, 2010, pp. 807-814.
[61] S. P. Singh and R. S. Sutton, "Reinforcement learning with replacing eligibility traces," Machine Learning, vol. 22, no. 1, pp. 123-158, 1996.
[62] A. Y. Ng, D. Harada, and S. J. Russell, "Policy invariance under reward transformations: Theory and application to reward shaping," in International Conference on Machine Learning, 1999, pp. 278-287.
[63] T. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation," in Advances in neural information processing systems, 2016, pp. 3675-3683.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Kun Shao received the B.S. degree in automation from Beijing Jiaotong University, Beijing, China, in 2014. He is currently pursuing the Ph.D. degree in control theory and control engineering with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China. His current research interests include reinforcement learning, deep learning and game AI.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Yuanheng Zhu received the B.S. degree from Nanjing University, Nanjing, China, in 2010, and the Ph.D. degree with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China, in 2015. He is currently an Associate Professor with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences. His research interests include optimal control, adaptive dynamic programming and reinforcement learning.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Dongbin Zhao (M’06-SM’10) received the B.S., M.S., Ph.D. degrees from Harbin Institute of Technology, Harbin, China, in 1994, 1996, and 2000 respectively. He was a postdoctoral fellow at Tsinghua University, Beijing, China, from 2000 to 2002. He has been a professor at Institute of Automation, Chinese Academy of Sciences since 2012, and also a professor with the University of Chinese Academy of Sciences, China. From 2007 to 2008, he was also a visiting scholar at the University of Arizona. He has published 4 books, and over 60 international journal papers. His current research interests are in the area of computational intelligence, adaptive dynamic programming, deep reinforcement learning, robotics, intelligent transportation systems, and smart grids.</p>
<p>Dr. Zhao is the Associate Editor of IEEE Transactions on Neural Networks and Learning Systems (2012-), IEEE Computation Intelligence Magazine (2014-), etc. He is the Chair of Beijing Chapter, and was the Chair of Adaptive Dynamic Programming and Reinforcement Learning Technical Committee (2015-2016), Multimedia Subcommittee (2015-2016) of IEEE Computational Intelligence Society (CIS). He works as several guest editors of renowned international journals. He is involved in organizing several international conferences.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>K. Shao, Y. Zhu and D. Zhao are with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences. Beijing 100190, China. They are also with the University of Chinese Academy of Sciences, Beijing, China (e-mail: shaokun2014@ia.ac.cn; yuanheng.zhu@ia.ac.cn; dongbin.zhao@ia.ac.cn).</p>
<p>This work is supported by National Natural Science Foundation of China (NSFC) under Grants No. 61573353 , No. 61603382 and No. 61533017.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ http://bwapi.github.io/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
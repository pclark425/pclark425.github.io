<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-832 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-832</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-832</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-268249086</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.03031v4.pdf" target="_blank">Learning to Use Tools via Cooperative and Interactive Agents</a></p>
                <p><strong>Paper Abstract:</strong> Tool learning empowers large language models (LLMs) as agents to use external tools and extend their utility. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating execution results into the next action prediction. Despite their progress, these methods suffer from performance degradation when addressing practical tasks due to: (1) the pre-defined pipeline with restricted flexibility to calibrate incorrect actions, and (2) the struggle to adapt a general LLM-based agent to perform a variety of specialized actions. To mitigate these problems, we propose ConAgents, a Cooperative and interactive Agents framework, which coordinates three specialized agents for tool selection, tool execution, and action calibration separately. ConAgents introduces two communication protocols to enable the flexible cooperation of agents. To effectively generalize the ConAgents into open-source models, we also propose specialized action distillation, enhancing their ability to perform specialized actions in our framework. Our extensive experiments on three datasets show that the LLMs, when equipped with the ConAgents, outperform baselines with substantial improvement (i.e., up to 14% higher success rate).</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e832.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e832.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cooperative and interactive Agents framework (ConAgents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multi-agent framework that decomposes tool-use into three specialized agents (Grounding, Execution, Review) and coordinates them via two communication protocols (automatic and adaptive) to improve multi-step tool use and error calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ConAgents (Grounding / Execution / Review agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three specialized agents: Grounding (plans/selects tools), Execution (generates executable code to call tools using tool docs), Review (provides feedback to calibrate planning/execution). Coordination via automatic (planning-review and execution-review alternation) and adaptive (review triggered on failures) protocols. Implemented with system prompts for closed models and as LoRA-finetuned separate student models for open-source backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>RestBench (TMDB), RestBench (Spotify), ToolBench (sampled complex tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step procedural tool-based task (planning + tool execution + calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>With gpt-3.5-turbo backbone (Auto protocol): Success Rate = 79.0% (RestBench-TMDB), 71.21% (RestBench-Spotify), 79.17% (ToolBench). Improves over many single-agent baselines (examples below).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Modularized agents (grounding/planning, execution/code generation, review/feedback), communication protocols (automatic/adaptive), ability to alternate planning/execution with review, LoRA-based separate student agents for distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>For closed-model experiments: prompting/in-context use of gpt-3.5-turbo (and GPT-4 for trajectory synthesis). For open-source adaptation: specialized action distillation (SPAN) using GPT-4-generated trajectories and LoRA parameter-efficient fine-tuning of separate student agents.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change + training method</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Architectural: split single-agent tool-use into three specialized cooperating agents with explicit review channel and two communication protocols (automatic alternating planning-review/execution-review; adaptive review on failures). Training: distill trajectories from a powerful teacher (GPT-4) into separate student agents (SPAN) and fine-tune using LoRA to make open-source models better at the modular agent flow.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large improvement over strong baselines — e.g., on RestBench-TMDB ConAgents (gpt-3.5-turbo, Auto) achieves 79.0% success vs RestGPT 65.0% (∼14 pp improvement). When adapting to an open-source 8B backbone (Mixtral/Mistral-8x7B), SPAN distillation improved Auto prompting success from 49.0% → 53.0% (TMDB), and Ada from 47.0% → 51.0% (TMDB).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors attribute poor interactive/procedural performance of single-agent approaches and smaller models to (1) rigid pre-defined pipelines that lack flexibility to calibrate incorrect actions, (2) difficulty for a single LLM to learn varied specialized actions (planning, execution, reflection), and (3) limited working memory/context that hurts multi-step tool-use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Use Tools via Cooperative and Interactive Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e832.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e832.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specialized Action Distillation (SPAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distillation pipeline that collects high-quality multi-agent task-solving trajectories from a strong teacher (GPT-4) under ConAgents and separately distills grounding, execution, and review behaviors into separate student models using LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>SPAN (Specialized Action Distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training procedure: sample diverse tasks, run ConAgents with GPT-4 to synthesize multi-agent trajectories, reorganize trajectories into agent-specific actions, and fine-tune separate student models (one per agent role) via LoRA on these agent-specific objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>student models typically initialized from Mistral/Mixtral 8x7B in experiments (8B each)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>RestBench (TMDB/Spotify) and ToolBench (sampled complex tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step procedural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Improves open-source backbone performance: Mixtral-8x7B Auto prompting improved from 49.0% → 53.0% (TMDB) after SPAN; Ada prompting from 47.0% → 51.0% (TMDB). Similar modest improvements on Spotify.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Not an architecture per se—creates separate student models (grounding/execution/review) to mirror ConAgents modularization.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Distillation of GPT-4-generated trajectories + parameter-efficient fine-tuning (LoRA) on agent-specific language modeling objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (distillation) / parameter-efficient fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Generate task-solving trajectories from GPT-4 within ConAgents (automatic protocol), split trajectories per agent role, and LoRA-finetune separate student models to imitate the role-specific behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Quantified boosts for Mixtral-8x7B: Auto: 49.0% → 53.0% (TMDB); Ada: 47.0% → 51.0% (TMDB). Authors report that distillation 'further improves overall performance substantially' for open-source backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Open-source models initially underperform in modularized multi-agent workflows due to limited capability to perform role-specialized behaviors; distillation transfers multi-agent behavioral structure from a strong teacher to the weaker student models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Use Tools via Cooperative and Interactive Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e832.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e832.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source conversational LLM used as the backbone for each agent in most experiments; used via prompting with system messages and deterministic decoding (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>gpt-3.5-turbo (as agent backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI conversational LLM used via prompting; agents implemented by giving different system prompts (for Grounding/Execution/Review). Deterministic decoding (temp=0) for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>RestBench (TMDB/Spotify), ToolBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step procedural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>When used as ConAgents backbone (Auto): Success Rate = 79.0% (TMDB), 71.21% (Spotify), 79.17% (ToolBench). As baseline ReAct (single-agent) with same backbone: Success Rate ~40.0% (TMDB) indicating large gains from ConAgents architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Used as a prompted backbone for modular agents; no internal architectural modification reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Prompting / system-prompted agent specialization; no additional fine-tuning for closed-model experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Use Tools via Cooperative and Interactive Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e832.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e832.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral / Mixtral 8x7B (open-source 8B LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter open-source LLM used as a backbone for ConAgents and baselines to evaluate open-source model performance and the effect of specialized action distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Mistral-8x7B / Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 8B-parameter LLM used either via prompting (vanilla) or as initialization for LoRA-finetuned student agents (after SPAN).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>RestBench (TMDB/Spotify), ToolBench (sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step procedural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Vanilla ConAgents prompting (Auto): Success Rate = 49.0% (TMDB), 34.21% (Spotify). After SPAN distillation (Auto): Success Rate = 53.0% (TMDB), 36.09% (Spotify). Baseline single-agent ReAct with same backbone: 26.0% (TMDB).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Vanilla prompting; intervention: LoRA fine-tuning via SPAN distillation into separate agents.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method (distillation / LoRA fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Initialize three student models from Mistral-8x7B and LoRA-fine-tune them separately to imitate GPT-4-generated ConAgents trajectories (grounding/execution/review).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Improved success on TMDB from 49.0% → 53.0% (Auto) and 47.0% → 51.0% (Ada) after SPAN distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Limited model capacity and weaker ability to learn role-specialized behaviors under modular multi-agent workflows make open-source LLMs perform worse; targeted distillation partially closes this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Use Tools via Cooperative and Interactive Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e832.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e832.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Synergizing reasoning and acting in language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent prompting method that interleaves chain-of-thought reasoning and external actions; used as a competitive baseline for tool-use tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ReAct (single-agent chain-of-thought + actions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting strategy that asks a model to alternate reasoning traces (chain-of-thought) with action emissions (tool calls) in a single agent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>RestBench (TMDB/Spotify), ToolBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / multi-step procedural tasks</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>With gpt-3.5-turbo backbone: Success Rate ≈ 40.0% (TMDB) in Table 2. With Mixtral-8x7B backbone: 26.0% (TMDB) in Table 3. Lower than ConAgents by a substantial margin on these interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>single-agent chain-of-thought + action interleaving</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Single-agent approach places burden on one model to perform planning, execution, and calibration, reducing flexibility to correct errors and leading to lower success in procedural tool-use compared to specialized cooperating agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Use Tools via Cooperative and Interactive Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e832.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e832.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolLLM (DFSDT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-use baseline that augments LLMs with a Depth First Search-based Decision Tree to select tools and execute multi-step tool sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolLLM (DFSDT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that guides tool selection using a DFSDT (search/planning) on top of LLM capabilities; used as a strong baseline for tool-use tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>RestBench (TMDB/Spotify), ToolBench</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use / planning + execution</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>With gpt-3.5-turbo backbone: Success Rate ≈ 68.0% (TMDB) in Table 2. With Mixtral-8x7B baseline: ToolLLM = 37.0% (TMDB) in Table 3. ConAgents outperforms ToolLLM in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>DFSDT search/decision-tree aided tool selection layered on LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>combines prompting with algorithmic search-based decision making</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Not directly discussed, but authors contrast static/pre-defined pipelines like DFSDT-based flows with ConAgents' dynamic review-enabled cooperation which better handles execution errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to Use Tools via Cooperative and Interactive Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ToolLLM: Facilitating large language models to master 16000+ real-world apis <em>(Rating: 2)</em></li>
                <li>Tool learning with foundation models <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>RestGPT: Connecting large language models with real-world applications via restful apis <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-832",
    "paper_id": "paper-268249086",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ConAgents",
            "name_full": "Cooperative and interactive Agents framework (ConAgents)",
            "brief_description": "A modular multi-agent framework that decomposes tool-use into three specialized agents (Grounding, Execution, Review) and coordinates them via two communication protocols (automatic and adaptive) to improve multi-step tool use and error calibration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ConAgents (Grounding / Execution / Review agents)",
            "model_description": "Three specialized agents: Grounding (plans/selects tools), Execution (generates executable code to call tools using tool docs), Review (provides feedback to calibrate planning/execution). Coordination via automatic (planning-review and execution-review alternation) and adaptive (review triggered on failures) protocols. Implemented with system prompts for closed models and as LoRA-finetuned separate student models for open-source backbones.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "RestBench (TMDB), RestBench (Spotify), ToolBench (sampled complex tasks)",
            "interactive_task_type": "tool use / multi-step procedural tool-based task (planning + tool execution + calibration)",
            "interactive_performance": "With gpt-3.5-turbo backbone (Auto protocol): Success Rate = 79.0% (RestBench-TMDB), 71.21% (RestBench-Spotify), 79.17% (ToolBench). Improves over many single-agent baselines (examples below).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Modularized agents (grounding/planning, execution/code generation, review/feedback), communication protocols (automatic/adaptive), ability to alternate planning/execution with review, LoRA-based separate student agents for distillation.",
            "training_method": "For closed-model experiments: prompting/in-context use of gpt-3.5-turbo (and GPT-4 for trajectory synthesis). For open-source adaptation: specialized action distillation (SPAN) using GPT-4-generated trajectories and LoRA parameter-efficient fine-tuning of separate student agents.",
            "intervention_type": "architectural change + training method",
            "intervention_description": "Architectural: split single-agent tool-use into three specialized cooperating agents with explicit review channel and two communication protocols (automatic alternating planning-review/execution-review; adaptive review on failures). Training: distill trajectories from a powerful teacher (GPT-4) into separate student agents (SPAN) and fine-tune using LoRA to make open-source models better at the modular agent flow.",
            "intervention_effect": "Large improvement over strong baselines — e.g., on RestBench-TMDB ConAgents (gpt-3.5-turbo, Auto) achieves 79.0% success vs RestGPT 65.0% (∼14 pp improvement). When adapting to an open-source 8B backbone (Mixtral/Mistral-8x7B), SPAN distillation improved Auto prompting success from 49.0% → 53.0% (TMDB), and Ada from 47.0% → 51.0% (TMDB).",
            "hypothesized_cause_of_gap": "Authors attribute poor interactive/procedural performance of single-agent approaches and smaller models to (1) rigid pre-defined pipelines that lack flexibility to calibrate incorrect actions, (2) difficulty for a single LLM to learn varied specialized actions (planning, execution, reflection), and (3) limited working memory/context that hurts multi-step tool-use.",
            "uuid": "e832.0",
            "source_info": {
                "paper_title": "Learning to Use Tools via Cooperative and Interactive Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "SPAN",
            "name_full": "Specialized Action Distillation (SPAN)",
            "brief_description": "A distillation pipeline that collects high-quality multi-agent task-solving trajectories from a strong teacher (GPT-4) under ConAgents and separately distills grounding, execution, and review behaviors into separate student models using LoRA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "SPAN (Specialized Action Distillation)",
            "model_description": "Training procedure: sample diverse tasks, run ConAgents with GPT-4 to synthesize multi-agent trajectories, reorganize trajectories into agent-specific actions, and fine-tune separate student models (one per agent role) via LoRA on these agent-specific objectives.",
            "model_size": "student models typically initialized from Mistral/Mixtral 8x7B in experiments (8B each)",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "RestBench (TMDB/Spotify) and ToolBench (sampled complex tasks)",
            "interactive_task_type": "tool use / multi-step procedural tasks",
            "interactive_performance": "Improves open-source backbone performance: Mixtral-8x7B Auto prompting improved from 49.0% → 53.0% (TMDB) after SPAN; Ada prompting from 47.0% → 51.0% (TMDB). Similar modest improvements on Spotify.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Not an architecture per se—creates separate student models (grounding/execution/review) to mirror ConAgents modularization.",
            "training_method": "Distillation of GPT-4-generated trajectories + parameter-efficient fine-tuning (LoRA) on agent-specific language modeling objectives.",
            "intervention_type": "training method (distillation) / parameter-efficient fine-tuning",
            "intervention_description": "Generate task-solving trajectories from GPT-4 within ConAgents (automatic protocol), split trajectories per agent role, and LoRA-finetune separate student models to imitate the role-specific behavior.",
            "intervention_effect": "Quantified boosts for Mixtral-8x7B: Auto: 49.0% → 53.0% (TMDB); Ada: 47.0% → 51.0% (TMDB). Authors report that distillation 'further improves overall performance substantially' for open-source backbones.",
            "hypothesized_cause_of_gap": "Open-source models initially underperform in modularized multi-agent workflows due to limited capability to perform role-specialized behaviors; distillation transfers multi-agent behavioral structure from a strong teacher to the weaker student models.",
            "uuid": "e832.1",
            "source_info": {
                "paper_title": "Learning to Use Tools via Cooperative and Interactive Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "gpt-3.5-turbo (OpenAI)",
            "brief_description": "A closed-source conversational LLM used as the backbone for each agent in most experiments; used via prompting with system messages and deterministic decoding (temperature=0).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "gpt-3.5-turbo (as agent backbone)",
            "model_description": "Closed-source OpenAI conversational LLM used via prompting; agents implemented by giving different system prompts (for Grounding/Execution/Review). Deterministic decoding (temp=0) for experiments.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "RestBench (TMDB/Spotify), ToolBench",
            "interactive_task_type": "tool use / multi-step procedural tasks",
            "interactive_performance": "When used as ConAgents backbone (Auto): Success Rate = 79.0% (TMDB), 71.21% (Spotify), 79.17% (ToolBench). As baseline ReAct (single-agent) with same backbone: Success Rate ~40.0% (TMDB) indicating large gains from ConAgents architecture.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Used as a prompted backbone for modular agents; no internal architectural modification reported.",
            "training_method": "Prompting / system-prompted agent specialization; no additional fine-tuning for closed-model experiments reported.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e832.2",
            "source_info": {
                "paper_title": "Learning to Use Tools via Cooperative and Interactive Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mistral / Mixtral 8x7B (open-source 8B LLM)",
            "brief_description": "An 8B-parameter open-source LLM used as a backbone for ConAgents and baselines to evaluate open-source model performance and the effect of specialized action distillation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Mistral-8x7B / Mixtral-8x7B",
            "model_description": "Open-source 8B-parameter LLM used either via prompting (vanilla) or as initialization for LoRA-finetuned student agents (after SPAN).",
            "model_size": "8B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "RestBench (TMDB/Spotify), ToolBench (sampled)",
            "interactive_task_type": "tool use / multi-step procedural tasks",
            "interactive_performance": "Vanilla ConAgents prompting (Auto): Success Rate = 49.0% (TMDB), 34.21% (Spotify). After SPAN distillation (Auto): Success Rate = 53.0% (TMDB), 36.09% (Spotify). Baseline single-agent ReAct with same backbone: 26.0% (TMDB).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "Vanilla prompting; intervention: LoRA fine-tuning via SPAN distillation into separate agents.",
            "intervention_type": "training method (distillation / LoRA fine-tuning)",
            "intervention_description": "Initialize three student models from Mistral-8x7B and LoRA-fine-tune them separately to imitate GPT-4-generated ConAgents trajectories (grounding/execution/review).",
            "intervention_effect": "Improved success on TMDB from 49.0% → 53.0% (Auto) and 47.0% → 51.0% (Ada) after SPAN distillation.",
            "hypothesized_cause_of_gap": "Limited model capacity and weaker ability to learn role-specialized behaviors under modular multi-agent workflows make open-source LLMs perform worse; targeted distillation partially closes this gap.",
            "uuid": "e832.3",
            "source_info": {
                "paper_title": "Learning to Use Tools via Cooperative and Interactive Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Synergizing reasoning and acting in language models)",
            "brief_description": "A single-agent prompting method that interleaves chain-of-thought reasoning and external actions; used as a competitive baseline for tool-use tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ReAct (single-agent chain-of-thought + actions)",
            "model_description": "Prompting strategy that asks a model to alternate reasoning traces (chain-of-thought) with action emissions (tool calls) in a single agent.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "RestBench (TMDB/Spotify), ToolBench",
            "interactive_task_type": "tool use / multi-step procedural tasks",
            "interactive_performance": "With gpt-3.5-turbo backbone: Success Rate ≈ 40.0% (TMDB) in Table 2. With Mixtral-8x7B backbone: 26.0% (TMDB) in Table 3. Lower than ConAgents by a substantial margin on these interactive tasks.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "single-agent chain-of-thought + action interleaving",
            "training_method": "prompting / in-context",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Single-agent approach places burden on one model to perform planning, execution, and calibration, reducing flexibility to correct errors and leading to lower success in procedural tool-use compared to specialized cooperating agents.",
            "uuid": "e832.4",
            "source_info": {
                "paper_title": "Learning to Use Tools via Cooperative and Interactive Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ToolLLM",
            "name_full": "ToolLLM (DFSDT)",
            "brief_description": "A tool-use baseline that augments LLMs with a Depth First Search-based Decision Tree to select tools and execute multi-step tool sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolLLM (DFSDT)",
            "model_description": "Method that guides tool selection using a DFSDT (search/planning) on top of LLM capabilities; used as a strong baseline for tool-use tasks.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "RestBench (TMDB/Spotify), ToolBench",
            "interactive_task_type": "tool use / planning + execution",
            "interactive_performance": "With gpt-3.5-turbo backbone: Success Rate ≈ 68.0% (TMDB) in Table 2. With Mixtral-8x7B baseline: ToolLLM = 37.0% (TMDB) in Table 3. ConAgents outperforms ToolLLM in reported experiments.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "DFSDT search/decision-tree aided tool selection layered on LLM outputs",
            "training_method": "combines prompting with algorithmic search-based decision making",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Not directly discussed, but authors contrast static/pre-defined pipelines like DFSDT-based flows with ConAgents' dynamic review-enabled cooperation which better handles execution errors.",
            "uuid": "e832.5",
            "source_info": {
                "paper_title": "Learning to Use Tools via Cooperative and Interactive Agents",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ToolLLM: Facilitating large language models to master 16000+ real-world apis",
            "rating": 2,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "Tool learning with foundation models",
            "rating": 2,
            "sanitized_title": "tool_learning_with_foundation_models"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "RestGPT: Connecting large language models with real-world applications via restful apis",
            "rating": 2,
            "sanitized_title": "restgpt_connecting_large_language_models_with_realworld_applications_via_restful_apis"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        }
    ],
    "cost": 0.0170605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Use Tools via Cooperative and Interactive Agents with Large Language Models
22 Jun 2024</p>
<p>Zhengliang Shi 
Shandong University</p>
<p>Shen Gao 
University of Electronic Science
Technology of China</p>
<p>Xiuyi Chen 
Baidu Inc
BeijingChina</p>
<p>Yue Feng 
University of Birmingham
BirminghamUK</p>
<p>Lingyong Yan 
Baidu Inc
BeijingChina</p>
<p>Haibo Shi shizhl@mail.sdu.edu.cn 
Baidu Inc
BeijingChina</p>
<p>Dawei Yin 
Baidu Inc
BeijingChina</p>
<p>Pengjie Ren 
Shandong University</p>
<p>Suzan Verberne 
Leiden University
LeidenThe Netherlands</p>
<p>Zhaochun Ren z.ren@liacs.leidenuniv.nl 
Leiden University
LeidenThe Netherlands</p>
<p>Learning to Use Tools via Cooperative and Interactive Agents with Large Language Models
22 Jun 202457574E40F261E11A32BF3BFB54255445arXiv:2403.03031v4[cs.CL]
Tool learning empowers large language models (LLMs) as agents to use external tools and extend their utility.Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating execution results into the next action prediction.Despite their progress, these methods suffer from performance degradation when addressing practical tasks due to: (1) the pre-defined pipeline with restricted flexibility to calibrate incorrect actions, and (2) the struggle to adapt a general LLM-based agent to perform a variety of specialized actions.To mitigate these problems, we propose ConAgents, a Cooperative and interactive Agents framework, which coordinates three specialized agents for tool selection, tool execution, and action calibration separately.ConAgents introduces two communication protocols to enable the flexible cooperation of agents.To effectively generalize the ConAgents into open-source models, we also propose specialized action distillation, enhancing their ability to perform specialized actions in our framework.Our extensive experiments on three datasets show that the LLMs, when equipped with the ConAgents, outperform baselines with substantial improvement (i.e., up to 14% higher success rate).</p>
<p>Introduction</p>
<p>Although large language models (LLMs) have achieved remarkable performance in a broad range of natural language processing tasks (Wang et al., 2023c;Chang et al., 2023), they still encounter inherent limitations such as out-of-date information (Qin et al., 2023b;Mallen et al., 2023).Tool learning is proposed to equip LLMs with various auxiliary resources, e.g., a search engine (Qin et al., 2023a;Nakano et al., 2021) or a calculator (Schick et al., 2023;Gao et al., 2023a), which empower them as tool-use agents and improve their proficiency in tackling concrete complex tasks.As shown in Figure 1(a), most previous studies allow the LLM-based agent to interleave multiple actions in a pre-defined order to interact with tools (Yao et al., 2023;Yang et al., 2023b;Zhuang et al., 2023).The agent first breaks down the task and plans a series of tools in a stepby-step manner.For each step, the agent executes the tools by passing arguments and continuously incorporates useful intermediates into the next action prediction.Despite the advancement of existing methods, they face two challenges in practice.First, most of them alternate the planning and execution with a pre-defined pipeline (Yang et al., 2023b;Song et al., 2023), which inevitably constrains their flexibility in handling exceptional errors that frequently occur during a tool-use workflow (Shi et al., 2024;Wang et al., 2023b;Prasad et al., 2023).When failing to invoke tools, it is crucial to enable agents to revise their incorrect actions instead of directly shifting to the next step with the error response of previous steps.Second, it is struggling to adapt a single LLM-based agent to learn a variety of specialized actions in solving a task (Dziri et al., 2023;Yin et al., 2023).Solving a practical task involves varied actions with substantial differences, e.g., planning, execution, and reflection, drawing upon different facets of the LLMs (Shen et al., 2024;Qiao et al., 2024).Therefore, developing effective agent flow and adapting tool-use models to solve practical tasks remains a challenging research topic.</p>
<p>In this work, we propose ConAgents, a Cooperative and interative Agents framework for tool learning tasks.</p>
<p>As illustrated in Figure 1, ConAgents decomposes the overall tool-use workflow using three specialized agents: Grounding, Execution, and Review agents.The grounding agent reasons the task description and grounds it into planning by specifying which tool to use.The execution agent follows the planning to execute the selected tool by generating executable code.The review agent reviews the incorrectness in planning or execution, providing feedback for revision.To enable the dynamic cooperation of these specialized agents, we propose two communication protocols, including automatic and adaptive interaction.In the process of automatic interaction, the review agent provides real-time reviews to calibrate incorrect actions.Thus, the agent flow alternates between the planning-review and execution-review phases as shown in Figure 1.In the process of adaptive interaction, the review agent only provides feedback when exceptional errors are captured while executing the tools.</p>
<p>For a comprehensive evaluation, we conduct experiments on two benchmarks, i.e., ToolBench and RestBench, using various LLMs as backbones.We find that ConAgents outperforms the state-ofthe-art baseline with both communication protocols (6% improvement in Success Rate on average).</p>
<p>Despite closed-source LLMs performing well with our framework, we find the open-source models may struggle with the modulized agent flow.Thus, we propose an approach called specialized action distillation (SPAN), enhancing the performance of open-source models in ConAgents.We heuristically sample 2,919 highquality tasks from the ToolBench (Qin et al., 2024) training set, and cluster them based on their similarity, retaining only one task in each cluster to avoid duplication.For each task, we guide the GPT-4 to generate solutions using ConAgents, and reorganize them into actions tailored to specialized agent functionalities in ConAgents.These actions are separately distilled into different student models through instruction tuning.We employ parameterefficient tuning techniques, i.e., LoRA (Hu et al., 2021)</p>
<p>Related Work</p>
<p>LLMs for tool learning.Enhancing LLMs with external tools has been proven a promising method for solving practical tasks (Bran et al., 2023;Qu et al., 2024;Wang et al., 2024b).Previous works empower a tool-learning agent typically by supervised fine-tuning (Patil et al., 2023;Yang et al., 2023a;Gao et al., 2023b) or prompt learning (Lu et al., 2023;Shen et al., 2023).Specifically, the former trains LLMs on tool-use dataset (Wang et al., 2023c), teaching LLMs how to use tools from the data.The latter directly demonstrates tool usages to LLM using in-context examples (Paranjape et al., 2023;Kim et al., 2023).However, solving complex tasks with tools involves various actions, e.g., deciding which tools to use, what arguments to pass, and how to utilize the results (Schick et al., 2023;Qiao et al., 2024).Compelling one single agent to learn all abilities places even greater pressure on it (Yin et al., 2023;Prasad et al., 2023).In addition, as the tasks become complex, LLMs-based agents struggle to incorporate lengthy task-solving contexts to predict the next actions correctly due to their limited working memory (Shi et al., 2023).In contrast, our proposed ConAgents coordinates three specialized agents, generating a solution through agent cooperation.</p>
<p>Multi-agent cooperation.Synergizing multiple agents has demonstrated strong performance on a variety of tasks (Liu et al., 2023;Sun et al., 2023;Zhang et al., 2023), enhancing the capabilities of individual agents (Talebirad and Nadiri, 2023;Mohtashami et al., 2023;Qian et al., 2023).Recent studies take multiple agents into a debate for a fixed number of rounds (Wang et al., 2023a;Liang et al., 2023), boosting their factuality (Cohen et al., 2023) and reasoning abilities (Du et al., 2023;Fu et al., 2023).In the tool learning tasks, recent work separately implements the task planning and execution with different agents, thereby reducing the workload of a single agent (Shen et al., 2024;Song et al., 2023;Qiao et al., 2024).Despite their progress, their agent flow is simplified into a predefined pipeline (Prasad et al., 2023), struggling to handle exceptional errors that frequently occur during the tool-use workflows (Zhuang et al., 2023;Wang et al., 2023b).In our work, we propose two communication protocols, which enable the action calibrations and dynamic cooperation of agents.</p>
<p>Methodology</p>
<p>Overall Framework</p>
<p>Our cooperative framework, ConAgents, is proposed to enable the dynamic cooperation of agents to solve complex tasks.As shown in Figure 2, ConAgents streamlines and modularizes the workflow of tool learning tasks into a grounding agent M G , execution agent M E , and review agent M R .These agents are implemented with different system prompt or learnable parameters.Given a complex task x, the M G first decomposes x into simpler sub-tasks and generates tool-use planning t in a step-by-step manner.For each step, the M E executes the selected tool by writing executable code following the planning t.The execution result r is then incorporated into the context of the grounding agent M G to predict planning in the next iteration.The M R is employed to simulate an expert to provide feedback to agent M G and M E , guiding them to revise their incorrect planning or execution.To coordinate these three specialized agents, we explore and analyze two communication protocols, including the automatic and adaptive interactions.</p>
<p>Specialized Agents</p>
<p>Grounding Agent.The grounding agent is designed to break down an input task and generate a series of tool-use planing.At ith iteration, the grounding agent generates planning t i on the condition of the task x and current trajectory H i = {(t j , r j )|j &lt; i}, consisting of the accumulation of previous planning t &lt;i and results r &lt;i .It can be formulated as:
t i = M G (x, S, H i ),(1)
where t i contains a tool selected from the provided toolset S and necessary arguments to invoke the tool, such as "Use the Bing search to find a movie shown on Dec 24, 2023".</p>
<p>Execution Agent.Following the generated planning t i , the execution agent M E executes the selected tool by generating executable code c with the assistance of the tool documentation d.This process can be formulated as:
c i = M E (d, t i ).
The execution result r i is obtained by running the generated code c i to request the data from the backend servers of tools, denoted as r i = Execute(c i ).When the tool fails to execute, the r i indicates an error message as a failure signal.</p>
<p>When the tool executes successfully, the result r i contains the targeted information in response to the planning t i .</p>
<p>Review Agent.Incorrect planning and execution are frequently observed during the tool-use workflow.The review agent M R is employed as an expert, providing feedback to agent M G and M E for revision.Specifically, if the planning generated by M G is vague or selects a non-existing tool, the agent M R generates verbal feedback to instruct the M G to reformulate planning.It can be formulated as:
f R→G = M R (x, S, t i )(2)
Similarly, if M E hallucinates generating a wrong program to execute the tool, the agent M R reviews execution results (or errors) and re-checks the tool documentation, providing instructions for calibration:
f R→E = M R (x, d, c i , r i )(3)
We denote the maximum turns of interaction between agent M R and agent M G (or M E ) is denoted as α (or β).Their communication protocol and action flow are explained in § 3.3.</p>
<p>Agent communication protocols</p>
<p>We propose two agent communication protocols, including automatic and adaptive interaction.Automatic interaction.As illustrated in Figure 2, our automatic interaction alternates between planning-review and execution-review phases.For the ith step, it starts with the interaction between the agent M G and M R until a correct planning t i is determined or up to the maximum turns α.Formally, it can be formulated as:
t j i = M G (x, S, H i , {t &lt;j i , f &lt;j R→G } planning calibration )(4)
Here, j indicates jth interaction of two agents.Following the planning t, the agent M E generates executable programs to execute the selected tool and calibrates the incorrect result r with the feedback of agent M R for up to β turns.This process can be formulated as:
c j i = M E (t i , d, {c &lt;j i , f &lt;j R→E } execution calibration ) (5)
The calibrated result is then incorporated into the context of M G for the next planning generation.</p>
<p>Adaptive interaction.In our adaptive interaction strategy, the agent flow primarily alternates from (1) generating tool-use planning by agent M G and</p>
<p>(2) generating execution code by agent M E , in a step-by-step manner.The review agent M R is adaptively triggered to provide feedback only when the generated code fails to execute correctly.</p>
<p>Specifically, a runtime error can be caused by either unfeasible planning or coding faulty.Thus, the agent M R first reviews the generated planning and code, routines the errors to agent M G or M E accordingly, and provides feedback for revision.</p>
<p>Specialization by Agent Distillation</p>
<p>Our initial experiment shows that powerful LLMs such as GPT-4, achieve promising results when equipped with our framework.However, these model are often considered black boxes (Qin et al., 2023a;Gao et al., 2023b) with potential privacy issues.Thus, we aim to adapt our framework to open-source models.We propose specialized action distillation (SPAN), which distills the tasksolving trajectory of powerful commercial LLMs into different open-source LLM agents tailored to specific functionalities in ConAgents.</p>
<p>Synthesize the Training Dataset</p>
<p>Our distillation method collects the task-solving trajectory of specialized agents simulated by GPT-4, in ConAgents ( § 3.1).To achieve this, we first sample tasks from ToolBench (Qin et al., 2024), which contains nearly 200k practical tasks across 3,451 tools.We select 2,919 tasks using various heuristic strategies (see Appendix A.2 for more details).Each task x is paired with a list of relevant tools.Since we find that some tasks in Statistic # The data scale 500 # The average tokens of input task 52.48 # The average number of candidate tools 20 # The average number of ground truth tools per task 3.39 # The average turns of planning-review interaction 4.62 # The average turns of execution-review interaction 5.21</p>
<p>Table 1: The statistics of our synthetic dataset in our specialized action distillation method.</p>
<p>ToolBench are very similar to each other, we cluster them based on the semantic similarities between task descriptions and retain one instance for each cluster.Next, we supplement each of these selected tasks with a detailed solution.Specifically, we separately implement our grounding, execution, and review agent with GPT-4, and coordinate them using the proposed automatic communication protocol ( § 3.3) to generate solutions.Finally, we synthesize a dataset with 500 diverse examples.Each example contains a task x, a candidate toolset S, and the task-solving trajectory of three agents.The statistics of our synthetic dataset are provided in Table 1.</p>
<p>Agent Training</p>
<p>Due to the large number of parameters of the LLM, we employ a parameter-efficient tuning technique (i.e., LoRa (Hu et al., 2021)) to train each specialized agent separately.The objective is to optimize the delta parameters ∆θ of the LLM θ to minimize the loss function.</p>
<p>We reorganize the dataset according to the agents' functionality ( § 3.1), thereby distilling specific abilities into different student models.Formally, given a task x, in the ith step, the H i contains historical planning and execution results.We train the agent M G to generate the ith tool-use planning t i on the condition of H i and revise its incorrect planning following the review from agent M R ( Eq. 4 ).We train the agent M E to generate programs c for tool execution following the generated planning t and feedback of agent M R ( Eq. 5 ).Similarly, the agent M R are trained to provide feedback as Eq. 2 and Eq. 3 .We apply the standard language modeling loss for the optimization.More details and formulations can be found in Appendix A.1.</p>
<p>Experimental Setup</p>
<p>Datasets and Evaluation Metrics</p>
<p>Datasets.We conduct experiments on two well established benchmarks, i.e., RestBench (Song et al., 2023) and Toolbench (Qin et al., 2024).The RestBench consists of two subsets, including: (1) TMDB, a high-quality human annotated dataset consisting of 54 movie-related tools; and (2) Spotify, a dataset with 40 music-related tools.The Toolbench contains various practical tasks across diverse scenarios.We provide more details for these datasets in Appendix A.3.Evaluation metrics.Following Yang et al. (2023a); Gao et al. (2023b), we use two evaluation metrics: (1) Success Rate (Success%) measuring the proportion of successful query completions, and (2) Correct Path Rate (Path%) calculating the F1 score between the generated tool sequence and ground-truth tool sequence.We also conduct a human evaluation, in which three well-educated volunteers are invited to evaluate 30 randomly sampled cases with a three-scale rating in two aspects: (1) Executability (Exec): whether multiple tools are invoked in a correct logical order; and (2) Utility: whether the execution results of tools can be used to generate an answer.</p>
<p>Baselines</p>
<p>We compare our method with agent-based tool learning methods, including: (1) Chameleon (Lu et al., 2023), an LLM-based agent that directly generates multi-step plans for tool use and then sequentially executes the plan; (2) ReAct (Yao et al., 2023), which prompts LLM to generate the chainof-thought and actions in an interleaved manner.;</p>
<p>(3) CodeAct (Wang et al., 2024a), which allows the LLM to generate executable code snippets as actions to use tools; (4) ToolLLM (DFSDT, Qin et al., 2024), which enhances LLMs with the Depth First Search-based Decision Tree (DFSDT) to select tools to solve a task.For further comparison, Since our ConAgents coordinates three specialized agents, we also establish two baselines, i.e., ReAct@N and ToolLLM@N, which are up to N times runs of their vanilla method (ReAct or ToolLLM) until an input task is completed.</p>
<p>We also consider baselines with multi-agent architecture, including (1) RestGPT (Song et al., 2023): which consists of a planning module, a tool selector, an executor, and a response parsing module; (2) Reflexion (Shinn et al., 2023), which employs an LLM for task execution and uses another LLM to verbally reflect on task feedback signals; and (3) α-UMi (Shen et al., 2024), which consists of a planner, an executor, and an answer generator.</p>
<p>Implementation Details</p>
<p>We use gpt-3.5-turbo 1 from OpenAI as the LLM backbone for each agent in our method and all baselines.We instruct the three agents to perform specific actions with different system prompts.The decoding temperature is set to 0 for the most deterministic generation.We also repeat the experiment with an open-source model Mistral-1 https://openai.com/chatgpt8x7B2 for further comparison.In our agent communication ( § 3.3), we set the maximum iteration of interactions α = 3 and β = 3, respectively.For each sample in the test set, we provide all the baselines with the same candidate toolset for a fair comparison, which contains the required tools and ten randomly sampled tools.Our action distillation separately trains three Mistral-8x7B using the corresponding optimization objectives in § 4.2 with the learning rate of 5×10 −5 .</p>
<p>The training of our model can be done within 4 hours with 3 NVIDIA A800-PCIE-80GB GPUs using LoRA (Hu et al., 2021).</p>
<p>Results and Analysis</p>
<p>Experimental Results</p>
<p>Overall performance.Table 2 demonstrates the experimental performances of all methods.We find that our proposed ConAgents outperforms all the baselines in three datasets in terms of all metrics.A reason here is that our cooperative framework design enables each agent to perform specialized actions instead of grasping all required capabilities, thereby reducing the workload encountered by a single agent.The significant improvement over ReAct@N and ToolLLM@N baselines can further validate the effectiveness of our framework.</p>
<p>Compared with baselines with multi-agent architecture like RestGPT, ConAgents achieves about 12% higher Success Rate.The potential reason for our improvement is that the Table 4: The ablation study on two datasets with gpt-3.5-turboas backbone.See § 6.3 for details proposed two communication protocols enable the dynamic interaction of agents, which is more flexible to handle exception errors.</p>
<p>Performance with the open-source LLM.We further evaluate our ConAgents by swapping the backbone LLM with Mistral-8x7B and repeating the experiment under the same conditions.As shown in Table 3, we implement our framework in two ways with Mistral-8x7B: (1) directly prompting (w/ Auto and w/ Ada);</p>
<p>(2) tuning with our proposed action distillation (w/ Auto † and w/ Ada † ).We observe that directly prompting Mistral-8x7B with ConAgents yields better performance than baselines.The action distillation further improves overall performance substantially, such as pushing the Success Rate from 47.00 to 51.00 in the TMDB dataset.These results further prove the effectiveness of our cooperative framework.</p>
<p>Human Evaluation</p>
<p>Table 5 shows the results of the human evaluation.We find that ConAgents achieves the best results in the Executability aspect with 0.08~0.12improvement.These results further validate the necessity of agent specialization and cooperation.The overall Kappa statistics for Executability and Utility are 0.75 and 0.71, illustrating substantial agreement (Landis and Koch, 1977) among the annotators.</p>
<p>Ablation Study</p>
<p>To better understand the impact of different components of our method, we make the following modifications to the architecture and measure the effect.</p>
<p>-w/o M R → M G .We remove the interaction between agent M R and M G in our framework.As shown in -w/o M R → M E .We remove the interaction between agent M R and M E in our framework when programming to execute tools.As shown in Table 4, the Success Rate suffers from obvious decrease in both two datasets.These results indicate that the agent M R can review the generated programs of agent M E and provide useful instruction for calibrating errors.</p>
<p>-w/ static cooperation.We implement the M R with a code compiler, which is triggered to provide static feedback only when runtime errors are raised during executing tools by agent M E .This allows us to compare our framework with a static algorithm for agent cooperation.Table 4 present the results, where we observe a 4.12 average decrease in the Success Rate, e.g., dropping from 79.00 to 75.00 on the TMDB dataset.The same trend is also observed in the Correct Path Rate, e.g., a 2.5 decrease on the Spotify dataset.These results indicate the superiority of our dynamic agent cooperation framework.</p>
<p>Case Study</p>
<p>We conduct the case studies and find that our cooperative agent framework is more effective at executing various tools and handle exceptional errors in solving tasks.We also provide examples to explain the detailed process of agent cooperation.The details can be found in Appendix A.5.</p>
<p>Discussion</p>
<p>Qualitative analysis for the maximum number of interactions.In our automatic agent interaction, agents M G and M E revise their actions following the feedback of agent M R for up to α and β turns, respectively.To further explore the impact of the interaction times on overall performance, we conduct a quantitative and qualitative analysis by varying to 5. Then we evaluate our framework using the RestBench-TMDB dataset with the same settings as in Table 2.As illustrated in Figure 3, we find an increasing Success Rate when the maximum iteration turns shifts from 1 to 3. In addition, a relatively stable trend is observed when the α and β keep increasing (from 3 to 5), which indicates the agents can correct most errors within 3 turns.</p>
<p>We also look at the poorly performing cases where we find that since the planning from agent M G is typically open-ended, the M R struggles to detect all the incorrect planning.For example, the planning may be plausible and clear but lacks the required arguments to execute tools, thus resulting in a failure of M E in subsequent steps.</p>
<p>Qualitative analysis for the efficiency of inference.Due to the intensive inference cost of LLMs-based agents, we further explore the efficiency of our ConAgents.To explain more intuitively, we compare the token consumption for the ConAgents and baselines using the RestBench-TMDB dataset with the same settings as in Table 2.As illustrated in Figure 4, we find that although our framework achieves better performance, we spend fewer tokens compared with strong baselines such as RestGPT and ToolLLM@3.The reason is that the cooperative framework ConAgents enables each agent to perform specific tasks more efficiently, reducing the length exploration trajectory by the single agent.</p>
<p>The quality of generated review.We further analyze the quality of reviews given by review agent M R .Specifically, we randomly sample 50 task-solving trajectories in Table 2 (w/ Auto) manually analyze the review of review agent.For most tasks, we find that the agent M R can assist agent M E to revise its generated code or provides useful reviews for the planning generated by agent M G , such as only select tools from given list.In addition, we find that in less than 5% of tasks, the agent M R hallucinates giving an incorrect review, indicating its reliability.</p>
<p>Runtime consistency.Considering the nondeterministic nature of LLM generation, we analyze the consistency of our framework.We repeat our method multiple times with the same settings as in Table 2.The statistical significance of differences observed between the performance of two runs is tested using a two-tailed paired ttest.We find no significant difference between the results of two randomly conducted experiments (significance level α = 0.05).</p>
<p>Conclusions</p>
<p>We present a cooperative and interactive agents framework (ConAgents) for tool learning, which diverges from previous work by allowing the cooperation of agents to solve complex tasks.The ConAgents first modularizes the overall workflow with three specialized agents for tool planning, tool execution, and action calibration, respectively.Then, two communication protocols are introduced to enable the dynamic cooperation of these agents.</p>
<p>To generalize our framework to open-source models, we propose specialized action distillation, enhancing the models' capability to perform specific actions.Extensive experiments conducted on three datasets demonstrate the superiority of our ConAgents, e.g., pushing the success rate to 77.00 with 13.2% point improvement.Our future work includes: (1) extending our method to agents empowered by multi-modal foundation models, incorporating image and sound; (2) coordinating the cooperation between strong and weak agents.</p>
<p>Limitations</p>
<p>The main limitation is that LLM-based agent is limited when perceiving multi-modal tasks.When executing the tools, we represent the image and speech input with url, following previous works.</p>
<p>In the future, we plan to extend our method to agents empowered by multi-modal foundation models.</p>
<p>Ethics Statement</p>
<p>The paper proposes a cooperative agent framework, synergizing specialized agents to solve complex tasks.The modularized design enables the agents to utilize feedback from the tool environment to calibrate themselves adaptively.In addition to the use of state-of-the-art commercial LLMs, we have experimented with an open-source LLM, for reproducibility reasons and to allow the use of our method in lower-resource contexts.All the tools used in our experiment are provided by open-source platforms, including TMDB, Spotify, and Rapid API, thus ensuring a high level of transparency and reproducibility.</p>
<p>We have made every effort to ensure that our research does not harm individuals or groups, nor does it involve any form of deception or potential misuse of information.</p>
<p>A Appendix</p>
<p>A.1 Details of Action Distillation</p>
<p>Our specialized action distillation (SPAN) trains three student models separately using the tasksolving trajectory of a powerful model, i.e., GPT-4 in our implementation.These three student models are trained to conduct specific actions of the grounding agent, execution agent, and review agent, respectively.Their initial parameters weights θ are initialized from the same open-source model M θ .Since we use LoRa (Hu et al., 2021) for parameterefficient tuning, the optimization objective of our distillation is to search for the delta parameter ∆θ to minimize the loss function.Here, we introduce their detailed optimization objectives.</p>
<p>Notations.As mentioned in § 3, we denote an input task as x, which is solved in a stepby-step manner while the task-solving context is denoted as H.In ith step, the context H i contains historical planning t &lt;i and execution results r &lt;i .The planning t specifies a tool to use in a current step which is selected from a candidate toolset S.</p>
<p>Training of grounding agent.Given a task x, we train the grounding agent M G to decompose x into simpler sub-tasks and ground each sub-task into tool-use planning t on the condition of the current context H and revise incorrect planning following the feedback f R→G of the review agent M R .For each step t i , we use the standard language modeling loss for optimization, which can be formulated as:
L G = − log P θ+∆θ G t j i |x, H i , S, {t &lt;j i , f &lt;j R→G }
Here, the j indicate the jth interaction between the agent M G and M R .The {t &lt;j i , f &lt;j R→G } indicates the planning-review alternated from agent M G to M R .The LoRa parameter of agent M G is denoted as ∆θ G .</p>
<p>Training of execution agent.Similarly, in the ith step, we train the execution agent M E to execute a tool following the planning t i by generating an executable program, and then calibrate incorrect code following the review of agent M R .Formally, the optimization objective can be formulated as:
L E = − log P θ+∆θ E c j i |x, t, d, {c &lt;j i , f &lt;j R→E }
Here, d indicates the tool documentation.The LoRa parameter of agent M E is denoted as ∆θ E .</p>
<p>Training of review agent.The review agent agent is trained to provide reviews for agent M E and M R , calibrating their incorrect actions, i.e., planning or execution.Thus, the optimization objective can be formulated as:
L R = − α j=1 log P θ+∆θ R f j R→G |x, S, t j−1 i − β j=1 log P θ+∆θ R f j R→E |x, d, c j−1 i , r j−1 i
Here, the LoRa parameter of agent M R is denoted as ∆θ R .</p>
<p>Heuristic Strategies for Data Selection</p>
<p>We employ the following heuristic methods to filter low-quality tasks in the original ToolBench:</p>
<p>• Each task in ToolBench is paired with a list of candidate tools.Generally, the more candidate tools there are, the more complex the task.Thus, we filter out tasks with fewer than 10 candidate tools to ensure the overall complexity of the sampled tasks.</p>
<p>• To improve the quality of our training dataset, we remove tasks if their tools are not callable or deprecated.</p>
<p>• We remove tasks if their tools lack the required documentation or if the documentation is less than 100 words in length.</p>
<p>A.3 Datasets</p>
<p>Experiment dataset We conduct experiments on three commonly-used datasets with tool learning tasks, including:</p>
<p>• RestBench (Song et al., 2023): a high-quality human annotated dataset consisting of 54 tools about movie scenarios.</p>
<p>• RestBench-Spotify (Song et al., 2023): a dataset with 40 tools for music scenarios.</p>
<p>• ToolBench (Qin et al., 2024): a dataset containing diverse real-world tools across various applications, which contains the simple tasks, i.e., solving a task with one single tool, and complex tasks, i.e., executing multiple tools in a logic order to solve a task.</p>
<p>We conduct experiments on the full dataset of TMDB and Spotify.Due to the intensive inference cost of LLMs-based agents, we randomly sample 117 cases as test sets from the complex tasks in Toolbench datasets to evaluate the performance of our cooperative agent framework in solving practical tasks.We will release the sampled task for the transparency consideration.</p>
<p>Extend existing datasets.The original ToolBench benchmark only provides a step-bystep task-solving trajectory of GPT-3.5, which consists of both valid ground truth tools and irrelevant tools.However, our evaluation involves computing the overlap between model-selected tools with ground truth tools.Therefore, we repurpose the ToolBench to support our evaluation methods.Specifically, for each task, we extract the tools in the original solution provided by ToolBench and only retain the relevant tools that are required for solving the task.We invite three well-educated masters with relevant research backgrounds to implement this process.To guarantee annotation quality, we ask at least two annotators to annotate the same task repeatedly.If there is a discrepancy between the two annotators (i.e., two annotators give different answers), we ask a third annotator to recheck it.We hold regular meetings and pre-annotation tests to ensure that each expert undergoes detailed training to familiarize themselves with our annotation task.We will release these repurposed tasks to facilitate future research.</p>
<p>A.4 Evaluation Metrics Details</p>
<p>Automatic evaluation.We mainly employ Success Rate and Correct Path Rate as two automatic evaluation metrics, following previous works (Yang et al., 2023a;Gao et al., 2023b).The Success Rate (Success%) computes the proportion of successful query completions.Specifically, when all the ground-truth tools are executed correctly, the Success Rate is set to 1; otherwise, it is set to 0. The Correct Path Rate (Path%) computes the F1 score between the generated tool sequence and the ground-truth tool sequence.</p>
<p>Human evaluation</p>
<p>We conduct a human evaluation on two metrics, including: (1) Executability (Exec): whether the multiple tools are invoked in a correct logical order to complete the task; and (2) Utility: whether the execution results of tools can be used to generate an answer.We invite three well-educated volunteers to evaluate 30 cases randomly sampled from RestBench-TMDB and RestBench-Spotify datasets, respectively, with a three-scale rating.Using a 3-point scale over a binary scale provides an option for the annotators to factor in their subjective interpretation of the extent of success or failure of a system's response to satisfy a user's request.The instructions used in our human evaluation are summarized as follows.</p>
<p>The evaluation guideline for our human evaluation.</p>
<p>In this evaluation task , you are provided with some question -solution pairs .The question can be solved by using real -world tools ( or APIs ).The solution is a sequential tool -use process , involving multi -step tool callings .</p>
<p>Your task is to rate the quality of the solution on a three scale based on the following two metrics : 1. Executability : Whether multiple tools are invoked in a correct logical order to complete the task .2. Utility : Whether the model can observe the relevant values from lengthy execution results , incorporate them to predict the next action , and finally output a correct answer .</p>
<p>We also provide scoring criteria for your reference .Please adhere to our criteria since we will re -check the score you provide .Now , read the following criteria and rate the provided question -solution pairs .Note that , you are encouraged to give us feedback and share any confusion you may have .</p>
<p>== Scoring Criteria == 1.For the Executability metric :</p>
<p>-Three points : Call all necessary tools correctly and solve the task .Allow for redundant tools or inference steps .-Two points : Not fully calling all necessary tools correctly , partially solving the task .</p>
<p>-One point : Only some sub -steps are solved and the entire task is not completed .And there is a lot of redundancy or incorrect reasoning .</p>
<p>For the Utility metric :</p>
<p>-Three points : A majority of the execution results of the tools are correctly used to address the question ( minor mistakes are allowed ).</p>
<p>-Two points : Only part of the execution results of the tools are used .For example , in a question requiring finding an actor ' s highest -grossing film , the correct solution is to sequentially look at all the films the actor has appeared in , instead of just counting the top -k like top -5 or top -10.-One point : Only a small part of the execution results of the tools are used , while other useful intermediates are ignored .</p>
<p>A.5 Case Study</p>
<p>We conduct several case studies and find that our method is effective at executing various tools and incorporating execution results to solve the input tasks.Figure 5 presents a concrete example of the workflow of our proposed cooperative framework.</p>
<p>Case for our automatic agent communication.Figure 5 shows an example of our proposed automatic communication protocol.For each turn, the communication starts with the planning-andreview between the grounding agent and review agent.Following the planning , the execution agent generates programs to execute tools and calibrates the incorrect result with the review of review agent.For example, in the first turn, the agent M G regenerate a planning following the review from agent M R , and finally output a clear planning.This example also illustrate the interaction between grounding agent M G and review agent M R , where the agent M G calibrates its execution programs following the feedback of M R , and finally generate Case for our adaptive agent communication Figure 6 shows an example of our proposed adaptive communication protocol.The agent flow mainly alternates between (1) generating tool-use planning by grounding agent and (2) generating execution code by execution agent, in a step-by-step manner.The review agent is adaptively triggered to provide feedback only when the generated code fails to execute correctly.For example, in the second turn, agent M E initially generates a wrong program due to the lack of necessary arguments.Then, agent M R reviews the current context, routes this error to agent M G , and instructs M G to supplement this argument, instead of directly shifting to the next state with an error response.This example intuitively illustrates the process of our adaptive interaction.</p>
<p>Figure 1 :
1
Figure 1: Comparison between (a) existing single-agent tool learning method and (b) our cooperative agent framework ConAgents.The ConAgents coordinates three agents through two proposed communication protocols, e.g., automatic and adaptive interaction.</p>
<p>Question:Figure 2 :
2
Figure 2: Our proposed cooperative and interactive agent framework.The left shows the three specialized agents in our framework ( § 3.1).The right illustrates two proposed communication protocols to coordinate these specialized agents, including the automatic and adaptive communication ( § 3.3).</p>
<p>α and β from 1 Figure 3 :
13
Figure3: The qualitative analysis for the maximum interaction turns α and β in our agent communication protocols (Section 3.3) on the TMDB dataset.</p>
<p>Figure 4 :
4
Figure 4: The efficiency analysis for different methods, where we count the average consumed tokens.</p>
<p>Table 2 :
2
The results on three datasets.The metrics Success% and Path% indicate the Success Rate and Correct Path Rate, respectively.The icon denotes the single-agent method and symbolizes multi-agent architecture.
MethodRestBench-TMDBRestBench-SpotifyToolBenchSuccess Rate Path% Success Rate Path% Success Rate Path%gpt-3.5-turboReAct (Yao et al., 2023)40.0071.1951.2860.3539.3965.04Chameleon (Lu et al., 2023)63.0066.1056.2064.5537.4467.55CodeAct (Wang et al., 2024a)63.0080.9154.3076.64--ToolLLM (DFSDT, Qin et al., 2024)68.0076.7761.4074.7766.3986.43Reflexion (Shinn et al., 2023)53.0055.0049.1050.90--α-UMi (Shen et al., 2024)62.0070.2366.7470.2767.5578.37RestGPT (Song et al., 2023)65.0069.2167.1070.7563.8877.40ConAgents w/ Ada78.0079.5769.4377.5469.8481.58ConAgents w/ Auto79.0081.9771.2179.1772.1583.33ReAct@N → N = 254.0067.9056.7159.4741.4163.67ReAct@N → N = 362.0065.4058.1363.2642.6766.12ToolLLM@N → N = 270.0076.5463.1675.2768.3786.43ToolLLM@N → N = 371.0078.1163.1676.3068.7787.54MethodTMDBSpotifySuccess% Path% Success% Path%ConAgents (Mixtral-8x7B)w/ Auto (Distilled) 53.0079.3236.0973.92w/ Auto (Vanilla)49.0076.2234.2168.14w/ Ada (Distilled)51.0078.7435.4769.86w/ Ada (Vanilla)47.0074.0533.3366.41Baselines (Mixtral-8x7B)ReAct26.0061.2121.3547.21ReAct@333.0063.2726.9350.31ToolLLM37.0064.3228.0752.31ToolLLM@345.0074.4031.5857.68RestGPT34.0072.2031.5867.82</p>
<p>Table 3 :
3
We employ the Mixtral-8x7B as the backbone LLM of for our method and baselines.The Vanilla and Distilled indicate enable our framework by prompting and our action distillation, respectively.</p>
<p>M R →M G 77.00↓ 2.0 78.10↓ 3.9 68.42↓ 3.0 75.33↓ 2.2 w/o M R →M E 75.00↓ 4.0 74.23↓ 7.7 64.91↓ 6.5 72.41↓ 5.1 w/ static coop.75.00↓ 4.00 75.74↓ 6.2 67.12↓ 4.3 75.07 ↓ 2.5
MethodTMDBSpotifySuccess% Path% Success%Ours w/ Auto79.0081.9771.4377.54w/o</p>
<p>Table 4
4MethodTMDBSpotifyExec Utility Exec Utilitygpt-3.5-turboReAct1.891.931.772.10ToolLLM2.261.872.262.30RestGPT2.352.452.302.40Ours w/ Auto 2.472.562.432.50Ours w/ Ada2.432.502.382.45Table 5: Human evaluation on Executability (Exec) andCorrect Rate of Parsing (Parsing).
, the Success Rate has a average 2.50 decline, while the Correct Path Rate has a 3.05 average decline on two datasets.This results validate the necessity of feedback of M R which can instruct the M G to revise incorrect planning.</p>
<p>https://huggingface.co/mistralai
directed the top-1 rated movie Decision: wrong Review: The code correctly makes a request.However, it does not parse the response to extract the target values as specified in the instruction.Input: <code>python url = base_url+"/movie/top_rated" params = {'page': 1, 'region': 'US'} response = requests.get(url,headers=headers, params=params) data = response.json()print(top_rated_movie_id)</code> Output: {'id': 278, 'genreids': [18, 80], ori...
Sam Andres M Bran, Andrew D Cox, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 2023</p>
<p>Lm vs lm: Detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, arXiv:2305.132812023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Improving language model negotiation with self-play and in-context learning from ai feedback. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Hwang, arXiv:2305.18654arXiv:2305.10142Peng, Tushar Khot, and Mirella Lapata2023. 2023arXiv preprintFaith and fate: Limits of transformers on compositionality</p>
<p>PAL: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, PMLR2023a</p>
<p>Confucius: Iterative tool learning from introspection feedback by easy-todifficult curriculum. Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, Zhaochun Ren, 2023b</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, ArXiv, abs/2303.174912023</p>
<p>The measurement of observer agreement for categorical data. Richard Landis, Gary G Koch, biometrics. 1977</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, arXiv:2310.021702023arXiv preprint</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, ArXiv, abs/2304.098422023</p>
<p>When not to trust language models: Investigating effectiveness of parametric and nonparametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Amirkeivan Mohtashami, Florian Hartmann, Sian Gooding, Lukas Zilka, Matt Sharifi, arXiv:2312.11441Social learning: Towards collaborative learning with large language models. 2023arXiv preprint</p>
<p>Webgpt: Browserassisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, S Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, ArXiv, abs/2112.09332Tyna Eloundou, Gretchen Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2021</p>
<p>Automatic multistep reasoning and tool-use for large language models. Bhargavi Paranjape, Scott M Lundberg, Sameer Singh, Hanna Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , ArXiv, abs/2303.090142023Art</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, arXiv:2311.05772Adapt: As-needed decomposition and planning with language models. 2023arXiv preprint</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, arXiv:2401.052682024arXiv preprint</p>
<p>WebCPM: Interactive web search for Chinese longform question answering. Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou, ACL. 2023a</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Jun-Han Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, ArXiv, abs/2304.083542023bZhiyuan Liu, and Maosong Sun</p>
<p>Toolllm: Facilitating large language models to master 16000+ real-world apis. Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun, ICLR. 2024</p>
<p>Toolformer: Language models can teach themselves to use tools. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen, arXiv:2405.17935ArXiv, abs/2302.04761Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom2024. 2023arXiv preprintTool learning with large language models: A survey</p>
<p>Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, Fei Huang, arXiv:2401.07324Small llms are weak tool learners: A multi-llm agent. 2024arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dong Sheng Li, Weiming Lu, Yue Ting, Zhuang , ArXiv, abs/2303.175802023</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, Denny Zhou, Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. the 40th International Conference on Machine Learning, Machine Learning ResearchPMLR2023</p>
<p>Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, Zhaochun Ren, arXiv:2405.16533Chain of tools: Large language model is an automatic multitool learner. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, cs.AI/2303.113662023. 2023arXiv preprint</p>
<p>Restgpt: Connecting large language models with real-world applications via restful apis. Yifan Song, Weimin Xiong, Dawei Zhu, Chengzu Li, Ke Wang, Ye Tian, Sujian Li, ArXiv, abs/2306.066242023</p>
<p>Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng Kong, arXiv:2310.00280Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. 2023arXiv preprint</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent llm agents. Yashar Talebirad, Amirhossein Nadiri, arXiv:2306.033142023arXiv preprint</p>
<p>Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Qian-Wen Zhang, Zhao Yan, Zhoujun Li, arXiv:2312.11242Mac-sql: Multi-agent collaboration for text-to-sql. 2023aarXiv preprint</p>
<p>Executable code actions elicit better llm agents. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, ICML. 2024a</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, arXiv:2309.10691Mint: Evaluating llms in multi-turn interaction with tools and language feedback. 2023barXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Noah A Liu, Daniel Smith, Hannaneh Khashabi, Hajishirzi, ACL. 2023c</p>
<p>Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, Graham Neubig, arXiv:2403.15452What are tools anyway? a survey from the language model perspective. 2024barXiv preprint</p>
<p>Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, ArXiv, abs/2305.18752Gpt4tools: Teaching large language model to use tools via self-instruction. 2023a</p>
<p>Mm-react: Prompting chatgpt for multimodal reasoning and action. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, ArXiv, abs/2303.113812023b</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, ICLR. 2023</p>
<p>Lumos: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, arXiv:2311.056572023arXiv preprint</p>
<p>Agentcf: Collaborative learning with autonomous language agents for recommender systems. Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian Mcauley, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen, arXiv:2310.092332023arXiv preprint</p>
<p>Toolqa: A dataset for llm question answering with external tools. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, Chao Zhang, arXiv:2306.133042023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
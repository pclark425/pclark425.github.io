<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-344 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-344</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-344</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-274306413</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.18564v2.pdf" target="_blank">Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often struggle with spatial reasoning. This paper presents a novel neural-symbolic framework that enhances LLMs' spatial reasoning abilities through iterative feedback between LLMs and Answer Set Programming (ASP). We evaluate our approach on two benchmark datasets: StepGame and SparQA, implementing three distinct strategies: (1) direct prompting baseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with iterative refinement. Our experimental results demonstrate that the LLM+ASP pipeline significantly outperforms baseline methods, achieving an average 82% accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and 8-15% respectively over direct prompting. The success stems from three key innovations: (1) effective separation of semantic parsing and logical reasoning through a modular pipeline, (2) iterative feedback mechanism between LLMs and ASP solvers that improves program rate, and (3) robust error handling that addresses parsing, grounding, and solving failures. Additionally, we propose Facts+Rules as a lightweight alternative that achieves comparable performance on complex SparQA dataset, while reducing computational overhead.Our analysis across different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini) demonstrates the framework's generalizability and provides insights into the trade-offs between implementation complexity and reasoning capability, contributing to the development of more interpretable and reliable AI systems.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e344.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e344.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek (evaluated LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized LLM evaluated in this study as a semantic parser within a DSPy-managed neural-symbolic pipeline; used to generate ASP facts/rules and refined via iterative feedback from an ASP solver for textual spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a lightweight / specialized model (Deepseek) used as one of three representative LLMs; employed as a semantic parser to convert natural-language scene descriptions and queries into ASP facts and queries, and to iteratively refine ASP programs based on solver feedback within the DSPy pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>StepGame and SparQA (textual spatial reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>StepGame: grid-based multi-hop spatial relation reasoning across 1–10 hops using nine directional relations (including overlaps); model must infer relation between two named agents given chain of relations. SparQA: complex spatial question answering over NLVR-derived scenes with three blocks and ~4 objects per block, requiring 2D/3D topological, distance, and quantifier reasoning (Yes/No, Finding Relation, Choose Object, Finding Block). Both tasks operate from textual descriptions (no raw pixels provided to the pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step spatial reasoning / multi-hop planning (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (multi-hop relations, coordinates/offsets, quantifiers / set reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>in-context examples and prompt engineering; explicit symbolic knowledge base created at runtime (ASP facts and rules generated by the LLM); DSPy optimization of prompts; underlying model pretraining (text) for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting (direct prompting baseline; Facts+Rules prompting; iterative LLM→ASP generation with 1–3 feedback rounds via DSPy), program generation and solver-feedback-based refinement</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic representations produced by the LLM: ASP predicates such as block/1, object/5, is/3, location/3, query/2; coordinate/offset-based rules for chain linking in StepGame; Facts+Rules uses natural-language-embedded logical rules instead of formal ASP code; also implicit semantic parsing ability encoded in model weights</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact match for single-choice, partial-match for multi-answer), and ASP program executability / solver execution rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>StepGame overall accuracy (DSPy pipeline): 87.7% (Deepseek); Direct prompting baseline 31.3%; Facts+Rules 44.9%. SparQA overall accuracy (DSPy): 67.2% (Deepseek); Direct baseline 59.8%; Facts+Rules 66.4%. ASP program execution rate (SparQA) increased from 45.8% to 76.8% with iterative feedback for Deepseek.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully converted NL scene descriptions to explicit ASP facts (block/1, object/5, is/3) enabling symbolic inference; excelled at multi-hop relation chaining in StepGame using coordinate-offset rules; handled quantifier/set queries when encoded as logical expressions (universal conditional) producing fast and accurate answers; iterative solver feedback substantially improved program executability and downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Frequent syntax / parsing errors in generated ASP code (largest error mode for Deepseek: ~42% of its errors were syntax-related), grounding inconsistencies when variable or predicate naming didn't match facts, failures when implicit spatial relations were not explicitly encoded, and 'satisfiable but no result' when facts/rules were insufficient; struggled with consistent predicate argument ordering in some SparQA cases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against Direct prompting: Direct overall (StepGame) 31.3% vs DSPy 87.7%; Facts+Rules (lighter-weight) 44.9% sits between. On SparQA, Direct 59.8%, Facts+Rules 66.4%, DSPy 67.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Iterative feedback effect: execution rate improved from 45.8% to 76.8% (Deepseek) after feedback rounds; most gains occurred in first feedback round with diminishing returns thereafter. Removing iterative feedback (i.e., one-shot ASP generation) produced much lower executability and accuracy (approximated by the pre-feedback execution rates and baseline accuracy figures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deepseek's strengths arise when the model's NL-to-symbol mapping is constrained into explicit ASP predicates and combined with symbolic solvers; encoding spatial relations as explicit predicates and coordinate-offset rules enables reliable multi-hop inference even without sensory input; iterative solver feedback is critical to convert LLM-generated programs into executable, accurate reasoning artifacts, chiefly by reducing parsing and grounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e344.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e344.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3 (evaluated LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose LLM evaluated here as a semantic parser within the DSPy LLM+ASP pipeline; used to generate and iteratively refine ASP programs from textual spatial descriptions to improve multi-hop spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as an intermediate-capacity LLM in experiments (described in paper as balancing performance and efficiency); served as semantic parser to produce ASP facts/rules and to engage in iterative refinement guided by Clingo solver error messages within DSPy-managed workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>StepGame and SparQA (textual spatial reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>StepGame: multi-hop grid-based relation reasoning (1–10 hops). SparQA: complex NL-derived scenes with block/object structure requiring topological/distance/quantifier reasoning across multiple question types; both tasks are solved from textual descriptions (no direct sensory input to the models).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step spatial reasoning / multi-hop planning (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (directional relations, topological/distance relations, quantifiers, multi-object set queries)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining for language understanding, in-context examples and prompt engineering, explicit symbolic rules/facts generated by the LLM (ASP), and manually designed inverse/transitive/symmetric rules injected during refinement</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot / structured prompting variants: direct prompting baseline, Facts+Rules prompting (natural language rules), and iterative code-generation & refinement in the DSPy LLM+ASP pipeline with solver feedback</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic ASP predicates (block/1, object/5, is/3, query/2), coordinate-offset reasoning for chain linking in StepGame, Facts+Rules uses natural language-embedded logical rules; also uses LLM internal language semantics to map NL to predicate forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact/partial match) and ASP program execution rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>StepGame overall accuracy (DSPy pipeline): 75.6% (Llama3); Direct prompting overall 29.3%; Facts+Rules 47.0%. SparQA overall accuracy (DSPy): 69.4% (Llama3); Direct 55.2%; Facts+Rules 63.9%. ASP execution rate on SparQA improved from 34.2% to 80.2% with iterative feedback for Llama3.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Better handling of structured predicate formats and consistent category formatting; substantial gains on Finding Relation and Finding Block tasks when facts and rules were explicitly encoded; quantifier-based queries attained improved reliability when expressed as logical conditions in ASP.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Higher rates of 'satisfiable but no result' failures on complex nested relation chains and quantification cases, indicating gaps in extracting all implicit relations; struggled more with initial one-shot ASP executability (low pre-feedback rates), and some grounding errors without tailored prompts and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Direct prompting: StepGame 29.3% vs DSPy 75.6%; Facts+Rules 47.0% sits between. On SparQA: Direct 55.2%, Facts+Rules 63.9%, DSPy 69.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Iterative feedback produced the largest executability gains (34.2% → 80.2% execution rate). The paper reports most improvement in the first feedback iteration; limiting iterations to zero or one yields substantially lower program-execution and answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Llama3 benefits substantially from explicit symbolic encodings created by prompting and from iterative solver feedback; the model encodes spatial and object-relational knowledge more reliably when constrained into predicate forms and supported by hand-designed rules (inverse/transitive/symmetric), but remains sensitive to grounding and implicit-relation extraction without extensive prompt guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e344.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e344.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.0 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.0 mini (evaluated LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact variant of GPT-4 evaluated as a semantic parser and ASP-program generator in the DSPy-based neural-symbolic pipeline; used to produce facts and refine ASP programs for textual spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.0 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A smaller GPT-4 variant (referred to as 'mini' in the paper) used to produce ASP facts and program fragments, and to iteratively refine programs based on Clingo solver feedback within DSPy-managed workflows; leveraged for both StepGame and SparQA evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>StepGame and SparQA (textual spatial reasoning benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>StepGame: grid-based multi-hop relation reasoning over up to 10 hops. SparQA: NLVR-derived scenes requiring 2D/3D/topological/distance/quantifier reasoning over blocks and objects; both tasks are processed from textual descriptions and converted into symbolic ASP facts/rules for solver-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step spatial reasoning / multi-hop planning (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (directional offsets, topological relations, quantifier/set reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on text, in-context examples and prompt engineering, LLM-generated explicit ASP facts/rules, DSPy prompt optimization and iterative refinement guided by symbolic solver feedback</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>structured prompting: direct prompting baseline, Facts+Rules prompting, and iterative LLM→ASP program generation & refinement managed by DSPy with Clingo solver feedback</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic ASP predicates (block/1, object/5, is/3, location/3, query/2); coordinate-offset chain rules for StepGame; Facts+Rules uses natural language rule application; implicit knowledge in model weights aids parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact/partial match) and ASP program executability rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>StepGame overall accuracy (DSPy pipeline): 80.8% (GPT-4.0 mini); Direct prompting overall 29.9%; Facts+Rules 47.6%. SparQA overall accuracy (DSPy): 70.3% (GPT-4.0 mini); Direct 55.5%; Facts+Rules 58.6%. ASP execution rate for GPT-4.0 mini on SparQA increased from 44.8% to 73.2% with iterative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong initial code-generation capability leading to higher-quality ASP facts in some cases; substantial gains on Finding Relation and Finding Block when explicit predicate forms and rules were used; improved handling of object/5 predicate representations aided set/quantifier queries.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Consistent issues with argument ordering in block/5 predicates causing fact-query inconsistencies; grounding errors and occasional parsing/syntax errors remained significant without iterative solver feedback; Yes/No questions saw limited benefit from full pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Direct prompting: StepGame 29.9% vs DSPy 80.8%; Facts+Rules 47.6% intermediate. On SparQA: Direct 55.5%, Facts+Rules 58.6%, DSPy 70.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Iterative feedback improved execution rate from 44.8% → 73.2% (GPT-4.0 mini) on SparQA; most gains in first round. Removing feedback or iterative refinement reduces executability and accuracy toward baseline figures.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4.0 mini shows that stronger initial program generation reduces but does not eliminate parsing/grounding failures; explicit symbolic encoding (object/5, block/1, is/3) plus solver feedback yields large accuracy gains even without any direct sensory input, indicating that textual LLMs can leverage structured symbolic intermediates to perform complex spatial and object-relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e344.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e344.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSPy LLM+ASP pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DSPy-based LLM + Answer Set Programming neural-symbolic pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular neural-symbolic framework that uses DSPy to orchestrate LLM calls to convert natural language into ASP facts/rules, iteratively refine ASP programs using solver error feedback, and run Clingo to perform symbolic spatial reasoning; designed to improve spatial and object-relational reasoning without direct sensory inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DSPy-managed LLM+ASP pipeline (applies to multiple LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline uses DSPy to define modular tasks, manage multiple LLM calls, optimize prompts and weights, and log module outputs. The pipeline stages are: (a) Facts generation (LLM → ASP facts), (b) ASP refining (iterative LLM refinement guided by Clingo error messages, up to 3 iterations), (c) Symbolic reasoning (Clingo solver), and (d) Result interpretation/mapping (synonym handling and answer selection). It also includes a knowledge module for coordinate-based reasoning (StepGame) and manually designed inverse/transitive/symmetric rules (SparQA).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language-to-ASP spatial reasoning for StepGame and SparQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Transforms textual scene descriptions and NL queries into ASP facts and queries, refines ASP programs until executable, executes with Clingo to obtain symbolic solutions, and maps solver outputs to dataset answer formats; targets multi-hop spatial relation inference, object/attribute/set queries, and quantifier reasoning without using visual/pixel input.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step spatial reasoning / neural-symbolic program synthesis for QA</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational + limited procedural (iterative program refinement and multi-hop chaining rules)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM semantic parsing (in-context and prompt-optimized), explicit hand-designed symbolic rules (inverse/transitive/symmetric, coordinate offsets), and DSPy prompt/weight optimization</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>LLM prompting for semantic parsing and program generation (Facts+Rules alternative uses natural-language rules), iterative refinement using ASP solver feedback (error messages), and DSPy-driven optimization of prompts and module interfaces</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic ASP code and predicates (block/1, object/5, is/3, location/3, query/2), coordinate-offset numeric reasoning module for StepGame, and natural-language logical rules in the Facts+Rules variant</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact/partial) across question types and datasets; ASP program executability (solver execution rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Pipeline achieves large gains over baselines: StepGame overall accuracy: Deepseek 87.7%, Llama3 75.6%, GPT-4.0 mini 80.8% (DSPy pipeline). SparQA overall: Deepseek 67.2%, Llama3 69.4%, GPT-4.0 mini 70.3% (DSPy pipeline). Execution rates on SparQA improved substantially (example: Llama3 34.2% → 80.2%). Facts+Rules is a lighter-weight alternative achieving competitive SparQA scores (e.g., Deepseek Facts+Rules 66.4% vs DSPy 67.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Converting NL into constrained predicate forms greatly increases reasoning reliability; coordinate-offset rules enable robust multi-hop chaining in StepGame; explicit logical encoding of quantifiers and set membership yields accurate quantifier reasoning; iterative solver feedback corrects syntax/grounding errors and boosts executability dramatically (most gains in the first feedback iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LLM-generated ASP code often initially contains parsing/syntax errors, unsafe variables, argument-ordering mistakes, or omitted implicit relations; grounding errors occur when variable/predicate naming or missing facts prevent solver grounding; 'satisfiable but no result' occurs when facts/rules are insufficient to answer a query; computational overhead and engineering complexity are non-trivial (multiple LLM calls per program).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Benchmarked against Direct prompting and Facts+Rules: Direct prompting yields low accuracy (StepGame overall ~29–31% across models); Facts+Rules offers moderate gains (~44–47% on StepGame, competitive on SparQA), while DSPy pipeline yields substantially higher accuracy on StepGame and modest-to-large gains on SparQA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablation: removing iterative feedback (or reducing rounds) lowers ASP executability and downstream accuracy (pre-feedback execution rates are much lower); major reported numbers: execution rate improvements on SparQA — Deepseek 45.8%→76.8%, Llama3 34.2%→80.2%, GPT-4.0 mini 44.8%→73.2% over feedback iterations. Limiting or omitting coordinate-based knowledge module reduces StepGame multi-hop performance (pipeline depends on explicit chain-link offset rules).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When LLMs operate without sensory input, encoding spatial and object-relational knowledge as explicit symbolic predicates (ASP) and using an iterative feedback loop with a solver is highly effective: it converts implicit language knowledge into executable symbolic representations, fixes parsing/grounding errors, and enables accurate multi-hop spatial and quantifier reasoning. Facts+Rules is a lower-cost alternative that achieves competitive results on complex QA but the full LLM+ASP pipeline attains best accuracy, particularly on deep multi-hop benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dspy: Compiling declarative language model calls into selfimproving pipelines <em>(Rating: 2)</em></li>
                <li>A neuro-symbolic ASP pipeline for visual question answering <em>(Rating: 2)</em></li>
                <li>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts <em>(Rating: 2)</em></li>
                <li>Transfer learning with synthetic corpora for spatial role labeling and reasoning <em>(Rating: 2)</em></li>
                <li>Leveraging large language models to generate answer set programs <em>(Rating: 2)</em></li>
                <li>Coupling large language models with logic programming for robust and general reasoning from text <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-344",
    "paper_id": "paper-274306413",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Deepseek",
            "name_full": "Deepseek (evaluated LLM)",
            "brief_description": "A specialized LLM evaluated in this study as a semantic parser within a DSPy-managed neural-symbolic pipeline; used to generate ASP facts/rules and refined via iterative feedback from an ASP solver for textual spatial reasoning tasks.",
            "citation_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
            "mention_or_use": "use",
            "model_name": "Deepseek",
            "model_size": null,
            "model_description": "Described as a lightweight / specialized model (Deepseek) used as one of three representative LLMs; employed as a semantic parser to convert natural-language scene descriptions and queries into ASP facts and queries, and to iteratively refine ASP programs based on solver feedback within the DSPy pipeline.",
            "task_name": "StepGame and SparQA (textual spatial reasoning benchmarks)",
            "task_description": "StepGame: grid-based multi-hop spatial relation reasoning across 1–10 hops using nine directional relations (including overlaps); model must infer relation between two named agents given chain of relations. SparQA: complex spatial question answering over NLVR-derived scenes with three blocks and ~4 objects per block, requiring 2D/3D topological, distance, and quantifier reasoning (Yes/No, Finding Relation, Choose Object, Finding Block). Both tasks operate from textual descriptions (no raw pixels provided to the pipeline).",
            "task_type": "multi-step spatial reasoning / multi-hop planning (textual)",
            "knowledge_type": "spatial + object-relational (multi-hop relations, coordinates/offsets, quantifiers / set reasoning)",
            "knowledge_source": "in-context examples and prompt engineering; explicit symbolic knowledge base created at runtime (ASP facts and rules generated by the LLM); DSPy optimization of prompts; underlying model pretraining (text) for language understanding",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompting (direct prompting baseline; Facts+Rules prompting; iterative LLM→ASP generation with 1–3 feedback rounds via DSPy), program generation and solver-feedback-based refinement",
            "knowledge_representation": "explicit symbolic representations produced by the LLM: ASP predicates such as block/1, object/5, is/3, location/3, query/2; coordinate/offset-based rules for chain linking in StepGame; Facts+Rules uses natural-language-embedded logical rules instead of formal ASP code; also implicit semantic parsing ability encoded in model weights",
            "performance_metric": "accuracy (exact match for single-choice, partial-match for multi-answer), and ASP program executability / solver execution rate",
            "performance_result": "StepGame overall accuracy (DSPy pipeline): 87.7% (Deepseek); Direct prompting baseline 31.3%; Facts+Rules 44.9%. SparQA overall accuracy (DSPy): 67.2% (Deepseek); Direct baseline 59.8%; Facts+Rules 66.4%. ASP program execution rate (SparQA) increased from 45.8% to 76.8% with iterative feedback for Deepseek.",
            "success_patterns": "Successfully converted NL scene descriptions to explicit ASP facts (block/1, object/5, is/3) enabling symbolic inference; excelled at multi-hop relation chaining in StepGame using coordinate-offset rules; handled quantifier/set queries when encoded as logical expressions (universal conditional) producing fast and accurate answers; iterative solver feedback substantially improved program executability and downstream accuracy.",
            "failure_patterns": "Frequent syntax / parsing errors in generated ASP code (largest error mode for Deepseek: ~42% of its errors were syntax-related), grounding inconsistencies when variable or predicate naming didn't match facts, failures when implicit spatial relations were not explicitly encoded, and 'satisfiable but no result' when facts/rules were insufficient; struggled with consistent predicate argument ordering in some SparQA cases.",
            "baseline_comparison": "Compared against Direct prompting: Direct overall (StepGame) 31.3% vs DSPy 87.7%; Facts+Rules (lighter-weight) 44.9% sits between. On SparQA, Direct 59.8%, Facts+Rules 66.4%, DSPy 67.2%.",
            "ablation_results": "Iterative feedback effect: execution rate improved from 45.8% to 76.8% (Deepseek) after feedback rounds; most gains occurred in first feedback round with diminishing returns thereafter. Removing iterative feedback (i.e., one-shot ASP generation) produced much lower executability and accuracy (approximated by the pre-feedback execution rates and baseline accuracy figures reported).",
            "key_findings": "Deepseek's strengths arise when the model's NL-to-symbol mapping is constrained into explicit ASP predicates and combined with symbolic solvers; encoding spatial relations as explicit predicates and coordinate-offset rules enables reliable multi-hop inference even without sensory input; iterative solver feedback is critical to convert LLM-generated programs into executable, accurate reasoning artifacts, chiefly by reducing parsing and grounding errors.",
            "uuid": "e344.0",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Llama3",
            "name_full": "Llama3 (evaluated LLM)",
            "brief_description": "A general-purpose LLM evaluated here as a semantic parser within the DSPy LLM+ASP pipeline; used to generate and iteratively refine ASP programs from textual spatial descriptions to improve multi-hop spatial reasoning.",
            "citation_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
            "mention_or_use": "use",
            "model_name": "Llama3",
            "model_size": null,
            "model_description": "Used as an intermediate-capacity LLM in experiments (described in paper as balancing performance and efficiency); served as semantic parser to produce ASP facts/rules and to engage in iterative refinement guided by Clingo solver error messages within DSPy-managed workflows.",
            "task_name": "StepGame and SparQA (textual spatial reasoning benchmarks)",
            "task_description": "StepGame: multi-hop grid-based relation reasoning (1–10 hops). SparQA: complex NL-derived scenes with block/object structure requiring topological/distance/quantifier reasoning across multiple question types; both tasks are solved from textual descriptions (no direct sensory input to the models).",
            "task_type": "multi-step spatial reasoning / multi-hop planning (textual)",
            "knowledge_type": "spatial + object-relational (directional relations, topological/distance relations, quantifiers, multi-object set queries)",
            "knowledge_source": "pretraining for language understanding, in-context examples and prompt engineering, explicit symbolic rules/facts generated by the LLM (ASP), and manually designed inverse/transitive/symmetric rules injected during refinement",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot / structured prompting variants: direct prompting baseline, Facts+Rules prompting (natural language rules), and iterative code-generation & refinement in the DSPy LLM+ASP pipeline with solver feedback",
            "knowledge_representation": "explicit symbolic ASP predicates (block/1, object/5, is/3, query/2), coordinate-offset reasoning for chain linking in StepGame, Facts+Rules uses natural language-embedded logical rules; also uses LLM internal language semantics to map NL to predicate forms.",
            "performance_metric": "accuracy (exact/partial match) and ASP program execution rate",
            "performance_result": "StepGame overall accuracy (DSPy pipeline): 75.6% (Llama3); Direct prompting overall 29.3%; Facts+Rules 47.0%. SparQA overall accuracy (DSPy): 69.4% (Llama3); Direct 55.2%; Facts+Rules 63.9%. ASP execution rate on SparQA improved from 34.2% to 80.2% with iterative feedback for Llama3.",
            "success_patterns": "Better handling of structured predicate formats and consistent category formatting; substantial gains on Finding Relation and Finding Block tasks when facts and rules were explicitly encoded; quantifier-based queries attained improved reliability when expressed as logical conditions in ASP.",
            "failure_patterns": "Higher rates of 'satisfiable but no result' failures on complex nested relation chains and quantification cases, indicating gaps in extracting all implicit relations; struggled more with initial one-shot ASP executability (low pre-feedback rates), and some grounding errors without tailored prompts and examples.",
            "baseline_comparison": "Direct prompting: StepGame 29.3% vs DSPy 75.6%; Facts+Rules 47.0% sits between. On SparQA: Direct 55.2%, Facts+Rules 63.9%, DSPy 69.4%.",
            "ablation_results": "Iterative feedback produced the largest executability gains (34.2% → 80.2% execution rate). The paper reports most improvement in the first feedback iteration; limiting iterations to zero or one yields substantially lower program-execution and answer accuracy.",
            "key_findings": "Llama3 benefits substantially from explicit symbolic encodings created by prompting and from iterative solver feedback; the model encodes spatial and object-relational knowledge more reliably when constrained into predicate forms and supported by hand-designed rules (inverse/transitive/symmetric), but remains sensitive to grounding and implicit-relation extraction without extensive prompt guidance.",
            "uuid": "e344.1",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4.0 mini",
            "name_full": "GPT-4.0 mini (evaluated LLM)",
            "brief_description": "A compact variant of GPT-4 evaluated as a semantic parser and ASP-program generator in the DSPy-based neural-symbolic pipeline; used to produce facts and refine ASP programs for textual spatial reasoning tasks.",
            "citation_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-4.0 mini",
            "model_size": null,
            "model_description": "A smaller GPT-4 variant (referred to as 'mini' in the paper) used to produce ASP facts and program fragments, and to iteratively refine programs based on Clingo solver feedback within DSPy-managed workflows; leveraged for both StepGame and SparQA evaluations.",
            "task_name": "StepGame and SparQA (textual spatial reasoning benchmarks)",
            "task_description": "StepGame: grid-based multi-hop relation reasoning over up to 10 hops. SparQA: NLVR-derived scenes requiring 2D/3D/topological/distance/quantifier reasoning over blocks and objects; both tasks are processed from textual descriptions and converted into symbolic ASP facts/rules for solver-based reasoning.",
            "task_type": "multi-step spatial reasoning / multi-hop planning (textual)",
            "knowledge_type": "spatial + object-relational (directional offsets, topological relations, quantifier/set reasoning)",
            "knowledge_source": "pretraining on text, in-context examples and prompt engineering, LLM-generated explicit ASP facts/rules, DSPy prompt optimization and iterative refinement guided by symbolic solver feedback",
            "has_direct_sensory_input": false,
            "elicitation_method": "structured prompting: direct prompting baseline, Facts+Rules prompting, and iterative LLM→ASP program generation & refinement managed by DSPy with Clingo solver feedback",
            "knowledge_representation": "explicit symbolic ASP predicates (block/1, object/5, is/3, location/3, query/2); coordinate-offset chain rules for StepGame; Facts+Rules uses natural language rule application; implicit knowledge in model weights aids parsing.",
            "performance_metric": "accuracy (exact/partial match) and ASP program executability rate",
            "performance_result": "StepGame overall accuracy (DSPy pipeline): 80.8% (GPT-4.0 mini); Direct prompting overall 29.9%; Facts+Rules 47.6%. SparQA overall accuracy (DSPy): 70.3% (GPT-4.0 mini); Direct 55.5%; Facts+Rules 58.6%. ASP execution rate for GPT-4.0 mini on SparQA increased from 44.8% to 73.2% with iterative feedback.",
            "success_patterns": "Strong initial code-generation capability leading to higher-quality ASP facts in some cases; substantial gains on Finding Relation and Finding Block when explicit predicate forms and rules were used; improved handling of object/5 predicate representations aided set/quantifier queries.",
            "failure_patterns": "Consistent issues with argument ordering in block/5 predicates causing fact-query inconsistencies; grounding errors and occasional parsing/syntax errors remained significant without iterative solver feedback; Yes/No questions saw limited benefit from full pipeline.",
            "baseline_comparison": "Direct prompting: StepGame 29.9% vs DSPy 80.8%; Facts+Rules 47.6% intermediate. On SparQA: Direct 55.5%, Facts+Rules 58.6%, DSPy 70.3%.",
            "ablation_results": "Iterative feedback improved execution rate from 44.8% → 73.2% (GPT-4.0 mini) on SparQA; most gains in first round. Removing feedback or iterative refinement reduces executability and accuracy toward baseline figures.",
            "key_findings": "GPT-4.0 mini shows that stronger initial program generation reduces but does not eliminate parsing/grounding failures; explicit symbolic encoding (object/5, block/1, is/3) plus solver feedback yields large accuracy gains even without any direct sensory input, indicating that textual LLMs can leverage structured symbolic intermediates to perform complex spatial and object-relational reasoning.",
            "uuid": "e344.2",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "DSPy LLM+ASP pipeline",
            "name_full": "DSPy-based LLM + Answer Set Programming neural-symbolic pipeline",
            "brief_description": "A modular neural-symbolic framework that uses DSPy to orchestrate LLM calls to convert natural language into ASP facts/rules, iteratively refine ASP programs using solver error feedback, and run Clingo to perform symbolic spatial reasoning; designed to improve spatial and object-relational reasoning without direct sensory inputs.",
            "citation_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
            "mention_or_use": "use",
            "model_name": "DSPy-managed LLM+ASP pipeline (applies to multiple LLMs)",
            "model_size": null,
            "model_description": "Pipeline uses DSPy to define modular tasks, manage multiple LLM calls, optimize prompts and weights, and log module outputs. The pipeline stages are: (a) Facts generation (LLM → ASP facts), (b) ASP refining (iterative LLM refinement guided by Clingo error messages, up to 3 iterations), (c) Symbolic reasoning (Clingo solver), and (d) Result interpretation/mapping (synonym handling and answer selection). It also includes a knowledge module for coordinate-based reasoning (StepGame) and manually designed inverse/transitive/symmetric rules (SparQA).",
            "task_name": "Language-to-ASP spatial reasoning for StepGame and SparQA",
            "task_description": "Transforms textual scene descriptions and NL queries into ASP facts and queries, refines ASP programs until executable, executes with Clingo to obtain symbolic solutions, and maps solver outputs to dataset answer formats; targets multi-hop spatial relation inference, object/attribute/set queries, and quantifier reasoning without using visual/pixel input.",
            "task_type": "multi-step spatial reasoning / neural-symbolic program synthesis for QA",
            "knowledge_type": "spatial + object-relational + limited procedural (iterative program refinement and multi-hop chaining rules)",
            "knowledge_source": "LLM semantic parsing (in-context and prompt-optimized), explicit hand-designed symbolic rules (inverse/transitive/symmetric, coordinate offsets), and DSPy prompt/weight optimization",
            "has_direct_sensory_input": false,
            "elicitation_method": "LLM prompting for semantic parsing and program generation (Facts+Rules alternative uses natural-language rules), iterative refinement using ASP solver feedback (error messages), and DSPy-driven optimization of prompts and module interfaces",
            "knowledge_representation": "explicit symbolic ASP code and predicates (block/1, object/5, is/3, location/3, query/2), coordinate-offset numeric reasoning module for StepGame, and natural-language logical rules in the Facts+Rules variant",
            "performance_metric": "accuracy (exact/partial) across question types and datasets; ASP program executability (solver execution rate)",
            "performance_result": "Pipeline achieves large gains over baselines: StepGame overall accuracy: Deepseek 87.7%, Llama3 75.6%, GPT-4.0 mini 80.8% (DSPy pipeline). SparQA overall: Deepseek 67.2%, Llama3 69.4%, GPT-4.0 mini 70.3% (DSPy pipeline). Execution rates on SparQA improved substantially (example: Llama3 34.2% → 80.2%). Facts+Rules is a lighter-weight alternative achieving competitive SparQA scores (e.g., Deepseek Facts+Rules 66.4% vs DSPy 67.2%).",
            "success_patterns": "Converting NL into constrained predicate forms greatly increases reasoning reliability; coordinate-offset rules enable robust multi-hop chaining in StepGame; explicit logical encoding of quantifiers and set membership yields accurate quantifier reasoning; iterative solver feedback corrects syntax/grounding errors and boosts executability dramatically (most gains in the first feedback iteration).",
            "failure_patterns": "LLM-generated ASP code often initially contains parsing/syntax errors, unsafe variables, argument-ordering mistakes, or omitted implicit relations; grounding errors occur when variable/predicate naming or missing facts prevent solver grounding; 'satisfiable but no result' occurs when facts/rules are insufficient to answer a query; computational overhead and engineering complexity are non-trivial (multiple LLM calls per program).",
            "baseline_comparison": "Benchmarked against Direct prompting and Facts+Rules: Direct prompting yields low accuracy (StepGame overall ~29–31% across models); Facts+Rules offers moderate gains (~44–47% on StepGame, competitive on SparQA), while DSPy pipeline yields substantially higher accuracy on StepGame and modest-to-large gains on SparQA.",
            "ablation_results": "Key ablation: removing iterative feedback (or reducing rounds) lowers ASP executability and downstream accuracy (pre-feedback execution rates are much lower); major reported numbers: execution rate improvements on SparQA — Deepseek 45.8%→76.8%, Llama3 34.2%→80.2%, GPT-4.0 mini 44.8%→73.2% over feedback iterations. Limiting or omitting coordinate-based knowledge module reduces StepGame multi-hop performance (pipeline depends on explicit chain-link offset rules).",
            "key_findings": "When LLMs operate without sensory input, encoding spatial and object-relational knowledge as explicit symbolic predicates (ASP) and using an iterative feedback loop with a solver is highly effective: it converts implicit language knowledge into executable symbolic representations, fixes parsing/grounding errors, and enables accurate multi-hop spatial and quantifier reasoning. Facts+Rules is a lower-cost alternative that achieves competitive results on complex QA but the full LLM+ASP pipeline attains best accuracy, particularly on deep multi-hop benchmarks.",
            "uuid": "e344.3",
            "source_info": {
                "paper_title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dspy: Compiling declarative language model calls into selfimproving pipelines",
            "rating": 2,
            "sanitized_title": "dspy_compiling_declarative_language_model_calls_into_selfimproving_pipelines"
        },
        {
            "paper_title": "A neuro-symbolic ASP pipeline for visual question answering",
            "rating": 2,
            "sanitized_title": "a_neurosymbolic_asp_pipeline_for_visual_question_answering"
        },
        {
            "paper_title": "Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts",
            "rating": 2,
            "sanitized_title": "stepgame_a_new_benchmark_for_robust_multihop_spatial_reasoning_in_texts"
        },
        {
            "paper_title": "Transfer learning with synthetic corpora for spatial role labeling and reasoning",
            "rating": 2,
            "sanitized_title": "transfer_learning_with_synthetic_corpora_for_spatial_role_labeling_and_reasoning"
        },
        {
            "paper_title": "Leveraging large language models to generate answer set programs",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_to_generate_answer_set_programs"
        },
        {
            "paper_title": "Coupling large language models with logic programming for robust and general reasoning from text",
            "rating": 2,
            "sanitized_title": "coupling_large_language_models_with_logic_programming_for_robust_and_general_reasoning_from_text"
        }
    ],
    "cost": 0.015917999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs
12 Dec 2024</p>
<p>Rong Wang 
Kun Sun 
TübingenGermany</p>
<p>Jonas Kuhn 
The Institute of Natural Language Processing
Stuttgart University
StuttgartGermany</p>
<p>Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs
12 Dec 202479780BC935B8FF38FD83CBFB71135F92arXiv:2411.18564v2[cs.AI]
Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often struggle with spatial reasoning.This paper presents a novel neural-symbolic framework that enhances LLMs' spatial reasoning abilities through iterative feedback between LLMs and Answer Set Programming (ASP).We evaluate our approach on two benchmark datasets: StepGame and SparQA, implementing three distinct strategies: (1) direct prompting baseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with iterative refinement.Our experimental results demonstrate that the LLM+ASP pipeline significantly outperforms baseline methods, achieving an average 82% accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and 8-15% respectively over direct prompting.The success stems from three key innovations: (1) effective separation of semantic parsing and logical reasoning through a modular pipeline, (2) iterative feedback mechanism between LLMs and ASP solvers that improves program executability rate, and (3) robust error handling that addresses parsing, grounding, and solving failures.Additionally, we propose Facts+Rules as a lightweight alternative that achieves comparable performance on complex SparQA dataset, while reducing computational overhead.Our analysis across different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini) demonstrates the framework's generalizability and provides insights into the trade-offs between implementation complexity and reasoning capability, contributing to the development of more interpretable and reliable AI systems.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are known for their impressive performance across a range of tasks, demonstrating certain commonsense reasoning abilities.However, since LLMs are trained to predict subsequent words in a sequence, they seem to lack sufficient grounding to excel at tasks requiring spatial, physical, and embodied reasoning.Spatial reasoning, the ability to understand and manipulate relationships between objects in two-and threedimensional spaces, represents a crucial component of artificial intelligence systems, enabling practical applications in robotics, navigation, and physical task planning.Recent studies (Bang et al., 2023;Cohn, 2023) highlighted the limitations of models like ChatGPT in deductive logical reasoning, spatial reasoning, and non-textual semantic reasoning, underlining the need for further improvements in spatial reasoning.</p>
<p>Efforts to improve logical reasoning capability have focused on advanced prompting techniques.However, these methods have demonstrated notable limitations, particularly on challenging datasets like StepGame (Shi et al., 2022) and SparQA (Mirzaee and Kordjamshidi, 2022), which often require multi-step planning and complex natural language understanding.In these scenarios, LLMs often struggle with maintaining coherence, frequently hallucinating or losing sight of the original objectives, resulting in inaccurate and unreliable outputs.More recent work demonstrates that augmenting large language models (LLMs) with external tools for arithmetic, navigation, and knowledge lookups improves performance (Fang et al., 2024).Notably, neural-symbolic approaches, where LLMs extract facts while external symbolic solvers handle reasoning, show significant promise in enhancing logical inferenceg) (Yang et al., 2023a).Nevertheless, existing neural-symbolic methods face limitations in generalization, scalability, and comprehensive evaluation.(Yu et al, 2021) Neural-symbolic systems have been applied successfully in visual question answering and robotics (Yang et al., 2023a).These approaches often fail to fully harness LLMs' potential, particularly by omitting feedback loops between multiple LLMs and symbolic modules, limiting performance gains.</p>
<p>To overcome these challenges, this paper proposes a novel neural-symbolic framework that systematically integrates symbolic reasoning components with LLMs.Our approach leverages strategic prompting, feedback loops, and Answer Set Programming (ASP)-based verification, creating a robust pipeline that improves spatial reasoning across different LLM architectures.We evaluate this framework on two benchmark datasets, By integrating feedback loops and verification module, our methodology demonstrates strong generalizability when tackling complex spatial reasoning tasks, showing sig-nificant performance gains, with average accuracy improvements of 40% on StepGame and 20% on SparQA.These outcomes underscore the effectiveness of neural-symbolic integration in enhancing spatial reasoning and emphasize the importance of task-specific implementation strategies.</p>
<p>To address these limitations, we propose a pipeline that effectively enhances LLMs' spatial reasoning capabilities.Our approach combines strategic prompting with symbolic reasoning to create a robust framework that significantly improves performance of spatial reasoning across different LLM architectures.Specifically, building on these insights, we propose a novel neural-symbolic pipeline that integrates LLMs with ASP, aimed at enhancing the LLMs' capabilities of spatial reasoning in SparQA and StepGame datasets.We investigate the potential benefits of integrating symbolic reasoning components into LLMs to further boost their spatial reasoning capabilities.Within the broader field of neural-symbolic AI, the combination of LLMs as parsers with ASP solvers has emerged as a particularly effective approach for complex reasoning tasks.Additionally, the present study is one of the pioneering projects which uses DSPy (Khattab et al., 2023) to pipeline and program LLM.</p>
<p>In the broader field of neural-symbolic AI, combining large language models (LLMs) with symbolic reasoning systems like Prolog has proven successful for tackling complex reasoning tasks (Hamilton et al., 2022;Besold et al., 2021).Our study advances this field by demonstrating how incorporating symbolic reasoning modules can enhance LLMs' spatial reasoning capabilities.Specifically, we introduce a modular pipeline using DSPy, enabling seamless interaction between LLMs and symbolic system solvers.Our pipeline not only improves spatial reasoning but also shows promise in broader domains requiring structured logic, thereby contributing to both the theoretical foundations and practical applications of neural-symbolic integration.</p>
<p>Preliminaries</p>
<p>Prompting LLMs for spatial reasoning</p>
<p>Research on spatial reasoning spans both visual and textual domains, each with its own challenges.In Visual Spatial Reasoning (VSR), even advanced models like CLIP (Radford et al., 2021) face limitations in comprehending complex spatial relationships.In the textual domain, spatial reasoning is further complicated by linguistic ambiguity.</p>
<p>Prompting strategies have played a pivotal role in improving LLMs' reasoning capabilities.Approaches such as chain of thought (CoT) (Wei et al., 2022;Chu et al., 2023), few-shot prompting (Schick and Schütze, 2021), and least-to-most prompting (Zhou et al., 2022) enable LLMs to tackle complex problems by breaking them down into logical steps.Self-consistency methods (Wang et al., 2022), further refine reasoning by aggregating multiple solutions to improve accuracy.Recent techniques like like tree of thoughts, visualization of thought (Wang et al., 2023), and program-aided language models (Gao et al., 2022) provide structured, interpretable reasoning pathways.</p>
<p>These prompting strategies demonstrate the potential for LLMs to handle spatial reasoning, but they also highlight the need for more robust methods to address limitations in multi-hop inference and natural language ambiguity.Our approach integrates prompting strategies with symbolic reasoning to enhance LLMs' spatial reasoning capabilities more comprehensively.</p>
<p>Effective Neural-Symbolic Integration</p>
<p>Despite the success of deep learning, its limitations in reasoning, generalization and interpretability have spurred interest in hybrid approaches (Garcez and Lamb, 2023;Hamilton et al., 2022).Neural-symbolic integration aims to combine the strengths of neural networks (pattern recognition and uncertainty handling) with symbolic systems (logical reasoning and knowledge representation).Based on how neural and symbolic components interact and complement each other, research (Wan et al., 2024) has identified key integration patterns: enhancing symbolic systems with neural capabilities (Symbolic[Neural]), combining neural learning with symbolic inference (Neural|Symbolic), compiling symbolic rules into neural structures (Neural:Symbolic → Neural), integrating logic rules through embeddings (Neural ⊗ Symbolic), and augmenting neural systems with symbolic reasoning (Neural[Symbolic]).These patterns can be grouped into practical architectures, including sequential, iterative, and embedded approaches, as well as LLM-based tool integration (Weber et al., 2019;Riegel et al., 2020;Parisi et al., 2022).</p>
<p>The emergence of LLMs has introduced new possibilities for neural-symbolic integration.Modern approaches leverage LLMs as powerful semantic parsers and natural language interfaces, while delegating the reasoning task to external off-the-shelf symbolic reasoners or solvers.The integration typically follows a two-step process: first, the LLM translates natural language queries into formal logical representations (e.g., Prolog predicates or ASP rules), then the logical solver performs structured reasoning to derive answers.However, this approach faces significant challenges in ensuring accurate and consistent mapping between natural language and logical forms.LLMs are not trained on logical programming language and they tend to generate logically inconsistent or syntactically incorrect programs, and the in-executable logical program will lead to the collpase of the whole neural symbolic system.For some complex and realistic dataset, the parsing succesful rate can be as low as 17% (Feng et al., 2024).To addrress this, researchers (Pan et al., 2023) have proposed self-refinement modules to refine the parsing results based on the solver's erroneous messageses; however, the improvement of accuracy is not obvious with only 5% after 3 iterations, highlighting the need for a more efficient architecture to enable sophisticated and robust feedback loop between LLMs and symbolic solvers.</p>
<p>This study leverages Dspy framework to construct a novel neural-symbolic pipeline that integrates LLMs with Answer Set Programming (ASP) in an iterative manner.Through its modular architecture and systematic optimization approach, DSPy can address many of the limitations faced by previous neural-symbolic integration attempts.By defining clear input-output signatures for each module in the pipeline, the framework ensures that the generated ASP code maintains consistent structure and enable the error feedback verification modules working in concert.Dspy's optimization compiler iteratively refines both the prompting strategies and weights used in each stage, ensuring consistent performance improvement, enabling more robust and adaptable neural-symbolic systems.</p>
<p>Answer Set Programming</p>
<p>Answer Set Programming (ASP) is a declarative programming paradigm that leverages predicate logic and stable model semantics to address complex combinational search tasks.Unlike procedural programming, which specifies step-by-step computations, ASP allows users to define solutions through logical relationships, enabling the solver to autonomously determine how to satisfy these conditions (Brewka et al., 2011).ASP is case-sensitive, utilizing uppercase letters for variables and lowercase letters for constants, including atoms and predicates.The underscore character "_" serves as a "don't care" variable that can be instantiated by any value.However, using placeholders in rule heads without corresponding definitions can lead to unsafe variable errors due to ambiguity during grounding.</p>
<p>ASP is closely related to Prolog, having evolved from it and sharing foundational concepts.Both languages utilize logical rules and facts for knowledge representation.However, while Prolog emphasizes procedural query evaluation, ASP focuses on generating answer sets -collections of ground atoms that satisfy the program's rules.This distinction allows ASP to handle nonmonotonic reasoning more effectively, accommodating multiple stable models for a single program, which is less common in Prolog.The paradigm's expressive power derives from four fundamental components, each serving distinct roles in knowledge representation and reasoning:</p>
<p>Facts: Basic units of knowledge represented as unconditional logical atoms p(t1, ..., tn), where p is a predicate symbol and t1, ..., tn are terms.In spatial reasoning, facts typically represent object properties, locations, and basic spatial relationships.</p>
<p>Rules: Logical implications of the form "Head :-Body", where the Head is an atom and the Body consists of literals.Rules enable inference, allowing new knowledge to be derived from existing facts.</p>
<p>Constraints: Special rules expressed as ":-Body" that eliminate answer sets violating specific conditions, thus serving as integrity checks within the knowledge base.</p>
<p>Queries: Goal-directed statements that extract information from the knowledge base, implemented as special predicates whose extensions yield the requested data.</p>
<p>In spatial reasoning contexts, ASP provides a robust framework for representing both static and dynamic spatial relationships through foundational predicates such as block/1 for defining spatial regions, object/5 for specifying object properties (identifier, size, color, shape, location), is/3 for expressing spatial relationships, and location/3 for precise coordinate definitions.This combination of predicates with ASP's logical foundations enables sophisticated reasoning about spatial configurations and relationships.</p>
<p>Methods</p>
<p>Datasets</p>
<p>StepGame (Shi et al., 2022) StepGame is a dataset designed for robust multi-hop spatial reasoning using a grid-based system, featuring eight directional spatial relations: top (north), down (south), left (west), right (east), top-left (north-west), topright (north-east), down-left (south-west), and down-right (south-east).Each relation is defined by specific angles and distances for detailed visual representation, and it includes an "overlap" relation for scenarios where objects share the same location.The dataset supports reasoning hops from 1 to 10 across 10 subsets, each containing 10,000 samples corresponding to a single reasoning hop.However, the single Finding Relation question type may not fully capture the complexities of real-world spatial reasoning, as research indicates that large language models (LLMs) often struggle more with constructing object-linking chains from shuffled relations than with the spatial reasoning tasks themselves.Additionally, prior studies have identified template errors within StepGame that may skew model performance evaluations due to inadequate quality control during crowdsourcing, leading to inaccuracies in relationship mappings that can misrepresent an LLM's spatial reasoning abilities.</p>
<p>SpartQA/SparTUN(Mirzaee and Kordjamshidi, 2022) SpartQA is built upon the NLVR (Natural Language for Visual Reasoning) images, featuring synthetically generated scenes depicting various spatial arrangements.Typically, each scenario consists of three blocks arranged either vertically or horizontally, with each block containing around four objects characterized by attributes like size, color, and shape.Besides, the dataset incorporates a wider range of spatial relationships, including 3D spatial reasoning, topological relations and distance relations.For FR questions, the candidate choices include ['left', 'right', 'above', 'below', 'near to', 'far from', 'touching', 'DK'], but there are more synonym relation names involved in the context and question.</p>
<p>SparQA differs significantly from StepGame in its complexity and scope.Its language structure is 2.5 times longer, featuring three blocks with approximately four objects each.The dataset handles multiple question types including Yes/No, FR, CO, and FB, with questions often involving complex relationships between multiple objects.For example, "What is the relation between the blue circle touching the top edge of block B and the small square?"Except Yes/No questions, FR, CO and FB questions often have more than one answer.SparQA incorporates 3D spatial reasoning, topological relations, and distance relations, with options like left, above, and near to.</p>
<p>A distinctive feature of SparQA is its focus on quantifier-based reasoning, with questions that test higher-level logic through universal ("all") and exclusive ("only") statements.For example, one third questions process queries like "Are all of the squares in B?" and "Which block has only small black things inside?"These questions require comprehensive evaluation of object sets and their properties rather than simple relational comparisons.</p>
<p>By utilizing these diverse datasets and question types, we aim to assess the spatial reasoning capabilities of LLMs across various complexity levels and spatial relation types.</p>
<p>LLM + ASP pipeline with DSPy</p>
<p>Recent studies have demonstrated LLMs' effectiveness as semantic parsers, often surpassing traditional parsing tools.While Geibinger (2023) and Eiter et al. (2022) showed promising results integrating LLMs with ASP, challenges remain.Ishay et al. (2023) found LLMs could generate complex ASP programs but often with errors, while Yang et al. (2023a) achieved 90% accuracy on StepGame using GPT-3 and ASP, their method's scalability remains uncertain.2023)'s LOGIC-LM framework and integration neural-symbolic strategies, we propose a novel neural-symbolic pipeline employing ASP using DSPy that treats the LLM as an agent capable of feedback and iteration.DSPy is a Python framework that uses a declarative and selfimproving approach to simplify working with LLMs (Khattab et al., 2023).It automates the optimization of prompts and model tuning, enhancing reliability and scalability in AI applications.By defining tasks and metrics rather than manual prompts, DSPy streamlines the development of various NLP tasks and complex AI systems.The framework of this pipeline is shown in Fig. 2.</p>
<p>The pipeline consists of four main stages: a) Facts Generation Stage: LLM converts natural language descriptions into symbolic formulations and formal queries.b) ASP Refining Stage: LLM iteratively refines the ASP representation over three iterations, adding rules, checking consistency, and incorporating feedback from error messages.c) Symbolic Reasoning Stage: The refined ASP undergoes inference using the Clingo solver, ensuring accurate and explainable reasoning by combining LLM capabilities with logical inference.d) Result Interpretation and Evaluation: This stage involves mapping the Clingo solver's outputs to candidate answers.For cer-tain question types, like Yes/No and Finding-Block questions, the solver's output can directly serve as the correct answer.However, for Finding Relations and Choose Object questions, additional processing is necessary to filter relevant solutions.Outputs from the ASP solver are evaluated against a synonym dictionary to determine the accuracy.</p>
<p>Overall, this pipeline requires multiple interactions with LLMs during ASP generation and refinement.We employ the DSPy framework to manage these complex workflows (e.g., interfacing with Llama3 60B DeepSeek and GPT 4.0 mini models via their APIs).DSPy's modular features enhance memory retention between modules, enabling adjustments and optimizations while maintaining workflow integrity.</p>
<p>Additionally, DSPy optimizes LLM prompts and weights, reducing the need for manual prompt engineering and ensuring consistent performance across datasets.Its optimization compiler iteratively generates and refines prompts, enhancing task performance.To support transparency and debugging, outputs from all modules are logged, capturing errors and providing insights for prompt engineering and system optimization, facilitating continuous improvement of the system and enhancing the integration of neural and symbolic components.In this way, this integrated neural-symbolic pipeline could greatly facilitate spatial reasoning in LLMs.</p>
<p>To evaluate the effectiveness of our approaches in spatial reasoning comprehensively, we selected three representative LLMs with diverse architectures and capabilities: DeepSeek, Llama3, and GPT4.0 Mini.These LLMs were chosen to ensure a comprehensive assessment across different types of language representations, ranging from lightweight and specialized models like DeepSeek to more advanced general-purpose systems like GPT4.0 Mini.Llama3, known for its balance between performance and computational efficiency, provides an intermediate perspective.By testing our methods on these distinct models, we want to demonstrate the adaptability and robustness of our approach across a variety of LLM architectures and reasoning capacities.</p>
<p>Given the task nature of multiple choices, we primarily employed exact match metrics for single-choice questions and partial match metrics for multiple-choice questions, aligning with the specific requirements of spatial reasoning tasks.To ensure accurate evaluation, we implemented custom post-processing to normalize responses and developed specialized metrics to handle both exact and partial matches between model outputs and ground truth.</p>
<p>Baselines</p>
<p>Direct prompting</p>
<p>As the most straightforward approach, it involves presenting the task to the LLM without additional guidance.The typical form is " Given the question, please answer: While simple, this method is believed not to elicit the model's full potential, particularly in complex reasoning tasks".This straightforward approach involves presenting the task to the LLM without additional guidance.The basic structure consists of a simple template: "Given the context and question, please answer the question by choosing from the choices".While minimal, this method serves as a crucial baseline for model evaluation as it reflects the model's fundamental ability to handle spatial tasks without any reasoning scaffolds.Despite its simplicity, this approach can achieve competitive results, particularly with well-trained models that have developed robust internal reasoning mechanisms.</p>
<p>Facts + Rule prompting</p>
<p>We explore an alternative approach that retains the benefits of structured knowledge representation while minimizing the complexity of formal logical programming.This method embeds logical rules directly into natural language prompts, testing LLMs' ability to reason within structured frameworks.Using the DSPy framework, this approach functions as a rule-based chain-of-thought (CoT) prompting strategy executed in two stages.</p>
<p>This approach aligns with the core principle of neural-symbolic AI: converting raw data into structured, symbolic representations for reasoning.By using predicates with precise argument structures, LLMs create consistent intermediate knowledge representations that facilitate question answering.This streamlined process maintains the advantages of formal reasoning while reducing computational complexity and implementation overhead.</p>
<p>In the LLM+ASP pipeline, generating a single ASP program requires multiple LLM calls due to the iterative refinement process, creating substantial computational overhead even with a modest three-iteration limit.To mitigate this, our alternative approach replaces formal ASP code generation with direct application of logical rules within natural language.The process begins by instructing LLMs to convert natural language inputs into structured facts using predefined predicates, a step retained from the LLM+ASP pipeline.Instead of formal logic programs, LLMs then apply relevant logical rules-such as inverse, symmetric, and transitive relations for the SparQA dataset, or offset-based chain-linking rules for StepGame-to derive answers through natural language reasoning.</p>
<p>By prompting LLMs to explicitly apply specific rules for different scenarios, this approach avoids dependence on external solvers.Consequently, it is termed "Facts+Rules."This method offers a more straightforward and reliable path to spatial reasoning compared to generating and executing formal logic programs.</p>
<p>Experiment Results and Discussion</p>
<p>In this section, we present the results of three methods: Direct Prompting Baseline, Facts + Rules Prompting, and Iterative LLM+ASP, on two benchmark datasets: StepGame and SparQA.We analyze their performance, computational complexity, and suitability for different types of spatial reasoning tasks.Additionally, we conduct an ablation study to assess the impact of iterative feedback loops in the LLM+ASP approach.</p>
<p>StepGame</p>
<p>Implementation</p>
<p>Another important aspect of the StepGame dataset is its reasoning hops range from 1 to 10 hops, distributed across 10 subsets.Specifically, each subset consists of 10,000 samples, each corresponding to a single reasoning hop.We sampled 300 instances for each reasoning hop 1 to 10, k ∈ {1, . . ., 10} , ensuring comprehensive evaluation across complexity levels.To standardize the task, we prompt LLMs to convert language descriptions into is3 and query2 facts using nine predefined spatial relations (e.g., left, right, top-right).Additional guidelines ensure accurate relation mapping, distinguishing between clockwise directions and cardinal references.This systematic approach maintains consistency and aids the models in handling spatial reasoning effectively.</p>
<p>Our pipeline integrates a customized knowledge module adapted from Yang et al. (2023b).This module employs coordinate-based reasoning rules, treating the second queried object as the reference point (0,0).By applying cardinal offsets, the module calculates the relative positions of connected objects, determining their spatial relationships iteratively.This method refines the models' intermediate inferences, enhancing accuracy in multi-hop tasks.</p>
<p>Results</p>
<p>As shown in Table 1, the DSPy pipeline consistently outperforms baseline methods (Direct prompting and Facts+Rules) across all reasoning depths for Deepseek, Llama3, and GPT-4.0 mini models.This demonstrates the effectiveness of integrating linguistic processing with logical reasoning.For examples, at reasoning hop k=5, Deepseek's accuracy jumps from 33.3% (Direct) to 88.5% (+55.2%) using DSPy.Llama3 improves from 28.2% to 77.9% (+49.7%), and GPT-4.0 mini increases from 29.9% to 87.1% (+57.2%).These gains highlight the power of neural-symbolic feedback.Accuracy declines as reasoning depth increases, reflecting task complexity.For example, at k=10, Direct prompting achieves only 16.8% (Deepseek), 14.2% (Llama3), and 13.5% (GPT-4.0mini), while DSPy maintains significantly higher performance: 79.8%, 65.7%, and 68.9%, respectively.</p>
<p>Adding Facts+Rules boosts the overall accuracy to 44.9%, 47.0%, and 47.6%, though it remains limited by sequential dependencies.In contrast, DSPy achieves the highest overall accuracy: 87.7% (Deepseek), 75.6% (Llama3), and 80.8% (GPT-4.0mini).It is obvious that the DSPy pipeline's iterative approach consistently enhances accuracy, particularly for deeper reasoning tasks, validating the benefits of combining symbolic reasoning with neural methods.</p>
<p>Analysis</p>
<p>Due to the controlled complexity and simple language structure, StepGame provides an ideal testbed for evaluating neural-symbolic integration for reasoning, allowing us to isolate the effects of multi-hop reasoning depth from linguistic complexity.There are no co-reference or named entity recognition issues since each agent is clearly defined (eg., "A", "B", "C", "D"), and there is only one question type, querying the relation between two agents.</p>
<p>The success of LLM + ASP can be attributed to several key factors.The simplified predicate structure, utilizing only is/3 and query/2 predicates, provides a clear bridge between natural language and logical forms.This simplification, com-bined with our well-designed knowledge module, enables efficient handling of spatial relationships while maintaining robust error detection capabilities.</p>
<p>Interestingly, the LLM+ASP method functions as a dataset quality checker.Aligning with Yang et al. (2023b), we identify labeling errors in 10% of the instances.Ambiguities in crowdsourced data accumulate with reasoning depth, revealing issues when models output multiple answers.This highlights the potential of neural-symbolic approaches not only for reasoning but also for improving dataset integrity.</p>
<p>SparQA</p>
<p>In order to evaluate the generalizability of our LLM+ASP approach against the more challenging benchmark SparQA, which focuses on complex natural language queries involving spatial relationships and requires precise fact extraction and reasoning over intricate descriptions.We aims to understand both the capabilities and limitations of neural-symbolic integration when confronted with realistic spatial reasoning tasks that more closely approximate real-world complexity.</p>
<p>Implementation</p>
<p>A representative test set of 220 examples, with 55 samples from each question type, was constructed to balance computational constraints and ensure comprehensive coverage.We also deliberately include challenging quantifier questions to assess the system's capabilities.</p>
<p>We adopted the same pipeline as StepGame to SparQA: (1) Converting Natural Language Context and Question to ASP Facts; (2) Adding Rules and Refining ASP Program; (3) Symbolic Reasoning; (4) Result Mapping and Evaluation.The first module prompted LLMs to identify blocks, objects, and relation facts using three predicates: block/1, object/5, and is/3, constraining the specific relation sets to minimize grounding errors.Rule refinement incorporates predefined inverse, transitive, and symmetric rules for spatial relations, enabling reasoning across multi-block environments.These rules were manually designed and updated, enhancing the system's capability to handle complex spatial queries that go beyond simple 2D reasoning tasks.The specific code and samples are seen in Appendix.</p>
<p>Key challenges in the implementation include parsing context-level descriptions for coreference resolution, representing implicit spatial relationships to avoid grounding errors, and managing object references with varying complexity (object/5).Additionally, query generation is difficult due to diverse question structures and complex quantification requirements.Despite careful prompt engineering, LLMs still struggle with generating queries that are different from the provided examples in the prompts.To overcome the challenge, we try to provide question type-wise query examples, quantification encoding query exampels to guide LLMs to write error free query.As shown in Table 2, the neural-symbolic LLM+ASP pipeline showed mixed results across different models and question types.Finding Relation (FR) questions demonstrated significant improvement with accuracy increasing by approximately 20% across all models (from 26.9% to 53.1% on Llama3, 38.2% to 58.9% on Deepseek, and 45.4% to 65.3% on GPT 4.0).Finding Block (FB) questions benefited from structured block/5 predicate representation, showing substantial gains particularly in GPT 4.0 (from 60.9% to 80.5%).Choose Object (CO) questions showed varied results, with GPT-4.0 achieving a notable 15% improvement while other models showed minimal changes.Interestingly, Yes/No (YN) questions does not benefit too much from complexity of neural-symbolic methods, only showing a slight improvement overr with direct prompting across all models.</p>
<p>Results</p>
<p>SparQA include a lot quantifier reasoning question, almost one third questions include "all, only, any".For examples,"Which block has only squares inside?"."What block has all of the black objects inside of it?","Are all of the triangles to the left of the black circle?",Besides that, it also envolves a set, namely, using one attribute to represent the whole set of objects.Thanks to the object/5 atoms, we use use object(<em>,</em>,black,<em>,</em>) in the query to match all the objects with all the black objects.Quantification typically challenges the pure neural approach.I If an LLM were to rely solely on natural language inference, it would need to exhaustively examine all objects with the black color.In contrast, converting the questions into logical expressions using the universal conditional (:) is relatively straightforward.As long as the logical expression is correctly encoded, the reasoning process becomes swift and accurate.</p>
<p>For example, "What block has all the black objects inside of it?"will be represented as: query(Block):block(Block), not object(<em>, </em>, black, _, OtherBlock): block(OtherBlock), OtherBlock != Block.</p>
<p>As shown in the table, the Facts+Rules method achieves competitive and similar performance with the LLM+ASP approach while significantly outperforming direct prompting (more than 5% improvement).While LLM+ASP achieves marginally higher accuracy in some cases, Facts+Rules offers advantages in implementation simplicity and reduced computational overhead.The improvement over baseline might be attributed to two factors: more consistent entity naming conventions in natural language prompts, and explicit instructions to follow the spatial logical rules.</p>
<p>Analysis</p>
<p>The error analysis revealed four primary categories of issues in the neuralsymbolic system: parsing errors (31%), grounding errors (23%), satisfiable but no result issues (28%), and wrong answers (18%).Parsing errors, the most frequent issue, stemmed from syntax-related problems in ASP code generation, including unqualified relation specifications and improper predicate formatting.Grounding errors emerged from inconsistencies between variable naming and knowledge base content, while satisfiability issues arose when the provided facts and rules were insufficient for query resolution, particularly in cases involving implicit spatial relationships that the system failed to capture explicitly in the ASP code.</p>
<p>Each model exhibited distinct error patterns that impacted their performance differently.Deepseek demonstrated strength in handling complex spatial relationships but frequently generated syntax errors (42% of its errors) and struggled with consistent comment handling in ASP code.GPT-4.0 mini showed promising initial code generation capabilities but consistently encountered issues with argument ordering in block/5 predicates, leading to fact-query inconsistencies.Meanwhile, Llama3 maintained consistent cate formatting but showed higher rates of "satifiable but no result" failures, particularly in cases involving complex spatial relationship chains and nested relationships.To address these challenges, the system would benefit from model-specific optimization strategies with tailored prompting approaches and custom validation rules, alongside an enhanced knowledge representation system capable of better handling implicit relationships and ambiguous spatial descriptions.</p>
<p>Impact of Feedback Loop in the DSPy-based Pipeline</p>
<p>While language models demonstrate remarkable capabilities in semantic parsing task, their ability to generate executable logical programs remains challenging.Previous research points out that the direct translation from natural language to logical rules often results in low success rates, even below random baseline performance.(Feng et al., 2024) This observation reinforces the importance of neural-symbolic integration especially the feedback loop between the two components, so that LLMs can benefit from symbolic solvers output, as shown in (Yang et al., 2023b).</p>
<p>To systematically analyze the effectiveness of our iterative feedback mechanism, we examine three primary error types in ASP solver execution: parsing errors (syntax errors, undefined predicates), grounding failures (infinite grounding scenarios, memory constraints), and solving stage failures (overconstrained conditions, inconsistent rules).Additionally, even successfully executed programs might produce solutions that don't align with ground truth labels due to semantic gaps between natural language specifications and logical encodings.Our feedback mechanism addresses these challenges through carefully designed prompts that instruct LLMs on error patterns and their respective fixes.It is hypothesized that the iterative feedback loop would reduce parsing errors and grounding failures.</p>
<p>In the SparQA dataset, which features complex spatial reasoning tasks, our iterative feedback loop demonstrated substantial improvements across all models.As shown in Figure 2, the mechanism shows substantial improvements across all models: Deepseek's execution rate increased from 45.8% to 76.8%, Llama3 from 34.2% to 80.2%, and GPT4mini from 44.8% to 73.2%These results demonstrate that the feedback loop between LLMs and solvers effectively addresses the inherent challenges in natural language to logic translation.The most substantial improvements occur in the first feedback round, with continued but diminishing gains in the second round.While additional feedback rounds might yield further improvements, we limited our experiment to three rounds due to computational costs.These findings demonstrate that the feedback loop significantly enhances both program executability and solution accuracy, establishing the effectiveness of our neural-symbolic integration approach.</p>
<p>Conclusion</p>
<p>This paper presents a neural-symbolic integration approach that significantly enhances LLMs' spatial reasoning capabilities.Our experimental results demonstrate that iterative feedback between LLMs and ASP solvers effectively improves both program executability and accuracy.The pipeline achieves an average 82% accuracy on StepGame and 69% on SparQA, marking substantial improvements over traditional approaches.For instance, our experiments demonstrate significant improvements over the baseline prompt-ing methods, with accuracy increases of 40-50% on StepGame dataset and 8-15% on the more complex SparQA dataset.</p>
<p>The success of neural symbolic integration stems from three key factors: (1) the effective separation of semantic parsing and logical reasoning, enabling precise control over each component; (2) the well-defined spatial relationships in a 2D environment, allowing for unambiguous predicate representation; and (3) the efficient handling of multi-hop reasoning chains through explicit logical rules.</p>
<p>We explored a simplified neural-symbolic approach, Facts+Rules, as an alternative to complex logical programming.This method showed modest improvements of 15-30% over baseline prompting on both datasets and even a comparable perforamce as LLM+ASP pipleine on SparQA.This performance comparison reveals an important trade-off in neural-symbolic integration: LLM+ASP offers superior accuracy at the cost of implementation complexity and computational overhead, while Facts+Rules provides a more lightweight solution with reduced performance on structured tasks.These findings suggest that effective neural-symbolic integration can be achieved through different approaches, each offering distinct advantages in the balance between computational efficiency and reasoning capability.</p>
<p>The key contributions of our work include: (1) a systematic approach to boost spatial reasoning through neural-symbolic integration, (2) a cohesive pipeline that combines the strengths of LLMs and symbolic reasoning, and (3) robust knowledge representation techniques that generalize across different spatial reasoning tasks.Our iterative feedback mechanism particularly demonstrates the value of combining LLMs' natural language understanding with ASP's precise logical inference capabilities.</p>
<p>However, several limitations remain.The performance gap and the variable implementation complexity between StepGame (92%) and SparQA (65%) highlights the challenge of domain sensitivity in neural-symbolic systems.The pipeline struggles particularly with complex queries involving quantifiers and implicit spatial relationships.This study tackles the challenge by careful prompt engineering and providing more examples.Additionally, the conversion from natural language to logical programs remains error-prone, with execution rates varying significantly across different question types.Future work could finetune LLM on specialized logical program dataset and design more sophisticated feedback mechanisms and improved integration between neural and symbolic components.</p>
<p>This work establishes a foundation for enhancing LLMs' reasoning capabilities through neural-symbolic integration, driving forward the quest for more intelligent, interpretable, and efficient systems.In essence, the neuralsymbolic approach treats LLMs as agents within a carefully orchestrated system.This perspective shifts the focus from improving individual LLM performance to optimizing the interplay between different components of the system.Future work should explore optimizing interactions between multiple models and reasoning components, involving more sophisticated orchestration techniques, improved integration of probabilistic reasoning with symbolic solvers, and employing the strengths of different LLMs at various stages of neural-symbolic systems.</p>
<p>Figure 1 :
1
Figure 1: LLM +ASP Pipeline</p>
<p>Figure 2 :
2
Figure 2: Effects of Feedback Loop between LLM and ASP on the SparQA Dataset</p>
<p>Table 1 :
1
Three Methods on StepGame (Accuracy %)
Model &amp; Methodk=1k=2k=3k=4k=5k=6k=7k=8k=9k=10OverallDeepseekDirect53.7 47.4 44.3 34.6 33.3 24.2 22.3 19.5 17.3 16.831.3Facts+Rules61.7 58.2 55.5 48.3 46.2 42.7 38.6 35.2 32.5 30.844.9DSPy Pipeline93.7 89.2 92.5 89.3 88.5 87.7 86.3 85.2 84.5 79.887.7Llama3Direct48.2 49.4 42.5 32.4 28.2 26.3 18.1 17.6 15.8 14.229.3Facts+Rules58.2 56.4 54.5 50.2 48.9 45.4 42.8 40.1 38.3 35.747.0DSPy Pipeline82.2 76.4 79.5 81.2 77.9 80.4 72.8 70.1 69.3 65.775.6GPT-4.0 miniDirect54.6 48.4 47.3 33.2 29.9 27.2 16.3 14.8 13.9 13.529.9Facts+Rules62.8 60.4 58.3 52.3 48.6 45.5 42.2 38.3 35.5 32.947.6DSPy Pipeline85.8 82.4 85.5 85.3 87.1 85.5 79.2 75.3 72.5 68.980.8</p>
<p>Table 2 :
2
Performance Comparison Across Methods on SparQA (Accuracy %)
Model &amp; Method FR FB YN CO OverallDeepseekDirect (Baseline)38.2 74.5 78.2 48.259.8Facts+Rules47.3 80.6 80.9 56.666.4DSPy Pipeline58.9 85.4 81.8 42.867.2Llama3Direct (Baseline)26.9 65.7 72.5 55.855.2Facts+Rules46.8 70.2 81.4 57.163.9DSPy Pipeline53.1 83.3 80.5 60.869.4GPT4.0 miniDirect (Baseline)45.4 60.9 58.2 57.655.5Facts+Rules54.3 68.2 50.4 61.558.6DSPy Pipeline65.3 80.5 64.8 72.770.3</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Proceedings of the 13th international joint conference on natural language processing and the 3rd conference of the asia-pacific chapter. the 13th international joint conference on natural language processing and the 3rd conference of the asia-pacific chapterthe association for computational linguistics20231Long papers</p>
<p>Neural-symbolic learning and reasoning: A survey and interpretation 1. T R Besold, A Garcez, S Bader, H Bowman, P Domingos, P Hitzler, K.-U Kühnberger, L C Lamb, P M V Lima, L De Penning, Neuro-symbolic artificial intelligence: The state of the art. IOS press2021</p>
<p>Answer set programming at a glance. G Brewka, T Eiter, M Truszczyński, Communications of the ACM. 54122011</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, ArXiv, abs/2309.154022023</p>
<p>An evaluation of ChatGPT-4's qualitative spatial reasoning capabilities in RCC-8. A G Cohn, arXiv:2309.155772023arXiv preprint</p>
<p>A neuro-symbolic ASP pipeline for visual question answering. T Eiter, N Higuera, J Oetsch, M Pritz, Theory and Practice of Logic Programming. 2252022Cambridge University Press</p>
<p>Large language models are neurosymbolic reasoners. M Fang, S Deng, Y Zhang, Z Shi, L Chen, M Pechenizkiy, J Wang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence20243816</p>
<p>Language models can be deductive solvers. J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao, W Chen, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>PAL: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, ArXiv, abs/2211.104352022</p>
<p>Neurosymbolic AI: The 3 rd wave. A D Garcez, L C Lamb, Artificial Intelligence Review. 56112023Springer</p>
<p>Explainable answer-set programming. T Geibinger, arXiv:2308.159012023arXiv preprint</p>
<p>Is neuro-symbolic ai meeting its promises in natural language processing? a structured review. Semantic Web. K Hamilton, A Nayak, B Božić, L Longo, 2022Publisher: IOS PressPreprint</p>
<p>Leveraging large language models to generate answer set programs. A Ishay, Z Yang, J Lee, arXiv:2307.076992023arXiv preprint</p>
<p>Dspy: Compiling declarative language model calls into selfimproving pipelines. O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, S Vardhamanan, S Haq, A Sharma, T T Joshi, H Moazam, Others, arXiv:2310.037142023arXiv preprint</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. R Mirzaee, P Kordjamshidi, arXiv:2210.169522022arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Y Wang, arXiv:2305.122952023arXiv preprint</p>
<p>A Parisi, Y Zhao, N Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.13155Logical neural networks. 2020arXiv preprint</p>
<p>True few-shot learning with prompts-a real-world perspective. T Schick, H Schütze, Transactions of the Association for Computational Linguistics. 102021</p>
<p>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Z Shi, Q Zhang, A Lipani, ber: 10Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202236</p>
<p>Z Wan, C.-K Liu, H Yang, C Li, H You, Y Fu, C Wan, T Krishna, Y Lin, A Raychowdhury, arXiv:2401.01040Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI. 2024</p>
<p>Review of large vision models and visual prompt engineering. J Wang, Z Liu, L Zhao, Z Wu, C Ma, S Yu, H Dai, Q Yang, Y.-H Liu, S Zhang, E Shi, Y Pan, T Zhang, D Zhu, X Li, X Jiang, B Ge, Y Yuan, D Shen, T Liu, S Zhang, ArXiv, abs/2307.008552023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, ArXiv, abs/2203.111712022</p>
<p>Nlprolog: Reasoning with weak unification for question answering in natural language. L Weber, P Minervini, J Münchmeyer, U Leser, T Rocktäschel, arXiv:1906.061872019arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, F Xia, Q Le, D Zhou, ArXiv, abs/2201.119032022</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Z Yang, A Ishay, J Lee, arXiv:2307.076962023aarXiv preprint</p>
<p>Z Yang, A Ishay, J Lee, arXiv:2307.07700Neurasp: Embracing neural networks into answer set programming. 2023barXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Scharli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, O Bousquet, Q Le, Chi , E , ArXiv, abs/2205.106252022</p>            </div>
        </div>

    </div>
</body>
</html>
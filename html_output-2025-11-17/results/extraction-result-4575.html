<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4575 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4575</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4575</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-129cbad01be98ee88a930e31898cb76be79c41c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1" target="_blank">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work investigates evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available and shows that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain.</p>
                <p><strong>Paper Abstract:</strong> We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4575.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4575.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU: Bilingual Evaluation Understudy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram overlap metric that computes corpus-level precision of matched n-grams between candidate and reference text, with a brevity penalty to penalize overly short candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BLEU: a method for automatic evaluation of machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU (BLEU-1 to BLEU-4)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes n-gram precisions P_n across candidate and reference responses, combines them via a (usually uniform) weighted geometric mean, and applies a brevity penalty b(r, ̂r) to penalize short candidates. Calculated typically at corpus level; smoothing used at sentence level to avoid zero scores when higher-order n-grams are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Surface n-gram overlap (precision), adjusted for candidate length via brevity penalty; implicitly assumes lexical/phrase overlap as proxy for quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation (Twitter, Ubuntu dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BLEU-2 showed a modest positive correlation with human judgements on the Twitter corpus (Spearman 0.3576, p < 0.01; Pearson 0.3874, p < 0.01). Higher-order BLEU (BLEU-3, BLEU-4) were near-zero for most response pairs due to lack of 3- or 4-gram overlap; BLEU-3/4 thus provided almost no signal. On the Ubuntu (technical) corpus BLEU variants showed near-zero correlation with human judgements (e.g., BLEU-1/2 Spearman ~0.04 or not significant). Removing stopwords/punctuation modestly changed correlations (see Table 4: BLEU-2 Spearman 0.2030, p=0.043).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (lexical overlap) compared against human adequacy ratings; correlations (Pearson and Spearman) computed to validate alignment with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation analysis with human ratings (Spearman and Pearson), statistical significance (p-values); also analysis of score distributions and sensitivity to response length and stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Assumes valid responses have substantial word overlap with references, which fails in high-diversity dialogue settings; higher-order n-gram BLEU often zero; requires multiple reference responses for better coverage; smoothing can obscure interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluated on Twitter conversational corpus and the Ubuntu Dialogue Corpus (technical support chats).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4575.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METEOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alignment-based MT metric that aligns candidate and reference tokens using exact matches, stems, synonyms (WordNet), and paraphrases, and computes an F-measure combining precision and recall with penalties for disordered matches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>METEOR: An automatic metric for mt evaluation with improved correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>METEOR</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Creates explicit token-level alignments between candidate and reference using exact matches, WordNet synonyms, stemming, and paraphrase tables; computes harmonic mean of precision and recall of aligned tokens, with penalties for fragmentation and order violations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lexical and semantic token alignment (precision and recall), with penalties for discontiguous or disordered alignments; aims to capture closer semantic equivalence than raw n-gram overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>METEOR showed weak positive correlation with human judgements on the Twitter corpus (Spearman 0.1887, p=0.06; Pearson ~0.1927, p~0.055) and negligible correlation on the Ubuntu corpus (Spearman ~0.0631, p~0.53). Overall correlations were weaker than the top-performing BLEU-2 on Twitter.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (alignment-based) metric compared to human adequacy ratings; correlation (Pearson/Spearman) used for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with human judgements, statistical testing (p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Relies on external resources (WordNet, paraphrase tables); still assumes reference sufficiently captures possible valid responses; showed weak or no correlation in high-diversity and technical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Twitter corpus and Ubuntu Dialogue Corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4575.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (Longest Common Subsequence-based ROUGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A summarization evaluation metric that uses the length of the longest common subsequence (LCS) between candidate and reference to compute an F-measure reflecting overlap while allowing non-contiguous matches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rouge: A package for automatic evaluation of summaries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes sentence-level F-measure based on the Longest Common Subsequence (LCS) between candidate and reference; LCS allows matching words in order but not necessarily contiguous, capturing partial ordering similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Sequence overlap captured by LCS (precision/recall/F-measure) indicating degree of shared subsequences and ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>ROUGE-L showed weak or non-significant correlation with human judgements on both domains (Twitter Spearman 0.1235, p=0.22; Ubuntu Spearman ~0.054, p~0.59), indicating poor alignment with human adequacy ratings in dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated sequence-overlap metric compared to human ratings; correlations computed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with human judgements (Pearson/Spearman) and p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Like other overlap metrics, fails in high-diversity settings where valid responses share few ordered tokens; does not account for semantic equivalence beyond token ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Twitter corpus and Ubuntu Dialogue Corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4575.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy Matching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Matching (embedding-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based similarity that for each word in one sentence greedily matches the most similar word in the other sentence using cosine similarity of word embeddings, averaging scores and symmetrizing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Greedy Matching</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each token in reference r, finds the token in candidate ̂r with maximum cosine similarity between their word embeddings, averages these maxima over tokens, and symmetrizes by averaging the score computed in both directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lexical semantic similarity at token level via word embeddings; favors presence of semantically similar key words rather than surface overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>On the Twitter corpus Greedy Matching had small positive correlation with human judgements (Spearman 0.2119, p=0.034; Pearson 0.1994, p=0.047). On the Ubuntu corpus correlations were negligible/non-significant (Spearman 0.0528, p~0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated embedding-based metric compared to human ratings via correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation (Spearman and Pearson) with human adequacy scores and statistical testing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Captures topical/word-level semantic similarity but ignores sentence-level compositionality and conversational context; can be misled by common frequent tokens (e.g., 'i') producing high similarity despite inappropriate responses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Twitter corpus and Ubuntu Dialogue Corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4575.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding Average</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding Average (sentence embedding via mean of word vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence-level embedding computed by averaging (additively composing) word embedding vectors and comparing candidate and reference via cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding Average (cosine similarity of mean word vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes sentence embedding as the mean (normalized) of constituent word embeddings for candidate and reference sentences, then computes cosine similarity between these sentence vectors as the evaluation score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Sentence-level topical/semantic similarity captured by averaged distributional vectors; measures global topical alignment rather than compositional semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Embedding Average had small positive correlation with human judgements on Twitter (Spearman 0.2259, p=0.024; Pearson 0.1971, p=0.049) but negative or non-significant correlation on Ubuntu (Spearman -0.1387, p=0.17; Pearson -0.1631, p=0.10).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated embedding-based evaluation compared to human adequacy ratings using correlation analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with human judgements (Spearman and Pearson), p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Simple averaging loses sentence compositionality and is sensitive to common tokens; treats topical similarity as proxy for adequacy and can over-score contextually wrong but topically related responses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Twitter corpus and Ubuntu Dialogue Corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4575.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vector Extrema</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Extrema (sentence embedding by extrema pooling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence embedding method taking, for each embedding dimension, the most extreme value across words in the sentence (largest magnitude positive or negative), then comparing sentence vectors by cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bootstrapping dialog systems with word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Vector Extrema</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes per-dimension extreme value among word vectors in a sentence (selecting max positive or most negative magnitude) to form a sentence vector; similarity is cosine between candidate and reference extrema vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Emphasizes informative/rare words (which lie far from origin) over common words; aims to capture salient semantic content of sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Vector Extrema showed small positive correlation with human judgements on Twitter (Spearman 0.2103, p=0.036; Pearson 0.1842, p=0.067), and negligible/non-significant correlation on Ubuntu (Spearman 0.0924, p=0.36; Pearson -0.0029, p=0.98).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated embedding pooling method evaluated via correlation with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Spearman/Pearson correlation with human adequacy scores and reporting p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Although designed to ignore common tokens, still insufficient to model sentence-level compositionality and contextual appropriateness; can be misled by near-synonyms in embedding space despite context mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Twitter corpus and Ubuntu Dialogue Corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4575.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>deltaBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>deltaBLEU (discriminative BLEU variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified BLEU that incorporates multiple human-evaluated reference responses with weights to better account for diverse valid targets in generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>deltableu: A discriminative metric for generation tasks with intrinsically diverse targets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>deltaBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Extends BLEU to use several human-annotated reference responses, weighting references based on human evaluations to create a discriminative variant that accounts for variability in acceptable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Weighted n-gram overlap across multiple human-judged references; attempts to reflect human preference distribution among references.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported in related work to have weak-to-moderate correlation with human judgements in Twitter when multiple human-evaluated references are available; the paper cites deltaBLEU as promising but notes human annotation for multiple references is often infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (modified BLEU) using human-annotated multiple references; validated by correlation to human judgements in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation with human judgements (in cited Galley et al. work); this paper only references deltaBLEU rather than using it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires multiple human-evaluated reference responses (costly to obtain); still may not fully address context-dependent appropriateness; not evaluated directly in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4575.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Adequacy Rating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human adequacy rating (1-5 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation procedure where annotators rate response appropriateness given context on a 1-5 adequacy scale; used as the ground truth for validating automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human adequacy rating (1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human judges are shown a dialogue context and a single proposed response and asked to rate how appropriate/sensible the response is from 1 (not appropriate) to 5 (very reasonable); used as the gold standard for correlational validation of automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Adequacy (contextual appropriateness) as judged by humans; inter-annotator agreement monitored.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>25 volunteers (2 excluded for low agreement) rated 100 items per dataset; human-vs-human split correlation was very high (Spearman ~0.95), indicating strong internal consistency and suitability as validation gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based evaluation; used as the ground truth to which automated metrics were compared.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Inter-rater agreement assessed with Cohen's kappa (median ~0.55), and human split-half correlation measured (Spearman ~0.95) to validate reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Human evaluation is costly and time-consuming; may vary with annotator expertise; the study focused on adequacy only (not fluency or informativeness separately).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to curated sets from Twitter and Ubuntu Dialogue Corpora (20 contexts x 5 responses each per annotator).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4575.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CohenKappa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's kappa (inter-rater agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical measure of inter-rater agreement for categorical ratings that accounts for chance agreement; used here to screen annotators and report median agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Cohen's kappa (inter-rater reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Calculates agreement between pairs of annotators beyond chance; used to identify and exclude annotators with low agreement (kappa < 0.2) and to report median kappa (~0.55) across participants.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Inter-annotator reliability (degree of concordance), used to validate human rating quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Annotation quality control within Dialogue evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Out of 25 volunteers, 23 had kappa > 0.2 and were retained; median kappa ~0.55 indicating moderate-to-strong agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human annotation quality metric (not an automated evaluation metric for generated text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard computation of Cohen's kappa across annotator pairs; used as quality filter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Kappa thresholds are heuristic; low kappa leads to annotator exclusion which reduces data and may bias results; kappa does not capture all dimensions of annotation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4575.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learned Evaluator / Discriminative Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned evaluation model (discriminative or regression-style evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposed class of learned evaluators trained to predict human-like scores or discriminate human vs model outputs using data, potentially conditioning on both context and response.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Learned evaluation model (context-aware discriminative/regression evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A model trained on human-labelled data that takes context and candidate response as input and outputs a human-like quality score or a binary human/model label; could be discriminative (classify human vs model) or regression (predict human rating).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predicts human judgements (adequacy) directly, potentially capturing context-dependent appropriateness, compositionality, and pragmatic factors that lexical metrics miss.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Natural Language Generation (proposed future method)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not implemented in this paper; proposed as a promising direction. Authors caution that training such evaluators may be as hard as dialogue generation itself and that if so, human evaluation will still be required.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid idea: automated (learned) but supervised by human labels; intended to emulate human evaluation automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Suggested validation would be direct correlation with held-out human judgments or discrimination accuracy between human and model outputs; not performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires labeled human data which may be costly; risk that learned evaluator overfits dataset-specific artifacts; may be as difficult as the generation task, offering limited practical gain.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4575.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity (language model likelihood metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A measure of how well a probabilistic language model predicts a sample; lower perplexity indicates better fit, but it is not directly comparable across model families and not per-response for retrieval systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Computes the exponentiated average negative log-likelihood per token under a model; used to evaluate supervised language models but not appropriate for retrieval models or per-response evaluation in this paper's setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Model predictive likelihood of data (fluency/fit to corpus), not contextual adequacy per-response.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Language modeling / Dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as used elsewhere for unsupervised dialogue models but explicitly not considered here because it is not computed per-response and cannot be applied to retrieval models.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric unrelated to direct human adequacy judgements in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>N/A within this paper; general use validated by predictive performance on held-out text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not suitable for per-response comparison to ground truth or retrieval models; does not measure contextual appropriateness as judged by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4575.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recall/Precision (retrieval metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recall and Precision (retrieval evaluation metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard retrieval metrics measuring whether the correct item is present and ranked among retrieved candidates; applicable to retrieval-based dialogue systems when ground-truth responses are in the candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Recall/Precision for retrieval models</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate retrieval models on their ability to retrieve the ground-truth response from a corpus (recall at k, precision), often by removing the test instance from the index to approximate generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability to retrieve (and rank) the true response or high-quality responses; measures retrieval accuracy rather than human-perceived adequacy of novel generated responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Dialogue / Information Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Described as typical for retrieval models; in experiments, the authors remove one occurrence of the ground-truth response and ask model to retrieve the best remaining response. The paper focuses instead on comparing retrieved responses to ground-truth via automatic text-similarity metrics and human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated retrieval metrics; used alongside human evaluation for overall assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standard IR evaluation procedures (recall@k, etc.); not the central validation mechanism in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Retrieval metrics do not reflect how a system will perform on unseen contexts where the exact ground-truth response isn't present; they measure retrieval accuracy rather than appropriateness of novel generated responses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Ubuntu Dialogue Corpus and Twitter corpus used for retrieval experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4575.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4575.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PARADISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PARADISE framework for evaluating spoken dialogue agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed framework for evaluating spoken dialogue systems that combines task-success measures and user satisfaction for supervised/dialogue-system evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paradise: A framework for evaluating spoken dialogue agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>PARADISE framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>An evaluation framework that links task-based performance measures (e.g., task completion, errors) with user satisfaction scores to produce an overall evaluation metric for spoken dialogue systems; typically used in supervised, task-oriented settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task completion, efficiency, user satisfaction, error rates and trade-offs between them.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Spoken Dialogue Systems / Human-Computer Interaction</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as part of related work on supervised dialogue evaluation; not applied in the unsupervised/chit-chat dialogue experiments of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid (combines objective task metrics with human-derived satisfaction scores).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Historically validated via task-based studies and user trials in spoken dialogue research (cited work); not validated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Applicable primarily for task-oriented supervised systems; less applicable to open-domain, high-entropy dialogue settings considered in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BLEU: a method for automatic evaluation of machine translation <em>(Rating: 2)</em></li>
                <li>METEOR: An automatic metric for mt evaluation with improved correlation with human judgments <em>(Rating: 2)</em></li>
                <li>Rouge: A package for automatic evaluation of summaries <em>(Rating: 2)</em></li>
                <li>deltableu: A discriminative metric for generation tasks with intrinsically diverse targets <em>(Rating: 2)</em></li>
                <li>A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics <em>(Rating: 1)</em></li>
                <li>Bootstrapping dialog systems with word embeddings <em>(Rating: 1)</em></li>
                <li>Paradise: A framework for evaluating spoken dialogue agents <em>(Rating: 1)</em></li>
                <li>Skip-thought vectors <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4575",
    "paper_id": "paper-129cbad01be98ee88a930e31898cb76be79c41c1",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "BLEU",
            "name_full": "BLEU: Bilingual Evaluation Understudy",
            "brief_description": "An n-gram overlap metric that computes corpus-level precision of matched n-grams between candidate and reference text, with a brevity penalty to penalize overly short candidates.",
            "citation_title": "BLEU: a method for automatic evaluation of machine translation",
            "mention_or_use": "use",
            "evaluation_method_name": "BLEU (BLEU-1 to BLEU-4)",
            "evaluation_method_description": "Computes n-gram precisions P_n across candidate and reference responses, combines them via a (usually uniform) weighted geometric mean, and applies a brevity penalty b(r, ̂r) to penalize short candidates. Calculated typically at corpus level; smoothing used at sentence level to avoid zero scores when higher-order n-grams are absent.",
            "evaluation_criteria": "Surface n-gram overlap (precision), adjusted for candidate length via brevity penalty; implicitly assumes lexical/phrase overlap as proxy for quality.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation (Twitter, Ubuntu dialogue)",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "BLEU-2 showed a modest positive correlation with human judgements on the Twitter corpus (Spearman 0.3576, p &lt; 0.01; Pearson 0.3874, p &lt; 0.01). Higher-order BLEU (BLEU-3, BLEU-4) were near-zero for most response pairs due to lack of 3- or 4-gram overlap; BLEU-3/4 thus provided almost no signal. On the Ubuntu (technical) corpus BLEU variants showed near-zero correlation with human judgements (e.g., BLEU-1/2 Spearman ~0.04 or not significant). Removing stopwords/punctuation modestly changed correlations (see Table 4: BLEU-2 Spearman 0.2030, p=0.043).",
            "automated_vs_human_evaluation": "Automated metric (lexical overlap) compared against human adequacy ratings; correlations (Pearson and Spearman) computed to validate alignment with humans.",
            "validation_method": "Correlation analysis with human ratings (Spearman and Pearson), statistical significance (p-values); also analysis of score distributions and sensitivity to response length and stopping.",
            "limitations_challenges": "Assumes valid responses have substantial word overlap with references, which fails in high-diversity dialogue settings; higher-order n-gram BLEU often zero; requires multiple reference responses for better coverage; smoothing can obscure interpretability.",
            "benchmark_dataset": "Evaluated on Twitter conversational corpus and the Ubuntu Dialogue Corpus (technical support chats).",
            "uuid": "e4575.0",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "METEOR",
            "name_full": "METEOR (Metric for Evaluation of Translation with Explicit ORdering)",
            "brief_description": "An alignment-based MT metric that aligns candidate and reference tokens using exact matches, stems, synonyms (WordNet), and paraphrases, and computes an F-measure combining precision and recall with penalties for disordered matches.",
            "citation_title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments",
            "mention_or_use": "use",
            "evaluation_method_name": "METEOR",
            "evaluation_method_description": "Creates explicit token-level alignments between candidate and reference using exact matches, WordNet synonyms, stemming, and paraphrase tables; computes harmonic mean of precision and recall of aligned tokens, with penalties for fragmentation and order violations.",
            "evaluation_criteria": "Lexical and semantic token alignment (precision and recall), with penalties for discontiguous or disordered alignments; aims to capture closer semantic equivalence than raw n-gram overlap.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "METEOR showed weak positive correlation with human judgements on the Twitter corpus (Spearman 0.1887, p=0.06; Pearson ~0.1927, p~0.055) and negligible correlation on the Ubuntu corpus (Spearman ~0.0631, p~0.53). Overall correlations were weaker than the top-performing BLEU-2 on Twitter.",
            "automated_vs_human_evaluation": "Automated (alignment-based) metric compared to human adequacy ratings; correlation (Pearson/Spearman) used for validation.",
            "validation_method": "Correlation with human judgements, statistical testing (p-values).",
            "limitations_challenges": "Relies on external resources (WordNet, paraphrase tables); still assumes reference sufficiently captures possible valid responses; showed weak or no correlation in high-diversity and technical domains.",
            "benchmark_dataset": "Twitter corpus and Ubuntu Dialogue Corpus.",
            "uuid": "e4575.1",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "ROUGE-L",
            "name_full": "ROUGE-L (Longest Common Subsequence-based ROUGE)",
            "brief_description": "A summarization evaluation metric that uses the length of the longest common subsequence (LCS) between candidate and reference to compute an F-measure reflecting overlap while allowing non-contiguous matches.",
            "citation_title": "Rouge: A package for automatic evaluation of summaries",
            "mention_or_use": "use",
            "evaluation_method_name": "ROUGE-L",
            "evaluation_method_description": "Computes sentence-level F-measure based on the Longest Common Subsequence (LCS) between candidate and reference; LCS allows matching words in order but not necessarily contiguous, capturing partial ordering similarity.",
            "evaluation_criteria": "Sequence overlap captured by LCS (precision/recall/F-measure) indicating degree of shared subsequences and ordering.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "ROUGE-L showed weak or non-significant correlation with human judgements on both domains (Twitter Spearman 0.1235, p=0.22; Ubuntu Spearman ~0.054, p~0.59), indicating poor alignment with human adequacy ratings in dialogue.",
            "automated_vs_human_evaluation": "Automated sequence-overlap metric compared to human ratings; correlations computed.",
            "validation_method": "Correlation with human judgements (Pearson/Spearman) and p-values.",
            "limitations_challenges": "Like other overlap metrics, fails in high-diversity settings where valid responses share few ordered tokens; does not account for semantic equivalence beyond token ordering.",
            "benchmark_dataset": "Twitter corpus and Ubuntu Dialogue Corpus.",
            "uuid": "e4575.2",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Greedy Matching",
            "name_full": "Greedy Matching (embedding-based)",
            "brief_description": "An embedding-based similarity that for each word in one sentence greedily matches the most similar word in the other sentence using cosine similarity of word embeddings, averaging scores and symmetrizing.",
            "citation_title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics",
            "mention_or_use": "use",
            "evaluation_method_name": "Greedy Matching",
            "evaluation_method_description": "For each token in reference r, finds the token in candidate ̂r with maximum cosine similarity between their word embeddings, averages these maxima over tokens, and symmetrizes by averaging the score computed in both directions.",
            "evaluation_criteria": "Lexical semantic similarity at token level via word embeddings; favors presence of semantically similar key words rather than surface overlap.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "On the Twitter corpus Greedy Matching had small positive correlation with human judgements (Spearman 0.2119, p=0.034; Pearson 0.1994, p=0.047). On the Ubuntu corpus correlations were negligible/non-significant (Spearman 0.0528, p~0.6).",
            "automated_vs_human_evaluation": "Automated embedding-based metric compared to human ratings via correlation.",
            "validation_method": "Correlation (Spearman and Pearson) with human adequacy scores and statistical testing.",
            "limitations_challenges": "Captures topical/word-level semantic similarity but ignores sentence-level compositionality and conversational context; can be misled by common frequent tokens (e.g., 'i') producing high similarity despite inappropriate responses.",
            "benchmark_dataset": "Twitter corpus and Ubuntu Dialogue Corpus.",
            "uuid": "e4575.3",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Embedding Average",
            "name_full": "Embedding Average (sentence embedding via mean of word vectors)",
            "brief_description": "A sentence-level embedding computed by averaging (additively composing) word embedding vectors and comparing candidate and reference via cosine similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Embedding Average (cosine similarity of mean word vectors)",
            "evaluation_method_description": "Computes sentence embedding as the mean (normalized) of constituent word embeddings for candidate and reference sentences, then computes cosine similarity between these sentence vectors as the evaluation score.",
            "evaluation_criteria": "Sentence-level topical/semantic similarity captured by averaged distributional vectors; measures global topical alignment rather than compositional semantics.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "Embedding Average had small positive correlation with human judgements on Twitter (Spearman 0.2259, p=0.024; Pearson 0.1971, p=0.049) but negative or non-significant correlation on Ubuntu (Spearman -0.1387, p=0.17; Pearson -0.1631, p=0.10).",
            "automated_vs_human_evaluation": "Automated embedding-based evaluation compared to human adequacy ratings using correlation analyses.",
            "validation_method": "Correlation with human judgements (Spearman and Pearson), p-values.",
            "limitations_challenges": "Simple averaging loses sentence compositionality and is sensitive to common tokens; treats topical similarity as proxy for adequacy and can over-score contextually wrong but topically related responses.",
            "benchmark_dataset": "Twitter corpus and Ubuntu Dialogue Corpus.",
            "uuid": "e4575.4",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Vector Extrema",
            "name_full": "Vector Extrema (sentence embedding by extrema pooling)",
            "brief_description": "A sentence embedding method taking, for each embedding dimension, the most extreme value across words in the sentence (largest magnitude positive or negative), then comparing sentence vectors by cosine similarity.",
            "citation_title": "Bootstrapping dialog systems with word embeddings",
            "mention_or_use": "use",
            "evaluation_method_name": "Vector Extrema",
            "evaluation_method_description": "Computes per-dimension extreme value among word vectors in a sentence (selecting max positive or most negative magnitude) to form a sentence vector; similarity is cosine between candidate and reference extrema vectors.",
            "evaluation_criteria": "Emphasizes informative/rare words (which lie far from origin) over common words; aims to capture salient semantic content of sentences.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "Vector Extrema showed small positive correlation with human judgements on Twitter (Spearman 0.2103, p=0.036; Pearson 0.1842, p=0.067), and negligible/non-significant correlation on Ubuntu (Spearman 0.0924, p=0.36; Pearson -0.0029, p=0.98).",
            "automated_vs_human_evaluation": "Automated embedding pooling method evaluated via correlation with human ratings.",
            "validation_method": "Spearman/Pearson correlation with human adequacy scores and reporting p-values.",
            "limitations_challenges": "Although designed to ignore common tokens, still insufficient to model sentence-level compositionality and contextual appropriateness; can be misled by near-synonyms in embedding space despite context mismatch.",
            "benchmark_dataset": "Twitter corpus and Ubuntu Dialogue Corpus.",
            "uuid": "e4575.5",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "deltaBLEU",
            "name_full": "deltaBLEU (discriminative BLEU variant)",
            "brief_description": "A modified BLEU that incorporates multiple human-evaluated reference responses with weights to better account for diverse valid targets in generation tasks.",
            "citation_title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets",
            "mention_or_use": "mention",
            "evaluation_method_name": "deltaBLEU",
            "evaluation_method_description": "Extends BLEU to use several human-annotated reference responses, weighting references based on human evaluations to create a discriminative variant that accounts for variability in acceptable outputs.",
            "evaluation_criteria": "Weighted n-gram overlap across multiple human-judged references; attempts to reflect human preference distribution among references.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "Reported in related work to have weak-to-moderate correlation with human judgements in Twitter when multiple human-evaluated references are available; the paper cites deltaBLEU as promising but notes human annotation for multiple references is often infeasible.",
            "automated_vs_human_evaluation": "Automated (modified BLEU) using human-annotated multiple references; validated by correlation to human judgements in cited work.",
            "validation_method": "Correlation with human judgements (in cited Galley et al. work); this paper only references deltaBLEU rather than using it.",
            "limitations_challenges": "Requires multiple human-evaluated reference responses (costly to obtain); still may not fully address context-dependent appropriateness; not evaluated directly in this paper's experiments.",
            "benchmark_dataset": "",
            "uuid": "e4575.6",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Human Adequacy Rating",
            "name_full": "Human adequacy rating (1-5 scale)",
            "brief_description": "Human evaluation procedure where annotators rate response appropriateness given context on a 1-5 adequacy scale; used as the ground truth for validating automated metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Human adequacy rating (1–5)",
            "evaluation_method_description": "Human judges are shown a dialogue context and a single proposed response and asked to rate how appropriate/sensible the response is from 1 (not appropriate) to 5 (very reasonable); used as the gold standard for correlational validation of automatic metrics.",
            "evaluation_criteria": "Adequacy (contextual appropriateness) as judged by humans; inter-annotator agreement monitored.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "25 volunteers (2 excluded for low agreement) rated 100 items per dataset; human-vs-human split correlation was very high (Spearman ~0.95), indicating strong internal consistency and suitability as validation gold standard.",
            "automated_vs_human_evaluation": "Human-based evaluation; used as the ground truth to which automated metrics were compared.",
            "validation_method": "Inter-rater agreement assessed with Cohen's kappa (median ~0.55), and human split-half correlation measured (Spearman ~0.95) to validate reliability.",
            "limitations_challenges": "Human evaluation is costly and time-consuming; may vary with annotator expertise; the study focused on adequacy only (not fluency or informativeness separately).",
            "benchmark_dataset": "Applied to curated sets from Twitter and Ubuntu Dialogue Corpora (20 contexts x 5 responses each per annotator).",
            "uuid": "e4575.7",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "CohenKappa",
            "name_full": "Cohen's kappa (inter-rater agreement)",
            "brief_description": "A statistical measure of inter-rater agreement for categorical ratings that accounts for chance agreement; used here to screen annotators and report median agreement.",
            "citation_title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
            "mention_or_use": "use",
            "evaluation_method_name": "Cohen's kappa (inter-rater reliability)",
            "evaluation_method_description": "Calculates agreement between pairs of annotators beyond chance; used to identify and exclude annotators with low agreement (kappa &lt; 0.2) and to report median kappa (~0.55) across participants.",
            "evaluation_criteria": "Inter-annotator reliability (degree of concordance), used to validate human rating quality.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Annotation quality control within Dialogue evaluation",
            "theory_type": null,
            "human_comparison": null,
            "evaluation_results": "Out of 25 volunteers, 23 had kappa &gt; 0.2 and were retained; median kappa ~0.55 indicating moderate-to-strong agreement.",
            "automated_vs_human_evaluation": "Human annotation quality metric (not an automated evaluation metric for generated text).",
            "validation_method": "Standard computation of Cohen's kappa across annotator pairs; used as quality filter.",
            "limitations_challenges": "Kappa thresholds are heuristic; low kappa leads to annotator exclusion which reduces data and may bias results; kappa does not capture all dimensions of annotation quality.",
            "benchmark_dataset": "",
            "uuid": "e4575.8",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Learned Evaluator / Discriminative Model",
            "name_full": "Learned evaluation model (discriminative or regression-style evaluator)",
            "brief_description": "Proposed class of learned evaluators trained to predict human-like scores or discriminate human vs model outputs using data, potentially conditioning on both context and response.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Learned evaluation model (context-aware discriminative/regression evaluator)",
            "evaluation_method_description": "A model trained on human-labelled data that takes context and candidate response as input and outputs a human-like quality score or a binary human/model label; could be discriminative (classify human vs model) or regression (predict human rating).",
            "evaluation_criteria": "Predicts human judgements (adequacy) directly, potentially capturing context-dependent appropriateness, compositionality, and pragmatic factors that lexical metrics miss.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Natural Language Generation (proposed future method)",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "Not implemented in this paper; proposed as a promising direction. Authors caution that training such evaluators may be as hard as dialogue generation itself and that if so, human evaluation will still be required.",
            "automated_vs_human_evaluation": "Hybrid idea: automated (learned) but supervised by human labels; intended to emulate human evaluation automatically.",
            "validation_method": "Suggested validation would be direct correlation with held-out human judgments or discrimination accuracy between human and model outputs; not performed in this paper.",
            "limitations_challenges": "Requires labeled human data which may be costly; risk that learned evaluator overfits dataset-specific artifacts; may be as difficult as the generation task, offering limited practical gain.",
            "benchmark_dataset": "",
            "uuid": "e4575.9",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Perplexity",
            "name_full": "Perplexity (language model likelihood metric)",
            "brief_description": "A measure of how well a probabilistic language model predicts a sample; lower perplexity indicates better fit, but it is not directly comparable across model families and not per-response for retrieval systems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Perplexity",
            "evaluation_method_description": "Computes the exponentiated average negative log-likelihood per token under a model; used to evaluate supervised language models but not appropriate for retrieval models or per-response evaluation in this paper's setting.",
            "evaluation_criteria": "Model predictive likelihood of data (fluency/fit to corpus), not contextual adequacy per-response.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Language modeling / Dialogue",
            "theory_type": null,
            "human_comparison": false,
            "evaluation_results": "Mentioned as used elsewhere for unsupervised dialogue models but explicitly not considered here because it is not computed per-response and cannot be applied to retrieval models.",
            "automated_vs_human_evaluation": "Automated metric unrelated to direct human adequacy judgements in this paper.",
            "validation_method": "N/A within this paper; general use validated by predictive performance on held-out text.",
            "limitations_challenges": "Not suitable for per-response comparison to ground truth or retrieval models; does not measure contextual appropriateness as judged by humans.",
            "benchmark_dataset": "",
            "uuid": "e4575.10",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Recall/Precision (retrieval metrics)",
            "name_full": "Recall and Precision (retrieval evaluation metrics)",
            "brief_description": "Standard retrieval metrics measuring whether the correct item is present and ranked among retrieved candidates; applicable to retrieval-based dialogue systems when ground-truth responses are in the candidate pool.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Recall/Precision for retrieval models",
            "evaluation_method_description": "Evaluate retrieval models on their ability to retrieve the ground-truth response from a corpus (recall at k, precision), often by removing the test instance from the index to approximate generalization.",
            "evaluation_criteria": "Ability to retrieve (and rank) the true response or high-quality responses; measures retrieval accuracy rather than human-perceived adequacy of novel generated responses.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Dialogue / Information Retrieval",
            "theory_type": null,
            "human_comparison": false,
            "evaluation_results": "Described as typical for retrieval models; in experiments, the authors remove one occurrence of the ground-truth response and ask model to retrieve the best remaining response. The paper focuses instead on comparing retrieved responses to ground-truth via automatic text-similarity metrics and human judgements.",
            "automated_vs_human_evaluation": "Automated retrieval metrics; used alongside human evaluation for overall assessment.",
            "validation_method": "Standard IR evaluation procedures (recall@k, etc.); not the central validation mechanism in this paper.",
            "limitations_challenges": "Retrieval metrics do not reflect how a system will perform on unseen contexts where the exact ground-truth response isn't present; they measure retrieval accuracy rather than appropriateness of novel generated responses.",
            "benchmark_dataset": "Ubuntu Dialogue Corpus and Twitter corpus used for retrieval experiments.",
            "uuid": "e4575.11",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "PARADISE",
            "name_full": "PARADISE framework for evaluating spoken dialogue agents",
            "brief_description": "A previously proposed framework for evaluating spoken dialogue systems that combines task-success measures and user satisfaction for supervised/dialogue-system evaluation.",
            "citation_title": "Paradise: A framework for evaluating spoken dialogue agents",
            "mention_or_use": "mention",
            "evaluation_method_name": "PARADISE framework",
            "evaluation_method_description": "An evaluation framework that links task-based performance measures (e.g., task completion, errors) with user satisfaction scores to produce an overall evaluation metric for spoken dialogue systems; typically used in supervised, task-oriented settings.",
            "evaluation_criteria": "Task completion, efficiency, user satisfaction, error rates and trade-offs between them.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Spoken Dialogue Systems / Human-Computer Interaction",
            "theory_type": null,
            "human_comparison": true,
            "evaluation_results": "Mentioned as part of related work on supervised dialogue evaluation; not applied in the unsupervised/chit-chat dialogue experiments of this paper.",
            "automated_vs_human_evaluation": "Hybrid (combines objective task metrics with human-derived satisfaction scores).",
            "validation_method": "Historically validated via task-based studies and user trials in spoken dialogue research (cited work); not validated in this paper.",
            "limitations_challenges": "Applicable primarily for task-oriented supervised systems; less applicable to open-domain, high-entropy dialogue settings considered in this paper.",
            "benchmark_dataset": "",
            "uuid": "e4575.12",
            "source_info": {
                "paper_title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                "publication_date_yy_mm": "2016-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BLEU: a method for automatic evaluation of machine translation",
            "rating": 2
        },
        {
            "paper_title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments",
            "rating": 2
        },
        {
            "paper_title": "Rouge: A package for automatic evaluation of summaries",
            "rating": 2
        },
        {
            "paper_title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets",
            "rating": 2
        },
        {
            "paper_title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics",
            "rating": 1
        },
        {
            "paper_title": "Bootstrapping dialog systems with word embeddings",
            "rating": 1
        },
        {
            "paper_title": "Paradise: A framework for evaluating spoken dialogue agents",
            "rating": 1
        },
        {
            "paper_title": "Skip-thought vectors",
            "rating": 1
        }
    ],
    "cost": 0.01849925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</h1>
<p>Chia-Wei Liu ${ }^{1}$, Ryan Lowe ${ }^{1 <em>}$, Iulian V. Serban ${ }^{2 </em>}$, Michael Noseworthy ${ }^{1 *}$, Laurent Charlin ${ }^{1}$, Joelle Pineau ${ }^{1}$<br>${ }^{1}$ School of Computer Science, McGill University<br>{chia-wei.liu,ryan.lowe,michael.noseworthy}@mail.mcgill.ca<br>{lcharlin, jpineau}@cs.mcgill.ca<br>${ }^{2}$ DIRO, Université de Montréal<br>iulian.vlad.serban@umontreal.ca</p>
<h4>Abstract</h4>
<p>We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.</p>
<h2>1 Introduction</h2>
<p>An important aspect of dialogue response generation systems, which are trained to produce a reasonable utterance given a conversational context, is how to evaluate the quality of the generated response. Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; Möller et al., 2006; Kamm, 1995), which are relevant when the dialogue is task-focused. We call models optimized for such supervised objectives supervised dialogue models, while those that do not are unsupervised dialogue models.</p>
<p>This paper focuses on unsupervised dialogue response generation models, such as chatbots. These</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>models are receiving increased attention, particularly using end-to-end training with neural networks (Serban et al., 2016; Sordoni et al., 2015; Vinyals and Le, 2015). This avoids the need to collect supervised labels on a large scale, which can be prohibitively expensive. However, automatically evaluating the quality of these models remains an open question. Automatic evaluation metrics would help accelerate the deployment of unsupervised response generation systems.</p>
<p>Faced with similar challenges, other natural language tasks have successfully developed automatic evaluation metrics. For example, BLEU (Papineni et al., 2002a) and METEOR (Banerjee and Lavie, 2005) are now standard for evaluating machine translation models, and ROUGE (Lin, 2004) is often used for automatic summarization. These metrics have recently been adopted by dialogue researchers (Ritter et al., 2011; Sordoni et al., 2015; Li et al., 2015; Galley et al., 2015b; Wen et al., 2015; Li et al., 2016). However these metrics assume that valid responses have significant word overlap with the ground truth responses. This is a strong assumption for dialogue systems, where there is significant diversity in the space of valid responses to a given context. This is illustrated in Table 1, where two reasonable responses are proposed to the context, but these responses do not share any words in common and do not have the same semantic meaning.</p>
<p>In this paper, we investigate the correlation between the scores from several automatic evaluation metrics and human judgements of dialogue response quality, for a variety of response generation models. We consider both statistical word-overlap similar-</p>
<p>Context of Conversation
Speaker A: Hey John, what do you want to do tonight?
Speaker B: Why don't we go see a movie?
Ground-Truth Response
Nah, I hate that stuff, let's do something active.
Model Response
Oh sure! Heard the film about Turing is out!
Table 1: Example showing the intrinsic diversity of valid responses in a dialogue. The (reasonable) model response would receive a BLEU score of 0 .
ity metrics such as BLEU, METEOR, and ROUGE, and word embedding metrics derived from word embedding models such as Word2Vec (Mikolov et al., 2013). We find that all metrics show either weak or no correlation with human judgements, despite the fact that word overlap metrics have been used extensively in the literature for evaluating dialogue response models (see above, and Lasguido et al. (2014)). In particular, we show that these metrics have only a small positive correlation on the chitchat oriented Twitter dataset, and no correlation at all on the technical Ubuntu Dialogue Corpus. For the word embedding metrics, we show that this is true even though all metrics are able to significantly distinguish between baseline and state-of-the-art models across multiple datasets. We further highlight the shortcomings of these metrics using: a) a statistical analysis of our survey's results; b) a qualitative analysis of examples from our data; and c) an exploration of the sensitivity of the metrics.</p>
<p>Our results indicate that a shift must be made in the research community away from these metrics, and highlight the need for a new metric that correlates more strongly with human judgement.</p>
<h2>2 Related Work</h2>
<p>We focus on metrics that are model-independent, i.e. where the model generating the response does not also evaluate its quality; thus, we do not consider word perplexity, although it has been used to evaluate unsupervised dialogue models (Serban et al., 2015). This is because it is not computed on a per-response basis, and cannot be computed for retrieval models. Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which
has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015). We also do not consider evaluation methods for supervised evaluation methods. ${ }^{1}$</p>
<p>Several recent works on unsupervised dialogue systems adopt the BLEU score for evaluation. Ritter et al. (2011) formulate the unsupervised learning problem as one of translating a context into a candidate response. They use a statistical machine translation (SMT) model to generate responses to various contexts using Twitter data, and show that it outperforms information retrieval baselines according to both BLEU and human evaluations. Sordoni et al. (2015) extend this idea using a recurrent language model to generate responses in a context-sensitive manner. They also evaluate using BLEU, however they produce multiple ground truth responses by retrieving 15 responses from elsewhere in the corpus, using a simple bag-of-words model. Li et al. (2015) evaluate their proposed diversity-promoting objective function for neural network models using BLEU score with only a single ground truth response. A modified version of BLEU, deltaBLEU (Galley et al., 2015b), which takes into account several humanevaluated ground truth responses, is shown to have a weak to moderate correlation to human judgements using Twitter dialogues. However, such human annotation is often infeasible to obtain in practice. Galley et al. (2015b) also show that, even with several ground truth responses available, the standard BLEU metric does not correlate strongly with human judgements.</p>
<p>There has been significant previous work that evaluates how well automatic metrics correlate with human judgements in in both machine translation (Callison-Burch et al., 2010; Callison-Burch et al., 2011; Bojar et al., 2014; Graham et al., 2015) and natural language generation (NLG) (Stent et al., 2005; Cahill, 2009; Reiter and Belz, 2009; Espinosa et al., 2010). There has also been work criticizing the usefulness of BLEU in particular for machine translation (Callison-Burch et al., 2006). While many of the criticisms in these works apply to dialogue generation, we note that generating dialogue responses conditioned on the conversational</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>context is in fact a more difficult problem. This is because most of the difficulty in automatically evaluating language generation models lies in the large set of correct answers. Dialogue response generation given solely the context intuitively has a higher diversity (or entropy) than translation given text in a source language, or surface realization given some intermediate form (Artstein et al., 2009).</p>
<h2>3 Evaluation Metrics</h2>
<p>Given a dialogue context and a proposed response, our goal is to automatically evaluate how appropriate the proposed response is to the conversation. We focus on metrics that compare it to the ground truth response of the conversation. In particular, we investigate two approaches: word based similarity metrics and word-embedding based similarity metrics.</p>
<h3>3.1 Word Overlap-based Metrics</h3>
<p>We first consider metrics that evaluate the amount of word-overlap between the proposed response and the ground-truth response. We examine the BLEU and METEOR scores that have been used for machine translation, and the ROUGE score that has been used for automatic summarization. While these metrics have been shown to correlate with human judgements in their target domains (Papineni et al., 2002a; Lin, 2004), they have not been thoroughly investigated for dialogue systems. ${ }^{2}$</p>
<p>We denote the ground truth response as $r$ (thus we assume that there is a single candidate ground truth response), and the proposed response as $\hat{r}$. The $j$ 'th token in the ground truth response $r$ is denoted by $w_{j}$, with $\hat{w}_{j}$ denoting the $j$ 'th token in the proposed response $\hat{r}$.</p>
<p>BLEU. BLEU (Papineni et al., 2002a) analyzes the co-occurrences of n-grams in the ground truth and the proposed responses. It first computes an n-gram precision for the whole dataset (we assume that there is a single candidate ground truth response</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>per context):</p>
<p>$$
P_{n}(r, \hat{r})=\frac{\sum_{k} \min \left(h(k, r), h\left(k, \hat{r}<em k="k">{i}\right)\right)}{\sum</em>
$$} h\left(k, r_{i}\right)</p>
<p>where $k$ indexes all possible n-grams of length $n$ and $h(k, r)$ is the number of n-grams $k$ in $r .{ }^{3}$ To avoid the drawbacks of using a precision score, namely that it favours shorter (candidate) sentences, the authors introduce a brevity penalty. BLEU-N, where $N$ is the maximum length of n-grams considered, is defined as:</p>
<p>$$
\text { BLEU-N }:=b(r, \hat{r}) \exp \left(\sum_{n=1}^{N} \beta_{n} \log P_{n}(r, \hat{r})\right)
$$</p>
<p>$\beta_{n}$ is a weighting that is usually uniform, and $b(\cdot)$ is the brevity penalty. The most commonly used version of BLEU uses $N=4$. Modern versions of BLEU also use sentence-level smoothing, as the geometric mean often results in scores of 0 if there is no 4-gram overlap (Chen and Cherry, 2014). Note that BLEU is usually calculated at the corpus-level, and was originally designed for use with multiple reference sentences.</p>
<p>METEOR. The METEOR metric (Banerjee and Lavie, 2005) was introduced to address several weaknesses in BLEU. It creates an explicit alignment between the candidate and target responses. The alignment is based on exact token matching, followed by WordNet synonyms, stemmed tokens, and then paraphrases. Given a set of alignments, the METEOR score is the harmonic mean of precision and recall between the proposed and ground truth sentence.</p>
<p>ROUGE. ROUGE (Lin, 2004) is a set of evaluation metrics used for automatic summarization. We consider ROUGE-L, which is a F-measure based on the Longest Common Subsequence (LCS) between a candidate and target sentence. The LCS is a set of words which occur in two sentences in the same order; however, unlike n-grams the words do not have to be contiguous, i.e. there can be other words in between the words of the LCS.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>3.2 Embedding-based Metrics</h3>
<p>An alternative to using word-overlap based metrics is to consider the meaning of each word as defined by a word embedding, which assigns a vector to each word. Methods such as Word2Vec (Mikolov et al., 2013) calculate these embeddings using distributional semantics; that is, they approximate the meaning of a word by considering how often it co-occurs with other words in the corpus. ${ }^{4}$ These embeddingbased metrics usually approximate sentence-level embeddings using some heuristic to combine the vectors of the individual words in the sentence. The sentence-level embeddings between the candidate and target response are compared using a measure such as cosine distance.</p>
<p>Greedy Matching. Greedy matching is the one embedding-based metric that does not compute sentence-level embeddings. Instead, given two sequences $r$ and $\hat{r}$, each token $w \in r$ is greedily matched with a token $\hat{w} \in \hat{r}$ based on the cosine similarity of their word embeddings $\left(e_{w}\right)$, and the total score is then averaged across all words:</p>
<p>$$
\begin{gathered}
G(r, \hat{r})=\frac{\sum_{w \in r:} \max <em w="w">{\hat{w} \in \hat{r}} \cos _ \operatorname{sim}\left(e</em> \
G M(r, \hat{r})=\frac{G(r, \hat{r})+G(\hat{r}, r)}{2}
\end{gathered}
$$}, e_{\hat{w}}\right)}{|r|</p>
<p>This formula is asymmetric, thus we must average the greedy matching scores $G$ in each direction. This was originally introduced for intelligent tutoring systems (Rus and Lintean, 2012). The greedy approach favours responses with key words that are semantically similar to those in the ground truth response.</p>
<p>Embedding Average. The embedding average metric calculates sentence-level embeddings using additive composition, a method for computing the meanings of phrases by averaging the vector representations of their constituent words (Foltz et al., 1998; Landauer and Dumais, 1997; Mitchell and Lapata, 2008). This method has been widely used in other domains, for example in textual similarity</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tasks (Wieting et al., 2015). The embedding average, $\bar{e}$, is defined as the mean of the word embeddings of each token in a sentence $r$ :</p>
<p>$$
\bar{e}<em _in="\in" r="r" w="w">{r}=\frac{\sum</em>
$$} e_{w}}{\left|\sum_{w^{\prime} \in r} e_{w^{\prime}}\right|</p>
<p>To compare a ground truth response $r$ and retrieved response $\hat{r}$, we compute the cosine similarity between their respective sentence level embeddings: $\mathrm{EA}:=\cos \left(\bar{e}<em _hat_r="\hat{r">{r}, \bar{e}</em>\right)$.}</p>
<p>Vector Extrema. Another way to calculate sentence-level embeddings is using vector extrema (Forgues et al., 2014). For each dimension of the word vectors, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding:</p>
<p>$$
e_{r d}= \begin{cases}\max <em d="d" w="w">{w \in r} e</em>&gt;\left|\min } &amp; \text { if } e_{w d<em w_prime="w^{\prime">{w^{\prime} \in r} e</em>\right| \ \min } d<em d="d" w="w">{w \in r} e</em>
$$} &amp; \text { otherwise }\end{cases</p>
<p>where $d$ indexes the dimensions of a vector; $e_{w d}$ is the $d^{\prime}$ th dimensions of $e_{w}$ ( $w$ 's embedding). The min in this equation refers to the selection of the largest negative value, if it has a greater magnitude than the largest positive value.</p>
<p>Similarity between response vectors is again computed using cosine distance. Intuitively, this approach prioritizes informative words over common ones; words that appear in similar contexts will be close together in the vector space. Thus, common words are pulled towards the origin because they occur in various contexts, while words carrying important semantic information will lie further away. By taking the extrema along each dimension, we are thus more likely to ignore common words.</p>
<h2>4 Dialogue Response Generation Models</h2>
<p>In order to determine the correlation between automatic metrics and human judgements of response quality, we obtain response from a diverse range of response generation models in the recent literature, including both retrieval and generative models.</p>
<h3>4.1 Retrieval Models</h3>
<p>Ranking or retrieval models for dialogue systems are typically evaluated based on whether they can retrieve the correct response from a corpus of predefined responses, which includes the ground truth</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ubuntu Dialogue Corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Twitter Corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Embedding <br> Averaging</td>
<td style="text-align: center;">Greedy <br> Matching</td>
<td style="text-align: center;">Vector <br> Extrema</td>
<td style="text-align: center;">Embedding <br> Averaging</td>
<td style="text-align: center;">Greedy <br> Matching</td>
<td style="text-align: center;">Vector <br> Extrema</td>
</tr>
<tr>
<td style="text-align: center;">R-TFIDF</td>
<td style="text-align: center;">$0.536 \pm 0.003$</td>
<td style="text-align: center;">$0.370 \pm 0.002$</td>
<td style="text-align: center;">$0.342 \pm 0.002$</td>
<td style="text-align: center;">$0.483 \pm 0.002$</td>
<td style="text-align: center;">$0.356 \pm 0.001$</td>
<td style="text-align: center;">$0.340 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: center;">C-TFIDF</td>
<td style="text-align: center;">$0.571 \pm 0.003$</td>
<td style="text-align: center;">$0.373 \pm 0.002$</td>
<td style="text-align: center;">$0.353 \pm 0.002$</td>
<td style="text-align: center;">$0.531 \pm 0.002$</td>
<td style="text-align: center;">$0.362 \pm 0.001$</td>
<td style="text-align: center;">$0.353 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">$\mathbf{0 . 6 5 0} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$0.413 \pm 0.002$</td>
<td style="text-align: center;">$0.376 \pm 0.001$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 7} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: center;">$0.384 \pm 0.001$</td>
<td style="text-align: center;">$0.365 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: center;">LSTM</td>
<td style="text-align: center;">$0.130 \pm 0.003$</td>
<td style="text-align: center;">$0.097 \pm 0.003$</td>
<td style="text-align: center;">$0.089 \pm 0.002$</td>
<td style="text-align: center;">$0.593 \pm 0.002$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 9} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 0} \pm \mathbf{0 . 0 0 2}$</td>
</tr>
<tr>
<td style="text-align: center;">HRED</td>
<td style="text-align: center;">$0.580 \pm 0.003$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 8} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 4} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 9} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 9} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 2} \pm \mathbf{0 . 0 0 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Models evaluated using the vector-based evaluation metrics, with $95 \%$ confidence intervals.
response to the conversation (Schatzmann et al., 2005). Such systems can be evaluated using recall or precision metrics. However, when deployed in a real setting these models will not have access to the correct response given an unseen conversation. Thus, in the results presented below we remove one occurrence of the ground-truth response from the corpus and ask the model to retrieve the most appropriate response from the remaining utterances. Note that this does not mean the correct response will not appear in the corpus at all; in particular, if there exists another context in the dataset with an identical ground-truth response, this will be available for selection by the model.</p>
<p>We then evaluate each model by comparing the retrieved response to the ground truth response of the conversation. This closely imitates real-life deployment of these models, as it tests the ability of the model to generalize to unseen contexts.</p>
<p>TF-IDF. We consider a simple Term Frequency - Inverse Document Frequency (TF-IDF) retrieval model (Lowe et al., 2015). TF-IDF is a statistic that intends to capture how important a given word is to some document, which is calculated as: $\operatorname{tfidf}(w, c, C)=f(w, c) \times \log \frac{N}{\mid{c \in C: w \in c} \mid}$, where $C$ is the set of all contexts in the corpus, $f(w, c)$ indicates the number of times word $w$ appeared in context $c, N$ is the total number of dialogues, and the denominator represents the number of dialogues in which the word $w$ appears.</p>
<p>In order to apply TF-IDF as a retrieval model for dialogue, we first compute the TF-IDF vectors for each context and response in the corpus. We then return the response with the largest cosine similarity in the corpus, either between the input context and corpus contexts (C-TFIDF), or between the input context and corpus responses (R-TFIDF).</p>
<p>Dual Encoder. Next we consider the recurrent neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al., 2015). The DE model consists of two RNNs which respectively compute the vector representation of an input context and response, $c, r \in \mathbb{R}^{n}$. The model then calculates the probability that the given response is the ground truth response given the context, by taking a weighted dot product: $p(r$ is correct $\mid c, r, M)=$ $\sigma\left(c^{T} M r+b\right)$ where $M$ is a matrix of learned parameters and $b$ is a bias. The model is trained using negative sampling to minimize the cross-entropy error of all (context, response) pairs. To our knowledge, our application of neural network models to large-scale retrieval in dialogue systems is novel.</p>
<h3>4.2 Generative Models</h3>
<p>In addition to retrieval models, we also consider generative models. In this context, we refer to a model as generative if it is able to generate entirely new sentences that are unseen in the training set.</p>
<p>LSTM language model. The baseline model is an LSTM language model (Hochreiter and Schmidhuber, 1997) trained to predict the next word in the (context, response) pair. During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).</p>
<p>HRED. Finally we consider the Hierarchical Recurrent Encoder-Decoder (HRED) (Serban et al., 2015). In the traditional Encoder-Decoder framework, all utterances in the context are concatenated together before encoding. Thus, information from previous utterances is far outweighed by the most recent utterance. The HRED model uses a hierarchy of encoders; each utterance in the context passes through an 'utterance-level' encoder, and the</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Twitter</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ubuntu</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">p-value</td>
</tr>
<tr>
<td style="text-align: left;">Greedy</td>
<td style="text-align: center;">0.2119</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">0.1994</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.05276</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.02049</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">0.2259</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.1971</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">-0.1387</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">-0.1631</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: left;">Extrema</td>
<td style="text-align: center;">0.2103</td>
<td style="text-align: center;">0.036</td>
<td style="text-align: center;">0.1842</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.09243</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">-0.002903</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">0.1887</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.1927</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.06314</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.1419</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-1</td>
<td style="text-align: center;">0.1665</td>
<td style="text-align: center;">0.098</td>
<td style="text-align: center;">0.1288</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">-0.02552</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.01929</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-2</td>
<td style="text-align: center;">0.3576</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">0.3874</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">0.03819</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.0586</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-3</td>
<td style="text-align: center;">0.3423</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">0.1443</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.0878</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.1116</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-4</td>
<td style="text-align: center;">0.3417</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">0.1392</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.1218</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.1132</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE</td>
<td style="text-align: center;">0.1235</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.09714</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.05405</td>
<td style="text-align: center;">0.5933</td>
<td style="text-align: center;">0.06401</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">0.9476</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.9550</td>
<td style="text-align: center;">$&lt;0.01$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Correlation between each metric and human judgements for each response. Correlations shown in the human row result from randomly dividing human judges into two groups.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Spearman</th>
<th style="text-align: center;">p-value</th>
<th style="text-align: center;">Pearson</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BLEU-1</td>
<td style="text-align: center;">0.1580</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.2074</td>
<td style="text-align: center;">0.038</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-2</td>
<td style="text-align: center;">0.2030</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.1300</td>
<td style="text-align: center;">0.20</td>
</tr>
</tbody>
</table>
<p>Table 4: Correlation between BLEU metric and human judgements after removing stopwords and punctuation for the Twitter dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Mean score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\Delta w&lt;6$</td>
<td style="text-align: center;">$\Delta w&gt;6$</td>
<td style="text-align: center;">p-value</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$(\mathrm{n}=47)$</td>
<td style="text-align: center;">$(\mathrm{n}=53)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BLEU-1</td>
<td style="text-align: center;">0.1724</td>
<td style="text-align: center;">0.1009</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-2</td>
<td style="text-align: center;">0.0744</td>
<td style="text-align: center;">0.04176</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">0.6587</td>
<td style="text-align: center;">0.6246</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">0.2386</td>
<td style="text-align: center;">0.2073</td>
<td style="text-align: center;">$&lt;0.01$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">0.73</td>
</tr>
</tbody>
</table>
<p>Table 5: Effect of differences in response length for the Twitter dataset, $\Delta w=$ absolute difference in #words between a ground truth response and proposed response
output of these encoders is passed through another 'context-level' encoder, which enables the handling of longer-term dependencies.</p>
<h3>4.3 Conclusions from an Incomplete Analysis</h3>
<p>When evaluation metrics are not explicitly correlated to human judgement, it is possible to draw misleading conclusions by examining how the metrics rate different models. To illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which contains technical vocabulary and where conversations are often oriented towards solv-
ing a particular problem, and a non-technical Twitter corpus collected following the procedure of Ritter et al. (2010). We consider these two datasets since they cover contrasting dialogue domains, i.e. technical help vs casual chit-chat, and because they are amongst the largest publicly available corpora, making them good candidates for building data-driven dialogue systems.</p>
<p>Results on the proposed embedding metrics are shown in Table 2. For the retrieval models, we observe that the DE model significantly outperforms both TFIDF baselines on all metrics across both datasets. Further, the HRED model significantly outperforms the basic LSTM generative model in both domains, and appears to be of similar strength as the DE model. Based on these results, one might be tempted to conclude that there is some information being captured by these metrics, that significantly differentiates models of different quality. However, as we show in the next section, the embedding-based metrics correlate only weakly with human judgements on the Twitter corpus, and not at all on the Ubuntu Dialogue Corpus. This demonstrates that metrics that have not been specifically correlated with human judgements on a new task should not be used to evaluate that task.</p>
<h2>5 Human Correlation Analysis</h2>
<p>Data Collection. We conducted a human survey to determine the correlation between human judgements on the quality of responses, and the score assigned by each metric. We aimed to follow the procedure for the evaluation of BLEU (Papineni et al.,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Scatter plots showing the correlation between metrics and human judgements on the Twitter corpus (a) and Ubuntu Dialogue Corpus (b). The plots represent BLEU-2 (left), embedding average (center), and correlation between two randomly selected halves of human respondents (right).</p>
<p>2002a). 25 volunteers from the Computer Science department at the author's institution were given a context and one proposed response, and were asked to judge the response quality on a scale of 1 to 5.5; a 1 indicates that the response is not appropriate or sensible given the context, and a 5 indicates that the response is very reasonable. Out of the 25 respondents, 23 had Cohen's kappa scores $\kappa&gt;0.2$ w.r.t. the other respondents, which is a standard measure for inter-rater agreement (Cohen, 1968). The 2 respondents with $\kappa&lt;0.2$, indicating slight agreement, were excluded from the analysis below. The median $\kappa$ score was approximately 0.55 , roughly indicating moderate to strong annotator agreement.</p>
<p>Each volunteer was given 100 questions per dataset. These questions correspond to 20 unique contexts, with 5 different responses: one utterance</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>randomly drawn from elsewhere in the test set, the response selected from each of the TF-IDF, DE, and HRED models, and a response written by a human annotator. These were chosen as they cover the range of qualities almost uniformly (see Figure 1).</p>
<p>Survey Results. We present correlation results between the human judgements and each metric in Table 3. We compute the Pearson correlation, which estimates linear correlation, and Spearman correlation, which estimates any monotonic correlation.</p>
<p>The first observation is that in both domains the BLEU-4 score, which has previously been used to evaluate unsupervised dialogue systems, shows very weak if any correlation with human judgement. In fact we found that the BLEU-3 and BLEU-4 scores were near-zero for a majority of response pairs; for BLEU-4, only four examples had a score $&gt;10^{-9}$. Despite this, they still correlate with human judgements on the Twitter Corpus at a rate similar to BLEU-2. This is because of the smoothing constant, which gives a tiny weight to unigrams and bigrams despite the absence of higher-order n-grams. BLEU-3 and BLEU-4 behave as a scaled, noisy version of BLEU-2; thus, if one is to evaluate dialogue</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Context of Conversation</th>
<th style="text-align: left;">Context of Conversation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A: dearest! question. how many thousands of people</td>
<td style="text-align: left;">A: never felt more sad than i am now</td>
</tr>
<tr>
<td style="text-align: left;">can panaad occupy?</td>
<td style="text-align: left;">B: @user aww why?</td>
</tr>
<tr>
<td style="text-align: left;">B: @user panaad has <number> k seat capacity while rizal</td>
<td style="text-align: left;">A: @user @user its a long story ! sure you wanna know</td>
</tr>
<tr>
<td style="text-align: left;">has <number> k thats why they choose rizal i think .</td>
<td style="text-align: left;">it ? bahaha and thanks for caring btw <heart></td>
</tr>
<tr>
<td style="text-align: left;">Ground Truth Response</td>
<td style="text-align: left;">Ground Truth Response</td>
</tr>
<tr>
<td style="text-align: left;">A: now i know about the siting capacity . thanks for the</td>
<td style="text-align: left;">A: @user i don 't mind to hear it i 've got all day and</td>
</tr>
<tr>
<td style="text-align: left;">info @user great evening.</td>
<td style="text-align: left;">youre welcome <number></td>
</tr>
<tr>
<td style="text-align: left;">Proposed Response</td>
<td style="text-align: left;">Proposed Response</td>
</tr>
<tr>
<td style="text-align: left;">A: @user makes sense. thanks!</td>
<td style="text-align: left;">A: @user i know , i 'm just so happy for you ! ! ! ! ! ! !</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !</td>
</tr>
</tbody>
</table>
<p>efit from a weighting of word saliency.
The right of the figure shows the reverse scenario: the embedding-based metrics score the proposed response highly, while humans do not. This is most likely due to the frequently occurring 'i' token, and the fact that 'happy' and 'welcome' may be close together in the embedding space. However, from a human perspective there is a significant semantic difference between the responses as they pertain to the context. Metrics that take into account the context may be required in order to differentiate these responses. Note that in both responses in Figure 2, there are no overlapping n-grams greater than unigrams between the ground truth and proposed responses; thus, all of BLEU-2,3,4 would assign a score near 0 to the response.</p>
<h2>6 Discussion</h2>
<p>We have shown that many metrics commonly used in the literature for evaluating unsupervised dialogue systems do not correlate strongly with human judgement. Here we elaborate on important issues arising from our analysis.</p>
<p>Constrained tasks. Our analysis focuses on relatively unconstrained domains. Other work, which separates the dialogue system into a dialogue planner and a natural language generation component for applications in constrained domains, may find stronger correlations with the BLEU metric. For example, Wen et al. (2015) propose a model to map from dialogue acts to natural language sentences and use BLEU to evaluate the quality of the generated sentences. Since the mapping from dialogue acts to natural language sentences has lower diversity and is more similar to the machine translation task, it seems likely that BLEU will correlate better with human judgements. However, an empirical investigation is still necessary to justify this.</p>
<p>Incorporating multiple responses. Our correlation results assume that only one ground truth response is available given each context. Indeed, this is the common setting in most of the recent literature on training end-to-end conversation models. There has been some work on using a larger set of automatically retrieved plausible responses when evaluating with BLEU (Galley et al., 2015b). However,
there is no standard method for doing this in the literature. Future work should examine how retrieving additional responses affects the correlation with word-overlap metrics.</p>
<p>Searching for suitable metrics. While we provide evidence against existing metrics, we do not yet provide good alternatives for unsupervised evaluation. Despite the poor performance of the word embedding-based metrics in this survey, we believe that metrics based on distributed sentence representations hold the most promise for the future. This is because word-overlap metrics will simply require too many ground-truth responses to find a significant match for a reasonable response, due to the high diversity of dialogue responses. As a simple example, the skip-thought vectors of Kiros et al. (2015) could be considered. Since the embedding-based metrics in this paper only consist of basic averages of vectors obtained through distributional semantics, they are insufficiently complex for modeling sentence-level compositionality in dialogue. Instead, these metrics can be interpreted as calculating the topicality of a proposed response (i.e. how on-topic the proposed response is, compared to the ground-truth).</p>
<p>All of the metrics considered in this paper directly compare a proposed response to the ground-truth, without considering the context of the conversation. However, metrics that take into account the context could also be considered. Such metrics could come in the form of an evaluation model that is learned from data. This model could be either a discriminative model that attempts to distinguish between model and human responses, or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses. Finally, we must consider the hypothesis that learning such models from data is no easier than solving the problem of dialogue response generation. If this hypothesis is true, we must concede and always use human evaluations together with metrics that only roughly approximate human judgements.</p>
<h2>References</h2>
<p>R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and D. Traum. 2009. Semi-formal evaluation of conversational characters. In Languages: From Formal to Natural, pages 22-35. Springer.</p>
<p>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.
O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. SaintAmand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12-58. Association for Computational Linguistics Baltimore, MD, USA.
A. Cahill. 2009. Correlating human and automatic evaluation of a german surface realiser. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 97-100. Association for Computational Linguistics.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-evaluation the role of bleu in machine translation research. In EACL, volume 6, pages 249-256.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, M. Przybocki, and O. F. Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17-53. Association for Computational Linguistics.
C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22-64. Association for Computational Linguistics.
B. Chen and C. Cherry. 2014. A systematic comparison of smoothing techniques for sentence-level bleu. ACL 2014, page 362.
J. Cohen. 1968. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213.
D. Espinosa, R. Rajkumar, M. White, and S. Berleant. 2010. Further meta-evaluation of broad-coverage surface realization. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564-574. Association for Computational Linguistics.
P. W. Foltz, W. Kintsch, and T. K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse processes, 25(2-3):285-307.
G. Forgues, J. Pineau, J.-M. Larcheveque, and R. Tremblay. 2014. Bootstrapping dialog systems with word embeddings.
M. Galley, C. Brockett, A. Sordoni, Y. Ji, M. Auli, C. Quirk, M. l, J. Gao, and B. Dolan. 2015a. deltaBLEU:</p>
<p>A discriminative metric for generation tasks with intrinsically diverse targets. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (Short Papers).
M. Galley, C. Brockett, A. Sordoni, Y. Ji, M. Auli, C. Quirk, M. Mitchell, J. Gao, and B. Dolan. 2015b. deltableu: A discriminative metric for generation tasks with intrinsically diverse targets. arXiv preprint arXiv:1506.06863.
Y. Graham, N. Mathur, and T. Baldwin. 2015. Accurate evaluation of segment-level machine translation metrics. In Proc. of NAACL-HLT, pages 1183-1191. Citeseer.
A. Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.
S. Hochreiter and J. Schmidhuber. 1997. Long shortterm memory. Neural Computation, 9(8):1735-1780.
E. Hovy. 1999. Toward finely differentiated evaluation metrics for machine translation. In Proceedings of the Eagles Workshop on Standards and Evaluation.
K. Jokinen and M. McTear. 2009. Spoken Dialogue Systems. Morgan Claypool.
C. Kamm. 1995. User interfaces for voice applications. Proceedings of the National Academy of Sciences, 92(22):10031-10037.
R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. 2015. Skip-thought vectors. In Advances in Neural Information Processing Systems, pages 3276-3284.
T. K. Landauer and S. T. Dumais. 1997. A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.
N. Lasguido, S. Sakti, G. Neubig, T. Tomoki, and S. Nakamura. 2014. Utilizing human-to-human conversation examples for a multi domain chat-oriented dialog system. IEICE TRANSACTIONS on Information and Systems, 97(6):1497-1505.
J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. 2015. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055.
J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. 2016. A persona-based neural conversation model. arXiv preprint arXiv:1603.06155.
C.-Y. Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8.
R. Lowe, N. Pow, I. V. Serban, and J. Pineau. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In SIGDIAL.</p>
<p>T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 31113119 .
J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In $A C L$, pages 236-244.
S. Möller, R. Englert, K. Engelbrecht, V. Hafner, A. Jameson, A. Oulasvirta, A. Raake, and N. Reithinger. 2006. MeMo: towards automatic usability evaluation of spoken dialogue services by user error simulations. In INTERSPEECH.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002a. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL).
K. Papineni, S. Roukos, T. Ward, J. Henderson, and F. Reeder. 2002b. Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results. In Proceedings of the second international conference on Human Language Technology Research, pages 132-137.
E. Reiter and A. Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529-558.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised modeling of twitter conversations. In North American Chapter of the Association for Computational Linguistics (NAACL).
A. Ritter, C. Cherry, and W. B. Dolan. 2011. Datadriven response generation in social media. In Proceedings of the conference on empirical methods in natural language processing, pages 583-593. Association for Computational Linguistics.
V. Rus and M. Lintean. 2012. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157-162, Stroudsburg, PA, USA. Association for Computational Linguistics.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quantitative evaluation of user simulation techniques for spoken dialogue systems. In 6th Special Interest Group on Discourse and Dialogue (SIGDIAL).
I. V. Serban, A. Sordoni, Y. Bengio, A. Courville, and J. Pineau. 2015. Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Networks. In AAAI Conference on Artificial Intelligence.
I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville, and Y. Bengio. 2016. A hierarchical latent variable encoder-decoder model for generating dialogues. arXiv preprint arXiv:1605.06069.
A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2015).
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 341-351. Springer.
O. Vinyals and Q. Le. 2015. A neural conversational model. arXiv preprint arXiv:1506.05869.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997. Paradise: A framework for evaluating spoken dialogue agents. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 271-280. ACL.
T.-H. Wen, M. Gasic, N. Mrksic, P.-H. Su, D. Vandyke, and S. Young. 2015. Semantically conditioned lstmbased natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745.
J. Wieting, M. Bansal, K. Gimpel, and K. Livescu. 2015. Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Studies asking humans to evaluate text often rate different aspects separately, such as 'adequacy', 'fluency' and 'informativeness' of the text (Hovy, 1999; Papineni et al., 2002b) Our evaluation focuses on adequacy. We did not consider fluency because 4 out of the 5 proposed responses to each context were generated by a human. We did not consider informativeness because in the domains considered, it is not necessarily important (in Twitter), or else it seems to correlate highly with adequacy (in Ubuntu).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Note that the min in this equation is calculating the number of co-occurrences of n -gram $k$ between the ground truth response $r$ and the proposed response $\hat{r}$, as it computes the fewest appearances of $k$ in either response.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
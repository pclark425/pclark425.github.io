<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5233 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5233</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5233</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-264555213</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18152v1.pdf" target="_blank">Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models. Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines. Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5233.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5233.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DGTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disentangled Graph-Text Learner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that compresses neighborhood structure of text-attributed graphs into a small set of learned disentangled embeddings (channels) produced by lightweight GNNs and injects those embeddings into a frozen large language model across layers to perform downstream TAG tasks (node classification) with natural-language explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Disentangled embedding injection (DGTL)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Three-step pipeline: (1) compute node text embeddings by averaging the last-layer hidden states of an upstream LLM; (2) run multiple parallel 2-layer GNN channels (disentangled GNNs) where each channel uses a learned continuous adjacency A_i(u,v) = δ A(u,v) + (1−δ) A(u,v) · σ((S_ui h_u)^T (S_vi h_v)) to produce per-channel (disentangled) neighborhood embeddings; (3) reserve token positions and inject these disentangled embeddings into the downstream LLM's key/query/value projections at all transformer layers (via additive or rotary-position variants) while keeping the LLM weights frozen and only fine-tuning the disentangled GNN parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (TAGs) — citation networks (Cora, PubMed) and e-commerce co-purchase graphs (Books-History).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compact: compresses neighborhood info into a small number of learned tokens; Differentiable: adjacency weights are continuous and learned via gradient descent; Disentangled: multiple channels capture different structural factors; Interpretable: enables generation of natural-language explanations grounded in neighbor info; Parameter-efficient/delta-tuning: LLM frozen, only small GNN parameters tuned; Encourages diverse knowledge via varying positional encoding of injected embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Semi-supervised node classification on TAG benchmarks (Cora, PubMed, Books-History). Also evaluated interpretability by producing natural-language explanations for predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match / classification accuracy reported. As LLM-predictor comparison (Table 4): Cora: DGTL 81.10 ± 0.20 (exact match / accuracy unitless percent); PubMed: DGTL 87.10 ± 1.54; Books-History: DGTL 55.70 ± 0.89. (Metrics: for GNN-predictor comparisons, classification accuracy; for LLM predictors, exact match score.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to prompt-only baselines (0-hop prompt, Neighbor Summary) DGTL outperforms by a large margin (e.g., on Cora: 0-hop 60.80, Neighbor Summary 77.30, DGTL 81.10; on PubMed: 0-hop 62.30, Neighbor Summary 67.70, DGTL 87.10). Against GNN predictor baselines that use various text encoders (TF-IDF, Word2Vec, DeBERTa, Sentence-BERT, e5) DGTL achieves comparable performance (paper reports comparable/SOTA-level results on Cora and stronger results on PubMed).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Described challenges addressed: prompt-based approaches can produce overly long prompts and insufficiently capture complex structural relationships; backpropagating through a full LLM is difficult, motivating freezing the LLM and only tuning GNN parameters. The paper does not provide detailed analysis of scalability of reserved token positions for very large graphs, nor full ablations on number of reserved tokens vs. prompt length trade-offs beyond reported channel counts; potential limitations implicit in needing to reserve token positions and design positional injection are discussed but not exhaustively quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5233.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5233.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neighbor-summary prompting (Graph-LLM style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighbor-summary / ego-graph description prompting (as used in Graph-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based strategy that summarizes a node's neighborhood (ego-graph) or lists neighbors in natural language and appends that text to an LLM prompt to convey graph structure for downstream prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the potential of large language models (llms) in learning on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neighbor summary prompt / ego-graph description</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph structure to natural-language by either (a) describing the ego-graph or listing neighbors in text, or (b) producing a compact summary of neighboring node texts; this textual summary is concatenated into the LLM prompt so the LLM can condition on neighbor information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (TAGs), e.g., citation networks.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Straightforward and directly human-readable (interpretability), but can be verbose (long prompts) and may include excessive/unselective neighbor information; represents structure implicitly via natural-language summaries rather than learned continuous embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification on TAG benchmarks (used as LLM predictor baselines in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match / accuracy as reported in experiments: Neighbor Summary baseline: Cora 77.30, PubMed 67.70, Books-History 17.90 (percent, with no ± reported in table).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>DGTL substantially outperforms Neighbor Summary prompting on the evaluated TAG tasks (e.g., on PubMed DGTL 87.10 vs Neighbor Summary 67.70), demonstrating the benefit of learned compressed embeddings injected into the LLM versus raw textual neighbor summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prone to long prompts when many neighbors exist; may pass irrelevant information to LLMs; insufficiently structured to capture complex multi-factor structural relationships according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5233.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5233.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM token-expansion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-GLM node-token vocabulary expansion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that expands the LLM vocabulary by assigning a distinct new token to each graph node, enabling generative fine-tuning of the LLM to handle TAG tasks by referring to node-specific tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Per-node new-token vocabulary expansion (Instruct-GLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create new special tokens (one per node) and extend the model vocabulary so nodes can be referenced as tokens in prompts or generation; the LLM is then tuned to interpret these tokens in TAG tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (TAGs).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Makes nodes explicitly addressable as tokens enabling generative modeling over nodes, but implies a large vocabulary expansion (scaling considerations for large graphs) and requires model adaptation to new tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>TAG tasks in prior work (generative prediction on nodes); cited as an approach for tuning LLMs for TAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Mentioned as an alternative to prompt-only methods; the paper contrasts vocabulary expansion with DGTL's compact learned-token injection approach but does not report direct numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Vocabulary expansion for every node may not scale to large graphs; described in the paper as an alternative with practical challenges (implied: token explosion, need to retokenize/retune).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5233.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5233.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph->text prompting / code conversion (NLGraph, GPT4Graph, LLMtoGraph)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLGraph / GPT4Graph / LLMtoGraph family (graph-to-natural-language or code serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of approaches that serialize graphs into natural-language descriptions or code-like formats so that general-purpose LLMs can be evaluated or used to solve graph reasoning tasks (connectivity, shortest path, max flow, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph serialization to natural language or code</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Reorganize/serialize graphs as human-readable natural-language descriptions (NLGraph) or as code/data structures (GPT4Graph) that encode nodes, edges and optionally attributes; these textual/code representations are then provided to LLMs to perform graph reasoning and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs used in synthetic graph reasoning tasks (connectivity, shortest path, maximum flow), and also TAGs in some works.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables application of LLMs to classic graph reasoning by expressing graph structure in the model's native text or code modality; expressivity depends on serialization scheme; can be evaluated systematically across different graph problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning tasks (connectivity, shortest path, maximum flow, simulating GNNs), empirical benchmarking of LLM graph capabilities (as cited: NLGraph, GPT4Graph, LLMtoGraph).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Various task-dependent metrics reported in the cited works (e.g., task success/accuracy), but the current paper does not report numeric results from these works (so performance values are null here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>These works are cited as existing strategies that convert graphs into text/code for LLMs and are contrasted with DGTL's approach; the present paper claims that prompt-based serializations face limitations in fully capturing complex structural relationships and that a learned compressed embedding injection can be more effective for TAG tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Serialization can produce very long textual prompts for large graphs, may omit important structural facets depending on encoding choices, and may not scale or be selective for task-relevant structural factors; the paper cites these general concerns but does not provide exhaustive empirical evaluation of them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the potential of large language models (llms) in learning on graphs. <em>(Rating: 2)</em></li>
                <li>Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs. <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language?. <em>(Rating: 2)</em></li>
                <li>Prompt-based node feature extractor for fewshot learning on text-attributed graphs. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5233",
    "paper_id": "paper-264555213",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "DGTL",
            "name_full": "Disentangled Graph-Text Learner",
            "brief_description": "A method that compresses neighborhood structure of text-attributed graphs into a small set of learned disentangled embeddings (channels) produced by lightweight GNNs and injects those embeddings into a frozen large language model across layers to perform downstream TAG tasks (node classification) with natural-language explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Disentangled embedding injection (DGTL)",
            "representation_description": "Three-step pipeline: (1) compute node text embeddings by averaging the last-layer hidden states of an upstream LLM; (2) run multiple parallel 2-layer GNN channels (disentangled GNNs) where each channel uses a learned continuous adjacency A_i(u,v) = δ A(u,v) + (1−δ) A(u,v) · σ((S_ui h_u)^T (S_vi h_v)) to produce per-channel (disentangled) neighborhood embeddings; (3) reserve token positions and inject these disentangled embeddings into the downstream LLM's key/query/value projections at all transformer layers (via additive or rotary-position variants) while keeping the LLM weights frozen and only fine-tuning the disentangled GNN parameters.",
            "graph_type": "Text-attributed graphs (TAGs) — citation networks (Cora, PubMed) and e-commerce co-purchase graphs (Books-History).",
            "representation_properties": "Compact: compresses neighborhood info into a small number of learned tokens; Differentiable: adjacency weights are continuous and learned via gradient descent; Disentangled: multiple channels capture different structural factors; Interpretable: enables generation of natural-language explanations grounded in neighbor info; Parameter-efficient/delta-tuning: LLM frozen, only small GNN parameters tuned; Encourages diverse knowledge via varying positional encoding of injected embeddings.",
            "evaluation_task": "Semi-supervised node classification on TAG benchmarks (Cora, PubMed, Books-History). Also evaluated interpretability by producing natural-language explanations for predictions.",
            "performance_metrics": "Exact-match / classification accuracy reported. As LLM-predictor comparison (Table 4): Cora: DGTL 81.10 ± 0.20 (exact match / accuracy unitless percent); PubMed: DGTL 87.10 ± 1.54; Books-History: DGTL 55.70 ± 0.89. (Metrics: for GNN-predictor comparisons, classification accuracy; for LLM predictors, exact match score.)",
            "comparison_to_other_representations": "Compared to prompt-only baselines (0-hop prompt, Neighbor Summary) DGTL outperforms by a large margin (e.g., on Cora: 0-hop 60.80, Neighbor Summary 77.30, DGTL 81.10; on PubMed: 0-hop 62.30, Neighbor Summary 67.70, DGTL 87.10). Against GNN predictor baselines that use various text encoders (TF-IDF, Word2Vec, DeBERTa, Sentence-BERT, e5) DGTL achieves comparable performance (paper reports comparable/SOTA-level results on Cora and stronger results on PubMed).",
            "limitations_or_challenges": "Described challenges addressed: prompt-based approaches can produce overly long prompts and insufficiently capture complex structural relationships; backpropagating through a full LLM is difficult, motivating freezing the LLM and only tuning GNN parameters. The paper does not provide detailed analysis of scalability of reserved token positions for very large graphs, nor full ablations on number of reserved tokens vs. prompt length trade-offs beyond reported channel counts; potential limitations implicit in needing to reserve token positions and design positional injection are discussed but not exhaustively quantified.",
            "uuid": "e5233.0",
            "source_info": {
                "paper_title": "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Neighbor-summary prompting (Graph-LLM style)",
            "name_full": "Neighbor-summary / ego-graph description prompting (as used in Graph-LLM)",
            "brief_description": "A prompt-based strategy that summarizes a node's neighborhood (ego-graph) or lists neighbors in natural language and appends that text to an LLM prompt to convey graph structure for downstream prediction.",
            "citation_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "mention_or_use": "use",
            "representation_name": "Neighbor summary prompt / ego-graph description",
            "representation_description": "Convert graph structure to natural-language by either (a) describing the ego-graph or listing neighbors in text, or (b) producing a compact summary of neighboring node texts; this textual summary is concatenated into the LLM prompt so the LLM can condition on neighbor information.",
            "graph_type": "Text-attributed graphs (TAGs), e.g., citation networks.",
            "representation_properties": "Straightforward and directly human-readable (interpretability), but can be verbose (long prompts) and may include excessive/unselective neighbor information; represents structure implicitly via natural-language summaries rather than learned continuous embeddings.",
            "evaluation_task": "Node classification on TAG benchmarks (used as LLM predictor baselines in experiments).",
            "performance_metrics": "Exact-match / accuracy as reported in experiments: Neighbor Summary baseline: Cora 77.30, PubMed 67.70, Books-History 17.90 (percent, with no ± reported in table).",
            "comparison_to_other_representations": "DGTL substantially outperforms Neighbor Summary prompting on the evaluated TAG tasks (e.g., on PubMed DGTL 87.10 vs Neighbor Summary 67.70), demonstrating the benefit of learned compressed embeddings injected into the LLM versus raw textual neighbor summaries.",
            "limitations_or_challenges": "Prone to long prompts when many neighbors exist; may pass irrelevant information to LLMs; insufficiently structured to capture complex multi-factor structural relationships according to the paper.",
            "uuid": "e5233.1",
            "source_info": {
                "paper_title": "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "InstructGLM token-expansion",
            "name_full": "Instruct-GLM node-token vocabulary expansion",
            "brief_description": "A method that expands the LLM vocabulary by assigning a distinct new token to each graph node, enabling generative fine-tuning of the LLM to handle TAG tasks by referring to node-specific tokens.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Per-node new-token vocabulary expansion (Instruct-GLM)",
            "representation_description": "Create new special tokens (one per node) and extend the model vocabulary so nodes can be referenced as tokens in prompts or generation; the LLM is then tuned to interpret these tokens in TAG tasks.",
            "graph_type": "Text-attributed graphs (TAGs).",
            "representation_properties": "Makes nodes explicitly addressable as tokens enabling generative modeling over nodes, but implies a large vocabulary expansion (scaling considerations for large graphs) and requires model adaptation to new tokens.",
            "evaluation_task": "TAG tasks in prior work (generative prediction on nodes); cited as an approach for tuning LLMs for TAGs.",
            "performance_metrics": null,
            "comparison_to_other_representations": "Mentioned as an alternative to prompt-only methods; the paper contrasts vocabulary expansion with DGTL's compact learned-token injection approach but does not report direct numerical comparisons.",
            "limitations_or_challenges": "Vocabulary expansion for every node may not scale to large graphs; described in the paper as an alternative with practical challenges (implied: token explosion, need to retokenize/retune).",
            "uuid": "e5233.2",
            "source_info": {
                "paper_title": "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Graph-&gt;text prompting / code conversion (NLGraph, GPT4Graph, LLMtoGraph)",
            "name_full": "NLGraph / GPT4Graph / LLMtoGraph family (graph-to-natural-language or code serialization)",
            "brief_description": "A set of approaches that serialize graphs into natural-language descriptions or code-like formats so that general-purpose LLMs can be evaluated or used to solve graph reasoning tasks (connectivity, shortest path, max flow, etc.).",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph serialization to natural language or code",
            "representation_description": "Reorganize/serialize graphs as human-readable natural-language descriptions (NLGraph) or as code/data structures (GPT4Graph) that encode nodes, edges and optionally attributes; these textual/code representations are then provided to LLMs to perform graph reasoning and downstream tasks.",
            "graph_type": "General graphs used in synthetic graph reasoning tasks (connectivity, shortest path, maximum flow), and also TAGs in some works.",
            "representation_properties": "Enables application of LLMs to classic graph reasoning by expressing graph structure in the model's native text or code modality; expressivity depends on serialization scheme; can be evaluated systematically across different graph problems.",
            "evaluation_task": "Graph reasoning tasks (connectivity, shortest path, maximum flow, simulating GNNs), empirical benchmarking of LLM graph capabilities (as cited: NLGraph, GPT4Graph, LLMtoGraph).",
            "performance_metrics": "Various task-dependent metrics reported in the cited works (e.g., task success/accuracy), but the current paper does not report numeric results from these works (so performance values are null here).",
            "comparison_to_other_representations": "These works are cited as existing strategies that convert graphs into text/code for LLMs and are contrasted with DGTL's approach; the present paper claims that prompt-based serializations face limitations in fully capturing complex structural relationships and that a learned compressed embedding injection can be more effective for TAG tasks.",
            "limitations_or_challenges": "Serialization can produce very long textual prompts for large graphs, may omit important structural facets depending on encoding choices, and may not scale or be selective for task-relevant structural factors; the paper cites these general concerns but does not provide exhaustive empirical evaluation of them.",
            "uuid": "e5233.3",
            "source_info": {
                "paper_title": "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Natural language is all a graph needs.",
            "rating": 2,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?.",
            "rating": 2,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Prompt-based node feature extractor for fewshot learning on text-attributed graphs.",
            "rating": 1,
            "sanitized_title": "promptbased_node_feature_extractor_for_fewshot_learning_on_textattributed_graphs"
        }
    ],
    "cost": 0.0141145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs
27 Oct 2023</p>
<p>Yijian Qin 
Department of Computer Science and Tech-nology
Tsinghua University</p>
<p>Xin Wang wang@tsinghua.edu.cn&gt;. 
Department of Computer Science and Tech-nology
Tsinghua University</p>
<p>Ziwei Zhang 
Department of Computer Science and Tech-nology
Tsinghua University</p>
<p>Wenwu Zhu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#119;&#119;&#122;&#104;&#117;&#64;&#116;&#115;&#105;&#110;&#103;&#104;&#117;&#97;&#46;&#101;&#100;&#117;&#46;&#99;&#110;">&#119;&#119;&#122;&#104;&#117;&#64;&#116;&#115;&#105;&#110;&#103;&#104;&#117;&#97;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</a> 
Department of Computer Science and Tech-nology
Tsinghua University</p>
<p>Xin Wang</p>
<p>Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs
27 Oct 20238FDD30AE774B2C76A4B1F663F33D33F9arXiv:2310.18152v1[cs.CL]
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community.Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks.However, the existing works focus on harnessing the potential of LLMs solely rely on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs.To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs.Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors.Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models.Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines.Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.</p>
<p>Introduction</p>
<p>Text-attributed graphs (TAGs) are employed to represent a group of structured data where textual entities are connected by graph relations.TAGs are ubiquitous on the web, such as citation networks, e-commerce networks, social media, recommendation systems, web pages etc.As such, representation learning on TAGs has become an important research problem recently in the web community, where the TAGs are explored to capture rich semantic relationships and dependencies among connected textual elements, providing valuable contexts for better understanding and reasoning in various downstream tasks.Classical text-attributed graph representation approaches normally utilize graph neural networks (GNNs) to capture structural information, transforming textual attributes into shallow or hand-crafted representations such as bag-of-words or skip-gram features, which will then be used for prediction tasks in TAG (Kipf &amp; Welling, 2017).Some works also use natural language processing models to enhance GNN classifiers by augmenting the node features and capture the rich semantic information (Zhang et al., 2020;Yang &amp; Cui, 2021).</p>
<p>Recent advancements in machine learning and artificial intelligence have witnessed the emergence of large language models (LLMs) that exhibit unprecedented capabilities in various tasks (Zhao et al., 2023).These models have demonstrated remarkable proficiency in natural language processing related tasks including language generation, machine translation and sentiment analysis, as well as other fields such as recommendation system (Wu et al., 2023), social network analysis (Gao et al., 2023), code analysis (Chen et al., 2021), bioinformatics (Thirunavukarasu et al., 2023) and many more.</p>
<p>Therefore, the advent of LLMs has promoted the direct exploration of LLMs to solve prediction problems in TAG without GNN classifiers.These existing approaches solely rely on prompts to convey graph structure information to LLMs, suffering from insufficient understanding of the complex structural relationships within TAGs.For example, Graph-LLM (Chen et al., 2021) uses neighbor summary to generate prompts with structural information, while Instruct-GLM (Ye et al., 2023) directly describes all the neighbors in the prompt.Nevertheless, only utilizing prompts to pass information about graph structure over LLMs hinders the model's ability to capture and utilize the intricate relationships and dependencies encoded in the graph structures of TAGs, resulting in limited capability of exploiting the potential power of LLMs in TAG tasks.</p>
<p>To tackle this problem, in this paper we go beyond the vanilla prompt-based methods and effectively integrate graph structure information into LLMs, in order to enable the holistic utilization of LLMs' exceptional powers in TAG tasks.However, achieving this goal poses the following technical challenges.First, TAGs usually contain rich yet entangled structural information, i.e., not all the information is relevant or helpful for downstream tasks.LLMs need to effectively filter out and extract the pertinent information while disregarding the irrelevant details carried in the graph structure.Second, adapting LLMs to a specific TAG task is challenging, given that LLMs typically require extensive training and learning of task-specific knowledge.The process of fine-tuning LLMs for particular tasks involves striking the balance between maintaining the pre-trained knowledge and acquiring new knowledge specific to the target tasks.</p>
<p>To solve these challenges, we propose the Disentangled Graph Text Learner (DGTL) model in this paper, which can boost LLMs in deeply exploiting structural information to enhance their reasoning and predicting capabilities for TAG tasks.The proposed DGTL model first encodes raw text information carried in TAGs using a pre-trained LLM.Then, a group of tailored disentangled GNN layers are developed to capture graph neighborhood information in TAGs from multiple structural factors.By injecting the learned features with disentangled factors into the LLM predictor, we enable the model to comprehend complex graph structure information in TAGs.Moreover, our DGTL model allows the pre-trained LLMs to remain frozen, thereby reducing computation costs and mitigating the risk of catastrophic forgetting in LLMs.This flexibility enables our DGTL model to be compatible with different LLM models, ensuring its practical applicability.Overall, DGTL is able to serve as a general framework for combining text-attributed graph learning and natural language modeling to improve the explainability and predictive performance of LLMs for TAG tasks.</p>
<p>To demonstrate the effectiveness of our proposed method, we compare it with state-of-the-art approaches on various TAG benchmarks.Our method achieves superior or comparable results with baseline methods.Additionally, we demonstrate that DGTL can offer human-understandable explanations for the model predictions in natural language.</p>
<p>In summary, we make the following contributions:</p>
<p>• We propose Disentangled Graph Text Learner (DGTL), a novel model which deeply exploits graph structure information to enhance the reasoning and predicting capabilities of LLMs for TAG tasks.DGTL also serves as a general framework for integrating structural analysis abilities of GNNs with the powerful language modeling capabilities of LLMs.</p>
<p>• We propose tailored disentangled GNN layers to capture graph neighborhood information from multiple structural factors, enabling the LLMs to comprehend complex graph structure information in TAGs.</p>
<p>• Our proposed DGTL enables pre-trained LLMs to remain frozen, benefiting from reduced computation costs as well as mitigating the risk of catastrophic forgetting.</p>
<p>• We conduct extensive experiments on various TAG benchmarks and compare DGTL with state-of-the-art baselines.</p>
<p>The results demonstrate that DGTL is able to achieve superior or comparable performance, as well as provide users with human understandable natural language explanations for the model's predictions.</p>
<p>Related Works</p>
<p>Text-attributed Graphs</p>
<p>Text-attributed graphs (TAGs) have gained significant attention in the field of graph machine learning in recent years (Zhao et al., 2022;Makarov et al., 2021;Huang et al., 2023;Duan et al., 2023).A TAG is a type of graphs where each node is associated with a text attribute.This representation captures the rich semantic relationships and dependencies among textual elements, making TAGs valuable for understanding and reasoning tasks.Commonly used TAG benchmark datasets include Cora, CiteSeer, PubMed (Sen et al., 2008), and OGBN-arXiv (Hu et al., 2020), where nodes represent papers and edges represent reference relationships.</p>
<p>Message-passing GNNs (Kipf &amp; Welling, 2017;Veličković et al., 2018;Xu et al., 2019;Hamilton et al., 2017) have been proposed as an effective framework for graph machine learning following the neighborhood aggregation scheme.At each layer, nodes learn representations by aggregating their neighbors' representations (Gilmer et al., 2017).GNNs have also made significant progress in TAG tasks by considering both node attributes and graph structures.Classical GNN pipelines typically handle text attributes by converting them into shallow or hand-crafted features such as bag-ofwords (Zhang et al., 2010) or skip-gram (Mikolov et al., 2013) representations.However, with the advancements in natural language processing, there has been a shift towards utilizing language models to generate more comprehensive node features based on the text attribute.This approach allows for a deeper understanding and representation of learning on TAGs.</p>
<p>Despite the progress made by LLMs and GNNs in capturing textual or structural information for representation learning on TAGs, there are still large rooms for improvement.The integration of these models can lead to enhanced performance and more effective utilization of the rich information contained within TAGs.</p>
<p>LLMs for Graph Tasks</p>
<p>Recent researches have also delved into the exploration of leveraging LLMs directly for addressing graph-related tasks (Zhang et al., 2023).The fundamental concept behind this approach is to convert graph data, including both structural components and features, as well as graph tasks, into natural language representations.By treating graph problems as conventional NLP problems, researchers have unlocked the potential of utilizing LLMs for graph-related tasks.In the subsequent sections, we present a comprehensive overview of these recent advancements in the field.</p>
<p>Pioneer researches begin with explorations on synthetic graph tasks.NLGraph (Wang et al., 2023) reorganizes graphs as natural language description and conducts a systematic evaluation of LLMs on eight graph reasoning tasks in natural language, including connectivity, shortest path, maximum flow, simulating GNNs, etc.Meanwhile, GPT4Graph (Guo et al., 2023) also conducts extensive experiments by converting graphs into specific code formats.They evaluate the graph understanding capabilities of LLMs across ten distinct tasks, including structure understanding tasks and semantic understanding tasks.LLMtoGraph (Liu &amp; Wu, 2023) also tests GPT-3.5 and GPT-4 for various graph tasks by natural language description and makes some interesting observations.</p>
<p>More recently, several works carry out explorations on TAGs with LLMs.Graph-LLM (Chen et al., 2023) systematically investigates two strategies on TAGs: LLMs-as-Enhancers and LLMs-as-Predictors.The former strategy uses LLM to enhance the representations of text attributes of nodes before passing them to GNNs, while the latter one directly employs LLM as TAG task predictors with natural language prompts.They also explore using ego-graph description and neighbor summary to incorporate structural information by prompt.InstructGLM (Ye et al., 2023) expands the vocabulary by creating new tokens for every node in the TAG, which enables tuning LLMs to handle various TAG tasks in a generative manner.Nevertheless, the existing methods use prompt to convey neighborhood information for downstream LLMs, which faces several challenges, such as the issue of excessive neighboring information leading to lengthy prompt texts.</p>
<p>In our method, we employ a disentangled graph learning approach to compress the neighboring information on the TAG into a small number of tokens.This enables LLMs to learn and utilize the rich knowledge contained within these compressed tokens for downstream inference tasks.Besides, our method follows a delta-tuning scheme where the LLM is kept frozen and only a small number of parameters are tuned (Ding et al., 2023), making our method easy to integrate with off-the-shelf LLMs.</p>
<p>Problem Formulation and Preliminaries</p>
<p>Text-attributed Graphs</p>
<p>A text-attributed graph can by formulated as G = (V, A, y), where V denotes the set of nodes, A denotes the adjacent matrix, and y denotes the labels of nodes.In TAGs, each node is associated with a text description, e.g., the abstract of papers for citation graphs.Before the message-passing procedure in the GNNs, we need to process the raw texts into real-valued features, i.e., text embeddings.</p>
<p>In this paper, we focus on node classification, one of the most typical tasks on TAGs.We adopt the semi-supervised settings, where the text information of all the node set V and A is given at the training procedure, as well as a part of the node labels {y u |u ∈ V tr }, where V tr is the training node set.The task aims at predicting the labels {y u |u ∈ V te } of the testing node set V te .</p>
<p>Graph Neural Network</p>
<p>GNNs are state-of-the-art models for graph machine learning, which typically follow a message passing scheme where nodes aggregate information from their neighbors in each layer formulated as:
m (l) i = Agg(h (l) j |j ∈ N i ),(1) h(l+1) i = Update(m (l) i ),(2) where h (l)
i is the representation of node i at the l-th layer, N i denotes the neighbors of node i derived from the adjacent matrix, Agg(•) is the aggregation function, Update(•) is an updating function between two node representations.</p>
<p>Large Language Models</p>
<p>Large language models (LLMs) have revolutionized natural language processing tasks by demonstrating remarkable capabilities in understanding and generating human-like text.One of the key architectural advancements that underpins the success of LLMs is the transformer model, which has become the de facto standard for modeling sequential data and computer vision.</p>
<p>The transformer architecture (Vaswani et al., 2017)  Step 1: Generating text embedding by taking average of the features at the last layer in the upstream LLM.</p>
<p>Step 2: Using our proposed disentangled graph learning to learn embeddings with diverse structural information.</p>
<p>Step 3: Injecting the features with neighborhood information into the downstream LLM.</p>
<p>enables the model to weigh the importance of different parts of the input sequence when making predictions.The attention mechanism calculates attention scores between each pair of positions in the input sequence, allowing the model to focus on relevant information while disregarding irrelevant or redundant elements.This attention mechanism is crucial for capturing long-range dependencies and contextual relationships in text.The self-attention in current decoder-based LLMs can be formulated as follows:
Attention(H) = softmax( f q (H)f k (H) √ d )f v (H). (3)
Here, H is the hidden features, f q (•), f k (•), and f v (•) are the learnable projection functions to calculate the query, key, and values.</p>
<p>In a transformer, the attention mechanism operates through multiple self-attention layers.Each layer consists of multiple attention heads, which learn different representations and capture diverse aspects of the input sequence.By employing self-attention across multiple layers, transformers can effectively model both local and global dependencies in the input sequence.</p>
<p>Furthermore, LLMs are typically pre-trained on massive amounts of text data using unsupervised learning objectives.The most commonly used task is language modeling, which is formulated as:
L = − log p(s i |s 1:i−1 ),(4)
where s 1:i is a natural language sequence.This pre-training phase enables the models to acquire a rich understanding of language patterns and structures, and commonsense knowledge.The pre-trained LLMs can then be fine-tuned on specific downstream tasks with task-specific supervised learning, allowing them to adapt their knowledge to specific tasks and domains.</p>
<p>Method</p>
<p>Overall Framework</p>
<p>We aim to leverage LLMs to provide predictions for TAG tasks while enabling interpretability.The main challenge lies in efficiently equipping the LLM with neighborhood knowledge of the target nodes in the TAG.To address this, our framework incorporates disentangled graph learning, which allows us to learn rich semantic information from the neighborhood and inject it into the downstream LLM.An overall framework of our method is shown in Figure 1.Next, we elaborate our proposed method.</p>
<p>• In</p>
<p>Step 1, we generate text embeddings by computing the average of the features at the last layer in the upstream LLM.This process captures the overall context and semantics of the text associated with each node in the TAG, which form the foundation for subsequent steps.</p>
<p>• In</p>
<p>Step 2, we employ disentangled graph learning to learn embeddings that incorporate diverse neighborhood information.The disentangled graph learning approach allows us to capture varied information from the neighborhood, Compute H</p>
<p>(1) i
= GNN i,1 (H (0) , A) 6:
Compute the disentangled graph structure A i by Eq. ( 5)</p>
<p>7:</p>
<p>Compute H</p>
<p>(2) i
= GNN i,2 (H (1) i , A i ) 8:
for node u in the mini-batch do Update θ by gradient descent 14: end while facilitating a comprehensive understanding of the TAG.</p>
<p>• In</p>
<p>Step 3, we inject the learned features with neighborhood information into the downstream LLM.This integration of the LLM and the disentangled GNN facilitates more accurate and informed predictions for the downstream TAG tasks.</p>
<p>The combination of disentangled graph learning and information injection in the downstream LLM forms the core of our framework.This approach allows us to harness the strengths of both graph representation learning and language modeling, enabling effective utilization of structural information and enhancing the interpretability of our predictions.</p>
<p>Moreover, we adopt a scheme where the pre-trained LLMs remain frozen in our approach.We only focus on tuning the disentangled GNN to generate structural information for LLMs.This scheme benefits us from several advantages.Firstly, we keep a very low computation cost on fine-tuning since the disentangled GNN only cover a small rate of the parameters.Even if the LLM is updated to a new version, we can quickly adapt to the new one with minimal tuning procedure.Secondly, our approach maximizes the utilization of LLMs' pre-existing knowledge sand mitigates the risk of catastrophic forgetting.</p>
<p>Text Embedding Generation</p>
<p>The first step in our method is to generate an effective text embedding that encapsulates the semantics and contextual information of the input text.Traditionally, previous works (Chen et al., 2023) have primarily focused on utiliz-ing the embedding of the End-Of-Sentence (EOS) token at the last layer as the text embedding.However, we propose that taking the average of the hidden states in the last layer provides a more comprehensive representation of the entire input text.</p>
<p>Considering the average of the hidden states enable us to capture the collective information and contextual understanding of the text across all positions in the input sequence.This approach allows us to incorporate a broader range of linguistic cues and dependencies into the text embedding, resulting in a more robust and informative representation.</p>
<p>Disentangled Graph Learning</p>
<p>Our next step is learning hidden features that capture relevant neighborhood information for the target task.By injecting these features into the downstream LLM, we enable the LLM to effectively utilize the complex neighborhood information of the nodes in the TAG to assist in predicting the downstream task.</p>
<p>To achieve this, we employ disentangled graph learning, which allows us to capture and represent the diverse neighborhood information present in the TAG.Specifically, we adopt multiple parallel 2-layer GNNs to learn the features, and disentangle them by assigning diverse graph structures.</p>
<p>We use a parameter-efficient and differentiable manner to generate the weights of edges for the graph structures, which can be formulated as follows:
A i,(u,v) = δA (u,v) + (1 − δ)A (u,v) • σ((S u i h u ) ⊤ (S v i h v )),(5)
where A i,(u,v) is the weight of edge between node u and v in the adjacent matrix of the i-th GNN, A (u,v) indicates if there is an edge between node u and v in the original graph, S u i and S v i are learnable parameters to generate edge weights of the i-th GNN, δ is the hyper-parameter, and σ(•) denotes the sigmoid function.Consequently, we generate diverse graph structure by assign different edge weights in a continuous space.We can also conveniently use gradient based methods to optimize the parameters.</p>
<p>Our disentangled GNN architecture incorporates diverse graph structures to ensure the learning of varied information from the neighborhood.These graph structures highlight different aspects of the TAG's topology and semantics, enabling the following LLM to leverage specific types of neighborhood information during downstream task predicting.</p>
<p>Neighborhood Information Injection</p>
<p>To take full advantages of the LLMs' ability to understand and model complex patterns and semantics in TAGs, we inject the learned disentangled embedding directly into the downstream LLM.Specifically, we reserved a set of token positions for placing these disentangled embeddings in the prompt input to the downstream LLM.Even if these embeddings are not in the form of natural language, the form of our input will still allow the LLM to think that they are aligned with the natural semantic space that humans can understand.In this way, we can optimize our disentangled GNNs by gradient descent methods during fine-tuning.</p>
<p>However, the gradient back-propagation through the entire LLM neural architecture can make the optimization process extremely difficult during the fine-tuning procedure.Therefore, we take a step further by performing the embedding injection in all layers of the downstream LLM.Specifically, we add the disentangled embedding to the text embedding at the reserved positions in key and query projection functions as follows:
f {q,k} (x i ) = W {q,k} (x i + p (i) {q,k} + h (i) u ),(6)
where h
(i)
u is the disentangled embedding of node u placed in position i, W {q,k} is the projection matrix, x i is the corresponding feature from the last layer in the LLM, and p (i) {q,k} indicates the absolute or relative position embedding of position i.As an alternative, we use the following function for rotary position embedding:
f {q,k} (x i ) = R i W {q,k} (x i + h (i) u ),(7)
where R i is the rotary matrix of the position embedding.Due to the varying position encoding of the injected features, our disentangled GNNs are encouraged to learn diverse knowledge, further enriching the semantic knowledge learned by different parts of GNNs.In addition, we also use the disentangled embedding to in value calculation by a similar way:
f v (x i ) = W v (x i + h (i) u ).(8)
As such, our method incorporates neighborhood information from GNNs into each layer of the large language model LLM, enabling the LLM to benefit from a comprehensive understanding of the graph structure throughout the entire model.This injection of information in all layers facilitates a direct gradient flow to the GNNs, resulting in more accurate and informative gradient updates.This integration of language modeling and graph representation learning allows our model to leverage the contextual information captured by the LLM and the structural patterns learned by the GNNs, leading to effective learning and improved performance.</p>
<p>Fine-tuning Procedure</p>
<p>We summarize the fine-tuning procedure in Algorithm.1.After generating node embedding by LLMs for the input graph at the beginning of the algorithm, the algorithm iteratively updates the parameters until convergence.Each iteration involves sampling a mini-batch from the training set and performing forward propagation through the disentangled GNN layers to capture both the graph structure and the text attributes.We follow the classic auto-regressive scheme to fine-tune.For each node in the mini-batch, we construct its prompt (refer to Section 5.1.2for more details) as the input.Since we are only concerned about the LLM's prediction for node categories, we design a response template (some examples are shown in Table 2) as part of the prompt.Consequently, the LLM can directly predict the category at the first token of the generation procedure.</p>
<p>Then we can conveniently calculate the loss function with respect to the category prediction, which can be formulated as follows:
L = − log p(s lab |s pro + s res ),(9)
where s lab indicates the label token, s pro and s res indicate the sequences of the prompt and response template.We find that only calculating the loss function of the first token can work well in practice.As such, we can fine-tune the model in a more targeted manner, enabling it to learn the most useful information for downstream classification tasks.Back-propagation is performed after each node classification, and the parameters θ are updated through a gradient descent method when all nodes in the mini-batch are predicted.Through this iterative process, DGTL learns a text generation model that leverages the rich information present in text-attributed graphs.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>DATASETS</p>
<p>We conducted experiments on benchmark TAGs to assess the effectiveness of our framework.These TAGs encompass both citation networks and e-commerce networks, providing a diverse range of domains for evaluation.</p>
<p>For the citation networks, we utilize widely recognized datasets including Cora and PubMed (Sen et al., 2008).The Cora dataset focuses on computer science research papers.PubMed, on the other hand, is specific to biomedical literature, making it particularly relevant for medical-related tasks.These datasets consist of academic papers as nodes Here is a paper title and abstract: (paper content).Some information about the references cited in this paper: (neighbor content).Task: there are following categories: (list of categories).Which category does this paper belong to?Output the most 1 possible category of this paper.</p>
<p>Response Template</p>
<p>Based on the content of the paper, the most appropriate category for this paper would be (category prediction).
• • • E-commerce Dataset Prompt
Here is a book description and title: (book content).Some information about the books frequently purchased together: (neighbor content).Task: there are following categories: (list of categories).Which category does this book belong to?Output the most 1 possible category of this book.</p>
<p>Response Template</p>
<p>Based on the information of the book, the most appropriate category for this book would be (category prediction).• • •</p>
<p>and their citation relationships as edges.Each paper has its title and abstract as text information.The task on these citation networks is to determine the category of the papers.</p>
<p>In addition, we also incorporated e-commerce graphs into our evaluation.Specifically, we employ Book-History dataset (sktsherlock, 2023), which contains a rich variety of historical books.In the dataset, nodes represent books, while the edges indicate the connected books are frequently purchased together.Each book also has its description as the text information.The task on the e-commerce network is to determine the category of the books.</p>
<p>The statistics of the datasets are shown in Table 1.For all datasets, we follow the classical semi-supervised learning setting, i.e., we random select 20 nodes of each category as the training set, and randomly select 1,000 nodes from the rest nodes as the test set.</p>
<p>IMPLEMENTATIONS</p>
<p>In our experiments, we employed Llama-2-13B-chat (Touvron et al., 2023b) as the backbone LLM.Llama2 is a representative LLM known for its impressive language modeling capabilities and extensive pre-training on a vast corpus of text data.By utilizing Llama2 as our backbone LLM, we aimed to validate the effectiveness of our approach when collaborated with LLMs.</p>
<p>To tailor our framework to different datasets and tasks, we designed specific prompts for different types of datasets.</p>
<p>The prompts served as initial instructions or cues provided to the LLM to guide our fine-tuning for the target task.</p>
<p>The details of the designed prompts are summarized in Table 2.The table outlines the specific prompt structure, including any additional instructions or formatting applied, for each dataset used in our experiments.By customizing the prompts, we ensured that the LLM could effectively adapt its pre-trained knowledge to the specific requirements and characteristics of each dataset.</p>
<p>For other hyper-parameters, we set the number of disentangled channels is 16 for citation datasets, and 8 for the e-commerce datasets.The number of hidden dimensions of each disentangled GNN channel is 32.The hyper-parameter to control the disentangled structure δ is 0.8.For optimization, we use the Adam optimizer with a learning rate of 0.001.</p>
<p>BASELINES</p>
<p>We compare our model with baselines from the following two categories.</p>
<p>• GNN Predictors.Following Chen at el. (Chen et al., 2023), we consider different language models to enhance the node features of the TAG, including Deberta (He et al., 2020), Llama (Touvron et al., 2023a), Sentence-BERT (Reimers &amp; Gurevych, 2019), and e5 (Wang et al., 2022).Shallow embeddings TF-IDF and Word2vec are also included.We include GLEM (Zhao et al., 2022) as a baseline as well.For all these feature enhancer methods, three GNN backbones, including GCN (Kipf &amp; Welling, 2017), GAT (Veličković et al., 2018), and MLP are adopted.For these baselines, we directly use the classification accuracy as the evaluation metric.</p>
<p>• LLM predictors.We also consider using LLM (Llama2-13B-chat) as the predictors as our baselines, including using prompts without neighborhood information (0-hop prompt) and neighborhood summarization (Chen et al., 2023).For these methods, we use exact match scores (Rajpurkar et al., 2016), i.e., the percentage of predictions that match any one of the ground truth, as the metric.truth answers exactly.</p>
<p>Experimental Results</p>
<p>The result comparison with the GNN predictor baselines is shown in Table 3. From the table, we can find that GNN backbones are generally better than MLP backbone, indicating that structural information is essential to solve TAG tasks.Our method achieves comparable performance to SOTA baselines on Cora and outperforms all baselines on PubMed.Besides, our method offers a distinct advantage in terms of interpretability.Unlike the baselines, our method can provide natural language explanations for the predictions, enhancing the transparency and comprehensibility of the model's decision-making process.This interpretability aspect is particularly valuable in scenarios where understanding the reasoning behind the predictions is crucial.</p>
<p>The result comparison with the LLM predictor baselines is shown in Table 4.Although neighbor summary prompt achieves better performance than 0-hop prompt by considering neighborhood information, our method outperforms these baselines by a large margin, indicating that our method enables LLMs to effectively learn and utilize task-relevant knowledge and benefits the downstream task prediction.</p>
<p>Interpretability with Structural Information</p>
<p>We showcase some examples to illustrate the interpretation of our method.The results as well as the comparison with two LLM predictor baselines are shown in Table 5.</p>
<p>The results indicate that our proposed method is more capa-ble of predicting the correct label on the target node while the other methods fail.Moreover, our method also generates explanations related to neighborhood information after giving the prediction.For example, in the first case, our method predicts the paper belongs to "rule learning", and a supporting evidence for this prediction is "the paper also cites several references related to rule-based machine learning."This supporting evidence is exactly derived from the neighborhood information on the TAG, which demonstrates that our method can effectively capture the semantic information on the graph to make more accurate predictions.In case 2, our method predicts that the book belongs to military history.In addition to making predictions based on the content of the book, it also utilize the neighborhood information in the TAG to assist in the prediction process, as it states "there are also other books that are frequently purchased together, which are related to military history, such as 'The Second World War' and 'The Cold War'".</p>
<p>The shown cases demonstrate that our method generates interpretations for classification predictions that are accurate and structurally relevant.By contrast, the baselines produce incorrect predictions without using structural information for interpretation.Our method effectively integrates the graph's structural information in LLM generation, providing meaningful insights and justifications for the classification results.The experimental results highlight the importance of incorporating structural information in achieving accurate and interpretable predictions in TAG tasks.More importantly, through our approach, humans can harness intuitive interpretations based on graph structures when using LLMs to tackle TAG problems.This greatly enhances the utilization of knowledge embedded in TAG and unleashes the potential of LLMs on graphs.</p>
<p>Ablation Study</p>
<p>In this section, we evaluate the effectiveness of the disentanglement component of our method through an ablation study.The results are also shown in Table 4.The results demonstrate that the disentanglement is greatly beneficial and crucial for our method to learn better structural information and enables the downstream LLM to give more accurate predictions accordingly.</p>
<p>Conclusion</p>
<p>In conclusion, this paper addresses the challenge of effectively integrating structural information into large language models (LLMs) for text-attributed graph (TAG) tasks.</p>
<p>We propose the Disentangled Graph Text Learner (DGTL) model, which leverages tailored disentangled graph neural network layers to capture complex graph structure information in TAGs.Our method enhances the reasoning and prediction capabilities of LLMs while providing natural language explanations for model predictions, which is crucial for interpretability in TAG tasks.Through extensive experiments on various TAG benchmarks, we demonstrate that DGTL achieves competitive performance compared to stateof-the-art baselines while offering human-understandable explanations for model predictions.Our work contributes to advancing the field of TAG analysis by harnessing the power of LLMs and improving their interpretability for real-world applications.</p>
<p>Figure 1 .
1
Figure 1.An overview of our proposed DGTL method.Step 1: Generating text embedding by taking average of the features at the last layer in the upstream LLM.Step 2: Using our proposed disentangled graph learning to learn embeddings with diverse structural information.Step 3: Injecting the features with neighborhood information into the downstream LLM.</p>
<p>Algorithm 1
1
The training process of DGTL Require: Text-attributed graph G = (T, A), LLM L up , L down Ensure: Parameters of the disentangled GNN θ 1: Get node embedding H (0) of T by L up 2: Initialize θ randomly 3: while not converge do 4: Sample a mini-batch from the training set 5:</p>
<p>Table 1 .
1
Dataset Statistics.
DatasetNodesEdgesClassesDomainCora2,7085,4297AcademicPubMed19,71744,3383AcademicBooks-History41,551358,57412E-commerce</p>
<p>Table 2 .
2
The illustration of prompts and typical responses of different types of datasets.The blue parts indicate the texts populated based on the content of the dataset, the red parts indicate the embeddings obtained by fine-tuning based on neighborhood information.and the green parts indicate the prediction given by the LLM.
DatasetContentCitation DatasetPrompt</p>
<p>Table 3 .
3
Performance comparison with GNN predictor baselines.
DatasetCoraPubMedGNN backboneGCNGATMLPGCNGATMLPTF-IDF81.99±0.63 82.30±0.65 67.18±1.01 78.86±2.00 77.65±0.91 71.07±0.78Word2Vec74.01±1.24 72.32±0.17 55.34±1.31 70.10±1.80 69.30±0.66 63.48±0.54Deberta-base48.49±1.86 51.02±1.22 30.40±0.57 62.08±0.06 62.63±0.27 53.50±0.43Fine-tuned Deberta-base 59.23±1.16 57.38±2.01 30.98±0.68 62.12±0.07 61.57±0.07 53.65±0.26Llama-7B66.80±2.20 59.74±1.53 52.88±1.96 73.53±0.06 67.52±0.07 66.07±0.56Sentence-BERT82.20±0.49 82.77±0.59 74.26±1.44 81.01±1.32 79.08±0.07 76.66±0.50e5-large82.56±0.73 81.62±1.09 74.26±0.93 82.63±1.13 79.67±0.80 80.38±1.94GLEM-GNN48.49±1.86 51.02±1.22-62.08±0.06 62.63±0.27-GLEM-LM59.23±1.16 57.38±2.01-62.12±0.07 61.57±0.07-DGTL81.10±0.20 80.40±0.42-87.10±1.31 85.63±1.54-</p>
<p>Table 4 .
4
Performance comparison with LLM predictor baselines and the ablation model.
MethodCoraPubMedBooks-History0-hop60.8062.3017.70Neighbor Summary77.3067.7017.90DGTL81.10±0.2087.10±1.5455.70±0.89DGTL w/o disen80.67±0.4785.00±1.4755.40±1.04</p>
<p>Table 5 .
5
The interpretation comparison with LLM predictor methods on two cases.Case 1: a paper classification task on Cora dataset.Case 2: a book classification task on Book-History dataset.The blue parts are the labels.The red parts are the wrong predictions, while the green parts are the correct predictions and the corresponding structurally relevant interpretations.Based on the content of the paper, the most appropriate category for this paper would be 'rule learning'.The paper is discussing the use of a hybrid branch predictor scheme that uses a limited form of dual path execution along with dynamic branch prediction to improve execution times, which is a type of rule-based approach.The paper also cites several references related to rule-based machine learning, • • • Based on the information provided, the book "One Thousand Miles" by Matt Matthews belongs to the category of "Memoir/Biography" or "History".The book is described as a "compelling journey through the past and present" that explores themes of identity, family, and the human experience, and draws on a wide range of sources including autobiography, history, and culture.••• Based on the book description, the most appropriate category for "Matt Matthews One Thousand Miles" would be "Americas" or "North American Historical Study."Thebook is described as a story of a son's pilgrimage into the heart of his father's story, set against the backdrop of World War II and the generation that endured it.•••Based on the information of the book, the most appropriate category for this book would be 'Military History'.The book is about a son's journey to understand his father's experience during WWII, which is a historical event.There are also other books that are frequently purchased together, which are related to military history, such as 'The Second World War' and 'The Cold War'.Therefore, the most appropriate category for this book would be 'Military History'.
Text0-hopNeighbor SummaryDGTL(label: rule learning)Based on the content of the pa-Based on the content of the pa-Here is a paper titleper, the most appropriate cat-per, the most appropriate cat-and abstract: Limitedegory for this paper would beegory for this paper would beDual Path Execution. :"theoretical" or "probabilistic'theory'. The paper presentsThis work presents amethods". The paper presentsa new approach to branch pre-hybrid branch predictora new approach to branch pre-diction and speculative execu-scheme that uses a lim-diction that uses a hybrid pre-tion, and proposes a hybrid pre-ited form of dual pathdictor model that combines dy-dictor model that combines dy-execution along with dy-namic branch prediction withnamic branch prediction andnamic branch predictionlimited dual path execution.• • •dual path execution. • • •to improve executiontimes.• • •(label:military) Here isa book description andtitle: Description: Be-tween Frederick Buechn-ers profound autobiogra-phy Sacred Journey andRussell Bakers warmand humorous GrowingUp, Matt Matthews OneThousand Miles chartsa compelling journey allits own through the pastand the present.• • •</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, arXiv:2307.033932023arXiv preprint</p>
<p>Parameterefficient fine-tuning of large-scale pre-trained language models. N Ding, Y Qin, G Yang, F Wei, Z Yang, Y Su, S Hu, Y Chen, C.-M Chan, W Chen, Nature Machine Intelligence. 532023</p>
<p>K Duan, Q Liu, T.-S Chua, S Yan, W T Ooi, Q Xie, J He, Simteg, arXiv:2308.02565A frustratingly simple approach improves textual graph learning. 2023arXiv preprint</p>
<p>C Gao, X Lan, Z Lu, J Mao, J Piao, H Wang, D Jin, Y Li, S3, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 2023arXiv preprint</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, International conference on machine learning. 2017</p>
<p>Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, Gpt4graph, arXiv:2305.150662023arXiv preprint</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Advances in neural information processing systems. 2017</p>
<p>Decodingenhanced bert with disentangled attention. P He, X Liu, J Gao, W Chen, Deberta, arXiv:2006.036542020arXiv preprint</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, Advances in neural information processing systems. 2020</p>
<p>Prompt-based node feature extractor for fewshot learning on text-attributed graphs. X Huang, K Han, D Bao, Q Tao, Z Zhang, Y Yang, Q Zhu, arXiv:2309.028482023arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Proceedings of the 5th International Conference on Learning Representations. the 5th International Conference on Learning Representations2017</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Fusion of text and graph information for machine learning problems on networks. I Makarov, M Makarov, D Kiselev, PeerJ Computer Science. 7e5262021</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, 201326Advances in neural information processing systems</p>
<p>Squad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Collective classification in network data. P Sen, G Namata, M Bilgic, L Getoor, B Galligher, T Eliassi-Rad, AI magazine. 2932008. Oct 2023</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, Proceedings of the 6th International Conference on Learning Representations. the 6th International Conference on Learning Representations2018</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, arXiv:2305.100372023arXiv preprint</p>
<p>L Wang, N Yang, X Huang, B Jiao, L Yang, D Jiang, R Majumder, F Wei, arXiv:2212.03533Text embeddings by weakly-supervised contrastive pre-training. 2022arXiv preprint</p>
<p>A survey on large language models for recommendation. L Wu, Z Zheng, Z Qiu, H Wang, H Gu, T Shen, C Qin, C Zhu, H Zhu, Q Liu, arXiv:2305.198602023arXiv preprint</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, 7th International Conference on Learning Representations. 2019</p>
<p>Bert-enhanced text graph neural network for classification. Y Yang, X Cui, Entropy. 231115362021</p>
<p>R Ye, C Zhang, R Wang, S Xu, Y Zhang, arXiv:2308.07134Natural language is all a graph needs. 2023arXiv preprint</p>
<p>Graph-bert: Only attention is needed for learning graph representations. J Zhang, H Zhang, C Xia, L Sun, arXiv:2001.051402020arXiv preprint</p>
<p>Understanding bagof-words model: a statistical framework. Y Zhang, R Jin, Z.-H Zhou, International journal of machine learning and cybernetics. 12010</p>
<p>Z Zhang, H Li, Z Zhang, Y Qin, X Wang, W Zhu, arXiv:2308.14522Large graph models: A perspective. 2023arXiv preprint</p>
<p>Learning on large-scale text-attributed graphs via variational inference. J Zhao, M Qu, C Li, H Yan, Q Liu, R Li, X Xie, J Tang, The Eleventh International Conference on Learning Representations. 2022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
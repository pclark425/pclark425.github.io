<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1173 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1173</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1173</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-227162474</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2011.12491v3.pdf" target="_blank">World Model as a Graph: Learning Latent Landmarks for Planning</a></p>
                <p><strong>Paper Abstract:</strong> Planning - the ability to analyze the structure of a problem in the large and decompose it into interrelated subproblems - is a hallmark of human intelligence. While deep reinforcement learning (RL) has shown great promise for solving relatively straightforward control tasks, it remains an open problem how to best incorporate planning into existing deep RL paradigms to handle increasingly complex environments. One prominent framework, Model-Based RL, learns a world model and plans using step-by-step virtual rollouts. This type of world model quickly diverges from reality when the planning horizon increases, thus struggling at long-horizon planning. How can we learn world models that endow agents with the ability to do temporally extended reasoning? In this work, we propose to learn graph-structured world models composed of sparse, multi-step transitions. We devise a novel algorithm to learn latent landmarks that are scattered (in terms of reachability) across the goal space as the nodes on the graph. In this same graph, the edges are the reachability estimates distilled from Q-functions. On a variety of high-dimensional continuous control tasks ranging from robotic manipulation to navigation, we demonstrate that our method, named L3P, significantly outperforms prior work, and is oftentimes the only method capable of leveraging both the robustness of model-free RL and generalization of graph-search algorithms. We believe our work is an important step towards scalable planning in reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1173.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1173.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L3P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Latent Landmarks for Planning (L3P)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-structured latent world model that learns a small set of decoded latent centroids (landmarks) scattered across goal space, with directed edges weighted by learned reachability estimates; planning is performed over this sparse multi-step transition graph using a soft-Floyd graph search and an online planner that enforces temporal abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>L3P (World Model as a Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space world model: an auto-encoder maps goals to a constrained latent space (encoder f_E, decoder f_D) where distances are regularized to reflect temporal reachability; clustering (mixture of Gaussians / ELBO + Greedy Latent Sparsification) produces N latent centroids whose decodings become graph nodes (decoded landmarks). Edge weights between nodes (and to the episode goal) are negative reachability estimates -V(decoded_i, decoded_j) where V is regressed to estimate step-distance; graph search (soft-Floyd) computes multi-step distances and an online planner selects decoded landmarks as subgoals, using the reachability estimate to decide how long to pursue a subgoal (temporal abstraction).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / graph-structured world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous-control robotic navigation and manipulation (PointMaze, AntMaze, Fetch manipulation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss L_rec (MSE) for auto-encoder; latent-constraint loss L_latent (squared Euclidean distance in latent space vs. 0.5*(V(g1,g2)+V(g2,g1)) ) to align latent distances with reachability; regression loss (MSE) for V towards D (distance-from-Q estimates). Graph edge trust is gated by an edge cutoff d_max (clipping).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Relatively interpretable: learned landmarks correspond to decoded goal states (visualizable); latent-space clustering yields human-meaningful waypoints scattered across reachable regions; the graph structure (nodes and directed weighted edges) is explicit and can be visualized as landmarks + chosen paths. Latent dimensions are constrained to reflect reachability rather than purely abstract factors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualizing decoded centroids (landmark states) in environment state space; plotting landmark locations and chosen landmark sequences on mazes; clustering in latent space (mixture of Gaussians) yields interpretable nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantitatively specified; design choices reduce graph size to N landmarks (e.g., N≈50 in AntMaze experiments) versus using full replay (hundreds to tens of thousands). Uses soft-Floyd relaxation for graph search run per episode (S iterations) and online computation of d_s→c (distance estimates from current state to each decoded landmark) via D(s,π(...),landmark) queries. Training requires joint optimization of policy π, Q/D, V, encoder/decoder, and clustering centroids; no GPU/CPU-hour counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitatively more sample-efficient and computationally lean than replay-buffer graph methods that use full/subsampled replay (SORB/MSS) because L3P uses a small learned landmark set (e.g., 50) instead of hundreds/thousands of nodes; authors report better sample efficiency and stability, but no absolute run-time or FLOP comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported qualitatively: L3P achieves higher asymptotic performance, faster learning, and better generalization to longer-horizon test goals (e.g., solving AntMaze-Hard consistently and outperforming HER, SORB, MSS on several navigation and Fetch manipulation tasks). Exact numeric success rates and learning curves are presented in the paper figures but numerical values are not enumerated in the text body.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The world model prioritizes task-relevant reachability over full-fidelity reconstruction: latent constraints force distances to reflect number-of-steps reachability (via V/D) rather than only minimizing reconstruction error, which improves planning utility for long-horizon goals. The sparse landmark graph improves scalability and generalization: high utility for temporally-extended planning even if local distance estimates are noisy because soft graph operations and temporal abstraction mitigate errors.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs highlighted include sensitivity to the local-edge trust threshold d_max (too large hurts local validity; too small restricts connectivity), and a tension between using hard max (faster but unstable) vs softmax relaxations (slower/tamer but more robust). Reducing the number of landmarks increases abstraction but risks coarser planning; L3P shows robustness to a range of landmark counts. Overall, L3P trades some per-step predictive fidelity for a compact, task-focused representation that yields better long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: (1) constrain auto-encoder latent metric with reachability loss using V, (2) parameterize Q to extract step-distance D(s,a,g), (3) regress V to D to get goal→goal reachability, (4) latent clustering (mixture of Gaussians, ELBO) with Greedy Latent Sparsification for initialization and mini-batch training, (5) decode centroids as graph nodes, (6) use negative V as edge weights, clip edges beyond d_max to −∞, (7) perform soft-Floyd graph search and an online planner that commits to subgoals for K predicted steps and removes immediate previous landmark on re-plan.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to classical MBRL (step-by-step learned dynamics), L3P avoids long-horizon model compounding by planning over sparse multi-step transitions (landmarks) instead of action-sequence rollouts; compared to replay-buffer graph approaches (SORB/MSS) L3P uses far fewer learned nodes (e.g., 50 vs hundreds/tens of thousands) improving sample efficiency and stability; compared to model-free HER, L3P improves sample efficiency and generalization for long-horizon tasks. No hard computational benchmarks are provided — comparisons are qualitative and based on success rates and learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations/insights: use soft relaxations in graph search for robustness (soft-Floyd with temperature β), tune edge cutoff d_max carefully to trust only local distance estimates, learn a moderate number of landmarks (L3P is robust across a range, e.g., ~50 used in hard mazes), and use temporal abstraction in the online planner (commit to subgoal for predicted K steps and drop previous landmark on re-plan). The paper suggests these choices balance fidelity (local reachability trust), interpretability (decoded landmarks), efficiency (small N), and utility (robust long-horizon planning).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1173.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1173.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBRL (step-by-step)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-Based Reinforcement Learning with step-by-step rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Traditional model-based RL approach that learns a dynamics/world model to perform action-sequence rollouts (MPC-style or Dyna-style) for planning, planning at the level of individual actions over short horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Traditional MBRL (action-sequence dynamics model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Explicit dynamics predictor (neural network) over next-state or latent-next-state, used to generate imagined rollouts at the action-sequence level for planning (MPC, Dyna); typically used together with model predictive control or policy optimization on imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit learned dynamics / latent dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotics and continuous control tasks (general robotics context mentioned in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured by next-state prediction error (MSE) and downstream policy performance; in the paper MBRL fidelity is discussed qualitatively as diverging over long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally a black-box neural dynamics predictor; interpretability varies by representation but not emphasized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>none mentioned in this paper; classical MBRL sometimes inspects rollouts or latent states but not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High per-episode planning cost because planning occurs at the action-sequence level (rollouts at each 100ms step in robotics), and accuracy degrades as horizon increases; specific compute numbers not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper argues MBRL suffers from compounding error and thus is bottlenecked by model accuracy and planning horizon — less effective for long-horizon planning compared to L3P's sparse multi-step graph approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Described as effective on short-horizon or well-modelled tasks but struggles on long-horizon robotics tasks due to model divergence; no numeric comparisons provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High per-step fidelity (short-term predictions) can be useful for immediate control, but this fidelity does not translate to robust long-horizon plans due to model error accumulation; hence utility falls for temporally extended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High short-term fidelity and fine-grained control vs poor long-horizon robustness and high computational cost from frequent re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Plan at action-sequence granularity, rely on explicit dynamics models and frequent re-planning (MPC style); trade-off between model expressiveness and horizon length.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to L3P, MBRL is more granular but less robust for long-horizon tasks; L3P trades per-step predictive completeness for sparse, multi-step transitions that generalize better across long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that for long-horizon, complex robotics tasks, planning over multi-step sparse transitions (as in L3P) is preferable to action-level rollouts; no single optimal MBRL configuration is specified.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1173.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1173.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Replay-Graph Methods (SORB / MSS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replay-buffer graph planning methods (e.g., SORB; Mapping State Space - MSS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that build graphs for planning using stored experiences: SORB searches on the replay buffer (many vertices) while MSS reduces vertices by sub-sampling the replay buffer; both perform graph search over replay-derived nodes with distance estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Search on the replay buffer: Bridging planning and reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Replay-buffer graph models (SORB, MSS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-parametric graph world model derived from stored transitions: nodes correspond to states/goals stored in replay buffer (SORB uses entire buffer; MSS subsamples), edges weighted by learned or empirical reachability/distance estimates; planning is performed by graph search over these stored states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>non-parametric graph world model (experience replay based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>goal-conditioned RL for navigation and control (same benchmark domains as L3P: mazes, continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Implicitly measured via success rate of graph search and accuracy of stored-edge distance estimates; authors discuss scaling / noise issues as buffer size grows rather than formal fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Nodes are actual stored states (interpretable as concrete visited states); however large node counts (hundreds to tens of thousands) make global interpretation harder.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Direct inspection of stored replay states / plotted paths; no further interpretability methods discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High computational and memory cost when using entire replay as nodes (SORB uses tens of thousands of landmarks in cited experiments); MSS reduces cost by subsampling but still has more nodes than L3P's learned sparse landmark set. Exact compute costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper reports that SORB and MSS can be less sample-efficient and less stable as buffer size increases; L3P's learned compact landmark set (e.g., 50 nodes) is presented as more sample-efficient and stable in experiments like AntMaze-Hard.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported to perform worse than L3P on some long-horizon generalization tasks (SORB and MSS fail more often on AntMaze-Hard in the presented comparisons); precise numeric values are plotted in the paper but not quoted in body text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Direct replay-based graphs can provide fine-grained coverage but suffer from scalability and stability issues (wormholes in distance estimates, frequent re-planning leading to loops). Utility degrades when buffer-driven graphs become very large or when distance estimates are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High coverage (many stored nodes) vs computational/memory cost and instability (wormholes, frequent subgoal switching). Subsampling (MSS) reduces nodes but may lose coverage; L3P trades off coverage for compact learned landmarks that better reflect reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of entire or subsampled replay as graph vertices; rely on learned distance estimates between stored states; often require an edge cutoff (d_max) and heuristics to avoid wormholes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to L3P, replay-graph methods use concrete stored states which can be plentiful but noisy and computationally heavy; L3P learns compact, latent landmarks with explicit clustering and reachability constraints to produce a smaller, more useful planning graph.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies that reducing node count and ensuring local trust (edge cutoff) are important for practicality; suggests learned sparse landmarks (L3P) plus temporal abstraction is a superior configuration for long-horizon tasks, but does not prescribe a single optimal replay-graph configuration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Search on the replay buffer: Bridging planning and reinforcement learning <em>(Rating: 2)</em></li>
                <li>Mapping state space using landmarks for universal goal reaching <em>(Rating: 2)</em></li>
                <li>Plan2vec: Unsupervised representation learning by latent plans <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1173",
    "paper_id": "paper-227162474",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "L3P",
            "name_full": "Learning Latent Landmarks for Planning (L3P)",
            "brief_description": "A graph-structured latent world model that learns a small set of decoded latent centroids (landmarks) scattered across goal space, with directed edges weighted by learned reachability estimates; planning is performed over this sparse multi-step transition graph using a soft-Floyd graph search and an online planner that enforces temporal abstraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "L3P (World Model as a Graph)",
            "model_description": "Latent-space world model: an auto-encoder maps goals to a constrained latent space (encoder f_E, decoder f_D) where distances are regularized to reflect temporal reachability; clustering (mixture of Gaussians / ELBO + Greedy Latent Sparsification) produces N latent centroids whose decodings become graph nodes (decoded landmarks). Edge weights between nodes (and to the episode goal) are negative reachability estimates -V(decoded_i, decoded_j) where V is regressed to estimate step-distance; graph search (soft-Floyd) computes multi-step distances and an online planner selects decoded landmarks as subgoals, using the reachability estimate to decide how long to pursue a subgoal (temporal abstraction).",
            "model_type": "latent world model / graph-structured world model",
            "task_domain": "continuous-control robotic navigation and manipulation (PointMaze, AntMaze, Fetch manipulation tasks)",
            "fidelity_metric": "Reconstruction loss L_rec (MSE) for auto-encoder; latent-constraint loss L_latent (squared Euclidean distance in latent space vs. 0.5*(V(g1,g2)+V(g2,g1)) ) to align latent distances with reachability; regression loss (MSE) for V towards D (distance-from-Q estimates). Graph edge trust is gated by an edge cutoff d_max (clipping).",
            "fidelity_performance": null,
            "interpretability_assessment": "Relatively interpretable: learned landmarks correspond to decoded goal states (visualizable); latent-space clustering yields human-meaningful waypoints scattered across reachable regions; the graph structure (nodes and directed weighted edges) is explicit and can be visualized as landmarks + chosen paths. Latent dimensions are constrained to reflect reachability rather than purely abstract factors.",
            "interpretability_method": "Visualizing decoded centroids (landmark states) in environment state space; plotting landmark locations and chosen landmark sequences on mazes; clustering in latent space (mixture of Gaussians) yields interpretable nodes.",
            "computational_cost": "Not quantitatively specified; design choices reduce graph size to N landmarks (e.g., N≈50 in AntMaze experiments) versus using full replay (hundreds to tens of thousands). Uses soft-Floyd relaxation for graph search run per episode (S iterations) and online computation of d_s→c (distance estimates from current state to each decoded landmark) via D(s,π(...),landmark) queries. Training requires joint optimization of policy π, Q/D, V, encoder/decoder, and clustering centroids; no GPU/CPU-hour counts reported.",
            "efficiency_comparison": "Qualitatively more sample-efficient and computationally lean than replay-buffer graph methods that use full/subsampled replay (SORB/MSS) because L3P uses a small learned landmark set (e.g., 50) instead of hundreds/thousands of nodes; authors report better sample efficiency and stability, but no absolute run-time or FLOP comparisons are provided.",
            "task_performance": "Reported qualitatively: L3P achieves higher asymptotic performance, faster learning, and better generalization to longer-horizon test goals (e.g., solving AntMaze-Hard consistently and outperforming HER, SORB, MSS on several navigation and Fetch manipulation tasks). Exact numeric success rates and learning curves are presented in the paper figures but numerical values are not enumerated in the text body.",
            "task_utility_analysis": "The world model prioritizes task-relevant reachability over full-fidelity reconstruction: latent constraints force distances to reflect number-of-steps reachability (via V/D) rather than only minimizing reconstruction error, which improves planning utility for long-horizon goals. The sparse landmark graph improves scalability and generalization: high utility for temporally-extended planning even if local distance estimates are noisy because soft graph operations and temporal abstraction mitigate errors.",
            "tradeoffs_observed": "Trade-offs highlighted include sensitivity to the local-edge trust threshold d_max (too large hurts local validity; too small restricts connectivity), and a tension between using hard max (faster but unstable) vs softmax relaxations (slower/tamer but more robust). Reducing the number of landmarks increases abstraction but risks coarser planning; L3P shows robustness to a range of landmark counts. Overall, L3P trades some per-step predictive fidelity for a compact, task-focused representation that yields better long-horizon planning.",
            "design_choices": "Key choices: (1) constrain auto-encoder latent metric with reachability loss using V, (2) parameterize Q to extract step-distance D(s,a,g), (3) regress V to D to get goal→goal reachability, (4) latent clustering (mixture of Gaussians, ELBO) with Greedy Latent Sparsification for initialization and mini-batch training, (5) decode centroids as graph nodes, (6) use negative V as edge weights, clip edges beyond d_max to −∞, (7) perform soft-Floyd graph search and an online planner that commits to subgoals for K predicted steps and removes immediate previous landmark on re-plan.",
            "comparison_to_alternatives": "Compared to classical MBRL (step-by-step learned dynamics), L3P avoids long-horizon model compounding by planning over sparse multi-step transitions (landmarks) instead of action-sequence rollouts; compared to replay-buffer graph approaches (SORB/MSS) L3P uses far fewer learned nodes (e.g., 50 vs hundreds/tens of thousands) improving sample efficiency and stability; compared to model-free HER, L3P improves sample efficiency and generalization for long-horizon tasks. No hard computational benchmarks are provided — comparisons are qualitative and based on success rates and learning curves.",
            "optimal_configuration": "Paper recommendations/insights: use soft relaxations in graph search for robustness (soft-Floyd with temperature β), tune edge cutoff d_max carefully to trust only local distance estimates, learn a moderate number of landmarks (L3P is robust across a range, e.g., ~50 used in hard mazes), and use temporal abstraction in the online planner (commit to subgoal for predicted K steps and drop previous landmark on re-plan). The paper suggests these choices balance fidelity (local reachability trust), interpretability (decoded landmarks), efficiency (small N), and utility (robust long-horizon planning).",
            "uuid": "e1173.0"
        },
        {
            "name_short": "MBRL (step-by-step)",
            "name_full": "Model-Based Reinforcement Learning with step-by-step rollouts",
            "brief_description": "Traditional model-based RL approach that learns a dynamics/world model to perform action-sequence rollouts (MPC-style or Dyna-style) for planning, planning at the level of individual actions over short horizons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Traditional MBRL (action-sequence dynamics model)",
            "model_description": "Explicit dynamics predictor (neural network) over next-state or latent-next-state, used to generate imagined rollouts at the action-sequence level for planning (MPC, Dyna); typically used together with model predictive control or policy optimization on imagined rollouts.",
            "model_type": "explicit learned dynamics / latent dynamics model",
            "task_domain": "robotics and continuous control tasks (general robotics context mentioned in paper)",
            "fidelity_metric": "Typically measured by next-state prediction error (MSE) and downstream policy performance; in the paper MBRL fidelity is discussed qualitatively as diverging over long horizons.",
            "fidelity_performance": null,
            "interpretability_assessment": "Generally a black-box neural dynamics predictor; interpretability varies by representation but not emphasized in this paper.",
            "interpretability_method": "none mentioned in this paper; classical MBRL sometimes inspects rollouts or latent states but not discussed here.",
            "computational_cost": "High per-episode planning cost because planning occurs at the action-sequence level (rollouts at each 100ms step in robotics), and accuracy degrades as horizon increases; specific compute numbers not reported.",
            "efficiency_comparison": "Paper argues MBRL suffers from compounding error and thus is bottlenecked by model accuracy and planning horizon — less effective for long-horizon planning compared to L3P's sparse multi-step graph approach.",
            "task_performance": "Described as effective on short-horizon or well-modelled tasks but struggles on long-horizon robotics tasks due to model divergence; no numeric comparisons provided in text.",
            "task_utility_analysis": "High per-step fidelity (short-term predictions) can be useful for immediate control, but this fidelity does not translate to robust long-horizon plans due to model error accumulation; hence utility falls for temporally extended tasks.",
            "tradeoffs_observed": "High short-term fidelity and fine-grained control vs poor long-horizon robustness and high computational cost from frequent re-planning.",
            "design_choices": "Plan at action-sequence granularity, rely on explicit dynamics models and frequent re-planning (MPC style); trade-off between model expressiveness and horizon length.",
            "comparison_to_alternatives": "Compared to L3P, MBRL is more granular but less robust for long-horizon tasks; L3P trades per-step predictive completeness for sparse, multi-step transitions that generalize better across long horizons.",
            "optimal_configuration": "Paper suggests that for long-horizon, complex robotics tasks, planning over multi-step sparse transitions (as in L3P) is preferable to action-level rollouts; no single optimal MBRL configuration is specified.",
            "uuid": "e1173.1"
        },
        {
            "name_short": "Replay-Graph Methods (SORB / MSS)",
            "name_full": "Replay-buffer graph planning methods (e.g., SORB; Mapping State Space - MSS)",
            "brief_description": "Methods that build graphs for planning using stored experiences: SORB searches on the replay buffer (many vertices) while MSS reduces vertices by sub-sampling the replay buffer; both perform graph search over replay-derived nodes with distance estimates.",
            "citation_title": "Search on the replay buffer: Bridging planning and reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "Replay-buffer graph models (SORB, MSS)",
            "model_description": "Non-parametric graph world model derived from stored transitions: nodes correspond to states/goals stored in replay buffer (SORB uses entire buffer; MSS subsamples), edges weighted by learned or empirical reachability/distance estimates; planning is performed by graph search over these stored states.",
            "model_type": "non-parametric graph world model (experience replay based)",
            "task_domain": "goal-conditioned RL for navigation and control (same benchmark domains as L3P: mazes, continuous control)",
            "fidelity_metric": "Implicitly measured via success rate of graph search and accuracy of stored-edge distance estimates; authors discuss scaling / noise issues as buffer size grows rather than formal fidelity metrics.",
            "fidelity_performance": null,
            "interpretability_assessment": "Nodes are actual stored states (interpretable as concrete visited states); however large node counts (hundreds to tens of thousands) make global interpretation harder.",
            "interpretability_method": "Direct inspection of stored replay states / plotted paths; no further interpretability methods discussed in this paper.",
            "computational_cost": "High computational and memory cost when using entire replay as nodes (SORB uses tens of thousands of landmarks in cited experiments); MSS reduces cost by subsampling but still has more nodes than L3P's learned sparse landmark set. Exact compute costs not reported.",
            "efficiency_comparison": "Paper reports that SORB and MSS can be less sample-efficient and less stable as buffer size increases; L3P's learned compact landmark set (e.g., 50 nodes) is presented as more sample-efficient and stable in experiments like AntMaze-Hard.",
            "task_performance": "Reported to perform worse than L3P on some long-horizon generalization tasks (SORB and MSS fail more often on AntMaze-Hard in the presented comparisons); precise numeric values are plotted in the paper but not quoted in body text.",
            "task_utility_analysis": "Direct replay-based graphs can provide fine-grained coverage but suffer from scalability and stability issues (wormholes in distance estimates, frequent re-planning leading to loops). Utility degrades when buffer-driven graphs become very large or when distance estimates are noisy.",
            "tradeoffs_observed": "High coverage (many stored nodes) vs computational/memory cost and instability (wormholes, frequent subgoal switching). Subsampling (MSS) reduces nodes but may lose coverage; L3P trades off coverage for compact learned landmarks that better reflect reachability.",
            "design_choices": "Use of entire or subsampled replay as graph vertices; rely on learned distance estimates between stored states; often require an edge cutoff (d_max) and heuristics to avoid wormholes.",
            "comparison_to_alternatives": "Compared to L3P, replay-graph methods use concrete stored states which can be plentiful but noisy and computationally heavy; L3P learns compact, latent landmarks with explicit clustering and reachability constraints to produce a smaller, more useful planning graph.",
            "optimal_configuration": "Paper implies that reducing node count and ensuring local trust (edge cutoff) are important for practicality; suggests learned sparse landmarks (L3P) plus temporal abstraction is a superior configuration for long-horizon tasks, but does not prescribe a single optimal replay-graph configuration.",
            "uuid": "e1173.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Search on the replay buffer: Bridging planning and reinforcement learning",
            "rating": 2,
            "sanitized_title": "search_on_the_replay_buffer_bridging_planning_and_reinforcement_learning"
        },
        {
            "paper_title": "Mapping state space using landmarks for universal goal reaching",
            "rating": 2,
            "sanitized_title": "mapping_state_space_using_landmarks_for_universal_goal_reaching"
        },
        {
            "paper_title": "Plan2vec: Unsupervised representation learning by latent plans",
            "rating": 2,
            "sanitized_title": "plan2vec_unsupervised_representation_learning_by_latent_plans"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 1,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        }
    ],
    "cost": 0.01309925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>World Model as a Graph: Learning Latent Landmarks for Planning
2021</p>
<p>Lunjun Zhang 
University of Toronto</p>
<p>Ge Yang 
University of Toronto</p>
<p>Bradly Stadie 
University of Toronto</p>
<p>World Model as a Graph: Learning Latent Landmarks for Planning</p>
<p>2 Vector Institute 3 MIT 4 Toyota Tech- Proceedings of the 38 th International Conference on Machine Learning
2021nological Institute at Chicago. Correspondence to: Lunjun Zhang <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#117;&#110;&#106;&#117;&#110;&#64;&#99;&#115;&#46;&#116;&#111;&#114;&#111;&#110;&#116;&#111;&#46;&#101;&#100;&#117;">&#108;&#117;&#110;&#106;&#117;&#110;&#64;&#99;&#115;&#46;&#116;&#111;&#114;&#111;&#110;&#116;&#111;&#46;&#101;&#100;&#117;</a>.
Planning, the ability to analyze the structure of a problem in the large and decompose it into interrelated subproblems, is a hallmark of human intelligence. While deep reinforcement learning (RL) has shown great promise for solving relatively straightforward control tasks, it remains an open problem how to best incorporate planning into existing deep RL paradigms to handle increasingly complex environments. One prominent framework, Model-Based RL, learns a world model and plans using step-by-step virtual rollouts. This type of world model quickly diverges from reality when the planning horizon increases, thus struggling at long-horizon planning. How can we learn world models that endow agents with the ability to do temporally extended reasoning? In this work, we propose to learn graph-structured world models composed of sparse, multi-step transitions. We devise a novel algorithm to learn latent landmarks that are scattered (in terms of reachability) across the goal space as the nodes on the graph. In this same graph, the edges are the reachability estimates distilled from Q-functions. On a variety of high-dimensional continuous control tasks ranging from robotic manipulation to navigation, we demonstrate that our method, named L 3 P , significantly outperforms prior work, and is oftentimes the only method capable of leveraging both the robustness of model-free RL and generalization of graph-search algorithms. We believe our work is an important step towards scalable planning in reinforcement learning.</p>
<p>Introduction</p>
<p>An intelligent agent should be able to solve difficult problems by breaking them down into sequences of simpler problems. Classically, planning algorithms have been the tool of choice for endowing AI agents with the ability to reason over complex long-horizon problems (Doran &amp; Michie, 1966;Hart et al., 1968). Recent years have seen an uptick in monographs examining the intersection of classical planning techniques -which excel at temporal abstraction -with deep reinforcement learning (RL) algorithms -which excel at state abstraction. Perhaps the ripest fruit born of this relationship is the AlphaGo algorithm, wherein a model free policy is combined with a MCTS (Coulom, 2006) planning algorithm to achieve superhuman performance on the game of Go (Silver et al., 2016a).</p>
<p>In the field of robotics, progress on combining planning and reinforcement learning has been somewhat less rapid, although still resolute. Indeed, the laws of physics in the real world are vastly more complex than the simple rules of Go. Unlike board games such as chess and Go, which have deterministic and known dynamics and discrete action space, robots have to deal with a probabilistic and unpredictable world. Moreover, the action space for robotics is often continuous. As a result of these difficulties, planning in robotics presents a much harder problem. One general class of methods (Sutton, 1991) seeks to combine modelbased planning and deep RL. These methods can be thought of as an extension of model-predictive control (MPC) algorithms, with the key difference being that the agent is trained over hypothetical experience in addition to the actually collected experience. The primary shortcoming of this class of methods is that, like MCTS in AlphaGo, they resort to planning with action sequences -forcing the robot to plan for each action at every hundred milliseconds. Planning on the level of action sequences is fundamentally bottlenecked by the accuracy of the learned dynamics model and the horizon of a task, as the learned world model quickly diverges over a long horizon. This limitation shows that world models in the traditional Model-based RL (MBRL) setting often fail to deliver the promise of planning.</p>
<p>Another general class of methods, Hierarchical RL (HRL), introduces a higher-level learner to address the problem of arXiv:2011.12491v3 [cs.AI] 30 Jun 2021 Figure 1. MBRL versus L 3 P (World Model as a Graph). MBRL does step-by-step virtual rollouts with the world model and quickly diverges from reality when the planning horizon increases. L 3 P models the world as a graph of sparse multi-step transitions, where the nodes are learned latent landmarks and the edges are reachability estimates. L 3 P succeeds at temporally extended reasoning. Code for L 3 P is available at: https://github.com/ LunjunZhang/world-model-as-a-graph.</p>
<p>planning (Dayan &amp; Hinton, 1993;Vezhnevets et al., 2017;Nachum et al., 2018). In this case, a goal-based RL agent serves as the worker, and a manager learns what sequences of goals it must set for the worker to achieve a complex task. While this is apparently a sound solution to the problem of planning, hierarchical learners neither explicitly learn a higher-level model of the world nor take advantage of the graph structure inherent to the problem of search.</p>
<p>To better combine classical planning and reinforcement learning, we propose to learn graph-structured world models composed of sparse multi-step transitions. To model the world as a graph, we borrow a concept from the navigation literature -the idea of landmarks (Wang et al., 2008). Landmarks are essentially states that an agent can navigate between in order to complete tasks. However, rather than simply using previously seen states as landmarks, as is traditionally done, we will instead develop a novel algorithm to learn the landmarks used for planning. Our key insight is that by mapping previously achieved goals into a latent space that captures the temporal distance between goals, we can perform clustering in the latent space to group together goals that are easily reachable from one another. Subsequently, we can then decode the latent centroids to obtain a set of goals scattered (in terms of reachability) across the goal space. Since our learned landmarks are obtained from latent clustering, we call them latent landmarks. The chief algorithmic contribution of this paper is a new method for planning over learned latent landmarks for high-dimensional continuous control domains, which we name Learning Latent Landmarks for Planning (L 3 P ).</p>
<p>The idea of reducing planning in RL to a graph search problem has enjoyed some attention recently (Savinov et al., 2018a;Eysenbach et al., 2019;Huang et al., 2019;Liu et al., 2019;Yang et al., 2020;Emmons et al., 2020). A key difference between those works and L 3 P is that our use of learned latent landmarks allows us to substantially reduce the size of the search space. What's more, we make improvements to the graph search module and the online planning algorithm to improve the robustness and sample efficiency of our method. As a result of those decisions, our algorithm is able to achieve superior performance on a variety of robotics domains involving both navigation and manipulation. In addition to the results presented in Section 5, videos of our algorithm's performance and a more detailed analysis of the sub-tasks discovered by the latent landmarks can be found at: https://sites.google. com/view/latent-landmarks/.</p>
<p>Related Works</p>
<p>The problem of learning landmarks to aid in robotics problems has a long and rich history (Gillner &amp; Mallot, 1998;Wang &amp; Spelke, 2002;Wang et al., 2008). Prior art has been deeply rooted in the classical planning literature. For example, traditional methods would utilize (Dijkstra et al., 1959) to plan over generated waypoints, SLAM (Durrant-Whyte &amp; Bailey, 2006) to simultaneously integrate mapping, or the RRT algorithm (LaValle, 1998) for explicit path planning. The A* algorithm (Hart et al., 1968) further improved the computational efficiency of Dijkstra. Those types of methods often heavily rely on a hand-crafted configuration space that provides prior knowledge.</p>
<p>Planning is intimately related to model-based RL (MBRL), as the core ideas underlying learned models and planners can enjoy considerable overlap. Perhaps the most clear instance of this overlap is Model Predictive Control (MPC), and the related Dyna algorithm (Sutton, 1991). When combined with modern techniques (Kurutach et al., 2018;Luo et al., 2018;Nagabandi et al., 2018;Ha &amp; Schmidhuber, 2018;Hafner et al., 2019;Wang &amp; Ba, 2019;Janner et al., 2019), MBRL is able to achieve some level of success. (Corneil et al., 2018) and (Hafner et al., 2020) also learn a discrete latent representation of the environment in the MBRL framework. As discussed in the introduction, planning on action sequences will fundamentally struggle to scale in robotics.</p>
<p>Our method makes extensive use of a parametric goal-based RL agent to accomplish low-level navigation between states. This area has seen rapid progress recently, largely stemming from the success of Hindsight Experience Replay (HER) (Andrychowicz et al., 2017). Several improvements to HER augment the goal relabeling and sampling strategies to boost performance (Nair et al., 2018;Pong et al., 2018;Zhao et al., 2019;Pitis et al., 2020). There have also been attempts at incorporating search as inductive biases within the value function (Silver et al., 2016b;Tamar et al., 2016;Farquhar et al., 2017;Racanière et al., 2017;Lee et al., 2018;Srinivas et al., 2018). The focus of this line of work is to improve the low-level policy and is thus orthogonal to our work. Recent work in Hierarchical RL (HRL) builds upon goalbased RL by learning a high-level parametric manager that feeds goals to the low-level goal-based agent (Dayan &amp; Hinton, 1993;Vezhnevets et al., 2017;Nachum et al., 2018). This can be viewed as a parametric alternative to classical planning, as discussed in the introduction. Recently, (Jurgenson et al., 2020;Pertsch et al., 2020) have derived HRL methods that are intimately tied to tree search algorithms. These papers are further connected to a recent trend in the literature wherein classical search methods are combined with parametric control (Savinov et al., 2018a;Eysenbach et al., 2019;Huang et al., 2019;Liu et al., 2019;Yang et al., 2020;Emmons et al., 2020). Several of these articles will be discussed throughout this paper. LEAP (Nasiriany et al., 2019) also considers the problem of proposing sub-goals for a goal-conditioned agent: it uses a VAE (Kingma &amp; Welling, 2013) and does CEM on the prior distribution to form the landmarks. Our method constrains the latent space with temporal reachability between goals, a concept previously explored in (Savinov et al., 2018b), and uses latent clustering and graph search rather than sampling-based methods to learn and propose sub-goals.</p>
<p>Background</p>
<p>We consider the problem of Multi-Goal RL under a Markov Decision Process (MDP) parameterized by (S, A, P, G, Ψ, R, ρ 0 ). S and A are the state and action space. The probability distribution of the initial states is given by ρ 0 (s), and P(s |s, a) is the transition probability. Ψ : S → G is a mapping from the state space to the goal space, which assumes that every state s can be mapped to a corresponding achieved goal g. The reward function R can be defined as R(s, a, s , g) = −1{Ψ(s ) = g}. We further assume that each episode has a fixed horizon T .</p>
<p>A multi-goal policy is a probability distribution π : S × G × A → R + , which gives rise to trajectory samples of the form τ = {s 0 , a 0 , g, s 1 , · · · s T }. The purpose of the policy π is to learn how to reach the goals drawn from the goal distribution p g . With a discount factor γ ∈ (0, 1), it maximizes J (π) = E g∼pg,τ ∼π(g) [
T −1 t=0 γ t · R(s t , a t , s t+1 , g)].
Q-learning provides a sample-efficient way to optimize the above objective by utilizing off-policy data stored in a replay buffer B. Q(s, a, g) estimates the reward-to-go under the current policy π conditioned upon the given goal. An additional technique, called Hindsight Experience Replay, or HER (Andrychowicz et al., 2017), uses hindsight relabelling to drastically speed up training. This relabeling crucially relies upon the mapping Ψ : S → G in the multi-goal MDP setting. We can write the the joint objective of multi-goal Qlearning with HER as minimizing (with Q being the online network and Q being the target network):
Q(s t , a t , g) − R(s t+1 , g) + γ · Q(s t+1 , a , g) 2 (1) where τ ∼ B, t ∼ {0 · · · T − 1}, (s t , a t , s t+1 ) ∼ τ, k ∼ {t + 1 · · · T }, g = Ψ(s k ), a ∼ π(· | s t+1 , g).</p>
<p>The L 3 P Algorithm</p>
<p>Our overall objective in this section is to derive an algorithm that learns a small number of landmarks scattered across goal space in terms of reachability and use those learned landmarks for planning. There are three chief difficulties we must overcome when considering such an algorithm. First, how can we group together goals that are easily reachable from one another? The answer is to embed goals into a latent space, where the latent representation captures some notion of temporal distance between goals -in the sense that goals that would take many timesteps to navigate between are further apart in latent space. Second, we need to find a way to learn a sparse set of landmarks used for planning. Our method performs clustering on the constrained latent space, and decodes the learned centroids as the landmarks we seek. Finally, we need to develop a non-parametric planning algorithm responsible for selecting sequences of landmarks the agent must traverse to accomplish its high-level goal. The proposed online planning algorithm is simple, scalable, and robust.</p>
<p>Learning a Latent Space</p>
<p>Let us consider the following question: "How should we go about learning a latent space of goals where the metric re-flects reachability?" Suppose we have an auto-encoder (AE) in the agent's goal space, with deterministic encoder f E and decoder f D . As usual, the reconstruction loss is given by
L rec (g) = f D f E (g) − g 2 2 .
We want to make sure that the distance between two latent codes would roughly correspond to the number of steps it would take the policy to go from one goal to another. Concretely, for any pair of goals (g 1 , g 2 ), we optimize the following loss L latent (g 1 , g 2 ):
f E (g 1 ) − f E (g 2 ) 2 2 − 1 2 V (g 1 , g 2 ) + V (g 2 , g 1 ) 2
(2)</p>
<p>Where V : G × G → R + is a mapping that estimates how many steps it would take the policy π to go from one goal to another goal on average. By adding this constraint and solving a joint optimization L rec + λ · L latent , the encodingdecoding mapping can no longer be arbitrary, giving more structure to the latent space. Goals that are close by in terms of reachability will be naturally clustered in the latent space, and interpolations between latent codes will lead to meaningful results.</p>
<p>Of course, the constraint in Equation 2 is quite meaningless if we do not have a way to estimate the mapping V . We will proceed towards this objective by noting the following interesting connection between multi-goal Q-functions and reachability. In the multi-goal RL framework considered in the background section, the reward is binary in nature. The agent receives a reward of −1 until it reaches the goal, and then 0 when it reaches the desired goal. In this setting, the Q-function is implicitly estimating the number of steps it takes to reach the goal g from the current state s after the action a is taken. Denote this quantity as D(s, a, g), the Q-function can be re-written as:
Q(s, a, g) = D(s,a,g)−1 t=0 γ t · (−1) + T −1 t=D(s,a,g) γ t · 0 = − 1 − γ D(s,a,g) 1 − γ(3)
Choosing to parameterize Q-functions in this way disentangles the effect of γ on multi-goal Q-learning. It also provides us with access the direct distance estimation function D(s, a, g). We note that this distance is not a mathematical distance in the sense of a metric. Instead, we use the word distance to refer to the number of steps the policy π needs to take in the environment.</p>
<p>Given our tractable estimate of D, it is now a straightforward matter to estimate the desired quantity V , which approximates how many steps it takes the policy to transition between goals. To get the desired estimate, we regress V towards D by minimizing
min V D s t , a t , Ψ(s k ) − V Ψ(s t+1 ), Ψ(s k ) 2(4)
with τ ∼ B, t ∼ {0 · · · T − 1}, (s t , a t , s t+1 ) ∼ τ, k ∼ {t + 1 · · · T }, and Ψ being given by the environment to map the states to the goal space. One crucial detail is the use of Ψ(s t+1 ) rather than Ψ(s t ) in the inputs to V . This is due to the fact that D : S × A × G → R outputs the number of steps to go after an action is taken, when the state has transitioned into s t+1 . The objective above provides an unbiased estimate of the average number of steps between two goals.</p>
<p>The estimates D and V will prove useful beyond helping to optimize the auto-encoder in Equation 2. They will prove essential in weighting and planning over latent landmark nodes in Section 4.3.</p>
<p>Learning Latent Landmarks</p>
<p>Planning on a graph can be expensive, as the number of edges can grow quadratically with the number of nodes. To battle this issue in scalability, we use the constrained latent space to learn a sparse set of landmarks. A landmark can be thought of as a waypoint that the agent can pass through enroute to achieve a desired goal. Ideally, goals that are easily reachable from one another should be grouped to form one single landmark. Since our latent representation captures the temporal reachability between goals, this can be achieved by doing clustering in the latent space. The cluster centroids, when decoded from the decoder, will be precisely the latent landmarks we are seeking.</p>
<p>Clustering proceeds as follows. For N clusters to be learned, we define a mixture of Gaussians in the latent space with N trainable latent centroids, {c 1 · · · c N }, and a shared trainable variance vector σ. We maximize the evidence lower bound (ELBO) with a uniform prior p(c):
log p z = f E (g) ≥ E q(c|z) log p(z | c) − D KL q(c | z) p(c)(5)
Ideally, we would like each batch of data given to the latent clustering model to be representative of the whole replay buffer, such that the centroids will quickly learn to scatter out. To this end, we propose to use the Greedy Latent Sparsification (GLS) algorithm (see the Appendix) on each batch of data sampled from the replay before taking a gradient step with the batch. GLS is inspired by kmeans++ (Arthur &amp; Vassilvitskii, 2007), with several key differences: this sparsification process is used for both training and initialization, it uses a neural metric for determining the distance between data points, and that it is compatible with mini-batch-style gradient-based training.</p>
<p>Planning with Latent Landmarks</p>
<p>Having derived a latent encoding algorithm and an algorithm for learning latent landmarks, we at last turn our attention to search and planning. L 3 P is agnostic to the graph search algorithm being used. In practice, we use a variant of the Floyd algorithm, where our relaxation operations use a soft max rather than hard max for better stability (see the Appendix for more details). To construct a weight matrix that provides raw distance estimates between latent landmarks in the first place, we begin by decoding the learned centroids in the latent space into the nodes in the graph {f D (c 1 ) · · · f D (c N )}. To build the graph, we add two edges directed in reverse orders for every pair of latent landmarks. For instance, for an edge going from f D (c i ) to f D (c j ), the weight on that edge is
w i,j = −V (f D (c i ), f D (c j )).
Notice that the distances are negated. At the start of an episode, the agent receives a goal g, and we construct matrix W :
W =      0 . . . w 1,N −V (f D (c 1 ), g) . . . . . . . . . . . . w N,1 . . . 0 −V (f D (c N ), g) −∞ . . . −∞ 0     (6)
Algorithm 1 Online Planning in L 3 P Given: Environment env, initial state s, goal g. a ∼ π(s, SubG); s ← env.step(a).</p>
<p>16: end for</p>
<p>For online planning, when the agent receives a goal at the start of an episode, we use graph search to solve for d c→g (which is fixed throughout an episode). For an observation state s, the algorithm calculates d s→c :
d s→c =      −D s, π(s, f D (c 1 )), f D (c 1 ) . . . −D s, π(s, f D (c N )), f D (c N ) −D s, π(s, g), g     (7)
The chosen landmark is subgoal ← arg max(d s→c + Figure 3. We consider two environments involving a fetch robot, a block, and a box. In Box-Distractor-PickAndPlace, the fetch must learn to pick and place the block while avoiding collision with the box. In Place-Inside-Box, the fetch must pick the block and place it inside the box. We visualize the fetch states corresponding to learned landmarks in the second row of images.</p>
<p>d c→g ). To further provide temporal abstraction and robustness, the agent will be asked to consistently pursue subgoal for K = −d s→c [subgoal] number of steps, which is how many steps it thinks it will need. The proposed goal does not change during this period. In this way, L 3 P makes sure that the agent does not re-plan at every step, and this mechanism for temporal abstraction is crucial to its robustness. This mechanism is detailed in Algorithm 1.</p>
<p>After this K many steps, the agent will decide on the next landmark to pursue by re-calculating d s→c , but the immediate previous landmark will not be considered as a candidate landmark. The reason is that, if the agent has failed to reach a self-proposed landmark within the reachability limit it has set for itself, then the agent should try something new for the immediate next goal rather than stick to the immediate previous landmark for another round. We have found that this simple algorithm helps the agent avoid getting stuck and improves the overall robustness of the agent.</p>
<p>In summary, we have derived an algorithm that learns a sparse set of latent landmarks scattered across goal space in terms of reachability, and uses those learned landmarks for robust planning.</p>
<p>Experiments and Evaluation</p>
<p>We investigate the impact of L 3 P in a variety of robotic manipulation and navigation environments. These include standard benchmarks such as Fetch-PickAndPlace, and more difficult environments such as AntMaze-Hard and Place-Inside-Box that have been engineered to require testtime generalization. Videos of our algorithm in action are available at: https://sites.google.com/view/ latent-landmarks/.</p>
<p>Baselines</p>
<p>We compare our method with a variety of baselines. HER (Andrychowicz et al., 2017) is a model-free RL algorithm. L 3 P demonstrates better sample efficiency, higher asymptotic performance, and in some cases, the ability to generalize to longer horizons. Figure 5. For both Point and Ant, during training, the initialization state distribution and the goal proposal distribution are uniform around the maze. During test time, the agent is asked to traverse the longest path in the maze, which is not seen during training. Importantly, the map of the environment is not given to the agent at any given point; the agent has to learn the structure of the environment purely through interaction. The success rate during test is reported in Figure ??. This environment demonstrates L 3 P 's ability to generalize to longer horizon goals during test time.</p>
<p>SORB (Eysenbach et al., 2019) is a method that combines RL and graph search by using the entire replay buffer. Mapping State Space (MSS Huang et al. 2019) reduces the number of vertices by sub-sampling the replay buffer. L 3 P , SORB, and MSS all use the same hindsight relabelling strategy proposed in HER. All of the domains are continuous control tasks, so we adopt DDPG (Lillicrap et al., 2015) as the learning algorithm for the low-level actor.</p>
<p>Generalization to Longer Horizons</p>
<p>The PointMaze-Hard and AntMaze-Hard environments introduced in Figure 6 are designed to test an agent's ability to generalize to longer horizons. While PointMaze and AntMaze have been previously used in (Duan et al., 2016;Huang et al., 2019;Pitis et al., 2020), we make slight changes to those environments in order to increase their difficulty. We use a short, 200-timestep time horizon dur- Figure 6. Visualizing the paths taken by SORB, MSS and L 3 P on AntMaze at test time. The blue dots in the backgrounds are the learned landmarks using L 3 P . The orange dot is the starting location of the Ant. The red dot is the final goal. The blue stars indicate the landmarks chosen by the planning algorithms. As illustrated in the figure above, L 3 P addresses two major failure modes of graph-based planning with RL. Firstly, graph-based methods tend to switch proposed subgoals too frequently and fall into a loop due to wormholes in distance estimates, whereas L 3 P leverages temporal abstraction in both landmark learning and online planning to avoid this pitfall. Secondly, when the agent pursues a subgoal unsuccessfully (due to obstacles, etc), other methods tend to get stuck by continuing proposing the same subgoal, whereas L 3 P can adapt to the encountered failure and propose different subgoals in the event of getting stuck.</p>
<p>ing training and a ρ 0 that is uniform in the maze. At test time, we always initialize the agent on one end of the maze, and set the goal on the other end. The horizon of the test environment is 500 steps. Crucially, no prior knowledge on the shape of the maze is given to the agent. We also set a much stricter threshold for determining whether an agent has reached the goal. In Figure ??, we see L 3 P is the only algorithm capable of solving AntMaze-Hard consistently.</p>
<p>We observe an interesting trend where the success rates for some of other graph search methods crash and then slowly recover after making some initial progress. We postulate this occurs because methods that are based on using the entire replay or sub-sampling the replay for landmark selection will struggle as the buffer size increases. For instance, in the AntMaze-Hard environment, MSS and SORB use 400 and tens of thousands of landmarks respectively, whereas L 3 P obtains a lean graph that only contain 50 learnable landmarks. The result suggests that learning latent landmarks is significantly more sample efficient and stable than either directly using or sub-sampling the replay buffer to build the graph. The online planning algorithm in L 3 P , which effectively leverages temporal abstraction to improve robustness, also contributes to the asymptotic success rate. As explained in Figure 6, L 3 P successfully addresses the common failure modes of graph-based RL methods. The result convincingly shows that, at least on the navigation tasks considered, L 3 P is most effective at taking advantage of the problem's inherent graph structure (without any prior knowledge of the map or environment configurations) and generalizing to longer horizons during test time.</p>
<p>Robotic Manipulation Tasks</p>
<p>We also benchmark challenging robotic manipulations tasks with a Fetch robot introduced in (Plappert et al., 2018;Andrychowicz et al., 2017). Besides the PickAndPlace task, we also evaluate our method on two additional Fetch tasks involving a box on a table, as illustrated in Figure 3. In Box-Distractor-PickAndPlace environment, the agent needs to perform the pick-and-place task with a box in the middle of the table serving as a distractor. The Place-Inside-Box environment aims to teach the agent to place an object with randomly initialized locations into the box and has a simple curriculum. During training, the goal distribution has 80% regular pick-and-place goals, enabling the agent to first learn how to fetch in general. Meanwhile, only 20% of the goals are inside the box, which is the harder part of the task. During testing, we evaluate the agent's ability to pick up the object from the table and place it inside the box. Our method achieves dominant performance in both learning speed and test-time generalization on those three robotic manipulation environments. We note that on those manipulation tasks considered, many prior planning methods hurt the performance of the model-free agent. L 3 P is the only method that is able to help the model-free agent learn faster and perform better on all three tasks.</p>
<p>Understanding Model Choices in L 3 P</p>
<p>We investigate L 3 P 's sensitivity to different design choices and hyper-parameters via a set of ablation studies. More specifically, we study how the following four factors affect the performance of L 3 P : the choice of graph search algorithms, and edge weight cutoff threshold in graph search (a key hyper-parameter in the graph search module); the choice of online planning algorithms, and the number of latent landmarks being learned (a key hyper-parameter in the planning module).</p>
<p>While L 3 P is agnostic to the graph search algorithm being used, we study the effect of two possible choices: Floyd algorithm and a soft version of Floyd (soft Floyd). As shown in Figure 7, the choice seems to have a relatively small effect on learning. During the early phase of experimentation, we find that having a soft operation for relaxation in Floyd leads to better overall training stability. A hard version of relaxation helps the learning curve take off faster but suffers from greater instability during policy improvement. The likely reason is that neural distance estimates are not entirely accurate, and in the presence of occasional bad edges, using softmax rather than hard max improves robustness. We therefore use soft relaxation in L 3 P .</p>
<p>In the graph search module, a very sensitive hyper-parameter is the edge weight cutoff threshold, denoted as d max. This clipping threshold is commonly used in prior works such as (Savinov et al., 2018a;Eysenbach et al., 2019;Huang et al., 2019;Emmons et al., 2020). It essentially means that if the weight of an edge is bigger than d max, then it is set to be infinity during the graph search process. The motivation for introducing this common hyper-parameter is two-fold. Firstly, we only trust distance estimates when they are local, because value iterations are inherently local. Secondly, we want the next sub-goal to be relatively nearby in terms of reachability. The d max value determines the maximum (perceived) distance from the current state to next proposed subgoal. As shown in Figure 7, our current approach is still quite sensitive to this hyper-parameter; changes to d max can have a considerable impact on learning. As this weakness is common to this class of approaches, we believe that further research is required to discover more principled ways of encouraging the search results to be local.</p>
<p>For online planning, the L 3 P planner introduced in Algorithm 1 is essential to the success of L 3 P . Our planning algorithm can take advantage of the temporal abstraction provided by the graph-structured world model. As previously shown in Figure 6, the design of L 3 P planner avoids many common pitfalls. It does not re-plan at every step, but instead uses the reachability estimates to dynamically decide when to re-plan, striking a balance between adaptability and consistency in planning. This planner is also more tolerant of errors: it removes the immediate previ-ous landmark when it re-plans, so that the agent will be less prone to getting stuck. In Figure 8, we compare the L 3 P planner to a naive planner, which simply re-calculates the shortest path at every step. The result shows that our planning algorithm is crucial to the success of L 3 P .</p>
<p>An important hyper-parameter in graph-based planning is the number of landmarks being used. Intuitively, since L 3 P is learning the nodes on the graph, it should be robust to the changes in the number of nodes (landmarks) being learned.</p>
<p>In Figure 8, we show that this is indeed the case: L 3 P is robust to the number of latent landmarks. In contrast to prior methods, L 3 P is able to learn the nodes (landmarks) used for graph search from the agent's own experience. We vary this hyper-parameter in the challenging AntMaze-Hard environment, and we find that L 3 P is robust against a variety of values. This is expected, because the landmarks in the latent space of L 3 P will try to be equally scattered across the goal space according to the learned reachability metric. As the number of landmarks decreases, the learning procedure will automatically push the landmarks to be further away from one another.</p>
<p>Closing Remarks</p>
<p>In this work, we introduce a way of learning graphstructured world models that endow agents with the ability to do temporally extended reasoning. The algorithm, L 3 P , learns a set of latent landmarks scattered across the goal space to enable scalable planning. We demonstrate that L 3 P achieves significantly better sample efficiency, higher asymptotic performance, and generalization to longer horizons on a range of challenging robotic navigation and manipulation tasks. Here we briefly discuss two promising future directions. First, how can an agent quickly generate a set of plausible landmarks in a previously unseen environment? A lot of progress has been made on the topics of meta reinforcement learning and learning to explore; can L 3 P be combined with meta learning techniques for fast landmarks generation? Second, can we learn graph-structured world models from offline datasets? Batch RL is a more realistic setting for many RL applications, since online interaction can be expensive in the real world. Applying L 3 P to offline datasets might require a notion of uncertainty in different parts of the graph.</p>
<p>Figure 2 .
2An overview of L 3 P , which learns a small number of latent landmarks for planning. The main components of our method are: learning reachability estimates (via Q-learning and regression), learning a latent space (via an auto-encoder with reachability constraints), learning latent landmarks (via clustering in the latent space), graph search on the world model and online planning.</p>
<p>Figure 4 .
4Test time success rate vs. total number of timesteps, on a variety of challenging robotic navigation and manipulation environments.</p>
<p>Figure 7 .
7Ablation studies on the graph search module, including the choice of graph search algorithms and a key hyper-parameter in graph search: the edge weight cutoff threshold.</p>
<p>Figure 8 .
8Ablation studies on the online planning module, including the choice of planners and a key hyper-parameter in graphbased planning: the number of nodes (landmarks).</p>
<p>1 :
1Cnt = 0. SubG = None. 2: Solve for d c→g with graph search using W .3: for t = 1 to T do 
One episode </p>
<p>4: </p>
<p>if Cnt ≥ 1.0 then </p>
<p>5: </p>
<p>Cnt = Cnt − 1 </p>
<p>6: </p>
<p>else 
We do not re-plan at every step </p>
<p>7: </p>
<p>Calculate d s→c . </p>
<p>8: </p>
<p>d ← d s→c + d c→g </p>
<p>9: </p>
<p>Remove the immediate previous landmark </p>
<p>10: </p>
<p>if SubG = None then </p>
<p>11: </p>
<p>d[SubG] ← −∞ </p>
<p>12: </p>
<p>end if </p>
<p>13: </p>
<p>SubG, Cnt ← arg max d, − max d </p>
<p>14: </p>
<p>end if </p>
<p>15: </p>
<p>AcknowledgementsWe thank the anonymous reviewers for providing helpful comments on the paper. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence (www.vectorinstitute.ai/partners).Appendix A: Greedy Latent SparsificationThe Greedy Latent Sparsification (GLS) algorithm subsamples a large batch by sparsification. GLS first randomly selects a latent embedding from the batch, and then greedily chooses the next embedding that is furthest away from already selected embeddings. After collecting some warmup trajectories before planning starts (seeTable 1below) during training, we first use GLS to initialize the latent centroids, and then continue to use it to sample the batches used to train the latent clusters. GLS is strongly inspired by(Arthur &amp; Vassilvitskii, 2007), and this type of approach is known to improve clustering.Appendix B: Graph Search with Soft RelaxationsIn this paper, we employ a soft version of Floyd algorithm, which we find to empirically work well. Rather than simply using the min operation to do relaxation, the soft value iteration procedure uses a sof t min operation when doing an update (note that, since we negated the distances to be negative in the weight matrix of the graph, the operations we use are actually max and softmax). The reason is that neural distances can be inconsistent and inaccurate at times, and using a soft operation makes the whole procedure more robust. More concretely, we repeat the following update on the weight matrix for S steps with temperature β:Following the practice in(Eysenbach et al., 2019;Huang et al., 2019), we do the following initialization to the distance matrix: for entries smaller than the negative of d max, we penalize the entry by adding −∞ to it (in this paper, we use −10 6 as the −∞ value). The essential idea is that we only trust a neural estimate when it is local, and we rely on graph search to solve for global, longer-horizon distances. The −∞ penalty effectively masks out those entries with large negative values in the softmax operation above. If we replace softmax with a hard max, we recover the original update in Floyd algorithm; we can interpolate between a hard Floyd and a soft Floyd by tuning the temperature β.Appendix C: Overall Training ProcedureHere we provide an overall training procedure for L 3 P in Algorithm 3. Given an environment env and a training goal distribution p(g), we initialize a replay buffer B and the following trainble modules: policy π, distance function D, value function V , encoder f E and decoder f D , latent centroids {c 1 · · · c N }.Every K env episodes of sampling, we take gradient steps for the above modules. The ratio between the number of environment steps and the number of gradient steps is a hyper-parameter.Appendix D: Implementation Details• We find that having a centralized replay for all parallel workers is significantly more sample efficient than having separate replays for each worker and simply averaging the gradients across workers.• For Ant-Maze environment, we do grad norm clipping by a value of 15.0 for all networks. For Fetch tasks, we normalize the inputs by running means and standard deviations per input dimensions.• Since L 3 P is able to decompose a long-horizon goal into many short-horizon goals, we shorten the range of future steps where we do hindsight relabelling; as a result, the agent can focus its optimization effort on more immediate goals. This corresponds to the hyperparameter: hindsight relabelling range.World Model as a Graph: Learning Latent Landmarks for Planning• During training, we collect 50% of the data without the planning module, and the other 50% of the data with planning. This corresponds to the hyper-parameter: probability of using search during train.• At train time, to encourage exploration during planning, we temporarily add a small number of random landmarks from GLS (Algorithm 2) to the existing latent landmarks. A new set of random landmarks is selected for each episode before graph search starts (Algorithm 1). This corresponds to the hyper-parameter: random landmarks added during train.• We find that collecting a certain number of warm-up trajectories for every worker before the planning procedure starts (during training) and before GLS (Algorithm 2) is used for initialization to help improve the planning results. This corresponds to the hyperparameter: number of warm-up trajectories.Appendix E: Hyper-parametersThe first table below lists the common hyper-parameters across all environments. The second table below lists the hyper-parameters that differ across the environments.
Hindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O P Abbeel, W Zaremba, Advances in neural information processing systems. Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, O. P., and Zaremba, W. Hindsight experience replay. In Advances in neural information processing systems, pp. 5048-5058, 2017.</p>
<p>The advantages of careful seeding. D Arthur, S Vassilvitskii, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. the eighteenth annual ACM-SIAM symposium on Discrete algorithmsArthur, D. and Vassilvitskii, S. k-means++: The advantages of careful seeding. Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 2007.</p>
<p>Efficient modelbased deep reinforcement learning with variational state tabulation. D Corneil, W Gerstner, Brea , J , arXiv:1802.04325arXiv preprintCorneil, D., Gerstner, W., and Brea, J. Efficient model- based deep reinforcement learning with variational state tabulation. arXiv preprint arXiv:1802.04325, 2018.</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. R Coulom, International conference on computers and games. SpringerCoulom, R. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pp. 72-83. Springer, 2006.</p>
<p>Feudal reinforcement learning. P Dayan, G E Hinton, Advances in neural information processing systems. Dayan, P. and Hinton, G. E. Feudal reinforcement learning. In Advances in neural information processing systems, pp. 271-278, 1993.</p>
<p>A note on two problems in connexion with graphs. E W Dijkstra, Numerische mathematik. 11Dijkstra, E. W. et al. A note on two problems in connex- ion with graphs. Numerische mathematik, 1(1):269-271, 1959.</p>
<p>Experiments with the graph traverser program. J E Doran, D Michie, Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences. 294Doran, J. E. and Michie, D. Experiments with the graph traverser program. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 294(1437):235-259, 1966.</p>
<p>Benchmarking deep reinforcement learning for continuous control. Y Duan, X Chen, R Houthooft, J Schulman, Abbeel , P , International Conference on Machine Learning. Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pp. 1329-1338, 2016.</p>
<p>Simultaneous localization and mapping: part i. H Durrant-Whyte, T Bailey, IEEE robotics &amp; automation magazine. 132Durrant-Whyte, H. and Bailey, T. Simultaneous localiza- tion and mapping: part i. IEEE robotics &amp; automation magazine, 13(2):99-110, 2006.</p>
<p>S Emmons, A Jain, M Laskin, T Kurutach, P Abbeel, D Pathak, arXiv:2003.06417Sparse graphical memory for robust planning. arXiv preprintEmmons, S., Jain, A., Laskin, M., Kurutach, T., Abbeel, P., and Pathak, D. Sparse graphical memory for robust planning. arXiv preprint arXiv:2003.06417, 2020.</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. Eysenbach, B., Salakhutdinov, R. R., and Levine, S. Search on the replay buffer: Bridging planning and reinforce- ment learning. In Advances in Neural Information Pro- cessing Systems, pp. 15246-15257, 2019.</p>
<p>Treeqn and atreec: Differentiable tree-structured models for deep reinforcement learning. G Farquhar, T Rocktäschel, M Igl, S Whiteson, Farquhar, G., Rocktäschel, T., Igl, M., and Whiteson, S. Treeqn and atreec: Differentiable tree-structured models for deep reinforcement learning. October 2017. URL http://arxiv.org/abs/1710.11417.</p>
<p>Navigation and acquisition of spatial knowledge in a virtual maze. S Gillner, H A Mallot, Journal of cognitive neuroscience. 104Gillner, S. and Mallot, H. A. Navigation and acquisition of spatial knowledge in a virtual maze. Journal of cognitive neuroscience, 10(4):445-463, 1998.</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in Neural Information Processing Systems. Ha, D. and Schmidhuber, J. Recurrent world models facili- tate policy evolution. In Advances in Neural Information Processing Systems, pp. 2450-2462, 2018.</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, Davidson , J , International Conference on Machine Learning. PMLRHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555-2565. PMLR, 2019.</p>
<p>D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv:2010.02193Mastering atari with discrete world models. arXiv preprintHafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mas- tering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.</p>
<p>A formal basis for the heuristic determination of minimum cost paths. P E Hart, N J Nilsson, Raphael , B , IEEE transactions on Systems Science and Cybernetics. 42Hart, P. E., Nilsson, N. J., and Raphael, B. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100-107, 1968.</p>
<p>Mapping state space using landmarks for universal goal reaching. Z Huang, F Liu, H Su, Advances in Neural Information Processing Systems. Huang, Z., Liu, F., and Su, H. Mapping state space using landmarks for universal goal reaching. In Advances in Neural Information Processing Systems, pp. 1942-1952, 2019.</p>
<p>When to trust your model: Model-based policy optimization. M Janner, J Fu, M Zhang, S Levine, Advances in Neural Information Processing Systems. Janner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, pp. 12519-12530, 2019.</p>
<p>Subgoal trees-a framework for goal-based reinforcement learning. T Jurgenson, O Avner, E Groshev, Tamar , A , arXiv:2002.12361arXiv preprintJurgenson, T., Avner, O., Groshev, E., and Tamar, A. Sub- goal trees-a framework for goal-based reinforcement learning. arXiv preprint arXiv:2002.12361, 2020.</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.6114arXiv preprintKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Model-ensemble trust-region policy optimization. T Kurutach, I Clavera, Y Duan, A Tamar, Abbeel , P , arXiv:1802.10592arXiv preprintKurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning. Langley, P.the 17th International Conference on Machine LearningStanford, CAMorgan KaufmannLangley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207-1216, Stan- ford, CA, 2000. Morgan Kaufmann.</p>
<p>Rapidly-exploring random trees: A new tool for path planning. S M Lavalle, LaValle, S. M. Rapidly-exploring random trees: A new tool for path planning. 1998.</p>
<p>Gated path planning networks. L Lee, E Parisotto, D S Chaplot, E Xing, R Salakhutdinov, Lee, L., Parisotto, E., Chaplot, D. S., Xing, E., and Salakhut- dinov, R. Gated path planning networks. June 2018. URL http://arxiv.org/abs/1806.06408.</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Hallucinative topological memory for zero-shot visual planning. K Liu, T Kurutach, C K Tung, .-C Abbeel, P , Tamar , A , ArXiv. Liu, K., Kurutach, T., Tung, C. K.-C., Abbeel, P., and Tamar, A. Hallucinative topological memory for zero-shot visual planning. ArXiv, 2019. URL https://arxiv.org/ abs/2002.12336.</p>
<p>Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. Y Luo, H Xu, Y Li, Y Tian, T Darrell, T Ma, arXiv:1807.03858arXiv preprintLuo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. Algorithmic framework for model-based deep reinforce- ment learning with theoretical guarantees. arXiv preprint arXiv:1807.03858, 2018.</p>
<p>Dataefficient hierarchical reinforcement learning. O Nachum, S S Gu, H Lee, S Levine, Advances in Neural Information Processing Systems. Nachum, O., Gu, S. S., Lee, H., and Levine, S. Data- efficient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pp. 3303- 3313, 2018.</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. A Nagabandi, G Kahn, R S Fearing, S Levine, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEENagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. Neural network dynamics for model-based deep reinforce- ment learning with model-free fine-tuning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 7559-7566. IEEE, 2018.</p>
<p>Visual reinforcement learning with imagined goals. A V Nair, V Pong, M Dalal, S Bahl, S Lin, S Levine, Advances in Neural Information Processing Systems. Nair, A. V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S. Visual reinforcement learning with imagined goals. In Advances in Neural Information Processing Systems, pp. 9191-9200, 2018.</p>
<p>Planning with goal-conditioned policies. S Nasiriany, V Pong, S Lin, S Levine, Advances in Neural Information Processing Systems. Nasiriany, S., Pong, V., Lin, S., and Levine, S. Planning with goal-conditioned policies. In Advances in Neural Information Processing Systems, pp. 14843-14854, 2019.</p>
<p>Long-horizon visual planning with goal-conditioned hierarchical predictors. K Pertsch, O Rybkin, F Ebert, C Finn, D Jayaraman, S Levine, arXiv:2006.13205arXiv preprintPertsch, K., Rybkin, O., Ebert, F., Finn, C., Jayaraman, D., and Levine, S. Long-horizon visual planning with goal-conditioned hierarchical predictors. arXiv preprint arXiv:2006.13205, 2020.</p>
<p>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. S Pitis, H Chan, S Zhao, B Stadie, J Ba, arXiv:2007.02832arXiv preprintPitis, S., Chan, H., Zhao, S., Stadie, B., and Ba, J. Maximum entropy gain exploration for long horizon multi-goal re- inforcement learning. arXiv preprint arXiv:2007.02832, 2020.</p>
<p>M Plappert, M Andrychowicz, A Ray, B Mcgrew, B Baker, G Powell, J Schneider, J Tobin, M Chociej, P Welinder, arXiv:1802.09464Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprintPlappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej, M., Welinder, P., et al. Multi-goal reinforcement learn- ing: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.</p>
<p>Temporal difference models: Model-free deep rl for model-based control. V Pong, S Gu, M Dalal, S Levine, arXiv:1802.09081arXiv preprintPong, V., Gu, S., Dalal, M., and Levine, S. Temporal differ- ence models: Model-free deep rl for model-based control. arXiv preprint arXiv:1802.09081, 2018.</p>
<p>Skew-fit: State-covering self-supervised reinforcement learning. V H Pong, M Dalal, S Lin, A Nair, S Bahl, S Levine, arXiv:1903.03698arXiv preprintPong, V. H., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S. Skew-fit: State-covering self-supervised re- inforcement learning. arXiv preprint arXiv:1903.03698, 2019.</p>
<p>Imagination-augmented agents for deep reinforcement learning. S Racanière, T Weber, D Reichert, L Buesing, A Guez, D Jimenez Rezende, A Puigdomènech Badia, O Vinyals, N Heess, Y Li, R Pascanu, P Battaglia, D Hassabis, D Silver, D Wierstra, Neural Information Processing Systems. Racanière, S., Weber, T., Reichert, D., Buesing, L., Guez, A., Jimenez Rezende, D., Puigdomènech Badia, A., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia, P., Hassabis, D., Silver, D., and Wierstra, D. Imagination-augmented agents for deep reinforcement learning. Neural Informa- tion Processing Systems, 2017.</p>
<p>N Savinov, A Dosovitskiy, V Koltun, arXiv:1803.00653Semiparametric topological memory for navigation. arXiv preprintSavinov, N., Dosovitskiy, A., and Koltun, V. Semi- parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018a.</p>
<p>N Savinov, A Raichuk, R Marinier, D Vincent, M Pollefeys, T Lillicrap, S Gelly, arXiv:1810.02274Episodic curiosity through reachability. arXiv preprintSavinov, N., Raichuk, A., Marinier, R., Vincent, D., Polle- feys, M., Lillicrap, T., and Gelly, S. Episodic curiosity through reachability. arXiv preprint arXiv:1810.02274, 2018b.</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 5297587Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016a.</p>
<p>The predictron: Endto-end learning and planning. D Silver, H Van Hasselt, M Hessel, T Schaul, A Guez, T Harley, G Dulac-Arnold, D Reichert, N Rabinowitz, A Barreto, T Degris, Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert, D., Rabinowitz, N., Barreto, A., and Degris, T. The predictron: End- to-end learning and planning. December 2016b. URL http://arxiv.org/abs/1612.08810.</p>
<p>Universal planning networks. A Srinivas, A Jabri, P Abbeel, S Levine, C Finn, Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. Universal planning networks. April 2018. URL http://arxiv.org/abs/1804.00645.</p>
<p>an integrated architecture for learning, planning, and reacting. R S Sutton, Dyna, ACM Sigart Bulletin. 24Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160- 163, 1991.</p>
<p>Value iteration networks. A Tamar, Y Wu, G Thomas, S Levine, Abbeel , P , Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. Value iteration networks. February 2016. URL http: //arxiv.org/abs/1602.02867.</p>
<p>Feudal networks for hierarchical reinforcement learning. A S Vezhnevets, S Osindero, T Schaul, N Heess, M Jaderberg, D Silver, K Kavukcuoglu, arXiv:1703.01161arXiv preprintVezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161, 2017.</p>
<p>Human spatial representation: Insights from animals. R F Wang, E S Spelke, Trends in cognitive sciences. 69Wang, R. F. and Spelke, E. S. Human spatial representation: Insights from animals. Trends in cognitive sciences, 6(9): 376-382, 2002.</p>
<p>Exploring model-based planning with policy networks. T Wang, J Ba, arXiv:1906.08649arXiv preprintWang, T. and Ba, J. Exploring model-based planning with policy networks. arXiv preprint arXiv:1906.08649, 2019.</p>
<p>Robot navigation by waypoints. Y Wang, D Mulvaney, I Sillitoe, E Swere, Journal of Intelligent and Robotic Systems. 522Wang, Y., Mulvaney, D., Sillitoe, I., and Swere, E. Robot navigation by waypoints. Journal of Intelligent and Robotic Systems, 52(2):175-207, 2008.</p>
<p>Unsupervised representation learning by latent plans. G Yang, A Zhang, A S Morcos, J Pineau, P Abbeel, Calandra , R Plan2vec, arXiv:2005.03648Proceedings of The 2nd Annual Conference on Learning for Dynamics and Control. The 2nd Annual Conference on Learning for Dynamics and Control120Yang, G., Zhang, A., Morcos, A. S., Pineau, J., Abbeel, P., and Calandra, R. Plan2vec: Unsupervised representa- tion learning by latent plans. In Proceedings of The 2nd Annual Conference on Learning for Dynamics and Con- trol, volume 120 of Proceedings of Machine Learning Research, pp. 1-12, 2020. arXiv:2005.03648.</p>
<p>Maximum entropyregularized multi-goal reinforcement learning. R Zhao, X Sun, V Tresp, arXiv:1905.08786arXiv preprintZhao, R., Sun, X., and Tresp, V. Maximum entropy- regularized multi-goal reinforcement learning. arXiv preprint arXiv:1905.08786, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
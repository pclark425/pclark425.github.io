<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4629 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4629</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4629</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f86fb205cd4d85478e65304fc38bdf1e4bed2440" target="_blank">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain, demonstrating that appropriate pre-trained representations can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4629.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4629.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (fine-tuned, pre-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL (pre-trained then fine-tuned on synthetic addition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 48-layer, ~1.5B-parameter decoder-only Transformer (GPT-2-XL) that was pre-trained on web-scale data and then fine-tuned on a synthetic two-number addition dataset (numbers ≤ 260). The paper analyzes this model's internal representations and mechanisms for computing addition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer, 48 layers, ≈1.5B parameters; pre-trained on large text corpora then fine-tuned on a synthetic addition dataset (pairs of integers ≤ 260, 5 templates). Uses standard residual stream, attention and MLP blocks, and an output unembedding W^U over the number tokens (p≈521).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-step integer addition (adding two integers ≤ 260, expressed in natural-language prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Uses Fourier-feature-based distributed representations in the residual stream: low-frequency Fourier components (large period) encode coarse magnitude/approximation, primarily produced by MLP outputs, while high-frequency Fourier components (small period, e.g., period 2, 5, 10) encode modular/classification information (unit digit and other moduli), primarily produced by attention outputs; the final logits are a superposition of a sparse set of Fourier components.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layer-wise Logit Lens shows progressive approximation of the correct sum; DFT of intermediate logits (W^U·MLP and W^U·Attn) shows sparse outlier Fourier components concentrated at specific periods (2, 2.5, 5, 10); inverse-DFT of top components reconstructs a peaked distribution at the correct sum; targeted linear-projection ablations (low-pass and high-pass filters defined via projection to nullspace of selected Fourier components) causally degrade performance in ways predicted by the mechanism (Table 1 and analyses in §3.3). Token-embedding DFT shows matching Fourier components present in pretrained embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counterevidence presented for this model; however, some large-period (very low-frequency) components are imperfect at precisely locating the peak, requiring high-frequency components for exact classification (paper discusses limits of low-frequency resolution).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned accuracy on held-out test set: 99.74% exact-match; intermediate predictions show proximity to correct answer early (within ±2 or ±10) before final convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Logit Lens probing to extract intermediate predictions; discrete Fourier transform of intermediate logits and token embeddings; selective ablations via low-pass and high-pass filters applied to MLP and attention outputs (projection that zeroes selected DFT components) produce predictable failure modes: removing MLP low-frequency components causes large magnitude errors (off-by-10,50,100); removing attention high-frequency components causes unit-digit errors (small errors <6); removing high-freq from both degrades accuracy substantially (see Table 1: baseline acc 0.9974; ATTN high-freq removed -> acc 0.7836; MLP low-freq removed -> acc 0.3589; ATTN+MLP high-freq removed -> acc 0.2708; ATTN+MLP low-freq removed -> acc 0.0594).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Relies on a combination of approximation (low-frequency) and modular classification (high-frequency); if high-frequency components are ablated or absent, the model makes unit-digit errors; if low-frequency components are ablated, the model errs by tens/hundreds (magnitude errors). Low-frequency components alone lack resolution to place a precise peak, hence require high-frequency modular signals. Performance depends on pretrained token embeddings that contain useful Fourier features; without those, mechanism degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared experimentally to models trained from scratch (same architecture) and to smaller models with/without pretrained embeddings; shows that pretrained GPT-2-XL learns sparse Fourier features used in both MLP and attention, enabling near-perfect accuracy, in contrast to models trained from scratch which lack such features and perform worse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4629.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4629.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-XL (trained-from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-XL trained from random initialization on addition task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same GPT-2-XL architecture but randomly initialized and trained only on the synthetic addition dataset (no pretraining); analyzed to contrast the effect of pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-XL (random init)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer, 48 layers, ≈1.5B parameters, trained from scratch only on the addition synthetic data until convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-step integer addition (same dataset as fine-tuned experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Learns primarily low-frequency/approximation representations (coarse magnitude) without developing the sparse high-frequency Fourier components needed for modular classification; thus relies on approximation and cannot reliably compute modular/unit-digit information.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Fourier analysis of intermediate logits and token embeddings shows no outlier high-frequency components (Figures 6 and 7a); behavior shows frequent off-by-one errors (unit-digit mistakes) and other small errors consistent with missing modular classification signals; final accuracy lower than pre-trained model.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None presented; results are consistent across analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Final test accuracy after convergence: 94.44% (vs. 99.74% for pre-trained then fine-tuned GPT-2-XL).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>DFT of logits and token embeddings shows absence of sharp Fourier peaks at periods associated with modular tasks; behavioral error analysis (Figure 22 and related text) shows predominance of small errors (off-by-one) consistent with missing high-frequency features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Lacks high-frequency Fourier features and so makes frequent unit-digit errors and cannot reliably perform modular classification; poorer generalization/precision compared to pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Direct contrast to pre-trained GPT-2-XL: pretraining introduces Fourier structure in token embeddings and intermediate logits that the randomly initialized model does not develop when trained only on the addition dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4629.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4629.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-small (scratch vs. pretrained embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-small (124M params) trained from scratch, with and without frozen pre-trained token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small GPT-2 model (12 layers, 124M parameters) trained on the addition dataset either from scratch (random embeddings) or with frozen pretrained token embeddings; used to test causality of pretrained embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-small (experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer, 12 layers, ≈124M parameters; two training regimes reported: (a) full random init trained on addition; (b) randomly initialize most weights but freeze and use pre-trained token embeddings (from a pre-trained model) while training remaining weights on addition.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-step integer addition (same synthetic dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Pretrained token embeddings contain Fourier features (periods 2,2.5,5,10) that provide an inductive bias enabling the model to learn the MLP/attention decomposition and achieve perfect arithmetic performance; without these embeddings, the small model struggles to develop the necessary Fourier features.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training runs (5 seeds) show that GPT-2-small with frozen pre-trained token embeddings achieves 100% test accuracy and faster convergence, while GPT-2-small trained fully from scratch achieves ~53.95% test accuracy (Figures 7b and text). Fourier analysis of pretrained embeddings demonstrates outlier components at relevant periods (Figure 5a).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None shown; results consistent across seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-2-small trained from scratch: test accuracy ≈ 53.95%; GPT-2-small with frozen pre-trained token embeddings: test accuracy = 100% across 5 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Intervention: freezing/reusing pretrained token embeddings while training other weights causes a dramatic performance improvement and faster convergence; DFT of token embeddings shows presence of Fourier components hypothesized to be leveraged.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Small models without pretrained embeddings may fail to learn high-frequency modular signals and thus cannot reliably perform exact addition; reliance on pre-trained embeddings suggests sensitivity to representation priors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Shows that even small Transformers can attain perfect addition accuracy if given pretrained embeddings containing Fourier features; supports hypothesis that pretraining produces useful inductive biases absent in scratch-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4629.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4629.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-source prompted LMs (GPT-J, Phi-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (6B) and Phi-2 (2.7B) evaluated under in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large open-source models evaluated with few-shot in-context examples on the addition dataset; internal representation analysis (for Phi-2 and GPT-J) and behavioral error patterns analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (6B) and Phi-2 (2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer models of different sizes: GPT-J (~6B parameters) and Phi-2 (~2.7B). Evaluated in 4-shot in-context learning setting on the addition dataset; internal logits analyzed (for Phi-2 and GPT-J) across last 15 layers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-step integer addition via in-context learning (4-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Use Fourier-feature-like representations during in-context arithmetic: MLP and attention outputs show sparse Fourier components at similar periods (2, 2.5, 5, 10) as observed in fine-tuned GPT-2-XL, suggesting the same approximation + modular classification decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DFT of intermediate logits across last 15 layers shows outlier Fourier components with periods ≈2, 2.5, 5, 10 for Phi-2 and GPT-J (Figure 8 and Figure 18); behavioral error distributions: absolute errors are predominantly multiples of 10 (93% for GPT-J, 73% for Phi-2), consistent with absence or weakness of modular resolution beyond tens when high-frequency components are lacking or misaligned.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None presented; internal analyses support mechanism for these models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Behavioral: many absolute errors are multiples of 10 (GPT-J: 93% of absolute errors are multiples of 10; Phi-2: 73%). Exact accuracy numbers not stated for these in-context experiments, but models cannot perform addition without in-context conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>DFT probing of MLP and attention logits across last layers; behavioral error analysis (multiples-of-10 pattern).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Without appropriate alignment of Fourier components (or without sufficient high-frequency components), errors concentrate on multiples of 10 (i.e., incorrect unit digits), indicating difficulty in modular classification resolution; models require in-context examples to perform task (no few-shot-free performance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Behavior and internal DFT patterns are qualitatively similar to fine-tuned GPT-2-XL, suggesting a shared reliance on Fourier-like features across architectures and sizes when solving addition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4629.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4629.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed-source LLMs (GPT-3.5 / GPT-4 / PaLM-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-source instruction-tuned models GPT-3.5, GPT-4, and PaLM-2 (behavioral analysis only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large commercial models evaluated behaviorally (0-shot) on the addition dataset; internal representations not accessible, but error patterns were analyzed to infer likely mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, PaLM-2 (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language models (closed-source); evaluated on the addition dataset with 0-shot prompting; internal activations not available for DFT probing.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-step integer addition (0-shot behavioral evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Behavioral error distributions (predominance of multiples-of-10 errors) are consistent with a mechanism that uses Fourier-like modular signals (unit/ten-digit classification) combined with magnitude approximation, as in open-source models, though internal evidence is unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral error analysis: absolute errors are multiples of 10 for GPT-3.5 and GPT-4 (100% of absolute errors observed), and 87% for PaLM-2, suggesting a failure mode aligned with missing/incorrect unit-digit resolution consistent with Fourier-feature-based mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal probing possible; inference is based solely on behavioral statistics, so causality is not proven.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Behavioral error distribution: GPT-3.5 and GPT-4: 100% of observed absolute errors were multiples of 10; PaLM-2: 87% of observed absolute errors were multiples of 10. Exact overall accuracies are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>None (no internal access); only error-pattern analysis across outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Behavioral patterns suggest frequent unit-digit / small modular errors (multiples-of-10), but internal mechanism cannot be confirmed without activation-level access.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Behaviorally similar error signature to open-source and fine-tuned models, supporting the hypothesis that many LLMs rely on similar Fourier/modular decomposition for addition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4629.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4629.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier features (mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier-feature representation and computation (periodic basis in logits/embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A distributed representation scheme where numbers (token embeddings and intermediate logits) are represented in a Fourier basis; a sparse set of Fourier components (specific frequencies/periods) are used to encode both coarse magnitude (low-frequency) and modular residues (high-frequency), enabling addition via superposition and phase shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mechanism (general across pre-trained LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Representation: token embeddings and intermediate module outputs (W^U·MLP, W^U·Attn) exhibit sparsity in the discrete Fourier basis over the number token space (p≈521). Mechanistic decomposition: MLPs mainly contribute low-frequency components approximating magnitude; attention primarily contributes high-frequency components computing modular residues (mod 2,5,10,...); final logits are linear superpositions of a small set of Fourier basis functions whose phases and amplitudes align to locate the correct integer.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applies to single-step integer addition and modular classification subproblems (observed for addition; authors also mention extension to multiplication and other formats in appendices)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Pretraining induces number token embeddings containing prominent Fourier components; during (fine-tuning or in-context) computation, the Transformer combines low-frequency amplitude-encoding from MLPs with high-frequency modular signals from attention to compute exact sums via linear superposition and phase adjustments in Fourier space.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct analyses in the paper: (1) DFT of intermediate logits reveals sparse large-magnitude components at periods 2, 2.5, 5, 10 (Figures 2,3); (2) reconstructing logits with top Fourier components yields a peaked distribution at the correct answer (Figure 4); (3) causal ablations via projection-based low/high-pass filters on module outputs produce failure modes consistent with the hypothesized roles of low/high frequencies (Table 1); (4) token embeddings of pre-trained models contain matching Fourier peaks (Figure 5), and transferring pretrained embeddings to scratch-trained models rescues performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Models trained from scratch can still reach non-trivial accuracy using primarily low-frequency approximation (e.g., GPT-2-XL scratch achieves 94.44%), indicating alternative (less precise) solutions exist; extremely low-frequency components (very large periods) lack fine localization and therefore cannot alone resolve exact tokens without high-frequency support.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mechanism enables near-perfect exact-match performance when present (e.g., pre-trained GPT-2-XL fine-tuned: 99.74%); absence leads to reduced accuracy and characteristic error patterns (off-by-1 and multiples-of-10).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>DFT-based probing, identification of outlier components, inverse-DFT reconstructions, and linear-projection ablations (low-pass/high-pass via nullspace projection of B F W^U) that remove designated Fourier components from module outputs; single-component retention experiments (Definition B.1) also used to test causal role of individual Fourier frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Requires pre-existing Fourier structure in embeddings or else training may not converge to a precise modular+approximation decomposition; low-frequency components by themselves provide coarse magnitude but lack precision, and high-frequency components are essential for unit-digit classification yet can be sparse and brittle; misalignment of Fourier phases yields modular errors (e.g., multiples-of-10 mistakes).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Observed across multiple pre-trained and prompted models (GPT-2-XL fine-tuned, GPT-J, Phi-2) and hypothesized to underlie behavioral error patterns in closed-source models (GPT-3.5, GPT-4, PaLM-2); absent in models trained from scratch unless pretrained embeddings are introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pre-trained Large Language Models Use Fourier Features to Compute Addition', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Harmonics of learning: Universal fourier features emerge in invariant networks <em>(Rating: 2)</em></li>
                <li>Fourier features let networks learn high frequency functions in low dimensional domains <em>(Rating: 2)</em></li>
                <li>Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic <em>(Rating: 2)</em></li>
                <li>Feature emergence via margin maximization: case studies in algebraic tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4629",
    "paper_id": "paper-f86fb205cd4d85478e65304fc38bdf1e4bed2440",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "GPT-2-XL (fine-tuned, pre-trained)",
            "name_full": "GPT-2-XL (pre-trained then fine-tuned on synthetic addition)",
            "brief_description": "A 48-layer, ~1.5B-parameter decoder-only Transformer (GPT-2-XL) that was pre-trained on web-scale data and then fine-tuned on a synthetic two-number addition dataset (numbers ≤ 260). The paper analyzes this model's internal representations and mechanisms for computing addition.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL (fine-tuned)",
            "model_description": "Decoder-only Transformer, 48 layers, ≈1.5B parameters; pre-trained on large text corpora then fine-tuned on a synthetic addition dataset (pairs of integers ≤ 260, 5 templates). Uses standard residual stream, attention and MLP blocks, and an output unembedding W^U over the number tokens (p≈521).",
            "arithmetic_task_type": "Single-step integer addition (adding two integers ≤ 260, expressed in natural-language prompts)",
            "mechanism_hypothesis": "Uses Fourier-feature-based distributed representations in the residual stream: low-frequency Fourier components (large period) encode coarse magnitude/approximation, primarily produced by MLP outputs, while high-frequency Fourier components (small period, e.g., period 2, 5, 10) encode modular/classification information (unit digit and other moduli), primarily produced by attention outputs; the final logits are a superposition of a sparse set of Fourier components.",
            "evidence_for_mechanism": "Layer-wise Logit Lens shows progressive approximation of the correct sum; DFT of intermediate logits (W^U·MLP and W^U·Attn) shows sparse outlier Fourier components concentrated at specific periods (2, 2.5, 5, 10); inverse-DFT of top components reconstructs a peaked distribution at the correct sum; targeted linear-projection ablations (low-pass and high-pass filters defined via projection to nullspace of selected Fourier components) causally degrade performance in ways predicted by the mechanism (Table 1 and analyses in §3.3). Token-embedding DFT shows matching Fourier components present in pretrained embeddings.",
            "evidence_against_mechanism": "No direct counterevidence presented for this model; however, some large-period (very low-frequency) components are imperfect at precisely locating the peak, requiring high-frequency components for exact classification (paper discusses limits of low-frequency resolution).",
            "performance_metrics": "Fine-tuned accuracy on held-out test set: 99.74% exact-match; intermediate predictions show proximity to correct answer early (within ±2 or ±10) before final convergence.",
            "probing_or_intervention_results": "Logit Lens probing to extract intermediate predictions; discrete Fourier transform of intermediate logits and token embeddings; selective ablations via low-pass and high-pass filters applied to MLP and attention outputs (projection that zeroes selected DFT components) produce predictable failure modes: removing MLP low-frequency components causes large magnitude errors (off-by-10,50,100); removing attention high-frequency components causes unit-digit errors (small errors &lt;6); removing high-freq from both degrades accuracy substantially (see Table 1: baseline acc 0.9974; ATTN high-freq removed -&gt; acc 0.7836; MLP low-freq removed -&gt; acc 0.3589; ATTN+MLP high-freq removed -&gt; acc 0.2708; ATTN+MLP low-freq removed -&gt; acc 0.0594).",
            "limitations_and_failure_modes": "Relies on a combination of approximation (low-frequency) and modular classification (high-frequency); if high-frequency components are ablated or absent, the model makes unit-digit errors; if low-frequency components are ablated, the model errs by tens/hundreds (magnitude errors). Low-frequency components alone lack resolution to place a precise peak, hence require high-frequency modular signals. Performance depends on pretrained token embeddings that contain useful Fourier features; without those, mechanism degrades.",
            "comparison_to_other_models": "Compared experimentally to models trained from scratch (same architecture) and to smaller models with/without pretrained embeddings; shows that pretrained GPT-2-XL learns sparse Fourier features used in both MLP and attention, enabling near-perfect accuracy, in contrast to models trained from scratch which lack such features and perform worse.",
            "uuid": "e4629.0",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-XL (trained-from-scratch)",
            "name_full": "GPT-2-XL trained from random initialization on addition task",
            "brief_description": "Same GPT-2-XL architecture but randomly initialized and trained only on the synthetic addition dataset (no pretraining); analyzed to contrast the effect of pretraining.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2-XL (random init)",
            "model_description": "Decoder-only Transformer, 48 layers, ≈1.5B parameters, trained from scratch only on the addition synthetic data until convergence.",
            "arithmetic_task_type": "Single-step integer addition (same dataset as fine-tuned experiments)",
            "mechanism_hypothesis": "Learns primarily low-frequency/approximation representations (coarse magnitude) without developing the sparse high-frequency Fourier components needed for modular classification; thus relies on approximation and cannot reliably compute modular/unit-digit information.",
            "evidence_for_mechanism": "Fourier analysis of intermediate logits and token embeddings shows no outlier high-frequency components (Figures 6 and 7a); behavior shows frequent off-by-one errors (unit-digit mistakes) and other small errors consistent with missing modular classification signals; final accuracy lower than pre-trained model.",
            "evidence_against_mechanism": "None presented; results are consistent across analyses.",
            "performance_metrics": "Final test accuracy after convergence: 94.44% (vs. 99.74% for pre-trained then fine-tuned GPT-2-XL).",
            "probing_or_intervention_results": "DFT of logits and token embeddings shows absence of sharp Fourier peaks at periods associated with modular tasks; behavioral error analysis (Figure 22 and related text) shows predominance of small errors (off-by-one) consistent with missing high-frequency features.",
            "limitations_and_failure_modes": "Lacks high-frequency Fourier features and so makes frequent unit-digit errors and cannot reliably perform modular classification; poorer generalization/precision compared to pretrained model.",
            "comparison_to_other_models": "Direct contrast to pre-trained GPT-2-XL: pretraining introduces Fourier structure in token embeddings and intermediate logits that the randomly initialized model does not develop when trained only on the addition dataset.",
            "uuid": "e4629.1",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-2-small (scratch vs. pretrained embeddings)",
            "name_full": "GPT-2-small (124M params) trained from scratch, with and without frozen pre-trained token embeddings",
            "brief_description": "Small GPT-2 model (12 layers, 124M parameters) trained on the addition dataset either from scratch (random embeddings) or with frozen pretrained token embeddings; used to test causality of pretrained embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2-small (experiment)",
            "model_description": "Decoder-only Transformer, 12 layers, ≈124M parameters; two training regimes reported: (a) full random init trained on addition; (b) randomly initialize most weights but freeze and use pre-trained token embeddings (from a pre-trained model) while training remaining weights on addition.",
            "arithmetic_task_type": "Single-step integer addition (same synthetic dataset)",
            "mechanism_hypothesis": "Pretrained token embeddings contain Fourier features (periods 2,2.5,5,10) that provide an inductive bias enabling the model to learn the MLP/attention decomposition and achieve perfect arithmetic performance; without these embeddings, the small model struggles to develop the necessary Fourier features.",
            "evidence_for_mechanism": "Training runs (5 seeds) show that GPT-2-small with frozen pre-trained token embeddings achieves 100% test accuracy and faster convergence, while GPT-2-small trained fully from scratch achieves ~53.95% test accuracy (Figures 7b and text). Fourier analysis of pretrained embeddings demonstrates outlier components at relevant periods (Figure 5a).",
            "evidence_against_mechanism": "None shown; results consistent across seeds.",
            "performance_metrics": "GPT-2-small trained from scratch: test accuracy ≈ 53.95%; GPT-2-small with frozen pre-trained token embeddings: test accuracy = 100% across 5 seeds.",
            "probing_or_intervention_results": "Intervention: freezing/reusing pretrained token embeddings while training other weights causes a dramatic performance improvement and faster convergence; DFT of token embeddings shows presence of Fourier components hypothesized to be leveraged.",
            "limitations_and_failure_modes": "Small models without pretrained embeddings may fail to learn high-frequency modular signals and thus cannot reliably perform exact addition; reliance on pre-trained embeddings suggests sensitivity to representation priors.",
            "comparison_to_other_models": "Shows that even small Transformers can attain perfect addition accuracy if given pretrained embeddings containing Fourier features; supports hypothesis that pretraining produces useful inductive biases absent in scratch-trained models.",
            "uuid": "e4629.2",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Open-source prompted LMs (GPT-J, Phi-2)",
            "name_full": "GPT-J (6B) and Phi-2 (2.7B) evaluated under in-context learning",
            "brief_description": "Large open-source models evaluated with few-shot in-context examples on the addition dataset; internal representation analysis (for Phi-2 and GPT-J) and behavioral error patterns analyzed.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J (6B) and Phi-2 (2.7B)",
            "model_description": "Autoregressive Transformer models of different sizes: GPT-J (~6B parameters) and Phi-2 (~2.7B). Evaluated in 4-shot in-context learning setting on the addition dataset; internal logits analyzed (for Phi-2 and GPT-J) across last 15 layers.",
            "arithmetic_task_type": "Single-step integer addition via in-context learning (4-shot)",
            "mechanism_hypothesis": "Use Fourier-feature-like representations during in-context arithmetic: MLP and attention outputs show sparse Fourier components at similar periods (2, 2.5, 5, 10) as observed in fine-tuned GPT-2-XL, suggesting the same approximation + modular classification decomposition.",
            "evidence_for_mechanism": "DFT of intermediate logits across last 15 layers shows outlier Fourier components with periods ≈2, 2.5, 5, 10 for Phi-2 and GPT-J (Figure 8 and Figure 18); behavioral error distributions: absolute errors are predominantly multiples of 10 (93% for GPT-J, 73% for Phi-2), consistent with absence or weakness of modular resolution beyond tens when high-frequency components are lacking or misaligned.",
            "evidence_against_mechanism": "None presented; internal analyses support mechanism for these models.",
            "performance_metrics": "Behavioral: many absolute errors are multiples of 10 (GPT-J: 93% of absolute errors are multiples of 10; Phi-2: 73%). Exact accuracy numbers not stated for these in-context experiments, but models cannot perform addition without in-context conditioning.",
            "probing_or_intervention_results": "DFT probing of MLP and attention logits across last layers; behavioral error analysis (multiples-of-10 pattern).",
            "limitations_and_failure_modes": "Without appropriate alignment of Fourier components (or without sufficient high-frequency components), errors concentrate on multiples of 10 (i.e., incorrect unit digits), indicating difficulty in modular classification resolution; models require in-context examples to perform task (no few-shot-free performance).",
            "comparison_to_other_models": "Behavior and internal DFT patterns are qualitatively similar to fine-tuned GPT-2-XL, suggesting a shared reliance on Fourier-like features across architectures and sizes when solving addition.",
            "uuid": "e4629.3",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Closed-source LLMs (GPT-3.5 / GPT-4 / PaLM-2)",
            "name_full": "Closed-source instruction-tuned models GPT-3.5, GPT-4, and PaLM-2 (behavioral analysis only)",
            "brief_description": "Large commercial models evaluated behaviorally (0-shot) on the addition dataset; internal representations not accessible, but error patterns were analyzed to infer likely mechanisms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4, PaLM-2 (closed-source)",
            "model_description": "Instruction-tuned large language models (closed-source); evaluated on the addition dataset with 0-shot prompting; internal activations not available for DFT probing.",
            "arithmetic_task_type": "Single-step integer addition (0-shot behavioral evaluation)",
            "mechanism_hypothesis": "Behavioral error distributions (predominance of multiples-of-10 errors) are consistent with a mechanism that uses Fourier-like modular signals (unit/ten-digit classification) combined with magnitude approximation, as in open-source models, though internal evidence is unavailable.",
            "evidence_for_mechanism": "Behavioral error analysis: absolute errors are multiples of 10 for GPT-3.5 and GPT-4 (100% of absolute errors observed), and 87% for PaLM-2, suggesting a failure mode aligned with missing/incorrect unit-digit resolution consistent with Fourier-feature-based mechanisms.",
            "evidence_against_mechanism": "No internal probing possible; inference is based solely on behavioral statistics, so causality is not proven.",
            "performance_metrics": "Behavioral error distribution: GPT-3.5 and GPT-4: 100% of observed absolute errors were multiples of 10; PaLM-2: 87% of observed absolute errors were multiples of 10. Exact overall accuracies are not reported.",
            "probing_or_intervention_results": "None (no internal access); only error-pattern analysis across outputs.",
            "limitations_and_failure_modes": "Behavioral patterns suggest frequent unit-digit / small modular errors (multiples-of-10), but internal mechanism cannot be confirmed without activation-level access.",
            "comparison_to_other_models": "Behaviorally similar error signature to open-source and fine-tuned models, supporting the hypothesis that many LLMs rely on similar Fourier/modular decomposition for addition.",
            "uuid": "e4629.4",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Fourier features (mechanism)",
            "name_full": "Fourier-feature representation and computation (periodic basis in logits/embeddings)",
            "brief_description": "A distributed representation scheme where numbers (token embeddings and intermediate logits) are represented in a Fourier basis; a sparse set of Fourier components (specific frequencies/periods) are used to encode both coarse magnitude (low-frequency) and modular residues (high-frequency), enabling addition via superposition and phase shifts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mechanism (general across pre-trained LLMs)",
            "model_description": "Representation: token embeddings and intermediate module outputs (W^U·MLP, W^U·Attn) exhibit sparsity in the discrete Fourier basis over the number token space (p≈521). Mechanistic decomposition: MLPs mainly contribute low-frequency components approximating magnitude; attention primarily contributes high-frequency components computing modular residues (mod 2,5,10,...); final logits are linear superpositions of a small set of Fourier basis functions whose phases and amplitudes align to locate the correct integer.",
            "arithmetic_task_type": "Applies to single-step integer addition and modular classification subproblems (observed for addition; authors also mention extension to multiplication and other formats in appendices)",
            "mechanism_hypothesis": "Pretraining induces number token embeddings containing prominent Fourier components; during (fine-tuning or in-context) computation, the Transformer combines low-frequency amplitude-encoding from MLPs with high-frequency modular signals from attention to compute exact sums via linear superposition and phase adjustments in Fourier space.",
            "evidence_for_mechanism": "Direct analyses in the paper: (1) DFT of intermediate logits reveals sparse large-magnitude components at periods 2, 2.5, 5, 10 (Figures 2,3); (2) reconstructing logits with top Fourier components yields a peaked distribution at the correct answer (Figure 4); (3) causal ablations via projection-based low/high-pass filters on module outputs produce failure modes consistent with the hypothesized roles of low/high frequencies (Table 1); (4) token embeddings of pre-trained models contain matching Fourier peaks (Figure 5), and transferring pretrained embeddings to scratch-trained models rescues performance.",
            "evidence_against_mechanism": "Models trained from scratch can still reach non-trivial accuracy using primarily low-frequency approximation (e.g., GPT-2-XL scratch achieves 94.44%), indicating alternative (less precise) solutions exist; extremely low-frequency components (very large periods) lack fine localization and therefore cannot alone resolve exact tokens without high-frequency support.",
            "performance_metrics": "Mechanism enables near-perfect exact-match performance when present (e.g., pre-trained GPT-2-XL fine-tuned: 99.74%); absence leads to reduced accuracy and characteristic error patterns (off-by-1 and multiples-of-10).",
            "probing_or_intervention_results": "DFT-based probing, identification of outlier components, inverse-DFT reconstructions, and linear-projection ablations (low-pass/high-pass via nullspace projection of B F W^U) that remove designated Fourier components from module outputs; single-component retention experiments (Definition B.1) also used to test causal role of individual Fourier frequencies.",
            "limitations_and_failure_modes": "Requires pre-existing Fourier structure in embeddings or else training may not converge to a precise modular+approximation decomposition; low-frequency components by themselves provide coarse magnitude but lack precision, and high-frequency components are essential for unit-digit classification yet can be sparse and brittle; misalignment of Fourier phases yields modular errors (e.g., multiples-of-10 mistakes).",
            "comparison_to_other_models": "Observed across multiple pre-trained and prompted models (GPT-2-XL fine-tuned, GPT-J, Phi-2) and hypothesized to underlie behavioral error patterns in closed-source models (GPT-3.5, GPT-4, PaLM-2); absent in models trained from scratch unless pretrained embeddings are introduced.",
            "uuid": "e4629.5",
            "source_info": {
                "paper_title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Harmonics of learning: Universal fourier features emerge in invariant networks",
            "rating": 2,
            "sanitized_title": "harmonics_of_learning_universal_fourier_features_emerge_in_invariant_networks"
        },
        {
            "paper_title": "Fourier features let networks learn high frequency functions in low dimensional domains",
            "rating": 2,
            "sanitized_title": "fourier_features_let_networks_learn_high_frequency_functions_in_low_dimensional_domains"
        },
        {
            "paper_title": "Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic",
            "rating": 2,
            "sanitized_title": "fourier_circuits_in_neural_networks_unlocking_the_potential_of_large_language_models_in_mathematical_reasoning_and_modular_arithmetic"
        },
        {
            "paper_title": "Feature emergence via margin maximization: case studies in algebraic tasks",
            "rating": 1,
            "sanitized_title": "feature_emergence_via_margin_maximization_case_studies_in_algebraic_tasks"
        }
    ],
    "cost": 0.01572625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Pre-trained Large Language Models Use Fourier Features to Compute Addition</h1>
<p>Tianyi Zhou Deqing Fu Vatsal Sharan Robin Jia<br>Department of Computer Science<br>University of Southern California<br>Los Angeles, CA 90089<br>{tzhou029, deqingfu, vsharan, robinjia}@usc.edu</p>
<h4>Abstract</h4>
<p>Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features-dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 2
2 Problem Setup ..... 3
3 Language Models Solve Addition with Fourier Features ..... 3
3.1 Behavioral Analysis ..... 3
3.2 Fourier Features in MLP \&amp; Attention Outputs ..... 4
3.3 Fourier Features are Causally Important for Model Predictions ..... 7
4 Effects of Pre-training ..... 8
4.1 Fourier features in Token Embedding ..... 8
4.2 Contrasting Pre-trained Models with Models Trained from Scratch ..... 8
4.3 Fourier Features in Prompted Pre-Trained Models ..... 10
5 Related Work ..... 11
6 Conclusion ..... 12
A Formal Definition of Transformer and Logits in Fourier Space ..... 17
B Fourier Components Separation and Selection of $\tau$ ..... 20
C Does Fourier Features Generalize? ..... 23
C. 1 Token Embedding for Other LMs ..... 23
C. 2 Multiplication Task ..... 24
C. 3 Same Results for other format ..... 25
C. 4 Fourier Features in Other Pre-trained LM ..... 26
D Supporting Evidence For the Fourier Features ..... 27
E More Experiments on GPT-2-XL Trained from Scratch ..... 28
F Details of Experimental Settings ..... 29</p>
<h1>1 Introduction</h1>
<p>Mathematical problem solving has become a crucial task for evaluating the reasoning capabilities of large language models (LLMs) [HBK ${ }^{+} 21, \mathrm{CKB}^{+} 21, \mathrm{LBX}^{+} 24, \mathrm{FKL}^{+} 24$ ]. While LLMs exhibit impressive mathematical abilities [Ope23, Goo23b, Ant24, WLS17, TPSI21, BMR ${ }^{+} 20, \mathrm{FPG}^{+} 24$ ], it remains unclear how they perform even basic mathematical tasks. Do LLMs apply mathematical principles when solving math problems, or do they merely reproduce memorized patterns from the training data?</p>
<p>In this work, we unravel how pre-trained language models solve simple mathematical problems such as "Put together 15 and 93. Answer: __". Prior work has studied how Transformers, the underlying architecture of LLMs, perform certain mathematical tasks. Most studies [Cha23, GTLV22, vONR ${ }^{+} 22, \mathrm{BCW}^{+} 23, \mathrm{FCJS} 23, \mathrm{NCL}^{+} 23, \mathrm{GLL}^{+} 24, \mathrm{PBE}^{+} 22 \mathrm{~b}$ ] focus on Transformers with a limited number of layers or those trained from scratch; [HLV23] analyzes how the pre-trained GPT-2-small performs the greater-than task. Our work focuses on a different task from prior interpretability work-integer addition-and shows that pre-trained LLMs learn distinct mechanisms from randomly initialized Transformers.</p>
<p>In $\S 3$, we show that pre-trained language models compute addition with Fourier featuresdimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. First, we analyze the behavior of pre-trained LLMs on the addition task after fine-tuning, which leads to almost perfect accuracy on the task. Rather than merely memorizing answers from the training data, the models progressively compute the final answer layer by layer. Next, we analyze the contributions of individual model components using Logit Lens [BFS ${ }^{+} 23$ ]. We observe that some components primarily approximate the answer-they promote all numbers close to the correct answer in magnitude-while other components primarily classify the answer modulo $m$ for various numbers $m$. Then, we use Fourier analysis to isolate features in the residual stream responsible for the low-frequency "approximation" and high-frequency "classification" subtasks. Identifying these features allows us to precisely ablate the ability of the model to perform either approximation or classification by applying a low-pass or high-pass filter, respectively, to the outputs of different model components. We find that MLP layers contribute primarily to approximation, whereas attention layers contribute primarily to classification.</p>
<p>In $\S 4$, we show that pre-training is crucial for learning this mechanism. The same network trained from scratch with random initialization not only shows no signs of Fourier features, but also has lower accuracy. We identify pre-trained token embeddings as a key source of inductive bias that help the pre-trained model learn a more precise mechanism for addition. Across the pretrained token embeddings of many different pre-trained models, Fourier analysis uncovers large magnitudes of components with periods 2,5 , and 10 . Introducing pre-trained token embeddings when training the model from scratch enables the model to achieve perfect test accuracy. Finally, we show that the same Fourier feature mechanism is present not only in models that were pretrained and then fine-tuned, but also in frozen pre-trained LLMs when prompted with arithmetic problems.</p>
<p>Overall, our work provides a mechanistic perspective on how pre-trained LLMs compute addition through the lens of Fourier analysis. It not only broadens the scope from only investigating few-layer Transformers trained to fit a particular data distribution to understanding LLMs as a whole, but also hints at how pre-training can lead to more precise model capabilities.</p>
<h1>2 Problem Setup</h1>
<p>Task and Dataset. We constructed a synthetic addition dataset for fine-tuning and evaluation purposes. Each example involves adding two numbers $\leq 260$, chosen because the maximum number that can be represented by a single token in the GPT-2-XL tokenizer is 520 . For each pair of numbers between 0 and 260 , we randomly sample one of five natural language question templates and combine it with the two numbers. The dataset is shuffled and then split into training ( $80 \%$ ), validation ( $10 \%$ ), and test ( $10 \%$ ) sets. More details are provided in Appendix F. In Appendix C.3, we show our that results generalize to a different dataset formatted with reverse Polish notation.</p>
<p>Model. Unless otherwise stated, all experiments focus on the pre-trained GPT-2-XL model that has been fine-tuned on our addition dataset. This model, which consists of 48 layers and approximately 1.5 billion parameters, learns the task almost perfectly, with an accuracy of $99.74 \%$ on the held-out test set. We examine other models in $\S 4.2$ and $\S 4.3$.</p>
<p>Transformers. We focus on decoder-only Transformer models [VSP ${ }^{+} 17$ ], which process text sequentially, token by token, from left to right. Each layer $\ell$ in the Transformer has an attention module with output Attn $^{(l)}$ and an MLP module with output $\mathrm{MLP}^{(l)}$. Their outputs are added together to create a continuous residual stream $h\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation accumulates all additive updates within the residual stream, with the representation $h^{(\ell)}$ in the $\ell$-th layer given by:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\operatorname{Attn}^{(\ell)}+\operatorname{MLP}^{(\ell)}
$$</p>
<p>The output embedding $W^{U}$ projects the residual stream to the space of the vocabulary; applying the softmax function then yields the model's prediction. We provide formal definitions in Appendix A.</p>
<h2>3 Language Models Solve Addition with Fourier Features</h2>
<p>In this section, we analyze the internal mechanisms of LLMs when solving addition tasks, employing a Fourier analysis framework. We first show that the model initially approximates the solution before iteratively converging to the correct answer (§3.1). We then show that the model refines its initial approximation by computing the exact answer modulo 2, 5, and 10, employing Fourier components of those same periods (§3.2). Finally, we demonstrate through targeted ablations that the identified Fourier components are causally important for the model's computational processes (§3.3). Specifically, we show that MLP layers primarily approximate the magnitude of the answer, using low-frequency features, while attention layers primarily perform modular addition using high-frequency components.</p>
<h3>3.1 Behavioral Analysis</h3>
<p>Our first goal is to understand whether the model merely memorizes and recombines pieces of information learned during training, or it performs calculations to add two numbers.</p>
<p>Extracting intermediate predictions. To elucidate how LLMs perform computations and progressively refine their outputs towards the correct answer, we extract model predictions at each layer from the residual stream. Let $L$ denote the number of layers. Using the Logit Lens method [BFS $\left.{ }^{+} 23\right]$, instead of generating predictions by computing logits $W^{U} h^{(L)}$, predictions are derived through $W^{U} h^{(\ell)}$ where $\ell \in[L]$. We compute the accuracy of the prediction using each intermediate state $h^{(\ell)}$. If the models merely retrieve and recombine pieces of information learned during</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Visualization of predictions extracted from fine-tuned GPT-2-XL at intermediate layers. Between layers 20 and 30, the model's accuracy is low, but its prediction is often within 10 of the correct answer: the model first approximates the answer, then refines it. (b) Heatmap of the logits from different MLP layers for the running example, "Put together 15 and 93. Answer: 108". The y-axis represents the subset of the number space around the correct prediction, while the x-axis represents the layer index. The 33-rd layer performs mod 2 operations (favoring even numbers), while other layers perform other modular addition operations, such as mod 10 (45-th layer). Additionally, most layers allocate more weight to numbers closer to the correct answer, 108. (c) Analogous plot for attention layers. Nearly all attention modules perform modular addition.</p>
<p>Training, certain layers will directly map this information to predictions. For instance, [MEP23] demonstrates that there is a specific MLP module directly that maps a country to its capital.</p>
<p><strong>LLMs progressively compute the final answers.</strong> Figure 1a instead shows that the model progressively approaches the correct answer, layer by layer. The model is capable of making predictions that fall within the range of ±2 and ±10 relative to the correct answer in the earlier layers, compared to the exact-match accuracy. This observation implies that the Transformer's layer-wise processing structure is beneficial for gradually refining predictions through a series of transformations and updates applied to the token representations.</p>
<h3>3.2 Fourier Features in MLP &amp; Attention Outputs</h3>
<p><strong>Logits for MLP and attention have periodic structures.</strong> We now analyze how each MLP and attention module contributes to the final prediction. We transform the output of the attention and MLP output at layer ℓ into the token space using W<sup>U</sup>Attn<sup>(ℓ)</sup> and W<sup>U</sup>MLP<sup>(ℓ)</sup> at each layer, thereby obtaining the logits L for each MLP and attention module. We use the running example "Put together 15 and 93. Answer: 108" to demonstrate how the fine-tuned GPT-2-XL performs the computation. As illustrated in Figure 1b and Figure 1c, both the MLP and attention modules exhibit a periodic pattern in their logits across the output number space, e.g., the MLP in layer 33, outlined in green, promotes all numbers that are congruent to 108 mod 2 (in Figure 19 in the appendix, we zoom into such layers to make this clearer). Overall, we observe two distinct types of computation within these components. Some components predominantly assign a high weight to numbers around the correct answer, which we term <em>approximation</em>. Meanwhile, other components predominantly assign a high weight to all numbers congruent to a + b mod c for some constant c, which we term <em>classification</em>.</p>
<p><strong>Logits for MLP and attention are approximately sparse in the Fourier space.</strong> It is natural to transform the logits into Fourier space to gain a better understanding of their properties such as the periodic pattern. We apply the discrete Fourier transform to represent the logits as the sum of sine and cosine waves of different periods: the k-th component in Fourier space has period</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The intermediate logits in Fourier space. We annotate the top-10 outlier high-frequency Fourier components based on their magnitudes. $T$ stands for the period of that Fourier component. (a) The logits in Fourier space for the MLP output of the 33-rd layer, i.e., $\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(33)}$. The component with period 2 has the largest magnitude, aligning with the observations in Figures 1b and 19a. (b) The logits in Fourier space for the attention output of the 40-th layer, i.e., $\tilde{\mathcal{L}}</em>$. The components with periods 5 and 10 have the largest magnitude, aligning with the observations in Figures 1c and 19b.
$520 / k$ and frequency $k / 520$ (see Appendix A for more details). Let $\tilde{\mathcal{L}}$ denote the logits in Fourier space. Figure 2 shows the Fourier space logits for two layers from Figure 1b and Figure 1c that have a clear periodic pattern. We find that the high-frequency components in Fourier space, which we define as components with index greater or equal to 50, are approximately sparse as depicted in Figure 2. This observation aligns with $\left[\mathrm{NCL}^{+} 23\right]$, which found that a one-layer Transformer utilizes particular Fourier components within the Fourier space to solve the modular addition task.}}^{(40)</p>
<p>In Figure 3, we show that similar sparsity patterns in Fourier space hold across the entire dataset. We compute the logits in Fourier space for the last 15 layers, i.e., $\tilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(t)}$ and $\tilde{\mathcal{L}}</em>$ where $\ell \in[32,47]$, for all test examples and average them. We annotate the top-10 outlier high-frequency components based on their magnitude. The MLPs also exhibit some strong low-frequency components; the attention modules do not exhibit strong low-frequency components, only high-frequency components.}}^{(t)</p>
<p>Final logits are superpositions of these outlier Fourier components. The final logits, $\mathcal{L}^{(L)}$, are the sum of all $\mathcal{L}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{MLP}}^{(l)}$ and $\mathcal{L}</em>$ based on their magnitudes and transfer them back to logits in number space via the inverse discrete Fourier transform (Figure 4a). The large-period (low-frequency) components approximate the magnitude while the small-period (high-frequency) components are crucial for modular addition. Figure 4b shows that aggregating these 5 waves is sufficient to predict the correct answer.}}^{(l)}$ across all layers $l \in[L]$. Figure 4 elucidates how these distinct Fourier components contribute to the final prediction, for the example "Put together 15 and 93. Answer: 108". We select the top-5 Fourier components of $\tilde{\mathcal{L}}^{(L)</p>
<p>Why is high-frequency classification helpful? The Fourier basis comprises both cos and sin waves (see Definition A.3). By adjusting the coefficients of cos and sin, the trained model can manipulate the phase of the logits in Fourier space (number shift in number space), aligning the peak</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Analysis of logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, outlier Fourier components have periods around 2, 2.5, 5, and 10.
of the wave more closely with the correct answer. As shown in Figure 4a, consider a wave with a period of 2 . Here, the peak occurs at every even number in the number space, corresponding to the $\bmod 2$ task. In contrast, for components with a large period such as 520 , the model struggles to accurately position the peak at 108 (also see Figure 13 in the appendix for the plot of this component with period 520 in the full number space). This scenario can be interpreted as solving a "mod 520 " task-a classification task among 520 classes-which is challenging for the model to learn accurately. Nevertheless, even though the component with a period of 520 does not solve the "mod 520 " task precisely, it does succeed in assigning more weight to numbers near 108. The classification results from the high-frequency components can then provide finer-grained resolution to distinguish between all the numbers around 108 assigned a large weight by the lower frequencies. Due to this, the low-frequency components need not be perfectly aligned with the answer to make accurate predictions.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of how a sparse subset Fourier components can identify the correct answer. (a) Shows the top-5 Fourier components for the final logits. (b) Shows the sum of these top-5 Fourier components, highlighting how the cumulative effect identifies the correct answer, 108.</p>
<h1>3.3 Fourier Features are Causally Important for Model Predictions</h1>
<p>In the previous section, we demonstrated that there are outlier Fourier components in the logits generated by both the MLP and attention modules, as shown in Figure 3. We also illustrated that, in one example, the high-frequency components primarily approximate the magnitude, while the low-frequency components are crucial for modular addition tasks, as depicted in Figure 4. In this section, through an ablation study conducted across the entire test dataset, we show that both types of components are essential for correctly computing sums. Moreover, we reveal that the MLP layers primarily approximate the magnitude of the answer using low-frequency features, whereas the attention layers are responsible for modular addition using high-frequency features.</p>
<p>Filtering out Fourier components. To understand the role various frequency components play for the addition task, we introduce low-pass and high-pass filters $\mathcal{F}$. For an intermediate state $h$, and a set of frequencies $\Gamma=\left{\gamma_{1}, \ldots, \gamma_{k}\right}$, the filter $\mathcal{F}(h ; \Gamma)$ returns the vector $\widehat{h}$ that is closest in $L_{2}$ distance to $h$ subject to the constraint that the Fourier decomposition of $W^{U} \widehat{h}$ at every frequency $\gamma_{i}$ is 0 . We show in Appendix A that this has a simple closed-form solution involving a linear projection. We then apply either a low-pass filter by taking $\Gamma$ to be all the components whose frequencies are greater than the frequency of the $\tau$-th component for some threshold $\tau$ (i.e., removing high-frequency components), and a high-pass filter by taking $\Gamma$ to be all the components whose frequencies are less than the frequency of the $\tau$-th component (i.e., removing low-frequency components). As in the previous subsection, we take the high-frequency threshold $\tau=50$ for the following experiments (see Appendix B for more details).</p>
<p>Different roles of frequency components in approximation and classification tasks. We evaluated the fine-tuned GPT-2-XL model on the test dataset with different frequency filters applied to all of the output of MLP and attention modules. The results, presented in Table 1, indicate that removing low-frequency components from attention modules or high-frequency components from MLP modules does not impact performance. This observation suggests that attention modules are not crucial for approximation tasks, and MLP modules are less significant for classification tasks.</p>
<p>Eliminating high-frequency components from attention results in a noticeable decrease in accuracy. Furthermore, removing high-frequency components from both the attention and MLP modules simultaneously leads to an even greater reduction in accuracy. This finding corresponds with observations from Figure 1b,c and Figure 3, which indicate that both MLP and attention modules are involved in classification tasks due to the presence of high-frequency components in the logits. However, the approximation tasks are primarily performed by the MLP modules alone.</p>
<p>The errors induced by these ablations align with our mechanistic understanding. Ablating low-frequency parts of MLPs leads to off-by 10,50 , and 100 errors: the model fails to perform the approximation subtask, though it still accurately predicts the unit digit. Conversely, ablating highfrequency parts of attention leads to small errors less than 6 in magnitude: the model struggles to accurately predict the units digit, but it can still estimate the overall magnitude of the answer. See Figure 20 in the Appendix for more details. These observations validate our hypothesis that low-frequency components are crucial for approximation, while high-frequency components are vital for classification. The primary function of MLP modules is to approximate the magnitude of outcomes using low-frequency components, while the primary role of attention modules is to ensure accurate classification by determining the correct unit digit.</p>
<p>Table 1: Impact of Filtering out Fourier Components on Model Performance. Removing low-frequency components from attention modules (blue) or high-frequency components from MLP modules (red) does not impact performance</p>
<table>
<thead>
<tr>
<th>Module</th>
<th>Fourier Component Removed</th>
<th>Validation Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Without Filtering</td>
<td>0.0073</td>
<td>0.9974</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>Low-Frequency</td>
<td>4.0842</td>
<td>0.0594</td>
</tr>
<tr>
<td>ATTN</td>
<td>Low-Frequency</td>
<td>0.0352</td>
<td>0.9912</td>
</tr>
<tr>
<td>MLP</td>
<td>Low-Frequency</td>
<td>2.1399</td>
<td>0.3589</td>
</tr>
<tr>
<td>ATTN &amp; MLP</td>
<td>High-Frequency</td>
<td>1.8598</td>
<td>0.2708</td>
</tr>
<tr>
<td>ATTN</td>
<td>High-Frequency</td>
<td>0.5943</td>
<td>0.7836</td>
</tr>
<tr>
<td>MLP</td>
<td>High-Frequency</td>
<td>0.1213</td>
<td>0.9810</td>
</tr>
</tbody>
</table>
<h2>4 Effects of Pre-training</h2>
<p>The previous section shows that pre-trained LLMs leverage Fourier features to solve the addition problem. Now, we study where the models' reliance on Fourier features comes from. In this section, we demonstrate that LLMs learn Fourier features in the token embeddings for numbers during pre-training. These token embeddings are important for achieving high accuracy on the addition task: models trained from scratch achieve lower accuracy, but adding just the pre-trained token embeddings fixes this problem. We also show that pre-trained models leverage Fourier features not only when fine-tuned, but also when prompted.</p>
<h3>4.1 Fourier features in Token Embedding</h3>
<p>Number embedding exhibits approximate sparsity in the Fourier space. Let $W^{E} \in \mathbb{R}^{p \times D}$, where $p=521$ and $D$ is the size of the token embeddings, denote the token embedding for numbers. We apply the discrete Fourier transform to each column of $W^{E}$ to obtain a matrix $V \in \mathbb{R}^{p \times D}$, where each row represents a different Fourier component. Then we take the $L_{2}$ norm of each row to yield a $p$-dimensional vector. Each component $j$ in this vector measures the overall magnitude of the $j$-th Fourier component across all the token embedding dimensions. Figure 5a shows the magnitude of different Fourier components in the token embedding of GPT-2-XL. We see that the token embedding has outlier components whose periods are $2,2.5,5$, and 10. Therefore, similar to how the model uses different Fourier components to represent its prediction (as shown in Section 3.2), the token embeddings represent numbers with different Fourier components. Figure 14 in the Appendix shows that the token embeddings of other pre-trained models have similar patterns the Fourier space. This suggests that Fourier features are a common attribute in the token embedding of pre-trained LLMs. In Figure 5b, we use t-SNE and $k$-means to visualize the token embedding clustering. We can see that numbers cluster not only by magnitude but also by their multiples of 10.</p>
<h3>4.2 Contrasting Pre-trained Models with Models Trained from Scratch</h3>
<p>To understand the necessity of Fourier features for the addition problem, we trained the GPT-2-XL model from scratch on the addition task with random initialization. After convergence, it achieved only $94.44\%$ test accuracy (recall that the fine-tuned GPT-2-XL model achieved $99.74\%$ accuracy).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (a) Number embedding in Fourier space for fine-tuned GPT-2-XL. $T$ stands for the period of that Fourier component.(b) Visualization of token embedding clustering of GPT-2 using T-SNE and $k$-means with 10 clusters. The numbers are clustered based on their magnitude and whether they are multiples of 10 .
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of the logits in Fourier space on the test dataset from the last 15 layers for the GPT-2-XL model trained from scratch. For both the MLP and attention modules, there are no outlier Fourier components, in contrast with the clear outlier components in the fine-tuned model (Figure 3).</p>
<p>Fourier features are learned during pre-training. Figure 6 shows that there are no Fourier features in the intermediate logits of the GPT-2-XL model trained from scratch on the addition task. Furthermore, Figure 7a shows that the token embeddings also have no Fourier features. Without leveraging Fourier features, the model merely approximates the correct answer without performing modular addition, resulting in frequent off-by-one errors between the prediction and the correct answer (see details in Figure 22).</p>
<p>Pre-trained token embeddings improve model training. We also trained GPT-2-small, with 124 million parameters and 12 layers, from scratch on the addition task. GPT-2-small often struggles with mathematical tasks [MMV+22]. This model achieved a test accuracy of only $53.95 \%$</p>
<p>after convergence. However, when we freeze the token embedding layer and randomly initialize the weights for all other layers before training on the addition task, the test accuracy increases to $100 \%$, with a significantly faster convergence rate. This outcome was consistently observed across five different random seeds, as illustrated in Figure 7b. This demonstrates that given the number embeddings with Fourier features, the model can effectively learn to leverage these features to solve the addition task.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: (a) The number embedding in Fourier space for GPT-2-XL trained from scratch. There are no high-frequency outlier components, in contrast with the pre-trained embeddings (Figure 5a). (b) Validation accuracy of GPT-2-small trained from scratch either with or without pre-trained token embeddings. We show the mean and the standard deviation of the validation accuracy across 5 random seeds. GPT-2-small with pre-trained token embedding consistently achieves $100 \%$ accuracy, while GPT-2-small without pre-trained token embedding only achieves less than $60 \%$ accuracy.</p>
<h1>4.3 Fourier Features in Prompted Pre-Trained Models</h1>
<p>Finally, we ask whether larger language models use similar Fourier features during prompting.
Pre-trained LLMs use Fourier features to compute addition during in-context learning. We first test on the open-source models GPT-J [WK21] with 6B parameters, and Phi-2 [JBA ${ }^{+} 23$ ] with 2.7B parameters on the test dataset. Without in-context learning, the model cannot perform addition tasks. Therefore, we use 4 -shot in-context learning to test its performance. Their absolute errors are predominantly multiples of 10: $93 \%$ of the time for GPT-J, and $73 \%$ for Phi-2 . Using the Fourier analysis framework proposed in Section 3.2, we demonstrate that for Phi-2 and GPT-J, the outputs of MLP and attention modules exhibit approximate sparsity in Fourier space across the last 15 layers (Figure 8 and Figure 18). This evidence strongly suggests that these models leverage Fourier features to compute additions.</p>
<p>Closed-source models exhibit similar behavior. We study the closed-source models GPT-3.5 [Ope22], GPT-4 [Ope23], and PaLM-2 [Goo23a]. While we cannot analyze their internal representations, we can study whether their behavior on addition problems is consistent with reliance on Fourier features. Since closed-source LLMs are instruction tuned and perform well without incontext learning, we conduct error analysis with 0 -shot. Most absolute errors by these models are also multiples of 10: $100 \%$ of the time for GPT-3.5 and GPT-4, and $87 \%$ for PaLM-2. The similarity in error distribution to that of open-source models leads us to hypothesize that Fourier features play a critical role in their computational mechanism.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: For Phi-2 (4-shot), we analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules, the outlier Fourier components have periods around 2, 2.5, 5, and 10, similar to the fine-tuned GPT-2-XL logits (Figure 3).</p>
<h1>5 Related Work</h1>
<p>Learning mathematical tasks. Previous studies primarily explore what pre-trained LMs can achieve on arithmetic tasks, with less emphasis on the underlying mechanisms [NJL21, QWL ${ }^{+} 22$ ]. For instance, [LSL $\left.{ }^{+} 23\right]$ demonstrates that small Transformer models can effectively learn arithmetic by altering the question format and utilizing a scratchpad method [NAGA $\left.{ }^{+} 21\right]$. [HLV23] identifies activation patterns for the "greater-than" operation in GPT-2, and [Cha23] focuses on the enumeration and selection processes in GCD computation. In this paper, we dive into the specific roles of MLP and attention layers in solving mathematical tasks. Our research analyzes these components' distinct contributions to integer addition tasks.</p>
<p>Mechanisms of pre-trained LMs. Recent studies have significantly advanced our understanding of the underlying mechanisms of pre-trained Transformer models. For instance, research on "skill neurons" by [WWZ $\left.{ }^{+} 22\right]$ and "knowledge neurons" by [DDH $\left.{ }^{+} 21\right]$ underscores the development of specialized neural components that encode task-specific capabilities or hold explicit factual information in the pre-trained LMs, enhancing model performance on related tasks. [MEP23] and [GCWG22] discuss how MLPs and FFNs transform and update token representations for general language tasks. In contrast, we show that the pre-trained LMs use multiple layers to compute addition by combining the results of approximation and classification. Additionally, [ZL23] demonstrated the capacity of GPT-2 to consolidate similar information through pre-training in the model weights, which aligns with our observations on the importance of pre-training in developing effective number embedding and arithmetic computation strategies in LMs.</p>
<p>Fourier features in Neural Networks. Fourier features are commonly observed in image models, particularly in the early layers of vision models [OF97, OCS ${ }^{+} 20$, FS24]. These features enable the model to detect edges, textures, and other spatial patterns effectively. Recently, Fourier features have been noted in networks trained for tasks that allow cyclic wraparound, such as modular addition $\left[\mathrm{NCL}^{+} 23, \mathrm{MEO}^{+} 23\right]$, general group compositions [CCN23], or invariance to cyclic translations [SSOH22]. [NCL $\left.{ }^{+} 23\right]$ demonstrates that learning Fourier features can induce 'grokking' $\left[\mathrm{PBE}^{+} 22 \mathrm{a}\right]$. Furthermore, [MHKS23] provides a mathematical framework explaining the emergence of Fourier features when the network exhibits invariance to a finite group. We extend these</p>
<p>insights by observing Fourier features in tasks that do not involve cyclic wraparound. [TSM ${ }^{+20}$ ] found that by selecting problem-specific Fourier features, the performance of MLPs can be improved on a computer vision-related task.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we provide a comprehensive analysis of how pre-trained LLMs compute numerical sums, revealing a nuanced interplay of Fourier features within their architecture. Our findings demonstrate that LLMs do not simply memorize answers from training data but actively compute solutions through a combination of approximation and classification processes encoded in the frequency domain of their hidden states. Specifically, MLP layers contribute to approximating the magnitude of sums, while attention layers contribute to modular operations.</p>
<p>Our work also shows that pre-training plays a critical role in equipping LLMs with the Fourier features necessary for executing arithmetic operations. Models trained from scratch lack these crucial features and achieve lower accuracy; introducing pre-trained token embeddings greatly improves their convergence rate and accuracy. This insight into the arithmetic problem-solving capabilities of LLMs through Fourier features sets the stage for potential modifications to training approaches. By imposing specific constraints on model training, we could further enhance the ability of LLMs to learn and leverage these Fourier features, thereby improving their performance in mathematical tasks.</p>
<h2>Acknowledgments</h2>
<p>DF and RJ were supported by a Google Research Scholar Award. RJ was also supported by an Open Philanthropy research grant. VS was supported by NSF CAREER Award CCF-2239265 and an Amazon Research Award. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the funding agencies.</p>
<h1>References</h1>
<p>[Ant24] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.
[BCW ${ }^{+}$23] Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. ArXiv, abs/2306.04637, 2023.
[BFS ${ }^{+}$23] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.
[BMR ${ }^{+}$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[CCN23] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. In International Conference on Machine Learning, pages 6243-6267. PMLR, 2023.
[Cha23] François Charton. Can transformers learn the greatest common divisor? arXiv preprint arXiv:2308.15594, 2023.
[CKB ${ }^{+}$21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
$\left[\mathrm{DDH}^{+}\right.$21] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.
$\left[\mathrm{ENO}^{+}\right.$21] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.
[FCJS23] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higherorder optimization methods for in-context learning: A study with linear models, 2023.
[FKL ${ }^{+}$24] Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations, 2024.
[FPG ${ }^{+}$24] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Petersen, and Julius Berner. Mathematical capabilities of chatgpt. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>[FS24] Pierre-Étienne Fiquet and Eero Simoncelli. A polar prediction model for learning to represent visual transformations. Advances in Neural Information Processing Systems, 36, 2024.
[GCWG22] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feedforward layers build predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680, 2022.
[GLL ${ }^{+}$24] Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks: Unlocking the potential of large language models in mathematical reasoning and modular arithmetic, 2024.
[Goo23a] Google. Palm 2 technical report, 2023.
[Goo23b] Gemini Team Google. Gemini: A family of highly capable multimodal models, 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. ArXiv, abs/2208.01066, 2022.
$\left[\mathrm{HBK}^{+}\right.$21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.
[HLV23] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. arXiv preprint arXiv:2305.00586, 2023.
$\left[\mathrm{JBA}^{+}\right.$23] Mojan Javaheripi, Sebastien Bubeck, Marah Abdin, Jyoti Anejaand Caio Cesar Teodoro Mendes, Allie Del Giorno Weizhu Chen, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Piero Kauffmann, Yin Tat Lee, Yuanzhi L, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models, 2023.
[LBX ${ }^{+}$24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024.
[LSL ${ }^{+}$23] Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.
[MEO ${ }^{+}$23] Depen Morwani, Benjamin L Edelman, Costin-Andrei Oncescu, Rosie Zhao, and Sham Kakade. Feature emergence via margin maximization: case studies in algebraic tasks. arXiv preprint arXiv:2311.07568, 2023.
[MEP23] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Language models implement simple word2vec-style vector arithmetic. arXiv preprint arXiv:2305.16130, 2023.
[MHKS23] Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, and Sophia Sanborn. Harmonics of learning: Universal fourier features emerge in invariant networks. arXiv preprint arXiv:2312.08550, 2023.</p>
<p>[MMV ${ }^{+}$22] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660, 2022.
[NAGA ${ }^{+}$21] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[NCL ${ }^{+}$23] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
[NJL21] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.
[OCS ${ }^{+}$20] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionv1. Distill, 5(4):e00024-002, 2020.
[OF97] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311-3325, 1997.
[Ope22] OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2023-09-10.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
$\left[\mathrm{PBE}^{+}\right.$22a] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[PBE ${ }^{+}$22b] Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. ArXiv, abs/2201.02177, 2022.
[QWL ${ }^{+}$22] Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.
[SSOH22] Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral neural networks. arXiv preprint arXiv:2209.03416, 2022.
[TPSI21] Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. Representing numbers in nlp: a survey and a vision. arXiv preprint arXiv:2103.13136, 2021.
[TSM ${ }^{+}$20] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537-7547, 2020.
[vONR ${ }^{+}$22] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn incontext by gradient descent. In International Conference on Machine Learning, 2022.</p>
<p>[VSP ${ }^{+}$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.
[WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.
[WLS17] Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 845-854, 2017.
[WWZ ${ }^{+}$22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons in pre-trained transformer-based language models. arXiv preprint arXiv:2211.07349, 2022.
[ZL23] Zeyuan Allen Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023.</p>
<h1>Appendix</h1>
<p>Roadmap. In Appendix A, we introduce some formal definitions that used in our main content. In Appendix B, we show why we separate the Fourier components into the high-frequency part and the low-frequency part and why we choose $\tau$ to be 50 . In Appendix C, we show our observation generalizes to another format of dataset, another arithmetic task and other models. In Appendix D, we provide more evidence that shows the Fourier features in the model when computing addition. In Appendix E, we provide more evidence that shows the GPT-2-XL trained from scratch does not use Fourier feature to solve the addition task. In Appendix F, we give the details of our experimental settings.</p>
<h2>A Formal Definition of Transformer and Logits in Fourier Space</h2>
<p>We first introduce the formal definition of the Transformer structure that we used in this paper.
Definition A. 1 (Transformer). An autoregressive Transformer language model $G: \mathcal{X} \rightarrow \mathcal{Y}$ over vocabulary Vocab maps a token sequence $x=\left[x_{1}, \ldots, x_{N}\right] \in \mathcal{X}, x_{t} \in$ Vocab to a probability distribution $y \in \mathcal{Y} \subset \mathbb{R}^{|\text {Vocab }|}$ that predicts next-token continuations of $x$. Within the Transformer, the $i$-th token is embedded as a series of hidden state vectors $h_{t}^{(\ell)}$, beginning with $h_{t}^{(0)}=\mathrm{emb}\left(x_{t}\right)+\operatorname{pos}(i) \in \mathbb{R}^{D}$. Let $W^{U} \in \mathbb{R}^{|\operatorname{Vocab}| \times D}$ denote the output embedding. The final output $y=\operatorname{softmax}\left(W^{U}\left(h_{N}^{(L)}\right)\right)$ is read from the last hidden state. In the autoregressive case, tokens only draw information from past tokens:</p>
<p>$$
h_{t}^{(\ell)}=h_{t}^{(\ell-1)}+\operatorname{Attn}<em t="t">{t}^{(\ell)}+\operatorname{MLP}</em>
$$}^{(\ell)</p>
<p>where</p>
<p>$$
\operatorname{Attn}<em 1="1">{t}^{(\ell)}:=\operatorname{Attn}^{(\ell)}\left(h</em>}^{(\ell-1)}, h_{2}^{(\ell-1)}, \ldots, h_{t}^{(\ell-1)}\right) \quad \text { and } \quad \operatorname{MLP<em t="t">{t}^{(\ell)}:=\operatorname{MLP}</em>}^{(\ell)}\left(\operatorname{Attn<em t="t">{t}^{(\ell)}, h</em>\right)
$$}^{(\ell-1)</p>
<p>In this paper, we only consider the output tokens to be numbers. Hence, we have the unembedding matrix $W^{U} \in \mathbb{R}^{p \times D}$, where $p$ is the size of the number space. As we are given the length- $N$ input sequences and predict the $(N+1)$-th, we only consider $h_{N}^{(\ell)}=h_{N}^{(\ell-1)}+\operatorname{Attn}<em N="N">{N}^{(\ell)}+\operatorname{MLP}</em>$. For simplicity, we ignore the subscript $N$ in the following paper, so we get Eq. (1).}^{(\ell)</p>
<p>Definition A. 2 (Intermediate Logits). Let $\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\text {Attn }}^{(\ell)}:=W^{U} \operatorname{Attn}^{(\ell)}$ denote the intermediate logits of the attention module at the $\ell$-th layer. Let $\mathcal{L}</em>$.}}^{(\ell)}:=W^{U} \mathrm{MLP}^{(\ell)}$ denote the intermediate logits of the MLP module at the $\ell$-th layer. Let $\mathcal{L}^{(\ell)}:=W^{U} h^{(\ell)}$ denote the logits on intermediate state $h^{(\ell)</p>
<p>Throughout the model, $h$ undergoes only additive updates (Eq. (1)), creating a continuous residual stream $\left[\mathrm{ENO}^{+} 21\right]$, meaning that the token representation $h$ accumulates all additive updates within the residual stream up to layer $t$.</p>
<p>To analyze the logits in Fourier space, we give the formal definition of the Fourier basis as follows:</p>
<p>Definition A. 3 (Fourier Basis). Let $p$ denote the size of the number space. Let $\overrightarrow{\mathrm{x}}:=(0,1, \ldots,(p-1))$.</p>
<p>Let $\omega_{k}:=\frac{2 \pi k}{p-1}$. We denote the normalized Fourier basis $F$ as the $p \times p$ matrix:</p>
<p>$$
F:=\left[\begin{array}{c}
\sqrt{\frac{1}{p-1} \cdot \overrightarrow{\mathbf{1}}} \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{1} \overrightarrow{\mathbf{x}}\right) \
\sqrt{\frac{2}{p-1}} \cdot \sin \left(\omega_{2} \overrightarrow{\mathbf{x}}\right) \
\vdots \
\sqrt{\frac{2}{p-1}} \cdot \cos \left(\omega_{(p-1) / 2} \overrightarrow{\mathbf{x}}\right)
\end{array}\right] \in \mathbb{R}^{p \times p}
$$</p>
<p>The first component $F[0]$ is defined as a constant component. For $i \in[0, p-1], F[i]$ is defined as the $k$-th component in Fourier space, where $k=\left\lfloor\frac{i+1}{2}\right\rfloor$. The frequency of the $k$-th component is $f_{k}:=\frac{k}{p-1}$. The period of the $k$-th component is $T_{k}:=\frac{p-1}{k}$</p>
<p>We can compute the discrete Fourier transform under that Fourier basis as follows:
Remark A. 4 (Discrete Fourier transformer (DFT) and inverse DFT). We can transform any logits $u \in \mathbb{R}^{p}$ to Fourier space by computing $\widehat{u}=F \cdot u$. We can transform $\widehat{u}$ back to $u$ by $u=F^{\top} \cdot \widehat{u}$</p>
<p>Next, we define the logits in Fourier space.
Definition A. 5 (Logits in Fourier Space). Let $\mathcal{L}^{(L)}, \mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\mathcal{L}</em>$. The logits of the MLP and attention modules in Fourier space are defined as:}}^{(\ell)}$ denote the logits (Definition A.2). The output logits before softmax in Fourier space is defined as: $\widetilde{\mathcal{L}}^{(L)}=F \cdot \mathcal{L}^{(L)</p>
<p>$$
\tilde{\mathcal{L}}<em _mathrm_Attn="\mathrm{Attn">{\mathrm{Attn}}^{(\ell)}=F \cdot \mathcal{L}</em>}}^{(\ell)} \quad \text { and } \quad \tilde{\mathcal{L}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{MLP}}^{(\ell)}=F \cdot \mathcal{L}</em>
$$}}^{(\ell)</p>
<p>We ignore the first elements in $\widetilde{\mathcal{L}}^{(L)}, \widetilde{\mathcal{L}}<em _mathrm_MLP="\mathrm{MLP">{\mathrm{Attn}}^{(\ell)}$ and $\widetilde{\mathcal{L}}</em>$ for the Fourier analysis in this paper as they are the constant terms. Adding a constant to the logits will not change the prediction.}}^{(\ell)</p>
<p>Let $\tau \in \mathbb{R}$ denote a constant threshold. The low-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[1: 2 \tau]$. The high-frequency components for the logits in Fourier space are defined as $\widetilde{\mathcal{L}}^{(\ell)}[2 \tau:]$. For the following analysis, we choose $\tau=50$ (the specific choice of $\tau=50$ is explained in Appendix B).</p>
<p>Next, we propose the formal definition of low-pass/high-pass filter that is used in the following ablation study.</p>
<p>Definition A. 6 (Loss-pass / High-pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\tau \in R$ denote the frequency threshold. Let $W^{U} \in R^{p \times D}$ denote the output embedding. For low-pass filter, we define a diagonal binary matrix $B \in$ ${0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } i \geq \tau \ 0 &amp; \text { otherwise }\end{cases}$. For high-pass filter, we define a diagonal binary matrix $B \in{0,1}^{p \times p}$ as $b_{i i}= \begin{cases}1 &amp; \text { if } 1 \leq i&lt;\tau \ 0 &amp; \text { otherwise }\end{cases}$. Note that we retain the constant component, so $b_{i, i}=0$. The output of the filter $\mathcal{F}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined by the following objective function:</p>
<p>$$
\begin{array}{cl}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{array}
$$}^{2</p>
<p>The solution to the above optimization problem is given by a linear projection.
Remark A.7. The result of the optimization problem defined in Definition A. 6 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>B Fourier Components Separation and Selection of $\tau$
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: We analyzed the logits in Fourier space for all the test data across the last 15 layers. For both the MLP and attention modules. We only plot the first 50 Fourier components (a) The MLP exhibits some outlier low-frequency Fourier components. (b) The attention module's lowfrequency Fourier components are not as obvious as the ones in MLP.</p>
<p>Following Definition A.6, we define single-pass filter as follows:
Definition B. 1 (Single-Pass Filter). Let $x \in \mathbb{R}^{D}$ denote the output of MLP or attention modules. Let $F$ denote the Fourier Basis (Definition A.3). Let $\gamma \in R$ denote the $\gamma$-th Fourier component (Definition A.3) that we want to retain. Let $W^{U} \in R^{V \times D}$ denote the output embedding. We define a diagonal binary matrix $B \in{0,1}^{V \times V}$ as $b_{i i}= \begin{cases}0 &amp; \text { if }\left\lfloor\frac{i+1}{2}\right\rfloor=\gamma \text { or } i=0, \ 1 &amp; \text { otherwise. }\end{cases}$
The output of the filter $\mathcal{F}_{\gamma}(x): \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$ is defined as the following objective function:</p>
<p>$$
\begin{aligned}
\min <em 2="2">{y} &amp; |x-y|</em> \
\text { subject to } &amp; B F W^{U} y=0
\end{aligned}
$$}^{2</p>
<p>Remark B.2. The result of the optimization problem defined in Definition B. 1 is the projection of $x$ to the null space of $B F W^{U}$. Let $\mathcal{N}\left(B F W^{U}\right)$ denote the null space of $B F W^{U}$. We have</p>
<p>$$
\mathcal{F}_{\gamma}(x)=\mathcal{N}\left(B F W^{U}\right) \cdot \mathcal{N}\left(B F W^{U}\right)^{\top} \cdot x^{\top}
$$</p>
<p>For the single-pass filter, we only retrain one Fourier component and analyze how this component affects the model's prediction. The residual stream is then updated as follows:</p>
<p>$$
h^{(\ell)}=h^{(\ell-1)}+\mathcal{F}<em _gamma="\gamma">{\gamma}\left(\operatorname{Attn}^{(\ell-1)}\right)+\mathcal{F}</em>\right)
$$}\left(\operatorname{MLP}^{(\ell-1)</p>
<p>We evaluated the fine-tuned GPT-2-XL model on the addition dataset with the Fourier components period 520 and 2. Given that $T_{k}:=\frac{V-1}{k}$ (Definition A.3), we retained only the Fourier components with $\gamma=1$ and 260 , respectively.</p>            </div>
        </div>

    </div>
</body>
</html>
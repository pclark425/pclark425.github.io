<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2074 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2074</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2074</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-281681196</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.26603v1.pdf" target="_blank">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></p>
                <p><strong>Paper Abstract:</strong> While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of"hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2074.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2074.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepScientist (autonomous, goal-oriented scientific discovery system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent, LLM-driven autonomous discovery system that frames scientific discovery as a Bayesian Optimization problem and runs a hierarchical three-stage loop (Strategize & Hypothesize -> Implement & Verify -> Analyze & Report) using a Findings Memory to generate, filter, validate, and publish novel scientific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent system centered on large language models (LLM-based agent system)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automated scientific discovery applied to machine learning / AI research tasks (agent failure attribution, LLM inference acceleration, AI text detection)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>scientific hypotheses, algorithmic methods, experimental results, and full research papers</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>highly novel for targeted frontier AI tasks (produced SOTA-surpassing methods and novel conceptual shifts such as treating text as a non-stationary signal); many outputs are claimed to be out-of-distribution relative to prior SOTA</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven ideation contextualized by a Findings Memory (retrieval of Top-K prior findings), generating candidate hypothesis set P_new; generation is framed inside a Bayesian Optimization loop using a surrogate LLM reviewer to score candidates and an acquisition function (UCB) to trade off exploitation/exploration</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>hierarchical validation: (1) low-cost surrogate evaluation by an LLM Reviewer (valuation vector ⟨v_u,v_q,v_e⟩), (2) selection via Upper Confidence Bound acquisition and sandboxed Implement & Verify where a code-execution agent runs experiments (with automated re-execution for secondary verification), and (3) Analyze & Report stage with deeper analytical experiments (ablations, new datasets) and synthesis into papers; all experimental results manually inspected by human supervisors</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Generated ≈5,000 unique ideas over month-scale runs; selected ≈1,100 for experimental validation; produced 21 'Progress Findings' (SOTA-surpassing results). Generation per-idea API cost ≈ $5. Throughput: system-level claim that two weeks of DeepScientist progress equaled ~3 years of human research for one task (RAID detector timeline).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation yielded SOTA-surpassing results on three tasks with metrics: +183.7% (accuracy) on Agent Failure Attribution (A2P method; A2P scored 29.31 and 47.46 on Who&When handcraft and algorithm-generated settings), +1.9% tokens/sec (LLM inference acceleration, ACRA improved MBPP throughput from 190.25 → 193.90 tokens/s), and +7.9% AUROC (AI Text Detection: T-Detect → TDT → PA-Detect progression). Overall success rate (Progress Findings / Implemented) ≈ 1–3%; absolute success (Progress Findings / Generated) ≈ 21/5000 ≈ 0.42%.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Reported operational artefact: ~50% of initial implementation attempts were observed to 'fail to complete' due to internal timeouts in the code agent (i.e., agent-reported completion that did not actually finish), prompting a secondary re-execution step to counteract these false positives. No single end-to-end false-positive rate for scientific claims is provided, but many candidate ideas advanced to Implement stage did not yield validated improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported explicitly. The paper does not provide numeric false-negative rates for the surrogate classifier or selection mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validity/validation performance degrades as novelty increases: the system produces many novel ideas but the validated success rate is very low (<3% among implemented trials); implementation errors account for ~60% of failures and the remainder typically show no improvement or regression, indicating validation is much harder for more novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper explicitly documents a large asymmetry: generation capability is high (thousands of novel ideas rapidly produced), while validation capacity is the bottleneck (few validated breakthroughs, heavy computational cost per validation). Example: 5,000 ideas generated → 1,100 implemented → 21 progress results; naive full testing of all 5,000 would require >100,000 GPU hours vs the ~20,000 GPU hours actually used.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — the surrogate LLM Reviewer produces a structured valuation vector V = ⟨v_u, v_q, v_e⟩ with integer scores 0–100, where v_e encodes exploration/uncertainty value; the acquisition function (UCB) uses v_e and κ to trade off exploration vs exploitation. No probabilistic calibration metrics reported beyond these scores.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified in calibration metrics; no ECE/Brier/etc reported. The surrogate provides 0–100 scores but the paper does not evaluate how well those scores are calibrated against true success probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Task-specific examples: produced methods that outperformed human SOTA (e.g., A2P, ACRA, PA-Detect). There is no systematic OOD benchmark reported; the evidence is limited to targeted frontier tasks where the system discovered conceptual innovations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — surrogate and filtering rely on proxy scores: utility (v_u), quality (v_q), exploration value (v_e). Additional proxies: DeepReviewer paper-quality scores, reviewer ratings, and standard task metrics used in final verification (accuracy, AUROC, tokens/sec).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts manually inspected all experimental results; three human experts supervised the process and a program committee reviewed final papers — effectively human validation applied to every implemented experiment and all final reported findings.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (machine learning / AI experimental research); semi-formal rather than formally axiomatized domains, which contributes to a substantial generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Implemented strategies: (a) Findings Memory to avoid redundant tests, (b) surrogate LLM Reviewer scoring (v_u,v_q,v_e) to pre-filter ideas, (c) UCB acquisition to balance exploration/exploitation, (d) sandboxed Claude Code execution plus a secondary re-execution verification to reduce agent completion false-positives, (e) human supervision and program-committee review. Effectiveness: selection strategy raised success rate from ~0% (random) to ≈1–3%; re-execution countered ≈50% agent timeouts (operational false positives).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Quantitative evidence: ≈5,000 generated ideas vs ≈1,100 implemented vs 21 progress results; selection success ≈1–3%; implementation errors cause ~60% of failures; ~50% of agent runs initially timed out (operational false positives); naive full testing projected >100,000 GPU hours vs actual 20,000 GPU hours. The authors repeatedly state validation is the bottleneck and that generation far outpaces effective validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Counter-evidence: the system demonstrably validated multiple novel SOTA-surpassing methods across three tasks and produced papers that received program-committee-level review scores comparable to ICLR submissions and a 60% acceptance rate under automated DeepReviewer evaluation. These successes show validation can succeed, but they are rare relative to generation volume.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Generation cost per idea (strategize stage): ≈ $5 in API calls. Implement & Verify attempt: ≈ $20 in Claude-4-opus API usage + ≈1 GPU-hour of compute (H800 average execution ~70 minutes ≈ 1e16 FLOPs). Analyze & Report for a successful finding: ≈ $150 (≈$100 for analytical experiments + $50 for report generation). Thus validation (implement + analysis) costs are substantially higher than generation (≈4× API cost plus heavy GPU compute), and total wall-clock GPU-hours dominated by validation (authors report ≈20,000 GPU hours consumed overall; naive full evaluation of all candidates would have required >100,000 GPU hours).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2074.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2074.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surrogate LLM Reviewer (g_t)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surrogate LLM Reviewer (denoted g_t) used as the low-cost surrogate approximating the true value function f</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based surrogate evaluator that is contextualized with the Findings Memory and assigns each candidate hypothesis a valuation vector V = ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) scored 0–100 to approximate f and guide selection via an acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Surrogate LLM Reviewer (g_t)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as a surrogate evaluator / ranking model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>used inside automated scientific discovery (ML/AI research tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>valuation vectors / plausibility and exploration scores for candidate hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not an output generator in the creative sense; novelty is in estimating novelty/exploration value (v_e); surrogate itself is an engineered component, novelty moderate</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>scores candidates by conditioning on Findings Memory (retrieved Top-K) and applying internal heuristics to output integer scores for utility/quality/exploration</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>serves as low-cost validation proxy; its outputs are used to rank/select ideas which are subsequently validated by real experiments (Implement & Verify) and human inspection; surrogate is not treated as authoritative validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (does not generate domain artifacts); runtime/throughput not reported numerically beyond being low-cost relative to full experiments</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>No calibration or predictive-performance metrics reported (e.g., precision/recall of surrogate predictions vs actual experimental success); empirical effect: using the surrogate + UCB selection increases implemented-to-progress success to ≈1–3% compared to ~0% when selecting randomly.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported; surrogate induces some selection errors (both false positives and false negatives) but specific rates are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not directly quantified; surrogate contains an explicit exploration score (v_e) intended to encourage selection of uncertain/novel candidates, but no measured degradation curve versus candidate novelty provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Serves to narrow the gap by pre-filtering but is insufficient — selection reduces wasted experiments but validated success remains low; authors note need for better surrogate models to improve filtering accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes: exploration component v_e is an explicit proxy for uncertainty/novelty and is used inside UCB with coefficient κ to drive exploration. No formal probabilistic uncertainty estimates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; no calibration metrics for v_u/v_q/v_e vs true outcomes provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported; surrogate's capability on OOD (very novel) ideas not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses v_u (utility), v_q (quality), v_e (exploration/uncertainty) as proxy metrics instead of running expensive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human supervisors review outputs and filter hallucinations; humans also inspect experimental outcomes after surrogate-driven selection and implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (ML experiments); surrogate operates on unformalized scientific-method outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Designed as an explicit mitigation to reduce expensive validations by predicting candidate promise; used in combination with UCB acquisition to allocate experiments efficiently. Paper reports this strategy substantially reduces wasted experiments versus random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors show surrogate + selection raises success from ≈0% (random) to ≈1–3%, indicating surrogate helps but is far from perfect, supporting existence of generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No direct contradictory evidence; surrogate did enable all validated discoveries reported but without precise performance numbers it's partial evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Surrogate evaluation is 'low-cost' (LLM API calls during strategize stage; per-idea generation ≈ $5 includes such operations) versus orders-of-magnitude higher cost for full implementation (≈1 GPU-hour per Implement attempt). Exact compute FLOPs for surrogate not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2074.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2074.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.5-Promodel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.5-Promodel (LLM used as core logical/reasoning engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation LLM used as the core logic engine of DeepScientist to conduct highlevel planning, hypothesis generation, reasoning across the three-stage loop, and to drive agent orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gemini-2.5-Promodel</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>used for automated scientific ideation and agent orchestration within ML/AI research domains</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>long-form hypotheses, plans, and research-level textual artifacts (ideas and paper outlines)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>enables generation of highly novel ideas when combined with Findings Memory and exploration; model-specific novelty characterization not provided</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-conditioned decoding (context includes Findings Memory and retrieved records) to produce new candidate hypotheses and plans</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Outputs from Gemini are passed to the surrogate reviewer and downstream Implement & Verify stages; Gemini itself is not used as final validator</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>No raw model metrics provided; system-level outputs (5k ideas etc.) reflect Gemini's role in ideation but no per-token throughput or generation quality metrics provided for Gemini specifically</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not applicable directly; its generated outputs are subject to the pipeline's validation where only a small portion becomes validated progress</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported for Gemini separately; hallucination filtering performed by human supervisors and surrogate reviewers</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified per-model; the paper attributes much of the creative ideation success to LLM capability plus the Findings Memory and selection mechanisms rather than any single model calibration metric</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Gemini can produce many novel hypotheses quickly, but the system-level validation bottleneck still applies — the paper emphasizes generative capacity > validated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not directly — uncertainty is represented in the system via surrogate scores rather than by Gemini producing calibrated probabilities for downstream success.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not systematically evaluated for Gemini; successful frontier discoveries indicate Gemini-powered ideation can propose OOD ideas that sometimes validate.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Gemini outputs are scored by surrogate proxies (v_u/v_q/v_e) rather than being directly validated by Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts supervise all pipeline operations including outputs from Gemini; final experimental outputs manually inspected.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical ML research</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Gemini is used in conjunction with Findings Memory, surrogate reviewer, and UCB selection to try to restrict expensive validations to promising outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>No per-model evidence, but Gemini-powered ideation contributed to the 5k ideas vs 21 successes phenomenon described at system level.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None reported specific to Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Gemini usage cost included in the per-idea API $5 budget (generation-phase costs) — small relative to GPU compute for validations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2074.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2074.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-4-Opus / Claude Code agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-4-Opus and the Claude Code agent (used for code generation and executing Implement & Verify experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-generation/execution agent (Claude Code using Claude-4-Opus) that implements candidate hypotheses by modifying sandboxed copies of baseline repositories and running experiments; used as the primary executor in the Implement & Verify stage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude Code (powered by Claude-4-Opus)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based code-generation and code-execution agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>experimental implementation and verification for ML/AI research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>modified code, experiment logs, and experimental results (metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>operational/engineering agent rather than ideation; novelty in autonomous implementation of experiments is moderate</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>repository-level code understanding + program synthesis (planning -> read code -> implement changes -> run experiments) in a sandboxed folder with internet access for literature/code search</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Runs experiments and produces logs and results; DeepScientist performs secondary re-execution of main scripts to detect incomplete runs; human supervisors manually inspect results; sandboxing and replication applied for Analyze & Report experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Each Implement & Verify attempt averaged ≈1 GPU-hour of compute on an H800 server (average execution ~70 minutes) plus ≈ $20 API usage for Claude-4-opus. Many implementations run quickly but a substantial fraction timed out/failed internally.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Produced experiment outputs used to update Findings Memory, but a large fraction of implementations did not yield validated improvements. Implementation-level failure accounted for ~60% of failed trials (human expert analysis on sample of 300 failed implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Operational false-positive artifact: approximately 50% of initial implementation attempts failed to complete fully due to internal timeouts within the Claude Code agent (agent reported completion but run did not finish), leading to a high observed false-positive rate for 'agent reports success' which was mitigated by secondary re-execution.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically; no explicit count of cases where Claude Code failed to report a valid positive result that was actually valid, though implementation errors are a major failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Implementation errors and fragility increased the failure rate for more novel or complex modifications; authors report many ideas terminated prematurely by implementation faults rather than conceptually failing, indicating an implementation-fidelity degradation with complexity/novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Claude Code can rapidly implement many ideas, but its reliability is a bottleneck: high rate of timeouts and implementation mistakes dramatically reduces validated outcomes relative to generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Agent internal uncertainty metrics not reported. System-level uses re-execution and human supervision rather than agent self-confidence for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; observed mismatch between 'agent reported completion' and actual experiment completion indicates poor operational reliability absent external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified, but authors note many failures driven by implementation errors for novel ideas; agent reliability on highly novel code changes appears limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Agent reports experiment logs and results; system uses secondary re-execution and human inspection as stronger verification proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>All experimental results manually inspected by human supervisors; secondary verification (re-execution) used on every reported completion to address timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / engineering (code execution and experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Sandboxed execution folders, replication of baseline repository, secondary re-execution of main scripts to detect and remove 'false positive' agent completions, human manual inspection of results. These measures reduced but did not eliminate implementation failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>High operational failure rates: ~50% of initial agent runs timed out; human analysis found ~60% of failed trials were due to implementation errors — both illustrate that code-execution/validation capability lags behind generative ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Despite these issues, Claude Code executed many experiments that yielded the reported validated discoveries (21 progress results); so while error-prone, it is still capable of successful implementation when robust.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Each Implement attempt: ≈ $20 API + ≈1 GPU-hour compute (H800 ~70 minutes). Implement & Verify stage thus imposes much larger compute cost than generation-stage $5 per idea, making validation computationally dominant.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2074.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2074.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer / DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer (DeepReviewer-14B variant used for automated peer-reviewing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based automated reviewer used to simulate human peer-review (external search capability) for evaluating AI-generated papers and for benchmarking DeepScientist outputs against other AI Scientist publications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepReviewer (14B variant for automatic review)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as an automated peer reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>meta-evaluation of AI-generated research papers (NLP/ML domain)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>review scores and qualitative assessments (Soundness, Presentation, Contribution, overall Rating)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>tooling/analysis; not producing domain scientific findings, novelty moderate</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based critique with external search and predefined review rubric; outputs numerical scores and textual feedback</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as an automated proxy for human review; compared system outputs to other AI-Scientist papers; final human program committee also performed independent review</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable as a generator of scientific claims; used to score five DeepScientist-generated papers and 28 public AI-Scientist papers; DeepScientist papers achieved a 60% acceptance rate under DeepReviewer evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>DeepReviewer assigned scores that recognize novelty/value; DeepScientist's five generated papers averaged Rating 5.00 (variance reported), comparable to ICLR 2025 average 5.08. Interpreting these as validation signals is proxy-based (not experimental verification).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not provided; as an automated reviewer it may produce optimistic/overconfident accept signals but no numeric false-positive/negative rates are given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>DeepReviewer was used to recognize novelty in generated papers, but authors note human review identified weaknesses in experimental rigor — suggesting automated reviewer may not fully capture validation rigor for novel claims.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>DeepReviewer is a proxy-stage validator for paper quality but does not replace experimental validation; the paper uses DeepReviewer for benchmarking against other AI-Scientist outputs but still relies on human program-committee review and manual experimental inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported (no calibration metrics for reviewer confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not evaluated systematically.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — reviewer rubric (Soundness, Presentation, Contribution, overall Rating) used as proxy for paper quality/validity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used in tandem with a human program committee; all final papers additionally reviewed by three human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (peer review of ML/AI experimental research)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Used automated review to filter and benchmark generated papers before/alongside human review; however, authors emphasize human review remains essential to catch issues in experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Automated review (DeepReviewer) awarded high novelty/quality signals (60% acceptance) but human reviewers found systematic empirical weaknesses in many papers (lack of thorough validation), showing automated review can overestimate the rigor of novel claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>DeepReviewer did correctly identify novelty and gave scores comparable to human-reviewed benchmarks, indicating automated review can be a useful filter when combined with human inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not quantified; DeepReviewer use is described as part of evaluation pipeline but per-call costs and compute not enumerated relative to generation/validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search <em>(Rating: 2)</em></li>
                <li>AlphaEvolve: A coding agent for scientific and algorithmic discovery <em>(Rating: 1)</em></li>
                <li>Combining data and theory for derivable scientific discovery with AI-Descartes <em>(Rating: 2)</em></li>
                <li>Evolving scientific discovery by unifying data and background knowledge with AI-Hilbert <em>(Rating: 2)</em></li>
                <li>Agent Laboratory: Using LLM agents as research assistants <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2074",
    "paper_id": "paper-281681196",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "DeepScientist",
            "name_full": "DeepScientist (autonomous, goal-oriented scientific discovery system)",
            "brief_description": "A multi-agent, LLM-driven autonomous discovery system that frames scientific discovery as a Bayesian Optimization problem and runs a hierarchical three-stage loop (Strategize & Hypothesize -&gt; Implement & Verify -&gt; Analyze & Report) using a Findings Memory to generate, filter, validate, and publish novel scientific methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_type": "multi-agent system centered on large language models (LLM-based agent system)",
            "scientific_domain": "automated scientific discovery applied to machine learning / AI research tasks (agent failure attribution, LLM inference acceleration, AI text detection)",
            "output_type": "scientific hypotheses, algorithmic methods, experimental results, and full research papers",
            "novelty_level": "highly novel for targeted frontier AI tasks (produced SOTA-surpassing methods and novel conceptual shifts such as treating text as a non-stationary signal); many outputs are claimed to be out-of-distribution relative to prior SOTA",
            "generation_method": "LLM-driven ideation contextualized by a Findings Memory (retrieval of Top-K prior findings), generating candidate hypothesis set P_new; generation is framed inside a Bayesian Optimization loop using a surrogate LLM reviewer to score candidates and an acquisition function (UCB) to trade off exploitation/exploration",
            "validation_method": "hierarchical validation: (1) low-cost surrogate evaluation by an LLM Reviewer (valuation vector ⟨v_u,v_q,v_e⟩), (2) selection via Upper Confidence Bound acquisition and sandboxed Implement & Verify where a code-execution agent runs experiments (with automated re-execution for secondary verification), and (3) Analyze & Report stage with deeper analytical experiments (ablations, new datasets) and synthesis into papers; all experimental results manually inspected by human supervisors",
            "generation_performance": "Generated ≈5,000 unique ideas over month-scale runs; selected ≈1,100 for experimental validation; produced 21 'Progress Findings' (SOTA-surpassing results). Generation per-idea API cost ≈ $5. Throughput: system-level claim that two weeks of DeepScientist progress equaled ~3 years of human research for one task (RAID detector timeline).",
            "validation_performance": "Validation yielded SOTA-surpassing results on three tasks with metrics: +183.7% (accuracy) on Agent Failure Attribution (A2P method; A2P scored 29.31 and 47.46 on Who&When handcraft and algorithm-generated settings), +1.9% tokens/sec (LLM inference acceleration, ACRA improved MBPP throughput from 190.25 → 193.90 tokens/s), and +7.9% AUROC (AI Text Detection: T-Detect → TDT → PA-Detect progression). Overall success rate (Progress Findings / Implemented) ≈ 1–3%; absolute success (Progress Findings / Generated) ≈ 21/5000 ≈ 0.42%.",
            "false_positive_rate": "Reported operational artefact: ~50% of initial implementation attempts were observed to 'fail to complete' due to internal timeouts in the code agent (i.e., agent-reported completion that did not actually finish), prompting a secondary re-execution step to counteract these false positives. No single end-to-end false-positive rate for scientific claims is provided, but many candidate ideas advanced to Implement stage did not yield validated improvements.",
            "false_negative_rate": "Not reported explicitly. The paper does not provide numeric false-negative rates for the surrogate classifier or selection mechanism.",
            "performance_vs_novelty": "Validity/validation performance degrades as novelty increases: the system produces many novel ideas but the validated success rate is very low (&lt;3% among implemented trials); implementation errors account for ~60% of failures and the remainder typically show no improvement or regression, indicating validation is much harder for more novel outputs.",
            "generation_validation_comparison": "Paper explicitly documents a large asymmetry: generation capability is high (thousands of novel ideas rapidly produced), while validation capacity is the bottleneck (few validated breakthroughs, heavy computational cost per validation). Example: 5,000 ideas generated → 1,100 implemented → 21 progress results; naive full testing of all 5,000 would require &gt;100,000 GPU hours vs the ~20,000 GPU hours actually used.",
            "uncertainty_quantification": "Yes — the surrogate LLM Reviewer produces a structured valuation vector V = ⟨v_u, v_q, v_e⟩ with integer scores 0–100, where v_e encodes exploration/uncertainty value; the acquisition function (UCB) uses v_e and κ to trade off exploration vs exploitation. No probabilistic calibration metrics reported beyond these scores.",
            "calibration_quality": "Not quantified in calibration metrics; no ECE/Brier/etc reported. The surrogate provides 0–100 scores but the paper does not evaluate how well those scores are calibrated against true success probabilities.",
            "out_of_distribution_performance": "Task-specific examples: produced methods that outperformed human SOTA (e.g., A2P, ACRA, PA-Detect). There is no systematic OOD benchmark reported; the evidence is limited to targeted frontier tasks where the system discovered conceptual innovations.",
            "validation_proxy_metrics": "Yes — surrogate and filtering rely on proxy scores: utility (v_u), quality (v_q), exploration value (v_e). Additional proxies: DeepReviewer paper-quality scores, reviewer ratings, and standard task metrics used in final verification (accuracy, AUROC, tokens/sec).",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts manually inspected all experimental results; three human experts supervised the process and a program committee reviewed final papers — effectively human validation applied to every implemented experiment and all final reported findings.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (machine learning / AI experimental research); semi-formal rather than formally axiomatized domains, which contributes to a substantial generation-validation gap.",
            "gap_mitigation_strategies": "Implemented strategies: (a) Findings Memory to avoid redundant tests, (b) surrogate LLM Reviewer scoring (v_u,v_q,v_e) to pre-filter ideas, (c) UCB acquisition to balance exploration/exploitation, (d) sandboxed Claude Code execution plus a secondary re-execution verification to reduce agent completion false-positives, (e) human supervision and program-committee review. Effectiveness: selection strategy raised success rate from ~0% (random) to ≈1–3%; re-execution countered ≈50% agent timeouts (operational false positives).",
            "evidence_supporting_gap": "Quantitative evidence: ≈5,000 generated ideas vs ≈1,100 implemented vs 21 progress results; selection success ≈1–3%; implementation errors cause ~60% of failures; ~50% of agent runs initially timed out (operational false positives); naive full testing projected &gt;100,000 GPU hours vs actual 20,000 GPU hours. The authors repeatedly state validation is the bottleneck and that generation far outpaces effective validation.",
            "evidence_contradicting_gap": "Counter-evidence: the system demonstrably validated multiple novel SOTA-surpassing methods across three tasks and produced papers that received program-committee-level review scores comparable to ICLR submissions and a 60% acceptance rate under automated DeepReviewer evaluation. These successes show validation can succeed, but they are rare relative to generation volume.",
            "computational_cost_ratio": "Generation cost per idea (strategize stage): ≈ $5 in API calls. Implement & Verify attempt: ≈ $20 in Claude-4-opus API usage + ≈1 GPU-hour of compute (H800 average execution ~70 minutes ≈ 1e16 FLOPs). Analyze & Report for a successful finding: ≈ $150 (≈$100 for analytical experiments + $50 for report generation). Thus validation (implement + analysis) costs are substantially higher than generation (≈4× API cost plus heavy GPU compute), and total wall-clock GPU-hours dominated by validation (authors report ≈20,000 GPU hours consumed overall; naive full evaluation of all candidates would have required &gt;100,000 GPU hours).",
            "uuid": "e2074.0"
        },
        {
            "name_short": "Surrogate LLM Reviewer (g_t)",
            "name_full": "Surrogate LLM Reviewer (denoted g_t) used as the low-cost surrogate approximating the true value function f",
            "brief_description": "An LLM-based surrogate evaluator that is contextualized with the Findings Memory and assigns each candidate hypothesis a valuation vector V = ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) scored 0–100 to approximate f and guide selection via an acquisition function.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Surrogate LLM Reviewer (g_t)",
            "system_type": "large language model used as a surrogate evaluator / ranking model",
            "scientific_domain": "used inside automated scientific discovery (ML/AI research tasks)",
            "output_type": "valuation vectors / plausibility and exploration scores for candidate hypotheses",
            "novelty_level": "not an output generator in the creative sense; novelty is in estimating novelty/exploration value (v_e); surrogate itself is an engineered component, novelty moderate",
            "generation_method": "scores candidates by conditioning on Findings Memory (retrieved Top-K) and applying internal heuristics to output integer scores for utility/quality/exploration",
            "validation_method": "serves as low-cost validation proxy; its outputs are used to rank/select ideas which are subsequently validated by real experiments (Implement & Verify) and human inspection; surrogate is not treated as authoritative validation",
            "generation_performance": "Not applicable (does not generate domain artifacts); runtime/throughput not reported numerically beyond being low-cost relative to full experiments",
            "validation_performance": "No calibration or predictive-performance metrics reported (e.g., precision/recall of surrogate predictions vs actual experimental success); empirical effect: using the surrogate + UCB selection increases implemented-to-progress success to ≈1–3% compared to ~0% when selecting randomly.",
            "false_positive_rate": "Not numerically reported; surrogate induces some selection errors (both false positives and false negatives) but specific rates are not given.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Not directly quantified; surrogate contains an explicit exploration score (v_e) intended to encourage selection of uncertain/novel candidates, but no measured degradation curve versus candidate novelty provided.",
            "generation_validation_comparison": "Serves to narrow the gap by pre-filtering but is insufficient — selection reduces wasted experiments but validated success remains low; authors note need for better surrogate models to improve filtering accuracy.",
            "uncertainty_quantification": "Yes: exploration component v_e is an explicit proxy for uncertainty/novelty and is used inside UCB with coefficient κ to drive exploration. No formal probabilistic uncertainty estimates reported.",
            "calibration_quality": "Not reported; no calibration metrics for v_u/v_q/v_e vs true outcomes provided.",
            "out_of_distribution_performance": "Not reported; surrogate's capability on OOD (very novel) ideas not quantified.",
            "validation_proxy_metrics": "Yes — uses v_u (utility), v_q (quality), v_e (exploration/uncertainty) as proxy metrics instead of running expensive experiments.",
            "human_validation_required": true,
            "human_validation_frequency": "Human supervisors review outputs and filter hallucinations; humans also inspect experimental outcomes after surrogate-driven selection and implementation.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (ML experiments); surrogate operates on unformalized scientific-method outputs",
            "gap_mitigation_strategies": "Designed as an explicit mitigation to reduce expensive validations by predicting candidate promise; used in combination with UCB acquisition to allocate experiments efficiently. Paper reports this strategy substantially reduces wasted experiments versus random selection.",
            "evidence_supporting_gap": "Authors show surrogate + selection raises success from ≈0% (random) to ≈1–3%, indicating surrogate helps but is far from perfect, supporting existence of generation-validation gap.",
            "evidence_contradicting_gap": "No direct contradictory evidence; surrogate did enable all validated discoveries reported but without precise performance numbers it's partial evidence.",
            "computational_cost_ratio": "Surrogate evaluation is 'low-cost' (LLM API calls during strategize stage; per-idea generation ≈ $5 includes such operations) versus orders-of-magnitude higher cost for full implementation (≈1 GPU-hour per Implement attempt). Exact compute FLOPs for surrogate not reported.",
            "uuid": "e2074.1"
        },
        {
            "name_short": "Gemini-2.5-Promodel",
            "name_full": "Gemini-2.5-Promodel (LLM used as core logical/reasoning engine)",
            "brief_description": "A foundation LLM used as the core logic engine of DeepScientist to conduct highlevel planning, hypothesis generation, reasoning across the three-stage loop, and to drive agent orchestration.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Gemini-2.5-Promodel",
            "system_type": "large language model (foundation model)",
            "scientific_domain": "used for automated scientific ideation and agent orchestration within ML/AI research domains",
            "output_type": "long-form hypotheses, plans, and research-level textual artifacts (ideas and paper outlines)",
            "novelty_level": "enables generation of highly novel ideas when combined with Findings Memory and exploration; model-specific novelty characterization not provided",
            "generation_method": "LLM-conditioned decoding (context includes Findings Memory and retrieved records) to produce new candidate hypotheses and plans",
            "validation_method": "Outputs from Gemini are passed to the surrogate reviewer and downstream Implement & Verify stages; Gemini itself is not used as final validator",
            "generation_performance": "No raw model metrics provided; system-level outputs (5k ideas etc.) reflect Gemini's role in ideation but no per-token throughput or generation quality metrics provided for Gemini specifically",
            "validation_performance": "Not applicable directly; its generated outputs are subject to the pipeline's validation where only a small portion becomes validated progress",
            "false_positive_rate": "Not reported for Gemini separately; hallucination filtering performed by human supervisors and surrogate reviewers",
            "false_negative_rate": "Not reported",
            "performance_vs_novelty": "Not quantified per-model; the paper attributes much of the creative ideation success to LLM capability plus the Findings Memory and selection mechanisms rather than any single model calibration metric",
            "generation_validation_comparison": "Gemini can produce many novel hypotheses quickly, but the system-level validation bottleneck still applies — the paper emphasizes generative capacity &gt; validated outputs.",
            "uncertainty_quantification": "Not directly — uncertainty is represented in the system via surrogate scores rather than by Gemini producing calibrated probabilities for downstream success.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not systematically evaluated for Gemini; successful frontier discoveries indicate Gemini-powered ideation can propose OOD ideas that sometimes validate.",
            "validation_proxy_metrics": "Gemini outputs are scored by surrogate proxies (v_u/v_q/v_e) rather than being directly validated by Gemini.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts supervise all pipeline operations including outputs from Gemini; final experimental outputs manually inspected.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical ML research",
            "gap_mitigation_strategies": "Gemini is used in conjunction with Findings Memory, surrogate reviewer, and UCB selection to try to restrict expensive validations to promising outputs.",
            "evidence_supporting_gap": "No per-model evidence, but Gemini-powered ideation contributed to the 5k ideas vs 21 successes phenomenon described at system level.",
            "evidence_contradicting_gap": "None reported specific to Gemini.",
            "computational_cost_ratio": "Gemini usage cost included in the per-idea API $5 budget (generation-phase costs) — small relative to GPU compute for validations.",
            "uuid": "e2074.2"
        },
        {
            "name_short": "Claude-4-Opus / Claude Code agent",
            "name_full": "Claude-4-Opus and the Claude Code agent (used for code generation and executing Implement & Verify experiments)",
            "brief_description": "A code-generation/execution agent (Claude Code using Claude-4-Opus) that implements candidate hypotheses by modifying sandboxed copies of baseline repositories and running experiments; used as the primary executor in the Implement & Verify stage.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude Code (powered by Claude-4-Opus)",
            "system_type": "LLM-based code-generation and code-execution agent",
            "scientific_domain": "experimental implementation and verification for ML/AI research tasks",
            "output_type": "modified code, experiment logs, and experimental results (metrics)",
            "novelty_level": "operational/engineering agent rather than ideation; novelty in autonomous implementation of experiments is moderate",
            "generation_method": "repository-level code understanding + program synthesis (planning -&gt; read code -&gt; implement changes -&gt; run experiments) in a sandboxed folder with internet access for literature/code search",
            "validation_method": "Runs experiments and produces logs and results; DeepScientist performs secondary re-execution of main scripts to detect incomplete runs; human supervisors manually inspect results; sandboxing and replication applied for Analyze & Report experiments",
            "generation_performance": "Each Implement & Verify attempt averaged ≈1 GPU-hour of compute on an H800 server (average execution ~70 minutes) plus ≈ $20 API usage for Claude-4-opus. Many implementations run quickly but a substantial fraction timed out/failed internally.",
            "validation_performance": "Produced experiment outputs used to update Findings Memory, but a large fraction of implementations did not yield validated improvements. Implementation-level failure accounted for ~60% of failed trials (human expert analysis on sample of 300 failed implementations).",
            "false_positive_rate": "Operational false-positive artifact: approximately 50% of initial implementation attempts failed to complete fully due to internal timeouts within the Claude Code agent (agent reported completion but run did not finish), leading to a high observed false-positive rate for 'agent reports success' which was mitigated by secondary re-execution.",
            "false_negative_rate": "Not reported numerically; no explicit count of cases where Claude Code failed to report a valid positive result that was actually valid, though implementation errors are a major failure mode.",
            "performance_vs_novelty": "Implementation errors and fragility increased the failure rate for more novel or complex modifications; authors report many ideas terminated prematurely by implementation faults rather than conceptually failing, indicating an implementation-fidelity degradation with complexity/novelty.",
            "generation_validation_comparison": "Claude Code can rapidly implement many ideas, but its reliability is a bottleneck: high rate of timeouts and implementation mistakes dramatically reduces validated outcomes relative to generated ideas.",
            "uncertainty_quantification": "Agent internal uncertainty metrics not reported. System-level uses re-execution and human supervision rather than agent self-confidence for verification.",
            "calibration_quality": "Not reported; observed mismatch between 'agent reported completion' and actual experiment completion indicates poor operational reliability absent external verification.",
            "out_of_distribution_performance": "Not quantified, but authors note many failures driven by implementation errors for novel ideas; agent reliability on highly novel code changes appears limited.",
            "validation_proxy_metrics": "Agent reports experiment logs and results; system uses secondary re-execution and human inspection as stronger verification proxies.",
            "human_validation_required": true,
            "human_validation_frequency": "All experimental results manually inspected by human supervisors; secondary verification (re-execution) used on every reported completion to address timeouts.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / engineering (code execution and experiments)",
            "gap_mitigation_strategies": "Sandboxed execution folders, replication of baseline repository, secondary re-execution of main scripts to detect and remove 'false positive' agent completions, human manual inspection of results. These measures reduced but did not eliminate implementation failure modes.",
            "evidence_supporting_gap": "High operational failure rates: ~50% of initial agent runs timed out; human analysis found ~60% of failed trials were due to implementation errors — both illustrate that code-execution/validation capability lags behind generative ideation.",
            "evidence_contradicting_gap": "Despite these issues, Claude Code executed many experiments that yielded the reported validated discoveries (21 progress results); so while error-prone, it is still capable of successful implementation when robust.",
            "computational_cost_ratio": "Each Implement attempt: ≈ $20 API + ≈1 GPU-hour compute (H800 ~70 minutes). Implement & Verify stage thus imposes much larger compute cost than generation-stage $5 per idea, making validation computationally dominant.",
            "uuid": "e2074.3"
        },
        {
            "name_short": "DeepReviewer / DeepReviewer-14B",
            "name_full": "DeepReviewer (DeepReviewer-14B variant used for automated peer-reviewing)",
            "brief_description": "An LLM-based automated reviewer used to simulate human peer-review (external search capability) for evaluating AI-generated papers and for benchmarking DeepScientist outputs against other AI Scientist publications.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DeepReviewer (14B variant for automatic review)",
            "system_type": "large language model used as an automated peer reviewer",
            "scientific_domain": "meta-evaluation of AI-generated research papers (NLP/ML domain)",
            "output_type": "review scores and qualitative assessments (Soundness, Presentation, Contribution, overall Rating)",
            "novelty_level": "tooling/analysis; not producing domain scientific findings, novelty moderate",
            "generation_method": "LLM-based critique with external search and predefined review rubric; outputs numerical scores and textual feedback",
            "validation_method": "Used as an automated proxy for human review; compared system outputs to other AI-Scientist papers; final human program committee also performed independent review",
            "generation_performance": "Not applicable as a generator of scientific claims; used to score five DeepScientist-generated papers and 28 public AI-Scientist papers; DeepScientist papers achieved a 60% acceptance rate under DeepReviewer evaluation",
            "validation_performance": "DeepReviewer assigned scores that recognize novelty/value; DeepScientist's five generated papers averaged Rating 5.00 (variance reported), comparable to ICLR 2025 average 5.08. Interpreting these as validation signals is proxy-based (not experimental verification).",
            "false_positive_rate": "Not provided; as an automated reviewer it may produce optimistic/overconfident accept signals but no numeric false-positive/negative rates are given.",
            "false_negative_rate": "Not provided.",
            "performance_vs_novelty": "DeepReviewer was used to recognize novelty in generated papers, but authors note human review identified weaknesses in experimental rigor — suggesting automated reviewer may not fully capture validation rigor for novel claims.",
            "generation_validation_comparison": "DeepReviewer is a proxy-stage validator for paper quality but does not replace experimental validation; the paper uses DeepReviewer for benchmarking against other AI-Scientist outputs but still relies on human program-committee review and manual experimental inspection.",
            "uncertainty_quantification": "Not reported (no calibration metrics for reviewer confidence).",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not evaluated systematically.",
            "validation_proxy_metrics": "Yes — reviewer rubric (Soundness, Presentation, Contribution, overall Rating) used as proxy for paper quality/validity.",
            "human_validation_required": true,
            "human_validation_frequency": "Used in tandem with a human program committee; all final papers additionally reviewed by three human experts.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (peer review of ML/AI experimental research)",
            "gap_mitigation_strategies": "Used automated review to filter and benchmark generated papers before/alongside human review; however, authors emphasize human review remains essential to catch issues in experimental design.",
            "evidence_supporting_gap": "Automated review (DeepReviewer) awarded high novelty/quality signals (60% acceptance) but human reviewers found systematic empirical weaknesses in many papers (lack of thorough validation), showing automated review can overestimate the rigor of novel claims.",
            "evidence_contradicting_gap": "DeepReviewer did correctly identify novelty and gave scores comparable to human-reviewed benchmarks, indicating automated review can be a useful filter when combined with human inspection.",
            "computational_cost_ratio": "Not quantified; DeepReviewer use is described as part of evaluation pipeline but per-call costs and compute not enumerated relative to generation/validation.",
            "uuid": "e2074.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "rating": 2
        },
        {
            "paper_title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
            "rating": 1
        },
        {
            "paper_title": "Combining data and theory for derivable scientific discovery with AI-Descartes",
            "rating": 2
        },
        {
            "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI-Hilbert",
            "rating": 2
        },
        {
            "paper_title": "Agent Laboratory: Using LLM agents as research assistants",
            "rating": 1
        }
    ],
    "cost": 0.022387749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1</p>
<p>Yixuan Weng wengsyx@gmail.com 
Minjun Zhu zhu.minjun@westlake.edu.cn 
Qiujie Xie 
Qiyao Sun 
Zhen Lin 
Sifan Liu 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Engineering School
Westlake University</p>
<p>5 10 15 061 0.65 0.70 0.75 0.80 0.85</p>
<p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1CA0C9325E7EF8170420249F9A6A1FE33arXiv:2509.26603v1[cs.CL]
While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges.We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines.It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze".Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation.Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7%, 1.9%, and 7.9%.This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery.</p>
<p>Figure 1: Comparison of research progress timelines for AI text detection on the RAID (Dugan et al., 2024).The right panel shows that DeepScientist achieves progress in two weeks that is comparable to three years of human research (Su et al.;Bao et al., a;b;Hu et al., 2023) (left panel).All zero-shot methods, including the system-generated T-Detect, TDT, and PA-Detect, uniformly adopt Falcon-7B (Almazrouei et al., 2023) as the base model.Additionally, all methods produced by DeepScientist demonstrate higher throughput than the previous SOTA method, Binoculars (Hans et al., 2024).</p>
<p>INTRODUCTION</p>
<p>Scientific discovery is inherently a process of continuous exploration and trial-and-error, where vast amounts of time and effort are invested to push the boundaries of human knowledge forward by a small step.This principle of persistent, incremental advancement is visible across the history of technology.For example, the decades-long optimization of semiconductor manufacturing has seen the feature size of transistors systematically reduced from micrometers to single-digit nanometers (Moore, 1965).Similarly, the efficiency of photovoltaic cells has been continuously advanced over half a century, with myriad material and architectural innovations pushing conversion rates from nascent single-digit percentages ever closer to their theoretical limits (Green, 1993).These historical trajectories underscore a process where human scientists engage in decades of goal-directed, iterative work to advance the SoTA artifacts continuously.</p>
<p>Recently, the emergence of Large Language Models (LLMs) has propelled automated scientific discovery, where LLM-based AI Scientist systems take the lead in exploration (Xie et al., 2025b).With their powerful capacity for long-form generation and comprehension, LLMs enable end-toend, full-cycle automation in scientific discovery.This has inspired influential work such as AI SCIENTIST-V2 (Yamada et al., 2025), whose scientific artifacts have been published in top-tier conference workshops.However, in the absence of clearly defined scientific goals, current AI Scientist systems often fall into the trap of blindly recombining existing knowledge and methods.As a result, their research outputs frequently appear naive under human evaluation and lack genuine scientific value (Zhu et al., 2025c).AI Scientists are yet to solve human challenges.</p>
<p>To solve real-world challenges, We formally model the full cycle of scientific discovery as a goaldriven Bayesian Optimization problem, where the singular objective is to find a novel method that maximally improves a target performance metric.Building on this formulation, we introduce DeepScientist, a LLM-based agent system designed to explore progressively across the unknown space of possible candidate research methods to identify the optimal plan that maximizes a highly expensive-to-evaluate function of true scientific value.Specifically, DeepScientist employs an iterative workflow, together with a continuously expanding memory of prior research knowledge to efficiently manage uncertainty during exploration.It intelligently balances exploitation (deepening investigations into promising high-value directions) with exploration (venturing into uncharted areas to acquire new knowledge).Through large-scale parallel exploration, DeepScientist can generate innovative hypotheses and ultimately yield both valuable new methods and validation-proven scientific findings through continuous exploration.</p>
<p>We select three frontier scientific tasks (Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection ), take their state-of-the-art methods (ICML 2025Spotlight, ACL 2025Outstanding, ICLR 2024) as starting points, and ask DeepScientist to conduct continuous research.As shown in Figures 1 and 3, within a month-long cycle of exploration, validation, and iteration on 16 H800 GPUs, DeepScientist exceeds their respective human SOTA methods by 183.7% (Accuracy), 1.9% (Tokens/second), and 7.9% (AUROC) through autonomously redesigning core methodologies, rather than simply combining existing techniques (Section 4.1).</p>
<p>To understand how such progress emerged, we analyze DeepScientist's discovery logs, and formed a small program committee to review the generated papers (Section 4.2).These logs show that the system generated over 5,000 unique ideas, of which only 1,100 are selected for experimental validation, and just 21 ultimately lead to scientific innovations (Section 4.3).Moreover, through the scaling experiment on computational resource, we discover a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.</p>
<p>To our knowledge, we provide the first empirical demonstration of an automated full-cycle scientific discovery system capable of producing novel, SOTA-surpassing methods and continuously advancing scientific frontiers at a pace that substantially exceeds human researchers.Our findings reveal a stark reality: while the AI's exploratory speed is immense, its inherent success rate for innovation remains exceptionally low, making effective validation and filtering the new bottleneck at the frontier of automated science.Therefore, the central question of the field is no longer 'Can AI innovate?',but rather 'How can we efficiently guide its powerful, yet highly dissipative, exploratory process to maximize scientific return?'We hope this work can inspire the research community to develop AI Scientist systems with greater exploration efficiency to accelerate scientific discovery at a larger scale, paving the way for ground-breaking discoveries.</p>
<p>Replication and Optimization.A significant body of research focuses on engineering tasks that operate within established scientific frameworks.This includes replication-oriented works like Pa-perBench (Starace et al., 2025) and Paper2Agent (Miao et al., 2025), which aim to reproduce existing papers.Other works, such as Agent Laboratory (Schmidgall et al., 2025b) and MLE-Bench (Chan et al., 2024), tackle early-stage machine learning engineering problems.Similarly, systems like Al-phaTensor (Fawzi et al., 2022) and AlphaEvolve (Novikov et al., 2025) use massive trial-and-error with known engineering methods to improve the performance of codebases.The common goal of these efforts is engineering-driven optimization within an established scientific paradigm, enhancing existing systems without questioning their foundational assumptions.DeepScientist, in contrast, pursues scientific discovery by targeting the core limitations of the SOTA itself.Its objective is not to refine the current state-of-the-art, but to establish a new one by introducing fundamentally different methodologies.</p>
<p>Semi-Automated Scientific Assistance.The path toward automating scientific discovery begin not with replacing the scientist, but with assisting them, leading to the development of a paradigm of specialized AI tools for individual research tasks.Systems like CycleResearcher (Weng et al., 2025) handle writing, DeepReview (Zhu et al., 2025a) manages reviewing, and co-scientists (Gottweis et al., 2025;Penadés et al., 2025;Swanson et al., 2025;Baek et al., 2025) aid in hypothesis generation.These powerful tools address only isolated fragments of the scientific process, leaving the crucial loop of learning from failure and exploration to humans.In contrast, DeepScientist is an autonomous agent of inquiry, managing the entire end-to-end research cycle and closing the loop by learning from its own experiments and self-directing its research path.</p>
<p>Automated Scientific Discovery.Building on the capabilities of specialized assistants, a line of research pursue full, end-to-end research automation (Yang et al., 2023;Xie et al., 2025a).Pioneering efforts, such as the AI Scientist systems (Lu et al., 2024;Yamada et al., 2025) and subsequent work (Intology, 2025;Jiabin et al., 2025), successfully demonstrate that an AI system could manage the full research cycle and produce novel findings.However, their primary limitation often lies in their exploratory strategy, which lacks a specific scientific goal rooted in a field's grand challenges, resulting undirected discoveries that may be perceived as lacking genuine scientific value.Instead, DeepScientist is thus the first automated scientific discovery system that leverages a closedloop, iterative process to discover methods surpassing the human state-of-the-art.The exploration of DeepScientist is goal-oriented and insight-driven, beginning by identifying a recognized limitation in the human SOTA and then using failure attribution to ensure discoveries are both novel and scientifically meaningful.</p>
<p>DEEPSCIENTIST: A PROGRESSIVE SYSTEM FOR DISCOVERING</p>
<p>SOTA-SURPASSING FINDINGS</p>
<p>MODELING SCIENTIFIC DISCOVERY AS AN OPTIMIZATION PROBLEM</p>
<p>The fundamental goal of automated scientific discovery is to autonomously identify novel methods that yield significant advancements in a given scientific domain.This process can be formally conceptualized as a search for an optimal solution within a vast and unstructured space of possibilities.Let the space of all possible candidate research methods be denoted by I.Each individual method I ∈ I, such as a novel algorithm or a new model architecture, possesses an intrinsic scientific value.This value is determined by a latent, black-box true value function, f : I → R, which maps a method to its ultimate empirical impact.The objective of scientific discovery is therefore to find the optimal method I * that maximizes this function:
I * = arg max I∈I f (I)(1)
Unlike previously studied tasks such as early-stage machine learning (Schmidgall et al., 2025a), algorithmic design (Novikov et al., 2025;Lange et al., 2025), or scientific software development (Aygün et al., 2025), A defining characteristic of frontier scientific discovery is that each exploratory step demands immense computational and intellectual resources, making the evaluation of the true scientific value function, f (•), prohibitively costly.Any single evaluation, f (I),</p>
<p>Strategize &amp; Hypothesize Implement &amp; Verify Analyze &amp; Report</p>
<p>Paper Repositories  corresponds to a complete and resource-intensive research cycle of implementation, experimentation, and analysis, often consuming vast computational resources (e.g., on the order of 10 16 FLOPs for a frontier LLM problem, as illustrated in Figure 4.c).This extreme sample inefficiency renders brute-force or random exploration of the space I intractable.Therefore, we model the problem within the framework of Bayesian Optimization (Frazier, 2018;Garnett, 2023), which provides a principled methodology for global optimization of expensive black-box functions.By constructing a surrogate model to intelligently guide the search, Bayesian Optimization effectively reduces the number of costly real-world evaluations through a careful balance of exploration and exploitation.However, for scientific discovery, I is a conceptual space that is not explicitly defined.Candidate methods I must be formulated as creative, plausible, and coherent scientific hypotheses.The generation of high-quality candidate hypotheses is a critical bottleneck that traditional Bayesian Optimization algorithms are not designed to address.This challenge necessitates a new mechanism that integrates creative ideation with sample-efficient optimization.We detail our solution to this problem in the following subsections.</p>
<p>Human Findings DeepScientist Findings Memory</p>
<p>THE DEEPSCIENTIST FRAMEWORK</p>
<p>The architecture of DeepScientist actualizes the Bayesian Optimization loop through a multi-agent system equipped with an open-knowledge system and a continuously accumulating Findings Memory.This memory is composed of both frontier human knowledge (e.g., papers and codes) and the system's own historical findings, and it intelligently guides subsequent explorations.The entire discovery process is structured as a hierarchical and iterative three-stage exploration cycle.In this hierarchical scheme, only research ideas that exhibit promise are advanced to more expensive evaluations, while others are retained in the Findings Memory to inform subsequent explorations.This design ensures the computational resources are dynamically and precisely allocated to the most promising scientific trajectories, thereby maximizing discovery efficiency under constrained budgets.Specifically, each stage within the three-stage exploration cycle is associated with a distinct fidelity-cost tradeoff (Figure 2):</p>
<p>Strategize &amp; Hypothesize.Each research cycle begins by analyzing the Findings Memory (M t ), a list-style database containing thousands of structured records.Each record represents a unique scientific finding, which is categorized according to its stage of development.To overcome the LLM's context length constraints, we use a separate retrieval model (Wolters et al., 2024) when needed to select the Top-K Findings as input.The vast majority of records begin as Idea Findings-unverified hypotheses.During this first stage, the system identifies limitations in existing knowledge and gen- erates a new collection of hypotheses (P new ), and then they evaluated by a low-cost Surrogate Model (g t ).The surrogate model (an LLM Reviewer) is first contextualized with the entire Findings Memory.It then approximates the true value function f and, for each candidate finding I ∈ P new , produces a structured valuation vector V = ⟨v u , v q , v e ⟩, quantifying its estimated utility, quality, and exploration value as integer scores on a scale of 0 to 100.Each new hypothesis and its valuation vector is then used to initialize a new record in the Findings Memory as an "Idea Finding".</p>
<p>Implement &amp; Verify.This stage serves as the primary filter in the Findings Memory.To decide which of the numerous "Idea Findings" warrants the significant resource investment to be advanced in a real-world experiment, the system employs an Acquisition Function (α).Specifically, it uses the classic Upper Confidence Bound (UCB) algorithm to select the most promising record.The UCB formula maps the valuation vector V to balance the trade-off between exploiting promising avenues (represented by v u and v q ) and exploring uncertain ones (represented by v e ):
I t+1 = arg max I∈Pnew w u v u + w q v q Exploitation Score +κ • v e Exploration Score ,(2)
where w u and w q are hyperparameters and κ controls the intensity of exploration.The highestscoring finding I t+1 is selected for validation, and its record is promoted to the status of an Implement Finding.A coding agent then performs a repository-level implementation to executed the experiment.This agent operates within a sandboxed environment with full permissions, allowing it to read the complete code repository and access the internet for literature and code searches.Its objective is to implement the new hypothesis on top of the existing SOTA method's repositories.</p>
<p>The agent typically begins by planning the task, then reads the code to understand its structure, and finally implements the changes to produce the experimental logs and results.The experiment logs and results, f (I t+1 ), is used to update the corresponding record, enriching it with empirical evidence and thus closing the learning loop.</p>
<p>Analyze &amp; Report.The final and most selective stage of the Findings Memory is triggered only by a successful validation.When an "Implement Finding" succeeds in surpassing the baseline, its record is promoted to a Progress Finding.This transformation is implemented by a series of specialized agents capable of utilizing a suite of MCP (Hou et al., 2025) tools.These agents first autonomously design and execute a series of deeper analytical experiments (e.g., ablations, evaluations on new datasets), leveraging MCP tools to manage the experimental lifecycle, data collection, and result parsing.Subsequently, a synthesis agent employs the same toolset to collate all experimental results, analytical insights, and generated artifacts into a coherent, reproducible research paper.This deeply validated record becomes a new record in the system's knowledge base, thus influencing the decision-making process in all subsequent cycles.</p>
<p>EXPERIMENTS</p>
<p>As detailed in Table 1, we select three distinct SOTA methods (published in 2024 and 2025) as starting points, chosen for their frontier status, community interest, and human supervisability.Each SOTA method is manually reproduced, and we preserve execution logs and test scripts to allow DeepScientist to focus on research advancement.DeepScientist is provided with two servers, each with 8 Nvidia H800 GPUs.To maximize utilization, we launch a separate system instance for each GPU, employing the Gemini-2.5-Promodel for core logic and the Claude-4-Opus model for its robust code-generation capabilities.Three human experts supervise the process to verify outputs and filter out hallucinations.For more implementation details, please see Appendix C.</p>
<p>DEEPSCIENTIST ACHIEVEMENTS ON THREE RESEARCH DOMAINS</p>
<p>Agents Failure Attribution.The task addresses the question: in an LLM-based multi-agent system, which agent caused the task to fail and when?Starting from the baseline "All at once" method (Zhang et al., 2025c), DeepScientist identified that the current approach lacks the counterfactual reasoning capabilities essential for attribution.Through a process of trial, error, and synthesizing new findings-discovering the effectiveness of hypothetical prediction and simulated attempts-it ultimately proposed the A2P method.Named for its Abduction-Action-Prediction process, its core innovation elevates failure attribution from pattern recognition to causal reasoning, filling the critical gap in counterfactual capabilities by predicting if a proposed fix would have led to success.As shown in Figure 3.(a-b), A2P achieved scores of 29.31 and 47.46 in the "handcraft" and "algorithm-generated" settings of the Who&amp;When benchmark, respectively, setting a new state-of-the-art (SOTA).In this task, DeepScientist validated that a structured, zero-shot causal reasoning framework can be superior to less principled methods.As of September 2025, the training-free A2P method maintains its SOTA position, outperforming even 7B models trained on synthetic data.(Zhang et al., 2025a).</p>
<p>LLM Inference Acceleration is a highly optimized field aiming to maximize throughput and reduce latency during LLM inference (Xia et al., 2024).In this process, the system actively made many different attempts, such as using a Kalman Filter (Zarchan, 2005) to dynamically adjust an adjacency matrix to address the original method's lack of a memory function.Although most of these attempts failed, the system-generated ACRA method ultimately advanced the MPBB (Austin et al., 2021) from a human SOTA of 190.25 to 193.90 tokens/second by identifying stable suffix patterns, as shown in Figure 3. Scientifically, this innovation is significant because it uses this extra contextual information to dynamically adjust the decoding guess, effectively grafting a long-term memory onto the process and breaking the context-collapsing of standard decoders.This discovery highlights the system's primary goal: the creation of new, human-unknown knowledge rather than mere engineering optimization.For instance, one could likely achieve greater performance gains by combining ACRA with an established technique like layer skipping (Wang et al., 2022) or PageAttention (Kwon et al., 2023), but this would represent an engineering effort, not a scientific one.The exploration assessment within our process avoids such combinations of existing knowledge.</p>
<p>Table 2: Evaluation of AI-generated papers produced by various AI Scientist systems.Scores represent the average ratings given by DeepReviewer-14B (Zhu et al., 2025a) across the number ("Num") of available papers.Note: Publicly available papers may be curated and therefore may not fully represent the typical output of each system.AI Text Detection is a binary classification task where, given a text that may contain content from an LLM (and possibly additional noise), the goal is to determine if it was produced by a human or an LLM (Li et al., 2022;Ghosal et al., 2023).To validate its capacity for sustained advancement, DeepScientist made numerous attempts that included addressing the Boundary-Aware Extension problem and exploring approaches like Volatility-Aware and Wavelet Subspace Energy methods.</p>
<p>The final results show a dramatic acceleration in scientific discovery: in a rapid evolution over just two weeks, the system produced three distinct, progressively superior methods (T-Detect, TDT, and PA-Detect).This began with T-Detect fixing core statistics with a robust t-distribution, then evolved conceptually with TDT and PA-Detect, which treat text as a signal and use wavelet and phase congruency analysis to pinpoint anomalies.Scientifically, this shift reveals the "non-stationarity" of AI-generated text, alleviating the information bottleneck in prior paradigms that average away localized evidence.As shown in Figure 1 and 3(d), this entire discovery trajectory demonstrates DeepScientist's ability for advancing frontier-pushing scientific findings progressively, establishing a new SOTA with a 7.9% higher AUROC while also doubling the inference speed.</p>
<p>ASSESSING THE QUALITY OF AI-GENERATED RESEARCH PAPER</p>
<p>Experimental Setup.To assess the quality of the final output, we evaluate the five research papers autonomously generated by DeepScientist's end-to-end process.Our evaluation protocol is twofold.First, to benchmark against existing work, we employ DeepReviewer (Zhu et al., 2025a), an AI agent that simulates the human peer-review process with an external search capability, comparing Deep-Scientist's output against 28 publicly available papers from other AI Scientist systems.Second, for a more rigorous assessment, we convene a dedicated program committee consisting of three active LLM researchers: two volunteers who have served as ICLR reviewers and one senior volunteer who has been invited to be an ICLR Area Chair.Human Expert Evaluation.The evaluation from our human program committee, shown in Table 3, reveal a remarkable and unanimous consensus: DeepScientist consistently excels at ideation, the most challenging and often rate-limiting step in human-led research.Full details on the review protocol are provided in Appendix A, and the core ideas within each paper are praised for their genuine novelty, ingenuity, and scientific contributions.The quality of these innovations is further demonstrated by the review scores: the system's average rating (5.00) closely mirrors the average of all ICLR 2025 submissions (5.08), with two of its papers significantly exceeding this (5.67).</p>
<p>ANALYSIS OF THE ITERATIVE TRAJECTORY OF AUTONOMOUS EXPLORATION</p>
<p>Experimental Setup.The findings in this section are derived from a series of post-hoc analyses conducted on the complete operational data generated by DeepScientist across the three frontier tasks.This data includes the full set of execution logs and the Findings Memory, providing the basis for all subsequent statistical analysis.To visualize the conceptual search space (Figure 5), we embed the complete description of each generated finding using the Qwen3-Embedding-8B model.</p>
<p>To assess scalability (Figure 6), we conduct a dedicated one-week experiment where N identified limitations of a single SOTA method are assigned to N parallel GPU instances.These instances explore solutions independently but share their findings to a central database, which are synchronized globally every five cycles to accommodate the asynchronous nature of the discovery process.Finally, to better understand the low success rate, our program committee experts perform a detailed causal attribution analysis on a sample of 300 failed implementations.</p>
<p>Our analysis of DeepScientist's experimental logs reveals the sheer scale of the trial-and-error process inherent in autonomous scientific discovery.Even in our relatively fast-executing domains, achieving progress required hundreds of trials per task.As show in Figure 4, the execution time distributions show that while individual experiments may be quick, the sheer volume of trialand-error necessary to uncover a successful idea is substantial.This suggests a clear application boundary for current autonomous science: for tasks with rapid feedback loops, such as knowledge editing or aspects of chip design, delegating massive-scale experimentation to AI is a powerful strategy.However, for high-cost endeavors like pre-training foundation models or pharmaceutical synthesis, the low success rate makes such an approach currently impractical, mandating continued reliance on human-led ideation.The autonomous research process is characterized by a vast exploratory funnel where promising ideas are exceptionally rare.Across the three tasks, DeepScientist generate over 5,000 unique ideas, yet only about 1,100 are deemed worthy of experimental validation by the system's selection mechanism, and a mere 21 ultimately result in scientific progress.An ablation study underscores the criticality of this selection process: without it, randomly sampling 100 ideas for each task and testing them yields a success rate of effectively zero.With our selection strategy, the success rate rises to approximately 1-3%, demonstrating that while still low, intelligent filtering is essential.The low success rate is not merely a matter of failed hypotheses; analysis by human experts on a sample of failed trials reveals that approximately 60% were terminated prematurely due to implementation errors, while the vast majority of the remaining 40% simply offered no performance improvement or caused a regression.This highlights that the probability of an LLMgenerated idea being both correct in its premise and flawless in its implementation is exceedingly  low.The success of this work, therefore, is not a product of brute-force computation but of search efficiency.A naive approach of fully testing all 5000 promising candidates would have required over 100,000 GPU hours, whereas our targeted exploration achieved its breakthroughs using only 20,000.</p>
<p>DeepScientist's discovery process follows a purposeful and progressive trajectory.The semantic distribution of ideas generated for the AI text detection task, as shown in Figure 5, reveals the characteristics of this sophisticated strategy.While the system generates thousands of diverse ideas across a vast conceptual landscape, its path to success is not random but is a series of focused, logical advancements.This indicates a capacity to progressively deepen its understanding: after achieving an initial breakthrough with T-Detect, the system effectively establishes a SoTA, identifies its subsequent limitations, and reorients its search towards a new goal.This dynamic exploration is exemplified by the conceptual shift towards TDT and PA-Detect, which build upon the previous success by leveraging new positional and temporal information.This ability to build upon its own discoveries, turning each successful finding into a new starting point for identifying and solving the next set of limitations, demonstrates a powerful capacity for scientific exploration.</p>
<p>Scaling Laws in DeepScientist's Scientific Discovery.To investigate the relationship between computational scale and the rate of scientific progress, we evaluated the number of "Progress Findings" generated by DeepScientist within a fixed one-week period as a function of available parallel resources in Figure 6.In this setup, the system first identified a set of limitations in the baseline method, and each parallel exploration path was tasked with resolving a distinct limitation, with all paths periodically synchronizing their results into a shared Findings Memory.Our results indicate a promising scaling trend.While minimal resources yielded no breakthroughs, the rate of discovery began to increase effectively as we scaled to 4 GPUs and beyond, growing from one SOTAsurpassing finding with 4 GPUs to eleven with 16 GPUs.This appears to establish a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.We hypothesize this efficiency stems from more than just parallel trial-and-error; it is a direct result of the shared knowledge architecture.As each parallel path explores, it enriches the shared Findings Memory.This creates a synergistic effect where the collective intelligence of the system grows (Schmidgall &amp; Moor, 2025;Zhang et al., 2025b), allowing each independent path to benefit from the successes and, just as importantly, the failures of others.This suggests that effectively scaling autonomous science is not just a matter of increasing brute-force computation, but of fostering a richer, interconnected knowledge base that accelerates discovery across all concurrent efforts.</p>
<p>DISCUSSION</p>
<p>The results from DeepScientist suggest a new paradigm in scientific exploration.The system's 1-5% progress rate mirrors the reality of frontier research, where breakthroughs are inherently rare.Its core strength is not infallibility, but the ability to conduct this trial-and-error process at a scale and speed previously unimaginable, compressing years of human exploration into weeks.The primary path forward, therefore, is to focus on systematically improving this discovery efficiency, enhancing both the quality of generated hypotheses and the robustness of their implementation.</p>
<p>This challenge highlights a powerful opportunity for human-AI synergy.We envision a future where DeepScientist serves as a massive-scale exploration engine, with its trajectory guided by human intellect.The role of human researchers can shift from laborious experimentation to the high-level cognitive tasks of formulating valuable scientific questions and providing strategic direction, thereby leveraging the AI for rapid, exhaustive exploration.To make the AI a more capable partner, future work should focus on key enhancements: developing simulated discovery environments to accelerate learning via reinforcement, creating frameworks for integrating feedback from the scientific community, and ultimately, bridging the gap to the physical sciences through robotics.</p>
<p>CONCLUSION</p>
<p>This work presents the first large-scale empirical evidence that an autonomous AI can achieve progressively, SOTA-surpassing progress on modern scientific frontiers.We introduced DeepScientist, a goal-oriented system achieving end-to-end autonomy from ideation to real progress, which learns by synthesizing human knowledge with its own findings from iteration of trials.Results across multiple domains serves to accelerate the progress of real-world scientific discovery, providing a crucial foundation.Our findings can signal a foundational shift in AI research, heralding an era where the pace of discovery is no longer solely dictated by the cadence of human thought.</p>
<p>ETHICS STATEMENT</p>
<p>The development of DeepScientist, an autonomous system capable of advancing scientific frontiers, carries profound ethical responsibilities.Our primary goal is to accelerate discovery for the benefit of humanity, but we recognize the potential for misuse.The most significant risks include the application of this technology to advance dangerous research and the potential degradation of the academic ecosystem.We have implemented specific, robust measures to address these concerns proactively.</p>
<p>A primary concern is the dual-use risk, where the system could be co-opted to accelerate research in harmful domains, such as developing novel toxins or malicious software.To assess and mitigate this, we conducted red-teaming exercises specifically targeting the generation of computer viruses.We tasked the system, powered by leading foundation models (including GPT-5, Gemini-2.5-Pro,and Claude-4.1-Opus in our testbed), with this malicious objective.In all instances, the underlying models exhibited robust safety alignment, refusing to proceed with the research.They correctly identified the task as illegal and harmful, and autonomously terminated the research cycle, demonstrating that foundation model safety protocols provide a critical defense layer.</p>
<p>We are also deeply conscious of the potential negative impact on the academic ecosystem.It is crucial to state that all results from DeepScientist presented in this paper, including code and experimental findings, have undergone rigorous human verification.Recognizing that others might neglect this critical oversight, we are adopting a selective open-sourcing policy to mitigate the risk of proliferating unreliable publications.We will open-source the core components that drive continuous discovery, as we believe their potential to accelerate progress for the community outweighs the risks.However, we will deliberately refrain from open-sourcing the "Analyze &amp; Report" module.This decision is made to prevent the automated generation of seemingly credible but scientifically unverified papers, thereby safeguarding the integrity of the academic record.</p>
<p>Ultimately, we envision DeepScientist as a powerful tool to augment, not replace, human intellect and judgment.To enforce this vision, our open-source components will be released under a license based on MIT, but with explicit addendums that codify our ethical framework.This license will strictly prohibit any use of the software for harmful research.Furthermore, it will legally require that a human user must supervise the entire operational process of DeepScientist and assumes full and final responsibility for all its outputs.By embedding these requirements directly into our terms of use, we aim to foster a research environment where AI-driven discovery proceeds with the necessary human accountability and ethical oversight.</p>
<p>A HUMAN EXPERT REVIEW</p>
<p>A.1 REVIEW PROCESS AND CRITERIA</p>
<p>To ensure a rigorous and impartial evaluation of the generated papers, we convened a small, dedicated program committee.The committee was composed of two active researchers who served as volunteer reviewers for ICLR 2025, and one senior researcher who had previously been invited to serve as an ICLR Area Chair.All committee members possess significant expertise in the field of Large Language Models.The entire review process, with the exception of a rebuttal phase, was designed to meticulously emulate the official standards of ICLR 2025.Each of the five papers generated by our system was assigned to the three reviewers for a thorough and independent assessment.The average review time for each paper was 55 minutes, during which reviewers were required to provide not only scores but also detailed written feedback, including a summary of the paper's strengths and weaknesses.</p>
<p>The evaluation was conducted on a custom-deployed review website where reviewers could not see each other's scores or feedback, ensuring that all initial assessments were made independently.The review form was structured to gather concise yet comprehensive feedback.First, reviewers were asked to state their Confidence in their review on a scale of 1 to 5. The core of the evaluation consisted of three sub-scores, each rated on a 1 to 4 scale: Soundness, assessing the technical correctness and experimental rigor; Presentation, evaluating the clarity and quality of the writing; and Contribution, measuring the significance and novelty of the work.Finally, reviewers provided a holistic Rating on a scale of 1 to 10, where a score of 5 represented a 'borderline reject' and a score of 6 represented a 'borderline accept'.</p>
<p>After the three reviewers submitted their independent evaluations for a paper, the volunteer acting as Area Chair would then read all submitted reviews.Drawing upon their experience from the ICLR review process, the Area Chair synthesized the feedback, weighed the arguments presented by the reviewers, and made a final executive decision on whether the paper should be accepted or rejected in the context of our study.This final decision was recorded as the definitive outcome for each paper's evaluation.</p>
<p>A.2 SUMMARY OF REVIEWER FEEDBACK</p>
<p>Across the five generated papers, a clear consensus emerged from the human reviewers: Deep-Scientist consistently excels at the ideation stage of research.The committee unanimously lauded the methods for their genuine novelty and tangible contributions, noting that each paper proposed a unique approach that meaningfully advanced the state-of-the-art in its respective subfield.This feedback validates the system's core strength as a powerful engine for identifying relevant research gaps and generating innovative, impactful solutions, confirming that it can successfully ideate beyond mere incremental improvements.</p>
<p>However, this strength in ideation was systematically undermined by a recurring pattern of weaknesses in scientific execution and rigor.The most critical and frequent concern was a lack of empirical soundness; reviewers consistently noted that DeepScientist failed to design comprehensive validation plans, citing insufficient evaluation on standard benchmarks and a lack of in-depth analytical experiments (e.g., ablations, motivation studies) to justify its claims.This was compounded by a failure to properly contextualize its contributions, with papers often omitting comparisons to essential baselines or failing to discuss closely related work, thereby weakening the perceived significance of the results.</p>
<p>This feedback pinpoints the primary bottleneck in current autonomous systems: a profound gap between the ability to generate novel concepts and the capacity for rigorous scientific execution and articulation.The observed weaknesses in experimental design directly reflect the low-success-rate problem discussed previously; the system struggles not just to implement ideas correctly, but to validate them convincingly.To bridge this gap, future work must endow these systems with a deeper, procedural understanding of the scientific method itself.This requires moving beyond simple implementation and reporting capabilities towards two key areas: First, developing agents explicitly trained in experimental design, capable of planning comprehensive evaluations that anticipate and address potential scientific critiques.Second, enhancing the system's ability for analytical reasoning, enabling it to not just describe results but to interpret their significance, formulate compelling arguments, and engage in the kind of deep, reflective discussion that characterizes high-impact research.</p>
<p>B ADDRESSING THE BOTTLENECKS IN AUTONOMOUS SCIENTIFIC DISCOVERY</p>
<p>The ever-increasing value of LLM is reshaping the paradigm of scientific exploration through their ability to generate hypotheses at a massive scale (Li &amp; Weng, 2022;Weng et al., 2023;Weng et al.;Wei et al., 2024;Weng et al., 2024;Berkovich et al., 2025;Zhu et al., 2025b).Consequently, this capability has pushed "verification" to the center stage, making it a critical bottleneck in the discovery process.Our research empirically reveals the severity of this challenge: on frontier scientific tasks, the success rate of ideas generated by AI systems that ultimately lead to substantial progress is typically below 3%, meaning the vast majority of computational resources are consumed exploring low-value hypotheses.This inefficient "needle in a haystack" model is the core obstacle preventing AI Scientists from evolving from "novel tools" to "efficient discoverers."(Cornelio et al., 2025) Therefore, to further accelerate the process of scientific discovery, future research must focus on constructing a systematic solution to overcome this bottleneck.As shown in Figure 7, future AI Scientist systems need to evolve synergistically in three key directions: optimizing the quality of initial hypotheses (Optimize Hypothesis Quality), enhancing filtering capabilities during the process (Enhance Filtering), and improving the quality of implementation and verification at the final stage (Improve Implementation Quality).One of the core future research directions is to develop AI systems capable of generating higherquality, more reliable hypotheses (as shown in Figure 7e), equipped with more precise filtering mechanisms to predict their success rate (as shown in Figure 7d).Methods that rely purely on a data-driven approach, while capable of discovering patterns, often produce outputs that lack a theoretical foundation and are prone to generating "hallucinations" that contradict known scientific theories.Future systems must move beyond this by more deeply integrating background knowledge and theory.For instance, the direction represented by "derivable models" (such as AI-Descartes (Cornelio et al., 2023) and AI-Hilbert (Cory-Wright et al., 2024)), which incorporate scientific axioms as constraints during the hypothesis generation phase, offers a promising path to improving hypothesis quality.Furthermore, systems must have the ability to learn from their own exploratory history.By establishing mechanisms similar to a "Findings Memory," a system can systematically record and analyze every success and failure, thereby avoiding redundant exploration of ineffective paths in subsequent iterations and gradually developing a more insightful scientific intuition.Building on this foundation, developing more advanced, low-cost surrogate models and acquisition functions to more accurately predict the scientific value of an idea will be key to enhancing filtering efficiency and conserving verification resources.</p>
<p>Hypotheses Implement &amp; Verify</p>
<p>Concurrently, an often-overlooked yet crucial future research direction is to significantly improve the quality and reliability of AI systems in the engineering implementation and verification stages (as shown in Figure 7c).Even the most brilliant scientific concept can never have its value confirmed if it cannot be accurately and flawlessly translated into an executable experiment.Our analysis indicates that up to 60% of exploratory failures stem from implementation-level errors, which represents a massive waste of resources and directly impedes scientific progress.History has repeatedly warned us that a lack of rigorous verification can lead to catastrophic consequences, whether in NASA missions or medical practice.Therefore, building a scalable and reliable automated verification platform is an essential path forward.This requires not only more powerful code-generation and self-debugging agents to reduce implementation errors but also standardized sandbox environments and automated testing procedures to ensure the stability and reproducibility of experimental results.</p>
<p>Ensuring the absolute reliability of the verification process is the final and most critical line of defense in transforming AI-generated "plausible ideas" into "solid scientific evidence."</p>
<p>Looking ahead, to truly accelerate scientific discovery, it is necessary to integrate the aforementioned strategies into an organic whole, advancing AI Scientists from "random explorers" to "goaloriented strategists."This is not about replacing humans with AI, but about pioneering a more efficient paradigm of human-AI collaboration.In this model, human scientists are responsible for defining grander, more valuable scientific goals and providing high-level strategic guidance, while the AI system serves as a powerful "exploration engine," executing efficient trial-and-error and verification cycles at an unprecedented scale and speed under human direction.To realize this vision, the community must also address a series of challenges, such as building benchmarks that can truly evaluate innovation and designing mechanisms that encourage diverse exploration to avoid the homogenization of research paradigms, thereby preserving the potential for serendipitous discoveries like Alexander Fleming's discovery of penicillin (Fleming, 1941).</p>
<p>C IMPLEMENTATION DETAILS</p>
<p>Our implementation relies on a distributed architecture to manage the distinct tasks of scientific reasoning and code execution.The core logic of DeepScientist is powered by the Gemini-2.5-promodel, while all code implementation tasks are delegated to Claude-4-opus, executed within the Claude Code framework (v1.0.53).To ensure stability and security, the DeepScientist system and the Claude Code agent are isolated in separate Docker containers, communicating via a port-based API.During the 'Implement &amp; Verify' stage, a human-verified baseline code repository is first duplicated into a new, sandboxed folder.The Claude Code agent's operations are strictly confined to this new directory to prevent unintended modifications.A critical step in our pipeline is a secondary verification process: after Claude Code reports completion, DeepScientist independently re-executes the main script via the command line.This measure was implemented to counteract a high rate of false positives-we observed that approximately 50% of initial implementation attempts failed to complete fully due to internal timeouts within the Claude Code agent.Throughout this project, all experimental results were manually inspected by human supervisors to guarantee their authenticity.</p>
<p>For the 'Analyze &amp; Report' stage, a similar process is followed: the validated code is replicated for each analytical experiment, with Claude Code executing them sequentially.Upon completion, DeepScientist aggregates all results, generates a paper outline, and then employs automated tools to write and compile the final PDF manuscript.For all experiments, we used a fixed set of hyperparameters: the retrieval count was set to K = 15, and the UCB parameters were set to utility weight w u = 1, quality weight w q = 1, and exploration coefficient κ = 1.</p>
<p>Figure 2 :
2
Figure 2: The autonomous, closed-loop discovery process of DeepScientist.The system iterates through a three-stage cycle, learning from both human knowledge and its own experiments.</p>
<p>Figure 3 :
3
Figure 3: Performance evaluation of DeepScientist across three research domains: (a-b) Agent Failure Attribution on Who&amp;When benchmark in handcraft and algorithm-generated settings; (c) LLM Inference Acceleration on MBPP dataset; (d) AI Text Detection with performance-latency tradeoff analysis.DeepScientist (shown in pink) consistently outperform human-designed SoTA approaches (shown in blue) across all tasks.</p>
<p>Figure 4 :
4
Figure 4: DeepScientist's experimental statistics.(a) The research pipeline from generated ideas to validated progress.(b) Success rates comparing our selection strategy against a baseline.(c) Distribution of wall-clock execution times for all implemented trials.</p>
<p>Figure 5 :
5
Figure5: Visualization of the conceptual search space for the AI text detection task.The plot shows a t-SNE visualization of the semantic embeddings for all 2,472 generated ideas.Markers identify the initial SOTA method (Initial Idea) and the three final SOTA-surpassing methods (Progress Ideas).</p>
<p>Figure 6 :
6
Figure6: Scaling analysis of autonomous scientific discovery.The plot illustrates the relationship between parallel computational resources (number of GPUs) and the number of SOTA-surpassing "Progress Findings" found by DeepScientist across all tasks within a one-week period.</p>
<p>Figure 7 :
7
Figure 7: Three strategies for improving the efficiency of autonomous scientific discovery.(a) and (b) illustrate the low success rate currently faced by both AI and human research.Future directions will need to accelerate the discovery process through the synergy of three approaches: (c) improving implementation success rates, (d) adding an efficient filtering stage before implementation, and (e) optimizing the quality of initial hypotheses from the source.</p>
<p>t e r a t i o n
IdeaImplementProgressFindingsFindingsFindings|---TODO.md|---Result.md|---USAGDP.csv |---plot_gdp.py.........Motivation I Human Findings Findings Search Papers DeepScientist+1-run.sh -src -logs -latex.tex -main.py -plan.md FILES Code Repositories if use_TDT: model = TDT model = Model from models import TDT main.py-Replace -Delete -ADDOpen Source PDF CompileDemo Create Review SelfExecutableExperimentEvidenceHypothesisCodeLogModel SurrogateFeasibility Effectiveness ExplorationSuccess FailedSuccess FailedOutlineHyper-parametersAblationExp EnvironmentCode Experiments Figure</p>
<p>Table 1 :
1
Overview of the three different human SOTA methods we selected.
TaskMethodVenueBenchmark Github StarAgents Failure Attribution All at OnceICML 2025 SpotlightWho&amp;When 302LLM Inference Accel.TokenRecycling ACL 2025 Outstanding MBPP323AI Text DetectionFastDetectGPTICLR 2024RAID414</p>
<p>Table 3 :
3
Evaluation of DeepScientist's papers produced by human experts.Values are presented as mean (variance) from three reviewers.Inter-rater reliability for Rating: Krippendorff's α = 0.739.
PaperConfidenceSoundnessPresentationContributionRatingHUMAN Avg. (ICLR 2025)-2.592.362.625.081. T-DE T E C T4.33 (0.33)2.00 (1.00)2.67 (0.33)2.67 (0.33)5.00 (0.00)2. TDT4.67 (0.33)3.00 (0.00)3.00 (0.00)3.00 (0.00)5.67 (0.33)3. PA-DE T E C T4.00 (0.00)1.67 (0.33)2.00 (1.00)2.00 (1.00)4.33 (1.33)4. A2P4.00 (0.00)3.00 (0.00)3.00 (0.00)2.67 (0.33)5.67 (0.33)5. ACRA3.33 (0.33)1.67 (0.33)2.00 (1.00)1.67 (0.33)4.33 (1.33)DeepScientist Avg.4.072.272.532.405.00</p>
<p>Average Execution Time of Implemented Ideas
0 500 1000 1500 2000 2500 Number of IdeasAI Text Detection 7 600 2,472 (a) Research Idea Pipeline Statistics Agents Failure Attribution LLM Inference Acceleration 12 2 196 312 1,077 1,330 Progress Ideas Implemented Ideas Total Ideas0 95 100 90 10 2 4 6 8 Success Rate (%)0 50 100 150 200 250 300 Execution Time (minutes) (b) Progress Success Rate (Progress/Implemented) AI Text Detection Agents Failure Attribution LLM Inference Acceleration with Selected w/o SelectedAI Text Detection n = 600 (c) Median Agents Failure Attribution LLM Inference Acceleration n = 196 n = 312 Mean Q1 Q3
The generated papers are available in Appendix C. Automated Review Against Other AI Scientist Systems.As shown in Table2, the results from the LLM-based automatic evaluation indicate that the system's outputs are recognized for their scientific novelty and value.When benchmarked against 28 publicly available papers from other AI Scientist systems using DeepReviewer, DeepScientist is the only AI Scientist system to produce papers that achieves a 60% acceptance rate.</p>
<p>ACKNOWLEDGEMENTSWe are grateful to Professor Linyi Yang for his insightful discussions on this paper.This work is inspired by pioneering efforts in automated scientific discovery, including AI Scientist(Lu et al., 2024;Yamada et al., 2025)and AlphaEvolve (Novikov et al., 2025).https://ai-researcher.netCode: https://github.com/ResearAI/DeepScientistThe financial and computational costs of this autonomous discovery process are substantial.Each idea generated during the 'Strategize &amp; Hypothesize' stage incurred an approximate cost of $5 in API calls.For each attempt in the 'Implement &amp; Verify' stage, the cost averaged $20 for Claude-4-opus API usage, in addition to the computational cost of approximately 1 GPU hour, as detailed in Figure ??.c.A successful finding that progressed to the 'Analyze &amp; Report' stage required a further expenditure of around $150, which includes $100 for running analytical experiments and $50 for the final report generation.The total cost to achieve the scientific advancements presented in this paper amounted to approximately $100,000.While significant, we believe these costs can be substantially reduced.We recommend that future iterations explore more economical alternatives, such as deploying high-throughput models like Qwen-3-Next-80B for the core DeepScientist system and leveraging subscription-based API access (e.g., Claude Max or OpenAI Pro) to mitigate per-call expenses.In this paper, each implementation was provided with a single H800 server for exploration.Since the H800 GPU has an FP16 computing power of approximately 2 TFLOPS, an average execution of 70 minutes corresponds to about 1 × 10 16 floating-point operations.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.
The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>An ai system to help scientists write expert-level empirical software. Anastasiya Eser Aygün, Gheorghe Belyaeva, Marc Comanici, Hao Coram, Jake Cui, Renee Garrison, Anton Johnston, Cory Y Kast, Peter Mclean, Zahra Norgaard, David Shamsi, James Smalling, Subhashini Thompson, Brian P Venugopalan, Chujun Williams, Sarah He, Martyna Martinson, Lai Plomecka, Yuchen Wei, Qian-Ze Zhou, Matthew Zhu, Erica Abraham, Anna Brand, Jeffrey A Bulanova, Chris Cardille, Scott Co, Grace Ellsworth, Malcolm Joseph, Ryan Kane, Johan Krueger, Dan Kartiwa, Jan-Matthis Liebling, Paul Lueckmann, Raccuglia, Xuefei, Katherine Wang, James Chou, Yossi Manyika, John C Matias, Lizzie Platt, Shibl Dorfman, Michael P Mourad, Brenner, 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas Chapterthe Association for Computational Linguistics20251</p>
<p>Glimpse: Enabling white-box methods to use proprietary models for zero-shot llm-generated text detection. Guangsheng Bao, Yanbin Zhao, Juncai He, Yue Zhang, The Thirteenth International Conference on Learning Representations. </p>
<p>Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, The Twelfth International Conference on Learning Representations, b. </p>
<p>Automatagpt: Forecasting and ruleset inference for two-dimensional cellular automata. Jaime A Berkovich, Noah S David, Markus J Buehler, 2025</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R Josephson, Joao Goncalves, Kenneth L Clarkson, Nimrod Megiddo, Bachir El Khadir, Lior Horesh, 10.1038/s41467-023-37236-yNature Communications. 2041-17231411777April 2023</p>
<p>The need for verification in ai-driven scientific discovery. Cristina Cornelio, Takuya Ito, Ryan Cory-Wright, Sanjeeb Dash, Lior Horesh, 2025</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. Ryan Cory-Wright, Cristina Cornelio, Sanjeeb Dash, Bachir El Khadir, Lior Horesh, 10.1038/s41467-024-50074-wNature Communications. 155922July 2024</p>
<p>RAID: A shared benchmark for robust evaluation of machinegenerated text detectors. Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Discovering faster matrix multiplication algorithms with reinforcement learning. Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, J R Francisco, Julian Ruiz, Grzegorz Schrittwieser, Swirszcz, Nature. 61079302022</p>
<p>. Alexander Fleming, Penicillin. British medical journal. 242103861941</p>
<p>Frazier Peter, arXiv:1807.02811A tutorial on bayesian optimization. 2018arXiv preprint</p>
<p>Bayesian optimization. Roman Garnett, 2023Cambridge University Press</p>
<p>A survey on the possibilities &amp; impossibilities of AI-generated text detection. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, Amrit Bedi, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Silicon solar cells: evolution, high-efficiency design and efficiency enhancements. Martin A Green, Semiconductor science and technology. 199381</p>
<p>Spotting llms with binoculars: Zeroshot detection of machine-generated text. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, International Conference on Machine Learning. PMLR2024</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Radar: Robust ai-text detection via adversarial learning. Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho, Advances in neural information processing systems. 2023. 202536</p>
<p>Ai-researcher: Autonomous scientific innovation. Tang Jiabin, Xia Lianghao, Li Zhonghang, Huang Chao, 2025</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th symposium on operating systems principles. the 29th symposium on operating systems principles2023</p>
<p>Shinkaevolve: Towards open-ended and sample-efficient program evolution. Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin, arXiv:2509.193492025arXiv preprint</p>
<p>Prompt-based system for personality and interpersonal reactivity prediction. Bin Li, Yixuan Weng, Software Impacts. 121002962022</p>
<p>Artificial text detection with multiple training strategies. Bin Li, Yixuan Weng, Qiya Song, Hanjun Deng, arXiv:2212.051942022arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Jiacheng Miao, Joe R Davis, Jonathan K Pritchard, James Zou, Paper2agent: Reimagining research papers as interactive and reliable ai agents. 2025</p>
<p>Moore's law. Gordon Moore, Electronics Magazine. 3881141965</p>
<p>Alphaevolve: A coding agent for scientific and algorithmic discovery. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Abbas Francisco Jr Ruiz, Mehrabian, Google DeepMind, 05 2025. Technical report</p>
<p>Ai mirrors experimental science to uncover a novel mechanism of gene transfer crucial to bacterial evolution. Juraj José R Penadés, Lingchen Gottweis, He, B Jonasz, Alexander Patkowski, Wei-Hung Shurick, Tao Weng, Anil Tu, Artiom Palepu, Annalisa Myaskovsky, Pawlosky, bioRxiv. 2025</p>
<p>Samuel Schmidgall, Michael Moor, arXiv:2503.18102Agentrxiv: Towards collaborative autonomous research. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227v1Agent laboratory: Using llm agents as research assistants. 2025aarXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025barXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov, The 2023 Conference on Empirical Methods in Natural Language Processing. </p>
<p>The virtual lab of ai agents designs new sars-cov-2 nanobodies. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, Nature. 2025</p>
<p>Skipbert: Efficient inference with shallow layer skipping. Jue Wang, Ke Chen, Gang Chen, Lidan Shou, Julian Mcauley, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Does knowledge localization hold true? surprising differences between entity and relation perspectives in language models. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Huanhuan Ma, Yuanzhe Zhang, Jun Zhao, Kang Liu, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Mastering symbolic operations: Augmenting language models with compiled neural networks. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, The Twelfth International Conference on Learning Representations. </p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao, arXiv:2402.10151Controllm: Crafting diverse personalities for language models. 2024arXiv preprint</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Memory is all you need: An overview of compute-in-memory architectures for accelerating large language model inference. Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, Toyotaro Suzumura, 2024</p>
<p>Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui, 10.18653/v1/2024.findings-acl.456Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>An empirical analysis of uncertainty in large language model evaluations. Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025a</p>
<p>How far are ai scientists from changing the world?. Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, arXiv:2507.232762025barXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Ai becomes a masterbrain scientist. Zijie Yang, Yukai Wang, Lijing Zhang, bioRxiv. 2023</p>
<p>Paul Zarchan, Progress in astronautics and aeronautics: fundamentals of Kalman filtering: a practical approach. Aiaa2005208</p>
<p>Agentracer: Who is inducing failure in the llm agentic systems?. Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan, 2025a</p>
<p>Scaling laws in scientific discovery with ai and robot scientists. Pengsong Zhang, Heng Zhang, Huazhe Xu, Renjun Xu, Zhenting Wang, Cong Wang, Animesh Garg, Zhibin Li, Arash Ajoudani, Xinyu Liu, arXiv:2503.224442025barXiv preprint</p>
<p>Which agent causes task failures and when? on automated failure attribution of LLM multi-agent systems. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu, Forty-second International Conference on Machine Learning. 2025c</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025aarXiv preprint</p>
<p>Personality alignment of large language models. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, The Thirteenth International Conference on Learning Representations. 2025b</p>
<p>Ai scientists fail without strong implementation capability. Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, Yue Zhang, arXiv:2506.013722025carXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
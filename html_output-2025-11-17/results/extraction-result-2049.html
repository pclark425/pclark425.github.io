<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2049 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2049</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2049</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-281658906</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.22008v1.pdf" target="_blank">Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Real-world decision-making tasks typically occur in complex and open environments, posing significant challenges to reinforcement learning (RL) agents'exploration efficiency and long-horizon planning capabilities. A promising approach is LLM-enhanced RL, which leverages the rich prior knowledge and strong planning capabilities of LLMs to guide RL agents in efficient exploration. However, existing methods mostly rely on frequent and costly LLM invocations and suffer from limited performance due to the semantic mismatch. In this paper, we introduce a Structured Goal-guided Reinforcement Learning (SGRL) method that integrates a structured goal planner and a goal-conditioned action pruner to guide RL agents toward efficient exploration. Specifically, the structured goal planner utilizes LLMs to generate a reusable, structured function for goal generation, in which goals are prioritized. Furthermore, by utilizing LLMs to determine goals'priority weights, it dynamically generates forward-looking goals to guide the agent's policy toward more promising decision-making trajectories. The goal-conditioned action pruner employs an action masking mechanism that filters out actions misaligned with the current goal, thereby constraining the RL agent to select goal-consistent policies. We evaluate the proposed method on Crafter and Craftax-Classic, and experimental results demonstrate that SGRL achieves superior performance compared to existing state-of-the-art methods.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2049.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2049.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Goal-guided Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-enhanced RL method that uses an LLM to synthesize a reusable, parameterized goal-generation function that outputs prioritized goals, together with an LLM-guided goal-conditioned action pruner and PPO agent, to improve exploration and long-horizon planning in open-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>DeepSeek-R1 (goal planner), DeepSeek-V3 (action pruner / baseline replication)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>An LLM is prompted (multi-stage: design, implementation, reflection) to produce executable Python code that implements a goal-generation function ϕ(s): given textual state observations it returns k candidate goals (g_i) each with normalized priority weights (w_i). The generated function is reusable and updated/optimized by the LLM only periodically (priority values updated every ~2M environment steps). During runtime the planner returns top-k forward-looking goals; a goal-action mask bank caches masks for repeated goals and an action pruner (DeepSeek-V3) produces binary masks for actions relevant to goals. A three-phase annealing schedule (three-stage cosine by default) controls how strictly masks constrain policy via stochastic relaxation (ξ). The curriculum thus generates prioritized goals/subgoals conditioned on current text observations, unlocked achievements, and recent trajectory/state, and adapts priorities over training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Crafter and Craftax-Classic</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-world, procedurally generated 2D survival domains with sparse rewards and hierarchical/compositional achievements (22 achievements, dependency depth up to 8). Require deep exploration, long-horizon multi-step tool crafting and resource dependencies, and generalization across procedurally generated maps.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Textual environment observations including map view (visible blocks/mobs), inventory contents, status values (Health, Fullness, Hydration, Wakefulness, Sky brightness), unlocked achievements, and recent state-goal history; the planner updates priorities based on unlocked achievements and agent trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Goal-conditioned action pruner (action masking with mask bank), PPO agent (policy learning), cached goal-action mask bank, annealing schedules for mask strength, ablation variants (static-pruning, no-pruner, no-priority) used for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>On Craftax-Classic at 5M steps: SGRL score 33.8 ± 1.5 vs AdaRefiner 28.5 ± 2.3, ELLM 28.4 ± 2.5, PPO 24.8 ± 5.7 (reported Table 1). SGRL unlocks the deepest achievement 'Collect Diamond' shortly after ~3.7M steps while AdaRefiner, ELLM, and PPO did not unlock it by 5M steps. At 10M steps (Table 2 / ablations) SGRL score 43.9 ± 2.6 and reward 14.9 ± 0.48; ablation and mask-schedule experiments show SGRL achieves superior late-stage success rates on deep achievements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>PPO baseline (no LLM goals) achieved lower performance: score 24.8 ± 5.7 at 5M steps on Craftax-Classic and failed to reach deep achievements by 5M.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td>Not reported as a single numeric diversity metric; planner outputs k candidate goals from a predefined task-specific goal set (examples: collect coal, collect diamond, make iron pickaxe, place furnace, place plant, wake up, etc.). Heatmaps show priority weightings across ~22 achievement goals over 10M steps (see Figure 21), but no explicit 'unique goals discovered' count reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed to minimize LLM invocations: structured goal function is generated/updated infrequently (priority updates every ~2M steps) and a goal-action mask bank caches masks; reported to require minimal LLM invocation and achieve faster training speed (higher SPS) compared to methods that call LLM frequently. Quantitative LLM-call counts or dollar costs are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Two main limitations observed: (1) integrating textual goals into the agent's decision process yields limited immediate benefits in the first ~2M steps in preliminary code-goal experiments; (2) strict/static action masking (over-reliance on LLM masks) gives strong early gains but can converge to suboptimal policies later (SGRL w/ Static-Prun performed worse in late stages). The method depends on LLM semantic alignment; inaccurate masks or priorities can reduce performance, mitigated via stochastic relaxation (annealing).</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>SGRL improves long-horizon planning: successfully unlocks deep achievements (e.g., Collect Diamond) at ~3.7M steps vs baselines that failed by 5M; exhibits higher success rates on late-stage/deep achievements and higher achievement depth in both 5M and 10M evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations on Craftax-Classic (10M steps): SGRL (full) score 43.9 ± 2.6; SGRL w/ Static-Prun 38.5 ± 1.9; SGRL w/o Prun 40.0 ± 2.1; SGRL w/o Priority 35.3 ± 1.6. Ablations show both goal priority weighting and action pruner contribute substantially; static pruning yields faster early progress but worse late-stage outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>LLM-generated structured, reusable goal-generators that output prioritized goals (with periodic priority updates) plus a goal-conditioned action pruner significantly improve exploration efficiency and long-horizon achievement unlocking compared to frequent-query LLM goal generators and pure RL: SGRL outperforms AdaRefiner and ELLM on score and achievement depth (e.g., SGRL 33.8±1.5 vs AdaRefiner 28.5±2.3 at 5M), unlocks Collect Diamond at ~3.7M steps, and requires fewer LLM calls due to reusable code and mask caching. Overly strict LLM constraints harm late-stage learning; adaptive annealing of mask strength (three-stage cosine) yields best balance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2049.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2049.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGoal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGoal (preliminary LLM-generated goal-code approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preliminary idea/experiment where an LLM generates executable code that specifies goals to guide RL agent exploration (i.e., generating goal-specifying code as an interface between LLM and RL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (executable code that outputs goals)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The LLM generates goal-specifying code which when executed outputs textual goals to guide the agent; the code is intended to be reusable as a stable interface, allowing fewer LLM invocations. Preliminary experiments (Figure 1) compare CodeGoal to PPO, ELLM and AdaRefiner on Craftax-Classic.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Craftax-Classic</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-world, hierarchical achievement structure, sparse rewards, procedural generation; same properties as Craftax/Crafter (see SGRL).</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Code is executed with textual state observations as input to produce goals (details implicit in description).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used alongside PPO agent in preliminary experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Preliminary results (Figure 1) show CodeGoal achieves competitive success rates on unlocking key achievements compared to PPO, ELLM and AdaRefiner, demonstrating viability; however, CodeGoal failed to quickly unlock deeper achievements (e.g., Collect Diamond) and showed limited immediate benefits within first ~2M steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>PPO baseline included in Figure 1; CodeGoal was competitive on some achievements but did not accelerate deep achievement unlocking.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Motivated as lower-cost than frequent LLM invocation approaches because code is reusable, but exact costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Limited immediate benefit early in training and inability in preliminary tests to rapidly unlock the deepest achievements; semantic mismatch and insufficient coordination with RL policy noted as challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Underperformed for deepest long-horizon tasks in preliminary tests (failed to quickly unlock Collect Diamond).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Generating a reusable, structured goal-generating function (code) is a viable LLM-to-RL interface but naive code-generation (CodeGoal) shows limited early-stage benefit and struggles with the deepest long-horizon achievements, motivating SGRL's additions (priority weighting, action-pruner, annealing).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2049.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2049.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Guiding pretraining in reinforcement learning with large language models (ELLM - goal generator baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that uses a pre-trained LLM to generate potential exploration goals online to guide RL agent exploration; used as a baseline in experiments and cited as requiring frequent online LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Guiding pretraining in reinforcement learning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (online goal generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>DeepSeek-V3 (used to replicate ELLM results in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>ELLM queries an LLM online during training to generate semantic subgoals / exploration goals conditioned on task description and history τ_h; goal outputs guide agent exploration directly (frequent LLM invocations).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Crafter / Craftax-Classic (reported baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-world, sparse reward, hierarchical achievement dependencies; same as SGRL domains.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>LLM conditioned on state-goal history τ_h and task description ω to produce next goal g_h.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Uses LLM as goal generator, combined with RL agent; relies on frequent online calls; no reusable code/mask caching reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Reported/reproduced within 5M steps: ELLM score ~28.4 ± 2.5 on Craftax-Classic at 5M steps (Table 1) and failed to unlock Collect Diamond by 5M in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: relies on frequent, intensive online LLM invocations (DeepSeek-V3 in replication), incurring substantial computational cost and training time; authors state low practical utility and poor computational efficiency for such methods.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Frequent LLM queries are costly; semantic mismatch and inability to effectively coordinate with RL components can limit performance; fails to achieve deepest achievements within 5M steps in this comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Underperforms on deepest long-horizon achievements compared to SGRL (did not unlock Collect Diamond by 5M in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Frequent online LLM goal generation can improve exploration versus pure RL, but suffers practical cost and, in this study, underperformed SGRL on deep achievements due to cost and semantic/coordination issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2049.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2049.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaRefiner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdaRefiner (adaptive LLM goal refinement baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based method that iteratively refines task understanding through feedback from the RL agent to improve quality of LLM-generated goals; used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adarefiner: Refining decisions of language models with adaptive feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated with adaptive prompt refinement</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>DeepSeek-V3 (used to replicate AdaRefiner results in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>AdaRefiner iteratively refines LLM prompts/goals using feedback from the RL agent to improve goal quality; requires frequent LLM queries during training (adaptive refinement), producing semantic subgoals to steer exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Craftax-Classic (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Open-world, sparse rewards, hierarchical achievements (same as Crafter), long-horizon dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>LLM conditioned on task descriptions and agent feedback to refine goals; specifics follow AdaRefiner design (agent feedback loop).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>LLM goal generator with adaptive feedback loop to refine goals; paired with RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Reproduced results within 5M steps: AdaRefiner score ~28.5 ± 2.3 on Craftax-Classic at 5M (Table 1); failed to unlock Collect Diamond by 5M in comparisons, while SGRL did at ~3.7M.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High: requires frequent DeepSeek-V3 calls during training for adaptive refinement, resulting in substantial computational cost and training time (explicitly noted by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Frequent querying is computationally costly; in experiments, AdaRefiner did not achieve the deepest achievements as quickly as SGRL and shows lower scores at 5M steps.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Underperforms relative to SGRL on deep long-horizon achievements in this paper's experiments (did not unlock Collect Diamond by 5M).</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Adaptive prompt refinement can improve goal quality but frequent online queries raise computational cost; in this comparison AdaRefiner was outperformed by SGRL in score and achievement depth at 5M steps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2049.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2049.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek family (DeepSeek-R1, DeepSeek-V3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary / referenced LLM models used in experiments: DeepSeek-R1 used to generate structured goal-generating code, and DeepSeek-V3 used for selecting actions related to goals and to replicate baseline LLM methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-r1 incentivizes reasoning in llms through reinforcement learning; Deepseek-v3 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (models used for goal-code generation and action pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>DeepSeek-R1 (goal code generation), DeepSeek-V3 (action selection and baseline replication)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>DeepSeek-R1: prompted in multi-stage design/implementation/reflection to output Python code implementing goal generation. DeepSeek-V3: used to produce goal-conditioned action masks and to reproduce behaviors of prior LLM-baseline methods (ELLM/AdaRefiner) for comparison; decoding parameters reported (temperature 0.5, top-p 1.0, max tokens 100).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Used in Crafter / Craftax-Classic experiments</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Same open-world, sparse-reward, hierarchical composition.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Prompts include textual environment observations, inventory, status, goal history, and periodic priority update signals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used in conjunction with mask bank, PPO agent, and the structured goal pipeline in SGRL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>Not evaluated as an independent curriculum; used as the engine for SGRL's goal-code generation (DeepSeek-R1) and action-pruner / baseline replication (DeepSeek-V3). Specific model-level scaling/performance not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Decoding hyperparameters reported; authors emphasize SGRL minimizes LLM calls vs frequent-query baselines that used DeepSeek-V3 often, but no per-call latency or dollar cost reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Model sizes and exact capabilities not disclosed; reliance on general-purpose LLMs can produce inaccurate masks or priorities requiring stochastic relaxation.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Using an LLM to synthesize reusable, structured goal-generation code (rather than issuing frequent online queries) reduces invocation frequency while preserving effective goal guidance; DeepSeek family served as the LLM backbone for these mechanisms in experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Guiding pretraining in reinforcement learning with large language models <em>(Rating: 2)</em></li>
                <li>Adarefiner: Refining decisions of language models with adaptive feedback <em>(Rating: 2)</em></li>
                <li>LGTS: Dynamic task sampling using LLM-generated sub-goals for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Words as beacons: guiding rl agents with high-level language prompts <em>(Rating: 2)</em></li>
                <li>From words to actions: Unveiling the theoretical underpinnings of llm-driven autonomous systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2049",
    "paper_id": "paper-281658906",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "SGRL",
            "name_full": "Structured Goal-guided Reinforcement Learning",
            "brief_description": "An LLM-enhanced RL method that uses an LLM to synthesize a reusable, parameterized goal-generation function that outputs prioritized goals, together with an LLM-guided goal-conditioned action pruner and PPO agent, to improve exploration and long-horizon planning in open-world environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated",
            "llm_model_name": "DeepSeek-R1 (goal planner), DeepSeek-V3 (action pruner / baseline replication)",
            "llm_model_size": null,
            "curriculum_description": "An LLM is prompted (multi-stage: design, implementation, reflection) to produce executable Python code that implements a goal-generation function ϕ(s): given textual state observations it returns k candidate goals (g_i) each with normalized priority weights (w_i). The generated function is reusable and updated/optimized by the LLM only periodically (priority values updated every ~2M environment steps). During runtime the planner returns top-k forward-looking goals; a goal-action mask bank caches masks for repeated goals and an action pruner (DeepSeek-V3) produces binary masks for actions relevant to goals. A three-phase annealing schedule (three-stage cosine by default) controls how strictly masks constrain policy via stochastic relaxation (ξ). The curriculum thus generates prioritized goals/subgoals conditioned on current text observations, unlocked achievements, and recent trajectory/state, and adapts priorities over training.",
            "domain_name": "Crafter and Craftax-Classic",
            "domain_characteristics": "Open-world, procedurally generated 2D survival domains with sparse rewards and hierarchical/compositional achievements (22 achievements, dependency depth up to 8). Require deep exploration, long-horizon multi-step tool crafting and resource dependencies, and generalization across procedurally generated maps.",
            "state_conditioning": true,
            "state_conditioning_details": "Textual environment observations including map view (visible blocks/mobs), inventory contents, status values (Health, Fullness, Hydration, Wakefulness, Sky brightness), unlocked achievements, and recent state-goal history; the planner updates priorities based on unlocked achievements and agent trajectory.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Goal-conditioned action pruner (action masking with mask bank), PPO agent (policy learning), cached goal-action mask bank, annealing schedules for mask strength, ablation variants (static-pruning, no-pruner, no-priority) used for analysis.",
            "performance_llm_curriculum": "On Craftax-Classic at 5M steps: SGRL score 33.8 ± 1.5 vs AdaRefiner 28.5 ± 2.3, ELLM 28.4 ± 2.5, PPO 24.8 ± 5.7 (reported Table 1). SGRL unlocks the deepest achievement 'Collect Diamond' shortly after ~3.7M steps while AdaRefiner, ELLM, and PPO did not unlock it by 5M steps. At 10M steps (Table 2 / ablations) SGRL score 43.9 ± 2.6 and reward 14.9 ± 0.48; ablation and mask-schedule experiments show SGRL achieves superior late-stage success rates on deep achievements.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": "PPO baseline (no LLM goals) achieved lower performance: score 24.8 ± 5.7 at 5M steps on Craftax-Classic and failed to reach deep achievements by 5M.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": "Not reported as a single numeric diversity metric; planner outputs k candidate goals from a predefined task-specific goal set (examples: collect coal, collect diamond, make iron pickaxe, place furnace, place plant, wake up, etc.). Heatmaps show priority weightings across ~22 achievement goals over 10M steps (see Figure 21), but no explicit 'unique goals discovered' count reported.",
            "transfer_generalization_results": null,
            "computational_cost": "Designed to minimize LLM invocations: structured goal function is generated/updated infrequently (priority updates every ~2M steps) and a goal-action mask bank caches masks; reported to require minimal LLM invocation and achieve faster training speed (higher SPS) compared to methods that call LLM frequently. Quantitative LLM-call counts or dollar costs are not provided.",
            "failure_modes_limitations": "Two main limitations observed: (1) integrating textual goals into the agent's decision process yields limited immediate benefits in the first ~2M steps in preliminary code-goal experiments; (2) strict/static action masking (over-reliance on LLM masks) gives strong early gains but can converge to suboptimal policies later (SGRL w/ Static-Prun performed worse in late stages). The method depends on LLM semantic alignment; inaccurate masks or priorities can reduce performance, mitigated via stochastic relaxation (annealing).",
            "long_horizon_performance": "SGRL improves long-horizon planning: successfully unlocks deep achievements (e.g., Collect Diamond) at ~3.7M steps vs baselines that failed by 5M; exhibits higher success rates on late-stage/deep achievements and higher achievement depth in both 5M and 10M evaluations.",
            "specialized_domain_performance": null,
            "ablation_studies": "Ablations on Craftax-Classic (10M steps): SGRL (full) score 43.9 ± 2.6; SGRL w/ Static-Prun 38.5 ± 1.9; SGRL w/o Prun 40.0 ± 2.1; SGRL w/o Priority 35.3 ± 1.6. Ablations show both goal priority weighting and action pruner contribute substantially; static pruning yields faster early progress but worse late-stage outcomes.",
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "LLM-generated structured, reusable goal-generators that output prioritized goals (with periodic priority updates) plus a goal-conditioned action pruner significantly improve exploration efficiency and long-horizon achievement unlocking compared to frequent-query LLM goal generators and pure RL: SGRL outperforms AdaRefiner and ELLM on score and achievement depth (e.g., SGRL 33.8±1.5 vs AdaRefiner 28.5±2.3 at 5M), unlocks Collect Diamond at ~3.7M steps, and requires fewer LLM calls due to reusable code and mask caching. Overly strict LLM constraints harm late-stage learning; adaptive annealing of mask strength (three-stage cosine) yields best balance.",
            "uuid": "e2049.0"
        },
        {
            "name_short": "CodeGoal",
            "name_full": "CodeGoal (preliminary LLM-generated goal-code approach)",
            "brief_description": "A preliminary idea/experiment where an LLM generates executable code that specifies goals to guide RL agent exploration (i.e., generating goal-specifying code as an interface between LLM and RL).",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (executable code that outputs goals)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "The LLM generates goal-specifying code which when executed outputs textual goals to guide the agent; the code is intended to be reusable as a stable interface, allowing fewer LLM invocations. Preliminary experiments (Figure 1) compare CodeGoal to PPO, ELLM and AdaRefiner on Craftax-Classic.",
            "domain_name": "Craftax-Classic",
            "domain_characteristics": "Open-world, hierarchical achievement structure, sparse rewards, procedural generation; same properties as Craftax/Crafter (see SGRL).",
            "state_conditioning": true,
            "state_conditioning_details": "Code is executed with textual state observations as input to produce goals (details implicit in description).",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Used alongside PPO agent in preliminary experiments.",
            "performance_llm_curriculum": "Preliminary results (Figure 1) show CodeGoal achieves competitive success rates on unlocking key achievements compared to PPO, ELLM and AdaRefiner, demonstrating viability; however, CodeGoal failed to quickly unlock deeper achievements (e.g., Collect Diamond) and showed limited immediate benefits within first ~2M steps.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": "PPO baseline included in Figure 1; CodeGoal was competitive on some achievements but did not accelerate deep achievement unlocking.",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Motivated as lower-cost than frequent LLM invocation approaches because code is reusable, but exact costs not reported.",
            "failure_modes_limitations": "Limited immediate benefit early in training and inability in preliminary tests to rapidly unlock the deepest achievements; semantic mismatch and insufficient coordination with RL policy noted as challenges.",
            "long_horizon_performance": "Underperformed for deepest long-horizon tasks in preliminary tests (failed to quickly unlock Collect Diamond).",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Generating a reusable, structured goal-generating function (code) is a viable LLM-to-RL interface but naive code-generation (CodeGoal) shows limited early-stage benefit and struggles with the deepest long-horizon achievements, motivating SGRL's additions (priority weighting, action-pruner, annealing).",
            "uuid": "e2049.1"
        },
        {
            "name_short": "ELLM",
            "name_full": "Guiding pretraining in reinforcement learning with large language models (ELLM - goal generator baseline)",
            "brief_description": "Prior method that uses a pre-trained LLM to generate potential exploration goals online to guide RL agent exploration; used as a baseline in experiments and cited as requiring frequent online LLM calls.",
            "citation_title": "Guiding pretraining in reinforcement learning with large language models",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (online goal generation)",
            "llm_model_name": "DeepSeek-V3 (used to replicate ELLM results in this paper)",
            "llm_model_size": null,
            "curriculum_description": "ELLM queries an LLM online during training to generate semantic subgoals / exploration goals conditioned on task description and history τ_h; goal outputs guide agent exploration directly (frequent LLM invocations).",
            "domain_name": "Crafter / Craftax-Classic (reported baselines)",
            "domain_characteristics": "Open-world, sparse reward, hierarchical achievement dependencies; same as SGRL domains.",
            "state_conditioning": true,
            "state_conditioning_details": "LLM conditioned on state-goal history τ_h and task description ω to produce next goal g_h.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Uses LLM as goal generator, combined with RL agent; relies on frequent online calls; no reusable code/mask caching reported.",
            "performance_llm_curriculum": "Reported/reproduced within 5M steps: ELLM score ~28.4 ± 2.5 on Craftax-Classic at 5M steps (Table 1) and failed to unlock Collect Diamond by 5M in reported comparisons.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "High: relies on frequent, intensive online LLM invocations (DeepSeek-V3 in replication), incurring substantial computational cost and training time; authors state low practical utility and poor computational efficiency for such methods.",
            "failure_modes_limitations": "Frequent LLM queries are costly; semantic mismatch and inability to effectively coordinate with RL components can limit performance; fails to achieve deepest achievements within 5M steps in this comparison.",
            "long_horizon_performance": "Underperforms on deepest long-horizon achievements compared to SGRL (did not unlock Collect Diamond by 5M in experiments).",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Frequent online LLM goal generation can improve exploration versus pure RL, but suffers practical cost and, in this study, underperformed SGRL on deep achievements due to cost and semantic/coordination issues.",
            "uuid": "e2049.2"
        },
        {
            "name_short": "AdaRefiner",
            "name_full": "AdaRefiner (adaptive LLM goal refinement baseline)",
            "brief_description": "An LLM-based method that iteratively refines task understanding through feedback from the RL agent to improve quality of LLM-generated goals; used as a baseline in experiments.",
            "citation_title": "Adarefiner: Refining decisions of language models with adaptive feedback",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated with adaptive prompt refinement",
            "llm_model_name": "DeepSeek-V3 (used to replicate AdaRefiner results in this paper)",
            "llm_model_size": null,
            "curriculum_description": "AdaRefiner iteratively refines LLM prompts/goals using feedback from the RL agent to improve goal quality; requires frequent LLM queries during training (adaptive refinement), producing semantic subgoals to steer exploration.",
            "domain_name": "Craftax-Classic (evaluated as baseline)",
            "domain_characteristics": "Open-world, sparse rewards, hierarchical achievements (same as Crafter), long-horizon dependencies.",
            "state_conditioning": true,
            "state_conditioning_details": "LLM conditioned on task descriptions and agent feedback to refine goals; specifics follow AdaRefiner design (agent feedback loop).",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "LLM goal generator with adaptive feedback loop to refine goals; paired with RL agent.",
            "performance_llm_curriculum": "Reproduced results within 5M steps: AdaRefiner score ~28.5 ± 2.3 on Craftax-Classic at 5M (Table 1); failed to unlock Collect Diamond by 5M in comparisons, while SGRL did at ~3.7M.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "High: requires frequent DeepSeek-V3 calls during training for adaptive refinement, resulting in substantial computational cost and training time (explicitly noted by the authors).",
            "failure_modes_limitations": "Frequent querying is computationally costly; in experiments, AdaRefiner did not achieve the deepest achievements as quickly as SGRL and shows lower scores at 5M steps.",
            "long_horizon_performance": "Underperforms relative to SGRL on deep long-horizon achievements in this paper's experiments (did not unlock Collect Diamond by 5M).",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Adaptive prompt refinement can improve goal quality but frequent online queries raise computational cost; in this comparison AdaRefiner was outperformed by SGRL in score and achievement depth at 5M steps.",
            "uuid": "e2049.3"
        },
        {
            "name_short": "DeepSeek LLMs",
            "name_full": "DeepSeek family (DeepSeek-R1, DeepSeek-V3)",
            "brief_description": "Proprietary / referenced LLM models used in experiments: DeepSeek-R1 used to generate structured goal-generating code, and DeepSeek-V3 used for selecting actions related to goals and to replicate baseline LLM methods.",
            "citation_title": "Deepseek-r1 incentivizes reasoning in llms through reinforcement learning; Deepseek-v3 technical report",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-generated (models used for goal-code generation and action pruning)",
            "llm_model_name": "DeepSeek-R1 (goal code generation), DeepSeek-V3 (action selection and baseline replication)",
            "llm_model_size": null,
            "curriculum_description": "DeepSeek-R1: prompted in multi-stage design/implementation/reflection to output Python code implementing goal generation. DeepSeek-V3: used to produce goal-conditioned action masks and to reproduce behaviors of prior LLM-baseline methods (ELLM/AdaRefiner) for comparison; decoding parameters reported (temperature 0.5, top-p 1.0, max tokens 100).",
            "domain_name": "Used in Crafter / Craftax-Classic experiments",
            "domain_characteristics": "Same open-world, sparse-reward, hierarchical composition.",
            "state_conditioning": true,
            "state_conditioning_details": "Prompts include textual environment observations, inventory, status, goal history, and periodic priority update signals.",
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": "Used in conjunction with mask bank, PPO agent, and the structured goal pipeline in SGRL.",
            "performance_llm_curriculum": "Not evaluated as an independent curriculum; used as the engine for SGRL's goal-code generation (DeepSeek-R1) and action-pruner / baseline replication (DeepSeek-V3). Specific model-level scaling/performance not reported.",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Decoding hyperparameters reported; authors emphasize SGRL minimizes LLM calls vs frequent-query baselines that used DeepSeek-V3 often, but no per-call latency or dollar cost reported.",
            "failure_modes_limitations": "Model sizes and exact capabilities not disclosed; reliance on general-purpose LLMs can produce inaccurate masks or priorities requiring stochastic relaxation.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Using an LLM to synthesize reusable, structured goal-generation code (rather than issuing frequent online queries) reduces invocation frequency while preserving effective goal guidance; DeepSeek family served as the LLM backbone for these mechanisms in experiments.",
            "uuid": "e2049.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Guiding pretraining in reinforcement learning with large language models",
            "rating": 2
        },
        {
            "paper_title": "Adarefiner: Refining decisions of language models with adaptive feedback",
            "rating": 2
        },
        {
            "paper_title": "LGTS: Dynamic task sampling using LLM-generated sub-goals for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Words as beacons: guiding rl agents with high-level language prompts",
            "rating": 2
        },
        {
            "paper_title": "From words to actions: Unveiling the theoretical underpinnings of llm-driven autonomous systems",
            "rating": 1
        }
    ],
    "cost": 0.01891575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning
26 Sep 2025</p>
<p>Yajie Qi 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Wei Wei 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Lin Li 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Lijun Zhang 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Zhidong Gao 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Da Wang 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Huizhong Song 
School of Computer and Information Technology
Shanxi University
TaiyuanShanxiChina</p>
<p>Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning
26 Sep 202540C40744A1D7335FCA07EA86451FAEDEarXiv:2509.22008v1[cs.LG]You see: grasssand... Inventory: wood: 3 Status: Health: 100%Fullness: 100%Hydration: 88%Wakefulness: 100%Sky brightness level: 78% Obs_text Obs_pixel
Real-world decision-making tasks typically occur in complex and open environments, posing significant challenges to reinforcement learning (RL) agents' exploration efficiency and long-horizon planning capabilities.A promising approach is LLM-enhanced RL, which leverages the rich prior knowledge and strong planning capabilities of LLMs to guide RL agents in efficient exploration.However, existing methods mostly rely on frequent and costly LLM invocations and suffer from limited performance due to the semantic mismatch.In this paper, we introduce a Structured Goal-guided Reinforcement Learning (SGRL) method that integrates a structured goal planner and a goal-conditioned action pruner to guide RL agents toward efficient exploration.Specifically, the structured goal planner utilizes LLMs to generate a reusable, structured function for goal generation, in which goals are prioritized.Furthermore, by utilizing LLMs to determine goals' priority weights, it dynamically generates forward-looking goals to guide the agent's policy toward more promising decision-making trajectories.The goalconditioned action pruner employs an action masking mechanism that filters out actions misaligned with the current goal, thereby constraining the RL agent to select goal-consistent policies.We evaluate the proposed method on Crafter and Craftax-Classic, and experimental results demonstrate that SGRL achieves superior performance compared to existing state-of-the-art methods.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) has achieved impressive success in addressing challenging decisionmaking tasks across a wide range of domains, such as Atari games (Hessel et al., 2018;Vinyals et al., 2019), robotics (Brunke et al., 2022;Haarnoja et al., 2024), and natural language processing (Padmakumar &amp; Mooney, 2021;Ouyang et al., 2022).However, these remarkable successes have mostly occurred in environments characterized by closed, predefined tasks with clear goals and immediate feedback, which fail to capture the complexity and dynamics of the real world (Team et al., 2021;Cai et al., 2023).In recent years, increasing attention has been devoted to decision-making in openworld environments such as Minecraft (Fan et al., 2022;Lin et al., 2022) and Crafter (Hafner, 2022;Moon et al., 2023), which pose significant challenges in generalization, deep exploration, long-term decision-making, and reasoning.Consequently, solving decision-making problems in open-world environments is widely recognized as a pressing and significant challenge.</p>
<p>Recently, LLM-enhanced RL has been regarded as a promising direction for addressing decisionmaking challenges in open-world environments (Liu et al., 2023;Zhou et al., 2024;He et al., 2024;Schoepp et al., 2025) due to the remarkable capabilities of LLMs in various decision-making and reasoning tasks.A variety of methods, such as using LLMs as decision-makers (Shinn et al., 2023;Carta et al., 2023;Gaven et al., 2024) or as skill planners (Ichter et al., 2022;Zhang et al., 2023;Lin et al., 2023;Yang et al., 2025), have emerged and significantly improved sample efficiency and generalization, thereby enhancing performance.However, these methods still struggle to fully unlock and leverage the planning capabilities of LLMs, as they either force the models to perform fine-grained planning (an area where they are not particularly adept) or fail to effectively coordinate the relationship between the LLM and RL agent components.In order to harness the capabilities of LLMs, recent studies have explored a more direct and effective approach that uses an LLM to generate high-level goals for guiding exploration in the RL agent (Du et al., 2023;Zheng et al., 2024;Shukla et al., 2024;Zhang &amp; Lu, 2024).To mention a few, ELLM (Du et al., 2023)  a pre-trained LLM to generate potential exploration goals, guiding the RL agent to explore towards potentially useful targets.AdaRefiner (Zhang &amp; Lu, 2024) iteratively refines task understanding through feedback from the RL agent, thereby improving the quality of LLM-generated goals and fostering more effective collaboration between LLM and RL agent.However, due to the reliance on frequent and intensive LLM invocations, these methods suffer from low practical utility and poor computational efficiency.
leverages
Inspired by Ma et al. (2024); Xie et al. (2024), in which LLMs generate reward-shaping code to provide immediate feedback to RL agents and achieve strong performance on several benchmarks, we hypothesize that structured, goal-specifying code can also serve as a stable and executable interface between LLMs and RL agents, thereby enabling effective long-horizon exploration.Thus, we conducted preliminary experiments based on the simple idea of leveraging LLM-generated code to guide the exploration of RL agents, which we term CodeGoal.Figure 1 presents the success rate of CodeGoal in comparison with PPO, as well as ELLM (Du et al., 2023) 1 and AdaRefiner (Zhang &amp; Lu, 2024) 2 on the Craftax-Classic benchmark.From Figure 1, we can observe that compared with ELLM (Du et al., 2023), AdaRefiner (Zhang &amp; Lu, 2024), and PPO, CodeGoal achieves competitive success rates when unlocking key achievements, demonstrating that using code-generated goals to guide RL agents is a viable approach.</p>
<p>It is worth noting that the experimental results in Figure 1 also reveal two key limitations: (1) the agent fails to quickly unlock deeper achievements such as Collect Diamond;</p>
<p>(2) integrating textual goals into the RL agent's decision-making process yields limited immediate benefits, evidenced by the apparent ineffectiveness of goal guidance within the first 2M steps.This motivates us to propose a novel LLM-based goal-conditioned RL approach, which first leverages LLMs to generate a parameterized, well-structured, and reusable goal-generation function, and then utilizes the LLMs to optimize both the parameters of this function and the goal-conditioned policy constraints, thereby encouraging RL agents to explore effectively in open-world environments.</p>
<p>The main contributions are summarized as follows:</p>
<p>• We propose Structured Goal-guided Reinforcement Learning (SGRL), an LLM-enhanced RL method that constructs a structured goal-generation function and dynamically adjusts both goal priority weights and goal-conditioned action constraints, thereby significantly improving the RL agent's exploration efficiency and overall performance, while maintaining low LLM invocation frequency and minimal input token consumption per call.</p>
<p>• We develop a structured goal planner that leverages the LLM to construct a reusable, structured goal-generating function that selects forward-looking goals and dynamically adjusts their priority weights during training.Furthermore, a goal-conditioned action pruner is designed to filter out actions misaligned with the goal, thereby constraining agents to select goal-consistent policies.</p>
<p>• Extensive experimental results in the challenging open-world environments Crafter and Craftax-Classic demonstrate that SGRL consistently outperforms or matches existing LLM-enhanced RL methods across multiple metrics, including success rate, total score, cumulative reward, and achievement depth.</p>
<p>Structured Goal Planner</p>
<p>{"Sleep": [0, 0, 0, 0, 0, 0, 1, 0, ..., 0]} {"Find cows": [0, 1, 1, 1, 1, 0, ..., 0]} {"Craft stone pickaxe": [0, ... 1, 0, ...]} ... 2 Preliminary
Obs_text</p>
<p>Goal-Conditioned Reinforcement Learning</p>
<p>A goal-conditioned reinforcement learning (GCRL) (Liu et al., 2022) problem can be formulated as a goal-augmented MDP, denoted as a tuple &lt; S, A, G, p g , ϕ, P, r, ρ 0 , γ &gt;, where S, A and G denote the state space, action space and goal space, respectively; p g and ρ 0 denote the desired goal distribution and initial state distribution, respectively; ϕ : S → G is a mapping function that maps the state to a specific goal; P : S×A → S represents the state transition function; r : S×A×G → R is the reward function; and γ is the discount factor.</p>
<p>At each timestep t, given the state s t and the desired goal g, the agent takes an action a t according to its policy π(a t | s t , g) to interact with the environment and then receives the next state s t+1 and reward r t+1 .The agent's goal is to maximize the expected sum of discounted rewards over the state-goal distribution:
J (π) = Eg∼pg,s0∼ρ0, at∼π(•|st,g) t γ t r (s t , g, a t ) .
(1)</p>
<p>LLM as Goal Planner</p>
<p>Formally, given a natural language task description ω ∈ Ω, a sequence of goals is generated by an LLM-based goal planner.At each planning step h, the LLM takes the state-goal history τ h = {s 1 , g 1 , . . ., s h−1 , g h−1 , s h } as input and generates the next goal:
g h ∼ π LLM (• | τ h , ω),
where the goal g h is expressed in natural language and belongs to the language space L, which contains possible linguistic expressions such as sentences or phrases describing actionable intentions.</p>
<p>For practical purposes, a task-specific subset G ⊆ L is often considered to restrict the goal space to valid and executable instructions.Subsequently, the agent choose a policy π conditioned on both the current environment state s h ∈ S and the goal g h :
a h ∼ π(• | s h , g h ),
where a h ∈ A denotes the agent's action at time step h.</p>
<p>Method</p>
<p>This section develops a novel LLM-enhanced RL method called Structured Goal-guided Reinforcement Learning (SGRL).Figure 2 (c) shows an overview of the SGRL framework, which consists of three key components: (1) a structured goal planner, which receives textual observations from the environment and provides forward-looking goals;</p>
<p>(2) a goal-conditioned action pruner, which takes the goal from the structured goal planner as input and generates an action mask; and (3) an RL agent, which executes actions according to a policy conditioned on the current environmental state, goal and most recent reward.The details are introduced in the next subsections.</p>
<p>Structured Goal Planner</p>
<p>The core task of the structured goal planner is to generate the goal function and optimize the goal priority weights by LLMs through task-specific prompting, as illustrated in Figure 2 (a).Specifically, given a basic task introduction, its rules, and a few code examples, the LLM generates an executable, structured function for goal generation according to task-specific and environmental state-related prompting.Then, in just a few iterations and optimizations, we can obtain a reusable, structured goal generation function with priority weights, which can provide forward-looking goals to guide RL agent exploration efficiency.Furthermore, in actual execution, the structured goal planner adjusts the weights of goals based on the agent's unlocked achievements at different training stages.</p>
<p>Formally, at each timestep t, the structured goal planner constructs a function {(g i t , w i t )} k i=1 ∼ ϕ(s t ), which maps the current state s t ∈ S to a set of k candidate goals g t with priority weights w t .Here, g i t ∈ G denotes the i-th goal from the goal space G, and w i t ∈ [0, 1] is the normalized priority value for that goal, satisfying k i=1 w i = 1.These priority values serve to rank candidate goals to guide the agent's exploration toward the most relevant and achievable objectives.The goal planning function ϕ(s) is constructed directly by LLM with task-specific and current state-related prompts, unlike the method in which the LLM as a goal planner that generates goals g t ∼ π LLM (• | τ t , ω) without explicitly defining a reusable goal-generating function.It is worth noting that the structured goal generation method can provide semantically forward-looking goals to improve the RL agent's exploration efficiency, while maintaining a low LLM invocation frequency and minimal input token consumption per call.</p>
<p>Goal-Conditioned Action Pruner</p>
<p>The core task of the goal-conditioned action pruner is to constrain the agent to select goal-consistent policies, as shown in Figure 2 (b).Specifically, we first leverage the LLM with rich prior knowledge to filter from the candidate action set those actions aligned with the current goal.This filtering is implemented via a goal-action masking mechanism.In practice, we construct a goal-action mask bank to store goal-mask pairs, eliminating redundant LLM queries when the same goal reappears.</p>
<p>Formally, at each timestep t, the action pruner produces a binary mask m(g i t ) ∈ 0, 1 |A| base on the candidate goals {g 1 t , . . ., g k t }, which are obtained from the LLM or the goal-action mask bank.Each element of m(g i t ) indicates whether the action is relevant to the goal g i t .Then, the action pruner extracts actions relevant to any given goal g i t in an element-wise fashion as follows:
M = max i=1,...,k m(g i t ).(2)
However, it is worth noting that strictly adhering to goal guidance at all times is not always optimal, especially when using a general-purpose LLM without fine-tuning or domain-specific expertise.Therefore, we introduce a masking coefficient that grants the RL agent some autonomy in decision-making.Specifically, we design a three-phase cosine annealing schedule for the exploration coefficient ξ ∈ [0, 1], which gradually adjusts the strength of action masking throughout training.Specifically:
ξ(t) =        1 2 1 + cos t 0.4T • π , 0 ≤ t &lt; 0.4T 1 2 1 − cos t−0.4T 0.4T • π , 0.4T ≤ t &lt; 0.8T 1.0, t ≥ 0.8T .(3)
Then, the coefficient ξ is used to stochastically relax the mask via a Bernoulli sampling mechanism:
M j = max M j , Bernoulli(ξ • (1 − M j )) ,(4)
where M j is the the j −th element of M , j = 1, . . ., |A|.Equation (4) allows masked-out actions to be sampled with a small probability, thereby mitigating policy rigidity caused by inaccurate masks or shifts in environmental dynamics.Notably, we provide the performance comparison of algorithms with different masking strategies in Appendix F.</p>
<p>RL Agent</p>
<p>In this subsection, we introduce how the RL agent makes decisions based on its current state, as well as goals and action mask constraints.</p>
<p>First, the goal set with priority weights {(g .Then, based on the action mask M , the raw logits output can be obtained to enforce action feasibility:
logits = actor logits ⊙ M + (1 − M ) • (−C),
(5) where ⊙ denotes element-wise multiplication, and C ≫ 0 is a sufficiently large constant (e.g., 10 6 ) that suppresses the logits of invalid actions to near negative infinity.</p>
<p>In practice, the policy π θ (a t | s t , g emb t ) of the RL agent is updated using the PPO algorithm (Schulman et al., 2017) with state augmentation by optimizing the following objective:
L π = E st,g emb t ∼D, at∼πold(•|st,g emb t ), st+1∼P(•|st,at) min π θ (a t | s t , g emb t ) π old (a t | s t , g emb t ) Ât , clip π θ (a t | s t , g emb t ) π old (a t | s t , g emb t ) , 1 − ϵ, 1 + ϵ Ât ,(6)
where Ât is the estimated advantage function, ϵ is the clipping parameter, and D denotes the replay buffer or on-policy rollout distribution.The mask M affects both the behavior policy π old and the updated policy π θ , ensuring consistency between sampling and optimization.</p>
<p>4 Related Works 4.1 Open-World Environments Open-world environments (Team et al., 2021;Cai et al., 2023) are inherently challenging due to requirements for generalization, exploration, multi-objective optimization, and long-horizon planning and reasoning (Hafner, 2022;Wang et al., 2023).There are three main approaches for applying RL in open-world environments in the existing literature.One approach is hierarchical reinforcement learning (Hutsebaut-Buysse et al., 2022), which simplifies complex decision-making processes by constructing a multi-level subtask structure.However, due to the inherent limitations of reinforcement learning algorithms in planning and reasoning, the generalization and long-term decision-making capabilities of these methods are still constrained.Another approach is modelbased RL (Moerland et al., 2023;Walker et al., 2023), which learns an explicit environment dynamics model to enable more sample efficient through simulated rollouts.However, these methods require learning an accurate world model, which results in significantly higher computational overhead, particularly in open-world environments with high-dimensional observations.With the rapid development of LLMs, recent studies have explored integrating LLMs into RL pipelines (Zhou et al., 2024;Schoepp et al., 2025).Leveraging their extensive prior knowledge, reasoning capabilities, and strong generalization, LLMs have been employed to provide high-level planning for RL agents (Du et al., 2023;Zhang &amp; Lu, 2024;Prakash et al., 2023;Yan et al., 2025), which are discussed in detail in the following subsection.</p>
<p>LLM-Enhanced RL</p>
<p>LLM-enhanced RL (Zhou et al., 2024;Schoepp et al., 2025), in which LLMs are employed as goal generators or policy selectors, with the core idea being to exploit their extensive prior knowledge for more effective task decomposition and decision-making.To mention a few, SayCan (Ichter et al., 2022), BOSS (Zhang et al., 2023) and When2Ask (Hu et al., 2024) utilize LLMs as skill planner to construct high-level plans or feasible skill sequences base on natural language instructions or task descriptions.However, these methods rely on assumed access to pretrained skills, and the generated plans lack structured representations, limiting their scalability and adaptability.Furthermore, some works (Hu &amp; Sadigh, 2023;Prakash et al., 2023;Yan et al., 2025) take a more direct straightforward approach, utilizing LLMs as policy teacher.Due to the fact that these methods either rely on a library of pretrained skills for high-level decision-making or depend on the LLMs' reasoning and language-to-action capabilities for low-level guidance, they typically require significant computational resources or extensive pretraining infrastructure.In addition, ELLM (Du et al., 2023), AdaRefiner (Zhang &amp; Lu, 2024), LLMV-AgE (Chi et al., 2025) and Ruiz-Gonzalez et al. (2024) employ LLMs as goal generators to produce semantic subgoals that guide exploration.Unfortunately, due to their reliance on frequent and intensive LLM invocations, these methods suffer from low practical utility and poor computational efficiency.</p>
<p>Experiments</p>
<p>In this section, experiments are conducted on two open-world RL benchmarks: Crafter (Hafner, 2022) and Craftax-Classic (Matthews et al., 2024).The experiments aim to answer the following questions: 1) How does the exploration efficiency of SGRL compare with existing LLM-enhanced RL methods?2) How do goal-conditioned policy constraints contribute to the performance of SGRL?</p>
<p>To answer these questions, we compare SGRL against the following algorithms: ELLM (Du et al., 2023), which generates goals to guide agent exploration through online queries of an LLM; AdaRefiner (Zhang &amp; Lu, 2024), which enhances the quality of the LLM-generated goals by refining the prompts; and PPO (Schulman et al., 2017), which is a pure RL algorithm that does not involve an LLM.Notably, for the specific hyperparameter settings of the PPO algorithm, we follow the Stable-Baselines3 (Raffin et al., 2021) 3 implementation for Crafter, and follow the official in Craftax benchmark (Matthews et al., 2024) 4 implementation for Craftax-Classic.In addition, human expert performance (Hafner, 2022) is included as a reference.</p>
<p>Experimental Setup and Evaluation Metrics</p>
<p>Environment</p>
<p>Crafter is a widely used benchmark for open-world environments, evaluating agents on generalization, exploration, and long-term reasoning through 22 diverse achievements.The Craftax-Classic environment re-implements Crafter in JAX.Both benchmarks feature sparse rewards, complex goal hierarchies, and open-world exploration, making them ideal for evaluating our framework's ability to provide structured semantic guidance.Further details of the environmental setup are provided in Appendix A.1.</p>
<p>The evaluation metrics from Matthews et al. (2024), including success rate, score, return, and achievement depth, to comprehensively assess the performance of SGRL compared to the baseline algorithms.In addition, the training speed is reported in steps per second (SPS), presented in the tables as SPS (×10 2 ) for readability.Further details can be found in Appendix A.2.</p>
<p>Compute Resources</p>
<p>Experiments on Crafter were conducted using a single A100 GPU with 40 GB of VRAM.Experiments on Craftax-Classic were performed on a system equipped with an NVIDIA GeForce RTX 4090 (24 GB) and an Intel(R) Core(TM) i9-14900K CPU.Results for both our algorithm and baseline methods are based on the same configurations.All reported results are averaged over five random seeds and learning curves are smoothed over time.
H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6*5/ $GD5HILQHU (//0 332
Figure 4: Success rates across all achievements on Craftax-Classic at 5M steps.</p>
<p>Main Results</p>
<p>Figure 3 shows the success rate curves for all 22 achievements on Craftax-Classic.From Figure 3, we can observe that SGRL consistently achieves higher success rates than the baselines when reaching the final few achievements (see Figure 3 (q)-(v)).However, for achievements like Wake Up (see Figure 3 (c)) and Place Plant (see Figure 3 (h)), which do not facilitate later exploration, SGRL's performance plateaus once high success rates are reached.This phenomenon demonstrates that our goal-generation method produces farsighted objectives, enabling SGRL to transcend short-term rewards and maintain a stable and coherent policy in long-horizon decision-making tasks.Moreover, as shown in Figure 3 (v), SGRL successfully unlocks the Collect Diamond achievement shortly after 3.7M steps, whereas AdaRefiner, ELLM, and PPO fail to achieve it even by 5M steps, demonstrating SGRL's effectiveness in enhancing exploration efficiency.Figure 4 provides a more direct visualization, which clearly highlights SGRL's advantage in unlocking late-stage achievements, confirming its capability for effective long-horizon planning.More experimental results can be found in Appendix D.   Table 1 summarizes the overall performance of SGRL and baseline methods on Craftax-Classic at 5M steps.As demonstrated in Table 1, SGRL outperforms AdaRefiner, ELLM, and PPO in terms of overall score and achievement depth on Craftax-Classic.In addition, SGRL maintains a high reward level, yet does not exhibit a significant advantage in this regard.We attribute this to the misalignment between reward magnitude and exploration depth in the environment, which forces a trade-off between completing simple, reliably rewarded achievements and pursuing more challenging but potentially high-impact ones.This is evidenced by human performance: experts achieve very high scores without a corresponding increase in total reward.Further, SGRL requires only minimal LLM invocation, resulting in faster training speed.Moreover, to better illustrate the performance of the proposed method, we provide results for various algorithms in the Crafter environment, which can be found in Appendix D.</p>
<p>Method
H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6<em>5/ 6</em>5/Z6WDWLF3UXQ 6<em>5/ZR3UXQ 6</em>5/ZR3ULRULW\</p>
<p>Ablation Study</p>
<p>Ablation Variants</p>
<p>To evaluate the contributions of each component of SGRL, we conducted ablation studies using three variants of SGRL on Craftax-Classic as follows: (1) SGRL w/ Static-Prun: This variant retains the action pruning mechanism but replaces the adaptive pruning coefficient with a static masking scheme;</p>
<p>(2)SGRL w/o Prun: This variant removes the goal-conditioned action pruner entirely;</p>
<p>(3)SGRL w/o Priority: This variant removes the priority assignment for goals.</p>
<p>Ablation Analysis</p>
<p>Figure 5 shows the success rate curves of SGRL and its ablation variants for all 22 achievements on Crafter.From Figure 5, we can observe that SGRL significantly outperforms all variants on most of the deeper achievements after approximately 8M steps, indicating that both components of our algorithm contribute substantially to its performance.Moreover, it is noteworthy that SGRL w/ Static-Prun, a variant of SGRL with strict action masking, exhibits strong early performance but is ultimately outperformed by SGRL in later stages, suggesting that over-reliance on the LLM may lead the agent to converge to suboptimal policies (see Figure 5 (c), (o), (p) and (q)).Furthermore, for an in-depth analysis of SGRL's superiority, Figure 7 presents the goals with priority weights on Craftax-Classic.For clarity, a higherlevel goal is visualized only from the point at which it is assigned a non-zero weight.The figure shows that SGRL consistently assigns a small priority weight to more forward-looking goals compared to the ablation algorithms at earlier stage, which motivates its agent to begin exploring more challenging achievements sooner.We hypothesize that this is the primary driving force behind SGRL's rapid attainment of superior performance.For more detailed results, refer to Appendix E.2, where we present the heatmap of goals with priority weights and provide a detailed analysis.</p>
<p>In summary, all ablation results collectively indicate that both the goal priority weights and action mask in our proposed SGRL play distinct roles.Specifically, assigning priorities to goals within the structured goal planner is crucial for generating reasonable and effective goals.The goal-conditioned action pruner effectively enhances the agent's exploration capability in long-horizon tasks.</p>
<p>Conclusion</p>
<p>This paper proposes a novel LLM-enhanced RL method called SGRL that leverages LLMs to improve RL agents' exploration efficiency and long-horizon planning capabilities in open-world environments.Specifically, we develop a structured goal planner that leverages the LLM to construct reusable goal-generating functions that select forward-looking goals and dynamically adjust their priority weights.Then, a goal-conditioned action pruner is designed to filter out actions misaligned with the goal, thereby guiding RL agents to select goal-consistent policies.Finally, extensive experimental results demonstrate that SGRL achieves superior performance compared to existing LLMenhanced RL baselines in terms of long-horizon planning and exploration efficiency.</p>
<p>A Environments and Evaluation Metrics</p>
<p>A.2 Evaluation Metrics</p>
<p>To demonstrate the effectiveness of our algorithm, we introduce the evaluation metrics as follows:</p>
<p>• Achievement Success Rate.This metric reflects the agent's learning capability and exploration depth by measuring the probability that each predefined achievement is successfully unlocked.</p>
<p>• Geometric Mean Score.This metric reflects the balance between both easy and difficult goals.Following the official Crafter evaluation protocol (Hafner, 2022), it is defined as:
score = exp 1 N N i=1 log(1 + ϱ i ) − 1,
where ϱ i ∈ [0, 100] denotes the success rate of the i-th achievement, and N is the total number of predefined achievements.</p>
<p>• Achievement Depth.This metric measures the agent's exploration depth based on the furthest achievement it unlocks.</p>
<p>• Episode Return.This metric reports the cumulative reward received per episode.</p>
<p>• Steps Per Second (SPS): This metric measures the number of environment steps processed per second, indicating the computational efficiency and speed of learning for each method.</p>
<p>B Implementation Details B.1 LLM</p>
<p>We utilize DeepSeek-R1 (Guo et al., 2025) model to generate structured goal-generating planner code.Additionally, we use DeepSeek-V3 (Liu et al., 2024) model for selecting actions related to the defined goals.For all LLM queries, we follow the implementation of AdaRefiner (Zhang &amp; Lu, 2024) to set the decoding parameters: a temperature of 0.5, top-p of 1.0, and a maximum token limit of 100.DeepSeek-V3 model is employed to replicate the results of ELLM and AdaRefiner.</p>
<p>B.2 Text Embedding</p>
<p>For text embedding, we use paraphrase-MiniLM-L6-v2 (Reimers &amp; Gurevych, 2019) model as the encoder.</p>
<p>B.3 Prompt Design</p>
<p>B.3.1 Prompt Design for Structured Goal Planner</p>
<p>In the Structured Goal Planner, the large model is employed to generate goal-generation code and to update the priorities of goals.To enable the LLM to produce high-quality code, we adopt a multi-stage prompting process:</p>
<p>• Design Stage.At this stage, the model is asked to first design the class structure and key functional modules according to the task requirements.• Implementation Stage.After the design, the model is prompted to output detailed, complete Python code that follows PEP8 standards with clear logic and sufficient comments.• Reflection and Revision Stage.Finally, the model is prompted to reflect on the generated code, identify potential issues, and provide corrections or optimizations.</p>
<p>To guide the model in updating goal priorities, the prompt additionally specifies that every 2 million steps the LLM should update the priority values of goals within the generated code.This ensures that goal selection remains dynamic and aligned with the agent's current objectives.</p>
<p>Prompt Template for Structured Goal Planner Design</p>
<p>Crafter is a 2D open-world survival game with visual input; its world is procedurally generated.Players must search for food and water, find shelter to sleep, defend against monsters, gather materials, and craft tools.Crafter's objective is to evaluate an agent's capabilities through a series of semantically meaningful achievements that can be unlocked in each playthrough-for example, discovering resources and crafting tools.Consistently unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning.</p>
<p>You are an experienced Python developer.The task is to create a key functional module of an advanced goal-generation system that can dynamically produce prioritized goals based on textual environment observations.The system must include functions for survival-need assessment, a tool-crafting tree, resource-collection configuration, and achievement tracking.You should primarily design the OptimizedGoalGenerator class structure and its key functional modules.The Agent will call the determine goal function to obtain goals:
def determine_goal(self, text_obs): return top_three_goal
Each goal is a dictionary of the form: collect coal, collect diamond, collect drink, collect iron, collect sapling, collect stone, collect wood, defeat skeleton, defeat zombie, eat cow, eat plant, make iron pickaxe, make iron sword, make stone pickaxe, make stone sword, make wood pickaxe, make wood sword, place furnace, place plant, place stone, place table, wake up Environment Text Rendering Function: def render_craftax_text_describ_2(self, view_arr, index):</p>
<p>(map_view, mob_map, inventory_values, status_values) = view_arr mob_id_to_name = ["zombie", "cows", "skeletons", "arrows"] block_id_to_name = ["invalid", "out of bounds", "grass", "water", "stone", "tree", "wood", "path", "coal", "iron", "diamond", "crafting table", "furnace", "sand", "lava", "plant", "ripe plant"] inv_names = ["wood", "stone", "coal", "iron", "diamond", "sapling", "wood pickaxe", "stone pickaxe", "iron pickaxe", "wood sword", "stone sword", "iron sword"] text_obs_inv = ", ".join([f"{name}: {value}" for name, value in zip(inv_names, inventory_values[index]) if value &gt; 0]) status_names = ["Health", "Fullness", "Hydration", "Wakefulness", "Sky brightness level"] You are an expert Python developer and code reviewer for Crafter's goal generation system.Your task is to critically analyze the previously designed goal planner and provide an evaluation.For each submitted code version:</p>
<p>• If the code is complete, correct, and efficiently implements all required functionalities (survival needs assessment, resource collection, crafting, achievement tracking, threat handling, and goal prioritization), label it as good.</p>
<p>• If there are issues, missing features, or opportunities for optimization, label it as bad, and provide a clear explanation of the problems.</p>
<p>After evaluation, if the code is labeled bad, generate a fully optimized and corrected version of the code that addresses all identified issues.The optimized code should:</p>
<p>• Correctly handle all environmental observations and edge cases.</p>
<p>• Properly assess and prioritize survival needs.</p>
<p>• Integrate resource collection, crafting, and achievement goals with correct dependencies.</p>
<p>• Handle threats and defensive behaviors appropriately.• Read the goal generation code and the agent state/trajectory.</p>
<p>• Update the numeric priority weights in the code to reflect the current importance of each goal.</p>
<p>• Do not change any function definitions, logic, or class structures.Only modify numeric values associated with goal priorities.</p>
<p>• Ensure the updated code is fully executable and maintains its original structure.</p>
<p>• Keep all interfaces unchanged so that the planner can call the updated code directly.Example Workflow:</p>
<ol>
<li>
<p>Load the goal generation code and parse the goals.</p>
</li>
<li>
<p>Analyze the agent's current state and past actions.</p>
</li>
<li>
<p>Determine new priority weights for each goal.4. Replace only the priority numbers in the code with the updated values.</p>
</li>
</ol>
<p>Output the complete updated Python code.</p>
<p>Output Format:</p>
<p>• Return the entire Python code as a single code block.</p>
<p>• Ensure all class and function definitions remain intact.</p>
<p>• Only the numeric priority values are changed.</p>
<p>B.3.2 Prompt Design for Goal-Conditioned Action Pruner</p>
<p>In the Goal-Conditioned Action Pruner, the large model selects actions that are directly relevant to a given goal through the use of prompts, thereby enabling goal-driven behavior.To guide the model in generating goal-consistent actions, the prompt explicitly defines the meaning of each action in the action space and provides examples illustrating which actions should be chosen for specific goals.In this way, the model can effectively filter actions that align with the goal, ensuring that the agent maintains coherent and goal-directed behavior during execution.</p>
<p>Prompt Template for Goal-Conditioned Action Pruner Implementation</p>
<p>You are the goal analyst for Crafter games, and a goal planner provides goal guidance for game characters.The agent needs to perform one or more steps to achieve this goal, and you help the agent choose the appropriate actions to accomplish it.Tips:</p>
<p>• The goal is something that the intelligent agent is currently capable of executing under certain conditions.</p>
<p>• The intelligent agent may need to move to a certain location to trigger the execution condition.</p>
<p>Prompt Template for Goal-Conditioned Action Pruner Implementation (continued)</p>
<p>• The action space consists of the following 17 actions:</p>
<p>1</p>
<p>D Additional Main Results</p>
<p>E.2 Heatmap of Goal with Priority Weights</p>
<p>Figure 21 show the heatmap of the goals with priority weights generated by the structured goal planner on Craftax-Classic within 10M steps.The vertical axis on the left shows goals ranked from low to high, while the right axis (ranging from 0 to 0.8) indicates the corresponding weights.</p>
<p>From Figure 21, we can observe the following key phenomena:</p>
<p>• SGRL w/o Priority only sets collect diamond as an exploration goal after 9M steps, which likely accounts for its significantly low exploration efficiency.• SGRL, SGRL w/ Static Pruning, and SGRL w/o Pruning-methods that assign priority weights to goals-consistently treat long-horizon, high-impact achievements (e.g., Collect stone for stone pickaxe, Collect iron) as important objectives and assign them larger priority weights.Success rate curves for all achievements on Craftax-Classic within 5M steps.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 10.
H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH &amp;RGH*RDO $GD5HILQHU (//0 332
Figure 9: Preliminary Results.Success rates across all Craftax-Classic achievements at 5M steps.</p>
<p>• In contrast to SGRL w/ Static Pruning and SGRL w/o Pruning, SGRL consistently assigns relatively small priority weights to more forward-looking goals (e.g., Collect diamond; see the red dashed lines in the figure), which encourages the agent to begin exploring these challenging achievements earlier.We hypothesize that this adaptive prioritization strategy is the primary driver behind SGRL's superior performance.</p>
<p>F Additional Pruner Annealing Strategies and Implementation</p>
<p>F.1 Pruner Annealing Strategies</p>
<p>To dynamically adjust the agent's reliance on constraints during training, we implement several annealing strategies for ξ.These strategies smoothly modulate ξ over training steps.These annealing schedules aim to balance the agent's adherence to the action pruner and the freedom of policy exploration.</p>
<p>• Linear Annealing.This strategy linearly anneals ξ from 0 to 1, gradually reducing the influence of the pruner.
ξ(t) = t T , (7)
where t is current training step, T is total training steps.• Exponential Annealing.This strategy uses exponential decay to increase ξ rapidly from 0 toward 1, then asymptotically approach full freedom.
ξ(t) = 1 − exp − t τ , (8)
where τ is the time constant of the exponential decay.• Three-Stage Linear Annealing.This strategy consists of three phases and is defined by the piecewise function:
ξ(t) =        1 − t 0.4T , t &lt; 0.4T t 0.4T − 1, 0.4T ≤ t &lt; 0.8T 1, t ≥ 0.8T . (9)
• Three-Stage Cosine Annealing.This strategy uses a smooth cosine function for the first two stages:
ξ(t) =        1 2 1 + cos t 0.4T • π , 0 ≤ t &lt; 0.4T 1 2 1 − cos t−0.4T 0.4T • π , 0.4T ≤ t &lt; 0.8T 1.0, t ≥ 0.8T . (10)
In our experiments, the following naming convention is used to denote different annealing strategies applied to the SGRL: (1) SGRL w/ Linear Ann: SGRL equipped with linear annealing schedule (see Equation ( 7)); (2) SGRL w/ Exp Ann: SGRL with exponential annealing (see Equation ( 8)); (3) SGRL w/ 3-Stage Linear: SGRL with piecewise linear three-phase annealing (see Equation ( 9)); and (4) SGRL w/ 3-Stage Cos: SGRL with cosine-based three-phase annealing (see Equation ( 10)).</p>
<p>F.2 Experimental Results</p>
<p>Figures 22-24 present detailed results with different mask mechanism on Craftax-Classic.</p>
<p>As shown in Figures 22-24, the performance of SGRL with four different ξ annealing strategies on Craftax-Classic is presented:</p>
<p>• Figure 22  .We hypothesize that the piecewise linear transitions in 3-Stage Linear Annealing introduce abrupt changes in exploration pressure, whereas the smooth cosine modulation in 3-Stage Cosine Annealing facilitates more stable learning.Interestingly, SGRL w/ Exp Ann is the only variant failing to unlock Collect Diamond by 10M steps.This indicates that the rapid decay of ξ in exponential annealing diminishes goal guidance too early, impairing the agent's ability to align goals with agents' actions and hindering the acquisition of complex, multi-step behaviors.</p>
<p>Overall, these results highlight the critical role of the annealing schedule in modulating the trade-off between goal-driven exploration and policy autonomy.The three-stage cosine strategy achieves the most effective balance, enabling sustained guidance during critical phases of skill acquisition while allowing gradual transition to policy-based control.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 13.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 17.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 20.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 24.
H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6<em>5/ $GD5HILQHU (//0 332 (a) 1M Steps H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6</em>5/ $GD5HILQHU (//0 332 (b) 5M Steps H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6<em>H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6</em>5/ 332 (a) 1M Steps H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6<em>5/ 332 (b) 5M StepsH D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6</em>5/ 6<em>5/Z6WDWLF3UXQ 6</em>5/ZR3UXQ 6<em>5/ZR3ULRULW\ (a) 1M Steps H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6</em>5/ 6<em>5/Z6WDWLF3UXQ 6</em>5/ZR3UXQ 6<em>5/ZR3ULRULW\ (b) 5M Steps H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R OO H F W G LD P R Q G 6XFFHVV5DWH 6</em>H D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F RH D W F R Z F R OO H F W G U LQ N Z D N H X S F R OO H F W V D S OL Q J F R OO H F W Z R R G G H I H D W ] R P E LH G H I H D W V N H OH WR Q S OD F H S OD Q W S OD F H W D E OH H D W S OD Q W P D N H Z R R G V Z R U G P D N H Z R R G S LF N D [ H F R OO H F W F R D O F R OO H F W V WR Q H S OD F H V WR Q H S OD F H I X U Q D F H P D N H V WR Q H V Z R U G P D N H V WR Q H S LF N D [ H F R OO H F W LU R Q P D N H L U R Q V Z R U G P D N H L U R Q S LF N D [ H F R
Figure 1 :
1
Figure 1: Success rate curves for 8 main achievements on Craftax-Classic, ordered from left to right by achievement depth from 1 to 8. A complete and more intuitive version is shown in Figure 10 of the Appendix C.</p>
<p>goal analyst for Crafter games.Agent have a goal: {Goal}.L i s t a c t i o n s t h a t a r e r e l a t e d t o achieving the goal.</p>
<p>Figure 2 :
2
Figure 2: (a) Structured Goal Planner: generates goals with priority weights based on environment states and distilled task knowledge; (b) Goal-Conditioned Action Pruner: filters invalid or irrelevant actions based on current goals; (c) The overall framework of SGRL.</p>
<p>Figure 3 :
3
Figure 3: Success rate curves for all achievements on Craftax-Classic.Achievements are ranked based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 13 in Appendix D.</p>
<p>Figure 5 :
5
Figure5: Ablation Studies.Success rate curves for all achievements on Craftax-Classic.Achievements are ranked based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure20in Appendix E.1.</p>
<p>Figure 6 :
6
Figure 6: Ablation Studies.Success rates across all achievements on Craftax-Classic at 10M steps.</p>
<p>text_view_values = set() block_names = np.vectorize(lambdax: block_id_to_name[x])(map_view[index]) text_view_values.update(block_names.flatten()) mob_ids = np.argmax(mob_map[index],axis=-1) mob_names = np.vectorize(lambdax: mob_id_to_name[x])(mob_ids) mob_mask = mob_map[index].max(axis=-1)&gt; 0.5 text_view_values.update(mob_names[mob_mask].flatten())text_view = ", ".join(text_view_values)</p>
<p>Figure 8 :
8
Figure8: Preliminary Results.Success rate curves for all achievements on Craftax-Classic within 5M steps.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure10.</p>
<p>Figure 10 :
10
Figure 10: Preliminary Study.Success rate curves for all achievements on Craftax-Classic within 5M steps.Solid and dashed arrows indicate direct and cross-depth dependencies, respectively.The bottom-left panel visualizes the full achievement dependency graph, with achievement depth encoded by color (depth 1-8 from top to bottom).</p>
<p>displays the success rate curves across 22 achievements on Craftax-Classic, comparing four mask annealing strategies.Notably, SGRL w/ 3-Stage Cos achieves diamond collection at 3.7M steps and maintains superior performance on the most challenging achievements (Make Iron Pickaxe and Collect Diamond), as shown in Figures22 (b)-(c).This suggests that the Three-Stage Cosine Annealing strategy enables SGRL to more effectively prioritize high-value, long-horizon objectives by adaptively balancing exploration and exploitation.In contrast, SGRL w/ Linear Ann demonstrates stronger early-stage performance, unlocking depth-7 achievements faster (see Figure22 (a)).However, its success rate plateaus in later training phases (see Figures22 (b)-(c)), likely due to the rigid linear decay of ξ, which prematurely restricts exploration and hinders adaptation to complex tasks.A success rate plot that intuitively reflects achievement depth is shown in Figure24.•Figure23presents the success rates across all 22 achievements on Craftax-Classic at different training steps.From Figure 23, we can observe that while SGRL w/ 3-Stage Linear and SGRL w/ 3-Stage Cos exhibit similar performance early on (Figure 23 (a)), the latter significantly outperforms the former in late-stage deep achievements (see Figures 23 (b)-(c))</p>
<p>Figure 11 :
11
Figure11: Main Results.Success rate curves for all achievements on Craftax-Classic within different training steps.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure13.</p>
<p>Figure 13 :
13
Figure 13: Main Results.Success rate curves for all achievements on Craftax-Classic within 5M steps.Solid and dashed arrows indicate direct and cross-depth dependencies, respectively.The bottom-left panel visualizes the full achievement dependency graph, with achievement depth encoded by color (depth 1-8 from top to bottom).</p>
<p>Figure 14 :
14
Figure 14: Main Results.Success rate curves for all achievements on Craftax-Classic within 10M steps.Solid and dashed arrows indicate direct and cross-depth dependencies, respectively.The bottom-left panel visualizes the full achievement dependency graph, with achievement depth encoded by color (depth 1-8 from top to bottom).</p>
<p>Figure 15 :
15
Figure15: Main Results.Success rate curves for all achievements on Crafter within different training steps.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure17.</p>
<p>Figure 16 :
16
Figure 16: Main Results.Success rates across all achievements on Crafter at different training steps.</p>
<p>Figure 17 :
17
Figure 17: Main Results.Success rate curves for all achievements on Crafter within 5M steps.Solid and dashed arrows indicate direct and cross-depth dependencies, respectively.The bottom-left panel visualizes the full achievement dependency graph, with achievement depth encoded by color (depth 1-8 from top to bottom).</p>
<p>Figure 18 :
18
Figure 18: Ablation Studies.Success rate curves for all achievements on Craftax-Classic within different training steps.We rank the achievements based on their depth and the importance of unlocking them for subsequent tasks.Achievements ranked later have greater depth and exert a stronger influence on subsequent achievements.A more intuitive version is shown in Figure 20.</p>
<p>Figure 20 :
20
Figure 20: Ablation Studies.Success rate curves for all achievements on Craftax-Classic within 5M steps.Solid and dashed arrows indicate direct and cross-depth dependencies, respectively.The bottom-left panel visualizes the full achievement dependency graph, with achievement depth encoded by color (depth 1-8 from top to bottom).</p>
<p>Figure 21 :Figure 22 :
2122
Figure 21: Ablation Studies.Heatmap of the goals with priority weights generated by the structured goal planner on Craftax-Classic within 10M steps.The vertical axis on the left shows goals ranked from low to high, while the right axis (ranging from 0 to 0.8) indicates the corresponding weights.</p>
<p>Figure 23 :
23
Figure 23: Mask Comparison.Success rates across all achievements on Craftax-Classic at different training steps.</p>
<p>Figure 24 :
24
Figure 24: Mask Comparison.Success rate curves for all achievements on Craftax-Classic within 5M steps.Solid and dashed arrows indicate direct and cross-depth dependencies, respectively.The bottom-left panel visualizes the full achievement dependency graph, with achievement depth encoded by color (depth 1-8 from top to bottom).</p>
<p>Table 1 :
1Score(%) Reward Achievement Depth SPS (×10 2 )Human50.5 ± 6.8 14.3 ± 2.38-SGRL33.8 ± 1.5 13.0 ± 0.3818.5AdaRefiner 28.5 ± 2.3 12.3 ± 0.970.3ELLM28.4 ± 2.5 12.2 ± 1.060.9PPO24.8 ± 5.7 11.9 ± 1.16135.3
Performance of SGRL and baseline methods on Craftax-Classic at 5M steps.</p>
<p>Table 2 :
2
Ablation results on Craftax-Classic at 10M steps.Make Iron Pickaxe and Collect Diamond, which require sequential planning and tool construction.Notably, the design of goal prioritization plays a critical role in enabling SGRL to rapidly unlock deep achievements like Collect Diamond by guiding the agent to focus on high-value, forward-looking goals during exploration.More experimental results are in Appendix E.1.
Figure 6 and Table 2 present the success rates, scores, rewards,MethdScore (%) RewardAchievement Depthand achievement depth acrossSGRL43.9 ± 2.6 14.9 ± 0.48all 22 achievements in theSGRL w/ Static-Prun 38.5 ± 1.9 14.2 ± 0.48Craftax-Classic environment atSGRL w/o Prun40.0 ± 2.1 14.7 ± 0.2810M steps. The results demon-SGRL w/o Priority35.3 ± 1.6 13.9 ± 0.67strate that SGRL achieves clearadvantages on long-horizontasks such as
Figure7: Ablation Studies.Goals with priority weights on Craftax-Classic, where a higher-level goal is visualized only from the point it is assigned a non-zero weight.</p>
<p>(Matthews et al., 2024ws et al., 2024) is a high-performance, JAX-based reimplementation of Crafter that achieves a 250x simulation speedup via vectorization and parallelization.It faithfully reproduces Crafter's core task structure, environmental dynamics, and evaluation metrics, while enabling large-scale experimentation at 1B+ environment steps within practical compute budgets.
A.1 EnvironmentsCrafter (Hafner, 2022) is a widely adopted benchmark for open-world RL, designed to assess agents'generalization, exploration, and long-term reasoning capabilities through 22 diverse achievements.These achievements are organized in a hierarchical dependency structure of up to 8 depth levels,where early-stage skills (e.g., collect wood, place table) unlock preconditions for increasingly com-plex tasks. The deepest and most challenging achievement (i.e., Collect Diamond) requires agents tomaster long sequences of dependencies, from crafting stone tools to mining iron and finally access-ing diamonds deep underground. Crafter's procedurally generated environments further exacerbatechallenges in sparse rewards, efficient exploration, and hierarchical planning, making it a strongtestbed for evaluating structured goal-guided learning.</p>
<p>Prompt Template for Structured Goal Planner ImplementationCrafter is a 2D open-world survival game with visual input, where the world is procedurally generated.Players need to find food and water, locate a place to sleep, defend against monsters, gather materials, and craft tools.The objective of Crafter is to evaluate an agent's capabilities through a series of semantically meaningful achievements, which can be unlocked in each game session, such as discovering resources and crafting tools.Continuously unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning.As a Python expert, your task is to create an advanced goal generation system that dynamically generates prioritized goals based on environmental observation text.The system should include survival needs assessment, a crafting dependency tree, resource collection configuration, and achievement tracking.Environment Details: Items: sapling, wood, stone, coal, iron, diamond, wood pickaxe, stone pickaxe, iron pickaxe, wood sword, stone sword, iron sword (all with max: 9, initial: 0) Collectable resources: tree, stone, coal, iron, diamond, water, grass (with required tools, output, and leaves defined) Placable objects: stone, table, furnace, plant (with usage, location, and type defined) Craftable tools: wood pickaxe, stone pickaxe, iron pickaxe, wood sword, stone sword, iron sword (with required materials, nearby crafting stations, and output quantity) Achievements:
{'goal':,'priority':, }The state of environmental text is represented by text obs:Example 1:You see: plant, zombie, tree, grass, sand, path, stoneInventory: wood: 1Status: health: 11%, Fullness: 0%, Hydration: 0%, Wakefulness: 88%Sky brightness level: 68% Example 2:You see: plant, tree, grass, path, stoneInventory:Status: health: 99%, Fullness: 77%, Hydration: 66%, Wakefulness: 77%Sky brightness level: 99%Now please provide the OptimizedGoalGenerator class structure and its key func-tional modules.
class OptimizedGoalGenerator: def <strong>init</strong>(self): ... def determine_goal(self, text_obs):...</p>
<p>Prompt Template for Structured Goal Planner Implementation (continued)
status = ", ".join([f"{name}: {int(value / 0.09)}%"for name, value in zip(status_names,status_values[index])])text_obs = "You see: " + text_view + "\nInventory: " +text_obs_inv + "\nStatus: " + statusreturn text_obsExample Environmental Text Observations:Example 1:You see: plant, zombie, tree, grass, sand, path, stoneInventory: wood: 1Status: health: 11%, Fullness: 0%, Hydration: 0%, Wakefulness: 88%, Sky brightness level:68%Example 2:You see: plant, tree, grass, path, stoneInventory:Status: health: 99%, Fullness: 77%, Hydration: 66%, Wakefulness: 77%, Sky brightnesslevel: 99%You have already designed the code architecture. Your goal is to complete this code andcreate an advanced goal generation system capable of dynamically generating prioritizedgoals based on environmental observation text.The code architecture:{last llm response}Prompt Template for Structured Goal Planner Reflection and Revision</p>
<p>•</p>
<p>Be modular, readable, and maintainable, following Python best practices.Please review the following Structured Goal Planner code.Evaluate its quality: provide the label good if it is fully correct and functional, or bad if improvements are needed.For bad code, explain the deficiencies clearly and provide a complete, optimized version of the code that fixes all issues while preserving the intended functionality.The goal-generated code: {last llm response} Prompt Template for Updating Goal Priority Values You are the Goal Priority Analyst for Crafter games.Your task is to update the priority weights of goals in the goal generation code, based on the current state and action trajectory of the intelligent agent.You act as a specialized assistant whose only responsibility is to adjust priority values to improve goal selection; do not change any code logic or structure.
Inputs provided to you:• Goal generation code: A Python code module that defines goals, their attributes,and initial priority weights. ({goal code.py})• Agent state and trajectory: A structured representation of the current state of theintelligent agent, including completed goals, actions taken, and environment status.({agent state.json})Your instructions:</p>
<p>Table 4
4
Since the Crafter environment does not adopt the JAX framework and runs extremely slowly, we report the original results of ELLM and AdaRefiner from their papers in Table 4, rather than reproducing their experiments.
Note: Since ELLM and AdaRefiner require frequent online calls to the LLM (DeepSeek-V3)during training, they incur substantial computational costs and training time. Therefore, we onlyreproduce the results within 5M steps.E Additional Ablation ExperimentsE.1 Performance of Ablation AlgorithmTable 5 and Figures 18-20 present more detailed results of the ablation experiments on Craftax-Classic.
presents performance of SGRL and baseline methods on Crafter at 5M steps.Figures 11-17 present more detailed results of the main experiments on Crafter and Craftax-Classic.It is worth noting that: • In the experiments on Craftax-Classic, ELLM and AdaRefiner require frequent online calls to the LLM (DeepSeek-V3) during training, incurring substantial computational costs and training time.Therefore, we only reproduce the results within 5M steps.•</p>
<p>Table 4 :
4
Main Results.Success rates across all achievements on Craftax-Classic at different training steps.Note: Since ELLM and AdaRefiner require frequent online calls to the LLM (DeepSeek-V3) during training, they incur substantial computational costs and training time.Therefore, we only reproduce the results within 5M steps.Main Results.Performance of SGRL and baseline methods on Crafter at 5M steps.
5/332(c) 10M StepsFigure 12: MethodScore(%) Reward Achievement Depth SPS (×10 2 )Human50.5 ± 6.8 14.3 ± 2.38-SGRL30.5 ± 1.2 12.7 ± 0.481.0AdaRefiner 28.2 ± 1.8 12.9 ± 1.27-ELLM-6.0 ± 0.4--PPO18.5 ± 6.1 10.1 ± 1.361.2</p>
<p>Table 5 :
5
Ablation Studies.Performance of SGRL and ablation methods on Craftax-Classic at 5M steps.
5/6<em>5/Z6WDWLF3UXQ6</em>5/ZR3UXQ6*5/ZR3ULRULW(c) 10M StepsFigure 19: Ablation Studies. Success rates across all achievements on Craftax-Classic at differenttraining steps.MethodScore(%) Reward Achievement DepthSGRL33.8 ± 1.5 13.0 ± 0.38SGRL w/ Static-Prun 34.2 ± 1.7 12.9 ± 0.48SGRL w/o Prun30.9 ± 1.3 12.7 ± 0.27SGRL w/o Priority30.4 ± 0.9 12.3 ± 0.57
using DeepSeek-V3 model as goal generator, queried every
steps under training time and cost constraints.2 using DeepSeek-V3 model as both adapter and decision LLM, queried every 200 steps under training time and cost constraints.
available at https://github.com/DLR-RM/stable-baselines3
available at https://github.com/MichaelTMatthews/Craftax
Available at https://github.com/DLR-RM/stable-baselines3
Available at https://github.com/MichaelTMatthews/Craftax</p>
<p>Safe learning in robotics: From learning-based control to safe reinforcement learning. Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, Angela P Schoellig, Robotics, and Autonomous Systems. 512022Annual Review of Control</p>
<p>Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, ICML. 2023</p>
<p>LLMV-age: verifying LLM-guided planning for agentic exploration in open-world RL. Haotian Chi, Songwei Zhao, Ivor Tsang, Yew-Soon Ong, Hechang Chen, Yi Chang, Haiyan Yin, ICLR 2025 Workshop: VerifAI: AI Verification in the Wild. 2025</p>
<p>Guiding pretraining in reinforcement learning with large language models. Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas, ICML. 2023</p>
<p>De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, NeurIPS. 2022</p>
<p>Loris Gaven, Clement Romac, Thomas Carta, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2410.12481Sac-glam: Improving online rl for llm agents with soft actor-critic and hindsight relabeling. 2024arXiv preprint</p>
<p>Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Nature. 6452025</p>
<p>From words to actions: Unveiling the theoretical underpinnings of llm-driven autonomous systems. Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H Huang, Dhruva Tirumala, Jan Humplik, Markus Wulfmeier, Saran Tunyasuvunakool, Noah Y Siegel, Roland Hafner, ICLR, 2022. Jianliang He, Siyu Chen, Fengzhuo Zhang, and Zhuoran Yang. 2024. 202498022ICML</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver, AAAI. 2018</p>
<p>Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach. Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu, RLC2024</p>
<p>Language instructed reinforcement learning for human-AI coordination. Hengyuan Hu, Dorsa Sadigh, ICML. 2023</p>
<p>Hierarchical reinforcement learning: A survey and open research challenges. Matthias Hutsebaut-Buysse, Kevin Mets, Steven Latré, Machine Learning and Knowledge Extraction. 412022</p>
<p>. Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, J Nikhil, Kyle Joshi, Rosario Jeffrey, Jasmine Jauregui Ruano, Keerthana Hsu, Byron Gopalakrishnan, Andy David, Chuyuan Kelly Zeng, Fu, 2022Do as i can, not as i say: Grounding language in robotic affordances. In CoRL</p>
<p>Text2motion: From natural language instructions to feasible plans. Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, Autonomous Robots. 4782023</p>
<p>Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning. Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, IJCAI. 2022</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Goal-conditioned reinforcement learning: problems and solutions. Minghuan Liu, Menghui Zhu, Weinan Zhang, IJCAI. 2022</p>
<p>Eureka: Human-level reward design via coding large language models. Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Linxi Zhu, Anima Fan, Anandkumar, ICLR. 2024</p>
<p>Craftax: A lightning-fast benchmark for open-ended reinforcement learning. Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Thomas Jackson, Samuel Coward, Jakob Nicolaus Foerster, ICML. 2024</p>
<p>Seungyong Moon, Junyoung Yeom, Bumsoo Park, and Hyun Oh Song. Discovering hierarchical achievements in reinforcement learning via contrastive learning. Joost Thomas M Moerland, Aske Broekens, Catholijn M Plaat, Jonker, NeurIPS. 2023. 202316Model-based reinforcement learning: A survey. Foundations and Trends® in Machine Learning</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, NeurIPS2022</p>
<p>Dialog policy learning for joint clarification and active learning queries. Aishwarya Padmakumar, Raymond J Mooney, AAAI. 2021</p>
<p>Bharat Prakash, Tim Oates, Tinoosh Mohsenin, arXiv:2311.05596LLM augmented hierarchical agents. 2023arXiv preprint</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, Noah Dormann, Journal of Machine Learning Research. 222682021</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Unai Ruiz-Gonzalez, Alain Andres, Pedro G Bascoy, Javier Del Ser, arXiv:2410.08632Words as beacons: guiding rl agents with high-level language prompts. 2024arXiv preprint</p>
<p>Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Matthew E Osmar R Zaiane, Taylor, arXiv:2502.15214The evolving landscape of llm-and vlm-integrated reinforcement learning. 2025arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, NeurIPS2023</p>
<p>Lgts: Dynamic task sampling using llm-generated sub-goals for reinforcement learning agents. Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, Jivko Sinapov, AAMAS. 2024</p>
<p>Open-ended learning leads to generally capable agents. Adam Team, Anuj Stooke, Catarina Mahajan, Charlie Barros, Jakob Deck, Jakub Bauer, Maja Sygnowski, Max Trebacz, Michael Jaderberg, Mathieu, arXiv:2107.12808Open Ended Learning. 2021arXiv preprint</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 57577822019</p>
<p>Investigating the role of model-based learning in exploration and transfer. Eszter Jacob C Walker, Yazhe Vértes, Gabriel Li, Ankesh Dulac-Arnold, Théophane Anand, Jessica B Weber, Hamrick, ICML. 2023</p>
<p>Describe, explain, plan and select: Interactive panning with LLMs enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, NeurIPS2023</p>
<p>Text2reward: Automated dense reward function generation for reinforcement learning. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu, ICLR2024</p>
<p>Efficient reinforcement learning with large language model priors. Xue Yan, Yan Song, Xidong Feng, Mengyue Yang, Haifeng Zhang, Haitham Bou Ammar, Jun Wang, ICLR2025</p>
<p>Automated skill discovery for language agents through exploration and iterative feedback. Yongjin Yang, Sinjae Kang, Juyong Lee, Dongjun Lee, Se-Young Yun, Kimin Lee, arXiv:2506.042872025arXiv preprint</p>
<p>Bootstrap your own skills: Learning to solve new tasks with large language model guidance. Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, Joseph J Lim, CoRL. 2023</p>
<p>Adarefiner: Refining decisions of language models with adaptive feedback. Wanpeng Zhang, Zongqing Lu, NAACL. 2024</p>
<p>Online intrinsic rewards for decision making agents from large language model feedback. Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, Brandon Amos, arXiv:2410.230222024arXiv preprint</p>
<p>Llm4rl: Enhancing reinforcement learning with large language models. Jiehan Zhou, Yang Zhao, Jiahong Liu, Peijun Dong, Xiaoyu Luo, Hang Tao, Chang Shi, Hanjiang Luo, CCECE2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1531 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1531</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1531</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-30d265c4dec1276579942b42fb6c46a5ff490e7f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/30d265c4dec1276579942b42fb6c46a5ff490e7f" target="_blank">Meta-Sim: Learning to Generate Synthetic Datasets</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> Meta-Sim is proposed, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine, and can greatly improve content generation quality over a human-engineered probabilistic scene grammar.</p>
                <p><strong>Paper Abstract:</strong> Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1531.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1531.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UE4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unreal Engine 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A photorealistic game engine used in this work as the non-differentiable renderer to convert scene graphs sampled/modified by Meta-Sim into images and ground-truth labels for 3D driving experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Unreal Engine 4 (UE4)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A commercial game engine providing high-quality, photorealistic 3D rendering used to render scene graphs (roads, vehicles, context) into images and pixel-level labels for training vision models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer vision / autonomous driving simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High-fidelity photorealistic rendering for visual appearance; physics/dynamics fidelity not emphasized or detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Produces photorealistic imagery and supports rich 3D assets; the renderer is non-differentiable in this work (authors approximate gradients via finite differences); paper does not document vehicle dynamics, sensor noise models, or other physical fidelity features.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TaskNet (Mask R-CNN detector) trained on UE4-rendered data</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mask R-CNN object detector with ResNet-FPN backbone (neural network) used as the downstream Task Network trained on datasets rendered by UE4.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Object detection (car detection) in driving scenes (sim-to-real transfer to KITTI)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world KITTI dataset (validation and test splits)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>When trained on UE4-rendered datasets derived from the baseline probabilistic grammar: AP@0.5 IoU (Easy/Moderate/Hard) = 63.7 / 63.7 / 62.2 (baseline). Using Meta-Sim with UE4-rendered data improves to ~66.4 / 66.5 / 65.6 (MetaSim (Cars)) and up to ~66.7 / 66.3 / 66.2 when including task loss (see Table 3). Using Meta-Sim + MUNIT image-to-image translation yields (Table 4) Prob. Grammar: 71.1 / 75.5 / 65.3 vs Meta-Sim: 77.5 / 75.1 / 68.2 (appearance-translation experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper discusses appearance (photorealism) vs content (layout) gaps: photorealistic rendering (appearance) alone does not resolve the content distribution mismatch; adding image-to-image translation (MUNIT) reduces appearance gap but content mismatch remains important. No explicit statement of minimal simulator fidelity required for transfer is given.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported failure cases when using UE4-rendered scenes include unrealistic object overlaps and collisions originating from dense initial grammar samples and missing modeled attributes (e.g., object sizes not optimized), causing unreal or colliding placements that harm scene validity despite photoreal rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1531.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1531.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SDR-grammar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Domain Randomization probabilistic scene grammar (adapted SDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic scene grammar (adapted from Structured Domain Randomization) used to sample structured driving scenes (roads, lanes, cars, context) that Meta-Sim transforms; provides scene structure but uses parametric attribute distributions that may not match real data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured domain randomization: Bridging the reality gap by contextaware synthetic data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Probabilistic Scene Grammar (SDR-based)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A content-generation system (probabilistic program/grammar) for synthesizing structured driving scenes: samples scene graphs (road spline, lanes, car placements, context elements) and attributes from parametric distributions; not a physics simulator but a generator of scene content fed to a renderer.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>synthetic scene generation for autonomous driving / computer vision</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low-to-medium fidelity for content distribution: structure is assumed correct but attribute distributions are approximate and manually specified; visual fidelity depends on the renderer (e.g., UE4).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Encodes scene structure and samples object attributes (location, rotation, distance, offset) from hand-specified distributions; does not reliably model physical interactions, object-size variation modeling was not included in the features used here, and intersections/side-streets were omitted in the adaptation used by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>G_Î¸ Distribution Transformer (Meta-Sim) acting on SDR samples</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Graph Convolutional Network (Distribution Transformer) that takes scene graphs sampled from the grammar and predicts transformed attributes for mutable nodes to better match real data distributions or optimize downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Generate synthetic datasets (content distributions) to train vision TaskNets for object detection and semantic segmentation (e.g., car detection for KITTI, aerial segmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world KITTI dataset (for driving) and other target datasets used in experiments (MNIST-style and Aerial2D targets for controlled tests).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Baseline probabilistic grammar (SDR-adapted) => TaskNet AP@0.5 IoU on KITTI val: 63.7 / 63.7 / 62.2 (Easy/Moderate/Hard). After Meta-Sim transformations of SDR samples, TaskNet improves to ~66.4 / 66.5 / 65.6 and up to ~66.7 / 66.3 / 66.2 with task loss (see Table 3). For aerial segmentation, mean IoU improved from 80.3 (Prob. Grammar) to 95.2 (Meta-Sim) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper assumes correct scene structure from the grammar and focuses on adapting attributes (content) rather than learning structure; it does not specify a minimal fidelity threshold but indicates that correct structural priors plus adaptive attributes are important for successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Failures stem from limitations of the grammar and exposed attribute set (e.g., not modeling object sizes), leading to overlapping/colliding objects that Meta-Sim sometimes cannot resolve; dense initial samples from the grammar can produce irrecoverable collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1531.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1531.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open urban driving simulator cited in related work as an example of a realistic environment for training/testing models and agents before real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CARLA: An open urban driving simulator</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An open-source urban driving simulator used by the community to train and evaluate autonomous driving models; cited here as a representative realistic simulation environment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Not specified in this paper (reference only); generally considered to provide high visual fidelity and configurable sensor/traffic simulation, but authors give no fidelity details here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Mentioned as realistic environment; this paper does not detail which physical/sensor fidelity features are present or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>general training/testing of driving models in virtual environments (mentioned broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world deployment (general sim-to-real use case mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1531.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1531.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine for model-based control cited in related work as an example of a simulator used for robotic control and model-based reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>mujoco: A physics engine for model-based control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A physics engine that simulates multi-joint dynamics with contact, commonly used for control and RL research.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / control / reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Not specified in this paper (reference only); typically considered medium-to-high fidelity for rigid-body dynamics and contact, but no details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>robotic control and model-based control (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1531.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1531.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit of reinforcement learning environments referenced in related work as an example of standardized simulation environments for RL research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openai gym</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>OpenAI Gym</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A suite of environments and benchmarks for developing and comparing reinforcement learning algorithms; referenced as an example of virtual environments used prior to real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning / control</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Varies by included environment; this paper does not discuss fidelity details.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>general RL tasks (mentioned broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1531.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1531.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive 3D environment for visual AI cited in related work as an example of simulated environments for embodied agents and visual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI2-THOR: An interactive 3d environment for visual ai</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AI2-THOR</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An interactive indoor 3D environment for tasks in visual AI and embodied agents; referenced among other simulation environments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied AI / computer vision / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>visual/embodied tasks in simulated indoor environments (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1531.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1531.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aerial Informatics & Robotics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aerial Informatics and Robotics platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aerial robotics platform cited in related work as an example of simulation/testing environments for aerial autonomy and robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aerial Informatics and Robotics platform</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Aerial Informatics and Robotics platform</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A platform/technical report (Microsoft Research) for aerial informatics and robotics referenced in related work; cited as an example of environments used for navigation/control.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>aerial robotics / autonomous navigation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>aerial navigation and robotics (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Meta-Sim: Learning to Generate Synthetic Datasets', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CARLA: An open urban driving simulator <em>(Rating: 2)</em></li>
                <li>mujoco: A physics engine for model-based control <em>(Rating: 2)</em></li>
                <li>Structured domain randomization: Bridging the reality gap by contextaware synthetic data <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Learning to simulate <em>(Rating: 1)</em></li>
                <li>Adversarial variational optimization of non-differentiable simulators <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1531",
    "paper_id": "paper-30d265c4dec1276579942b42fb6c46a5ff490e7f",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "UE4",
            "name_full": "Unreal Engine 4",
            "brief_description": "A photorealistic game engine used in this work as the non-differentiable renderer to convert scene graphs sampled/modified by Meta-Sim into images and ground-truth labels for 3D driving experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Unreal Engine 4 (UE4)",
            "simulator_description": "A commercial game engine providing high-quality, photorealistic 3D rendering used to render scene graphs (roads, vehicles, context) into images and pixel-level labels for training vision models.",
            "scientific_domain": "computer vision / autonomous driving simulation",
            "fidelity_level": "High-fidelity photorealistic rendering for visual appearance; physics/dynamics fidelity not emphasized or detailed in this paper.",
            "fidelity_characteristics": "Produces photorealistic imagery and supports rich 3D assets; the renderer is non-differentiable in this work (authors approximate gradients via finite differences); paper does not document vehicle dynamics, sensor noise models, or other physical fidelity features.",
            "model_or_agent_name": "TaskNet (Mask R-CNN detector) trained on UE4-rendered data",
            "model_description": "Mask R-CNN object detector with ResNet-FPN backbone (neural network) used as the downstream Task Network trained on datasets rendered by UE4.",
            "reasoning_task": "Object detection (car detection) in driving scenes (sim-to-real transfer to KITTI)",
            "training_performance": null,
            "transfer_target": "Real-world KITTI dataset (validation and test splits)",
            "transfer_performance": "When trained on UE4-rendered datasets derived from the baseline probabilistic grammar: AP@0.5 IoU (Easy/Moderate/Hard) = 63.7 / 63.7 / 62.2 (baseline). Using Meta-Sim with UE4-rendered data improves to ~66.4 / 66.5 / 65.6 (MetaSim (Cars)) and up to ~66.7 / 66.3 / 66.2 when including task loss (see Table 3). Using Meta-Sim + MUNIT image-to-image translation yields (Table 4) Prob. Grammar: 71.1 / 75.5 / 65.3 vs Meta-Sim: 77.5 / 75.1 / 68.2 (appearance-translation experiment).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "The paper discusses appearance (photorealism) vs content (layout) gaps: photorealistic rendering (appearance) alone does not resolve the content distribution mismatch; adding image-to-image translation (MUNIT) reduces appearance gap but content mismatch remains important. No explicit statement of minimal simulator fidelity required for transfer is given.",
            "failure_cases": "Reported failure cases when using UE4-rendered scenes include unrealistic object overlaps and collisions originating from dense initial grammar samples and missing modeled attributes (e.g., object sizes not optimized), causing unreal or colliding placements that harm scene validity despite photoreal rendering.",
            "uuid": "e1531.0",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "SDR-grammar",
            "name_full": "Structured Domain Randomization probabilistic scene grammar (adapted SDR)",
            "brief_description": "A probabilistic scene grammar (adapted from Structured Domain Randomization) used to sample structured driving scenes (roads, lanes, cars, context) that Meta-Sim transforms; provides scene structure but uses parametric attribute distributions that may not match real data.",
            "citation_title": "Structured domain randomization: Bridging the reality gap by contextaware synthetic data",
            "mention_or_use": "use",
            "simulator_name": "Probabilistic Scene Grammar (SDR-based)",
            "simulator_description": "A content-generation system (probabilistic program/grammar) for synthesizing structured driving scenes: samples scene graphs (road spline, lanes, car placements, context elements) and attributes from parametric distributions; not a physics simulator but a generator of scene content fed to a renderer.",
            "scientific_domain": "synthetic scene generation for autonomous driving / computer vision",
            "fidelity_level": "Low-to-medium fidelity for content distribution: structure is assumed correct but attribute distributions are approximate and manually specified; visual fidelity depends on the renderer (e.g., UE4).",
            "fidelity_characteristics": "Encodes scene structure and samples object attributes (location, rotation, distance, offset) from hand-specified distributions; does not reliably model physical interactions, object-size variation modeling was not included in the features used here, and intersections/side-streets were omitted in the adaptation used by the authors.",
            "model_or_agent_name": "G_Î¸ Distribution Transformer (Meta-Sim) acting on SDR samples",
            "model_description": "A Graph Convolutional Network (Distribution Transformer) that takes scene graphs sampled from the grammar and predicts transformed attributes for mutable nodes to better match real data distributions or optimize downstream task performance.",
            "reasoning_task": "Generate synthetic datasets (content distributions) to train vision TaskNets for object detection and semantic segmentation (e.g., car detection for KITTI, aerial segmentation).",
            "training_performance": null,
            "transfer_target": "Real-world KITTI dataset (for driving) and other target datasets used in experiments (MNIST-style and Aerial2D targets for controlled tests).",
            "transfer_performance": "Baseline probabilistic grammar (SDR-adapted) =&gt; TaskNet AP@0.5 IoU on KITTI val: 63.7 / 63.7 / 62.2 (Easy/Moderate/Hard). After Meta-Sim transformations of SDR samples, TaskNet improves to ~66.4 / 66.5 / 65.6 and up to ~66.7 / 66.3 / 66.2 with task loss (see Table 3). For aerial segmentation, mean IoU improved from 80.3 (Prob. Grammar) to 95.2 (Meta-Sim) (Table 2).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "The paper assumes correct scene structure from the grammar and focuses on adapting attributes (content) rather than learning structure; it does not specify a minimal fidelity threshold but indicates that correct structural priors plus adaptive attributes are important for successful transfer.",
            "failure_cases": "Failures stem from limitations of the grammar and exposed attribute set (e.g., not modeling object sizes), leading to overlapping/colliding objects that Meta-Sim sometimes cannot resolve; dense initial samples from the grammar can produce irrecoverable collisions.",
            "uuid": "e1531.1",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "CARLA",
            "name_full": "CARLA",
            "brief_description": "An open urban driving simulator cited in related work as an example of a realistic environment for training/testing models and agents before real-world deployment.",
            "citation_title": "CARLA: An open urban driving simulator",
            "mention_or_use": "mention",
            "simulator_name": "CARLA",
            "simulator_description": "An open-source urban driving simulator used by the community to train and evaluate autonomous driving models; cited here as a representative realistic simulation environment.",
            "scientific_domain": "autonomous driving / robotics",
            "fidelity_level": "Not specified in this paper (reference only); generally considered to provide high visual fidelity and configurable sensor/traffic simulation, but authors give no fidelity details here.",
            "fidelity_characteristics": "Mentioned as realistic environment; this paper does not detail which physical/sensor fidelity features are present or absent.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "general training/testing of driving models in virtual environments (mentioned broadly)",
            "training_performance": null,
            "transfer_target": "real-world deployment (general sim-to-real use case mentioned in related work)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1531.2",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo",
            "brief_description": "A physics engine for model-based control cited in related work as an example of a simulator used for robotic control and model-based reinforcement learning.",
            "citation_title": "mujoco: A physics engine for model-based control",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "A physics engine that simulates multi-joint dynamics with contact, commonly used for control and RL research.",
            "scientific_domain": "robotics / control / reinforcement learning",
            "fidelity_level": "Not specified in this paper (reference only); typically considered medium-to-high fidelity for rigid-body dynamics and contact, but no details provided here.",
            "fidelity_characteristics": "Not described in this paper.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "robotic control and model-based control (general mention)",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1531.3",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "OpenAI Gym",
            "name_full": "OpenAI Gym",
            "brief_description": "A toolkit of reinforcement learning environments referenced in related work as an example of standardized simulation environments for RL research.",
            "citation_title": "Openai gym",
            "mention_or_use": "mention",
            "simulator_name": "OpenAI Gym",
            "simulator_description": "A suite of environments and benchmarks for developing and comparing reinforcement learning algorithms; referenced as an example of virtual environments used prior to real-world deployment.",
            "scientific_domain": "reinforcement learning / control",
            "fidelity_level": "Varies by included environment; this paper does not discuss fidelity details.",
            "fidelity_characteristics": "Not provided in this paper.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "general RL tasks (mentioned broadly)",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1531.4",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "AI2-THOR",
            "name_full": "AI2-THOR",
            "brief_description": "An interactive 3D environment for visual AI cited in related work as an example of simulated environments for embodied agents and visual tasks.",
            "citation_title": "AI2-THOR: An interactive 3d environment for visual ai",
            "mention_or_use": "mention",
            "simulator_name": "AI2-THOR",
            "simulator_description": "An interactive indoor 3D environment for tasks in visual AI and embodied agents; referenced among other simulation environments.",
            "scientific_domain": "embodied AI / computer vision / robotics",
            "fidelity_level": "Not discussed in this paper.",
            "fidelity_characteristics": "Not detailed here.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "visual/embodied tasks in simulated indoor environments (general mention)",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1531.5",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Aerial Informatics & Robotics",
            "name_full": "Aerial Informatics and Robotics platform",
            "brief_description": "Aerial robotics platform cited in related work as an example of simulation/testing environments for aerial autonomy and robotics.",
            "citation_title": "Aerial Informatics and Robotics platform",
            "mention_or_use": "mention",
            "simulator_name": "Aerial Informatics and Robotics platform",
            "simulator_description": "A platform/technical report (Microsoft Research) for aerial informatics and robotics referenced in related work; cited as an example of environments used for navigation/control.",
            "scientific_domain": "aerial robotics / autonomous navigation",
            "fidelity_level": "Not described in this paper.",
            "fidelity_characteristics": "Not provided here.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": "aerial navigation and robotics (general mention)",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1531.6",
            "source_info": {
                "paper_title": "Meta-Sim: Learning to Generate Synthetic Datasets",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CARLA: An open urban driving simulator",
            "rating": 2
        },
        {
            "paper_title": "mujoco: A physics engine for model-based control",
            "rating": 2
        },
        {
            "paper_title": "Structured domain randomization: Bridging the reality gap by contextaware synthetic data",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2
        },
        {
            "paper_title": "Learning to simulate",
            "rating": 1
        },
        {
            "paper_title": "Adversarial variational optimization of non-differentiable simulators",
            "rating": 1
        }
    ],
    "cost": 0.01720425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Meta-Sim: Learning to Generate Synthetic Datasets</h1>
<p>Amlan Kar ${ }^{1,2,3}$ Aayush Prakash ${ }^{1}$ Ming-Yu Liu ${ }^{1}$ Eric Cameracci ${ }^{1}$ Justin Yuan ${ }^{1}$<br>Matt Rusiniak ${ }^{1}$ David Acuna ${ }^{1,2,3}$ Antonio Torralba ${ }^{4}$ Sanja Fidler ${ }^{1,2,3 *}$<br>${ }^{1}$ NVIDIA ${ }^{2}$ University of Toronto ${ }^{3}$ Vector Institute ${ }^{4}$ MIT</p>
<h4>Abstract</h4>
<p>Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task. Webpage: https: //nv-tlabs.github.io/meta-sim/</p>
<h2>1. Introduction</h2>
<p>Data collection and labeling is a laborious, costly and time consuming venture, and represents a major bottleneck in most current machine learning pipelines. To this end, synthetic content generation [5, 35, 10, 33] has emerged as a promising solution since all ground-truth comes for free - via the graphics engine. It further enables us to train and test our models in virtual environments [37, 7, 46, 23, 40] before deploying to the real world, which is crucial for both scalability and safety. Unfortunately, an important performance issue arises due to the domain gap existing between the synthetic and real-world domains.</p>
<p>Addressing the domain gap issue has led to a plethora of work on synthetic-to-real domain adaptation [17, 27, 51, $9,42,33,44]$. These techniques aim to learn domaininvariant features and thus more transferrable models. One of the mainstream approaches is to learn to stylize syn-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Meta-Sim is a method to generate synthetic datasets that bridge the distribution gap between real and synthetic data and are optimized for downstream task performance
thetic images to look more like those captured in the realworld $[17,27,48,30,18]$. As such, these models address the appearance gap between the synthetic and real-world domains. They share the assumption that the domain gap is due to the differences that are fairly low level.</p>
<p>Here, we argue that domain gap is also due to a content gap, arising from the fact that the synthetic content (e.g. layout and types of objects) mimics a limited set of scenes, not necessarily reflecting the diversity and distribution of objects of those captured in the real world. For example, the Virtual KITTI [10] dataset was created by a group of engineers and artists, to match object locations and poses in KITTI [12] which was recorded in Karlsruhe, Germany. But what if the target city changes to Tokyo, Japan, which has much heavier traffic and many more high-rise buildings? Moreover, what if the downstream task that we want to solve changes from object detection to lane estimation or rain drop removal? Creating synthetic worlds that ensure realism and diversity for any desired task requires significant effort by highly-qualified experts and does not scale to the fast demand of various commercial applications.</p>
<p>In this paper, we aim to learn a generative model of synthetic scenes that, by exploiting a graphics engine, produces labeled datasets with a content distribution matching that of imagery captured in the desired real-world datasets. Our Meta-Sim builds on top of probabilistic scene grammars which are commonly used in gaming and graphics to create diverse and valid virtual environments. In particular, we assume that the structure of the scenes sampled from the grammar are correct (e.g. a driving scene has a road and</p>
<p>cars), and learn to modify their attributes. By modifying locations, poses and other attributes of objects, Meta-Sim gains a powerful flexibility of adapting scene generation to better match real-world scene distributions. Meta-Sim also optimizes a meta objective of adapting the simulator to improve downstream real-world performance of a Task Network trained on the datasets synthesized by our model. Our learning framework optimizes several objectives using approximated gradients through a non-differentiable renderer.</p>
<p>We validate our approach on two toy simulators in controlled settings, where Meta-Sim is shown to excel at bridging the distribution gaps. We further showcase Meta-Sim on adapting a probabilistic grammar akin to SDR [33] to better match a real self-driving dataset, leading to improved content generation quality, as measured by sim-to-real performance. To the best of our knowledge, Meta-Sim is the first approach to enable dataset and task specific synthetic content generation, and we hope that our work opens the door to more adaptable simulation in the future.</p>
<h2>2 Related Work</h2>
<p>Synthetic Content Generation and Simulation. The community has been investing significant effort in creating high-quality synthetic content, ranging from driving scenes [37, 10, 35, 7, 33, 45], indoor navigation [46], household robotics [34, 23], robotic control [43], game playing [4], optical flow estimation [5], and quadcopter control and navigation [40]. While such environments are typically very realistic, they require qualified experts to spend a huge amount of time to create these virtual worlds. Domain Randomization (DR) is a cheaper alternative to such photo-realistic simulation environments [39, 42, 33]. The DR technique generates a large amount of diverse scenes by inserting objects in random locations and poses. As a result, the distribution of the synthetic scenes is very different to that of the real world scenes. We, on the other hand, aim to align the synthetic and real distributions through a direct optimization on the attributes and through a meta objective of optimizing for performance on a down-stream task.</p>
<p>Procedural modeling and probabilisic scene grammars are an alternative approach to content generation, which are able to produce worlds at the scale of full cities, and mimic diverse 3D scenes for self-driving. However, the parameters for generating the distributions that control how a scene is generated need to be manually specified. This is not only tedious but also error-prone. There is no guarantee that the specified parameters can generate distributions that faithfully reflect real world distributions. [24, 32] use such probabilistic programs to invert the generative process and infer a program given an image, while we</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>aim to learn the generative process itself from real data.
Domain Adaptation aims at addressing the domain gap, i.e. the mismatch between the distribution of data used to train a model and the distribution of data that the model is expected to work with. From synthetic to real, two kinds of domain gaps arise: the appearance (style) gap and the content (layout) gap. Most existing work [17, 27, 51, 9, 48, 30, 18] tackle the former by using generative adversarial networks (GANs) [13] to transform the appearance distribution of the synthetic images to look more like that of the real images. Others [17, 27] add additional task based constraints to ensure that the layout of the stylized images remain the same. Other techniques use pseudo label based self learning [51] and student-teacher networks [9] for domain adaptation. Our work is an early attempt to tackle the second kind of domain gap - the content gap. We note that the appearance gap is orthogonal to the content gap, and prior art could be directly plugged into our method.</p>
<p>Optimizing Simulators. Louppe et al. [31] attempt to optimize non-differentiable simulators with the key difference being in the method and the end goal. They optimize using a variational upperbound of a GAN-like objective to produce samples representative of a target distribution. We, on other hand, use the MMD [15] distance metric for comparing distributions and also optimize a meta objective to produce samples suitable for a downstream task. [6] learn to optimize simulator parameters for robotic control tasks, where trajectories between the real and simulated robot can be directly compared. [38] optimize high level exposed parameters by optimizing for downstream task performance using Reinforcement Learning. We, on the other hand, optimize low level scene parameters (at the level of every object) while also learning to match distributions along with optimizing downstream task performance. Ganin et al. [11] attempt to synthesize images by learning to generate even lower-level programs (at the level of brush strokes) that a graphics engine can interpret to generate realistic looking images, as measured by a trained discriminator.</p>
<h2>3 Meta-Sim</h2>
<p>In this section, we introduce Meta-Sim. Given a dataset of real imagery $X_{R}$ and a task $T$ (e.g. object detection), our goal is to synthesize a training dataset $D_{T}=\left(X_{T}, Y_{T}\right)$ with $X_{T}$ synthesized imagery that resembles the given real imagery, and $Y_{T}$ the corresponding ground-truth for task $T$. To simplify notation, we omit subscript $T$ from here on.</p>
<p>We parametrize data synthesis with a neural network, i.e. $D(\theta)=(X(\theta), Y(\theta))$. Our goal in this paper is to learn the parameters $\theta$ such that the distribution of $X(\theta)$ matches that of $X_{R}$ (real imagery). Optionally, if the real dataset comes with a small validation set $V$ that is labeled for task $T$, we additionally aim to optimize a meta-objective, i.e. downstream task performance. The latter assumes we also have a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Overview of Meta-Sim: The goal is to learn to transform samples coming from a probabilistic grammar with a distribution transformer, aiming to minimize the <em>distribution gap</em> between simulated and real data and maximize <em>sim-to-real performance</em></p>
<p>trainable task solving module (<em>i.e</em>. another neural network), the performance of which we want to maximize by training it on our generated training data. We refer to this module as a Task Network, which will be treated as a black box in our work. Note that Meta-Sim has parallels to Neural Architecture Search [50], where our search is over the input datasets to a fixed neural network instead of a search over the neural network architecture given fixed data.</p>
<p><strong>Image Synthesis vs Rendering.</strong> Generative models of pixels have only recently seen success in generating realistic high resolution images [3, 19]. Extracting task specific ground-truth (eg: segmentation) from them remains a challenge. Conditional generative models of pixels condition on input images and transform their appearance, producing compelling results. However, these methods assume ground truth labels remain unchanged, and thus are limited in their <em>content</em> (structural) variability. In Meta-Sim we aim to <em>learn</em> a <strong>generative model of synthetic 3D content</strong>, and obtain <em>D</em> via a graphics engine. Since the 3D assets come with semantic information (<em>i.e</em>., we know an asset is a <em>car</em>), compositing or modifying the synthetic scenes will still render perfect ground-truth. The main challenge is to learn the 3D scene composition by optimizing solely the distribution mismatch of rendered with real imagery. The following subsections layout Meta-Sim in detail and are structured as follows: Sec. 3.1 introduces the representation of parametrized synthetic worlds, while Sec. 3.2 describes our learning framework.</p>
<h3>3.1. Parametrizing Synthetic Scenes</h3>
<p><strong>Scene Graphs</strong> are a common way to represent 3D worlds in gaming/graphics. A scene graph represent elements of a scene in a concise hierarchical structure, with each element having a set of attributes (eg. class, location, or even the id of a 3D asset from a library) (see Fig. 3). The hierarchy defines parent-child dependencies, where the attributes of the child elements are typically defined relative to the parent's, allowing for an efficient and natural way to create and modify scenes. The corresponding image and pixel-level annotations can be rendered easily by placing objects as described in the scene graph.</p>
<p>In order to generate <em>diverse</em> and <em>valid</em> 3D worlds, the typical approach is to specify the generative process of the graph by a <em>probabilistic scene grammar</em> [49]. For example, to generate a traffic scene, one might first lay out the centerline of the road, add parallel lanes, position aligned cars on each lane, etc. The structure of the scene is defined by the grammar, while the attributes are typically sampled from parametric distributions, which require careful tuning.</p>
<p>In our work, we assume access to a probabilistic grammar from which we can sample initial scene graphs. We assume the <em>structure</em> of each scene graph is correct, <em>i.e</em>. the driving scene has a road, sky, and a number of objects. This is a reasonable assumption, given that inferring structure (inverse graphics) is known to be a hard problem. Our goal is to modify the <em>attributes</em> of each scene graph, such that the transformed scenes, when rendered, will resemble the distribution of the real scenes. By modifying the attributes, we give the model a powerful flexibility to change objects' locations, poses, colors, asset ids, etc. This amounts to learning a conditional generative model, which, by conditioning on an input scene graph transforms its node attributes. In essence, we keep the <em>structure</em> generated by the probabilistic grammar, but transform the distribution of the <em>attributes</em>. Thus, our model acts as a <em>Distribution Transformer</em>.</p>
<p><strong>Notation.</strong> Let <em>P</em> denote the probabilistic grammar from which we can sample scene graphs <em>s</em> â¼ <em>P</em>. We denote a single scene graph <em>s</em> as a set of vertices <em>s<sub>V</sub></em>, edges <em>s<sub>E</sub></em> and attributes <em>s<sub>A</sub></em>. We have access to a renderer <em>R</em>, that can take in a scene graph <em>s</em> and generate the corresponding image and ground truth, <em>R</em>(<em>s</em>) = (<em>x</em>, <em>y</em>). Let <em>G<sub>Î¸</sub></em> refer to our Distribution Transformer, which takes an input scene graph <em>s</em> and outputs a scene graph <em>G<sub>Î¸</sub></em>(<em>s</em>), with transformed attributes but the same structure, <em>i.e</em>. <em>G<sub>Î¸</sub></em>(<em>s</em> = [<em>s<sub>V</sub></em>, <em>s<sub>E</sub></em>, <em>s<sub>A</sub></em>]) = [<em>s<sub>V</sub></em>, <em>s<sub>E</sub></em>, <em>G<sub>Î¸</sub></em>(<em>s<sub>A</sub></em>)]. Note that by sampling many scene graphs, transforming their attributes, and rendering, we obtain a synthetic dataset <em>D</em>(<em>Î¸</em>).</p>
<p><strong>Architecture of</strong> <em>G<sub>Î¸</sub></em>. Given the graphical structure of scene graphs, modeling <em>G<sub>Î¸</sub></em> via a Graph Neural Network is a natural choice. In particular, we use Graph Convolutional Networks (GCNs) [22]. We follow [47] and use a graph convolutional layer that utilizes two different weight matrices to capture top-down and bottom-up information flow separately. Our model makes per node predictions <em>i.e</em>. generates transformed attributes <em>G<sub>Î¸</sub></em>(<em>s<sub>A</sub></em>) for each node in <em>s<sub>V</sub></em>.</p>
<p><strong>Mutable Attributes:</strong> We input to <em>G<sub>Î¸</sub></em> all attributes <em>s<sub>A</sub></em>, but we might want to only modify specific attributes and trust the probabilistic grammar <em>P</em> on the rest. For example, in Fig. 3 we may not want to change the heights of houses, or width of the sidewalks, if our final task is car detection. This reduces the number of exposed parameters our model is tasked to tune thus improving training time and complexity. Therefore, in the subsequent parts, we assume we have</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Simple scene graph example for a driving scene.</p>
<p>Figure 4. Illustration of different losses used in <em>Meta-Sim</em>, including forward and backward pass control flow for each step. We indicate transformed attributes of a scene graph by changing colors of the nodes.</p>
<p>a subset of attributes per node $v \in s_V$ which are mutable (modifiable), denoted by $s_{A,mut}(v)$. From here onwards, it is assumed that only the mutable attributes in $s_{A,mut}(v) \forall v$ are changed by $G_\theta$; others remain the same as in $s$.</p>
<h3>3.2. Training Meta-Sim</h3>
<p>We now introduce our learning framework. Since our learning problem is very hard and computationally intensive, we first pre-train our model using a simple autoencoder loss in Sec. 3.2.1. The distribution matching loss is presented in Sec 3.2.2, while meta-training is described in Sec 3.2.3. The overview of our model is given in Fig. 2, with the particular training objectives illustrated in Fig. 4.</p>
<h4>3.2.1 Pre-training: Autoencoder Loss</h4>
<p>A probabilistic scene grammar $P$ represents a prior on how a scene should be generated. Learning this prior is a natural way to pre-train our <em>Distribution Transformer</em>. This amounts to training $G_\theta$ to perform the identity function <em>i.e</em>. $G_\theta(s) = s$. The input feature of each node is its attribute set ($s_A$), which is defined consistently across all nodes (see suppl.). Since $s_A$ is composed of different categorical and continuous components, appropriate losses are used per feature component when training to reconstruct (<em>i.e</em>. cross-entropy loss for categorical attributes, and L1 loss for continuous attributes).</p>
<h4>3.2.2 Distribution Matching</h4>
<p>The first objective of training our model is to bring the distribution of the rendered images to be closer to the distribution of real imagery $X_R$. The Maximum Mean Discrepancy (MMD) [15] metric is a frequentist measure of the similarity of two distributions and has been used for training generative models [8, 29, 26] to match statistics of the generated distribution with the target distribution. An alternative, adversarial learning with discriminators, however, is known to suffer from mode collapse, and a general instability in training. Pixel-wise generative models with MMD have usually suffered from not being able to model high-frequency signals (resulting in blurry generations). Since our generative process goes through a renderer, we sidestep the issue altogether, and thus choose MMD for training stability.</p>
<p>We compute MMD in the feature space of an InceptionV3 [41] network (known as Kernel Inception Distance (KID) [2]). This feature extractor is denoted by the function $\phi$. We use the kernel trick for the computation with a gaussian kernel $k(x_i, x_j)$. We refer the reader to [29] for more details. The <em>Distribution Matching</em> box in Fig. 4 shows the training procedure pictorially. Specifically, given scene graphs $s_1, \ldots, s_N$ sampled from $P$ and target real images $X_R$, the squared MMD distance can be computed as,</p>
<p>$$
\mathcal{L}<em i="1">{MMD^2} = \frac{1}{N^2} \sum</em>), \phi(X^j'))
$$}^{N} \sum_{i'=1}^{N} k(\phi(X_{\theta}(s_i)), \phi(X_{\theta}(s_i')) + \frac{1}{M^2} \sum_{j=1}^{M} \sum_{j'=1}^{M} k(\phi(X^j_{R</p>
<p>$$
-\frac{1}{MN} \sum_{i=1}^{N} \sum_{j=1}^{M} k(\phi(X_{\theta}(s_i)), \phi(X^j_{R})) \tag{1}
$$</p>
<p>where the image rendered from $s$ is $X_\theta(s) = R(G_\theta(s)))$.</p>
<p><strong>Sampling from $G_\theta(s)$.</strong> For simplicity, we overloaded the notation $R(G_\theta(s))$, since $R$ would actually require sampling from the prediction $G_\theta(s)$. In general, we assume independence across scenes, nodes and attributes, which lets each attribute of each node in the scene graph be sampled independently. While training with $MMD$, we mark categorical attributes in $s_A$ as immutable. The predicted continuous attributes are directly passed as the sample.</p>
<p><strong>Backprop through a Renderer.</strong> For optimizing the MMD loss, we need to backpropagate the gradient through the non-differentiable rendering function $R$. The gradient of $R(G_\theta(s))$ w.r.t. $G_\theta(s)$ can be approximated using the method of finite differences[4]. While this gives us noisy gradients, we found it sufficient to be able to train our models in practice, with the benefit of being able to use photorealistic rendering. We note that recent work on differentiable rendering [20, 28] could potentially benefit this work.</p>
<h4>3.2.3 Optimizing Task Performance</h4>
<p>The second objective of training the model $G_\theta$ is to generate data $R(G_\theta(S))$ given samples $S = {s_1, \ldots, s_K}$ from</p>
<p>[4] computed by perturbing each attribute in the scene graph $G_\theta(s)$</p>
<p>Algorithm 1 Pseudocode for Meta-Sim's meta training phase
1: Given: $P, R, G_{\theta} \quad \triangleright$ Probabilistic grammar, Renderer, GCN Model
2: Given: TaskNet, $X_{R}, V \quad \triangleright$ Task Model, Real Images, Target Validation Data
3: Hyperparameters: $E_{m}, I_{m}, B_{m} \quad \triangleright$ Epochs, Iters, Batch size
4: while $e_{m} \leq E_{m}$ do $\triangleright$ Meta training
5: $\quad$ loss $=0$;
6: data $=[]$; samples $=[]$; $\triangleright$ Caching data \&amp; samples generated in epoch
7: while $i_{m} \leq I_{m}$ do
8: $\quad S=G_{\theta}\left(\right.$ sample $\left.\left(P, B_{m}\right)\right) ; \quad \triangleright$ Generate $B_{m}$ samples from $P$
9: and transform them
10: $\quad D=R(S) ; \quad \triangleright$ Render images, labels from $S$
11: data $+=D$; samples $+=S$;
12: $\quad$ loss $+=\mathcal{L}<em R="R">{M M D^{2}}\left(D, X</em>\right) ; \quad \triangleright$ MMD between generated and
13: target real images
14: end while
15: TaskNet $=$ train(TaskNet, data); $\quad \triangleright$ Train TaskNet on data
16: score $=$ text(TaskNet, $V$ ); $\quad \triangleright$ Test TaskNet on target val
17: $\quad$ loss $+=-($ score $-$ moving_avg $($ score $)) \cdot \log p_{G_{\theta}}$ (samples)
$\triangleright$ Eq. 3
18: $\quad G_{\theta}=$ optimize $\left(G_{\theta}\right.$, loss $) ; \quad \triangleright$ SGD step
19: end while
the probabilistic grammar $P$, such that a model trained on this data achieves best performance when tested on target data $V$. This can be interpreted as a meta-objective, where the input data must be optimized to improve accuracy on a validation set. We introduce a task network TaskNet to train using our data and to measure validation performance on. We train $G_{\theta}$ under the following objective,</p>
<p>$$
\max <em S_prime="S^{\prime">{\theta} \quad \mathbb{E}</em>\right)\right]
$$} \sim G_{\theta}(S)}\left[\operatorname{score}\left(S^{\prime</p>
<p>where $\operatorname{score}\left(S^{\prime}\right)$ is the performance metric achieved on validation data $V$ after training TaskNet on data $R\left(G_{\theta}\left(S^{\prime}\right)\right)$. The task loss in Eq. 2 is not differentiable w.r.t the parameters $\theta$, since the score is measured using validation data and not $S^{\prime}$. We use the REINFORCE score function estimator (which is an unbiased estimator of the gradient) to compute the gradients of Eq. 2. Reformulating the objective as a loss and writing the gradient gives,</p>
<p>$$
\begin{aligned}
\mathcal{L}<em S_prime="S^{\prime">{\text {task }} &amp; =-\mathbb{E}</em>\right)\right] \
\nabla_{\theta} \mathcal{L}} \sim G_{\theta}(S)}\left[\operatorname{score}\left(S^{\prime<em S_prime="S^{\prime">{\text {task }} &amp; =-\mathbb{E}</em>\right)\right]
\end{aligned}
$$} \sim G_{\theta}(S)}\left[\operatorname{score}\left(S^{\prime}\right) \times \nabla_{\theta} \log p_{G_{\theta}}\left(S^{\prime</p>
<p>To reduce the variance of the gradient from the estimator above, we keep track of an exponential moving average of previous scores and subtract it from the current score [14]. We approximate the expectation using one sample from $G_{\theta}(S)$. The Task Optimization box in Fig. 4 provides a pictorial overview of the task optimization.</p>
<p>Sampling from $\mathrm{G}_{\theta}(\mathrm{s})$. Eq. 3 requires us to be able to sample (and measure its likelihood) from our model. For continuous attributes, we interpret our model to be predicting the mean of a normal distribution per attribute, with a pre-defined variance. We use the reparametrization trick to sample from this normal distribution. For categorical attributes, it is possible to sample from a multinomial distribution from the predicted log probabilities per category. In this paper, we keep categorical attributes immutable.</p>
<p>Calculating $\log \mathrm{p}<em _theta="\theta">{\mathrm{G}</em>\right)$. Since we assume independence across scenes, attributes and objects in the scene, the likelihood in Eq 3 for the full scene is simply factorizable,}}\left(\mathrm{S}^{\prime</p>
<p>$$
\log p_{G}\left(S^{\prime}\right)=\sum_{s^{\prime} \in S^{\prime}} \sum_{v \in s_{V}^{\prime}} \sum_{a \in s_{A, m u t}^{\prime}(v)} \log p_{G_{\theta}}\left(s^{\prime}(v, a)\right)
$$</p>
<p>where $s^{\prime}(v, a)$ represents the attribute $a$ at node $v$ in a single scene $s^{\prime}$ in batch $S^{\prime}$. Note that the sum is only over mutable attributes per node $s_{A, m u t}(v)$. The individual log probabilities come from the defined sampling procedure.
Training Algorithm. The algorithm for training with Distribution Matching and Task Optimization is presented in Algorithm 1.</p>
<h2>4. Experiments</h2>
<p>We evaluate Meta-Sim on three target datasets with three different tasks. The subsequent sections follow a general structure where we first outline the desired task, the target data and the task network ${ }^{5}$. Then, we describe the probabilistic grammar that the Distribution Transformer utilizes for its input, and the associated renderer that generates labeled synthetic data. We show quantitative and qualitative results after training the task network using synthetic data generated by Meta-Sim. We show strong boosts in quantitative performance and noticeable qualitative improvements in content-generation quality.</p>
<p>The first two experiments presented are in a controlled setting, each with increasing complexity. The aim here is to probe Meta-Sim's capabilities when the shift between the target data distribution and the input distribution is known. The input distribution refers to the distribution of the scenes generated by samples from the probabilistic grammar that our Distribution Transformer takes as input. Target data for these tasks is created by carefully modifying the parameters of the probabilistic program, which represents a known distribution gap that the model must learn.</p>
<h3>4.1. MNIST</h3>
<p>We first evaluate our approach on digit classification on MNIST-like data. The probabilistic grammar samples a background texture, one digit texture (image) from the MNIST dataset [25] (which has an equal probability for any digit), and then samples a rotation and location for the digit. The renderer transforms the texture based on the sampled transformation and pastes it onto a canvas.
Task Network. Our task network is a small 2-layer CNN followed by 3 fully connected layers. We apply dropout in the fully connected layers (with 50, 100 and 10 features). We verify that this network can achieve greater than $99 \%$ accuracy on the regular MNIST classification task. We do not use data-augmentation while training (in all following</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Examples from the rotated-MNIST dataset</p>
<p>Figure 6. Examples from the rotated and translated MNIST experiments as well), as it might interfere with our model's training by changing the configuration of the generated data, making the task optimization signal unreliable.</p>
<p><strong>Rotating MNIST.</strong> In our first experiment, the probabilistic grammar generates input samples that are upright and centered, like regular MNIST digits (Fig 7 bottom). The target data $V$ and $X_R$ are images (at $32 \times 32$ resolution) where digits centered and always rotated by 90 degrees (Fig 5). Ideally, the model will learn this exact transformation, and rotate the digits in the input scene graph while keeping them in the same centered position.</p>
<p><strong>Rotating and Translating MNIST.</strong> For the second experiment, we additionally add translation to the distribution gap, making the task harder for Meta-Sim. We generate $V$ and $X_R$ as 1000 images (at $64 \times 64$ resolution) where in addition to being rotated by 90 degrees, the digits are moved to the bottom left corner of the canvas (Fig 6). The input probabilistic grammar remains the same, <em>i.e</em>. one that generates centered and upright digits (Fig. 8 bottom).</p>
<p><strong>Quantitative Results.</strong> Table 1 shows classification on the target datasets with the two distribution gaps described above. The target datasets are fresh samples from the target distribution (separate from $V$). Training directly on the input scenes (coming from the input probabilistic grammar <em>i.e</em>. generating upright and centered digits in this case) results in just above random performance. Our model recovers the transformation causing the distribution gap, and achieves greater than 99% classification accuracy.</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Rotation</th>
<th>Rotation + Translation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prob. Grammar</td>
<td>14.8</td>
<td>13.1</td>
</tr>
<tr>
<td>Meta-Sim</td>
<td>99.5</td>
<td>99.3</td>
</tr>
</tbody>
</table>
<p>Table 1. Classification performance on our MNIST with different distribution gaps in the data</p>
<p><strong>Qualitative Results.</strong> Fig. 7 and Fig. 8 show generations from our model at the end of training, and compares with the input scenes. Clearly, the model has learnt to perfectly transform the input distribution to replicate the target distribution, corroborating our quantitative results.</p>
<h3>4.2. Aerial Views (2D)</h3>
<p>Next, we evaluate our approach on <strong>semantic segmentation</strong> of simulated aerial views of simple roadways. In the probabilistic grammar, we sample a background grass texture, followed by a (straight) road at some location and</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7. (bottom) Input scenes, (top) Meta-Sim's generated examples for MNIST with rotation gap</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8. (bottom) Input scenes, (top) Meta-Sim's generated examples for MNIST with rotation and translation gap</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9. Example label and image from Aerial2D validation</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10. Example input scenes for Aerial2D rotation on the background. Next, we sample two cars with independent locations (constrained to be in the road by parametrizing in the road's coordinate system), and rotations. In addition, we also sample a tree and a house randomly in the scene. Each object in the scene gets a random texture from a set of textures we collected for each object. We ended up with nearly 600 car, 40 tree, 20 house, 7 grass and 4 road textures. Overall, this grammar has more complexity than MNIST, due to the scene graphs having higher depth, more objects, and variability in appearance.</p>
<p>$V$ and $X_R$ are created by tuning the grammar parameters to generate a realistic aerial view. (Fig. 9). The input probabilistic grammar uses random parameters (Fig. 10) bottom.</p>
<p><strong>Task Network.</strong> We use a small U-Net architecture [36] with a total of 7 convolutional layers (with 16 to 64 filters in the convolution layers) as our task-network.</p>
<p><strong>Quantitative Results.</strong> Table 2 shows semantic segmentation results on the target set. The results show that Meta-Sim effectively transforms the outputs of the probabilistic grammar, even in this relatively more complex setup, and improves the mean IoU. Specifically, it learns to drastically reduce the gap in performance for cars and also improves performance on roads.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 11: (bottom) input scenes, (top) Meta-Sim's generated examples for Aerial semantic segmentation</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Car</th>
<th>Road</th>
<th>House</th>
<th>Tree</th>
<th>Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prob. Grammar</td>
<td>30.0</td>
<td>93.1</td>
<td>98.3</td>
<td>99.7</td>
<td>80.3</td>
</tr>
<tr>
<td>MetaSim</td>
<td>86.7</td>
<td>99.6</td>
<td>95.0</td>
<td>99.5</td>
<td>95.2</td>
</tr>
</tbody>
</table>
<p>Table 2. Semantic segmentation results (IoU) on Aerial2D</p>
<p><strong>Qualitative Results.</strong> Qualitative results in Fig. 11 show that the model indeed learns to exploit the convolutional structure of the task network, by only learning to orient. This is sufficient to achieve its job since convolutions are translation equivariant, but not rotation equivariant.</p>
<h3>4.3 Driving Scenes (3D)</h3>
<p>After validating our approach on controlled experiments in a simulated setting, we now evaluate our approach for <strong>object detection</strong> on the challenging KITTI [12] dataset. KITTI was captured with a camera mounted on top of a car driving around the city of Karlsruhe in Germany. It consists of challenging traffic scenarios and scenes ranging from highways to urban to more rural neighborhoods. Contrary to the previous experiments, the distribution gap which we wish to reduce arises naturally here.</p>
<p>Current open-source self-driving simulators [7, 40] do not offer the amount of low-level control on object attributes that we require in our model. We thus turn to probabilistic grammars for road scenarios [33, 45]. Specifically, SDR [33] is a road scene grammar that has been shown to outperform existing synthetic datasets as measured by sim-to-real performance. We adopt a simpler version of SDR and implement portions of their grammar as our probabilistic grammar. Specifically, we remove support for intersections and side-roads for computational reasons. The exact parameters of the grammar used can be found in the supplementary material. We use the Unreal Engine 4 (UE4) [1] game engine for the 3D rendering from scene graphs. Fig. 12(left column) shows example renderings of scenes generated using our version of the SDR grammar. The grammar parameters were mildly tuned, since we aim to have our model do the heavy lifting in subsequent parts.</p>
<p><strong>Task Network.</strong> We use Mask-RCNN [16] with a Resnet-50-FPN backbone (ImageNet initialized) detection head as our task network for object detection.</p>
<p><strong>Experimental Setup.</strong> Following SDR [33], we use car detection as our task. Validation data <em>V</em> is formed by taking 100 random images (and their labels) from the KITTI train set. The rest of the training data (images only) forms <em>X</em>_{R}. We report results on the KITTI val set. Training and finer details can be found in the supplementary material.</p>
<p><strong>Complexity.</strong> To reduce training complexity (coming from rendering and numerical gradients), we train Meta-Sim to optimize specific parts of the scene sequentially. We first train to optimize attributes of cars. Next, we optimize car and camera parameters, and finally add parameters of context elements (buildings, pedestrians, trees) together to the training. Similarly, we decouple distribution and task training. We first train the above with MMD, and finally optimize all parameters above with the meta task loss.</p>
<p><strong>Quantitative Results.</strong> Table 3 reports the average precision at 0.5 IoU of the task network trained using data generated from different methods, when tested on the KITTI val set. We see that training with Meta-Sim beats just using the data from the probabilistic grammar.</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Easy</th>
<th>Moderate</th>
<th>Hard</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prob. Grammar</td>
<td>63.7</td>
<td>63.7</td>
<td>62.2</td>
</tr>
<tr>
<td>MetaSim (Cars)</td>
<td>66.4</td>
<td>66.5</td>
<td>65.6</td>
</tr>
<tr>
<td>+ Camera</td>
<td>65.9</td>
<td>66.3</td>
<td>65.9</td>
</tr>
<tr>
<td>+ Context</td>
<td>65.9</td>
<td>66.3</td>
<td>66.0</td>
</tr>
<tr>
<td>+ Task Loss</td>
<td>66.7</td>
<td>66.3</td>
<td>66.2</td>
</tr>
</tbody>
</table>
<p>Table 3. AP @ 0.5 IOU for car detection on the KITTI val dataset</p>
<p>Training the task network online with meta-sim and offline on final generated data results in similar final detection performance. This ensures the quality of the final generated data, since training while the transformation of data is being learned could be seen as data augmentation.</p>
<p><strong>Bridging the appearance gap.</strong> We additionally add a state-of-the-art image-to-image translation network, MUNIT [18] to attempt to bridge the appearance gap between the generated synthetic images and real images. Table 4 shows training with image-to-image translation still leaves a performance gap between MetaSim and the baseline, confirming our <em>content gap</em> hypothesis.</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>Easy</th>
<th>Moderate</th>
<th>Hard</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prob. Grammar</td>
<td>71.1</td>
<td>75.5</td>
<td>65.3</td>
</tr>
<tr>
<td>Meta-Sim</td>
<td>77.5</td>
<td>75.1</td>
<td>68.2</td>
</tr>
</tbody>
</table>
<p>Table 4. Effect of adding image-to-image translation to bridge the appearance gap in generated images</p>
<p><strong>Training on <em>V</em>.</strong> Since we have access to some labelled training data, a valid baseline is to train the models on <em>V</em> (100 images from KITTI train split). In Table 5 we show the effect of only training with <em>V</em> and finetuning using <em>V</em>.</p>
<table>
<thead>
<tr>
<th>TaskNet Initialization</th>
<th>Easy</th>
<th>Moderate</th>
<th>Hard</th>
</tr>
</thead>
<tbody>
<tr>
<td>ImageNet</td>
<td>61.2</td>
<td>62.0</td>
<td>60.7</td>
</tr>
<tr>
<td>Prob. Grammar</td>
<td>71.3</td>
<td>72.7</td>
<td>72.7</td>
</tr>
<tr>
<td>Meta-Sim (Task Loss)</td>
<td>72.4</td>
<td>73.9</td>
<td>73.9</td>
</tr>
</tbody>
</table>
<p>Table 5. Effect of finetuning on <em>V</em></p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12. (left) samples from our prob. grammar, (middle) Meta-Sim's corresponding samples, (right) random samples from KITTI</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13. Animation showing evolution of a scene through Meta-Sim training (animation works on Adobe Reader)</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 14. Car detection results (top) of task network trained with Meta-Sim vs (bottom) trained with our prob. grammar</p>
<h3>Qualitative Results</h3>
<p>Fig. 12 shows a few outputs of Meta-Sim compared to the inputs sampled from the grammar, along with a few random samples from KITTI(train). There is a noticeable difference, as Meta-Sim's cars are well aligned with the road, and the distances between cars are meaningful. Also notice the small changes in camera, and the differences in the context elements, including houses, trees, and pedestrians. The last row in Fig. 12 represents a failure case where Meta-Sim is unable to clear up a dense initial scene, resulting in collided cars. Interestingly, Meta-Sim perfectly overlaps two cars in the same image such that a single car is visible from the camera (first car in front of camera). This behavior is seen multiple times, indicating that the model learns to cheat its way to good data. Fig. 13 shows an animation representing how a scene evolves through meta-sim training. Elements are moved to final configurations sequentially, following our training procedure. We remind the reader that these scene configurations are learned with only image/task level supervision. In Fig. 18, we show results of training the task network on our grammar vs. training with Meta-Sim. We observe fewer false positives and negatives than the baseline. Meta-Sim shows better recall and GT overlap. Both models lose in precision, arguably because of not training for similar classes like Bus/Truck which would be negative examples.</p>
<h2>5 Conclusion</h2>
<p>We proposed Meta-Sim, an approach that generates synthetic data to match real content distributions while optimizing performance on downstream (real) tasks. Our model learns to transform sampled scenes from a probabilistic grammar so as to satisfy these objectives. Experiments on two toy and one real task showcased that Meta-Sim generates quantitatively better and noticeably higher quality samples than the baseline. We hope this opens a new exciting</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 15. Meta-Sim generated samples (top two rows) and input scenes (bottom two rows)
direction for simulation in the computer vision community. Like any other method, it has its limitations. It relies on obtaining valid scene structures from a grammar, and hence is still limited in the kinds of scenes it can model. Inferring rules of the grammar from real images, learning to generate structure of scenes and introducing multimodality in the model are intriguing avenues for future work.</p>
<p>Acknowledgements: The authors would like to thank Shaad Boochoon, Felipe Alves, Gavriel State, JeanFrancois Lafleche, Kevin Newkirk, Lou Rohan, Johnny Costello, Dane Johnston and Rev Lebaredian for their help and support throughout this project.</p>
<h2>6 Appendix</h2>
<p>We provide additional details about Meta-Sim, as well as include more results.</p>
<h3>6.1 Grammar and Attribute Sets</h3>
<p>First, we describe the probabilistic grammars for each experiment in more detail, as well as attribute sets. We remind the reader that the probabilistic grammar defines the structure of the scenes, and is used to sample scene graphs that are input to Meta-Sim. The Distribution Transformer transforms the attributes on the nodes in order to optimize its objectives.</p>
<h3>6.1.1 MNIST</h3>
<p>Grammar. The sampling process for the probabilistic grammar used in the MNIST experiments is explained in the main paper, and we review it here. It samples a background texture and one digit texture (image) from the MNIST dataset [25] (which has an equal probability for any digit), and then samples a rotation and location for the digit, to place it on the scene.</p>
<p>Attribute Set. The attribute set $s_{A}$ for each node consists of [class, rotation, locationX, locationY, size]. Class is a one-hot vector, spanning all possible node classes in the
graph. This includes, [scene, background, 0, ..., 9]. scene is the root node of the scene graph. The rotation, locations and size are floating point numbers between 0 to 1 , and represent values in appropriate ranges i.e. rotation in 0 to 360 degrees, each location within the parent's boundaries etc.</p>
<h3>6.1.2 Aerial Views (2D)</h3>
<p>Grammar. The probabilistic grammar is explained in the main paper, and we summarize it again here. First, a background grass texture is sampled. Next, a (straight) road is sampled at a location and rotation on the background. Two cars are then sampled with independent locations (constrained to be in the road by parametrizing in the roads coordinate system), and rotations. In addition, a tree and a house are sampled and placed randomly on the scene. Each object in the scene also gets assigned a random texture from a repository of collected textures.</p>
<p>Attribute Set. The attribute set $s_{A}$ for each node in the Aerial 2D experiment consists of [class, rotation, locationX, locationY, size]. Our choice of class includes road, car, tree, house and background, with class as a one-hot vector. Subclasses, such as type of tree, car, etc, are not included in the attribute set. They are sampled randomly in the grammar and are left unchanged by the Distribution Transformer. The values are normalized similarly to that in the MNIST experiment. We learn to transform the rotation and location for every object in the scene, but leave size unchanged, i.e. size $\notin s_{A, m u t}$.</p>
<h3>6.1.3 Driving Scenes (3D)</h3>
<p>Grammar. The grammar used in the driving experiment is adapted from [33] (Section III). Our adaptation uses some fixed global parameters (i.e. weather type, sky type) as compared to SDR. Our camera optimization experiment learns the height of the camera from the ground, while scene contrast, saturation and light direction are randomized. We also do not implement side-streets and parking lanes.</p>
<p>Attribute Set. The attribute set $s_{A}$ for each node in the Driving Scene 3D experiment consists of [class, rotation, distance, offset]. The classes, subclasses and ranges are treated exactly like in the Aerial 2D experiment. Distance indicates how far along the parent spline the object is being placed and offset indicates how much across the parent spline the object is placed. For each experiment, a subset of nodes in the graph is kept mutable. We optimized the attributes of cars, context elements such as buildings, foliage and people. Camera height from the global parameters was optimized, which was injected into the sky node encoded in the distance attribute.</p>
<h3>6.2 Training Details</h3>
<p>Let $a_{i n}$ be the number of attributes in $s_{A}$ for each dataset. Our Distribution Transformer use Graph Convolutional lay-</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 16. Successful cases: (left) input scenes from the probabilistic grammar, (right) Meta-Simâs generated examples for the task of car detection on KITTI. Notice how meta-sim learns to align objects in the scene, slightly change the camera position and move context elements such as buildings and trees, usually densifying the scene.</p>
<p>ers with different weight matrices for each direction of the edge following [47]. We use 2 layers for the encoder and 2 layers for the decoder. In the MNIST and Aerial2D experiments, it has the following structure: Encoder ($a_{in} \rightarrow 16-&gt;10$), and Decoder ($10-&gt;16-&gt;a_{in}$). For 3D driving scenes, we use 28 and 18 feature size in the intermediate layers.</p>
<p>After training the autoencoder step, the encoder is kept frozen and only the decoder is trained (<em>i.e</em>. for the distribution matching and task optimization steps). For the MMD computation, we always use the pool1 and pool2 feature layers of the InceptionV3 network [41]. For the MNIST and Aerial2D experiments, the images are not resized before passing them through the Inception network. For the</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 17. Failure cases: (left) input scenes from the probabilistic grammar, (right) Meta-Sim's generated examples for the task of car detection on KITTI. Initially dense scenes are sometimes unresolved, leading to collisions in the final scenes. There are unrealistic colours on cars since they are sampled from the prior and not optimized in this work. In the last row, meta-sim moves a car very close to the ego-car (camera).</p>
<p>Driving Scenes (3D) experiment, they are resized to 299 x 299 (standard inception input size) before computing the features. The task network training is iterative, our Distribution Transformer is trained for one epoch, followed by training of the task network for a few epochs.</p>
<h3>6.2.1 MNIST</h3>
<p>For the MNIST experiment, 500 samples coming from the probabilistic grammar are said to constitute 1 epoch.</p>
<p><strong>Distribution Transformer.</strong> We first train the autoencoder step of the Distribution Transformer for 8 epochs, with batch size of 8 and learning rate 0.001 using the Adam optimizer [21]. Next, training is done with the joint loss, i.e., Distribution Loss and Task Loss, for 100 epochs. Note that when training with the task loss, there is only a 1 gradient step per epoch. We note that the MNIST tasks could be solved with the Distribution Loss alone, in around 3 epochs (since we can backpropagate the gradient for the distribution loss per minibatch when training only with the Distribution Loss).</p>
<p><strong>Task Network.</strong> After getting each epoch of data from the Distribution Transformer, the Task Network is trained with this data for 2 epochs. The optimization was done using SGD with learning rate 0.01 and momentum 0.9. Note that the task network is not trained from scratch every time. Rather, training is resumed from the checkpoint obtained in the previous epoch.</p>
<h3>6.2.2 Aerial Views (2D)</h3>
<p>For the Aerial Views (2D) experiment, 100 samples coming from the probabilistic grammar are considered to be 1 epoch.</p>
<p><strong>Distribution Transformer.</strong> Here, the autoencoder step is trained for 300 epochs using the Adam optimizer with batch size 8 and learning rate 0.01, and then for an additional 100 epochs with learning rate 0.001. Finally, the model is trained jointly with the distribution and task loss for 100 epochs with the same batch size and a learning rate of 0.001.</p>
<p><strong>Task Network.</strong> After getting each epoch of data from the Distribution Transformer, the Task Network is trained with this data for 8 epochs using SGD with a learning rate of 1e-8. Similar to the previous experiment, the task-network training is resumed instead of training from scratch.</p>
<h3>6.2.3 Driving Scenes (3D)</h3>
<p>For Driving Scenes (3D), 256 samples coming from the probabilistic grammar are considered to be 1 epoch.</p>
<p><strong>Distribution Transformer.</strong> In this case, the autoencoder step is trained for 100 epochs using the Adam optimizer with a batch size of 16 and learning rate 0.005. Next, the model is trained with the distribution loss for 40 epochs with the same batch size and learning rate 0.001, finally stopping at the best validation (in our case, performance on the 100 images taken from the training set) performance. Finally, the model is trained with the task loss for 8 epochs with the same learning rate.</p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 18. Car detection results with Mask-RCNN trained on (right) the dataset generated by Meta-Sim, and (left) the original dataset obtained by the probabilistic grammar. Notice how the models trained with meta-sim have lesser false positives and negatives.</p>
<p>Task Network. For task network, we use MaskRCNN [16] with a FPN and Resnet-101 backbone. We pre-train the task network (with freely available data sampled from the probabilistic grammar), saving training time by not having to train the detector from scratch. We also do not train the task network from scratch every epoch. Rather, we resume training from the checkpoint from the previous epoch. We also heavily parallelize the rendering computation by working on multiple UE4 workers simultaneously and utilizing their batched computation features.</p>
<h3>6.3. Additional Results</h3>
<h3>6.3.1 Aerial Views (2D)</h3>
<p>Fig. 15 shows additional qualitative results for the Aerial 2D experiment, comparing the original samples to those obtained via Meta-Sim. The network learns to utilize the translation equivariance of convolutions and makes sufficient transformations to the objects for semantic segmentation to work on the validation data. Translating objects (such as cars) in the scene is still important, since our tasknetwork for Aerial-2D has a maximum receptive field which is a quarter of the longest side of the image.</p>
<h3>6.3.2 Driving Scenes (3D)</h3>
<p>Fig 16 shows a few positive results for Meta-Sim. Our approach learns to rotate cars to look more like those in KITTI scenes and learns to place objects to avoid collisions between objects. The examples show that it can handle these even with a lot of objects present in the scene, while failing in some cases. It has also learnt to push cars out of the</p>
<p>view of the camera when required. Sometimes, it positions two cars perfectly on top of each other, so that only one car is visible from the camera. This is in fact a viable solution since we solve for occlusion and truncation before generating ground truth bounding boxes. We also notice that the model modifies the camera height slightly, and moves context elements, usually densifying scenes.</p>
<p>Fig. 17 shows failure cases for Meta-Sim. Sometimes, the model does not resolve occlusions/intersections correctly, resulting in final scenes with cars intersection each other. Sometimes it places an extra car between two cars (top row), attempts a partial merge of two cars (second row), causes collision (third row) and fails to rotate cars in the driving lane (final row). In this experiment, we did not model the size of objects in the features used in the Distribution Transformer. This could be a major reason behind this, since different assets (cars, houses) have different sizes and therefore placing them at the same location is not enough to ensure perfect occlusion between them. Meta-Sim sometimes fails to deal with situations where the probabilistic grammar samples objects densely around a single location (Fig. 17). Another failure mode is when the model moves cars unrealistically close to the ego-car (camera) and when it overlaps buildings to create unrealistic composite buildings. The right column of fig. 18 show additional detection results on KITTI when the task-network is trained with images generated by Meta-Sim. The left column shows results when trained with samples from the probabilistic grammar. In general, we see see fewer false positives and negatives with Meta-sim. The results are also significant given that the task network has not seen any real KITTI images during training.</p>
<h2>References</h2>
<p>[1] https://www.unrealengine.com/. 7
[2] M. BiÅkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying mmd gans. ICLR, 2018. 4
[3] A. Brock, J. Donahue, and K. Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 3
[4] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. In arXiv:1606.01540, 2016. 2
[5] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In A. Fitzgibbon et al. (Eds.), editor, ECCV, Part IV, LNCS 7577, pages 611-625. Springer-Verlag, 2012. 1, 2
[6] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv preprint arXiv:1810.05687, 2018. 2
[7] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. CARLA: An open urban driving simulator. In $C O R L$, pages 1-16, 2017. 1, 2, 7
[8] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In $U A I, 2015.4$
[9] G. French, M. Mackiewicz, and M. Fisher. Self-ensembling for visual domain adaptation. In ICLR, 2018. 1, 2
[10] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In CVPR, 2016. 1,2
[11] Y. Ganin, T. Kulkarni, I. Babuschkin, S. Eslami, and O. Vinyals. Synthesizing programs for images using reinforced adversarial learning. arXiv preprint arXiv:1804.01118, 2018. 2
[12] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In CVPR, 2012. 1, 7
[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014. 2
[14] E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471-1530, 2004. 5
[15] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. SchÃ¶lkopf, and A. Smola. A kernel two-sample test. JMLR, 2012. 2, 4
[16] K. He, G. Gkioxari, P. DollÃ¡r, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 7, 11
[17] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isol, K. S. A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018. 1, 2
[18] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018. 1, 2,7
[19] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. arXiv preprint arXiv:1812.04948, 2018. 3
[20] H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In CVPR, 2018. 4
[21] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 11
[22] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017. 3
[23] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. Ai2-thor: An interactive 3d environment for visual ai. In arXiv:1712.05474, 2017. 1, 2
[24] T. D. Kulkarni, P. Kohli, J. B. Tenenbaum, and V. Mansinghka. Picture: A probabilistic programming language for scene perception. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 4390-4399, 2015. 2
[25] Y. LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/. 5, 9
[26] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. PÃ³czos. Mmd gan: Towards deeper understanding of moment matching network. In NIPS, 2017. 4
[27] P. Li, X. Liang, D. Jia, and E. P. Xing. Semantic-aware gradgan for virtual-to-real urban scene adaption. In BMVC, 2018. 1,2</p>
<p>[28] T.-M. Li, M. Aittala, F. Durand, and J. Lehtinen. Differentiable monte carlo ray tracing through edge sampling. ACM Trans. Graph. (Proc. SIGGRAPH Asia), 2018. 4
[29] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In ICML, 2015. 4
[30] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-toimage translation networks. In NIPS, 2017. 1, 2
[31] G. Louppe and K. Cranmer. Adversarial variational optimization of non-differentiable simulators. arXiv preprint arXiv:1707.07113, 2017. 2
[32] V. K. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. Tenenbaum. Approximate bayesian image interpretation using generative probabilistic graphics programs. In Advances in Neural Information Processing Systems, pages 1520-1528, 2013. 2
[33] A. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield. Structured domain randomization: Bridging the reality gap by contextaware synthetic data. In arXiv:1810.10093, 2018. 1, 2, 7, 9
[34] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs. In CVPR, 2018. 2
[35] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016. 1,2
[36] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234-241. Springer, 2015. 6
[37] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, 2016. 1, 2
[38] N. Ruiz, S. Schulter, and M. Chandraker. Learning to simulate. arXiv preprint arXiv:1810.02513, 2018. 2
[39] F. Sadeghi and S. Levine. Cad2rl: Real single-image flight without a single real image. arXiv preprint arXiv:1611.04201, 2016. 2
[40] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Aerial Informatics and Robotics platform. Technical Report MSR-TR-20179, Microsoft Research, 2017. 1, 2, 7
[41] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. 4, 10
[42] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, 2017. 1, 2
[43] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In Intl. Conf. on Intelligent Robots and Systems, 2012. 2
[44] Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018. 1
[45] M. Wrenninge and J. Unger. Synscapes: A photorealistic synthetic dataset for street scene parsing. In arXiv:1810.08705, 2018. 2, 7
[46] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tiani. Building generalizable agents with a realistic and rich 3d environment. In arXiv:1801.02209, 2018. 1, 2
[47] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relationship for image captioning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 684699, 2018. 3, 9
[48] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networkss. In ICCV, 2017. 1, 2
[49] S.-C. Zhu, D. Mumford, et al. A stochastic grammar of images. Foundations and Trends ${ }^{\circledR}$ in Computer Graphics and Vision, 2(4):259-362, 2007. 3
[50] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In ICLR, 2017. 3
[51] Y. Zou, Z. Yu, B. V. K. V. Kumar, and J. Wang. Domain adaptation for semantic segmentation via class-balanced selftraining. In ECCV, 2018. 1, 2</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Task Network training details in suppl. material&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
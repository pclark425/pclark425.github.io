<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5340 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5340</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5340</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-264828854</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.conll-1.25.pdf" target="_blank">Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</a></p>
                <p><strong>Paper Abstract:</strong> To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs’ robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5340.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5340.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A >340B-parameter, instruction-tuned transformer LLM from OpenAI, evaluated in this paper on multiple Theory of Mind (ToM) tests and consistently showing the strongest performance among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large transformer model trained with large-scale text and human feedback (alignment/instruction tuning) to follow natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (first-order SA1 and second-order SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief (social cognition, language)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Narrative false-belief tasks: SA1 asks where a character will search for an object she falsely believes is in its original location; SA2 asks about what one character believes another character believes (second-order). Each item includes an experimental question and a follow-up motivation 'Why?' question.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed well above child level on SA1 and remained above child level on many SA2 items; performance degraded with deviations (especially deviation level 2) but GPT-4 still outperformed most other models on SA tasks according to qualitative results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) performed at x = 0.45 on SA1 and x = 0.225 on SA2 (means reported for 7-8-year-old group used as child baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 generally outperformed the child baseline on SA1 and often remained above or near child level on SA2, but deviations from original scenarios reduced performance and could push it closer to or below child level for the hardest/deviated SA2 items.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Although GPT-4 is strongest, its SA2 performance is sensitive to test deviations (generalization limits). The study cautions that passing ToM tests does not imply human-like ToM; prompts and deviation manipulations affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5340.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on the Strange Stories (non-literal language / ToM) battery; shows near-ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM trained with human feedback; optimized for following instructions and conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / Theory of Mind / social reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven short vignettes involving non-literal language (white lie, sarcasm, pretend, misunderstanding, etc.); experimental question (e.g., 'Is what the girl says true?') and motivation question ('Why does she say this?').</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Approached perfect scores across Strange Stories items (near-ceiling performance reported qualitatively). Performance was robust across deviations from original formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) showed declining scores as item difficulty increased; exact numeric means not provided in text for each item.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 matched or exceeded child performance across Strange Stories items and was notably more robust to deviations than many other models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>High performance likely reflects exposure in training data to many non-literal language contexts; strong SS performance does not directly imply grounded social cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5340.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_IM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated on the Imposing Memory test (recursive intentionality vs. memory), showing sustained performance across recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM from OpenAI with strong instruction-following ability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>340B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality (higher-order ToM) and memory (working/episodic recall)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Two long narratives followed by true/false questions: 10 intentionality items requiring nested belief reasoning across up to five recursion levels, and 12 matched memory items requiring factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 performed consistently well across recursion levels and stayed above child-level performance after second-order intentionality; qualitative report indicates GPT-4 'passes well' compared to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y, n=36) showed high memory question accuracy (> .85 across levels except a dip at level 4) and a cut-off in intentionality after level two (significant drop between level 1 and 2; β = −0.222, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 outperformed other LLMs and exceeded child baseline on IM after second-order intentionality; most other LLMs performed below child level on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>IM was the most challenging test and only GPT-4 passed robustly; differences may reflect GPT-4's larger context/window and instruction tuning; prompt length/context window limits and chain-of-thought abilities may constrain other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5340.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned 175B-parameter GPT-3.5 series model evaluated across all ToM tests; strong on some tasks but sensitive to deviations and recursion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned member of the GPT family, trained with reinforcement learning from human feedback to follow instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First-order (SA1) and second-order (SA2) false-belief narratives with experimental and motivation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed well above child level on SA1 and remained above child level on SA2 in default (non-deviated) versions; however, combination of second-order ToM with deviation level 2 substantially reduced performance (pushed below child level).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 often outperformed child baseline on original SA1 and many SA2 items but showed reduced generalization under deviated formulations and more complex second-order/deviated cases.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Sensitive to deviations from standard SA formulations; generalization to novel phrasing is limited compared to raw performance on canonical prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5340.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on Strange Stories; good performance on simpler items and generally above child baseline but declines on later items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM capable of instruction-following and conversational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven items probing intention inference for non-literal language acts (lie, pretend, sarcasm, white lie, sarcasm, etc.) with experimental and motivation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed at or close to child level on item 1, then performance somewhat declined across more complex items but stayed well above child level overall (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) show decreasing performance as item complexity increases; exact numeric item-level scores not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5 generally matched or exceeded child-level performance on Strange Stories, particularly on earlier items; decline with item difficulty was milder for the model than for children.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Though relatively strong on SS, GPT-3.5 still shows item-dependent declines; robustness to deviations is better than for some models but inferior to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5340.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_IM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5's performance on the Imposing Memory test was among the weakest of the instruct-tuned models, especially on recursive intentionality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-series model trained with human feedback to follow instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality (higher-order ToM) and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Long narratives followed by true/false intentionality and memory questions across five recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-3.5 performed worst of the instruct-LLMs on IM; showed poor performance on intentionality questions and did not reach child-level performance overall.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y, n=36) showed high memory accuracy (> .85 across most levels) and declined on intentionality after level two (statistical drop between level 1 and 2: β = −0.222, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to the child baseline and relative to GPT-4; GPT-3.5 struggled especially on recursive intentionality while children showed a specific cut-off after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>IM is prompt-length intensive; GPT-3.5 may be constrained by context window and chain-of-thought limitations compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5340.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-003, instruction-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter GPT-3 family model (davinci variant) included as an instruct-LLM; showed mixed performance with structurally low scores on some tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLM (GPT-3 family), instruction-tuned variant (text-davinci-003) trained with reinforcement learning from human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief narrative tasks with experimental and motivation components.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported structurally low scores on SA tasks in this study, inconsistent with prior work (Kosinski, 2023) which used different prompting methods; performed worse relative to many other instruction-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to the child baseline on SA especially compared to larger instruct-LLMs (GPT-3.5, GPT-4); differences may be attributable to prompt format (open question prompting vs. completion).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The authors note that prompting format strongly affects GPT-3 results and that differences with previous reports likely stem from using open questions rather than text-completion prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5340.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3_SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3's performance on Strange Stories was above child level for simpler items but declined for more complex items; overall did well compared to many base models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant trained using human feedback for instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven vignettes requiring inference of intentions and motivations behind non-literal language acts; answers include experimental and motivation responses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed at or close to child level on item 1 and declined somewhat on more complex items but remained well above child level overall according to qualitative reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) whose performance drops more steeply as item complexity increases; item-level numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generally matched or exceeded child performance on early SS items and remained comparatively strong though not as robust as GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Prompting choices (open-question vs completion) influenced performance; deviations had limited effect on larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5340.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large Google transformer family model (estimated 175-340B parameters) evaluated as both base and chat variants; instruction-tuned variants performed better on some ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM from Google (PaLM family); PaLM2 and PaLM2-chat are instruction-tuned/chat-optimized variants (size estimated 175–340B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (estimated)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Canonical first- and second-order false-belief tasks with motivation follow-ups.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2 and PaLM2-chat performed well above child level on first-order SA1, but performance degraded on second-order SA2; some instruct variants (PaLM2-chat) were notably perturbed by second-order items and deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2 variants generally outperformed children on SA1 but tended to fall to or below child level on SA2, especially with deviations; some chat-tuned versions were more affected.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Second-order ToM and deviation manipulations exposed generalization limits; instruction-tuning helps but does not guarantee robust second-order ToM across novel formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5340.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2_SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 (chat and base variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM2 variants evaluated on Strange Stories: chat and some instruction variants matched or exceeded child-level performance on non-literal language items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2 / PaLM2-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer LLMs from Google; chat variants optimized for dialogue and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175-340B (estimated)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven-item battery probing non-literal language understanding and intention attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>PaLM2-chat and PaLM2 performed close to or above child level on SS items; deviation levels had little effect on larger PaLM2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) show decreasing performance with item complexity; precise values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PaLM2 variants matched or exceeded child-level performance on many SS items, particularly the chat-tuned version.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>PaLM2's strength on non-literal language likely reflects training data; second-order false-belief tasks are more challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5340.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (instruction-tuned, 11B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 11B-parameter instruction-tuned encoder-decoder Transformer (FLAN-T5) that shows mixed performance: comparatively good on some Strange Stories items and improving with recursion on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned T5-family encoder-decoder model (FLAN variants) trained to follow natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven non-literal-language vignettes requiring intention inference and motivation explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FLAN-T5 surpassed child level earlier than PaLM2 on some SS items and outperformed larger PaLM variants on more difficult SS items (qualitative); for IM, FLAN-T5 increased performance as recursion increased and ended at child level.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) show declining SS scores with increasing item difficulty; IM child memory > .85 and intentionality shows cutoff after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On SS, FLAN-T5 often matched or exceeded child baseline on mid-range items; on IM, FLAN-T5 rose to child-level performance at higher recursion, unlike many other instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>As a relatively small instruct-LLM, FLAN-T5 shows surprising strengths on non-literal language and certain recursive tasks, suggesting instruction-tuning can compensate for smaller model size in some respects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e5340.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon (base) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter base transformer LLM evaluated as a base (non-instruction-tuned) model; generally performs below children on ToM tests except some first-order SA1 cases for larger base LLMs only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B-parameter transformer LLM trained on web-scale data (base, not instruction-tuned in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief tasks with motivation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed poorly on SA tasks: smaller base models (including Falcon-7B) performed worse overall and tended to be at or below child level especially on second-order SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Falcon-7B underperformed compared to child baseline on SA1 and especially on SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base-LLMs generally fared worse than instruct-LLMs; prompt format differences and lack of instruction-tuning limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e5340.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (Falcon-7B-I)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned 7B Falcon variant; in this study it performed worse than most instruct-LLMs and failed on many second-order ToM items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of the Falcon family (7B parameters) optimized for following instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief narratives with motivation follow-ups.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Falcon-7B-I failed on all second-order SA2 questions and performed overall worse than larger instruct-LLMs; on Strange Stories it performed worst among instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline on second-order ToM and on Strange Stories compared to larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small instruction-tuned models may still lack sufficient capacity or fine-tuning breadth for robust ToM; deviation robustness was low.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e5340.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (30B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 30B-parameter base LLM (LLaMA family) tested as a base model; performed above child level on SA1 but fell to or below child level on SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>30B-parameter base transformer LLM (LLaMA family), trained on large text corpora but not instruction-tuned in this study's base configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>First- and second-order false-belief narrative tasks with motivation follow-ups.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed above child level on first-order SA1 but fell to or below child level on second-order SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Matched or exceeded child baseline on SA1 but underperformed on SA2.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>As a base model, LLaMA-30B lacks instruction-tuning advantages and shows limited generalization to more complex, recursive ToM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e5340.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLOOM (176B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 176B-parameter multilingual base LLM that in this study performed above child level on SA1 but fell at/below child level on SA2 and generally below child level on other tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large open multilingual transformer model trained on large corpora (base LLM, not instruction-tuned in the study's base configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>176B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic first- and second-order false-belief narratives with experimental and motivation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BLOOM performed above child level on SA1 but fell at or below child level on SA2; on Strange Stories and IM it performed below child level generally.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225; IM memory > .85 for 9-10y children and intentionality shows cutoff after level two.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperformed child baseline on SA1 only; underperformed on SA2 and on other ToM tests relative to children.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base-LLM status and prompt/format limitations likely suppressed some of BLOOM's latent capabilities; instruction-tuning appears critical for broader ToM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e5340.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-davinci (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-davinci (base-LLM, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter base GPT-3 family model used in base (non-instruction-tuned) mode; showed some improvement on higher recursion levels but overall below child level on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-davinci (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer base LLM (GPT-3/davinci family) evaluated in base completion mode rather than instruct mode in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Long narrative test with true/false questions probing nested intentionality up to five levels and matched memory items.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>As a larger base-LLM, GPT-davinci showed some improvement on higher levels of recursion compared to smaller base models but remained below child-level performance overall on IM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y) memory > .85 across most levels; intentionality drops after level two (statistical drop noted).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline on IM despite some gains at higher recursion; outperformed smaller base models but not instruction-tuned top models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base-LLM prompting differences and lack of instruction tuning limit comparability; authors note base-LLMs had different prompt formats (text-completion) which could both help and hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e5340.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-instruct (Falcon-7B-I)_SS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-instruct (Falcon-7B-I)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned Falcon 7B variant evaluated on Strange Stories; among the weakest performing instruct-LLMs on SS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-7B-I</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned 7B-parameter Falcon variant optimized for instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven vignettes requiring intention attribution when non-literal language is used.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performed overall worst among instruct-LLMs on Strange Stories according to qualitative summary.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) show decreasing performance as items become more complex; numeric values not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Underperformed relative to child baseline on many SS items and performed worse than larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Small instruction-tuned models may still lack capacity or broad instruction-tuning coverage to handle diverse non-literal scenarios robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e5340.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summary_base_vs_instruct_SA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Base-LLMs vs. Instruction-tuned LLMs on Sally-Anne</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate comparison reported in the paper: base-LLMs generally perform worse than instruct-LLMs on ToM false-belief tasks, especially on second-order items and deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregate of tested base and instruction-tuned LLMs (Falcon, LLaMA, BLOOM, GPT variants, PaLM2, FLAN-T5, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–>340B)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2) aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Theory of Mind / false-belief</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Aggregate evaluation across first- and second-order false-belief tests and deviated formulations to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>In aggregate: many base-LLMs could do SA1 sometimes (some larger base models), but most base-LLMs and smaller instruct-LLMs failed at SA2; larger instruct-LLMs (GPT-4, GPT-3.5) did better on SA2 but performance dropped with deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) SA1 mean = 0.45; SA2 mean = 0.225 (reference dashed lines used in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Instruction-tuned large models generally outperform base models and often exceed child baseline on SA1 and some SA2 items; base models underperform children on SA2 and other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Deviations from canonical test texts reveal poor generalization in many models; prompting style differences (instruct vs completion) affected some base-model results, complicating direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e5340.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summary_SS_aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate Strange Stories results (base vs instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate finding: instruct-LLMs (especially GPT-4) handle non-literal language in Strange Stories better than base-LLMs and often match or exceed child performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregate of base and instruction-tuned LLMs evaluated on Strange Stories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Strange Stories (SS) aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Non-literal language / ToM</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Seven-item battery of non-literal language/ToM vignettes, scored on experimental and motivation answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Base-LLMs generally below child level; several large instruct-LLMs (GPT-4, PaLM2, GPT-3/3.5) matched or surpassed child-level performance, with GPT-4 near-perfect; deviation levels had little effect on larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (7-8y) show decreasing scores with increasing item complexity (no exact item-level numbers provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On SS, large instruct-LLMs often outperformed children (especially on later/harder items where children's performance declined more sharply), while base models underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>SS success may reflect abundant training data containing non-literal language; success does not imply human-like, grounded intention understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e5340.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summary_IM_aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregate Imposing Memory results (base vs instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate finding: IM was the most challenging test; nearly all base- and many instruct-LLMs performed below child level, with GPT-4 as the key exception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregate of evaluated LLMs across IM's intentionality and memory items and recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Imposing Memory (IM) aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Recursive intentionality and memory (ToM vs factual recall)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Narratives with true/false intentionality (recursive beliefs) and memory questions across five recursion levels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All base-LLMs performed below child level; most instruct-LLMs also performed below child level on IM except GPT-4 which performed consistently well and stayed above child level after second-order intentionality; FLAN-T5 improved with recursion to reach child level.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children (9-10y) memory performance remained high (> .85) across levels except a dip at level 4; intentionality shows a cut-off after level two (significant drop: β = −0.222, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Most LLMs underperformed compared to the child baseline on IM; GPT-4 was an exception, outperforming or matching children on higher intentionality levels.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>IM's long prompts and recursive reasoning demands may exceed many models' context windows or chain-of-thought capabilities; prompt length and format likely affected base-LLM performance disproportionately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e5340.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt_and_deviation_notes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting format, deviation manipulation, and limitations (methodological notes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methodological aspects: paper tested original and two levels of deviations to probe memorization/heuristics; instruct-LLMs were prompted in QA format, base-LLMs with completion prompts, which affects comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>methodological (applies across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A (methodological description affecting all evaluated LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne, Strange Stories deviations (levels 0,1,2)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>robustness / generalization testing</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Deviation 0 = original scenario, deviation 1 = superficial changes, deviation 2 = novel scenario preserving the ToM phenomenon but changing surface details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Deviations often reduced performance, especially on SA second-order tasks; Strange Stories performance was more robust to deviations for larger instruct-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Children were given same tasks with digital presentation; SA child performance lower than expected possibly due to digital format and requiring typed motivations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Instruction-tuned models are more robust than base models but still show substantial sensitivity to deviation for some tasks (notably SA2).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Different prompt formats for base vs instruct-LLMs (completion vs QA) and insertion of correct answers for motivation prompts for base-LLMs introduce caveats; results emphasize generalization limits and prompt dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5340.21">
                <h3 class="extraction-instance">Extracted Data Instance 21 (e5340.21)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Children_SA_baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Child baseline performance (7-8y) on SA; (9-10y) on IM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical child baselines gathered in this study: 7-8y group (n=37) performed on SA and SS; 9-10y group (n=36) completed IM; scores used as dashed-line baselines in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>human participants (children)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neurotypical Dutch/international school children in two age groups: 7-8 years (n=37) and 9-10 years (n=36) tested digitally.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Sally-Anne (SA1 & SA2), Strange Stories (SS), Imposing Memory (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>human baseline for ToM tests</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same standardized ToM tasks administered in child-appropriate digital formats with open answers for SA/SS and yes/no for IM; voice-overs and drawings used for IM.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>N/A (human baseline entry)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>SA1 mean = 0.45 (7-8y); SA2 mean = 0.225 (7-8y). For IM (9-10y), memory question accuracy stayed high (> .85) across most levels; intentionality performance drops significantly after level two (β = −0.222, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Used as reference: many base models underperformed children across tests; largest instruct-LLMs often matched or exceeded these child baselines on parts of tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Child SA scores were lower than expected possibly due to screen-based presentation and requirement to type motivations; baseline should be interpreted with that caveat.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind may have spontaneously emerged in large language models. <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with GPT-4. <em>(Rating: 2)</em></li>
                <li>Neural theory-of-mind? on the limits of social intelligence in large LMs. <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks. <em>(Rating: 2)</em></li>
                <li>Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms. <em>(Rating: 1)</em></li>
                <li>Boosting theory-of-mind performance in large language models via prompting. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5340",
    "paper_id": "paper-264828854",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A &gt;340B-parameter, instruction-tuned transformer LLM from OpenAI, evaluated in this paper on multiple Theory of Mind (ToM) tests and consistently showing the strongest performance among tested models.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned large transformer model trained with large-scale text and human feedback (alignment/instruction tuning) to follow natural-language instructions.",
            "model_size": "&gt;340B",
            "cognitive_test_name": "Sally-Anne (first-order SA1 and second-order SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief (social cognition, language)",
            "cognitive_test_description": "Narrative false-belief tasks: SA1 asks where a character will search for an object she falsely believes is in its original location; SA2 asks about what one character believes another character believes (second-order). Each item includes an experimental question and a follow-up motivation 'Why?' question.",
            "llm_performance": "Performed well above child level on SA1 and remained above child level on many SA2 items; performance degraded with deviations (especially deviation level 2) but GPT-4 still outperformed most other models on SA tasks according to qualitative results reported.",
            "human_baseline_performance": "Children (7-8y) performed at x = 0.45 on SA1 and x = 0.225 on SA2 (means reported for 7-8-year-old group used as child baseline).",
            "performance_comparison": "GPT-4 generally outperformed the child baseline on SA1 and often remained above or near child level on SA2, but deviations from original scenarios reduced performance and could push it closer to or below child level for the hardest/deviated SA2 items.",
            "notable_differences_or_limitations": "Although GPT-4 is strongest, its SA2 performance is sensitive to test deviations (generalization limits). The study cautions that passing ToM tests does not imply human-like ToM; prompts and deviation manipulations affect outcomes.",
            "uuid": "e5340.0",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4_SS",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "GPT-4 evaluated on the Strange Stories (non-literal language / ToM) battery; shows near-ceiling performance.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned transformer LLM trained with human feedback; optimized for following instructions and conversational tasks.",
            "model_size": "&gt;340B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / Theory of Mind / social reasoning",
            "cognitive_test_description": "Seven short vignettes involving non-literal language (white lie, sarcasm, pretend, misunderstanding, etc.); experimental question (e.g., 'Is what the girl says true?') and motivation question ('Why does she say this?').",
            "llm_performance": "Approached perfect scores across Strange Stories items (near-ceiling performance reported qualitatively). Performance was robust across deviations from original formulations.",
            "human_baseline_performance": "Children (7-8y) showed declining scores as item difficulty increased; exact numeric means not provided in text for each item.",
            "performance_comparison": "GPT-4 matched or exceeded child performance across Strange Stories items and was notably more robust to deviations than many other models.",
            "notable_differences_or_limitations": "High performance likely reflects exposure in training data to many non-literal language contexts; strong SS performance does not directly imply grounded social cognition.",
            "uuid": "e5340.1",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4_IM",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "GPT-4 evaluated on the Imposing Memory test (recursive intentionality vs. memory), showing sustained performance across recursion levels.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned transformer LLM from OpenAI with strong instruction-following ability.",
            "model_size": "&gt;340B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality (higher-order ToM) and memory (working/episodic recall)",
            "cognitive_test_description": "Two long narratives followed by true/false questions: 10 intentionality items requiring nested belief reasoning across up to five recursion levels, and 12 matched memory items requiring factual recall.",
            "llm_performance": "GPT-4 performed consistently well across recursion levels and stayed above child-level performance after second-order intentionality; qualitative report indicates GPT-4 'passes well' compared to other models.",
            "human_baseline_performance": "Children (9-10y, n=36) showed high memory question accuracy (&gt; .85 across levels except a dip at level 4) and a cut-off in intentionality after level two (significant drop between level 1 and 2; β = −0.222, p &lt; .05).",
            "performance_comparison": "GPT-4 outperformed other LLMs and exceeded child baseline on IM after second-order intentionality; most other LLMs performed below child level on IM.",
            "notable_differences_or_limitations": "IM was the most challenging test and only GPT-4 passed robustly; differences may reflect GPT-4's larger context/window and instruction tuning; prompt length/context window limits and chain-of-thought abilities may constrain other models.",
            "uuid": "e5340.2",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo (instruction-tuned)",
            "brief_description": "An instruction-tuned 175B-parameter GPT-3.5 series model evaluated across all ToM tests; strong on some tasks but sensitive to deviations and recursion.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned member of the GPT family, trained with reinforcement learning from human feedback to follow instructions.",
            "model_size": "175B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First-order (SA1) and second-order (SA2) false-belief narratives with experimental and motivation questions.",
            "llm_performance": "Performed well above child level on SA1 and remained above child level on SA2 in default (non-deviated) versions; however, combination of second-order ToM with deviation level 2 substantially reduced performance (pushed below child level).",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.",
            "performance_comparison": "GPT-3.5 often outperformed child baseline on original SA1 and many SA2 items but showed reduced generalization under deviated formulations and more complex second-order/deviated cases.",
            "notable_differences_or_limitations": "Sensitive to deviations from standard SA formulations; generalization to novel phrasing is limited compared to raw performance on canonical prompts.",
            "uuid": "e5340.3",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5_SS",
            "name_full": "GPT-3.5-turbo (instruction-tuned)",
            "brief_description": "GPT-3.5 evaluated on Strange Stories; good performance on simpler items and generally above child baseline but declines on later items.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned transformer LLM capable of instruction-following and conversational tasks.",
            "model_size": "175B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM",
            "cognitive_test_description": "Seven items probing intention inference for non-literal language acts (lie, pretend, sarcasm, white lie, sarcasm, etc.) with experimental and motivation questions.",
            "llm_performance": "Performed at or close to child level on item 1, then performance somewhat declined across more complex items but stayed well above child level overall (qualitative).",
            "human_baseline_performance": "Children (7-8y) show decreasing performance as item complexity increases; exact numeric item-level scores not provided.",
            "performance_comparison": "GPT-3.5 generally matched or exceeded child-level performance on Strange Stories, particularly on earlier items; decline with item difficulty was milder for the model than for children.",
            "notable_differences_or_limitations": "Though relatively strong on SS, GPT-3.5 still shows item-dependent declines; robustness to deviations is better than for some models but inferior to GPT-4.",
            "uuid": "e5340.4",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5_IM",
            "name_full": "GPT-3.5-turbo (instruction-tuned)",
            "brief_description": "GPT-3.5's performance on the Imposing Memory test was among the weakest of the instruct-tuned models, especially on recursive intentionality.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Instruction-tuned GPT-series model trained with human feedback to follow instructions.",
            "model_size": "175B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality (higher-order ToM) and memory",
            "cognitive_test_description": "Long narratives followed by true/false intentionality and memory questions across five recursion levels.",
            "llm_performance": "GPT-3.5 performed worst of the instruct-LLMs on IM; showed poor performance on intentionality questions and did not reach child-level performance overall.",
            "human_baseline_performance": "Children (9-10y, n=36) showed high memory accuracy (&gt; .85 across most levels) and declined on intentionality after level two (statistical drop between level 1 and 2: β = −0.222, p &lt; .05).",
            "performance_comparison": "Underperformed relative to the child baseline and relative to GPT-4; GPT-3.5 struggled especially on recursive intentionality while children showed a specific cut-off after level two.",
            "notable_differences_or_limitations": "IM is prompt-length intensive; GPT-3.5 may be constrained by context window and chain-of-thought limitations compared to GPT-4.",
            "uuid": "e5340.5",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3 (text-davinci-003, instruction-tuned variant)",
            "brief_description": "A 175B-parameter GPT-3 family model (davinci variant) included as an instruct-LLM; showed mixed performance with structurally low scores on some tasks in this study.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003)",
            "model_description": "Large transformer LLM (GPT-3 family), instruction-tuned variant (text-davinci-003) trained with reinforcement learning from human feedback.",
            "model_size": "175B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief narrative tasks with experimental and motivation components.",
            "llm_performance": "Reported structurally low scores on SA tasks in this study, inconsistent with prior work (Kosinski, 2023) which used different prompting methods; performed worse relative to many other instruction-tuned models.",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.",
            "performance_comparison": "Underperformed relative to the child baseline on SA especially compared to larger instruct-LLMs (GPT-3.5, GPT-4); differences may be attributable to prompt format (open question prompting vs. completion).",
            "notable_differences_or_limitations": "The authors note that prompting format strongly affects GPT-3 results and that differences with previous reports likely stem from using open questions rather than text-completion prompts.",
            "uuid": "e5340.6",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3_SS",
            "name_full": "GPT-3 (text-davinci-003)",
            "brief_description": "GPT-3's performance on Strange Stories was above child level for simpler items but declined for more complex items; overall did well compared to many base models.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003)",
            "model_description": "Instruction-tuned GPT-3 variant trained using human feedback for instruction following.",
            "model_size": "175B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM",
            "cognitive_test_description": "Seven vignettes requiring inference of intentions and motivations behind non-literal language acts; answers include experimental and motivation responses.",
            "llm_performance": "Performed at or close to child level on item 1 and declined somewhat on more complex items but remained well above child level overall according to qualitative reporting.",
            "human_baseline_performance": "Children (7-8y) whose performance drops more steeply as item complexity increases; item-level numbers not provided.",
            "performance_comparison": "Generally matched or exceeded child performance on early SS items and remained comparatively strong though not as robust as GPT-4.",
            "notable_differences_or_limitations": "Prompting choices (open-question vs completion) influenced performance; deviations had limited effect on larger instruct-LLMs.",
            "uuid": "e5340.7",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM 2",
            "brief_description": "A large Google transformer family model (estimated 175-340B parameters) evaluated as both base and chat variants; instruction-tuned variants performed better on some ToM tasks.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "PaLM2",
            "model_description": "Large transformer-based LLM from Google (PaLM family); PaLM2 and PaLM2-chat are instruction-tuned/chat-optimized variants (size estimated 175–340B).",
            "model_size": "175-340B (estimated)",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "Canonical first- and second-order false-belief tasks with motivation follow-ups.",
            "llm_performance": "PaLM2 and PaLM2-chat performed well above child level on first-order SA1, but performance degraded on second-order SA2; some instruct variants (PaLM2-chat) were notably perturbed by second-order items and deviations.",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.",
            "performance_comparison": "PaLM2 variants generally outperformed children on SA1 but tended to fall to or below child level on SA2, especially with deviations; some chat-tuned versions were more affected.",
            "notable_differences_or_limitations": "Second-order ToM and deviation manipulations exposed generalization limits; instruction-tuning helps but does not guarantee robust second-order ToM across novel formulations.",
            "uuid": "e5340.8",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PaLM2_SS",
            "name_full": "PaLM 2 (chat and base variants)",
            "brief_description": "PaLM2 variants evaluated on Strange Stories: chat and some instruction variants matched or exceeded child-level performance on non-literal language items.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "PaLM2 / PaLM2-chat",
            "model_description": "Large transformer LLMs from Google; chat variants optimized for dialogue and instruction following.",
            "model_size": "175-340B (estimated)",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM",
            "cognitive_test_description": "Seven-item battery probing non-literal language understanding and intention attribution.",
            "llm_performance": "PaLM2-chat and PaLM2 performed close to or above child level on SS items; deviation levels had little effect on larger PaLM2 variants.",
            "human_baseline_performance": "Children (7-8y) show decreasing performance with item complexity; precise values not provided.",
            "performance_comparison": "PaLM2 variants matched or exceeded child-level performance on many SS items, particularly the chat-tuned version.",
            "notable_differences_or_limitations": "PaLM2's strength on non-literal language likely reflects training data; second-order false-belief tasks are more challenging.",
            "uuid": "e5340.9",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "FLAN-T5",
            "name_full": "FLAN-T5 (instruction-tuned, 11B)",
            "brief_description": "An 11B-parameter instruction-tuned encoder-decoder Transformer (FLAN-T5) that shows mixed performance: comparatively good on some Strange Stories items and improving with recursion on IM.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "Instruction-tuned T5-family encoder-decoder model (FLAN variants) trained to follow natural-language instructions.",
            "model_size": "11B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM",
            "cognitive_test_description": "Seven non-literal-language vignettes requiring intention inference and motivation explanations.",
            "llm_performance": "FLAN-T5 surpassed child level earlier than PaLM2 on some SS items and outperformed larger PaLM variants on more difficult SS items (qualitative); for IM, FLAN-T5 increased performance as recursion increased and ended at child level.",
            "human_baseline_performance": "Children (7-8y) show declining SS scores with increasing item difficulty; IM child memory &gt; .85 and intentionality shows cutoff after level two.",
            "performance_comparison": "On SS, FLAN-T5 often matched or exceeded child baseline on mid-range items; on IM, FLAN-T5 rose to child-level performance at higher recursion, unlike many other instruct-LLMs.",
            "notable_differences_or_limitations": "As a relatively small instruct-LLM, FLAN-T5 shows surprising strengths on non-literal language and certain recursive tasks, suggesting instruction-tuning can compensate for smaller model size in some respects.",
            "uuid": "e5340.10",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B",
            "name_full": "Falcon (base) 7B",
            "brief_description": "A 7B-parameter base transformer LLM evaluated as a base (non-instruction-tuned) model; generally performs below children on ToM tests except some first-order SA1 cases for larger base LLMs only.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "Falcon-7B (base)",
            "model_description": "Open-source 7B-parameter transformer LLM trained on web-scale data (base, not instruction-tuned in this study).",
            "model_size": "7B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief tasks with motivation questions.",
            "llm_performance": "Performed poorly on SA tasks: smaller base models (including Falcon-7B) performed worse overall and tended to be at or below child level especially on second-order SA2.",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.",
            "performance_comparison": "Falcon-7B underperformed compared to child baseline on SA1 and especially on SA2.",
            "notable_differences_or_limitations": "Base-LLMs generally fared worse than instruct-LLMs; prompt format differences and lack of instruction-tuning limit performance.",
            "uuid": "e5340.11",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-7B-I",
            "name_full": "Falcon-instruct (Falcon-7B-I)",
            "brief_description": "Instruction-tuned 7B Falcon variant; in this study it performed worse than most instruct-LLMs and failed on many second-order ToM items.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I",
            "model_description": "Instruction-tuned variant of the Falcon family (7B parameters) optimized for following instructions.",
            "model_size": "7B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief narratives with motivation follow-ups.",
            "llm_performance": "Falcon-7B-I failed on all second-order SA2 questions and performed overall worse than larger instruct-LLMs; on Strange Stories it performed worst among instruct-LLMs.",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.",
            "performance_comparison": "Underperformed relative to child baseline on second-order ToM and on Strange Stories compared to larger instruct-LLMs.",
            "notable_differences_or_limitations": "Small instruction-tuned models may still lack sufficient capacity or fine-tuning breadth for robust ToM; deviation robustness was low.",
            "uuid": "e5340.12",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-30B",
            "name_full": "LLaMA (30B)",
            "brief_description": "A 30B-parameter base LLM (LLaMA family) tested as a base model; performed above child level on SA1 but fell to or below child level on SA2.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B",
            "model_description": "30B-parameter base transformer LLM (LLaMA family), trained on large text corpora but not instruction-tuned in this study's base configuration.",
            "model_size": "30B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "First- and second-order false-belief narrative tasks with motivation follow-ups.",
            "llm_performance": "Performed above child level on first-order SA1 but fell to or below child level on second-order SA2.",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225.",
            "performance_comparison": "Matched or exceeded child baseline on SA1 but underperformed on SA2.",
            "notable_differences_or_limitations": "As a base model, LLaMA-30B lacks instruction-tuning advantages and shows limited generalization to more complex, recursive ToM tasks.",
            "uuid": "e5340.13",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BLOOM",
            "name_full": "BLOOM (176B)",
            "brief_description": "A 176B-parameter multilingual base LLM that in this study performed above child level on SA1 but fell at/below child level on SA2 and generally below child level on other tests.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "BLOOM",
            "model_description": "Large open multilingual transformer model trained on large corpora (base LLM, not instruction-tuned in the study's base configuration).",
            "model_size": "176B",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2)",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "Classic first- and second-order false-belief narratives with experimental and motivation questions.",
            "llm_performance": "BLOOM performed above child level on SA1 but fell at or below child level on SA2; on Strange Stories and IM it performed below child level generally.",
            "human_baseline_performance": "Children (7-8y) SA1 mean x = 0.45; SA2 mean x = 0.225; IM memory &gt; .85 for 9-10y children and intentionality shows cutoff after level two.",
            "performance_comparison": "Outperformed child baseline on SA1 only; underperformed on SA2 and on other ToM tests relative to children.",
            "notable_differences_or_limitations": "Base-LLM status and prompt/format limitations likely suppressed some of BLOOM's latent capabilities; instruction-tuning appears critical for broader ToM performance.",
            "uuid": "e5340.14",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-davinci (base)",
            "name_full": "GPT-davinci (base-LLM, 175B)",
            "brief_description": "A 175B-parameter base GPT-3 family model used in base (non-instruction-tuned) mode; showed some improvement on higher recursion levels but overall below child level on IM.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "GPT-davinci (base)",
            "model_description": "Large transformer base LLM (GPT-3/davinci family) evaluated in base completion mode rather than instruct mode in this study.",
            "model_size": "175B",
            "cognitive_test_name": "Imposing Memory (IM)",
            "cognitive_test_type": "Recursive intentionality and memory",
            "cognitive_test_description": "Long narrative test with true/false questions probing nested intentionality up to five levels and matched memory items.",
            "llm_performance": "As a larger base-LLM, GPT-davinci showed some improvement on higher levels of recursion compared to smaller base models but remained below child-level performance overall on IM.",
            "human_baseline_performance": "Children (9-10y) memory &gt; .85 across most levels; intentionality drops after level two (statistical drop noted).",
            "performance_comparison": "Underperformed relative to child baseline on IM despite some gains at higher recursion; outperformed smaller base models but not instruction-tuned top models.",
            "notable_differences_or_limitations": "Base-LLM prompting differences and lack of instruction tuning limit comparability; authors note base-LLMs had different prompt formats (text-completion) which could both help and hurt.",
            "uuid": "e5340.15",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon-instruct (Falcon-7B-I)_SS",
            "name_full": "Falcon-instruct (Falcon-7B-I)",
            "brief_description": "Instruction-tuned Falcon 7B variant evaluated on Strange Stories; among the weakest performing instruct-LLMs on SS.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "Falcon-7B-I",
            "model_description": "Instruction-tuned 7B-parameter Falcon variant optimized for instruction following.",
            "model_size": "7B",
            "cognitive_test_name": "Strange Stories (SS)",
            "cognitive_test_type": "Non-literal language / ToM",
            "cognitive_test_description": "Seven vignettes requiring intention attribution when non-literal language is used.",
            "llm_performance": "Performed overall worst among instruct-LLMs on Strange Stories according to qualitative summary.",
            "human_baseline_performance": "Children (7-8y) show decreasing performance as items become more complex; numeric values not provided.",
            "performance_comparison": "Underperformed relative to child baseline on many SS items and performed worse than larger instruct-LLMs.",
            "notable_differences_or_limitations": "Small instruction-tuned models may still lack capacity or broad instruction-tuning coverage to handle diverse non-literal scenarios robustly.",
            "uuid": "e5340.16",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Summary_base_vs_instruct_SA",
            "name_full": "Base-LLMs vs. Instruction-tuned LLMs on Sally-Anne",
            "brief_description": "Aggregate comparison reported in the paper: base-LLMs generally perform worse than instruct-LLMs on ToM false-belief tasks, especially on second-order items and deviations.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "multiple (aggregate)",
            "model_description": "Aggregate of tested base and instruction-tuned LLMs (Falcon, LLaMA, BLOOM, GPT variants, PaLM2, FLAN-T5, etc.)",
            "model_size": "various (7B–&gt;340B)",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2) aggregate",
            "cognitive_test_type": "Theory of Mind / false-belief",
            "cognitive_test_description": "Aggregate evaluation across first- and second-order false-belief tests and deviated formulations to test generalization.",
            "llm_performance": "In aggregate: many base-LLMs could do SA1 sometimes (some larger base models), but most base-LLMs and smaller instruct-LLMs failed at SA2; larger instruct-LLMs (GPT-4, GPT-3.5) did better on SA2 but performance dropped with deviations.",
            "human_baseline_performance": "Children (7-8y) SA1 mean = 0.45; SA2 mean = 0.225 (reference dashed lines used in figures).",
            "performance_comparison": "Instruction-tuned large models generally outperform base models and often exceed child baseline on SA1 and some SA2 items; base models underperform children on SA2 and other tasks.",
            "notable_differences_or_limitations": "Deviations from canonical test texts reveal poor generalization in many models; prompting style differences (instruct vs completion) affected some base-model results, complicating direct comparisons.",
            "uuid": "e5340.17",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Summary_SS_aggregate",
            "name_full": "Aggregate Strange Stories results (base vs instruct)",
            "brief_description": "Aggregate finding: instruct-LLMs (especially GPT-4) handle non-literal language in Strange Stories better than base-LLMs and often match or exceed child performance.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "multiple (aggregate)",
            "model_description": "Aggregate of base and instruction-tuned LLMs evaluated on Strange Stories.",
            "model_size": "various",
            "cognitive_test_name": "Strange Stories (SS) aggregate",
            "cognitive_test_type": "Non-literal language / ToM",
            "cognitive_test_description": "Seven-item battery of non-literal language/ToM vignettes, scored on experimental and motivation answers.",
            "llm_performance": "Base-LLMs generally below child level; several large instruct-LLMs (GPT-4, PaLM2, GPT-3/3.5) matched or surpassed child-level performance, with GPT-4 near-perfect; deviation levels had little effect on larger instruct-LLMs.",
            "human_baseline_performance": "Children (7-8y) show decreasing scores with increasing item complexity (no exact item-level numbers provided in text).",
            "performance_comparison": "On SS, large instruct-LLMs often outperformed children (especially on later/harder items where children's performance declined more sharply), while base models underperformed.",
            "notable_differences_or_limitations": "SS success may reflect abundant training data containing non-literal language; success does not imply human-like, grounded intention understanding.",
            "uuid": "e5340.18",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Summary_IM_aggregate",
            "name_full": "Aggregate Imposing Memory results (base vs instruct)",
            "brief_description": "Aggregate finding: IM was the most challenging test; nearly all base- and many instruct-LLMs performed below child level, with GPT-4 as the key exception.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "multiple (aggregate)",
            "model_description": "Aggregate of evaluated LLMs across IM's intentionality and memory items and recursion levels.",
            "model_size": "various",
            "cognitive_test_name": "Imposing Memory (IM) aggregate",
            "cognitive_test_type": "Recursive intentionality and memory (ToM vs factual recall)",
            "cognitive_test_description": "Narratives with true/false intentionality (recursive beliefs) and memory questions across five recursion levels.",
            "llm_performance": "All base-LLMs performed below child level; most instruct-LLMs also performed below child level on IM except GPT-4 which performed consistently well and stayed above child level after second-order intentionality; FLAN-T5 improved with recursion to reach child level.",
            "human_baseline_performance": "Children (9-10y) memory performance remained high (&gt; .85) across levels except a dip at level 4; intentionality shows a cut-off after level two (significant drop: β = −0.222, p &lt; .05).",
            "performance_comparison": "Most LLMs underperformed compared to the child baseline on IM; GPT-4 was an exception, outperforming or matching children on higher intentionality levels.",
            "notable_differences_or_limitations": "IM's long prompts and recursive reasoning demands may exceed many models' context windows or chain-of-thought capabilities; prompt length and format likely affected base-LLM performance disproportionately.",
            "uuid": "e5340.19",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prompt_and_deviation_notes",
            "name_full": "Prompting format, deviation manipulation, and limitations (methodological notes)",
            "brief_description": "Methodological aspects: paper tested original and two levels of deviations to probe memorization/heuristics; instruct-LLMs were prompted in QA format, base-LLMs with completion prompts, which affects comparability.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "methodological (applies across models)",
            "model_description": "N/A (methodological description affecting all evaluated LLMs)",
            "model_size": "N/A",
            "cognitive_test_name": "Sally-Anne, Strange Stories deviations (levels 0,1,2)",
            "cognitive_test_type": "robustness / generalization testing",
            "cognitive_test_description": "Deviation 0 = original scenario, deviation 1 = superficial changes, deviation 2 = novel scenario preserving the ToM phenomenon but changing surface details.",
            "llm_performance": "Deviations often reduced performance, especially on SA second-order tasks; Strange Stories performance was more robust to deviations for larger instruct-LLMs.",
            "human_baseline_performance": "Children were given same tasks with digital presentation; SA child performance lower than expected possibly due to digital format and requiring typed motivations.",
            "performance_comparison": "Instruction-tuned models are more robust than base models but still show substantial sensitivity to deviation for some tasks (notably SA2).",
            "notable_differences_or_limitations": "Different prompt formats for base vs instruct-LLMs (completion vs QA) and insertion of correct answers for motivation prompts for base-LLMs introduce caveats; results emphasize generalization limits and prompt dependencies.",
            "uuid": "e5340.20",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Children_SA_baseline",
            "name_full": "Child baseline performance (7-8y) on SA; (9-10y) on IM",
            "brief_description": "Empirical child baselines gathered in this study: 7-8y group (n=37) performed on SA and SS; 9-10y group (n=36) completed IM; scores used as dashed-line baselines in figures.",
            "citation_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "mention_or_use": "use",
            "model_name": "human participants (children)",
            "model_description": "Neurotypical Dutch/international school children in two age groups: 7-8 years (n=37) and 9-10 years (n=36) tested digitally.",
            "model_size": "N/A",
            "cognitive_test_name": "Sally-Anne (SA1 & SA2), Strange Stories (SS), Imposing Memory (IM)",
            "cognitive_test_type": "human baseline for ToM tests",
            "cognitive_test_description": "Same standardized ToM tasks administered in child-appropriate digital formats with open answers for SA/SS and yes/no for IM; voice-overs and drawings used for IM.",
            "llm_performance": "N/A (human baseline entry)",
            "human_baseline_performance": "SA1 mean = 0.45 (7-8y); SA2 mean = 0.225 (7-8y). For IM (9-10y), memory question accuracy stayed high (&gt; .85) across most levels; intentionality performance drops significantly after level two (β = −0.222, p &lt; .05).",
            "performance_comparison": "Used as reference: many base models underperformed children across tests; largest instruct-LLMs often matched or exceeded these child baselines on parts of tasks.",
            "notable_differences_or_limitations": "Child SA scores were lower than expected possibly due to screen-based presentation and requirement to type motivations; baseline should be interpreted with that caveat.",
            "uuid": "e5340.21",
            "source_info": {
                "paper_title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models.",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4.",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Neural theory-of-mind? on the limits of social intelligence in large LMs.",
            "rating": 2,
            "sanitized_title": "neural_theoryofmind_on_the_limits_of_social_intelligence_in_large_lms"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks.",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms.",
            "rating": 1,
            "sanitized_title": "do_conversational_agents_have_a_theory_of_mind_a_single_case_study_of_chatgpt_with_the_hinting_false_beliefs_and_false_photographs_and_strange_stories_paradigms"
        },
        {
            "paper_title": "Boosting theory-of-mind performance in large language models via prompting.",
            "rating": 1,
            "sanitized_title": "boosting_theoryofmind_performance_in_large_language_models_via_prompting"
        }
    ],
    "cost": 0.02754325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</p>
<p>Max Van Duijn m.j.van.duijn@liacs.leidenuniv.nl 
Leiden Institute of Advanced Computer Science</p>
<p>Bram Van Dijk 
Leiden Institute of Advanced Computer Science</p>
<p>Tom Kouwenhoven 
Leiden Institute of Advanced Computer Science</p>
<p>Werner De Valk 
Leiden Institute of Advanced Computer Science</p>
<p>Marco Spruit 
Leiden Institute of Advanced Computer Science</p>
<p>Leiden University Medical Centre</p>
<p>Peter Van Der Putten 
Leiden Institute of Advanced Computer Science</p>
<p>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests
1E65FAFC9595B1E8297BDE5B3DFE8E8D
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)?Here we add to this emerging debate by (i) testing 11 base-and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including nonliteral language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks.We find that instructiontuned LLMs from the GPT family outperform other models, and often also children.Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting.We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context.We conclude by arguing for a nuanced perspective on ToM in LLMs.</p>
<p>Introduction</p>
<p>Machines that can think like us have always triggered our imagination.Contemplation of such machines can be traced as far back as antiquity (Liveley and Thomas, 2020), and peaked with the advent of all kinds of 'automata' in the early days of the Industrial Revolution (Voskuhl, 2013) before settling in computer science from the 1950s (Turing, 1950).Currently people around the world can interact with powerful chatbots driven by Large Language Models (LLMs), such as OpenAI's ChatGPT (OpenAI, 2023), and wonder to what degree such systems are capable of thought.</p>
<p>LLMs are large-scale deep neural networks, trained on massive amounts of text from the web.*Equal contribution.</p>
<p>They are vastly complex systems: even if all details about their architecture, training data, and optional fine-tuning procedures are known (which is currently not the case for the most competitive models), it is very difficult to oversee their capabilities and predict how they will perform on a variety of tasks.Researchers from linguistics (Manning et al., 2020), psychology (Binz and Schulz, 2023b;Kosinski, 2023;Webb et al., 2023), psychiatry (Kjell et al., 2023), epistemology (Sileo and Lernould, 2023), logic (Creswell et al., 2022), and other fields, have therefore started to study LLMs as new, 'alien' entities, with their own sort of intelligence, that needs to be probed with experiments, an endeavour recently described as 'machine psychology' (Hagendorff, 2023).This not only yields knowledge about what LLMs are capable of, but also provides a unique opportunity to shed new light on questions surrounding our own intelligence (Dillion et al., 2023;Binz and Schulz, 2023a).</p>
<p>Here we focus on attempts to determine to what degree LLMs demonstrate a capacity for Theory of Mind (ToM), defined as the ability to work with beliefs, intentions, desires, and other mental states, to anticipate and explain behaviour in social settings (Apperly, 2010).We first address the question how LLMs perform on standardized, language-based tasks used to assess ToM capabilities in humans.We extend existing work in this area, surveyed in Section 2, in four ways: by (i) testing 11 models (see Table 1) for a broader suite of capabilities relevant to ToM beyond just the dominant falsebelief paradigm, including non-literal language understanding and recursive intentionality (A wants B to believe that C intends...); (ii) using newly written versions of standardized tests with varying degrees of deviation from the originals; (iii) including open questions besides closed ones; and (iv) benchmarking LLM performance against that of children aged 7-8 (n=37) and 9-10 (n=36) on the same tasks.Section 3 contains details of our test procedures for both children and LLMs.After reporting the results in Section 4, we turn to the question how variation in performance of the LLMs we tested can be explained in Section 5. We conclude by placing our findings in the broader context of strong links between language and ToM in human development and evolution, and tentatively interpret what it means for an LLM to pass (or fail) ToM tests.</p>
<p>We are aware of issues regarding LLM training and deployment, for example regarding the biases they inherit (Lucy and Bamman, 2021;Bender et al., 2021), problems for educators (Sparrow, 2022), and ethical concerns in obtaining human feedback (Perrigo, 2023).Ongoing reflection on the use of LLMs is necessary, but outside the scope of this paper.</p>
<p>Background</p>
<p>Large Language Models</p>
<p>The field of Natural Language Processing (NLP) has been revolutionized by the advent of Transformer models (Vaswani et al., 2017;Devlin et al., 2019), deep neural networks that can induce language structures through self-supervised learning.</p>
<p>During training, such models iteratively predict masked words from context in large sets of natural language data.They improve at this task by building representations of the many morphological, lexical, and syntactic rules governing human language production and understanding (Manning et al., 2020;Rogers et al., 2021;Grand et al., 2022).Models exclusively trained through such self-supervision constitute what we refer to as 'base-LLMs' in this paper.</p>
<p>Base-LLMs can generate natural language when prompted with completion queries ('A mouse is an ...').They can also be leveraged successfully for an array of other challenges, such as questionanswering and translation, which often requires task-specific fine-tuning or prompting with specific examples, known as few-shot-learning (Brown et al., 2020).This makes them different from a new generation of LLMs that we refer to as 'instruct-LLMs' in this paper, and to which the currently most competitive models belong.In instruction-tuning, various forms of human feedback are collected, such as ranking most suitable responses, which then forms the reward-signal for further aligning these models to human preferences through reinforcement learning (Ouyang et al., 2022).The resulting LLMs can be prompted with natural language in the form of instructions to perform a wide variety of tasks directly, amounting to zero-shot learning (Wei et al., 2022).</p>
<p>A key realization is thus that LLMs are given either no explicitly labelled data at all, or, in the case of instruct-LLMs, data with human labels pertaining to relatively general aspects of communicative interaction.As such they are part of a completely different paradigm than earlier language models that were trained on, for example, data sets of human-annotated language structures (e.g.Nivre et al., 2016).This means that when LLMs are capable of such tasks as solving co-reference relationships or identifying word classes (Manning et al., 2020), this arises as an emergent property of the model's architecture and training on different objectives.Given that such emergent linguistic capabilities have been observed (Reif et al., 2019;Grand et al., 2022), it is a legitimate empirical question which other capacities LLMs may have acquired as 'by-catch'.</p>
<p>Theory of Mind in Humans and LLMs</p>
<p>ToM, also known as 'mindreading', is classically defined as the capacity to attribute mental states to others (and oneself), in order to explain and anticipate behaviour.The concept goes back to research in ethology in which Premack and Woodruff (1978) famously studied chimpanzees' abilities to anticipate behaviour of caretakers.When focus shifted to ToM in humans, tests were developed that present a scenario in which a character behaves according to its false beliefs about a situation, and not according to the reality of the situation itself--which a successful participant, having the benefit of spectatorsight, can work out (see Section 3.1).</p>
<p>Initial consensus that children could pass versions of this test from the age of 4 was followed by scepticism about additional abilities it presumed, including language skills and executive functioning, which led to the development of simplified false-belief tests based on eye-gaze that even 15 month-olds were found to 'pass' (Onishi and Baillargeon, 2005).While this line of research also met important criticism (for a review see Barone et al., 2019), it highlights two key distinctions in debate from the past decades: implicit-behavioural versus explicit-representational and innate versus learned components of ToM.Some researchers see results from eye-gaze paradigms as evidence for a native or very early developing capacity for beliefattribution in humans (Carruthers, 2013) and hold that performance on more complex tests is initially 'masked' by a lack of expressive skills (cf.also Fodor, 1992).Others have attempted to explain eyegaze results in terms of lower-level cognitive mechanisms (Heyes, 2014) and argued that the capacity for belief-attribution itself develops gradually in interaction with more general social, linguistic, and narrative competencies (Heyes and Frith, 2014;Milligan et al., 2007;Hutto, 2008).Two-systems approaches (Apperly, 2010) essentially reconcile both sides by positing that our mindreading capacity encompasses both a basic, fast, and early developing component and a more advanced and flexible component that develops later.</p>
<p>In computational cognitive research, a variety of approaches to modelling ToM have been proposed (e.g.Baker and Saxe, 2011;Arslan et al., 2017).More recently neural agents (Rabinowitz et al., 2018) have been implemented, along with an increasing number of deep-learning paradigms aimed at testing first-and second-order ToM via question-answering.Initially this was done with recurrent memory networks (Grant et al., 2017;Nematzadeh et al., 2018) using data sets of classic false-belief tests from psychology, but after issues surfaced with simple heuristics for solving such tasks, scenarios were made more varied and challenging (Le et al., 2019).From the inception of BERT as one of the first LLMs (Devlin et al., 2019), we have seen roughly two approaches for testing ToM in LLMs: many different ToM scenarios integrated in large benchmark suites (e.g.Sap et al., 2022;Srivastava et al., 2023;Sileo and Lernould, 2023;Ma et al., 2023;Shapira et al., 2023), and studies that modified standardized ToM tests as used in developmental and clinical research for prompting LLMs (e.g.Kosinski, 2023;Ullman, 2023;Bubeck et al., 2023;Brunet-Gouet et al., 2023;Chowdhery et al., 2022;Moghaddam and Honey, 2023;Marchetti et al., 2023).This paper adds to the latter tradition in four respects, as listed in the introduction.</p>
<p>Methodology</p>
<p>Here we describe our tasks and procedures for testing LLMs and children; all code, materials, and data are on OSF: https://shorturl.at/FQR34.</p>
<p>ToM Tests</p>
<p>Sally-Anne test, first-order (SA1) --The Sally-Anne test (Wimmer and Perner, 1983;Baron-Cohen et al., 1985) is a classic first-order false belief test.It relies on a narrative in which Sally and Anne stand behind a table with a box and a basket on it.When Anne is still present, Sally puts a ball in her box.When Sally leaves, Anne retrieves the ball from the box and puts it in her own basket.The story ends when Sally returns and the participant is asked the experimental question 'Where will Sally look for the ball?'The correct answer is that she will look in her box.We followed up by asking a motivation question, 'Why?', to prompt an explanation to the effect of 'she (falsely) believes the object is where she left it'.</p>
<p>Sally-Anne test, second-order (SA2) --While SA1 targets the participant's judgement of what a character believes about the location of an unexpectedly displaced object, in SA2 the participant needs to judge what a character believes that another character believes about the location of an ice-cream truck (Perner and Wimmer, 1985).Sally and Anne are in a park this time, where an icecream man is positioned next to the fountain.Anne runs home to get her wallet just while the ice-cream man decides to move his truck to the swings.He tells Sally about this, but unknown to her, he meets Anne on the way and tells her too.Sally then runs after Anne, and finds her mother at home, who says that Anne picked up the wallet and went to buy ice cream.The experimental question now is 'Where does Sally think Anne went to buy ice cream?', with as correct answer 'to the fountain', also followed up with 'Why?', to prompt an explanation to the effect of 'Sally doesn't know that the ice-cream man told Anne that he was moving to the swings'.</p>
<p>Strange Stories test (SS) --The Strange Stories test (Happé, 1994;Kaland et al., 2005) depicts seven social situations with non-literal language use that can easily be misinterpreted, but causes no problems to typically developed adults.To understand the situations, subjects must infer the characters' intentions, applying ToM.For example, in one of the items a girl wants a rabbit for Christmas.When she opens her present, wrapped in a big enough box, it turns out that she received a pile of books.She says that she is really happy with her gift, after which subjects are asked the experimental question 'Is what the girl says true?', with correct answer 'No'.They can motivate their answer after the question 'Why does she say this?', with as correct answer 'to avoid her parents' feelings being hurt'.Items increase in difficulty and cover a lie, pretend-play scenario, practical joke, white lie (example above), misunderstanding, sarcasm, and double bluff.</p>
<p>Imposing Memory test (IM) --The Imposing Memory test was originally developed by Kinderman et al. (1998), but the test has been revised several times; we rely on an unpublished version created by Anneke Haddad and Robin Dunbar (van Duijn, 2016), originally for adolescents, which we adapted thoroughly to make it suitable for children aged 7-10.Our version features two different stories, followed by true/false questions, 10 of which are 'intentionality' and 12 are 'memory' questions.</p>
<p>For instance, in one story Sam has just moved to a new town.He asks one of his new classmates, Helen, where he can buy post stamps for a birthday card for his granny.When Helen initially sends him to the wrong location, Sam wonders whether she was playing a prank on him or just got confused about the whereabouts of the shop herself.He goes and asks another classmate, Pete, for help.As in the original IM, the intentionality questions involve reasoning about different levels of recursively embedded mental states (e.g., at third-level: 'Helen thought Sam did not believe that she knew the location of the store that sells post stamps'), whereas the memory questions require just remembering facts presented in the story (e.g., to match third-level intentionality questions, three elements from the story are combined: 'Sam was looking for a store where they sell post stamps.He told Pete that he had asked Helen about this').</p>
<p>Scoring Test Answers</p>
<p>Test scores for both children and LLMs were determined in the following way.For each of the SA1 and SA2 items, as well as for the seven SS items, a correct answer to the experimental question yielded 1 point.These answers were discrete and thus easy to assess ('box', 'fountain', 'no', etc.).For the motivation question a consensus score was obtained from two expert raters, on a range from 0-2, with 0 meaning a missing, irrelevant, or wrong motivation, 1 meaning a partly appropriate motivation, and 2 meaning a completely appropriate motivation that fully explained why the character in each scenario did or said something, or had a mental or emotional mind state.Thus, the maximum score for the SA1, SA2, and SS was 3 points per item, which were averaged to obtain a score between 0 and 1.For each correct answer to a true/false question in the IM, 1 point was given.All scores and ratings can be found on OSF.</p>
<p>Deviations</p>
<p>We tested the LLMs on the original SA and SS scenarios, but also on manually created deviations that increasingly stray from their original formulations, to prevent LLMs from leveraging heuristics and memorizing relevant patterns from the training data.Thus, deviations probe the degree to which performance on ToM tests in LLMs generalizes.Deviation 0 was always the original test scenario (likely present in the training data); deviation 1 was a superficial variation on the original with only e.g., objects and names changed (similar to Kosinski (2023)), whereas deviation 2 was a completely new scenario where only the ToM-phenomenon at issue was kept constant (e.g., 'second-order false belief' or 'irony').Since our adaptation of the IM test has hitherto not been used or published, we did not include deviations for this test.</p>
<p>Test Procedures for LLMs</p>
<p>We leveraged 11 state-of-the-art LLMs: 4 base-LLMs and 7 instruct-LLMs (see Table 1).Inference parameters were set such that their output was as deterministic as possible (i.e. a temperature ≊ zero or zero where possible) improving reproducibility.Each inference was done independently to avoid in-context learning or memory leakage between questions.This means that for each question, the prompt repeated the following general structure:
[instruction] + [test scenario] + [question].
Instruct-LLMs were prompted in a questionanswering format that stayed as close as possible to the questionnaires given to children, without any further custom prompting or provision of examples.Instructions were also similar to those given to children (e.g.'You will be asked a question.Please respond to it as accurately as possible without using many words.').The 'Why'-questions in SA1 and SA2 were created by inserting the experimental question and answer the LLM gave into the prompt:
[instruction] + [test scenario] + [ex- perimental question] + [LLM answer] +['Why?'].
This was not necessary for SS, given that experimental and motivation questions could be answered independently.For base-LLMs, known to continue prompts rather than follow instructions, staying this close to the children's questionnaires was not feasible.For the SA and SS we therefore fed base-LLMs the scenario as described before, but formulated the questions as text-completion exercises (e.g.'Sally will look for the ball in the ').Additionally, when creating the motivation questions for SA1 and SA2, we inserted the correct answer to the experimental question, instead of the LLM's answer.This was because base-LLMs so often derailed in their output that the method described for instruct-LLMs did not yield sensible prompts.Base-LLMs thus had an advantage here over children and instruct-LLMs, who were potentially providing a motivation following up on an incorrect answer they gave to the experimental question.</p>
<p>For the closed questions in the IM we attempted to streamline the output of base-LLMs by including two example continuations in the desired answer format.These examples were based on trivial information we added to the scenarios, unrelated to the actual experimental questions.For example: 'Helen: I wear a blue jumper today.This is [incorrect]', where it was added in the story that Helen wears a green jumper.This pushed nearly all base-LLM responses towards starting with '[correct]' or '[incorrect]', which we then assessed as answers to the true/false questions.We considered a similar prompt structure for SA and SS, amounting to adopting few-shot learning for base-LLMs throughout (Brown et al., 2020), but given that reformulating questions as text-completion exercises was by itself effective to get the desired output format, we refrained from inserting further differences from how instruct-LLMs are prompted.It is important to note that our prompts were in general not optimized for maximal test performance, but rather designed to stay as uniform and close to the way children were tested as possible, enabling a fair comparison among LLMs and with child performance.</p>
<p>Test Procedures for Children</p>
<p>Children were recruited from one Dutch and one international school in the South-West of the Netherlands: 37 children in the younger group (7-8y) and 36 children in the older group (9-10y).Children were administered digital versions of the SA and SS for the younger group, and of the IM for the older group, which they completed individually on tablets or PCs equipped with a touch screen.Test scenarios and questions were presented in a self-paced text format and all SA and SS questions were followed by an open text field in which they had to type their answer.As the IM features long scenarios, voice-overs of the text were included to alleviate reading fatigue.Here children had to answer by pressing yes/no after each question.To reduce memory bottlenecks, accompanying drawings were inserted (see OSF) and navigating back and forth throughout the tests was enabled.Informed consent for each child was obtained from caretakers, and the study was approved by the Leiden University Science Ethics Committee (ref. no. 2021-18).Test answers were evaluated and scored parallel to the approach for LLMs (Section 3.2).</p>
<p>Results</p>
<p>Sally-Anne</p>
<p>Overall performance on SA1 versus SA2 is given in Figure 1, left column.Most base-LLMs perform above child level on first-order ToM (BLOOM, Davinci, LLaMA-30B) but fall at or or below child level on second-order ToM.A similar pattern is visible for instruct-LLMs: most models perform well above child level on first-order (GPT-4, GPT-3.5, PaLM2-chat, PaLM2), but not on second-order ToM.Exceptions are GPT-4 and GPT-3.5:while degrading on second-order, they remain above child level.For both base-and instruct-LLMs, smaller models tend to perform worse (Falcon-7B, Falcon-7B-I, FLAN-T5) with GPT-3's structurally low scores as striking exception.This is inconsistent with results reported by (Kosinski, 2023) for GPT-3, which is probably due to the fact that Kosinski applied a text-completion approach whereas we prompted GPT-3 with open questions.</p>
<p>When we consider the performance on SA1 and SA2 over deviations (middle and right columns in Figure 1), we see once more that almost all LLMs struggle with second-order ToM, since performance decreases already on deviation 0 (i.e. the original test scenario), except for GPT-3.5 and GPT-4.Yet, it is the combination of second-order ToM and deviation 2 that pushes also GPT-3.5 and GPT-4 substantially below child levels, except for Falcon-7B, although the chat-optimized version of this model (Falcon-7B-I) fails on all second-order questions.</p>
<p>Strange Stories</p>
<p>General performance on SS is given in Figure 2, left column.Whereas child performance declines as items become more complex (from 1 to 7; see Section 3.1), this is overall less the case for LLM performance.For instruct-LLMs, we see that GPT-4 approaches perfect scores throughout.GPT-3 and GPT-3.5 perform at or close to child level on item 1, after which their performance somewhat declines, while staying well above child level.Other instruct-LLMs show a mixed picture: PaLM2-chat and FLAN-T5 surpass child level earlier than PaLM2.Interestingly, smaller FLAN-T5 outperforms large PaLM and PaLM2-chat on more difficult items.Falcon-7B-I, as smallest instruct-LLM, performs overall worst.</p>
<p>If performance is plotted over deviations (right column in Figure 2) we see little impact on most base-LLMs.For instruct-LLMs, it is striking that deviation levels have almost no effect on the larger models (GPT-4, PaLM2, PaLM2-chat, GPT-3, GPT-3.5), but do more dramatically lower performance of smaller models (FLAN-T5, Falcon-7B-I).In sum, base-LLMs perform below child level, except for the most complex items.Several large instruct-LLMs match or surpass child level throughout, others only for more complex items.Unlike for SA, deviation levels seem to have little negative impact.</p>
<p>Imposing Memory</p>
<p>The classical finding for the IM test is that error rates go up significantly for questions involving higher levels of recursive intentionality, but not for memory questions on matched levels of complexity, suggesting a limit to the capacity for recursive ToM specifically (Stiller and Dunbar, 2007). 1 We verified this for our child data (n=36) with two mixed linear models for memory and intentional questions with random intercepts.We included five predictors that were contrast-coded such that each predictor indicated the difference in average performance with the previous level.For intentional questions, only the difference between level two and one was significant (β = −0.222,p &lt; .05),marking a cutoff point after which performance remained consistently low.For memory questions, performance 1 While there is consensus in the literature that higher levels of intentionality are significantly harder for participants than lower levels, by various measures, there is debate about the difference with memory questions; see e.g.Lewis et al. (2017).For a critical discussion of measuring recursive intentionality in general, see Wilson et al. (2023).remained high across all levels (&gt; .85),except for level four, where scores were significantly lower than at level three (β = −0.292,p &lt; .00),but went up again at level five (β = 0.208, p &lt; .00).Thus, in line with earlier work, we find a cut-off point after which scores on intentionality questions remained consistently low, compared to scores on matched memory questions.We have no clear explanation for the dip in performance on memory questions at level four, but observe that it is driven by low scores on only one specific question out of a total of four for this level, which children may have found confusing.</p>
<p>In Figure 3 we see that all base-LLMs perform below child level, in general and on both intentionality and memory questions, and there is little variation in performance, except that larger base-LLMs (BLOOM, GPT-davinci) improve on higher levels of recursion.Regarding instruct-LLMs, we see largely the same picture, as they almost all perform below child level, in general and on both types of questions.The exception is GPT-4, which performs consistently well on all levels and stays above child level after second-order intentionality.For the difference between memory and intentional questions, instruct-LLMs perform better on easier memory questions, and drop towards the end, while on intentional questions, they already start lower and stay relatively constant.Lastly, it is remarkable that FLAN-T5, as one of the smallest instruct-LLMs, overall increases performance as recursion levels go up, and ends at child level.For GPT-3.5, which performs worst of all instruct-LLMs on this task, we see the exact opposite.</p>
<p>Notes on Child Performance</p>
<p>It can be observed that performance for SA was overall low compared to what could be expected from children aged 7-8 years: x = 0.45 for SA1 and x = 0.225 for SA2.We have two complementary explanations for this.Firstly, as discussed in Section 3.5, children had to read the tests on a screen, after which they had to type answers in open text fields.This is a challenging task by itself that relies on additional skills including language proficiency, conscientiousness, digital literacy, and more.Secondly, whereas 'passing' originally only means that a child can work out where Sally will look (for the ball, or for Anne on her way to buy ice cream), we also asked for a motivation, which makes the test more demanding.For the SS, completed by the same group of children, we see the expected pattern that scores show a downward tendency as test items increase in difficulty.The older group, aged 9-10, completed the IM.As discussed in Section 4.3, scores resonate with earlier work.Given that we see child performance not as the central phenomenon under observation in this paper, but rather as a reference for LLM performance, further discussion is outside our scope.</p>
<p>Discussion</p>
<p>Summing up the results for the Sally-Anne tests, while it is less surprising that base-LLMs and smaller instruct-LLMs struggle with increasing test complexity and deviations, it is striking that second-order ToM immediately perturbs some large instruct-LLMs (e.g.PaLM2-chat), and that adding deviations from the original test formula- tions pushed performance of even the most competitive models down (e.g.GPT-4, GPT-3.5).This initially suggests that performance on ToM tasks does not generalize well beyond a few standard contexts in LLMs, in line with earlier work (Sap et al., 2022;Shapira et al., 2023;Ullman, 2023).</p>
<p>For the Strange Stories we saw that base-LLMs perform generally below child level.Most instruct-LLMs perform close to or above child level, particularly as items become more complex and child performance drops much more dramatically than LLM performance.Levels of deviation from the original test formulation seem to have made almost no impact for the SS, suggesting that the capacity to deal with non-literal language targeted by the Strange Stories test does generalize to novel contexts.We conclude that instruct-LLMs are quite capable at interpreting non-literal language, a skill that in humans involves ToM.Since the training data of LLMs includes numerous books and fora, which are typically rich in irony, misunderstanding, jokes, sarcasm, and similar figures of speech, we tentatively suggest that LLMs are in general wellequipped to handle the sort of scenarios covered in the Strange Stories.This should in theory include base-LLMs, but it could be that their knowledge does not surface due to the test format, even after specialized prompting.Going one step further, we hypothesize that Sally-Ann is generally harder for LLMs given that this test relies less on a very specific sort of advanced language ability, but more on a type of behaviourally-situated reasoning that LLMs have limited access to during training (see also Mahowald et al., 2023).</p>
<p>The Imposing Memory test was the most chal-lenging for both base-and instruct-LLMs.Since our version of it was never published before, it constitutes another robustness test, which only GPT-4 as largest instruct-LLM seems to pass well.The gap between base-and instruct-LLMs is best summarized in Figure 4.Here we see that no base-LLM achieves child level: all LLMs approaching or exceeding child performance are larger instruct-LLMs.Our adapted prompts and insertion of correct answers for motivation questions did not make a difference.We suggest that another issue for base-LLMs, besides the prompt format, was prompt length.This was highest for IM, which can explain why they struggled most with this test.Prompt length, in relation to the models' varying context window sizes and ability to engage in what Hagendorff et al. ( 2023) call chain-of-thought reasoning, merits further research (see also Liu et al., 2023).We tested whether there was a difference between model performance on closed versus open questions across all three tasks, but found no signal: the models that struggled with closed questions were also those that performed low on open questions (for more details and additional information on prompting, see Appendix A on OSF).</p>
<p>Evidence is emerging that most LLM capacities are learned during self-supervised pre-training (Gudibande et al., 2023;Ye et al., 2023), which suggests that base-LLMs are essentially 'complete' models.Yet instruction-tuning, even in small amounts (Zhou et al., 2023), adds adherence to the desired interaction format and teaches LLMs, as it were, to apply their knowledge appropriately.We see a parallel between instruction-tuning and the role for rewarding cooperative communication in human evolution and development.It has been argued extensively that human communication is fundamentally cooperative in that it relies on a basic ability and willingness to engage in mental coordination (e.g Verhagen, 2015;Grice, 1975).It is a key characteristic of the socio-cultural niche in which we evolved that, when growing up, we are constantly being rewarded for showing such willingness and cooperating with others to achieve successful communicative interactions (Tomasello, 2008).Reversely, if we do not, we are being punished, explicitly or implicitly via increasing social exclusion (David-Barrett and Dunbar, 2016).This brings us back to our context: instruction-tuning essentially rewards similar cooperative principles, but punishes the opposite, which may amount to an enhanced capacity for coordinating with an interaction partner's perspective, in humans and LLMs alike.This is reflected in performance on ToM tasks, which are banking on this capacity too.</p>
<p>Finally, we do not claim that LLMs that performed well also have ToM in the way that humans have it.Validity of cognitive tests such as those used in ToM research is a general issue (e.g.van Duijn, 2016).Yet for humans ToM tests are validated 'quick probes': decades of research have shown that proficiency on such tests correlates with an array of real-world social and cognitive abilities (Beaudoin et al., 2020).For LLMs we are in a very early stage of figuring out what is entailed by proficon ToM tests: on the one hand it is impressive that some models show a degree of robust performance, without explicit training on ToM.On the other hand it remains an open question whether this amounts to any actual capacities in the social-cognitive domain, in which they are clearly very differently grounded (if at all) compared to humans.</p>
<p>For future research we believe in the format of testing models that differ in other respects than just size, on a varied array of tasks, with multiple tests per test item, to gain further insight into the aspects that explain variability in performance.For this, more openness about architecture and training procedures of current and future LLMs is imperative.In addition, we believe to have contributed to the debate by benchmarking LLM results on child data, but more of this is needed.We had limited samples and age distributions, and tests were not presented in optimal ways (see Section 3.5).</p>
<p>We emphasize that our results need to be seen within the time frame of late Spring 2023.The fast pace with which LLMs are currently released and, in some cases, updated, makes them a moving target.There are indications that specific capacities of models from the GPT-family have declined over time, perhaps as a result of such updates (e.g., handling math problems and producing code; Chen et al., 2023).Future studies need to address how such developments impact the capacities assessed in this paper.</p>
<p>Conclusion</p>
<p>We have shown that a majority of recent Large Language Models operate below performance of children aged 7-10 on three standardized tests relevant to Theory of Mind.Yet those that are largest in terms of parameters, and most heavily instructiontuned, surpass children, with GPT-4 well above all other models, including more recent competitors like PaLM2-chat and PaLM2 (see Figure 4).We have interpreted these findings by drawing a parallel between instruction-tuning and rewarding cooperative interaction in human evolution.We concede that researching the degree to which LLMs are capable of anything like thought in the human sense has only just begun, which leaves the field with exciting challenges ahead.</p>
<p>Figure 1 :
1
Figure 1: Performance on Sally-Anne tests for base-LLMs (top row) and instruct-LLMs (bottom row).Left column depicts performance on first-and second-order ToM (i.e.SA1 vs. SA2), averaged over the original and rewritten test versions.Middle and left columns depict performance for SA1 and SA2 over levels of deviation from the original test (0, 1, and 2; see Section 3.3).Dashed lines indicate child performance (n=37, age 7-8 years).</p>
<p>Figure 2 :
2
Figure 2: Performance on Strange Stories for base-LLMs (top row) and instruct-LLMs (bottom row).Left column shows overall performance, averaged over levels of deviation from the original test.Right column shows performance over deviation levels, averaged over items.Dashed lines indicate child performance (n=37, 7-8y).</p>
<p>Figure 3 :
3
Figure 3: Performance on Imposing Memory test for base-LLMs (top row) and instruct-LLMs (bottom row).Left column depicts overall performance over five levels of recursion, averaged over deviations.Middle and left columns depict performance for Memory and Intentional questions.Dashed lines indicate child performance (n=36, 9-10y).</p>
<p>Figure 4 :
4
Figure 4: Grand mean performance (stars) of all mean test scores (dots) for children and LLMs.</p>
<p>Table 1 :
1
Elias (2023))this study.Model sizes are undisclosed for GPT-4 and for PaLM2 and PaLM2-chat, thus we base ourselves on secondary sources for estimations;Knight (2023)andElias (2023), respectively.
Base-LLMsSourceSizeFalconPenedo et al. (2023)7BLLaMATouvron et al. (2023)30BGPT-davinciBrown et al. (2020)175BBLOOMScao et al. (2022)176BInstruct-LLMs""Falcon-instructPenedo et al. (2023)7BFlan-T5Chung et al. (2022)11BGPT-3(text-davinci-003) Ouyang et al. (2022)175BGPT-3.5-turboOuyang et al. (2022)175BPaLM2Anil et al. (2023)175-340BPaLM2-chatAnil et al. (2023)175-340BGPT-4OpenAI (2023)&gt;340B
AcknowledgementsThis research took place in the context of the project A Telling Story, financed by the Dutch Research Council NWO (VI.Veni.191C.051).We are grateful to the children and their caregivers and teachers for participating in our research, and we thank Li Kloostra, Lola Vandame, and three anonymous reviewers for their help and constructive feedback.
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, ; Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, arXiv:2305.10403v3Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin XuSlav PetrovYunhan Xu, Linting Xue, Pengcheng Yin,arXiv preprintand Yonghui Wu. 2023. PaLM 2 Technical Report</p>
<p>Mindreaders: the Cognitive Basis of "Theory of Mind. Ian Apperly, 2010Psychology Press</p>
<p>Five-year-olds' systematic errors in secondorder false belief tasks are due to first-order theory of mind strategy selection: A computational modeling study. Niels A Burcu Arslan, Rineke Taatgen, Verbrugge, Frontiers in psychology. 82752017</p>
<p>Bayesian theory of mind: Modeling joint belief-desire attribution. Chris Baker, Rebecca Saxe, Proceedings of the Thirty-Third Annual Conference of the Cognitive Science Society. the Thirty-Third Annual Conference of the Cognitive Science Society2011</p>
<p>Does the autistic child have a "theory of mind. Simon Baron-Cohen, Alan M Leslie, Uta Frith, Cognition. 2111985</p>
<p>Infants' performance in spontaneous-response false belief tasks: A review and meta-analysis. Pamela Barone, Guido Corradi, Antoni Gomila, 10.1016/j.infbeh.2019.101350Infant Behavior and Development. 571013502019</p>
<p>Systematic review and inventory of theory of mind measures for young children. Cindy Beaudoin, Élizabel Leblanc, Charlotte Gagner, Miriam H Beauchamp, Frontiers in psychology. 1029052020</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Turning large language models into cognitive models. Marcel Binz, Eric Schulz, arXiv.2306.039172023aarXiv preprint</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023b</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Do conversational agents have a theory of mind? a single case study of chatgpt with the hinting, false beliefs and false photographs, and strange stories paradigms. Eric Brunet-Gouet, Nathan Vidal, Paul Roux, 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with GPT-4. 2023</p>
<p>Peter Carruthers, 10.1111/mila.12014Mindreading in infancy. Mind &amp; Language. 201328</p>
<p>Lingjiao Chen, Matei Zaharia, James Zou, arXiv:2307.09009How is ChatGPT's behavior changing over time?. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311PaLM: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/ARXIV.2210.114162022Scaling instruction-finetuned language models</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Language as a coordination tool evolves slowly. Tamas David, - Barrett, Robin I M Dunbar, 10.1098/rsos.160259R. Soc. open sci. 31602592016</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019</p>
<p>Can AI language models replace human participants?. Danica Dillion, Niket Tandon, Yuling Gu, Kurt Gray, 10.1016/j.tics.2023.04.008Trends in Cognitive Sciences. 2772023</p>
<p>Google's newest A.I. model uses nearly five times more text data for training than its predecessor. Jennifer Elias, 2023</p>
<p>A theory of the child's theory of mind. J A Fodor, 10.1016/0010-0277(92)90004-2Cognition. 4431992</p>
<p>Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko, Nature human behaviour. 672022</p>
<p>How can memory-augmented neural networks pass a false-belief task?. Erin Grant, Aida Nematzadeh, Thomas L Griffiths, CogSci. 2017</p>
<p>Logic and conversation. Paul Grice, Syntax and semantics. Peter Cole, Jerry Morgan, New YorkAcademic Press19753Speech acts</p>
<p>The false promise of imitating proprietary llms. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, 2023</p>
<p>Humanlike intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, 10.1038/s43588-023-00527-xNature Compututer Science. 2023</p>
<p>Thilo Hagendorff, arXiv:2303.13988Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. 2023arXiv preprint</p>
<p>An advanced test of theory of mind: Understanding of story characters' thoughts and feelings by able autistic, mentally handicapped, and normal children and adults. G E Francesca, Happé, Journal of autism and Developmental disorders. 2421994</p>
<p>False belief in infancy: a fresh look. Cecilia Heyes, 10.1111/desc.12148Developmental Science. 1752014</p>
<p>The cultural evolution of mind reading. Cecilia M Heyes, Chris D Frith, 10.1126/science.1243091Science. 344619012430912014</p>
<p>Folk Psychological Narratives: The Sociocultural Basis of Understanding Reasons. D Daniel, Hutto, 2008The MIT Press</p>
<p>The Strange Stories test -a replication study of children and adolescents with Asperger syndrome. Nils Kaland, Annette Møller-Nielsen, Lars Smith, Erik Lykke Mortensen, Kirsten Callesen, Dorte Gottlieb, European child &amp; adolescent psychiatry. 1422005</p>
<p>Theory-of-mind deficits and causal attributions. P Kinderman, R Dunbar, R P Bentall, 10.1111/j.2044-8295.1998.tb02680British Journal of Psychology. 21998</p>
<p>Ai-based large language models are ready to transform psychological health assessment. Oscar Kjell, Katarina Kjell, Andrew Schwartz, 2023</p>
<p>A new chip cluster will make massive ai models possible. Will Knight, 2023</p>
<p>Michal Kosinski, arXiv:2302.02083Theory of mind may have spontaneously emerged in large language models. 2023arXiv preprint</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, 10.18653/v1/D19-1598Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Higher order intentionality tasks are cognitively more demanding. Penelope A Lewis, Amy Birch, Alexander Hall, Robin I M Dunbar, 10.1093/scan/nsx034Social Cognitive and Affective Neuroscience. 1272017</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>Homer's intelligent machines: AI in antiquity. Genevieve Liveley, Sam Thomas, 10.1093/oso/9780198846666.003.00022020</p>
<p>Gender and Representation Bias in GPT-3 Generated Stories. Li Lucy, David Bamman, 10.18653/v1/2021.nuse-1.5Proceedings of the Third Workshop on Narrative Understanding. the Third Workshop on Narrative UnderstandingVirtual. Association for Computational Linguistics2021</p>
<p>Tomchallenges: A principle-guided dataset and diverse evaluation tasks for exploring theory of mind. Xiaomeng Ma, Lingyu Gao, Qihui Xu, arXiv:2305.150682023arXiv preprint</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. D Christopher, Kevin Manning, John Clark, Urvashi Hewitt, Omer Khandelwal, Levy, 10.1073/pnas.1907367117Proceedings of the National Academy of Sciences. the National Academy of Sciences2020117</p>
<p>Developing chatgpt's theory of mind. Antonella Marchetti, Cinzia Di Dio, Angelo Cangelosi, Federico Manzi, Davide Massaro, 10.3389/frobt.2023.1189525Frontiers in Robotics and AI. 102023</p>
<p>Language and theory of mind: Metaanalysis of the relation between language ability and false-belief understanding. Karen Milligan, Janet Wilde Astington, Lisa Ain Dack, 10.1111/j.1467-8624.2007.01018.xChild development. 7822007</p>
<p>Boosting theory-of-mind performance in large language models via prompting. Rahimi Shima, Christopher J Moghaddam, Honey, 2023</p>
<p>Evaluating theory of mind in question answering. Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Tom Griffiths, 10.18653/v1/D18-1261Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Universal dependencies v1: A multilingual treebank collection. Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning, Ryan Mcdonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)2016</p>
<p>Do 15-month-old infants understand false beliefs?. Kristine H Onishi, Renée Baillargeon, 10.1126/science.1107621Science. 30857192005</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>attribution of second-order beliefs by 5-to 10-year-old children. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. 2023. 198539arXiv preprintJohn thinks that Mary thinks that</p>
<p>Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic. Billy Perrigo, 2023</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 141978</p>
<p>Machine theory of mind. Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S M Ali Eslami, Matthew Botvinick, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR201880of Proceedings of Machine Learning Research</p>
<p>Visualizing and measuring the geometry of BERT. Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, Been Kim, Advances in Neural Information Processing Systems. 201932</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, 10.1162/tacl_a_00349A primer in BERTology: What we know about how BERT works. 20218</p>
<p>Neural theory-of-mind? on the limits of social intelligence in large LMs. Maarten Sap, Le Ronan, Daniel Bras, Yejin Fried, Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100BLOOM: A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Natalie Shapira, Mosh Levy, Hossein Seyed, Xuhui Alavi, Yejin Zhou, Yoav Choi, Maarten Goldberg, Vered Sap, Shwartz, Clever Hans or neural theory of mind? stress testing social reasoning in large language models. 2023</p>
<p>MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. Damien Sileo, Antoine Lernould, arXiv.2305.033532023arXiv preprint</p>
<p>Full-on robot writing': the artificial intelligence challenge facing universities. Jeff Sparrow, 2022</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Damien Garbacea, Dan Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, C Roth, Daniel Daniel Freeman, Daniel Khashabi, Daniel Levy, Danielle Moseguí González, Danny Perszyk, Danqi Hernandez, Daphne Chen, Dar Ippolito, David Gilboa, David Dohan, David Drakard, Debajyoti Jurgens, Deep Datta, Denis Ganguli, Denis Emelin, Deniz Kleyko, Derek Yuret, Derek Chen, Dieuwke Tam, Diganta Hupkes, Dilyar Misra, Dimitri Coelho Buzan, Diyi Mollo, Dong-Ho Yang, Dylan Lee, Ekaterina Schrader, Ekin Shutova, Elad Dogus Cubuk, Eleanor Segal, Elizabeth Hagerman, Elizabeth Barnes, Ellie Donoway, Emanuele Pavlick, Emma Rodolà, Eric Lam, Eric Chu, Erkut Tang, Ernie Erdem, Ethan A Chang, Ethan Chi, Ethan Dyer, Ethan Jerzak, Eunice Engefu Kim, Evgenii Manyasi, Fanyue Zheltonozhskii, Fatemeh Xia, Fernando Siar, Francesca Martínez-Plumed, Francois Happé, Frieda Chollet, Gaurav Rong, ; Mishra, Germán Gerard De Melo, Giambattista Kruszewski, Giorgio Parascandolo, Gloria Xinyue Mariani, Gonzalo Wang, Gregor Jaimovitch-Lopez, Guy Betz, Hana Gur-Ari, Hannah Galijasevic, Hannah Kim, Hannaneh Rashkin, Harsh Hajishirzi, Hayden Mehta, Henry Bogar, Anthony Francis, Hinrich Shevlin, Hiromu Schuetze, Hongming Yakura, Hugh Mee Zhang, Ian Wong, Isaac Ng, Jaap Noble, Jack Jumelet, Jackson Geissinger, Jacob Kernion, Jaehoon Hilton, Jaime Fernández Lee, James B Fisac, James Simon, James Koppel, James Zheng, Jan Zou, Jana Kocon, Janelle Thompson, Jared Wingfield, Jarema Kaplan, Jascha Radom, Jason Sohl-Dickstein, Jason Phang, Jason Wei, Jekaterina Yosinski, Jelle Novikova, Jennifer Bosscher, Jeremy Marsh, Jeroen Kim, Jesse Taal, Jesujoba Engel, Jiacheng Alabi, Jiaming Xu, Jillian Song, Joan Tang, John Waweru, John Burden, John U Miller, Jonathan Balis, Jonathan Batchelder, Jörg Berant, Jos Frohberg, Jose Rozen, Joseph Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joshua B Jones, Joshua S Tenenbaum, Joyce Rule, Kamil Chua, Karen Kanclerz, Karl Livescu, Karthik Krauth, Katerina Gopalakrishnan, Katja Ignatyeva, Kaustubh Markert, Kevin Dhole, Kevin Gimpel, Kory Wallace Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mc-Donell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros-Colón, Lütfi Kerem Metz, Maarten Senel, Maarten Bosma, Maartje Sap, Maheen Ter Hoeve, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Jose Maru, Marie Ramirez-Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Hagen ; Mimee, Mirac Xu, Mitch Suzgun, Mo Walker, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans ; Peiyuan, Percy Liao, Peter W Liang, Sam Chang, Sam Shleifer, Samuel Wiseman, Gruetter, R Samuel, Samuel Bowman, Sanghyun Stern Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shammie Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven Prasad, Piantadosi ; Tao, Tao Li, Tariq Yu, Tatsunori Ali, William Hashimoto, William Fedus, William Saunders, Zhang ; Yasaman, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Wang, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain,; Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Stuart Shieber; Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick; Titus Tunduny, Tobias Gerstenberg, Trenton ChangTimofei Kornev29Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster,Pablo Antonio Moreno CasaresSocial Networks</p>
<p>Origins of Human Communication. Michael Tomasello, 2008MIT PressCambridge, MA</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 2023</p>
<p>Computing machinery and intelligence. Alan M Turing, 10.1093/mind/LIX.236.4331950Mind, LIX</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>The lazy mindreader: a humanities perspective on mindreading and multiple-order intentionality. Max J Van Duijn, 2016Leiden UniversityPh.D. thesis</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Grammar and cooperative communication. Arie Verhagen, 10.1515/9783110292022-012Handbook of Cognitive Linguistics. Ewa Dabrowska, Dagmar Divjak, Berlin, München, BostonDe Gruyter Mouton2015</p>
<p>Adelheid Voskuhl, 10.7208/chicago/9780226034331.003.0001One introduction: Androids, enlightenment, and the human-machine boundary. 2013</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M. Dai, and Quoc V. Le.2023. 2022Finetuned language models are zero-shot learners</p>
<p>Is recursive "mindreading" really an exception to limitations on recursive thinking. Robert Wilson, Alexander Hruby, Daniel Perez-Zapata, Sanne W Van Der Kleij, Ian A Apperly, 10.1037/xge0001322Journal of Experimental Psychology: General. 15252023</p>
<p>Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children's understanding of deception. Heinz Wimmer, Josef Perner, 10.1016/0010-0277(83)90004-5Cognition. 1311983</p>
<p>Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, arXiv:2303.10420A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. 2023arXiv preprint</p>
<p>. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, 2023Less is more for alignmentLima</p>            </div>
        </div>

    </div>
</body>
</html>
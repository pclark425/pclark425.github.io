<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3660 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3660</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3660</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-267976029</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.wiserpub.com/index.php/JEEE/article/download/4010/1964" target="_blank">Academic Education in the Era of Generative Artificial Intelligence</a></p>
                <p><strong>Paper Abstract:</strong> : This paper provides a technical review of the Generative AI technology and its challenges and opportunities in the education sector. Generative Artificial Intelligence (GAI), presented in some tools such as ChatGPT, has been continuously penetrating our normal lives. It has also attracted several research efforts, from both academia and industry researchers, to solve real-world problems in different applications such as finance and health. Generative AI is indeed a type of Artificial Intelligence that can efficiently “create” a wide variety of information in the form of images, videos, audio, text, and 3D models. Therefore, they can facilitate data analysis and visualization, and enhance personalized and adaptive learning in the education sector. Although instructors and teachers will not be substituted by Generative AI robots completely, education and academic delivery of courses are expected to experience a revolution in the presence of GAI. Similar to other new technologies, serious potential challenges and opportunities are expected in employing GAI in the education sector.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3660",
    "paper_id": "paper-267976029",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004439,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Academic Education in the Era of Generative Artificial Intelligence</p>
<p>Maryam Vafadar vafadar.mar@gmail.com 
Department of Electrical Engineering
Islamshahr Branch
Islamic Azad University
IslamshahrIran</p>
<p>Ali Moradi Amani 
School of Engineering
RMIT University
MelbourneAustralia</p>
<p>Academic Education in the Era of Generative Artificial Intelligence
19484ACADE25BB36FBEE5A49337E89D110.37256/jeee.3120244010Received: 29 November 2023; Revised: 1 January 2024; Accepted: 16 January 2024artificial intelligencegenerative artificial intelligenceChatGPTeducationadaptive learning
This paper provides a technical review of the Generative AI technology and its challenges and opportunities in the education sector.Generative Artificial Intelligence (GAI), presented in some tools such as ChatGPT, has been continuously penetrating our normal lives.It has also attracted several research efforts, from both academia and industry researchers, to solve real-world problems in different applications such as finance and health.Generative AI is indeed a type of Artificial Intelligence that can efficiently "create" a wide variety of information in the form of images, videos, audio, text, and 3D models.Therefore, they can facilitate data analysis and visualization, and enhance personalized and adaptive learning in the education sector.Although instructors and teachers will not be substituted by Generative AI robots completely, education and academic delivery of courses are expected to experience a revolution in the presence of GAI.Similar to other new technologies, serious potential challenges and opportunities are expected in employing GAI in the education sector.</p>
<p>Introduction</p>
<p>In recent years, machine learning and Artificial Intelligence have demonstrated significant applications in different industries including software development, product design, healthcare, finance, marketing, and education [1][2][3][4][5][6][7][8][9].The Generative Artificial Intelligence (GAI) is a type of Artificial Intelligence that provides the opportunity to create a wide variety of data, such as images, videos, audio, text, and 3D models.Indeed, traditional AI algorithms have been used for rather a long time to analyze and identify patterns within a training data set and make models for different applications, while GAI goes a step further by generating new records for the training dataset based on users' feedback to create personalized content.Obviously, in the education sector, GAI can be applied to visualize course material, personalize learning methods, generate research directions, and increase student engagement, more efficient than traditional AI methods.Furthermore, in emerging contagious diseases like Covid-19 epidemic situation, education should not be stopped and GAI-powered tools and technologies can enhance the learning experience for students from personalized learning algorithms to virtual and augmented reality.</p>
<p>Investment in GAI surged during the early 2020s in leading companies such as Microsoft, Google, Apple and Baidu as well as start-ups [10,11].In 2023, ChatGPT was unveiled as an efficient GAI-based framework by a research organization backed by Microsoft named OpenAI.Subsequently, ChatGPT quickly gained immense popularity on the internet, attracting over one million users within a mere five days.In response to the emergence of ChatGPT, Google introduced Bard on February 7, 2023 [12].Bard, Google's recently launched web application, is anticipated to be seamlessly integrated with the company's other offerings, such as Google</p>
<p>The History of GAI</p>
<p>Early stages: While AI has gained significant attention in recent decades, its roots trace back to a farreaching history.We provided a concise overview of the history of GAI in Figure 1.To be specific, Modern AI started to emerge in the 1950s, marked by Alan Turing's exploration of machine thinking and the introduction of the renowned Turing test [18,19].In the 1950s and 1960s, researchers delved into the potential of AI.The inception of trainable neural networks, the foundational technology behind GAI, occurred in 1957 through the work of Frank Rosenblatt, a psychologist at Cornell University [20].Subsequent advancements in neural networks propelled their extensive adoption in AI throughout the 1980s and beyond.During that era, AI researchers concentrated on creating rule-based systems capable of simulating human thought processes and decision-making.</p>
<p>Approaching the turn of the millennium, researchers began to explore the application of generative models across domains, including speech recognition, image processing, and Natural Language Processing (NLP).Their endeavors encompassed the utilization of statistical and generative techniques to model and anticipate data patterns.Novel generative models, such as Bayesian networks and Markov models, found practical use in the realms of robotics and computer vision, as exemplified in [21].During the early 2000s, the emergence of deep learning, a subset framework within machine learning characterized by neural networks with increased hidden layers, triggered substantial research activity in image classification, speech recognition, NLP, and related tasks.Notably, neural networks of this era were predominantly employed as discriminative models, primarily due to the inherent complexities associated with generative modeling, as described in [22].</p>
<p>GAI ramp-up: In 2014, a significant milestone was reached with the creation of generative adversarial networks (GANs) [23], a class of deep learning algorithms that could authentically generate images, videos, and audio of real individuals.During the same year, further progress emerged, including the introduction of the variational autoencoder, which marked a pivotal moment in the development of practical deep neural networks capable of learning generative models for complex data, such as images.This innovation allowed deep generative models to create entire images rather than merely providing class labels for them [24].In 2015, notable Convolutional Neural Networks (CNNs), including Resnet-50, were first unveiled, featuring distinct processing components that proved effective in handling intricate tasks like machine translation and recognition [25].The transformative moment came in 2017 with the advent of the Transformer network, which paved the way for the creation of the inaugural GPT in 2018 by OpenAI, a company with backing from Microsoft [26].Subsequent developments followed with the introduction of GPT-2 in 2019 and GPT-3 in 2020.These models, based on deep neural networks, were decoder-only transformer models utilizing attention mechanisms in place of previous recurrence and convolution-based architectures.They demonstrated the remarkable capacity to generalize unsupervised classifications across a wide range of tasks, serving as foundational models [27].</p>
<p>Recent advances: ChatGPT, a platform developed by OpenAI, is a large language model-based GAI platform.It marks the latest iteration following its predecessors and was officially launched on November 30, 2022.Subsequently, ChatGPT underwent optimization with the introduction of GPT-3.5 in January 2023, followed by GPT-4 in March 2023.These proprietary GPT models, GPT-3.5 and GPT-4, were specifically engineered by OpenAI for conversational applications through a blend of supervised and reinforcement learning techniques [28].OpenAI offers free access to the GPT 3.5-based version of ChatGPT, while the more advanced GPT-4-based version, along with priority access to newer features, is made available to subscribers under the commercial name "ChatGPT Plus."Remarkably, by January 2023, ChatGPT had experienced unprecedented growth, emerging as one of the fastest-growing consumer software applications in history, amassing over 100 million users.This extraordinary expansion contributed significantly to OpenAI's valuation, which surged to US$29 billion [29,30].The concept of GAI has been heavily investigated in recent years and has attracted significant investments.Despite the tech downturn, AI companies are still attracting much attention from investors.Investment in GAI has already reached over two billion dollars, up to 425 percent increase from 2020 [32].GAI has been and will be widely extended by scientists and experts.Most probably, it would impact our future lives and convey different fields such as education.Like other new technologies, despite promoting proficiencies in some aspects, serious potential issues need to be considered.In the next section, we explain the main concepts related to GAI and its different types to mitigate and harness the potential drawbacks.</p>
<p>Fundamental Concepts of GAI</p>
<p>In this section, we explain the fundamental concepts related to GAI, potential solutions, and its different types.Figure 2 demonstrates how the categorization of AI has progressed from 2022 to 2023 with the emergence of GAI technologies.The GAI has been categorized under ML.In fact, ML is a type of AI that emphasizes extracting patterns and learning from data, possibly for making predictions.Deep learning, located under ML in Figure 2, utilizes artificial neural networks (ANNs) to carry out intricate computations on vast volumes of data.It provides an advanced analysis tool in the Data-rich world where we are living thanks to the advancement of measurement and communication technologies.[33] to 2023 [34].</p>
<p>The GAI, often referred to as creative AI, constitutes a subset of machine learning techniques that leverage multi-layer ANNs to handle intricate patterns.Beyond traditional learning, GAI possesses the ability to generate diverse media, including text and images, based on the available data.GAI models undergo training, wherein they grasp the patterns and structures of the input data through neural network machine learning techniques applied to extensive language models.Consequently, they generate novel data with analogous characteristics [35].Notably, a significant contrast between Traditional AI and GAI lies in their respective roles: Traditional AI primarily focuses on data analysis and predictions, while GAI goes a step further by generating new data resembling its training dataset based on user guidance [34].</p>
<p>From an implementation standpoint, GAI empowers users to initiate the content generation process by submitting queries and pertinent data [34].Prominent examples of GAI systems relying on large language models include i) ChatGPT (along with its variant Bing Chat), a chatbot crafted by OpenAI using the foundational large language models GPT-3 and GPT-4 [36], ii) Bard, a chatbot developed by Google using their LaMDA foundation model, and iii) AI-driven artistic systems such as Stable Diffusion, Midjourney, and DALL-E [37].</p>
<p>The effectiveness of a GAI system is contingent upon the modality or the type of dataset it operates with.GAI systems can be categorized as unimodal or multimodal.Unimodal systems exclusively handle one type of input, while multimodal systems are proficient in processing multiple input types.For instance, one multimodal iteration of OpenAI's GPT-4 accommodates both textual and image inputs [38].GAI models are trained using vast datasets and deploy a spectrum of self-supervised, unsupervised, or semi-supervised ML techniques to refine their capabilities.Deep learning, neural networks, and machine learning methods contribute to the creation of outputs in GAI systems.In certain instances, various models collaborate to enhance GAI efficiency.Subsequently, we will briefly explore the primary learning strategies and models employed in GAI.</p>
<p>Generative Adversarial Networks (GANs)</p>
<p>Generative Adversarial Networks (GANs) comprise a duo of ANNs that function in tandem: a generator and a discriminator.The generator is responsible for producing fresh examples, while the discriminator assesses the disparities between the generated content and the pre-existing content [38].These disparities are subsequently relayed to the generator ANN, influencing its content generation process.This iterative mechanism continually enhances the performance of the GAI system with each cycle, progressing until the generated content becomes indistinguishable from the existing content.Notably, both ANNs undergo concurrent training and become increasingly proficient as the generator refines its content production, while the discriminator hones its detection abilities.Although GANs exhibit the capability to yield high-quality samples swiftly, their sample diversity is relatively limited.It is foreseen that the forthcoming generation of GANs may be constrained for application within unimodal systems, specialized for domain-specific data generation.</p>
<p>GANs have been used in several applications, from image and music generation to drug discovery.Incredibly realistic portraits of people, who do not exist, can be generated using ProGan by NVIDIA [39].The quality of old and low-resolution images can be significantly enhanced using Deep Zoom by Google (see Figure 3).Jukebox from OpenAI uses GANs to create new music or compose original pieces.GauGAN2 by NVIDIA is a powerful tool to create photorealistic art using advanced text-to-speech techniques.</p>
<p>Autoencoders</p>
<p>Autoencoders are unsupervised neural networks that first learn how to minimize and encode the information before teaching themselves how to decode the compressed and encoded data and rebuild it into a form that is as close to the original input as practical.By developing the ability to disregard data noise, it minimizes the dimensions of the data.Autoencoders can be used for image denoising [41] and Anomaly detection [42].They are applied to many problems, including facial recognition in image processing which is indeed the identification of a reference image, feature detection, and acquiring the meaning of words in machine translation [43].</p>
<p>Autoencoders are powerful tools in machine learning used for feature extraction, data compression, and image reconstruction.For example, viso (see https://viso.ai) is one of the solutions in computer vision based on computer vision.Several improved versions of AR have also been proposed by researchers for different applications, see e.g.[44] for a comprehensive review.</p>
<p>AutoRegressive Models</p>
<p>In order to increase the likelihood of training data, autoregressive (AR) models provide an attainable explicit density model.This makes it simple to estimate the likelihood of data observation and to provide an evaluation measure for the generative model using these approaches.It uses a regression model to estimate the value of the following time step after learning from a large number of timed steps and measurements from earlier activities.Indeed, an autoregressive model forecasts future behavior based on past behavior data.This type of analysis is used when there is a correlation between the time series values and their preceding and succeeding values.In other words, Autoregressive modeling relies only on past period values to predict current ones by a linear model, where current period values are a sum of past outcomes multiplied by a numeric factor.</p>
<p>During last decades, AR has shown several applications in time series analysis for economic and market price forecasting [45], traffic flow prediction [46], and manufacturing [47].In healthcare, AR methods have been widely used in monitoring, analysis, and prediction of vital signs of patients, as well as drug response prediction.Modelling and prediction of physiological signals, such as electrocardiogram (ECG) for arrhythmias and electroencephalogram (EEG) for brain activity, using AR has also been reported [48].</p>
<p>Variational Autoencoders</p>
<p>Variational autoencoders (VAEs) are structured as a combination of two interconnected ANNs, commonly referred to as the encoder and decoder.In essence, VAEs constitute a subset of deep generative networks sharing analogous encoder (inference) and decoder (generative) components with traditional autoencoders.When presented with an input, the encoder transforms it into a more compact and condensed representation of the data.This compressed representation retains the essential information required for the decoder to reconstruct the initial input data while filtering out irrelevant details.The collaborative operation of the encoder and decoder is aimed at acquiring an efficient and simplified latent data representation.Consequently, users gain the ability to conveniently generate fresh latent representations that can be processed through the decoder, resulting in the creation of novel data.While VAEs exhibit speed in generating outputs like images, it's important to note that the images they generate may not possess the same level of detail as those produced by diffusion models.</p>
<p>VAEs are typically used for generating data for images and audio signals [49].In fact, VAEs learn the latent distribution of data such that they can generate meaningful samples.A model of VAE has been used for text classification [50].Interestingly, [51] has focused on the energy efficiency of running VAEs and aims to reduce the carbon footprint and financial cost of these techniques.</p>
<p>Diffusion Models</p>
<p>Diffusion models, which are alternatively referred to as denoising diffusion probabilistic models (DDPMs), employ a dual-phase training process, encompassing forward diffusion and reverse diffusion.The forward diffusion phase systematically introduces gradual random Gaussian noise to the training data, while the reverse diffusion phase strives to eliminate this noise in order to reconstruct the original data samples.To generate novel data, the reverse denoising process can be initiated from a state of entirely random noise.</p>
<p>It's worth noting that diffusion models may necessitate a more extended training period compared to VAE models.However, their distinctive two-step training process allows for the training of numerous layers, potentially approaching an infinite number.This characteristic translates to diffusion models often delivering high-quality outputs, rendering them particularly suitable for the development of GAI models.Additionally, these models fall into the category of foundation models due to their large-scale and adaptable nature, offering exceptional output quality and suitability for various generalized use cases, such as image denoising, superresolution, and image generation [52].Nevertheless, it's important to acknowledge that the reverse sampling process involved in these foundation models may lead to slower and more time-consuming operations.Researchers from NVIDIA and Google AI have provided a deep overview of the fundamentals and applications of DDPMs, see https://cvpr2022-tutorial-diffusion-models.github.io/.</p>
<p>Recurrent Neural Networks</p>
<p>Recurrent neural networks (RNNs) represent a specific category of ANNs that find extensive application in the domains of NLP and speech recognition.While conventional deep neural networks typically assume that inputs and outputs are independent of each other, the output of RNN is intricately linked to preceding elements of the sequence using the concept of "memory".RNNs preserve a hidden state that can capture information from previous time steps in the sequential data.The hidden state is updated at each time step by combining the input data with the previous hidden state through a series of weight parameters and activation functions.RNN are mainly tailored for the processing of sequential and time series data.</p>
<p>RNNs can be classified into three variant architectures: Bidirectional recurrent neural networks (BRNN), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs).Unlike standard RNN, which requires delays for including future information, BRNN connects hidden layers' neurons using a forward-backward scheme, i.e. the input information from the past (backward) and future (forward) of the current time frame.LSTM and GRU architectures help RNNs overcome the gradient problem, which can occur when training RNNs on long sequences.When gradients become too small (vanish) or too large (explode) during training, the network can struggle to learn the underlying patterns in the data.LSTMs and GRUs are specifically designed to mitigate these issues, making them more effective for learning long-term sequential data by selectively updating information from earlier time steps.RNNs have been used extensively to model complex temporal relationships, suitable for time series forecasting, natural language processing, speech recognition, and sentiment analysis [53].</p>
<p>Transformer Models</p>
<p>Transformer models represent a class of deep learning architectures widely applied in natural language processing (NLP) and various other applications within the domain of GAI.The architectural structure of transformer models, outlined in Figure 4, involves an initial encoding phase where the input sentence is transformed into a sequence of vectors.This encoding process employs a self-attention mechanism that enables the model to capture intricate relationships among the words within the sentence.Subsequently, the encoded input sentence undergoes a decoding phase, generating a sequence of output tokens through another selfattention mechanism.Notably, the attention mechanism plays a pivotal role in enabling transformer models to discern and learn long-range dependencies by prioritizing the most relevant words in the input sentence during the decoding process.The learning process in Transformer models relies on two core techniques, namely self-attention and positional encodings.Self-attention empowers the model to assess the relative importance of different words in a sequence based on contextual factors.Within the self-attention layer, each component of the input is assigned a weight, effectively quantifying its contextual significance.Positional encoding, on the other hand, serves as a representation of the sequential order in which input words appear.For example, in NLP, the self-attention layers allow the model to learn long-range dependencies between words in a sentence.The functionality of selfattention layers involves the computation of scores for each word pair within the sentence which reflects the degree of interrelatedness between them.These computed scores are subsequently utilized to determine a weighted sum of input vectors, serving as the output of the self-attention layer.Concurrently, the positional encoding layer contributes crucial information regarding the position of each word in the sentence.This positional information is particularly significant for capturing long-range dependencies as it enables the model to discern the proximity of words within the sentence.While transformer models exhibit formidable capabilities, it is noteworthy that their training can pose computational challenges due to their inherent complexity.A notable advantage of these models is their ability to process input sequences concurrently, often outperforming RNNs across numerous NLP applications.</p>
<p>Before the ascendancy of transformer-based architectures, the most effective neural NLP models typically relied on supervised learning with extensive volumes of manually labeled data.This reliance on supervised learning posed limitations, particularly concerning datasets lacking adequate annotations, and introduced substantial costs and time requirements for training exceptionally large language models.In transformer models, a semi-supervised approach is adopted to establish a large-scale data generation system.This approach encompasses two pivotal stages: an unsupervised generative "pretraining" phase to initialize parameters through a language modeling objective, and a supervised discriminative stage to fine-tune these parameters for a specific target task.Transformer models can be used to write stories, essays, poems, machine translation, text summarization, question answering and chat with humans [54].Therefore, it can be used as an assistant for teachers and students to implement educational purposes, more efficiently.</p>
<p>Large Language Models</p>
<p>Large Language Models (LLMs) are AI systems rooted in the transformer model architecture, designed to comprehend and generate human language.To unravel the intricacies and inherent structures of language, these models undergo training that involves learning statistical associations among words and phrases within extensive text corpora.In essence, LLMs represent a category of AI algorithms that employ deep learning techniques to undertake a wide array of NLP tasks.These expansive language models rely on transformer architectures and are trained on massive datasets.Their training initially involves the acquisition of data, followed by the application of diverse techniques to deduce relationships and generate fresh content grounded in the acquired knowledge.In sum, LLMs signify the evolution of language models in AI, marked by a substantial enlargement in the volume of data employed for training and inference.Consequently, they bestow remarkable enhancements upon the capabilities of AI models.</p>
<p>LLM's deep learning technique uses a self-attention mechanism to weigh the significance of different words in a sequence, allowing the LLM to manage long-range dependencies between words.Indeed, LLMs use statistical models to analyze vast amounts of data, learning patterns and connections between words and phrases.This allows them to generate new content, such as essays or articles, that can be used for language translation, personalized content creation, and virtual assistants.LLMs are continuously evolving and as the Transformer models become more sophisticated, new applications are going to emerge such as human-computer interaction.These capabilities are beneficial to improve teaching efficiency and expand research opportunities in educational centers.</p>
<p>Bidirectional Encoder Representations from Transformers</p>
<p>Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture.It was introduced in October 2018 by Google and leveraged the attention model to get a deeper understanding of the language context [55].The BERT model makes use of Transformer architecture and unsupervised learning to better comprehend natural language inquiries.It is a stack of many encoder blocks and is trained using the "masked language model" and "next sentence prediction" simultaneously.The input text is separated into tokens as in the transformer model, and each token will be transformed into a vector at the output of BERT.</p>
<p>BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 110 million parameters, and (2) BERTLARGE: 24 encoders with 340 million parameters.Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia (2,500M words).BERT is capable of generating highly realistic and coherent text and performing various natural language processing tasks, such as language translation, text summarization, and question-answering.So, it can be useful to help understand the meaning of ambiguous language in the text, assist students in learning more carefully and teachers to deliver course materials more precisely in educational purposes.</p>
<p>Generative Pre-trained Transformers</p>
<p>Generative Pre-trained Transformer (GPT) models, rooted in the Transformer architecture introduced by OpenAI in 2018, adopt a semi-supervised approach for large-scale data generation.Contrary to the limitations posed by supervised learning in NLP, where reliance on well-labeled datasets proved restrictive and resourceintensive, GPT leverages the semi-supervised method.This involves pre-training on extensive datasets of unlabeled samples that facilitates the generation of nuanced and human-like content.The architectural framework of GPT, depicted in Figure 5, unfolds in two stages: an unsupervised generative 'pretraining' phase establishing initial parameters through a language modeling objective, followed by a supervised 'discriminative' stage that refines these parameters in transformer blocks for a specific task.OpenAI has sequentially released iterations in the 'GPT-n' series, each surpassing its predecessor in capabilities by incorporating more Transformer blocks and trainable parameters.The latest release, GPT-4, debuted in March 2023.These models serve as the foundation for task-specific GPT systems, powering services like the ChatGPT chatbot.</p>
<p>Comparing GPT with BERT, one can say that BERT was a pre-trained transformer (PT) but not designed to be Generative.BERT was an "encoder-only" model, while GPT uses a Transformer Decoder architecture which is essentially a Bidirectional Self-Attentive Model, that uses all the tokens in a sequence to attend each token in that sequence.</p>
<p>Obviously, GPTs are now all-purpose language models capable of writing code, summarizing text, extracting information from documents, and producing original content.Applications with GPT models may produce text and material (including photos, music, and more) that is human-like and can converse with users.Therefore, GPT models and ChatGPT chatbot services would be efficient options in education to assist teachers with course preparation and delivery purposes.It can also be more beneficial for students' engagement to learn and can provide more impressive research opportunities for them.</p>
<p>Application of GAI in Education: Opportunities and Challenges</p>
<p>The GAI is making significant inroads into educational applications [57,58], introducing innovative methods for crafting and delivering academic content to students.Its utility extends to the creation of interactive course materials, including simulation activities tailored to facilitate the comprehension of intricate concepts in fields like science and engineering.For instance, GAI can enhance the understanding of fundamental mathematical concepts such as hyperbolic and conical surfaces, or it can establish dynamic learning environments like flight simulators.Furthermore, GAI can pave the way for personalized learning experiences by generating course content, quizzes, and assessments that adapt to the unique requirements of each student.</p>
<p>In the realm of educational content creation, GAI serves as a potent tool, empowering educators to develop fresh instructional materials.Teachers, for example, can harness generative AI to construct innovative lesson plans, generate custom worksheets, and even produce learning resources such as textbooks.In disciplines such as literature, GAI proves invaluable by generating diverse narratives, analyzing existing texts, and aiding students in comprehending intricate themes [59,60].This multifaceted role of GAI also contributes to the reduction of plagiarism.</p>
<p>Despite the improvement that GAI can initiate in education proficiency in some aspects, several challenges and concerns have emerged in academia, mainly about the assessment of the originality of the students' works.Different countries have started to identify how to deal with this problem.For example, in Australia, the Department of Education of three state governments, i.e.New South Wales, Victoria and Queensland, have announced that they will ban ChatGPT in schools.A similar decision has been taken in New York City public schools [61][62][63].In other words, using GAI is still banned in the education sector of many countries because the concerns have not been addressed yet.Undoubtedly, these policies and regulations will not last forever.One of the main objectives of the education systems, especially in higher education, is to make ready the future workforce.Making students familiar with new technologies, that they will deal with after graduation, should be well covered during their studies.Therefore, the sooner we can bring GAI into the education sector, the better.Here, we review the opportunities and challenges of applying GAI in the education field.</p>
<p>Opportunities</p>
<p>Personalized learning: As we mentioned in previous sections, GAI creates personalized content in response to user-given prompts and based on each person's training curve.It brings the opportunity to move forward from the mass education system to personalized and adaptive learning methods.It can facilitate data analysis and visualization in academic course delivery and training.Therefore, it would lead us to appealing training for both students and teachers.On the other hand, students are normally technology-lovers, and using new technologies, such as GAI, will engage them in educational activities.In addition, teachers can monitor the student progress based on their results and feedback, and then generate the next step of the education based on their learning curves.This would lead to a higher student satisfaction level and greater academic performance.</p>
<p>Improved teaching efficiency:</p>
<p>The GAI can bring a great opportunity for performing a fair and quick assessment process.Generative AI can expedite the evaluation and student marking processes since tests and assignments have already been designed automatically.That means the whole process of designing, running, and marking tests and assignments can be done by GAI, which reduces the workload of academics.It can also provide more appropriate, timely, and precise feedback to students.Furthermore, teachers would have more time to do higher-level activities, such as curriculum design, creating ideas and effective content.</p>
<p>Expanded research opportunities: GAI would Enable the analysis of large datasets and new patterns and accelerate innovation in academic topics and concepts.That means students can work with real-world largescale datasets during their education which makes them better prepared for their future careers compared to the current condition where student activities on mainly based on synthetic data.In addition, GAI would have significant potential support for scientific discovery and enhance the development of new technologies.Therefore, it will conduce to expanded academic research opportunities.</p>
<p>Facilitating education for everyone: Using GAI in the education process provides the opportunity to adjust the learning process for students with different needs.That means fast and slow learners, those with different learning styles such as visual learners, and learners with special considerations will have the opportunity of high-quality learning.Students It leads us to more social equality, especially in education to overcome access barriers and support underserved student populations, efficiently.</p>
<p>Indeed, GAI can provide a paradigm shift from traditional mass education and pedagogies to modern learning.It would customize the learning experience for all learners matching their individual needs; increase accessibility for students with learning disabilities, anxiety, or language barriers; allow instructors to amend iterative learning; navigate students' brainstorming for learning, improve tasks in various domains including writing, coding, critical thinking, teachers course delivery and more.</p>
<p>Challenges</p>
<p>Ethical issues and concerns: Ethical issues and concerns are the main challenges of all AI applications, including GAI, in the education sector.Plagiarism may happen in the form of copying the assessment answers exactly from ChatGPT without any critical thinking and creativity, and creating fake reports and research results are some important concerns of emerging GAI in education.Simply, the assessor is not sure if the answers to the assessment have been provided by the student himself/herself, or have been generated by a GAI.</p>
<p>Limited human interaction: In addition to knowledge and skills, many so-called soft skills are developed in the students during the education process.These skills are normally developed through human interactions, for example when students work in groups to solve a problem.These interactions may be disrupted in the era of GAI since students can achieve the result by asking questions from GAI instead of performing teamwork.In addition, human emotional needs are not considered in learning through GAI.This issue may have a negative effect on student motivation and engagement, and even on their ordinary life.</p>
<p>Undeveloped technical skills: Due to generating enormous and beneficial data in many cases, more students and teachers are going to rely on GAI tools instead of their knowledge and creativity.In the long term, it could lead to Less critical thinking and creativity in academic and training centers.In the worst case, students may be dependent on GAI dialogues and chatbots such that they cannot deal with even a simple question in the field!The challenge becomes more severe in preparing future researchers since conducting research individually has been always a requirement, especially for doctorate programs.</p>
<p>Infrastructure challenges: Some technical challenges are clearly expected due to the required infrastructure and investments.In addition, we will have some potential difficulties with system compatibility and keeping it up to date with frequent technological advancements.Furthermore, we should consider some policies and efficient efforts for upskilling and training of educators with the GAI latest versions which are going to be used in education systems.</p>
<p>To the best of the authors' knowledge, there has been no comprehensive analysis to quantify the pros and cons of using GAI in the education sector so far.Recent research has investigated how GAI tools of ChatGPT and Midjounery affect school education [64].In this case study, teachers and leaders of different backgrounds were involved from 88 schools.This study could be the first empirical study to understand the impact of GAI on school education from the perspectives of teachers' and leaders.They completed a survey and joined a focus group to share GAI effect on education.Thematic analysis identified four main themes and 12 subthemes.The findings provide a comprehensive overview of the use of GAI across the four key educational domains, i.e. teaching, learning, assessment, and administration.The inter-rater reliability of this case investigation was 0.82 indicating suitable reliability of the research outcomes in spite of different prerequisite knowledge for students, teachers and administrators in the four domains.</p>
<p>Overall, it is clear that the journey towards Academic delivery in the era of Generative AI is going to be started very soon and we should prepare and adapt ourselves to it.We should capture the benefits of ChatGPT for education and, in parallel, implement strategies to mitigate and harness the drawbacks ahead.This issue requires our collaboration as teachers and researchers to implement Technological GAI solutions whenever they are needed and monitor the potential risks ahead.Undoubtedly, together, we will commit to continuous improvement and embrace the transformative power of generative AI for education and delivering academic materials, more efficiently and pave the way for a brighter future.</p>
<p>Future Research Direction</p>
<p>To address the aforementioned challenges, it is imperative to implement policies and instructions aimed at mitigating the drawbacks and leveraging the benefits of GAI within an efficient educational system in the future.Instructors play a pivotal role in this regard by educating students about the potential pitfalls of GAI technology and instilling a cautious approach when utilizing it.A paradigm shift necessitates a reevaluation and clear communication of definitions and policies of academic misconduct.Expectations concerning the permissible use of GAI in assignments and classes, including specific guidelines for its allowance or prohibition, require careful revision.Furthermore, proper attribution of GAI-generated content becomes paramount when its usage is permitted.Students need to cultivate a sense of responsibility for the accuracy and appropriate attribution of content generated by GAI, and instructors should emphasize the importance of validating information.While GAI can be a valuable tool, it is essential to recognize that foundational skills must still be developed independently to prevent overreliance on GAI.</p>
<p>Translating research achievements on GAI into real-world solutions has offered innovative products in the education sector and the trend is expected to boom in the near future.Here, we express insights into future research directions, emphasizing on adaptational techniques for educational institutions and recommend some policies for them.</p>
<p>Developing adaptive and personalized learning: A major pathway of advancement in using GAI in education is to tailor the content and activities considering different conditions of students.The revolution has started.For instance, DreamBox Learning, a company based in the United States, utilizes GAI to customize math lessons for students across various proficiency levels.Carnegie Learning (https://www.carnegielearning.com) is an innovative K-12 education technology and curriculum solutions provider, recognized for employing AI technology to deliver personalized math learning experiences that adapt to individual learning styles and paces, ensuring student success.Adaptive textbooks, i.e. books whose content dynamically adjusts based on the learning pace and understanding of the student, and interactive practice problems that are tailored to fill the student's gap in knowledge or skill based on his progress record, are two ideas in the field.Automated content generation can help instructors create personalized slides, lecture notes, and handouts based on course objectives and student needs, saving valuable time and effort.</p>
<p>Engaging and interactive course delivery: GAIs can revolutionize the way that complex concepts are taught.GAI-driven simulations and virtual labs can immerse students in interactive learning environments and foster deeper understanding.Virtual mentors can guide students through the curriculum, answer questions, and provide personalized feedback.In disciplines such as medical sciences, where education is based on samples and data which is difficult to achieve due to ethical or technical challenges, GAIs can generate synthetic images, very close to real-world, see an example for brain imaging in [65].GAIs can engage the students in the learning process by generating innovative activities, such as games or storytelling, based on the student's mood and feedback.</p>
<p>GAIs can improve the soft skills of the students such as critical thinking and creativity.A GAI-based brainstorming and collaboration tool helps students generate ideas, explore different perspectives, and co-create projects.It can also create realistic and complex scenarios for fast-learning students to tackle, requiring them to apply their knowledge, develop critical thinking skills, and find creative solutions.</p>
<p>Assessment: Although the emergence of AI has raised challenges in the assessment in the education sector, GAIs can still enhance the quality of assessments by addressing various limitations of traditional methods and creating new possibilities.GAIs can create personalized question sets based on individual student knowledge gaps, strengths, and learning pace.In this way, the student's progress can be assessed more frequently than the traditional teacher-based method, and continuous feedback can be provided.It can automate the scoring of objective questions and generate detailed reports on student performance which significantly saves instructors time and effort, reduces human error, and improves fairness.In terms of accessibility and language support, GAIs can translate questions and provide personalized accommodations for students with disabilities or language barriers to ensure equal access to assessments.Personalized questions for each student can also reduce the chance of academic misconduct in technology-based examinations.</p>
<p>Conclusion</p>
<p>In this article, we briefly reviewed the challenges and opportunities that GAI can bring to the education sector.We first went through the history of AI and the time scale of developing the GAI.Different GAI models were also reviewed and some applications of them were addressed.We then went through different challenges and opportunities that GAI could bring to the education sector.Personalized learning, improved teaching efficiency, expanded research opportunities, and facilitating "education for everyone" were considered as main opportunities.On the other hand, major concerns on ethical issues, limited human interaction, undeveloped technical skills, and infrastructure challenges were also reviewed.Obviously, the era of Generative AI is going to be started very soon and educational institutions and academic environments should adapt the education systems and course materials delivery to this technology.To achieve this goal, they should consider priorities in their policies to extract more benefits and harness the potential drawbacks, simultaneously.In this direction, collaborating teachers and researchers to implement technological GAI solutions, monitoring and controlling pitfalls like cheating, less critical thinking and limited human interaction would be suitable priority policies in the education centers.</p>
<p>Figure 1 .
1
Figure 1.Development of GAIDuring 2021-2022, the release of DALL-E, a pixel generative model based on transformers, marked a significant development, followed by the introduction of Midjourney and Stable Diffusion technologies.These innovations signaled the emergence of practical high-quality AI capable of generating content from natural language prompts.Building upon the commercial success of ChatGPT, Google recently unveiled an experimental service known as Bard.Bard exemplifies AI's potential to revolutionize search functionality by delivering more precise responses to user queries[31].Simultaneously, Microsoft introduced a new Bingpowered version powered by ChatGPT during the same timeframe.This version is designed to provide users with search results that are not only more accurate but also timely.Unlike the conventional ladder approach to search results, ChatGPT employs a different methodology.It accepts user queries, conducts a thorough search for answers, and subsequently provides responses that include citations to the sources.</p>
<p>Figure 2 .
2
Figure 2. Development in AI categorization from 2022[33] to 2023[34].</p>
<p>Figure 3 .
3
Figure 3. Recovery of a damaged image using GANs [40].</p>
<p>Figure 4 .
4
Figure 4. Architecture of the transformer model [54].</p>
<p>Figure 5 .
5
Figure 5. GPT's transformer model-based architecture[56]</p>
<p>Conflict of interestThere is no conflict of interest for this study.
Designing, developing, and deploying artificial intelligence sys tems: Lessons from and for the public sector. K C Desouza, G S Dawson, D Chenok, 10.1016/j.bushor.2019.11.004Bus. Horizons. 632019</p>
<p>Artificial intelligence and innovation management: A r eview, framework, and research agenda. N Haefner, J Wincent, V Parida, O Gassmann, 10.1016/j.techfore.2020.120392Technol. Forecast. Soc. Chang. 1621203922020</p>
<p>Innovation and Design in the Age of Artificial Intelligence. R Verganti, L Vendraminelli, M Iansiti, 10.1111/jpim.12523J. Prod. Innov. Manag. 372020</p>
<p>Artificial intelligence-driven innovation for enhancing supply chain resilience and performance under the effect of supply chain dynamism: an empiric al investigation. A Belhadi, V Mani, S S Kamble, S A R Khan, S Verma, 10.1007/s10479-021-03956-xAnn. Oper. Res. 2021</p>
<p>Artificial intelligence and business models in the sustai nable development goals perspective: A systematic literature review. Di Vaio, A Palladino, R Hassan, R Escobar, O , 10.1016/j.jbusres.2020.08.019J. Bus. Res. 1212020</p>
<p>Top Strategic Technology Trends for 2022: Generative AI, Gartner Research. Available online. 18 October 2021</p>
<p>A review of opportunities and challenges of chatbots in education. G.-J Hwang, C.-Y Chang, 10.1080/10494820.2021.195261531Interact. L earn. Environ. 2021</p>
<p>Here's how professionals in 3 different fields are using ChatGPT for work. February 2023</p>
<p>ChatGPT for (Finance) research: The Bananarama Conjecture. M Dowling, B Lucey, 10.1016/j.frl.2023.103662Finance Res. Lett. 532023</p>
<p>Benchmark Expected to Join Generative AI Rush with Deal for Startup LangChain. 13 February 2023</p>
<p>What Is Generative AI and Why Is It Keeping Google, Microsoft &amp; Meta on Their Toes? Available onlin e. February 2023</p>
<p>What AI Can and Can't Do (Yet) for Your Business. M Chui, J Manyika, M Miremadi, January 2018</p>
<p>McKinsey Technology Trends Outlook. M Chui, R Roberts, L Yee, 2023. 20 July 20 23</p>
<p>Generative AI and the future of edu cation: Ragnarök or reformation? A paradoxical perspective from management educators. W M Lim, A Gunasekara, J L Pallant, J I Pallant, E Pechenkina, 10.1016/j.ijme.2023.100790Int. J. Manag. E duc. 212023</p>
<p>Generative AI and the future of education, UNESCO Digital Library. S Giannini, 1 July 2023</p>
<p>Generative Artificial Intelligence in Education and Teaching. Available online. June 2023</p>
<p>An Executive's Guide to AI. 2 January 2024</p>
<p>Stanford Researcher Examines Earliest Concepts of Artificial Intelligence. Robots in Ancient Myths. Avai lable online. 28 February 2019</p>
<p>. What Is Artificial Intelligence. January 2024Available online</p>
<p>Neural Networks, MIT News. Available online. 14 April 2017</p>
<p>Hierarchical Model for Learning Natural Scene Categories. F.-F Li, P Perona, Bayesian, 10.1109/CVPR.2005.16Proceeding s of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). eeding s of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)CA, USAJune 2005S an Diego</p>
<p>Machine learning: Discriminative and generative. M Meila, 10.1007/bf02987011Math. Intell. 282006</p>
<p>Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, Y Bengio, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsMontreal, QC, Canada2014. 8-13 December 2014</p>
<p>J M Tomczak, 10.1007/978-3-030-93158-2Deep Generative Modeling. Cham, SwitzerlandSpringer2022197</p>
<p>A Review of Generative AI from Historical Perspectives. D Dasgupta, D Venugopal, K D Gupta, 10.36227/techrxiv.22097942.v1Tech Rxiv. 2023</p>
<p>Openai/Finetune-transformer-lm. February 2017</p>
<p>Language models are unsupervised mu ltitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, ID: 16002553318 July 2023</p>
<p>Wikipedia's Moment of Truth -Can the online encyclopedia help teach A.I. chatbots to get thei r facts right without destroying itself in the process?. J Gertner, 18 July 2023Available online</p>
<p>ChatGPT sets record for fastest-growing user base -analyst note. H Krystal, February 2023</p>
<p>ChatGPT creator OpenAI is in talks to sell shares in a tender offer that would double the start up's valuation to $29 billion. L Varanasi, January 2023penai-is-in-talks-to-sell-shares-in-a-tender-offer-that-would-double-the-startups-valuation-to</p>
<p>As ChatGPT hype hits fever pitch, Neeva launches its generative AI search engine international ly. P Sawers, 13 February 2023</p>
<p>How to Bell the Cat? A Theoretical Review of Generative Artificial Intel ligence towards Digital Disruption in All Walks of Life. S Mondal, S Das, V G Vrana, 10.3390/technologies11020044202311Technologies</p>
<p>Simplifying the difference: machine learning vs deep learning deep. 16 January 2022</p>
<p>Unlocking the Power of Generative AI Models and Systems such as GPT-4 and ChatGPT for Higher Education: A guide for students and lecturers. H Gimpel, K Hall, S Decker, T Eymann, L Lämmermann, A Mädche, R Maximilian, R Caroline, S Manfred, S Mareike, 10.13140/RG.2.2.20710.09287/22023</p>
<p>Artificial Intelligence Glossary: Neural Networks and Other Terms Explained. A Pasick, 27 March 2023</p>
<p>OpenAI Plans to Up the Ante in Tech's A.I. M Cade, 14 March 2023</p>
<p>K Roose, Coming, Out Party for Generative A.I. Available online. 21 October 2023</p>
<p>A comprehensive survey of ai-generated conte nt (aigc): A history of generative ai from gan to chatgpt. Y Cao, S Li, Y Liu, Z Yan, Y Dai, P S Yu, L Sun, 10.48550/arXiv.2303.042262023arXiv preprint</p>
<p>Progressive growing of gans for improved quality, stability, and variation. T Karras, T Aila, S Laine, J Lehtinen, 10.48550/arXiv.1710.101962017arXiv preprint</p>
<p>Face Inpainting via Nested Generative Adversarial Ne tworks. Z Li, H Zhu, L Cao, L Jiao, Y Zhong, A Ma, 10.1109/access.2019.2949614IEEE Access. 72019</p>
<p>Extracting and composing robust features with de noising autoencoders. P Vincent, H Larochelle, Y Bengio, P A Manzagol, 10.1145/1390156.1390294Proceedings of the 25th International Conference on Machine Learning (ICML'0 8). the 25th International Conference on Machine Learning (ICML'0 8)Helsinki, Finland5-9 July 2008</p>
<p>Unsupervised Anomaly Detecti on with Generative Adversarial Networks to Guide Marker Discovery. T Schlegl, P Seeböck, S M Waldstein, U Schmidt-Erfurth, G Langs, 10.1007/978-3-319-59050-9_12Proceedings of IPMI 2017: Infor mation Processing in Medical Imaging. IPMI 2017: Infor mation Processing in Medical ImagingBoone, NC, USAJune 2017</p>
<p>Autoencoder for words. C.-Y Liou, W.-C Cheng, J.-W Liou, D.-R Liou, 10.1016/j.neucom.2013.09.055Neurocomputing. 1392014</p>
<p>A comprehensive survey on design and application of autoencoder in deep learning. A ppl. Soft Comput. P Li, Y Pei, J Li, 10.1016/j.asoc.2023.1101762023138</p>
<p>Analysis of financial time series. R S Tsay, 2010John Wiley &amp; SonsHoboken, NJ, USA3rd ed.</p>
<p>Predictions of Freeway Traffic Speeds and Volumes Using Vector Autoregres sive Models. S R Chandra, H Al-Deek, 10.1080/15472450902858368J. Intell. Transp. Syst. 132009</p>
<p>Introduction to time series analysis and forecasting. D C Montgomery, C L Jennings, M Kulahci, 2015John Wiley &amp; SonsHoboken, NJ, USA2nd e d</p>
<p>Experimental Data and Geometric Analysis Repository-EDGAR. K Aras, W Good, J Tate, B Burton, D Brooks, J Coll-Font, O Doessel, W Schulze, D Potyagaylo, L Wang, 10.1016/j.jelectrocard.2015.08.008J. Electrocardiol. 2015</p>
<p>RAVE: A variational autoencoder for fast and high-quality neural audio synthesis, ar Xiv preprint. A Caillon, P Esling, 10.48550/arXiv.2111.050112021</p>
<p>Semisupervised text classification by variational autoencoder. W Xu, Y Tan, 10.1109/TNNLS.2019.2900734IEEE Trans. Neural Netw. Learn. Syst. 312019</p>
<p>A Survey on Variational Autoencoders from a Green AI Pe rspective. A Asperti, D Evangelista, E L Piccolomini, 10.1007/s42979-021-00702-9SN Comput. Sci. 2021</p>
<p>On the Design Fundamentals of Diffusion Models: A Survey. Z Chang, G A Koulieris, H P Shum, 10.48550/arXiv.2306.045422023</p>
<p>Toward Data-Driven Weather and Climate Forecasting: Approximating a Simple General Circul ation Model With Deep Learning. S Scher, 10.1029/2018gl080704Geophys. Res. Lett. 452018</p>
<p>Transformer Models: The future of Natural Language Processing, data science for everyone. A Saleem, August 2023</p>
<p>Pre-training of deep bidirectional transformers for l anguage understanding. J Devlin, M W Chang, K Lee, K Toutanova, Bert, 10.48550/arXiv.1810.048052018arXiv preprint</p>
<p>The full architecture of a generative pre-trained transformer (GPT) model. 2 January 2023</p>
<p>Unlocking the Power of ChatGPT: A Framework for Applying Generative AI i n Education. Weipengyang Jiahongsu, 10.1177/20965311231168423ECNU Rev. Educ. 62023</p>
<p>Speculative futures on ChatGPT and generative artificial intelligence (AI): A collective reflection from the educational landscape. A Bozkurt, J Xiao, S Lambert, A Pazurek, H Crompton, S Koseoglu, R Farrow, M Bond, C Neran Tzi, S Honeychurch, Asian J. Distance Educ. 182023</p>
<p>Empowering Educati on with Generative Artificial Intelligence Tools: Approach with an Instructional Design Matrix. L I Ruiz-Rojas, P Acosta-Vargas, J De-Moreta-Llovet, M Gonzalez-Rodriguez, 10.3390/su151511524Sustainabi lity 2023. 1511524</p>
<p>Education in the Era of Generative Artificial Intelligence (AI): Understa nding the Potential Benefits of ChatGPT in Promoting Teaching and Learning. D Baidoo-Anu, L Owusu Ansah, Journal of AI 2023, Availa ble online. 13 April 2023</p>
<p>Queensland to join NSW in banning access to ChatGPT in state schools. 23 Janua ry 2023</p>
<p>ChatGPT banned in New York City public schools over concerns about cheating, learning deve lopment. A Lukpat, January 2023</p>
<p>AI tool banned in Victorian state schools. C Jaeger, 1 February 2023</p>
<p>The impact of Generative AI (GenAI) on practices, policies and research direction in educati on: a case of ChatGPT and Midjourney. T K F Chiu, 10.1080/10494820.2023.2253861Interact. Learn. Environ. 2023</p>
<p>Generative Adversarial N etworks in Brain Imaging: A Narrative Review. M E Laino, P Cancian, L S Politi, M G Della Porta, L Saba, V Savevski, 10.3390/jimaging8040083J. Imaging. 82022</p>            </div>
        </div>

    </div>
</body>
</html>
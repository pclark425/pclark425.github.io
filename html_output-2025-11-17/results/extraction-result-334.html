<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-334 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-334</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-334</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-0427110f0e79f41e69a8eb00a3ec8868bac26a4f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f" target="_blank">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset and finds this model excels on questions that require numerical reasoning, i.e., it already captures numeracy.</p>
                <p><strong>Paper Abstract:</strong> The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e334.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e334.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerically-augmented QANet (NAQANet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A QANet-based reading-comprehension model augmented with output branches for passage span, question span, count, and addition/subtraction answers; uses concatenated GloVe + Char-CNN token representations and is trained end-to-end on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>QANet-based reading comprehension model with four answer-type output branches (passage span, question span, count, add/subtract)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>comparative (binary and non-binary comparisons), list maximum/minimum (argmax/argmin on lists), addition/subtraction (answer branch), counting (count answer branch)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>numbers as found in DROP (mostly small integers), stress tests include adding/multiplying paragraph numbers by random ints from ranges like [1,20], [21,100], multiply [2,10], [11,100]; digit-to-word conversion tested for [0,100]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuned end-to-end on DROP QA supervision using GloVe + Char-CNN embeddings; stress tests by programmatically adding/multiplying numbers and converting digits to words; data-augmentation experiments duplicating examples with modified numbers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On DROP validation: overall F1 49.2. Comparative binary (either-or) F1 = 89.0; non-binary comparative F1 ≈ 49.8; superlative number answers F1 = 69.2; superlative span answers F1 = 66.3. Stress tests: small changes (Add [1,20]) drop overall F1 by 1.5 to 47.7; larger changes (Add [21,100]) overall F1 41.4 (-7.8); Multiply [11,100] overall F1 38.8 (-10.4) and superlative F1 drops 35.7 points to 32.0. Digit→word conversions degrade performance more when larger numbers converted (e.g., Digits→Words [21,100] causes ~21.6 F1 drop on superlatives).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Numerical capability appears to emerge from the token embeddings (GloVe + Char-CNN) together with the model learning comparison algorithms end-to-end from QA supervision; no explicit symbolic number module is used. Failures to handle out-of-range numbers are linked to limitations in learned representations/probing generalization rather than explicit algorithmic components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Strong performance for numbers within the training distribution; accuracy degrades substantially as number magnitude or distribution shifts outside training range; data augmentation (modifying numbers in training) can partially recover extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severely degraded performance on numbers scaled up beyond training range (large multiplications), and on larger word-form numbers; struggles to extrapolate to unseen magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across question types (comparative, superlative, full validation) and human performance; ablation removing GloVe (leaving Char-CNN) reported; stress tests compare original vs modified-paragraph performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A reading-comprehension model (NAQANet) can learn to perform numerical comparisons and argmax-style reasoning using standard embeddings (GloVe + Char-CNN) from QA supervision, but it fails to extrapolate reliably to numbers outside its training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e334.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GloVe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GloVe word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained 300-dimensional static word vectors; when probed, they encode numeric magnitude information allowing downstream probes to decode and compare numeric tokens within a trained range.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GloVe: Global vectors for word representation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GloVe</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>300-d</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>static word vectors (global co-occurrence based)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding numeric magnitude (regression), list maximum (classification/argmax), addition (via probe network)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested on integer ranges [0,99], [0,999], [0,9999]; also integers and negatives (e.g., [-50,50]); word-form numbers not always in-vocab for large ranges</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fixed pre-trained embeddings probed with probing networks: LSTM+softmax for list-maximum, linear/MLP regressors for decoding and addition; trained on 80% of integers in a range and tested on held-out 20% (interpolation); extrapolation experiments also performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.90 ([0,99]), 0.78 ([0,999]), 0.72 ([0,9999]). Decoding RMSE = 2.23, 13.77, 174.21 for [0,99], [0,999], [0,9999]. Addition RMSE = 0.80, 16.51, 180.31 for those ranges. Degrades with larger ranges; poor extrapolation beyond training range.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Numeric magnitude information is present in the static word embedding space — for small ranges this appears capturable by a linear subspace; GloVe's distributional training unexpectedly encodes fine-grained numeric order/magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degrades as the numeric range increases (RMSE increases, classification accuracy drops); linear decoders work well for small ranges but fail for larger ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High RMSE and low accuracy for very large ranges; poor extrapolation to numbers outside training ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to word2vec, ELMo, BERT, char-level learned embeddings, and untrained baselines in same probing setups.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GloVe embeddings naturally contain information about numeric magnitude sufficient for interpolation-level decoding and comparisons, but accuracy falls as numeric range grows and extrapolation fails.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e334.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>word2vec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>word2vec embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained static word vectors (skip-gram/CBOW family); when probed, they encode numeric ordering/magnitude enabling decoding and comparison within trained ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Advances in pre-training distributed word representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>word2vec</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>static word vectors (skip-gram/CBOW)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding magnitude, list maximum (argmax), addition (via probe)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested on integer ranges [0,99], [0,999], [0,9999]; negatives in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fixed embeddings probed with same probing networks as for GloVe; 80/20 train/test split over integers in a range</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.90 ([0,99]), 0.78 ([0,999]), 0.71 ([0,9999]). Decoding RMSE = 2.34, 18.77, 333.47. Addition RMSE = 0.75, 21.23, 210.07. Degrades with larger ranges; varied extrapolation failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Distributional training encodes numeric structure sufficiently for probes to learn decoding/comparison in-range, though less reliably than char-level methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Degrades substantially with range size; exhibits different extrapolation behaviors (e.g., monotonic decrease in predicted value as input increases in one observed case).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High decoding/addition RMSE at large ranges and poor extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GloVe, ELMo, BERT, learned char-level embeddings, and random/untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>word2vec captures numeracy sufficient for interpolation-level numeric tasks but suffers similar range-scaling and extrapolation limitations as other static word vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e334.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ELMo (Deep contextualized word representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contextualized embedding model using character-level convolutions followed by LSTMs; when probed, ELMo encodes numeric magnitude strongly and is among the best pre-trained methods for numeracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep contextualized word representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>contextualized embeddings (Char-CNN + biLSTM stacks)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding magnitude, list maximum (argmax), addition (via probe)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested on ranges [0,99], [0,999], [0,9999], floats, and negatives</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fixed pre-trained ELMo embeddings probed with LSTM/MLP probes; inputs provided as digits, word-forms, floats, negatives where appropriate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.98 ([0,99]), 0.88 ([0,999]), 0.76 ([0,9999]). Decoding RMSE = 2.35, 13.48, 62.20. Addition RMSE = 0.94, 15.50, 45.71. Extrapolation for list max is relatively stronger (e.g., ELMo list-max accuracy 0.65 on [151,160] after training on [0,150]).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Character-level convolutional component (Char-CNN) in ELMo provides a strong architectural prior for encoding numeric form and magnitude; a linear subspace suffices for small ranges. ELMo's character-based front-end likely explains its superior numeracy over subword models like BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>High interpolation accuracy for small and medium ranges; decoding/addition RMSE grows with range but less severely than sub-word BERT; better extrapolation than many pre-trained token vector baselines on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still struggles on heavy extrapolation beyond training range for regression tasks (decoding/addition), though less so on list-maximum than purely lexical vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly against GloVe, word2vec, BERT, learned Char-CNN/Char-LSTM, and untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>ELMo's character-level representation captures numeracy particularly well, enabling strong decoding and comparison within training ranges and relatively better extrapolation on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e334.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained transformer producing subword-piece contextualized embeddings; when probed, BERT encodes some numeracy but is less precise than character-level methods, especially as numeric range increases or with floats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (base, uncased)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>bidirectional Transformer encoder (sub-word piece tokenization, 30k pieces for BERT-base uncased used here)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding magnitude, list maximum (argmax), addition (via probe)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested on integer ranges [0,99], [0,999], [0,9999]; floats and negatives were also evaluated (BERT struggled more on floats)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fixed pre-trained BERT token representations used as inputs to the same probing architectures (LSTM/MLP); numbers tokenized into WordPiece pieces; interpolation and extrapolation experiments performed</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.95 ([0,99]), 0.62 ([0,999]), 0.52 ([0,9999]). Decoding RMSE = 3.21, 29.00, 431.78. Addition RMSE = 4.56, 67.81, 454.78. Substantially worse than char-level learned embeddings and ELMo on large ranges; poor decoding/addition RMSE at large ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Sub-word tokenization fragments digit strings into inconsistent pieces, making similar numeric values map to different token sequences and harming numeric magnitude encoding. BERT representations therefore capture less reliable numeric magnitude, especially for floats and large integers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performs acceptably on very small ranges but accuracy and regression quality rapidly degrade with increasing numeric range; extrapolation is weak.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Particular weakness on floats and large integers due to subword splitting; very high RMSE and low list-max accuracy for large ranges; poor extrapolation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to ELMo, GloVe, word2vec, learned Char-CNN/Char-LSTM, and untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>BERT's subword tokenization hampers encoding of numeric magnitude compared to character-level encodings, leading to poorer arithmetic decoding and weak extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e334.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Char-CNN (learned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned character-level convolutional embedding (Char-CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned character-level CNN embedding trained from scratch on the probing tasks; shows the strongest numeracy across interpolation and better extrapolation on many tasks, indicating the power of character-level priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Char-CNN (learned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>character-level convolutional neural network producing token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding magnitude, list maximum (argmax), addition (via probe)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested across [0,99], [0,999], [0,9999] integer ranges, floats, negatives; also extrapolation test ranges like [151,160] after training on [0,150]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>learned end-to-end jointly with probing models on synthetic tasks; left character padding used to improve numeracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.97 ([0,99]), 0.93 ([0,999]), 0.88 ([0,9999]). Decoding RMSE = 2.50, 4.92, 11.57. Addition RMSE = 1.19, 7.75, 15.09. Extrapolation on list-maximum: high accuracies (e.g., Char-CNN list-max ~0.81–0.75 on [151,160]/[151,180]).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Character-level convolutions provide an architectural prior that encodes digit/word morphology and supports fine-grained numeric magnitude encoding; even untrained Char-CNNs provide useful features, suggesting strong inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Best-performing method overall for interpolation across ranges tested; degrades less steeply with range size compared to static word vectors and BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Some degradation for very large ranges and extrapolation on regression tasks still challenging but better than many pre-trained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Char-LSTM, ELMo, BERT, GloVe, word2vec, and untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A learned Char-CNN is the most effective encoder for capturing numeracy in the tested setups, enabling accurate decoding, addition, and list-maximum within and closer to outside training ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e334.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Char-LSTM (learned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned character-level LSTM embedding (Char-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned character-level LSTM embedding trained on probing tasks; provides strong numeracy though slightly weaker than Char-CNN on some large-range tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Char-LSTM (learned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>character-level LSTM producing token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding magnitude, list maximum (argmax), addition (via probe)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested across integer ranges [0,99], [0,999], [0,9999], negatives and floats; extrapolation tests included</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>learned jointly with probing models on synthetic tasks (80/20 train/test splits over numeric ranges)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.98 ([0,99]), 0.92 ([0,999]), 0.76 ([0,9999]). Decoding RMSE = 2.55, 8.65, 18.33. Addition RMSE = 1.21, 15.11, 25.37. Extrapolation on list-maximum: highest accuracies in Table 7 for some test ranges (e.g., 0.88 on [151,160]).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Character-level sequential modeling captures numeric token morphology but differs slightly in inductive bias from CNNs; performs very well and sometimes best in extrapolation for list-max.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Strong for small-to-medium ranges; comparable to Char-CNN though with different strengths (Char-LSTM sometimes excels on extrapolation list-max).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Degradation on very large ranges for regression tasks; not immune to extrapolation failures though competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Char-CNN, ELMo, BERT, static word vectors, and untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Char-LSTM is a robust character-level encoder that captures numeracy well and can outperform many pre-trained embeddings on interpolation and some extrapolation list-max tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e334.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct numeric-value embedding (Value Embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding strategy that maps a numeric token directly to its numeric value (optionally log-scaled); surprisingly difficult to train for large ranges despite providing explicit numeric magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>embedding where token vector is (function of) the numeric value (e.g., scalar or small vector representing the number or log(number))</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>decoding magnitude, list maximum, addition</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>tested on ranges [0,99], [0,999], [0,9999]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>used as direct input embedding to probing models; experimented with raw values, normalized values, and base-10 log scaling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): List maximum accuracy = 0.99 ([0,99]), 0.88 ([0,999]), 0.68 ([0,9999]). Decoding RMSE = 1.20, 11.23, 275.50. Addition RMSE = 0.30, 15.98, 654.33. Performance collapses for very large ranges on regression/addition unless special architectures are used.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Although the embedding directly contains numeric magnitude, training probes for large ranges is unstable/difficult (variance/magnitude issues); log-scaling helps but is not a full solution—architectural changes may be required to reliably exploit explicit numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Very good for small ranges; extreme degradation for decoding/addition in very large ranges due to optimization/variance issues; log transforms improve but don't fully fix.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Training instability and poor regression/addition performance for large numeric ranges, leading to huge RMSE values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to learned char-level embeddings, pre-trained embeddings, and untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Directly giving the numeric value as an embedding seems like an obvious solution but is hard to train for large ranges with standard probes; architecture/hyperparameter choices and transforms (e.g., log) materially affect success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e334.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e334.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random / Untrained baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random token vectors and untrained Char-CNN / Char-LSTM baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines using random static vectors or randomly-initialized but untrained character encoders; they perform poorly but untrained CNNs surprisingly capture some numeracy due to architectural priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Random vectors / Untrained Char-CNN / Untrained Char-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>random static vectors; randomly initialized char-level CNN/LSTM without pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>list maximum, decoding, addition (probed similarly to other embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>evaluated on the same integer ranges [0,99], [0,999], [0,9999]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>used as fixed embeddings for probing models (no pretraining); shows effect of inductive bias separate from learned corpus statistics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Interpolation (Table 4): Random vectors: list-max ~0.16–0.23; decoding RMSE extremely high (e.g., 29.86, 292.88, 2882.62). Untrained CNN surprisingly competitive on some tasks (list-max 0.97, 0.87, 0.84 across ranges; decoding/addition RMSE moderate), indicating strong architectural inductive bias. Untrained LSTM weaker but non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Architectural priors (particularly convolutional structure) can give useful numeric-discriminative features even without corpus-driven training; random CNNs act as useful feature extractors for numeric morphology.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Untrained CNNs give strong interpolation list-max results but still worse than learned Char-CNN; random vectors provide near-chance performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Random vectors fail to generalize; untrained models still struggle on regression/extrapolation for large ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Serves as baseline against pre-trained and learned embeddings in probing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Inductive bias from convolutional character architectures contributes heavily to numeracy encoding — even untrained CNNs can provide useful numeric features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NLP Models Know Numbers? Probing Numeracy in Embeddings', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural arithmetic logic units <em>(Rating: 2)</em></li>
                <li>Numeracy for language models: Evaluating and improving their ability to predict numbers <em>(Rating: 2)</em></li>
                <li>Exploring numeracy in word embeddings <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>On the practical computational power of finite precision rnns for language recognition <em>(Rating: 1)</em></li>
                <li>Order matters: Sequence to sequence for sets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-334",
    "paper_id": "paper-0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "NAQANet",
            "name_full": "Numerically-augmented QANet (NAQANet)",
            "brief_description": "A QANet-based reading-comprehension model augmented with output branches for passage span, question span, count, and addition/subtraction answers; uses concatenated GloVe + Char-CNN token representations and is trained end-to-end on DROP.",
            "citation_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs.",
            "mention_or_use": "use",
            "model_name": "NAQANet",
            "model_size": null,
            "model_architecture": "QANet-based reading comprehension model with four answer-type output branches (passage span, question span, count, add/subtract)",
            "arithmetic_operation_type": "comparative (binary and non-binary comparisons), list maximum/minimum (argmax/argmin on lists), addition/subtraction (answer branch), counting (count answer branch)",
            "number_range_or_complexity": "numbers as found in DROP (mostly small integers), stress tests include adding/multiplying paragraph numbers by random ints from ranges like [1,20], [21,100], multiply [2,10], [11,100]; digit-to-word conversion tested for [0,100]",
            "method_or_intervention": "fine-tuned end-to-end on DROP QA supervision using GloVe + Char-CNN embeddings; stress tests by programmatically adding/multiplying numbers and converting digits to words; data-augmentation experiments duplicating examples with modified numbers",
            "performance_result": "On DROP validation: overall F1 49.2. Comparative binary (either-or) F1 = 89.0; non-binary comparative F1 ≈ 49.8; superlative number answers F1 = 69.2; superlative span answers F1 = 66.3. Stress tests: small changes (Add [1,20]) drop overall F1 by 1.5 to 47.7; larger changes (Add [21,100]) overall F1 41.4 (-7.8); Multiply [11,100] overall F1 38.8 (-10.4) and superlative F1 drops 35.7 points to 32.0. Digit→word conversions degrade performance more when larger numbers converted (e.g., Digits→Words [21,100] causes ~21.6 F1 drop on superlatives).",
            "mechanistic_insight": "Numerical capability appears to emerge from the token embeddings (GloVe + Char-CNN) together with the model learning comparison algorithms end-to-end from QA supervision; no explicit symbolic number module is used. Failures to handle out-of-range numbers are linked to limitations in learned representations/probing generalization rather than explicit algorithmic components.",
            "performance_scaling": "Strong performance for numbers within the training distribution; accuracy degrades substantially as number magnitude or distribution shifts outside training range; data augmentation (modifying numbers in training) can partially recover extrapolation.",
            "failure_modes": "Severely degraded performance on numbers scaled up beyond training range (large multiplications), and on larger word-form numbers; struggles to extrapolate to unseen magnitudes.",
            "comparison_baseline": "Compared across question types (comparative, superlative, full validation) and human performance; ablation removing GloVe (leaving Char-CNN) reported; stress tests compare original vs modified-paragraph performance.",
            "key_finding": "A reading-comprehension model (NAQANet) can learn to perform numerical comparisons and argmax-style reasoning using standard embeddings (GloVe + Char-CNN) from QA supervision, but it fails to extrapolate reliably to numbers outside its training distribution.",
            "uuid": "e334.0",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "GloVe",
            "name_full": "GloVe word embeddings",
            "brief_description": "Pre-trained 300-dimensional static word vectors; when probed, they encode numeric magnitude information allowing downstream probes to decode and compare numeric tokens within a trained range.",
            "citation_title": "GloVe: Global vectors for word representation.",
            "mention_or_use": "use",
            "model_name": "GloVe",
            "model_size": "300-d",
            "model_architecture": "static word vectors (global co-occurrence based)",
            "arithmetic_operation_type": "decoding numeric magnitude (regression), list maximum (classification/argmax), addition (via probe network)",
            "number_range_or_complexity": "tested on integer ranges [0,99], [0,999], [0,9999]; also integers and negatives (e.g., [-50,50]); word-form numbers not always in-vocab for large ranges",
            "method_or_intervention": "fixed pre-trained embeddings probed with probing networks: LSTM+softmax for list-maximum, linear/MLP regressors for decoding and addition; trained on 80% of integers in a range and tested on held-out 20% (interpolation); extrapolation experiments also performed.",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.90 ([0,99]), 0.78 ([0,999]), 0.72 ([0,9999]). Decoding RMSE = 2.23, 13.77, 174.21 for [0,99], [0,999], [0,9999]. Addition RMSE = 0.80, 16.51, 180.31 for those ranges. Degrades with larger ranges; poor extrapolation beyond training range.",
            "mechanistic_insight": "Numeric magnitude information is present in the static word embedding space — for small ranges this appears capturable by a linear subspace; GloVe's distributional training unexpectedly encodes fine-grained numeric order/magnitude.",
            "performance_scaling": "Performance degrades as the numeric range increases (RMSE increases, classification accuracy drops); linear decoders work well for small ranges but fail for larger ranges.",
            "failure_modes": "High RMSE and low accuracy for very large ranges; poor extrapolation to numbers outside training ranges.",
            "comparison_baseline": "Compared to word2vec, ELMo, BERT, char-level learned embeddings, and untrained baselines in same probing setups.",
            "key_finding": "GloVe embeddings naturally contain information about numeric magnitude sufficient for interpolation-level decoding and comparisons, but accuracy falls as numeric range grows and extrapolation fails.",
            "uuid": "e334.1",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "word2vec",
            "name_full": "word2vec embeddings",
            "brief_description": "Pre-trained static word vectors (skip-gram/CBOW family); when probed, they encode numeric ordering/magnitude enabling decoding and comparison within trained ranges.",
            "citation_title": "Advances in pre-training distributed word representations.",
            "mention_or_use": "use",
            "model_name": "word2vec",
            "model_size": null,
            "model_architecture": "static word vectors (skip-gram/CBOW)",
            "arithmetic_operation_type": "decoding magnitude, list maximum (argmax), addition (via probe)",
            "number_range_or_complexity": "tested on integer ranges [0,99], [0,999], [0,9999]; negatives in some experiments",
            "method_or_intervention": "fixed embeddings probed with same probing networks as for GloVe; 80/20 train/test split over integers in a range",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.90 ([0,99]), 0.78 ([0,999]), 0.71 ([0,9999]). Decoding RMSE = 2.34, 18.77, 333.47. Addition RMSE = 0.75, 21.23, 210.07. Degrades with larger ranges; varied extrapolation failure modes.",
            "mechanistic_insight": "Distributional training encodes numeric structure sufficiently for probes to learn decoding/comparison in-range, though less reliably than char-level methods.",
            "performance_scaling": "Degrades substantially with range size; exhibits different extrapolation behaviors (e.g., monotonic decrease in predicted value as input increases in one observed case).",
            "failure_modes": "High decoding/addition RMSE at large ranges and poor extrapolation.",
            "comparison_baseline": "Compared to GloVe, ELMo, BERT, learned char-level embeddings, and random/untrained baselines.",
            "key_finding": "word2vec captures numeracy sufficient for interpolation-level numeric tasks but suffers similar range-scaling and extrapolation limitations as other static word vectors.",
            "uuid": "e334.2",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "ELMo",
            "name_full": "ELMo (Deep contextualized word representations)",
            "brief_description": "A contextualized embedding model using character-level convolutions followed by LSTMs; when probed, ELMo encodes numeric magnitude strongly and is among the best pre-trained methods for numeracy.",
            "citation_title": "Deep contextualized word representations.",
            "mention_or_use": "use",
            "model_name": "ELMo",
            "model_size": null,
            "model_architecture": "contextualized embeddings (Char-CNN + biLSTM stacks)",
            "arithmetic_operation_type": "decoding magnitude, list maximum (argmax), addition (via probe)",
            "number_range_or_complexity": "tested on ranges [0,99], [0,999], [0,9999], floats, and negatives",
            "method_or_intervention": "fixed pre-trained ELMo embeddings probed with LSTM/MLP probes; inputs provided as digits, word-forms, floats, negatives where appropriate",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.98 ([0,99]), 0.88 ([0,999]), 0.76 ([0,9999]). Decoding RMSE = 2.35, 13.48, 62.20. Addition RMSE = 0.94, 15.50, 45.71. Extrapolation for list max is relatively stronger (e.g., ELMo list-max accuracy 0.65 on [151,160] after training on [0,150]).",
            "mechanistic_insight": "Character-level convolutional component (Char-CNN) in ELMo provides a strong architectural prior for encoding numeric form and magnitude; a linear subspace suffices for small ranges. ELMo's character-based front-end likely explains its superior numeracy over subword models like BERT.",
            "performance_scaling": "High interpolation accuracy for small and medium ranges; decoding/addition RMSE grows with range but less severely than sub-word BERT; better extrapolation than many pre-trained token vector baselines on some tasks.",
            "failure_modes": "Still struggles on heavy extrapolation beyond training range for regression tasks (decoding/addition), though less so on list-maximum than purely lexical vectors.",
            "comparison_baseline": "Compared directly against GloVe, word2vec, BERT, learned Char-CNN/Char-LSTM, and untrained baselines.",
            "key_finding": "ELMo's character-level representation captures numeracy particularly well, enabling strong decoding and comparison within training ranges and relatively better extrapolation on some tasks.",
            "uuid": "e334.3",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "A large pre-trained transformer producing subword-piece contextualized embeddings; when probed, BERT encodes some numeracy but is less precise than character-level methods, especially as numeric range increases or with floats.",
            "citation_title": "BERT: pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_name": "BERT (base, uncased)",
            "model_size": null,
            "model_architecture": "bidirectional Transformer encoder (sub-word piece tokenization, 30k pieces for BERT-base uncased used here)",
            "arithmetic_operation_type": "decoding magnitude, list maximum (argmax), addition (via probe)",
            "number_range_or_complexity": "tested on integer ranges [0,99], [0,999], [0,9999]; floats and negatives were also evaluated (BERT struggled more on floats)",
            "method_or_intervention": "fixed pre-trained BERT token representations used as inputs to the same probing architectures (LSTM/MLP); numbers tokenized into WordPiece pieces; interpolation and extrapolation experiments performed",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.95 ([0,99]), 0.62 ([0,999]), 0.52 ([0,9999]). Decoding RMSE = 3.21, 29.00, 431.78. Addition RMSE = 4.56, 67.81, 454.78. Substantially worse than char-level learned embeddings and ELMo on large ranges; poor decoding/addition RMSE at large ranges.",
            "mechanistic_insight": "Sub-word tokenization fragments digit strings into inconsistent pieces, making similar numeric values map to different token sequences and harming numeric magnitude encoding. BERT representations therefore capture less reliable numeric magnitude, especially for floats and large integers.",
            "performance_scaling": "Performs acceptably on very small ranges but accuracy and regression quality rapidly degrade with increasing numeric range; extrapolation is weak.",
            "failure_modes": "Particular weakness on floats and large integers due to subword splitting; very high RMSE and low list-max accuracy for large ranges; poor extrapolation behavior.",
            "comparison_baseline": "Compared to ELMo, GloVe, word2vec, learned Char-CNN/Char-LSTM, and untrained baselines.",
            "key_finding": "BERT's subword tokenization hampers encoding of numeric magnitude compared to character-level encodings, leading to poorer arithmetic decoding and weak extrapolation.",
            "uuid": "e334.4",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Char-CNN (learned)",
            "name_full": "Learned character-level convolutional embedding (Char-CNN)",
            "brief_description": "A learned character-level CNN embedding trained from scratch on the probing tasks; shows the strongest numeracy across interpolation and better extrapolation on many tasks, indicating the power of character-level priors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Char-CNN (learned)",
            "model_size": null,
            "model_architecture": "character-level convolutional neural network producing token embeddings",
            "arithmetic_operation_type": "decoding magnitude, list maximum (argmax), addition (via probe)",
            "number_range_or_complexity": "tested across [0,99], [0,999], [0,9999] integer ranges, floats, negatives; also extrapolation test ranges like [151,160] after training on [0,150]",
            "method_or_intervention": "learned end-to-end jointly with probing models on synthetic tasks; left character padding used to improve numeracy",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.97 ([0,99]), 0.93 ([0,999]), 0.88 ([0,9999]). Decoding RMSE = 2.50, 4.92, 11.57. Addition RMSE = 1.19, 7.75, 15.09. Extrapolation on list-maximum: high accuracies (e.g., Char-CNN list-max ~0.81–0.75 on [151,160]/[151,180]).",
            "mechanistic_insight": "Character-level convolutions provide an architectural prior that encodes digit/word morphology and supports fine-grained numeric magnitude encoding; even untrained Char-CNNs provide useful features, suggesting strong inductive bias.",
            "performance_scaling": "Best-performing method overall for interpolation across ranges tested; degrades less steeply with range size compared to static word vectors and BERT.",
            "failure_modes": "Some degradation for very large ranges and extrapolation on regression tasks still challenging but better than many pre-trained baselines.",
            "comparison_baseline": "Compared to Char-LSTM, ELMo, BERT, GloVe, word2vec, and untrained baselines.",
            "key_finding": "A learned Char-CNN is the most effective encoder for capturing numeracy in the tested setups, enabling accurate decoding, addition, and list-maximum within and closer to outside training ranges.",
            "uuid": "e334.5",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Char-LSTM (learned)",
            "name_full": "Learned character-level LSTM embedding (Char-LSTM)",
            "brief_description": "A learned character-level LSTM embedding trained on probing tasks; provides strong numeracy though slightly weaker than Char-CNN on some large-range tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Char-LSTM (learned)",
            "model_size": null,
            "model_architecture": "character-level LSTM producing token embeddings",
            "arithmetic_operation_type": "decoding magnitude, list maximum (argmax), addition (via probe)",
            "number_range_or_complexity": "tested across integer ranges [0,99], [0,999], [0,9999], negatives and floats; extrapolation tests included",
            "method_or_intervention": "learned jointly with probing models on synthetic tasks (80/20 train/test splits over numeric ranges)",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.98 ([0,99]), 0.92 ([0,999]), 0.76 ([0,9999]). Decoding RMSE = 2.55, 8.65, 18.33. Addition RMSE = 1.21, 15.11, 25.37. Extrapolation on list-maximum: highest accuracies in Table 7 for some test ranges (e.g., 0.88 on [151,160]).",
            "mechanistic_insight": "Character-level sequential modeling captures numeric token morphology but differs slightly in inductive bias from CNNs; performs very well and sometimes best in extrapolation for list-max.",
            "performance_scaling": "Strong for small-to-medium ranges; comparable to Char-CNN though with different strengths (Char-LSTM sometimes excels on extrapolation list-max).",
            "failure_modes": "Degradation on very large ranges for regression tasks; not immune to extrapolation failures though competitive.",
            "comparison_baseline": "Compared to Char-CNN, ELMo, BERT, static word vectors, and untrained baselines.",
            "key_finding": "Char-LSTM is a robust character-level encoder that captures numeracy well and can outperform many pre-trained embeddings on interpolation and some extrapolation list-max tasks.",
            "uuid": "e334.6",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Value Embedding",
            "name_full": "Direct numeric-value embedding (Value Embedding)",
            "brief_description": "An embedding strategy that maps a numeric token directly to its numeric value (optionally log-scaled); surprisingly difficult to train for large ranges despite providing explicit numeric magnitude.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Value Embedding",
            "model_size": null,
            "model_architecture": "embedding where token vector is (function of) the numeric value (e.g., scalar or small vector representing the number or log(number))",
            "arithmetic_operation_type": "decoding magnitude, list maximum, addition",
            "number_range_or_complexity": "tested on ranges [0,99], [0,999], [0,9999]",
            "method_or_intervention": "used as direct input embedding to probing models; experimented with raw values, normalized values, and base-10 log scaling",
            "performance_result": "Interpolation (Table 4): List maximum accuracy = 0.99 ([0,99]), 0.88 ([0,999]), 0.68 ([0,9999]). Decoding RMSE = 1.20, 11.23, 275.50. Addition RMSE = 0.30, 15.98, 654.33. Performance collapses for very large ranges on regression/addition unless special architectures are used.",
            "mechanistic_insight": "Although the embedding directly contains numeric magnitude, training probes for large ranges is unstable/difficult (variance/magnitude issues); log-scaling helps but is not a full solution—architectural changes may be required to reliably exploit explicit numeric values.",
            "performance_scaling": "Very good for small ranges; extreme degradation for decoding/addition in very large ranges due to optimization/variance issues; log transforms improve but don't fully fix.",
            "failure_modes": "Training instability and poor regression/addition performance for large numeric ranges, leading to huge RMSE values.",
            "comparison_baseline": "Compared to learned char-level embeddings, pre-trained embeddings, and untrained baselines.",
            "key_finding": "Directly giving the numeric value as an embedding seems like an obvious solution but is hard to train for large ranges with standard probes; architecture/hyperparameter choices and transforms (e.g., log) materially affect success.",
            "uuid": "e334.7",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Random / Untrained baselines",
            "name_full": "Random token vectors and untrained Char-CNN / Char-LSTM baselines",
            "brief_description": "Baselines using random static vectors or randomly-initialized but untrained character encoders; they perform poorly but untrained CNNs surprisingly capture some numeracy due to architectural priors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Random vectors / Untrained Char-CNN / Untrained Char-LSTM",
            "model_size": null,
            "model_architecture": "random static vectors; randomly initialized char-level CNN/LSTM without pretraining",
            "arithmetic_operation_type": "list maximum, decoding, addition (probed similarly to other embeddings)",
            "number_range_or_complexity": "evaluated on the same integer ranges [0,99], [0,999], [0,9999]",
            "method_or_intervention": "used as fixed embeddings for probing models (no pretraining); shows effect of inductive bias separate from learned corpus statistics",
            "performance_result": "Interpolation (Table 4): Random vectors: list-max ~0.16–0.23; decoding RMSE extremely high (e.g., 29.86, 292.88, 2882.62). Untrained CNN surprisingly competitive on some tasks (list-max 0.97, 0.87, 0.84 across ranges; decoding/addition RMSE moderate), indicating strong architectural inductive bias. Untrained LSTM weaker but non-trivial.",
            "mechanistic_insight": "Architectural priors (particularly convolutional structure) can give useful numeric-discriminative features even without corpus-driven training; random CNNs act as useful feature extractors for numeric morphology.",
            "performance_scaling": "Untrained CNNs give strong interpolation list-max results but still worse than learned Char-CNN; random vectors provide near-chance performance.",
            "failure_modes": "Random vectors fail to generalize; untrained models still struggle on regression/extrapolation for large ranges.",
            "comparison_baseline": "Serves as baseline against pre-trained and learned embeddings in probing tasks.",
            "key_finding": "Inductive bias from convolutional character architectures contributes heavily to numeracy encoding — even untrained CNNs can provide useful numeric features.",
            "uuid": "e334.8",
            "source_info": {
                "paper_title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural arithmetic logic units",
            "rating": 2
        },
        {
            "paper_title": "Numeracy for language models: Evaluating and improving their ability to predict numbers",
            "rating": 2
        },
        {
            "paper_title": "Exploring numeracy in word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "On the practical computational power of finite precision rnns for language recognition",
            "rating": 1
        },
        {
            "paper_title": "Order matters: Sequence to sequence for sets",
            "rating": 1
        }
    ],
    "cost": 0.020111,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do NLP Models Know Numbers? Probing Numeracy in Embeddings</h1>
<p>Eric Wallace ${ }^{<em> 1}$, Yizhong Wang ${ }^{</em> 2}$, Sujian Li ${ }^{2}$, Sameer Singh ${ }^{3}$, Matt Gardner ${ }^{1}$<br>${ }^{1}$ Allen Institute for Artificial Intelligence<br>${ }^{2}$ Peking University<br>${ }^{3}$ University of California, Irvine<br>{ericw, mattg}@allenai.org, {yizhong, lisujian}@pku.edu.cn, sameer@uci.edu</p>
<h4>Abstract</h4>
<p>The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens-they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise-ELMo captures numeracy the best for all pre-trained methods-but BERT, which uses sub-word units, is less exact.</p>
<h2>1 Introduction</h2>
<p>Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2).</p>
<p>The first step in performing numerical reasoning over natural language is numeracy: the abil-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We train a probing model to decode a number from its word embedding over a random $80 \%$ of the integers from $[-500,500]$, e.g., " 71 " $\rightarrow 71.0$. We plot the model's predictions for all numbers from [-2000, 2000]. The model accurately decodes numbers within the training range (in blue), i.e., pre-trained embeddings like GloVe and BERT capture numeracy. However, the probe fails to extrapolate to larger numbers (in red). The Char-CNN (e) and Char-LSTM (f) are trained jointly with the probing model.
ity to understand and work with numbers in either digit or word form (Spithourakis and Riedel, 2018). For example, one must understand that the string " 23 " represents a bigger value than "twentytwo". Once a number's value is (perhaps implicitly) represented, reasoning algorithms can then process the text, e.g., extracting the list of field goals and computing that list's maximum (first question in Figure 2). Learning to reason numerically over paragraphs with only question-answer supervision appears daunting for end-to-end models; our work seeks to understand if and how "out-of-the-box" neural NLP models already learn this.</p>
<p>We begin by analyzing the state-of-the-art NAQANet model (Dua et al., 2019) for DROPtesting it on a subset of questions that evaluate numerical reasoning (Section 2). To our surprise,</p>
<p>the model exhibits excellent numerical reasoning abilities. Amidst reading and comprehending natural language, the model successfully computes list maximums/minimums, extracts superlative entities (argmax reasoning), and compares numerical quantities. For instance, despite NAQANet achieving only 49 F 1 on the entire validation set, it scores 89 F 1 on numerical comparison questions. We also stress test the model by perturbing the validation paragraphs and find one failure mode: the model struggles to extrapolate to numbers outside its training range.</p>
<p>We are especially intrigued by the model's ability to learn numeracy, i.e., how does the model know the value of a number given its embedding? The model uses standard embeddings (GloVe and a Char-CNN) and receives no direct supervision for number magnitude/ordering. To understand how numeracy emerges, we probe token embedding methods (e.g., BERT, GloVe) using synthetic list maximum, number decoding, and addition tasks (Section 3).</p>
<p>We find that all widely-used pre-trained embeddings, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GloVe (Pennington et al., 2014), capture numeracy: number magnitude is present in the embeddings, even for numbers in the thousands. Among all embeddings, characterlevel methods exhibit stronger numeracy than word- and sub-word-level methods (e.g., ELMo excels while BERT struggles), and character-level models learned directly on the synthetic tasks are the strongest overall. Finally, we investigate why NAQANet had trouble extrapolating-was it a failure in the model or the embeddings? We repeat our probing tasks and test for model extrapolation, finding that neural models struggle to predict numbers outside the training range.</p>
<h2>2 Numeracy Case Study: DROP QA</h2>
<p>This section examines the state-of-the-art model for DROP by investigating its accuracy on questions that require numerical reasoning.</p>
<h3>2.1 DROP Dataset</h3>
<p>DROP is a reading comprehension dataset that tests numerical reasoning operations such as counting, sorting, and addition (Dua et al., 2019). The dataset's input-output format is a superset of SQuAD (Rajpurkar et al., 2016): the answers are paragraph spans, as well as question</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Figure 2: Three DROP questions that require numerical reasoning; the state-of-the-art NAQANet answers every question correct. Plausible answer candidates to the questions are underlined and the model's predictions are shown in bold.
spans, number answers (e.g., 35), and dates (e.g., 03/01/2014). The only supervision provided is the question-answer pairs, i.e., a model must learn to reason numerically while simultaneously learning to read and comprehend.</p>
<h3>2.2 NAQANet Model</h3>
<p>Modeling approaches for DROP include both semantic parsing (Krishnamurthy et al., 2017) and reading comprehension (Yu et al., 2018) models. We focus on the latter, specifically on Numerically-augmented QANet (NAQANet), the current state-of-the-art model (Dua et al., 2019). ${ }^{1}$ The model's core structure closely follows QANet (Yu et al., 2018) except that it contains four output branches, one for each of the four answer types (passage span, question span, count answer, or addition/subtraction of numbers.)</p>
<p>Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing number magnitude or performing explicit comparisons. We refer readers to Yu et al. (2018) and Dua et al. (2019) for further details.</p>
<h3>2.3 Comparative and Superlative Questions</h3>
<p>We focus on questions that NAQANet requires numeracy to answer, namely Comparative and Superlative questions. ${ }^{2}$ Comparative questions</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question Type</th>
<th style="text-align: left;">Example</th>
<th style="text-align: left;">Reasoning Required</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Comparative (Binary)</td>
<td style="text-align: left;">Which country is a bigger exporter, Brazil or Uruguay?</td>
<td style="text-align: left;">Binary Comparison</td>
</tr>
<tr>
<td style="text-align: left;">Comparative (Non-binary)</td>
<td style="text-align: left;">Which player had a touchdown longer than 20 yards?</td>
<td style="text-align: left;">Greater Than</td>
</tr>
<tr>
<td style="text-align: left;">Superlative (Number)</td>
<td style="text-align: left;">How many yards was the shortest field goal?</td>
<td style="text-align: left;">List Minimum</td>
</tr>
<tr>
<td style="text-align: left;">Superlative (Span)</td>
<td style="text-align: left;">Who kicked the longest field goal?</td>
<td style="text-align: left;">Argmax</td>
</tr>
</tbody>
</table>
<p>Table 1: We focus on DROP Comparative and Superlative questions which test NAQANet's numeracy.
probe a model's understanding of quantities or events that are "larger", "smaller", or "longer" than others. Certain comparative questions ask about "either-or" relations (e.g., first row of Table 1), which test binary comparison. Other comparative questions require more diverse comparative reasoning, such as greater than relationships (e.g., second row of Table 1).</p>
<p>Superlative questions ask about the "shortest", "largest", or "biggest" quantity in a passage. When the answer type is a number, superlative questions require finding the maximum or minimum of a list (e.g., third row of Table 1). When the answer type is a span, superlative questions usually require an argmax operation, i.e., one must find the superlative action or quantity and then extract the associated entity (e.g., fourth row of Table 1). We filter the validation set to comparative and superlative questions by writing templates to match words in the question.</p>
<h3>2.4 Emergent Numeracy in NAQANet</h3>
<p>NAQANet's accuracy on comparative and superlative questions is significantly higher than its average accuracy on the validation set (Table 2). ${ }^{3}$</p>
<p>NAQANet achieves 89.0 F1 on binary (eitheror) comparative questions, approximately 40 F 1 points higher than the average validation question and within 7 F 1 points of human test performance. The model achieves a lower, but respectable, accuracy on non-binary comparisons. These questions require multiple reasoning steps, e.g., the second question in Table 1 requires (1) extracting all the touchdown distances, (2) finding the distance that is greater than twenty, and (3) selecting the player associated with the touchdown of that distance.</p>
<p>We divide the superlative questions into questions that have number answers and questions with span answers according to the dataset's provided answer type. NAQANet achieves nearly 70 F1 on superlative questions with number answers, i.e., it can compute list maximum and minimums.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Question Type | Count | EM | F1 |
| :-- | :--: | :--: | :--: |
| Human (Test Set) | 9622 | 92.4 | 96.0 |
| Full Validation | 9536 | 46.2 | 49.2 |
| $\quad$ Number Answers | 5842 | 44.3 | 44.4 |
| Comparative | 704 | 73.6 | 76.4 |
| $\quad$ Binary (either-or) | 477 | 86.0 | 89.0 |
| $\quad$ Non-binary | 227 | 47.6 | 49.8 |
| Superlative Questions | 861 | 64.6 | 67.7 |
| $\quad$ Number Answers | 475 | 68.8 | 69.2 |
| $\quad$ Span Answers | 380 | 59.7 | 66.3 |</p>
<p>Table 2: NAQANet achieves higher accuracy on questions that require numerical reasoning (Superlative and Comparative) than on standard validation questions. Human performance is reported from Dua et al. (2019).</p>
<p>The model answers about two-thirds of superlative questions with span answers correctly ( 66.3 F 1 ), i.e., it can perform argmax reasoning.</p>
<p>Figure 2 shows examples of superlative questions answered correctly by NAQANet. The first two questions require computing the maximum/minimum of a list: the model must recognize which digits correspond to field goals and touchdowns passes, and then extract the maximum/minimum of the correct list. The third question requires argmax reasoning: the model must first compute the longest touchdown pass and then find the corresponding receiver "Chaz Schilens".</p>
<h3>2.5 Stress Testing NAQANet's Numeracy</h3>
<p>Just how far does the numeracy of NAQANet go? Here, we stress test the model by automatically modifying DROP validation paragraphs.</p>
<p>We test two phenomena: larger numbers and word-form numbers. For larger numbers, we generate a random positive integer and multiply or add that value to the numbers in each paragraph. For word forms, we replace every digit in the paragraph with its word form (e.g., " 75 " $\rightarrow$ "seventyfive"). Since word-form numbers are usually small in magnitude when they occur in DROP, we perform word replacements for integers in the range $[0,100]$. We guarantee the ground-truth answer is</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Stress Test Dataset</th>
<th style="text-align: left;">All Questions</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Superlative</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">$\Delta$</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: left;">Original Validation Set</td>
<td style="text-align: left;">49.2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">67.7</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Add $[1,20]$</td>
<td style="text-align: left;">47.7</td>
<td style="text-align: left;">-1.5</td>
<td style="text-align: left;">64.1</td>
<td style="text-align: left;">-3.6</td>
</tr>
<tr>
<td style="text-align: left;">Add $[21,100]$</td>
<td style="text-align: left;">41.4</td>
<td style="text-align: left;">-7.8</td>
<td style="text-align: left;">40.4</td>
<td style="text-align: left;">-27.3</td>
</tr>
<tr>
<td style="text-align: left;">Multiply $[2,10]$</td>
<td style="text-align: left;">41.1</td>
<td style="text-align: left;">-8.1</td>
<td style="text-align: left;">39.3</td>
<td style="text-align: left;">-28.4</td>
</tr>
<tr>
<td style="text-align: left;">Multiply $[11,100]$</td>
<td style="text-align: left;">38.8</td>
<td style="text-align: left;">-10.4</td>
<td style="text-align: left;">32.0</td>
<td style="text-align: left;">-35.7</td>
</tr>
<tr>
<td style="text-align: left;">Digits to Words $[0,20]$</td>
<td style="text-align: left;">45.5</td>
<td style="text-align: left;">-3.7</td>
<td style="text-align: left;">63.8</td>
<td style="text-align: left;">-3.9</td>
</tr>
<tr>
<td style="text-align: left;">Digits to Words $[21,100]$</td>
<td style="text-align: left;">41.9</td>
<td style="text-align: left;">-7.3</td>
<td style="text-align: left;">46.1</td>
<td style="text-align: left;">-21.6</td>
</tr>
</tbody>
</table>
<p>Table 3: We stress test NAQANet's numeracy by manipulating the numbers in the validation paragraphs. Add or Multiply $[x, y]$ indicates adding or multiplying all of the numbers in the passage by a random integer in the range $[\mathrm{x}, \mathrm{y}]$. Digits $\rightarrow$ Words $[x, y]$ converts all integers in the passage within the range $[\mathrm{x}, \mathrm{y}]$ to their corresponding word form (e.g., " 75 " $\rightarrow$ "seventy-five").
still valid by only modifying NAQANet's internal representation (Appendix E).</p>
<p>Table 3 shows the results for different paragraph modifications. The model exhibits a tiny degradation in performance for small magnitude changes (e.g., NAQANet drops 1.5 F1 overall for Add [1,20]) but severely struggles on larger changes (e.g., NAQANet drops 35.7 F1 on superlative questions for Multiply [11,200]). Similar trends hold for word forms: the model exhibits small drops in accuracy when converting small numbers to words ( 3.9 degradation on Digits to Words [0,20]) but fails on larger magnitude word forms ( 21.6 F 1 drop over [21,100]). These results show that NAQANet has a strong understanding of numeracy for numbers in the training range, but, the model can fail to extrapolate to other values.</p>
<h3>2.6 Whence this behavior?</h3>
<p>NAQANet exhibits numerical reasoning capabilities that exceed our expectations. What enables this behavior? Aside from reading and comprehending the passage/question, this kind of numerical reasoning requires two components: numeracy (i.e., representing numbers) and comparison algorithms (i.e., computing the maximum of a list).</p>
<p>Although the natural emergence of comparison algorithms is surprising, previous results show neural models are capable of learning to count and sort synthetic lists of scalar values when given explicit supervision (Weiss et al., 2018; Vinyals et al., 2016). NAQANet demonstrates that a model can learn comparison algorithms while simultane-
ously learning to read and comprehend, even with only question-answer supervision.</p>
<p>How, then, does NAQANet know numeracy? The source of numerical information eventually lies in the token embeddings themselves, i.e., the character-level convolutions and GloVe embeddings of the NAQANet model. Therefore, we can understand the source of numeracy by isolating and probing these embeddings.</p>
<h2>3 Probing Numeracy of Embeddings</h2>
<p>We use synthetic numerical tasks to probe the numeracy of token embeddings.</p>
<h3>3.1 Probing Tasks</h3>
<p>We consider three synthetic tasks to evaluate numeracy (Figure 3). Appendix C provides further details on training and evaluation.</p>
<p>List Maximum Given a list of the embeddings for five numbers, the task is to predict the index of the maximum number. Each list consists of values of similar magnitude in order to evaluate fine-grained comparisons (see Appendix C). As in typical span selection models (Seo et al., 2017), an LSTM reads the list of token embeddings, and a weight matrix and softmax function assign a probability to each index using the model's hidden state. We use the negative log-likelihood of the maximum number as the loss function.</p>
<p>Decoding The decoding task probes whether number magnitude is captured (rather than the relative ordering of numbers as in list maximum). Given a number's embedding, the task is to regress to its value, e.g., the embedding for the string "five" has a target of 5.0. We consider a linear regression model and a three-layer fully-connected network with ReLU activations. The models are trained using a mean squared error (MSE) loss.</p>
<p>Addition The addition task requires number manipulation-given the embeddings of two numbers, the task is to predict their sum. Our model concatenates the two token embeddings and feeds the result through a three-layer fullyconnected network with ReLU activations, trained using MSE loss. Unlike the decoding task, the model needs to capture number magnitude internally without direct label supervision.</p>
<p>Training and Evaluation We focus on a numerical interpolation setting (we revisit extrapolation</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Our probing setup. We pass numbers through a pre-trained embedder (e.g., BERT, GloVe) and train a probing model to solve numerical tasks such as finding a list's maximum, decoding a number, or adding two numbers. If the probing model generalizes to held-out numbers, the pre-trained embeddings must contain numerical information. We provide numbers as either words (shown here), digits ("9"), floats ("9.1"), or negatives ("-9").
in Section 3.4): the model is tested on values that are within the training range. We first pick a range (we vary the range in our experiments) and randomly shuffle the integers over it. We then split $80 \%$ of the numbers into a training set and $20 \%$ into a test set. We report the mean and standard deviation across five different random shuffles for a particular range, using the exact same shuffles across all embedding methods.</p>
<p>Numbers are provided as integers ("75"), single-word form ("seventy-five"), floats ("75.1"), or negatives ("-75"). We consider positive numbers less than 100 for word-form numbers to avoid multiple tokens. We report the classification accuracy for the list maximum task ( 5 classes), and the Root Mean Squared Error (RMSE) for decoding and addition. Note that larger ranges will naturally amplify the RMSE error.</p>
<h3>3.2 Embedding Methods</h3>
<p>We evaluate various token embedding methods.
Word Vectors We use 300-dimensional GloVe (Pennington et al., 2014) and word2vec vectors Mikolov et al., 2018). We ensure all values are in-vocabulary for word vectors.
Contextualized Embeddings We use ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) embeddings. ${ }^{4}$ ELMo uses character-level convo-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>lutions of size 1-7 with max pooling. BERT represents tokens via sub-word pieces; we use lowercased BERT-base with 30k pieces.</p>
<p>NAQANet Embeddings We extract the GloVe embeddings and Char-CNN from the NAQANet model trained on DROP. We also consider an ablation that removes the GloVe embeddings.
Learned Embeddings We use a character-level CNN (Char-CNN) and a character-Level LSTM (Char-LSTM). We use left character padding, which greatly improves numeracy for characterlevel CNNs (details in Appendix B).
Untrained Embeddings We consider two untrained baselines. The first baseline is random token vectors, which trivially fail to generalize (there is no pattern between train and test numbers). These embeddings are useful for measuring the improvement of pre-trained embeddings. We also consider a randomly initialized and untrained Char-CNN and Char-LSTM.</p>
<p>Number's Value as Embedding The final embedding method is simple: map a number's embedding directly to its value (e.g., "seventy-five" embeds to [75]). We found this strategy performs poorly for large ranges; using a base-10 logarithmic scale improves performance. We report this as Value Embedding in our results. ${ }^{5}$</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>All pre-trained embeddings (all methods except the Char-CNN and Char-LSTM) are fixed during training. The probing models are trained on the synthetic tasks on top of these embeddings.</p>
<h3>3.3 Results: Embeddings Capture Numeracy</h3>
<p>We find that all pre-trained embeddings contain fine-grained information about number magnitude and order. We first focus on integers (Table 4).</p>
<p>Word Vectors Succeed Both word2vec and GloVe significantly outperform the random vector baseline and are among the strongest methods overall. This is particularly surprising given the training methodology for these embeddings, e.g., a continuous bag of words objective can teach finegrained number magnitude.</p>
<p>Character-level Methods Dominate Models which use character-level information have a clear advantage over word-level models for encoding numbers. This is reflected in our probing results: character-level CNNs are the best architecture for capturing numeracy. For example, the NAQANet model without GloVe (only using its Char-CNN) and ELMo (uses a Char-CNN) are the strongest pre-trained methods, and a learned Char-CNN is the strongest method overall. The strength of the character-level convolutions seems to lie in the architectural prior-an untrained Char-CNN is surprisingly competitive. Similar results have been shown for images (Saxe et al., 2011): random CNNs are powerful feature extractors.</p>
<p>Sub-word Models Struggle BERT struggles for large ranges (e.g., $52 \%$ accuracy for list maximum for $[0,9999])$. We suspect this results from subword pieces being a poor method to encode digits: two numbers which are similar in value can have very different sub-word divisions.</p>
<p>A Linear Subspace Exists For small ranges on the decoding task (e.g., $[0,99]$ ), a linear model is competitive, i.e., a linear subspace captures number magnitude (Appendix D). For larger ranges (e.g., $[0,999]$ ), the linear model's performance degrades, especially for BERT.</p>
<p>Value Embedding Fails The Value Embedding method fails for large ranges. This is surprising as the embedding directly provides a number's value, thus, the synthetic tasks should be easy to solve. However, we had difficulty training models for
large ranges, even when using numerous architecture variants (e.g., tiny networks with 10 hidden units and tanh activations) and hyperparameters. Trask et al. (2018) discuss similar problems and ameliorate them using new neural architectures.</p>
<p>Words, Floats, and Negatives are Captured Finally, we probe the embeddings on word-form numbers, floats, and negatives. We observe similar trends for these inputs as integers: pre-trained models exhibit natural numeracy and learned embeddings are strong (Tables 5, 6, and 10). The ordering of the different embedding methods according to performance is also relatively consistent across the different input types. One notable exception is that BERT struggles on floats, which is likely a result of its sub-word pieces. We do not test word2vec and GloVe on floats/negatives because they are out-of-vocabulary.</p>
<h3>3.4 Probing Models Struggle to Extrapolate</h3>
<p>Thus far, our synthetic experiments evaluate on held-out values within the same range as the training data (i.e., numerical interpolation). In Section 2.5, we found that NAQANet struggles to extrapolate to values outside the training range. Is this an idiosyncrasy of NAQANet or is it a more general problem? We investigate this using a numerical extrapolation setting: we train models on a specific integer range and test them on values greater than the largest training number and smaller than the smallest training number.</p>
<p>Extrapolation for Decoding and Addition For decoding and addition, models struggle to extrapolate. Figure 1 shows the predictions for models trained on $80 \%$ of the values from [-500,500] and tested on held-out numbers in the range [2000, 2000] for six embedding types. The embedding methods fail to extrapolate in different ways, e.g., predictions using word2vec decrease almost monotonically as the input increases, while predictions using BERT are usually near the highest training value. Trask et al. (2018) also observe that models struggle outside the training range; they attribute this to failures in neural models themselves.</p>
<p>Extrapolation for List Maximum For the list maximum task, accuracies are closer to those in the interpolation setting, however, they still fall short. Table 7 shows the accuracy for models trained on the integer range $[0,150]$ and tested on</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Interpolation</th>
<th style="text-align: center;">List Maximum (5-classes)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Decoding (RMSE)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition (RMSE)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Integer Range</td>
<td style="text-align: center;">$[0,99]$</td>
<td style="text-align: center;">$[0,999]$</td>
<td style="text-align: center;">$[0,9999]$</td>
<td style="text-align: center;">$[0,99]$</td>
<td style="text-align: center;">$[0,999]$</td>
<td style="text-align: center;">$[0,9999]$</td>
<td style="text-align: center;">$[0,99]$</td>
<td style="text-align: center;">$[0,999]$</td>
<td style="text-align: center;">$[0,9999]$</td>
</tr>
<tr>
<td style="text-align: center;">Random Vectors</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">29.86</td>
<td style="text-align: center;">292.88</td>
<td style="text-align: center;">2882.62</td>
<td style="text-align: center;">42.03</td>
<td style="text-align: center;">410.33</td>
<td style="text-align: center;">4389.39</td>
</tr>
<tr>
<td style="text-align: center;">Untrained CNN</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">9.67</td>
<td style="text-align: center;">44.40</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">14.43</td>
<td style="text-align: center;">69.14</td>
</tr>
<tr>
<td style="text-align: center;">Untrained LSTM</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">210.34</td>
<td style="text-align: center;">5.11</td>
<td style="text-align: center;">45.69</td>
<td style="text-align: center;">510.19</td>
</tr>
<tr>
<td style="text-align: center;">Value Embedding</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">275.50</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">15.98</td>
<td style="text-align: center;">654.33</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">2.34</td>
<td style="text-align: center;">18.77</td>
<td style="text-align: center;">333.47</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">210.07</td>
</tr>
<tr>
<td style="text-align: center;">GloVe</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">13.77</td>
<td style="text-align: center;">174.21</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">16.51</td>
<td style="text-align: center;">180.31</td>
</tr>
<tr>
<td style="text-align: center;">ELMo</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">13.48</td>
<td style="text-align: center;">62.20</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">45.71</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">3.21</td>
<td style="text-align: center;">29.00</td>
<td style="text-align: center;">431.78</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">67.81</td>
<td style="text-align: center;">454.78</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Char-CNN</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">4.92</td>
<td style="text-align: center;">11.57</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">7.75</td>
<td style="text-align: center;">15.09</td>
</tr>
<tr>
<td style="text-align: center;">Char-LSTM</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">8.65</td>
<td style="text-align: center;">18.33</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">15.11</td>
<td style="text-align: center;">25.37</td>
</tr>
<tr>
<td style="text-align: center;">DROP-trained</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NAQANet</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">2.99</td>
<td style="text-align: center;">14.19</td>
<td style="text-align: center;">62.17</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">90.01</td>
</tr>
<tr>
<td style="text-align: center;">- GloVe</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">2.87</td>
<td style="text-align: center;">5.34</td>
<td style="text-align: center;">35.39</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">9.91</td>
<td style="text-align: center;">60.70</td>
</tr>
</tbody>
</table>
<p>Table 4: Interpolation with integers (e.g., "18"). All pre-trained embedding methods (e.g., GloVe and ELMo) surprisingly capture numeracy. The probing model is trained on a randomly shuffled $80 \%$ of the Integer Range and tested on the remaining $20 \%$. The probing model architecture and train/test splits are equivalent across all embeddings. We show the mean over 5 random shuffles (standard deviation in Appendix D).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Interpolation</th>
<th style="text-align: left;">List Maximum (5-classes)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Float Range</td>
<td style="text-align: left;">$[0.0,99.9]$</td>
</tr>
<tr>
<td style="text-align: left;">Rand. Vectors</td>
<td style="text-align: left;">$0.18 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">ELMo</td>
<td style="text-align: left;">$0.91 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$0.82 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: left;">Char-CNN</td>
<td style="text-align: left;">$0.87 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: left;">Char-LSTM</td>
<td style="text-align: left;">$0.81 \pm 0.05$</td>
</tr>
</tbody>
</table>
<p>Table 5: Interpolation with floats (e.g., "18.1") for list maximum. Pre-trained embeddings capture numeracy for float values. The probing model is trained on a randomly shuffled $80 \%$ of the Float Range and tested on the remaining $20 \%$. See the text for details on selecting decimal values. We show the mean alongside the standard deviation over 5 different random shuffles.
the ranges [151,160], [151,180], and [151,200]; all methods struggle, especially token vectors.</p>
<h2>Augmenting Data to Aid Extrapolation Of</h2>
<p>course, in many real-word tasks it is possible to ameliorate these extrapolation failures by augmenting the training data (i.e., turn extrapolation into interpolation). Here, we apply this idea to aid in training NAQANet for DROP. For each superlative and comparative example, we duplicate the example and modify the numbers in its paragraph using the Add and Multiply techniques de-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Interpolation</th>
<th style="text-align: center;">List Maximum (5-classes)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Integer Range</td>
<td style="text-align: center;">$[-50,50]$</td>
</tr>
<tr>
<td style="text-align: left;">Rand. Vectors</td>
<td style="text-align: center;">$0.23 \pm 0.12$</td>
</tr>
<tr>
<td style="text-align: left;">Word2Vec</td>
<td style="text-align: center;">$0.89 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">GloVe</td>
<td style="text-align: center;">$0.89 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: left;">ELMo</td>
<td style="text-align: center;">$0.96 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">$0.94 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: left;">Char-CNN</td>
<td style="text-align: center;">$0.95 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: left;">Char-LSTM</td>
<td style="text-align: center;">$0.97 \pm 0.02$</td>
</tr>
</tbody>
</table>
<p>Table 6: Interpolation with negatives (e.g., " -18 ") on list maximum. Pre-trained embeddings capture numeracy for negative values.
scribed in Section 2.5. Table 11 shows that this data augmentation can improve both interpolation and extrapolation, e.g., the accuracy on superlative questions with large numbers can double.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Extrapolation</th>
<th style="text-align: center;">List Maximum (5-classes)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test Range</td>
<td style="text-align: center;">$[151,160]$</td>
<td style="text-align: center;">$[151,180]$</td>
<td style="text-align: center;">$[151,200]$</td>
</tr>
<tr>
<td style="text-align: left;">Rand. Vectors</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: left;">Untrained CNN</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Word2Vec</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: left;">GloVe</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: left;">ELMo</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: left;">Learned</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Char-CNN</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">Char-LSTM</td>
<td style="text-align: center;">$\mathbf{0 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 2}$</td>
</tr>
<tr>
<td style="text-align: left;">DROP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">NAQANet</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">- GloVe</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p>Table 7: Extrapolation on list maximum. The probing model is trained on the integer range $[0,150]$ and evaluated on integers from the Test Range. The probing model struggles to extrapolate when trained on the pre-trained embeddings.</p>
<h2>4 Discussion and Related Work</h2>
<p>An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)).</p>
<p>More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other "commonsense" phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts.</p>
<p>In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods.</p>
<p>Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). Khandel-
wal et al. (2018) show how language models leverage context, while Linzen et al. (2016) demonstrate that language models understand subjectverb agreement.</p>
<p>Numerical Value Prediction Spithourakis and Riedel (2018) improve the ability of language models to predict numbers, i.e., they go beyond categorical predictions over a fixed-size vocabulary. They focus on improving models; our focus is probing embeddings. Kotnis and García-Durán (2019) predict numerical attributes in knowledge bases, e.g., they develop models that try to predict the population of Paris.</p>
<p>Synthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrapolation: neural models have difficulty outputting numerical values outside the training range.</p>
<h2>5 Conclusion</h2>
<p>How much do NLP models know about numbers? By digging into a surprisingly successful model on a numerical reasoning dataset (DROP), we discover that pre-trained token representations naturally encode numeracy.</p>
<p>We analyze the limits of this numeracy, finding that CNNs are a particularly good prior (and likely the cause of ELMo's superior numeracy compared to BERT) and that it is difficult for neural models to extrapolate beyond the values seen during training. There are still many fruitful areas for future research, including discovering why numeracy naturally emerges in embeddings, and what other properties are similarly emergent.</p>
<h2>Acknowledgements</h2>
<p>We thank Mark Neumann, Suchin Gururangan, Pranav Goel, Shi Feng, Nikhil Kandpal, Dheeru Dua, the members of AllenNLP and UCI NLP, and the reviewers for their valuable feedback.</p>
<h2>References</h2>
<p>Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Finegrained analysis of sentence embeddings using auxiliary prediction tasks. In $I C L R$.</p>
<p>Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. 2019. Understanding the origins of bias in word embeddings. In ICML.</p>
<p>Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In $A C L$.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.</p>
<p>Maxwell Forbes and Yejin Choi. 2017. Verb physics: Relative physical knowledge of actions and objects. In $A C L$.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.</p>
<p>Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. In $A C L$.</p>
<p>Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In ICML.</p>
<p>Bhushan Kotnis and Alberto García-Durán. 2019. Learning numerical attributes in knowledge bases. In $A K B C$.</p>
<p>Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.</p>
<p>Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. In TACL.</p>
<p>Nelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A Smith. 2019. Linguistic knowledge and transferability of contextual representations. In NAACL.</p>
<p>Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in pre-training distributed word representations. In LREC.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In $A C L$.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In EMNLP.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. arXiv preprint arXiv:1901.03735.</p>
<p>Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. 2011. On random weights and unsupervised feature learning. In ICML.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models.</p>
<p>Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In $I C L R$.</p>
<p>Georgios Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. In $A C L$.</p>
<p>Andrew Trask, Felix Hill, Scott Reed, Jack W. Rae, Chris Dyer, and Phil Blunsom. 2018. Neural arithmetic logic units. In NeurIPS.</p>
<p>Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2016. Order-embeddings of images and language. In $I C L R$.</p>
<p>Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. 2016. Order matters: Sequence to sequence for sets. In $I C L R$.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision rnns for language recognition. In $A C L$.</p>
<p>Yiben Yang, Larry Birnbaum, Ji-Ping Wang, and Doug Downey. 2018. Extracting commonsense properties from embeddings with limited human guidance. In $A C L$.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehension. In $I C L R$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Since our inputs are numbers, not natural sentences, language models may exhibit strange behavior. We experimented with extracting the context-independent feature vector immediately following the character convolutions for ELMo but found little difference in results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ We suspect the failures result from the raw values being too high in magnitude and/or variance for the model. We also experimented with normalizing the values to mean 0 and variance 1; a logarithmic scale performed better.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
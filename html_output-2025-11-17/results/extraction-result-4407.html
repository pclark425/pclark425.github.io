<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4407 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4407</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4407</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-274822175</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.13612v5.pdf" target="_blank">Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4407.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4407.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey (Wang et al., 2024b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that generates academic surveys by incorporating up-to-date papers via retrieval-augmented generation (RAG), aiming to automate the collection and synthesis of recent literature for survey writing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoSurvey uses retrieval-augmented generation to incorporate recent papers into an LLM-driven pipeline for automatic survey writing. The system first retrieves relevant up-to-date literature from external sources and then conditions an LLM to generate survey text that integrates those retrieved documents. The authors emphasize support for freshness by connecting retrieval results to the generative model to produce surveys reflecting recent publications.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (described generically as 'large language models' / retrieval-augmented LLMs); the host paper notes common use of vanilla LLMs such as ChatGPT and GPT-family models in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval + retrieval-augmented generation (RAG) to fetch up-to-date papers from external sources.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>RAG-conditioned generation to synthesize retrieved documents into coherent survey text (single-pass generative synthesis grounded on retrieved documents).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified (designed to incorporate 'up-to-date papers' dynamically via retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General academic surveys / cross-domain (designed for survey writing across topics)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated surveys / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this host paper's mention (original AutoSurvey likely reports human evaluation and standard summarization metrics; host paper references RAG-based survey systems generally).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (host paper only cites AutoSurvey as a related system; no quantitative results provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper's mention (original system likely compared to non-retrieval baselines or human-written surveys).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AutoSurvey demonstrates the strategy of grounding generation with retrieval to incorporate recent literature; cited as an example that RAG enables more up-to-date surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Host paper notes general limitations of LLM-based survey systems (e.g., hallucinated references remain an issue even when grounding is used), but specific AutoSurvey limitations are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper's mention; AutoSurvey is described as retrieval-driven and therefore scales by retrieval coverage rather than fixed corpus size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4407.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Two-step retrieval & outlining (Agarwal et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot LLM review generation via two-step retrieval and outlining (Agarwal et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step approach where retrieval is used first to collect relevant literature and then an LLM generates an outline (and subsequently full text) in a zero-shot fashion, intended to produce structured literature reviews without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llms for literature review: Are we there yet?.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Two-step retrieval and outlining (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The approach performs an initial retrieval phase to gather candidate papers relevant to a topic, and then prompts an LLM to produce an outline (and later full review) using the retrieved items as context. The two-step nature separates identification of sources from composition, enabling zero-shot generation without model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper's mention; described as 'LLMs' in general for zero-shot review generation (commonly ChatGPT / GPT-4 in related literature).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval (query-based or embedding-based) to collect candidate references prior to generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Outline-first composition: generate structured outlines from retrieved materials, then expand outlines into full review text (multi-step generation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in the host paper's description (designed to retrieve multiple relevant studies per topic).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review generation across disciplines (zero-shot evaluation reported broadly).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstracts, outlines, and literature reviews (zero-shot generated content grounded on retrieved items)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Mentioned in host paper context: hallucination rates, semantic coverage, factual consistency (general metrics used for review generation evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (host paper only cites the approach; no quantitative numbers provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Zero-shot LLM generation without retrieval, and human-written reviews (implied in discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Two-step retrieval + outline is a pragmatic workflow for zero-shot review generation; retrieval prior to composition aims to reduce hallucinations and improve topical relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Residual hallucination of references and inconsistencies in factual coverage remain challenges even with retrieval and outlining.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail in the host paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4407.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyX (Liang et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient system that optimizes retrieval, extraction, and outline generation for automated academic survey creation, supporting multimodal outputs such as figures and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyx: Academic survey automation via large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SurveyX is described as a pipeline that optimizes three stages: retrieval of relevant papers, extraction of key content from them, and generation of an outline (and final survey). It additionally supports multimodal outputs (figures, tables) by extracting structured elements from source papers and incorporating them into generated surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in the host paper's mention; described as using large language models as the backbone for generation and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval + extraction (structured extraction of figures/tables and textual keypoints from papers) likely using a mix of retrieval and extraction prompts or tools.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical synthesis: retrieved documents are extracted and organized into outlines, then LLM-generated composition produces full survey with multimodal elements.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this mention; designed to process multiple papers per survey (scales with retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Academic surveys across domains; supports multimodal scientific content.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated academic surveys with text, figures, and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not given here; host paper groups SurveyX with systems optimized for retrieval, extraction and outline generation (common metrics would include ROUGE, KPR, factuality/hallucination measures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper (only cited as a recent system).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in host paper's brief mention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SurveyX exemplifies systems that combine retrieval, structured extraction, and outline-driven generation to produce richer, multimodal surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Host paper notes general challenges for such systems: hallucinated citations and variable performance across disciplines; specific SurveyX limitations not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4407.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLM assistance to hierarchically organize and structure scientific studies to support literature review workflows, facilitating organization and navigation of large collections of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CHIME</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CHIME leverages LLMs to build hierarchical organizations (clusters, outlines, topic trees) of scientific studies, aiding humans in literature review by structuring large numbers of papers into digestible hierarchies and summaries. It uses LLMs to extract key points and group them into hierarchical categories.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in host paper; described generally as LLM-assisted (paper cited in references: Hsu et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-based extraction of key points and topic labels; likely combined with clustering or retrieval to aggregate papers into hierarchies.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical summarization / organization: synthesize extracted points into multi-level overviews that support review composition.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here (designed to handle collections of studies for review support).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review support across scientific topics (general-purpose organizational tool).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical organization structures, summaries, navigable literature review scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper's mention; typical metrics might include clustering quality, human usefulness, and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this host paper mention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CHIME illustrates use of LLMs not just for text generation but for meta-organization of literature to assist in review workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>General LLM limitations (hallucinations, metadata mismatches) can affect hierarchical organization; specific CHIME constraints not detailed in host paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not described in this mention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4407.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long2RAG / KPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long2RAG: Evaluating long-context & long-form retrieval-augmented generation with Key Point Recall (KPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach and evaluation method that studies retrieval-augmented generation for long-context, long-form outputs and uses Key Point Recall (KPR) to measure coverage of important claims from source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Long2RAG (evaluation approach) with Key Point Recall (KPR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Long2RAG evaluates RAG systems on long-context and long-form generation tasks; it introduces Key Point Recall (KPR) as a metric that measures how many of the salient key points (extracted from long documents) are present in model outputs. The method typically extracts q key points per source document and uses NLI-like entailment checks to compute recall.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper's mention (KPR used as a metric with LLM-based RAG systems); host paper uses GPT-4o as NLI model for KPR computation in its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval to assemble long context; key-point extraction (often using an LLM) to define evaluation targets.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-augmented hierarchical or long-form summarization; synthesis evaluated by KPR to capture coverage of critical points.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for long documents / multiple long-context inputs (number depends on retrieval budget); not fixed here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-form summarization and literature review contexts across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form generated text, literature reviews, or summaries assessed for key-point coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Key Point Recall (KPR), ROUGE, NLI-based factuality checks (as used by host paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Host paper references KPR use and sets q=10 for its own evaluation; specific Long2RAG numeric results are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in host paper mention; Long2RAG compares RAG systems with baselines for long-context generation in its original work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported in host paper; host paper uses KPR to compare LLMs' review composition outputs to human-written reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>KPR is effective for measuring coverage of long documents' key points; host paper employs KPR to show variance in LLMs' ability to recall human-written claims.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>KPR depends on the quality of extracted key points (often obtained via another LLM) and requires reliable entailment checks; both extraction and NLI steps can introduce noise.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Host paper uses q=10 key points; Long2RAG addresses long-context scaling but specific scaling trends are not elaborated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4407.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation-aware generation (Gao et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enabling large language models to generate text with citations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to enable LLMs to generate text that includes citations, by integrating citation-aware prompting or training techniques so outputs reference source documents explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enabling large language models to generate text with citations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Citation-aware LLM generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This class of methods modifies prompting, retrieval, or training procedures to encourage LLMs to include explicit citations linked to retrieved or known source documents during generation, aiming to reduce hallucinated references and improve verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in host paper (original paper examines techniques applicable to generative LLMs broadly).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Often retrieval-based (RAG) to provide source metadata and text for grounding; may include structured prompts that request citation fields.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Grounded generation where the LLM composes text while emitting citation markers tied to retrieved sources; can be single-step or multi-step composition.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies by system; host paper does not give a fixed number.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific and academic text generation with citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated text with in-line citations and reference lists (aimed at reducing hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Hallucination rate of references, precision/recall/F1 for generated references (as used by the host paper), factual consistency metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this host paper beyond general claim that grounding with external citations reduces hallucination; precise results are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Ungrounded generation (vanilla LLM outputs) and retrieval-grounded generation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Host paper reports that grounding generated text with real external citations can effectively reduce hallucination rates (cited in Discussion), but concrete numeric comparisons are in original cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Grounding generation with retrieved, citable sources reduces hallucinated references and increases reference accuracy; citation-aware methods are promising for literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Correctly aligning generated citation text with real metadata remains hard; LLMs still produce hallucinated or partially incorrect references even when citation-aware techniques are used.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this mention; performance depends on retrieval coverage and the LLM's grounding fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4407.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) (general technique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of techniques that augment LLMs with a retrieval component that fetches relevant external documents to ground generation and improve factuality and topicality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG systems first retrieve relevant passages or documents from an external corpus, then condition an LLM on those retrieved contexts when generating output. The retrieval step can be embedding-based or BM25-style, and the synthesis is done by the LLM either by concatenation or more structured conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Generic LLMs (paper notes RAG can enhance domain-specific knowledge of LLMs; specific models vary across cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval or search-API retrieval to obtain documents/passages for grounding generation.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Conditioned generation where retrieved texts are appended or used in prompts to guide LLM outputs (single-step or multi-step).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Depends on retrieval budget; not fixed in general technique.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General â€” used to ground LLMs across domains including academic literature review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded generated text: summaries, reviews, answers, or surveys with references.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Factual consistency, hallucination rates (reference precision/recall/F1), semantic coverage (ROUGE, cosine similarity), KPR for long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Host paper cites that RAG can enhance domain knowledge and that grounding with real external citations improves reference accuracy in review composition tasks; no numeric RAG-specific results are given here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Vanilla LLM generation without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Host paper reports (citing prior work) that RAG generally improves grounding and reduces hallucination compared to vanilla LLMs; in their experiments, models generating references during composition (i.e., grounded) had higher precision than in standalone reference generation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG is an effective general strategy to reduce hallucination and incorporate up-to-date knowledge, but does not eliminate hallucinated references entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Retrieval coverage and alignment errors can still allow hallucinations; reliance on external search APIs and the quality of retrieved candidates limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance depends on retrieval corpus coverage and retrieval budget; host paper notes most prior researchers still rely on vanilla LLMs without RAG, indicating tradeoffs in complexity and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4407.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4407.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs-as-judges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs-as-judges (using LLMs to evaluate other LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses large language models themselves to assess the quality, factuality, or other properties of generated outputs (e.g., using LLMs for NLI, hallucination detection, or scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Humans or llms as the judge? a study on judgement biases.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMs-as-judges</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This approach uses LLMs (sometimes newer/more capable models) as automatic evaluators of other LLM-generated text. Typical uses include NLI-style factuality checks, hallucination detection, and scoring of semantic coverage or consistency. The host paper references this trend and uses GPT-4o and TRUE models for factual consistency evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>In host paper experiments GPT-4o is used as an NLI model for KPR entailment checks; TRUE (Honovich et al.) is also used for factual consistency in the Abstract Writing task.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not extraction per se; uses NLI-style prompting and entailment checks where the judge-LMM inspects candidate outputs against source/human text.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Evaluation synthesis: judge-LLM outputs binary/graded entailment scores or ranks candidate generations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (evaluation method rather than a multi-paper synthesis system).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Evaluation of generated scientific text across disciplines (as used in this paper for abstracts and reviews).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evaluation labels / entailment judgments / factuality scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Entailment (binary), TRUE factuality metric, semantic similarity (embeddings), ROUGE, KPR when combined with keypoint extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Host paper uses GPT-4o and TRUE as automatic evaluators and reports entailment and TRUE scores for generated abstracts and reviews; for example, Claude-3.5-Sonnet achieved a TRUE score of 78.10% on abstracts per host-paper evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human evaluation is used to validate the automatic method (host paper reports kappa=0.71 between automatic and human assessments for reference hallucination detection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Automatic LLM-based judgments aligned with human labels with kappa=0.71 and 86% accuracy on a 100-reference sample for hallucination detection in host paper validation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based evaluation (LLMs-as-judges) is increasingly common and can approximate human judgment for factuality/hallucination with decent agreement, but biases and judge-model limitations exist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Judge LLMs may have their own biases and can be overconfident; reliance on LLMs for evaluation risks circularity if the same model family is used for generation and judging.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Host paper applies judge-LLMs at scale to the dataset of 1,105 reviews; scaling evaluation is feasible but depends on API availability and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys. <em>(Rating: 2)</em></li>
                <li>Surveyx: Academic survey automation via large language models. <em>(Rating: 2)</em></li>
                <li>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support. <em>(Rating: 2)</em></li>
                <li>Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall. <em>(Rating: 2)</em></li>
                <li>Enabling large language models to generate text with citations. <em>(Rating: 2)</em></li>
                <li>Llms for literature review: Are we there yet?. <em>(Rating: 1)</em></li>
                <li>Humans or llms as the judge? a study on judgement biases. <em>(Rating: 1)</em></li>
                <li>Do language models know when they're hallucinating references?. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4407",
    "paper_id": "paper-274822175",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey (Wang et al., 2024b)",
            "brief_description": "A system that generates academic surveys by incorporating up-to-date papers via retrieval-augmented generation (RAG), aiming to automate the collection and synthesis of recent literature for survey writing.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys.",
            "mention_or_use": "mention",
            "system_name": "AutoSurvey",
            "system_description": "AutoSurvey uses retrieval-augmented generation to incorporate recent papers into an LLM-driven pipeline for automatic survey writing. The system first retrieves relevant up-to-date literature from external sources and then conditions an LLM to generate survey text that integrates those retrieved documents. The authors emphasize support for freshness by connecting retrieval results to the generative model to produce surveys reflecting recent publications.",
            "llm_model_used": "Not specified in this paper (described generically as 'large language models' / retrieval-augmented LLMs); the host paper notes common use of vanilla LLMs such as ChatGPT and GPT-family models in the literature.",
            "extraction_technique": "Embedding-based retrieval + retrieval-augmented generation (RAG) to fetch up-to-date papers from external sources.",
            "synthesis_technique": "RAG-conditioned generation to synthesize retrieved documents into coherent survey text (single-pass generative synthesis grounded on retrieved documents).",
            "number_of_papers": "Not specified (designed to incorporate 'up-to-date papers' dynamically via retrieval)",
            "domain_or_topic": "General academic surveys / cross-domain (designed for survey writing across topics)",
            "output_type": "Automated surveys / literature reviews",
            "evaluation_metrics": "Not detailed in this host paper's mention (original AutoSurvey likely reports human evaluation and standard summarization metrics; host paper references RAG-based survey systems generally).",
            "performance_results": "Not reported in this paper (host paper only cites AutoSurvey as a related system; no quantitative results provided here).",
            "comparison_baseline": "Not specified in this paper's mention (original system likely compared to non-retrieval baselines or human-written surveys).",
            "performance_vs_baseline": "Not reported in this paper.",
            "key_findings": "AutoSurvey demonstrates the strategy of grounding generation with retrieval to incorporate recent literature; cited as an example that RAG enables more up-to-date surveys.",
            "limitations_challenges": "Host paper notes general limitations of LLM-based survey systems (e.g., hallucinated references remain an issue even when grounding is used), but specific AutoSurvey limitations are not reported here.",
            "scaling_behavior": "Not discussed in this paper's mention; AutoSurvey is described as retrieval-driven and therefore scales by retrieval coverage rather than fixed corpus size.",
            "uuid": "e4407.0",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Two-step retrieval & outlining (Agarwal et al., 2024)",
            "name_full": "Zero-shot LLM review generation via two-step retrieval and outlining (Agarwal et al., 2024)",
            "brief_description": "A two-step approach where retrieval is used first to collect relevant literature and then an LLM generates an outline (and subsequently full text) in a zero-shot fashion, intended to produce structured literature reviews without finetuning.",
            "citation_title": "Llms for literature review: Are we there yet?.",
            "mention_or_use": "mention",
            "system_name": "Two-step retrieval and outlining (zero-shot)",
            "system_description": "The approach performs an initial retrieval phase to gather candidate papers relevant to a topic, and then prompts an LLM to produce an outline (and later full review) using the retrieved items as context. The two-step nature separates identification of sources from composition, enabling zero-shot generation without model fine-tuning.",
            "llm_model_used": "Not specified in this paper's mention; described as 'LLMs' in general for zero-shot review generation (commonly ChatGPT / GPT-4 in related literature).",
            "extraction_technique": "Retrieval (query-based or embedding-based) to collect candidate references prior to generation.",
            "synthesis_technique": "Outline-first composition: generate structured outlines from retrieved materials, then expand outlines into full review text (multi-step generation).",
            "number_of_papers": "Not specified in the host paper's description (designed to retrieve multiple relevant studies per topic).",
            "domain_or_topic": "Literature review generation across disciplines (zero-shot evaluation reported broadly).",
            "output_type": "Abstracts, outlines, and literature reviews (zero-shot generated content grounded on retrieved items)",
            "evaluation_metrics": "Mentioned in host paper context: hallucination rates, semantic coverage, factual consistency (general metrics used for review generation evaluation).",
            "performance_results": "Not reported in this paper (host paper only cites the approach; no quantitative numbers provided here).",
            "comparison_baseline": "Zero-shot LLM generation without retrieval, and human-written reviews (implied in discussion).",
            "performance_vs_baseline": "Not reported in this paper.",
            "key_findings": "Two-step retrieval + outline is a pragmatic workflow for zero-shot review generation; retrieval prior to composition aims to reduce hallucinations and improve topical relevance.",
            "limitations_challenges": "Residual hallucination of references and inconsistencies in factual coverage remain challenges even with retrieval and outlining.",
            "scaling_behavior": "Not discussed in detail in the host paper's mention.",
            "uuid": "e4407.1",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SurveyX",
            "name_full": "SurveyX (Liang et al., 2025)",
            "brief_description": "An efficient system that optimizes retrieval, extraction, and outline generation for automated academic survey creation, supporting multimodal outputs such as figures and tables.",
            "citation_title": "Surveyx: Academic survey automation via large language models.",
            "mention_or_use": "mention",
            "system_name": "SurveyX",
            "system_description": "SurveyX is described as a pipeline that optimizes three stages: retrieval of relevant papers, extraction of key content from them, and generation of an outline (and final survey). It additionally supports multimodal outputs (figures, tables) by extracting structured elements from source papers and incorporating them into generated surveys.",
            "llm_model_used": "Not specified in the host paper's mention; described as using large language models as the backbone for generation and synthesis.",
            "extraction_technique": "Retrieval + extraction (structured extraction of figures/tables and textual keypoints from papers) likely using a mix of retrieval and extraction prompts or tools.",
            "synthesis_technique": "Hierarchical synthesis: retrieved documents are extracted and organized into outlines, then LLM-generated composition produces full survey with multimodal elements.",
            "number_of_papers": "Not specified in this mention; designed to process multiple papers per survey (scales with retrieval).",
            "domain_or_topic": "Academic surveys across domains; supports multimodal scientific content.",
            "output_type": "Automated academic surveys with text, figures, and tables.",
            "evaluation_metrics": "Not given here; host paper groups SurveyX with systems optimized for retrieval, extraction and outline generation (common metrics would include ROUGE, KPR, factuality/hallucination measures).",
            "performance_results": "Not reported in this paper (only cited as a recent system).",
            "comparison_baseline": "Not specified in host paper's brief mention.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "SurveyX exemplifies systems that combine retrieval, structured extraction, and outline-driven generation to produce richer, multimodal surveys.",
            "limitations_challenges": "Host paper notes general challenges for such systems: hallucinated citations and variable performance across disciplines; specific SurveyX limitations not reported here.",
            "scaling_behavior": "Not discussed in this paper's mention.",
            "uuid": "e4407.2",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "CHIME",
            "name_full": "CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support",
            "brief_description": "A method that uses LLM assistance to hierarchically organize and structure scientific studies to support literature review workflows, facilitating organization and navigation of large collections of papers.",
            "citation_title": "CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support.",
            "mention_or_use": "mention",
            "system_name": "CHIME",
            "system_description": "CHIME leverages LLMs to build hierarchical organizations (clusters, outlines, topic trees) of scientific studies, aiding humans in literature review by structuring large numbers of papers into digestible hierarchies and summaries. It uses LLMs to extract key points and group them into hierarchical categories.",
            "llm_model_used": "Not specified in host paper; described generally as LLM-assisted (paper cited in references: Hsu et al.).",
            "extraction_technique": "LLM-based extraction of key points and topic labels; likely combined with clustering or retrieval to aggregate papers into hierarchies.",
            "synthesis_technique": "Hierarchical summarization / organization: synthesize extracted points into multi-level overviews that support review composition.",
            "number_of_papers": "Not specified here (designed to handle collections of studies for review support).",
            "domain_or_topic": "Literature review support across scientific topics (general-purpose organizational tool).",
            "output_type": "Hierarchical organization structures, summaries, navigable literature review scaffolds.",
            "evaluation_metrics": "Not specified in this paper's mention; typical metrics might include clustering quality, human usefulness, and coverage.",
            "performance_results": "Not reported here.",
            "comparison_baseline": "Not specified in this host paper mention.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "CHIME illustrates use of LLMs not just for text generation but for meta-organization of literature to assist in review workflows.",
            "limitations_challenges": "General LLM limitations (hallucinations, metadata mismatches) can affect hierarchical organization; specific CHIME constraints not detailed in host paper.",
            "scaling_behavior": "Not described in this mention.",
            "uuid": "e4407.3",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Long2RAG / KPR",
            "name_full": "Long2RAG: Evaluating long-context & long-form retrieval-augmented generation with Key Point Recall (KPR)",
            "brief_description": "An approach and evaluation method that studies retrieval-augmented generation for long-context, long-form outputs and uses Key Point Recall (KPR) to measure coverage of important claims from source documents.",
            "citation_title": "Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall.",
            "mention_or_use": "mention",
            "system_name": "Long2RAG (evaluation approach) with Key Point Recall (KPR)",
            "system_description": "Long2RAG evaluates RAG systems on long-context and long-form generation tasks; it introduces Key Point Recall (KPR) as a metric that measures how many of the salient key points (extracted from long documents) are present in model outputs. The method typically extracts q key points per source document and uses NLI-like entailment checks to compute recall.",
            "llm_model_used": "Not specified in this paper's mention (KPR used as a metric with LLM-based RAG systems); host paper uses GPT-4o as NLI model for KPR computation in its experiments.",
            "extraction_technique": "Retrieval to assemble long context; key-point extraction (often using an LLM) to define evaluation targets.",
            "synthesis_technique": "Retrieval-augmented hierarchical or long-form summarization; synthesis evaluated by KPR to capture coverage of critical points.",
            "number_of_papers": "Designed for long documents / multiple long-context inputs (number depends on retrieval budget); not fixed here.",
            "domain_or_topic": "Long-form summarization and literature review contexts across domains.",
            "output_type": "Long-form generated text, literature reviews, or summaries assessed for key-point coverage.",
            "evaluation_metrics": "Key Point Recall (KPR), ROUGE, NLI-based factuality checks (as used by host paper).",
            "performance_results": "Host paper references KPR use and sets q=10 for its own evaluation; specific Long2RAG numeric results are not provided here.",
            "comparison_baseline": "Not specified in host paper mention; Long2RAG compares RAG systems with baselines for long-context generation in its original work.",
            "performance_vs_baseline": "Not reported in host paper; host paper uses KPR to compare LLMs' review composition outputs to human-written reviews.",
            "key_findings": "KPR is effective for measuring coverage of long documents' key points; host paper employs KPR to show variance in LLMs' ability to recall human-written claims.",
            "limitations_challenges": "KPR depends on the quality of extracted key points (often obtained via another LLM) and requires reliable entailment checks; both extraction and NLI steps can introduce noise.",
            "scaling_behavior": "Host paper uses q=10 key points; Long2RAG addresses long-context scaling but specific scaling trends are not elaborated here.",
            "uuid": "e4407.4",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Citation-aware generation (Gao et al., 2023)",
            "name_full": "Enabling large language models to generate text with citations",
            "brief_description": "A method to enable LLMs to generate text that includes citations, by integrating citation-aware prompting or training techniques so outputs reference source documents explicitly.",
            "citation_title": "Enabling large language models to generate text with citations.",
            "mention_or_use": "mention",
            "system_name": "Citation-aware LLM generation",
            "system_description": "This class of methods modifies prompting, retrieval, or training procedures to encourage LLMs to include explicit citations linked to retrieved or known source documents during generation, aiming to reduce hallucinated references and improve verifiability.",
            "llm_model_used": "Not specified in host paper (original paper examines techniques applicable to generative LLMs broadly).",
            "extraction_technique": "Often retrieval-based (RAG) to provide source metadata and text for grounding; may include structured prompts that request citation fields.",
            "synthesis_technique": "Grounded generation where the LLM composes text while emitting citation markers tied to retrieved sources; can be single-step or multi-step composition.",
            "number_of_papers": "Varies by system; host paper does not give a fixed number.",
            "domain_or_topic": "General scientific and academic text generation with citations.",
            "output_type": "Generated text with in-line citations and reference lists (aimed at reducing hallucinations).",
            "evaluation_metrics": "Hallucination rate of references, precision/recall/F1 for generated references (as used by the host paper), factual consistency metrics.",
            "performance_results": "Not reported in this host paper beyond general claim that grounding with external citations reduces hallucination; precise results are in the cited work.",
            "comparison_baseline": "Ungrounded generation (vanilla LLM outputs) and retrieval-grounded generation variants.",
            "performance_vs_baseline": "Host paper reports that grounding generated text with real external citations can effectively reduce hallucination rates (cited in Discussion), but concrete numeric comparisons are in original cited works.",
            "key_findings": "Grounding generation with retrieved, citable sources reduces hallucinated references and increases reference accuracy; citation-aware methods are promising for literature review generation.",
            "limitations_challenges": "Correctly aligning generated citation text with real metadata remains hard; LLMs still produce hallucinated or partially incorrect references even when citation-aware techniques are used.",
            "scaling_behavior": "Not discussed in this mention; performance depends on retrieval coverage and the LLM's grounding fidelity.",
            "uuid": "e4407.5",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-Augmented Generation (RAG) (general technique)",
            "brief_description": "A class of techniques that augment LLMs with a retrieval component that fetches relevant external documents to ground generation and improve factuality and topicality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG systems first retrieve relevant passages or documents from an external corpus, then condition an LLM on those retrieved contexts when generating output. The retrieval step can be embedding-based or BM25-style, and the synthesis is done by the LLM either by concatenation or more structured conditioning.",
            "llm_model_used": "Generic LLMs (paper notes RAG can enhance domain-specific knowledge of LLMs; specific models vary across cited works).",
            "extraction_technique": "Embedding-based retrieval or search-API retrieval to obtain documents/passages for grounding generation.",
            "synthesis_technique": "Conditioned generation where retrieved texts are appended or used in prompts to guide LLM outputs (single-step or multi-step).",
            "number_of_papers": "Depends on retrieval budget; not fixed in general technique.",
            "domain_or_topic": "General â€” used to ground LLMs across domains including academic literature review tasks.",
            "output_type": "Grounded generated text: summaries, reviews, answers, or surveys with references.",
            "evaluation_metrics": "Factual consistency, hallucination rates (reference precision/recall/F1), semantic coverage (ROUGE, cosine similarity), KPR for long documents.",
            "performance_results": "Host paper cites that RAG can enhance domain knowledge and that grounding with real external citations improves reference accuracy in review composition tasks; no numeric RAG-specific results are given here.",
            "comparison_baseline": "Vanilla LLM generation without retrieval.",
            "performance_vs_baseline": "Host paper reports (citing prior work) that RAG generally improves grounding and reduces hallucination compared to vanilla LLMs; in their experiments, models generating references during composition (i.e., grounded) had higher precision than in standalone reference generation.",
            "key_findings": "RAG is an effective general strategy to reduce hallucination and incorporate up-to-date knowledge, but does not eliminate hallucinated references entirely.",
            "limitations_challenges": "Retrieval coverage and alignment errors can still allow hallucinations; reliance on external search APIs and the quality of retrieved candidates limit performance.",
            "scaling_behavior": "Performance depends on retrieval corpus coverage and retrieval budget; host paper notes most prior researchers still rely on vanilla LLMs without RAG, indicating tradeoffs in complexity and deployment.",
            "uuid": "e4407.6",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLMs-as-judges",
            "name_full": "LLMs-as-judges (using LLMs to evaluate other LLM outputs)",
            "brief_description": "An approach that uses large language models themselves to assess the quality, factuality, or other properties of generated outputs (e.g., using LLMs for NLI, hallucination detection, or scoring).",
            "citation_title": "Humans or llms as the judge? a study on judgement biases.",
            "mention_or_use": "mention",
            "system_name": "LLMs-as-judges",
            "system_description": "This approach uses LLMs (sometimes newer/more capable models) as automatic evaluators of other LLM-generated text. Typical uses include NLI-style factuality checks, hallucination detection, and scoring of semantic coverage or consistency. The host paper references this trend and uses GPT-4o and TRUE models for factual consistency evaluation.",
            "llm_model_used": "In host paper experiments GPT-4o is used as an NLI model for KPR entailment checks; TRUE (Honovich et al.) is also used for factual consistency in the Abstract Writing task.",
            "extraction_technique": "Not extraction per se; uses NLI-style prompting and entailment checks where the judge-LMM inspects candidate outputs against source/human text.",
            "synthesis_technique": "Evaluation synthesis: judge-LLM outputs binary/graded entailment scores or ranks candidate generations.",
            "number_of_papers": "Not applicable (evaluation method rather than a multi-paper synthesis system).",
            "domain_or_topic": "Evaluation of generated scientific text across disciplines (as used in this paper for abstracts and reviews).",
            "output_type": "Evaluation labels / entailment judgments / factuality scores.",
            "evaluation_metrics": "Entailment (binary), TRUE factuality metric, semantic similarity (embeddings), ROUGE, KPR when combined with keypoint extraction.",
            "performance_results": "Host paper uses GPT-4o and TRUE as automatic evaluators and reports entailment and TRUE scores for generated abstracts and reviews; for example, Claude-3.5-Sonnet achieved a TRUE score of 78.10% on abstracts per host-paper evaluation.",
            "comparison_baseline": "Human evaluation is used to validate the automatic method (host paper reports kappa=0.71 between automatic and human assessments for reference hallucination detection).",
            "performance_vs_baseline": "Automatic LLM-based judgments aligned with human labels with kappa=0.71 and 86% accuracy on a 100-reference sample for hallucination detection in host paper validation.",
            "key_findings": "LLM-based evaluation (LLMs-as-judges) is increasingly common and can approximate human judgment for factuality/hallucination with decent agreement, but biases and judge-model limitations exist.",
            "limitations_challenges": "Judge LLMs may have their own biases and can be overconfident; reliance on LLMs for evaluation risks circularity if the same model family is used for generation and judging.",
            "scaling_behavior": "Host paper applies judge-LLMs at scale to the dataset of 1,105 reviews; scaling evaluation is feasible but depends on API availability and cost.",
            "uuid": "e4407.7",
            "source_info": {
                "paper_title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys.",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Surveyx: Academic survey automation via large language models.",
            "rating": 2,
            "sanitized_title": "surveyx_academic_survey_automation_via_large_language_models"
        },
        {
            "paper_title": "CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support.",
            "rating": 2,
            "sanitized_title": "chime_llmassisted_hierarchical_organization_of_scientific_studies_for_literature_review_support"
        },
        {
            "paper_title": "Long2rag: Evaluating long-context & long-form retrieval-augmented generation with key point recall.",
            "rating": 2,
            "sanitized_title": "long2rag_evaluating_longcontext_longform_retrievalaugmented_generation_with_key_point_recall"
        },
        {
            "paper_title": "Enabling large language models to generate text with citations.",
            "rating": 2,
            "sanitized_title": "enabling_large_language_models_to_generate_text_with_citations"
        },
        {
            "paper_title": "Llms for literature review: Are we there yet?.",
            "rating": 1,
            "sanitized_title": "llms_for_literature_review_are_we_there_yet"
        },
        {
            "paper_title": "Humans or llms as the judge? a study on judgement biases.",
            "rating": 1,
            "sanitized_title": "humans_or_llms_as_the_judge_a_study_on_judgement_biases"
        },
        {
            "paper_title": "Do language models know when they're hallucinating references?.",
            "rating": 1,
            "sanitized_title": "do_language_models_know_when_theyre_hallucinating_references"
        }
    ],
    "cost": 0.0174725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</p>
<p>Xuemei Tang 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Xufeng Duan 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Zhenguang G Cai 
Department of Linguistics and Modern Languages
The Chinese University of Hong Kong</p>
<p>Brain and Mind Institute
The Chinese University of Hong Kong</p>
<p>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition
75CB31A8CFA0A1E5CF320604082F9DAB
Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization.However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews.This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature review writing: reference generation, abstract writing, and literature review composition.We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts.The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress.Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews.These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.The dataset and code used in this study are publicly available at an anonymous repository 1 .</p>
<p>Introduction</p>
<p>The literature review is a critical component of academic writing that aims to synthesize, critique, and assess the current state of knowledge in a particular field.It involves a comprehensive examination of published research articles, theoretical frameworks, and research methodologies related to a specific topic.Conducting a thorough literature review often necessitates extensive reading and summarizing of pertinent literature, which can be a complex and time-consuming process, especially in well-established fields where the number of relevant references can range from dozens to hundreds.To alleviate this burden, researchers have recently turned to advanced deep learning models as a potential tool to aid in the automated generation of literature reviews (Aliyu et al., 2018;Kontonatsios et al., 2020).</p>
<p>The emergence of large language models (LLMs) has introduced a promising avenue for automating key aspects of literature review writing, including identifying relevant sources, summarizing findings, and generating coherent syntheses (Wang et al., 2024b;Agarwal et al., 2024;Hsu et al., 2024).While techniques such as Retrieval-Augmented Generation (RAG) can enhance the domain-specific knowledge of LLMs, most researchers rely on vanilla LLMs, such as ChatGPT, for literature review writing without the use of RAG (Wang et al., 2024a).Consequently, it is crucial to evaluate the performance of these naive LLMs in the context of literature review writing to determine their effectiveness and limitations.</p>
<p>Therefore, in this paper, we propose a framework for automatically assessing the literature review writing ability of LLMs, using human-written literature reviews as the gold standard and designing metrics for a comprehensive evaluation.We first collect a dataset of human-written literature reviews to serve as a benchmark for evaluating the performance of LLMs.We then ask LLMs to complete three tasks based on the collected dataset: generating references, writing an abstract, and writing a complete literature review based on a given topic.Finally, we evaluate the generated results from several dimensions, including the presence of hallucinations in the references, as well as the semantic coverage and factual consistency of the generated abstract and literature review compared to the human-written context.By assessing the performance of LLMs across these tasks and evaluating their output using our proposed metrics, we arXiv:2412.13612v5[cs.CL] 21 Aug 2025 aim to provide a comprehensive understanding of their capabilities and limitations in writing literature reviews.</p>
<p>Our contribution can be summarized as follows.</p>
<p>â€¢ First, we propose a framework for automatically evaluating the literature review writing ability of LLMs, without requiring any human involvement.This framework encompasses multiple stages, including the compilation of a literature review dataset construction, the collection of LLM-generated output, and the evaluation of LLM performance.</p>
<p>â€¢ Second, we collect 1,105 literature reviews from 51 journals across 5 disciplines as the ground truth.We then design three tasks for accessing LLMs in literature writing: reference generation, abstract writing, and literature composition on a given topic.</p>
<p>â€¢ Then, we evaluate the generated results of LLMs from multiple perspectives, including the hallucination rate in generated references, factual consistency, and semantic coverage compared to human-written content.</p>
<p>â€¢ Finally, we assess five LLMs using the proposed framework.By analyzing the experimental results, we find that hallucinated references remain a prevalent issue for current LLMs.Furthermore, the performance of LLMs in writing literature reviews varies across different disciplines.</p>
<p>Related Work</p>
<p>Recent studies have explored LLMs for literature review generation.For example, Wang et al. (2024b) proposed AutoSurvey, which incorporates up-to-date papers via retrieval-augmented generation.Agarwal et al. (2024) examined zero-shot LLM review generation using a two-step retrieval and outlining process.More recently, Liang et al. (2025) presented SurveyX, an efficient system that optimizes retrieval, extraction, and outline generation, supporting multimodal outputs such as figures and tables.Additionally, recent efforts to evaluate literature review generation by LLMs have increasingly focused on assessing hallucinations in reference citations.For instance, Chelli et al. (2024) analyzed hallucination rates in 11 systematic reviews on shoulder rotator cuff pathology generated by ChatGPT, GPT-4, and Bard, finding Bard exhibited significantly higher hallucination rates.Similarly, Agrawal et al. (2024)</p>
<p>Methodology</p>
<p>In this section, we propose a framework for evaluating LLMs' literature review writing ability.The framework, as shown in Figure 1, consists of three main stages: dataset construction and task design for evaluation, collection LLM-generated output, and assessment of the generated output.</p>
<p>Dataset Construction</p>
<p>Assessing the ability to write literature reviews is a challenging task, as evaluating the quality of content is inherently complex.In this paper, we use human-written reviews as the gold standard, which simplifies the evaluation process to some extent.As illustrated in Figure 1, we first collect publicly available information of literature reviews (i.e., the title, authors, abstract, and keywords) from the Annual Reviews website (https: //www.annualreviews.org/).Annual Reviews, an independent nonprofit publisher, produces 51 review journals spanning various scientific disciplines.Invited experts write comprehensive, authoritative reviews that synthesize and summarize the most significant primary research literature in their field, providing a valuable resource for researchers to stay current with the latest developments.We crawl all articles published in 2023, including their title, keywords, abstracts, contents, and references, and then clean them to create the experimental dataset.</p>
<p>Then, the dataset D is the article set from 51 journals, D = {p 0 , ..., p i , ..., p M }, where M represents the number of articles.Each article p i = {t i , w i , a i , c i , R i }, where t i , w i , a i , c i , R i represent the title, keywords, abstract, context, and reference set R i = {r 1 , ..., r k , ..., r K }, and K represents the length of the reference set.</p>
<p>Task Design</p>
<p>Since literature review writing primarily involves the collection and synthesis of relevant research, we design three independent tasks as follows to evaluate LLMs' capabilities in different aspects of literature review writing.</p>
<p>â€¢ Reference Generation: Given the article title t i and keywords w i , ask LLMs to find the N most relevant studies R g i = {r g 1 , ..., r g n , ..., r g N } to the research topic.Each citation study must include 7 metadata elements: title, authors, journal, year, volumes, first page, and last page, r g n = {T, A, J, Y, V, F P, LP }.In this task, we evaluate whether LLMs can recommend reliable references based on the given topic.Note that these references are not reused in later tasks.</p>
<p>â€¢ Abstract Writing: Given an article title t i and its associated keywords w i , the LLMs are prompted to generate an abstract a g i that aligns with the research topic.The length of the generated abstract is constrained to match that of the original.This task serves as a proxy for literature review planning, as abstracts often outline the key components of a study-such as its objectives, methods, and covered subtopics-which are also critical in structuring comprehensive literature reviews.By evaluating the model's ability to generate coherent and topic-relevant abstracts, we assess its potential to assist researchers in the early planning stages of literature review writing.</p>
<p>â€¢ Review Composition: Given the article t i and keywords w i , and abstract a i , ask LLMs to write a short literature review c g i according to the research topic provided in the title, keywords, and abstract.To facilitate evaluation and accommodate computational budget constraints, the length of each literature review is limited to approximately 1000 words.LLMs also need to back up claims by citing relevent studies R g i = {r g 1 , ..., r g n , ..., r g N } (with a total of N citations in the literature review).These citations are newly generated to support the content of the review.In this task, we evaluate whether LLMs can write a high-quality literature review and cite truth studies.</p>
<p>Three task prompts are shown in Appendix Table 5.</p>
<p>Evaluation Metrics</p>
<p>Based on the type of generated text, we divide the evaluation of the model's results into two parts: first, the hallucination rate of the references generated by LLMs, and second, a comparison of the generated context with human-written results, including two dimensions: factual consistency and semantic coverage.</p>
<p>Reference hallucination evaluation metrics.Given that LLMs are trained on vast corpora, including academic sources, we aim to evaluate whether they can generate true references.In this section, we introduce the calculation process of the reference precision P recison, reference recall Recall, F 1, and title search rate S t for each LLM.A higher precision metric indicates a lower hallucination rate.A higher recall indicates that the LLM-generated references cover more of the ground-truth citations used by human authors, reflecting a better ability to identify key prior work relevant to the topic.</p>
<p>For each article p i âˆˆ D, each LLM generates N references R g i = {r g 1 , ..., r g n , ..., r g N } in both Reference Generation and Review Composition tasks, each r g n and includes 7 elements.Each element corresponding to a state label represents whether it is accurate or not {e d } 6</p>
<p>d=0 , e d = 1 or 0. Next, we describe how to obtain {e d } 6</p>
<p>d=0 .First, we use the generated titles T and the first author in A as the queries and search them separately from external academic search engines.This results in two sets of candidate articles, Z t and Z a respectively, We then merge the two sets and remove duplicates to obtain the final candidate set Z = {z 1 , ..., z j , ..., z J }. Subsequently, we compare the generated r g n with the article z j from candidate sets Z.For example, if the title of a candidate article z j matches the title of r g n , then e 0 = 1.Finally, we find the best candidate article based on the sum of {e d } 6</p>
<p>d=0 , and the one with the largest sum is the best candidate article z j of r g n .Then, we compare the alignment degree between the generated reference r g n and the best-matching candidate article z j to determine whether r g n corresponds to a real article (as shown in Eq. 1).We consider r g n to be reliable under either of the following two conditions:</p>
<p>Title-based matching: If the title T is correct (i.e., e 0 = 1), and at least one other metadata element (e.g., author, journal, year, etc.) also matches, the reference is deemed reliable.To allow for minor variations, we consider the title to be correct if it achieves a match rate of at least 80% with the ground-truth title-a threshold determined through human evaluation.</p>
<p>Metadata-based matching: If the title T is incorrect (i.e., e 0 = 0), we still consider the reference reliable if at least three of the remaining metadata elements (author, journal, year, volume, first page, last page) match those of a real article.This allows us to identify true references even when the title is noisy or incomplete.
True(r g n ) = ï£± ï£´ ï£² ï£´ ï£³ 1 if e0 = 1 and 6 i=1 ei â‰¥ 1 or e0 = 0 and 6 i=1 ei â‰¥ 3 0 otherwise (1)
For each paper p i in the dataset, we compute the reference precision of the LLM-generated references, denoted as Precision(p i ), as defined in Eq. 2. We then obtain the overall Precision score for each LLM by averaging Precision(p i ) across all papers in the dataset, as shown in Eq. 3.</p>
<p>P recision (p
i ) = 1 N N n=0
True(r g n )</p>
<p>(2)
P reicison = 1 M M i=0 P recison(pi)(3)
Precision is measured by comparing the LLMgenerated references with external academic databases.We also evaluate Recall by comparing the references generated by the LLM with those cited in the human-written original articles.The key difference between precision and recall in our setting lies in the candidate set Z: for precision, Z is constructed from external academic search results, whereas for recall, Z consists of the references actually cited in the human-written articles.</p>
<p>Based on precision and recall, we further compute the F1 score, which serves as a harmonic mean to reflect the overall accuracy of the reference generation.</p>
<p>Additionally, the title is intuitively the most critical element in determining the faithfulness of a generated reference.In the work of Agrawal et al. (2024), ground-truth labels were assigned based on results returned by the Bing Search API.Inspired by their approach, we also calculate the title search score for each LLM to estimate how many generated titles correspond to real publications.
St = 1 M N M N s (n) p i (4) s (n) p i = r g n âˆˆR g i ï£± ï£´ ï£² ï£´ ï£³ 1 if the T âˆˆ r g n has return value from external Scholar API, 0 otherwise (5)
Here, s (n) p i indicates whether the generated title in reference r g n for paper p i can be found using an external academic search engine.This metric helps estimate the proportion of references with verifiable titles among the total generated references.</p>
<p>Context evaluation metrics.In our study, we use the human-written article as the gold truth and then evaluate LLM-generated context from factual consistency and semantic coverage aspects.The resemblance of natural language inference (NLI) to factual consistency evaluation has led to utilizing NLI models for measuring factual consistency (Gao et al., 2023).Encouraged by previous works, we also use the NLI method to evaluate the factual consistency between LLMs generated and humanwritten text.For example, we calculate the NLI score Entail p i between the original article abstract a i and the LLM-generated abstract a g i as follows.
Entailp i = Î¸ NLI (a g i , ai) = 1 if a g i entails ai, 0 otherwise (6)
where Î¸ N LI denotes the NLI model.Finally, we obtain the NLI score Entail for each model according to Eq 7.
Entail = 1 M M i=0 Entailp i (7)
Additionally, we use commonly employed semantic similarity metrics and Key Point Recall (KPR) to calculate the semantic coverage between the context generated by LLMs and human-written context.Specifically, for the Abstract Writing task, we apply cosine similarity and the ROUGE metric for semantic coverage evaluation.For the Review Composition task, we use the ROUGE metric and KPR to measure the semantic coverage of the literature review generated by the LLMs relative to human-written content.KPR, first proposed by Qi et al. (2024), is a metric designed to evaluate the effectiveness of LLMs in utilizing RAG for long documents.Since humanwritten literature reviews are lengthy and difficult to compare directly, we adopt the KPR method to measure the extent to which LLM-generated content covers the key points in human-written literature reviews.Specifically, we first use GPT-4 to extract q key points X i = [x i 1 , x i 2 , ..., x iq ] from the human-written literature review c i , and then calculate the coverage of these key points by the model-generated literature review as Eq. 8.
KP R = 1 M M i=0 xâˆˆX i Î¸ NLI (x, c g i ) |Xi| (8)
where c g i denotes the literature review generated by LLMs.</p>
<p>Finally, we also concatenate key points and compute the ROUGE metric between the key points and c g i .</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Dataset.We collect 1,105 literature review articles published in 2023 from the Annual Reviews website.The distribution of articles across journals is shown in Appendix B, Figure 5. LLMs Selection.We evaluate five LLMs: Claude-3.5-Sonnet-20240620,GPT-4o-2024-08-16, Qwen-2.5-72B-Instruct,DeepSeek-V3, and Llama-3.2-3B-Instruct.All model outputs were generated via their official APIs with temperature set to 0 for consistency.</p>
<p>In Reference Generation and Review Composition tasks, we set N as 10, each model generates 10 references.For the generated reference evaluation, we use Semantic Scholar as the external database.Recently, LLMs-as-judges has become more common (Chen et al., 2024;Zheng et al., 2023;Shangyu et al., 2024).So, for the Abstract Writing task, we employ TRUE (Honovich et al., 2022), along with GPT-4o as NLI models for evaluating factual consistency in context; to compute semantic similarity, we use text-embedding-3-large to convert texts into embeddings.For the Review Composition task, we employ GPT-4o as the NLI model in Eq 8, and set q as 10.</p>
<p>Main Results</p>
<p>We present the results for the three tasks in Table 1, 2, and 3.The performance of different models on each task is analyzed as follows.</p>
<p>Results for Reference Generation.As shown in Table 1, Claude-3.5-Sonnetachieves the highest F 1 score and S t , while Llama-3.2-3Bperforms the worst on both metrics.Notably, Claude-3.5-Sonnetalso achieves the highest precision and recall, indicating that it not only generates more correct references overall but also shares the highest overlap with those cited in the human-written article.When evaluating the author dimension of LLMgenerated references, we consider the reference to match in this dimension if the first author is correctly matched.Applying this criterion results in a 1-3% increase in F1 scores across all models.This suggests that generating complete and accurate author lists remains a major challenge for LLMs.</p>
<p>We further conduct a year-wise analysis of the correctly generated references, as illustrated in Figure 2. The results reveal that the majority of accurate citations produced by the models are concentrated in the period between 2010 and 2020, a trend consistent across nearly all LLMs evaluated in this task.</p>
<p>Results for Abstract Writing.As shown in Table 2, Claude-3.5-Sonnetachieves the best overall performance across most evaluation metrics.It generates abstracts with the highest average semantic similarity to human-written ones (81.17%) and shows strong factual consistency, achieving a TRUE score of 78.10%.DeepSeek-V3 also performs well in factual consistency, with the highest GPT-4o-based assessment score (96.84%).In contrast, Llama-3.2-3Bobtains the highest ROUGE-L score but does not show clear advantages on other metrics.These results highlight the importance of using multiple evaluation metrics to comprehensively assess the diverse outputs of LLMs.</p>
<p>Results for Review Composition.As shown in Table 3, compared to the Reference Generation task, all LLMs demonstrate a significant increase in Precision when generating references within the Review Composition task.Prior research indicates that grounding generated text with real external citations can effectively reduce hallucination rates (Gao et al., 2023).Consistently, our experiments reveal that when LLMs generate references alongside the literature review, the accuracy of these references improves markedly.This suggests a mutual constraint between the generated references and the review text, leading to enhanced overall reliability.</p>
<p>On the other hand, Claude-3.5-Sonnetachieves the highest performance on the KPR metric, indicating that its generated literature reviews recall the greatest number of claims from the human-written versions.Meanwhile, the literature reviews produced by DeepSeek-V3 excel on the ROUGE metrics, demonstrating stronger overlap with reference texts in terms of lexical similarity.</p>
<p>Analyze LLM-Generated References from Different Dimensions</p>
<p>In both Reference Generation and Review Composition tasks, we ask LLMs to generate references.The overall performance was discussed in the previous section.In this section, we provide a detailed across all dimensions in the Reference Generation task.Additionally, the accuracy of reference generation in the Reference Generation task for Claude-3.5-Sonnet,GPT-4o, and Qwen-2.5-72Bfollows a consistent trend across all dimensions, with the highest accuracy observed in the title dimension.Accuracy for journal name, page, and author is also relatively high.However, DeepSeek-V3 performs worse in the author dimension compared to the other dimensions.In contrast, Llama-3.2 demonstrates higher accuracy in the page and author dimensions than in other dimensions.However, overall, Llama-3.2-3Bdoes not exhibit a competitive advantage in reference generation accuracy.Next, we examine the accuracy of LLMgenerated references across various dimensions in Review Composition, as shown in Figure 3(b), and comparing it with Figure 3(a), we observe improvements across all dimensions for Claude-3.5-Sonnet,DeepSeek-V3, GPT-4o, and Qwen-2.5-72B,with particularly obvious gains in the author dimension.The possible reason is that, in the generated text, the LLMs tend to cite the first author's name, which may lead the models to place more emphasis on this dimension.Notably, the accuracy of DeepSeek-V3 and GPT-4o in certain dimensions approaches or even exceeds that of Claude-3.5-Sonnet.However, the performance of LLaMA-3.2-3Bremains suboptimal.</p>
<p>Cross-Disciplinary Analysis</p>
<p>In this section, we compare the performance of LLMs across different disciplines.</p>
<p>in Technology.</p>
<p>We then present bar charts in Figures 4, which illustrate the performance of different models across various tasks and disciplines.</p>
<p>First, we observe that in the Reference Generation task, as shown in Figure 4(a), almost all models exhibit the highest precision in the Mathematics discipline and the lowest precision in the Chemistry discipline.To validate these differences, we conduct one-way ANOVA tests for each LLM across five disciplines.Significant differences are found for all models except Llama-3.2-3B.Detailed ANOVA results are reported in Appendix G.</p>
<p>Secondly, as shown in Figure 4(b), the NLI scores evaluated by TRUE in the Abstract Writing task indicate that all models perform the worst in Social Science.GPT-4o performs best in Technology, while Claude 3.5-Sonnet achieves the highest performance in Biology.One-way ANOVA tests reveal significant differences across disciplines for all models.See Appendix G for detailed results.Thirdly, we examine the references precision of each model across five disciplines in the Review Composition task, as illustrated in Figure 4(c).It is evident that the precision of Claude 3.5 Sonnet, DeepSeek-V3, and GPT-4o is significantly higher than that of Qwen-2.5-72B and LLaMA-3.2-3Bacross all disciplines.Furthermore, Claude 3.5 Sonnet, DeepSeek-V3, and Qwen-2.5-72Bexhibit the highest precision in Mathematics, while GPT-4o performs best in Social Science.ANOVA tests confirm significant differences across disciplines for all models (see Appendix G).</p>
<p>Finally, we observe the KPR metric across different disciplines in Review Composition, as shown in Figure 4(d).The results from the figure indicate that the differences between models-Claude- 3.5-Sonnet, DeepSeek-V3, GPT-4o, and Qwen-2.5-72B-arenot significant across various disciplines, a finding that is also supported by statistical tests (see Appendix G).</p>
<p>Citation Frequency and Precision Across Disciplines.Additionally, we report statistics on the citation frequency of correctly generated references by LLMs in the Reference Generation task, as shown in Table 4, using Claude-3.5 and DeepSeek-V3 as examples.The data indicates that the references generated by the LLMs are highly cited, which might be due to their frequent presence in online sources, making them more likely to appear in the LLMs training datasets.As a result, LLMs tend to generate more accurate metadata (e.g., author, year) for these well-known references.</p>
<p>Furthermore, when analyzing different disciplines, we observe that the Mathematics discipline has the highest precision, and the relevant references generated for Mathematics also have the highest citation count.We compute the correlation between citation precision and average citation counts, finding that the correlation coefficient for Claude-3.5 is 0.4, and for DeepSeek-V3 it is 0.51, indicating a positive relationship between the two.</p>
<p>Human Evaluation</p>
<p>To evaluate the reliability of our automatic assessment method for identifying hallucinated refer-ences, we conduct a comparative analysis involving 100 LLM-generated references.These references were assessed by three annotators and the final manual results were obtained by majority vote.The results demonstrated a kappa agreement of 0.71 between the automatic and human assessments, signifying a relatively high level of consistency and supporting the reliability of our method.Furthermore, when using human assessment results as the gold standard, the automatic assessment method achieved an accuracy of 86%, further validating its effectiveness.</p>
<p>Conclusion</p>
<p>In this paper, we present a framework to assess the literature review writing abilities of LLMs.This framework includes three tasks designed to evaluate LLMs' literature review writing capabilities.The generated outputs are then evaluated from multiple dimensions using various tools, such as Semantic Scholar and NLI models, focusing on aspects like hallucination rate, semantic coverage, and factual consistency compared to humanwritten texts.Finally, we analyze the performance of LLMs in writing literature reviews from the perspective of different academic disciplines.</p>
<p>Limitations</p>
<p>In this paper, we evaluate the ability of LLMs to write literature reviews.However, several limitations remain: First, instead of evaluating the generated reviews from conventional perspectives such as fluency or topic coverage, we primarily compare LLMgenerated results with human-written ones.As such, our current evaluation metrics may not be comprehensive.In the future, we plan to incorporate additional aspects of review quality to improve the completeness of our evaluation.These may include the coverage of cited works (i.e., whether the review offers a comprehensive overview of the relevant field) and the coherence of the overall structure (i.e., whether the review is organized in a way that facilitates information-seeking).</p>
<p>Second, there is a possibility that our test data overlaps with the training data of the LLMs.When we initiated this study in August 2024, the dataset from the Annual Reviews website had not yet been updated to include 2024 articles, so we relied on the complete 2023 dataset.To mitigate potential data leakage, we plan to deploy a leaderboard on Hugging Face to continuously evaluate the performance of various LLMs in literature review writing, with real-time updates to the test dataset.However, due to the rapid iteration of LLMs, data leakage cannot be completely ruled out.That said, our experimental results-particularly those related to reference generation-show that all models still perform poorly.If data contamination were present, the actual scores would likely be lower than those reported.This reinforces, rather than undermines, our conclusion that significant challenges remain in using LLMs for literature review generation.</p>
<p>Additionally, when processing LLM-generated outputs, we frequently encountered abbreviated author names and journal titles.Although we have taken care to address these issues thoroughly (see Appendix E), minor discrepancies may still exist.</p>
<p>Finally, to verify the precision of LLMgenerated references, we primarily used Semantic Scholar as our auxiliary tool.Although we also experimented with Google Scholar, its lack of an accessible API led us to rely on the freely available Semantic Scholar API for consistency and ease of access.However, this may have resulted in incomplete reference retrieval.</p>
<p>B Data Distribution</p>
<p>Statistics of the dataset are shown in Figure 5.</p>
<p>C Comparison of LLM-Cited and</p>
<p>Human-cited References from Different Dimensions</p>
<p>We provide a more detailed comparison of the LLM-cited and human-cited references across various dimensions.As shown in Figure 6, for Reference Generation, we observe that the overlap rate is higher in the "Title" and other numerical dimensions, while the overlap rates for the "Journal" and "Author" dimensions are relatively lower.For Review Composition, Claude-3.5-Sonnetand GPT-4o exhibit a higher overlap rate on the "Author" dimension compared to Reference Generation.This trend is consistent with the findings in Figure 6, as the citation of author names in the literature for Review Composition leads to the generation of more accurate author information.</p>
<p>D Discussion</p>
<p>We select four LLMs for task evaluation and find that Claude-3.5-Sonnetoutperforms DeepSeek-V3, GPT-4o, Qwen-2.5-72B, and Llama-3.2-3Bacross all three tasks, particularly excelling in the task of generating accurate references.This advantage is likely influenced by the training data of each model.Additionally, we observed that each model has different strengths across disciplines.Overall, for the reference generation task, nearly all models perform better in Mathematics, while their performance is weaker in Chemistry and Technology.However, when writing abstracts, all models exhibit the lowest factual consistency in Social Science, as indicated by the entailment scores, compared to human-written texts.</p>
<p>When comparing the references generated by the models in Reference Generation and Review Composition, we find that in Review Composition, nearly all models generate more accurate references.This suggests that LLMs cite references during the writing process, which improves the authenticity of the references.Moreover, the inclusion of the first author's name in the generated context also enhances the accuracy of the author dimension.</p>
<p>In the future, we will deploy this evaluation framework on Hugging Face to enable real-time assessments of newly released LLMs.</p>
<p>E Data Processing Strategy</p>
<p>Author name and journal title variations often pose challenges when aligning LLM-generated references with articles from Semantic Scholar.To address this, we adopt the following normalization strategies:</p>
<p>Author names.When comparing author names between LLM-generated references and candidate articles, if an exact match is not found (e.g., "John Smith"), we consider common variants such as "Smith, John", "Smith, J.", or "J.Smith" to account for different citation formats.</p>
<p>Journal titles.For journal names like Journal of Chemical Physics, we incorporate standard abbreviation forms (e.g., J. Chem.Phys.) based on widely used abbreviation conventions.Nonetheless, certain non-standard or ambiguous cases may still be unmatched.</p>
<p>F LLM Output Example</p>
<p>Here is one example of the output generated by Claude-3.5-Sonnetfor the literature review composition task.</p>
<p>{"Literature Review": "CRISPR-Cas systems have emerged as a powerful tool in the field of biosensing, offering unprecedented capabilities for detecting nucleic acids, proteins, and small molecules.This review explores the technical developments and application prospects of CRISPRbased biosensing strategies, highlighting their potential to revolutionize diagnostics and molecular detection.The core biochemical properties of CRISPR-Cas systems provide the foundation for their application in biosensing.These properties include diverse reaction temperatures, programmability in design, high reaction efficiency, and recognition specificity.Chen et al. (2018) demonstrated the versatility of CRISPR-Cas9 in detecting DNA targets across a wide temperature range, from 20Â°C to 60Â°C, enabling its use in various environmental conditions.The programmability of CRISPR systems allows for the easy design of guide RNAs (gRNAs) to target specific sequences, as shown by Gootenberg et al. (2017) in their development of the SHERLOCK (Specific High-sensitivity Enzymatic Reporter unLOCKing) platform for nucleic acid detection.The high reaction efficiency of CRISPR-Cas systems contributes to their sensitivity in biosensing applications.Li et al. ( 2019) reported a CRISPR-Cas12a-based assay capable of detecting attomolar concentrations of DNA targets, demonstrating the potential for ultrasensitive detection.The recognition specificity of CRISPR systems allows for precise discrimination between closely related sequences, as evidenced by the work of Myhrvold et al. ( 2018), who developed a CRISPR-based diagnostic platform capable of distinguishing between Zika and Dengue virus strains with single-base resolution.Recent efforts have focused on improving these core properties to enhance the performance of CRISPR-based biosensors.For example, Harrington et al. ( 2018) engineered Cas12a variants with improved activity and specificity, expanding the range of targetable sequences and reducing off-target effects.Similarly, Gootenberg et al. (2018) developed an enhanced version of the SHERLOCK system, incorporating lateral flow readouts and multiplexing capabilities, further improving its utility in point-of-care diagnostics.One of the key areas of technical development in CRISPR-based biosensing is improving sensitivity and quantification capabilities.Wang et al. (2020) introduced a CRISPR-Cas12a-based digital PCR method that achieved attomolar sensitivity and precise quantification of target nucleic acids.This approach combines the specificity of CRISPR recognition with the quantitative power of digital PCR, opening new avenues for highly sensitive and accurate molecular diagnostics.Multiplex detection is another important aspect of CRISPR biosensing development.Ackerman et al. (2020) demonstrated a CRISPR-Cas13-based multiplexed assay capable of simultaneously detecting and distinguishing multiple SARS-CoV-2 variants, showcasing the potential of CRISPR technology in rapid and accurate pathogen identification.The ability to detect multiple targets in a single reaction not only saves time and resources but also provides more comprehensive diagnostic information.Efforts to develop convenient one-pot assays have led to simplified and more user-friendly CRISPR-based detection methods.Joung et al. ( 2020) reported a one-pot, CRISPR-Cas12-based assay for SARS-CoV-2 detection that could be performed at a single temperature, eliminating the need for complex thermal cycling equipment.This advancement brings CRISPR-based diagnostics closer to pointof-care applications, where simplicity and ease of use are crucial.The creation of advanced sensors has further expanded the capabilities of CRISPRbased biosensing.Bruch et al. ( 2019) developed a CRISPR-Cas13a-based electrochemical sensor for nucleic acid detection, demonstrating the potential for integrating CRISPR technology with electronic readout systems.This approach offers the possibility of developing portable, electronic CRISPRbased diagnostic devices for rapid and sensitive detection in resource-limited settings.While CRISPRbased biosensing has shown great promise, there are still obstacles to its widespread commercial application.These include challenges in sample preparation, the need for standardization across different CRISPR systems and assay formats, and regulatory considerations for diagnostic use.Addressing these issues will be crucial for the successful translation of CRISPR biosensing technology from the laboratory to real-world applications.Looking ahead, the future of CRISPR-based biosensing appears bright, with numerous opportunities for further development and application.Potential directions include the integration of CRISPR biosensors with microfluidic devices for automated sample processing and analysis, the development of CRISPR-based wearable sensors for continuous health monitoring, and the application of machine learning algorithms to improve the design and performance of CRISPR biosensing systems.In conclusion, CRISPR-based biosensing strategies represent a rapidly evolving field with immense potential to transform molecular detection and diagnostics.As researchers continue to refine and expand upon the core capabilities of CRISPR systems, we can expect to see increasingly sensitive, specific, and versatile biosensing applications emerge, with far-reaching implications for healthcare, environmental monitoring, and beyond.","References": [{"title": "CRISPR-Cas12a target binding unleashes indiscriminate singlestranded DNase activity", "authors": "Chen, Janice S., Ma, Enbo, Harrington, Lucas B., Da Costa, Maria, Tian, Xiang, Palefsky, Joel M.,</p>
<p>Figure 1 :
1
Figure 1: Illustration of the evaluation framework.</p>
<p>Figure 2 :
2
Figure 2: Distribution of LLM-generated true references over years.</p>
<p>Figure 3: Radar chart of the accuracy of LLM-generated references across various dimensions.</p>
<p>Figure 4: Three tasks evaluation scores across different disciplines.</p>
<p>Figure 5 :
5
Figure 5: Statistics of dataset.</p>
<p>Figure 6 :
6
Figure 6: Radar chart of the accuracy of LLM-generated references with human-written references in the original article.</p>
<p>Table 1 :
1
The experimental results of the five LLMs in Reference Generation."S t " refers to the title search rate as defined in Eq 4, while "P" represents the Precision, "R" denotes the Recall."first author" refers to when evaluating the accuracy of references, the author dimension only comparing the first author.
First, based on</p>
<p>Table 2 :
2
Compare the performance of four LLMs on Abstract Writing.
Review CompositionModelsReferencesLiterature ReviewStâ†‘PRF1PR F1(first author) KPRâ†‘ ROUGE-1â†‘ ROUGE-2â†‘ ROUGE-Lâ†‘Qwen-2.5-72B 40.02 28.91 17.36 21.69 33.64 18.3123.71338.8229.959.0115.14Llama-3.2-3B21.78 4.86 8.28 6.12 7.28 8.447.8229.0728.077.7715.46DeepSeek-V362.29 52.81 26.79 35.55 55.38 27.3036.5756.0235.6510.4017.46GPT-4o60.05 50.62 27.88 35.96 54.16 28.8637.6559.1830.789.7215.54Claude-3.5-Sonnet 66.43 59.06 31.90 41.42 63.06 33.2543.5462.3228.598.9014.41</p>
<p>Table 3 :
3
The experimental results of the four LLMs in Review Composition."S
DisciplineCitation Count DeepSeek Claude DeepSeek Claude PrecisionBiology76367855.5558.00Mathematics2288198460.0062.22Physics89465247.6256.19Chemistry1334107943.1443.80Social Science 1321115146.8056.70Technology90474844.8849.01
t " refers to the title retrieval rate as defined in Eq 4. While "P" represents the Precision, "R" denotes the Recall."KPR" means the Key Point Recall rate."firstauthor" refers to when evaluating the accuracy of references, the author dimension only comparing the first author.</p>
<p>Table 4 :
4
Average citation counts and reference precision across disciplines.</p>
<p>https://anonymous.4open.science/r/Eval_LLM_ LR-C657
Ethics StatementThe human evaluations conducted in this study were carried out by members of the research team.No personal or sensitive information was collected, and all participants were fully informed of the purpose of the evaluation.Therefore, the study does not raise any ethical concerns.A Prompts for TasksPromptContent Prompt 1Imagine you are an experienced academic researcher with access to a vast library of scientific literature.I would like you to find the 10 studies that are most relevant to the research topic provided in the "Title" and the "Keywords" below.Please cite the studies according to the following JSON format.There is no need to provide any explanation before or after the JSON output.Ensure that the "authors" field lists the names of all authors and not exceeding 10 authors, and that there are no duplicate author names nor abbreviations such as "et al.".{ "References": [ { "title": "", "authors": "", "journal": "", "year": "", "volumes": "", "first page": "", "last page": "", } ] } Title: title Keywords: keywords Prompt 2Imagine you are an experienced academic researcher with access to a vast library of scientific literature.I would like you to write an abstract according to the research topic provided in the "Title" and the "Keywords" below.Please write the abstract for about xx words, according to the JSON format as follows.There is no need to provide any explanation before or after the JSON output.{"Abstract": ""} Title: title Keywords: keywords Prompt 3Imagine you are an experienced academic researcher with access to a vast library of scientific literature.I would like you to write a literature review according to the research topic provided in the "Title", "Abstract" and "Keywords" below.The literature review should be about 1000 words long.I would like you to back up claims by citing previous studies (with a total of 10 citations in the literature review).The output should be in JSON format as follows: { "Literature Review": "xxx", "References": [ { "title": "", "authors": "", "journal": "", "year": "", "volumes": "", "first page": "", "last page": "", } ] } The "Literature Review" field should be about 1000 words.The "References" field is a list of 10 references, and ensures that the "authors" field lists the names of all authors and not exceeding 10 authors, and that there are no duplicate author names nor abbreviations such as "et al.".Title: title Keywords: keywords Abstract: abstract
Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, 10.48550/arXiv.2412.15249arXiv:2412.15249ArXiv:2412.15249Llms for literature review: Are we there yet?. 2024</p>
<p>Do language models know when they're hallucinating references?. Ayush Agrawal, Mirac Suzgun, Lester Mackey, Adam Tauman, Kalai , 2024</p>
<p>The canonical model of structure for data extraction in systematic reviews of scientific research articles. Muhammad Bello Aliyu, Rahat Iqbal, Anne James, 10.1109/SNAMS.2018.85548962018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS). 2018</p>
<p>Reference hallucination score for medical artificial intelligence chatbots: Development and usability study. Mohamad-Hani Fadi Aljamaan, Ibraheem Temsah, Ayman Altamimi, Amr Al-Eyadhy, Khalid Jamal, Alhasan, A Tamer, Mohamed Mesallam, Khalid H Farahat, Malki, 10.2196/54345Company: JMIR Medical Informatics Distributor: JMIR Medical Informatics Institution: JMIR Medical Informatics Label: JMIR Medical Informatics publisher. Toronto, CanadaJMIR Publications Inc202412e54345</p>
<p>Exploring the boundaries of reality: Investigating the phenomenon of artificial intelligence hallucination in scientific writing through chatgpt references. Anirudh Sai, ; Athaluri, Manoj V S R Krishna, Vineel Kesapragada, Tirth Yarlagadda, Rama Dave, Siri Tulasi, Duddumpudi, 10.7759/cureus.37432Sandeep Varma Manthena. 202315e37432</p>
<p>Hallucination rates and reference accuracy of chatgpt and bard for systematic reviews: Comparative analysis. MikaÃ«l Chelli, Jules Descamps, Vincent LavouÃ©, Christophe Trojani, Michel Azar, Marcel Deckert, Jean-Luc Raynier, Gilles Clowez, Pascal Boileau, Caroline Ruetsch-Chelli, 10.2196/53164Journal of Medical Internet Research. 26e531642024</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, 10.48550/arXiv.2402.10669arXiv:2402.10669ArXiv:2402.10669Humans or llms as the judge? a study on judgement biases. 2024</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, 10.48550/arXiv.2305.14627arXiv:2305.14627ArXiv:2305.146272023</p>
<p>TRUE: Re-evaluating factual consistency evaluation. Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias, 10.18653/v1/2022.naacl-main.287Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>CHIME: LLM-assisted hierarchical organization of scientific studies for literature review support. Chao-Chun, Erin Hsu, Jenna Bransom, Bailey Sparks, Chenhao Kuehl, David Tan, Lucy Wadden, Aakanksha Wang, Naik, 10.18653/v1/2024.findings-acl.8Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Using a neural network-based feature extraction method to facilitate citation screening for systematic reviews. Georgios Kontonatsios, Sally Spencer, Peter Matthew, Ioannis Korkontzelos, 10.1016/j.eswax.2020.100030Expert Systems with Applications: X. 61000302020</p>
<p>Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu Li, 10.48550/arXiv.2502.14776arXiv:2502.14776ArXiv:2502.14776Surveyx: Academic survey automation via large language models. 2025</p>
<p>Long2rag: Evaluating long-context &amp; long-form retrieval-augmented generation with key point recall. Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, Wei Xu, 10.18653/v1/2024.findings-emnlp.279Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Efuf: Efficient fine-grained unlearning framework for mitigating hallucinations in multimodal large language models. Xing Shangyu, Zhao Fei, An Wu Zhen, Chen Tuo, Li Weihao, Zhang Chunhui, Dai Jianbing, Xinyu, 2024</p>
<p>Evaluating large language models on academic literature understanding and review: An empirical study among early-stage scholars. Jiyao Wang, Haolong Hu, Zuyuan Wang, Song Yan, 10.1145/3613904.3641917Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing SystemsHonolulu HI USAACM2024aYouyu Sheng, and Dengbo He</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric P Li, Hao Xing, Joseph E Zhang, Ion Gonzalez, Stoica, 10.48550/arXiv.2306.05685arXiv:2406.10252ArXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2024b. 2023</p>
<p>CRISPR-Cas13-based electrochemical biosensing of viral RNA: Application to detection of SARS-CoV-2. Jennifer A Doudna, Jonathan S Gootenberg, Omar O Abudayyeh, Jeong Lee, Wook, Essletzbichler, Patrick, Aaron J Dy, Joung, Julia, Verdine, Vanessa, Donghia, Nina, Nichole M Daringer, Catherine A Freije, Li, Suwei, Cheng, Qingmei, Wang, Jianming, Li, Xiaoyu, Zhang, Zhiwei, Gao, Shan, Cao, Rong, Zhao, Guoping, Jin ; Wang, Myhrvold, Cameron, Catherine A Freije, Jonathan S Gootenberg, Omar O Abudayyeh, Hayden C Metsky, Ann F Durbin, Max J Kellner, Amanda L Tan, Lauren M Paul, Leda A Parham, Lucas B Harrington, Paez-Espino, David, Staahl, T Brett, Janice S Chen, Ma, Enbo, Nikos C Kyrpides, Jennifer A Doudna, Jonathan S Gootenberg, Omar O Abudayyeh, Max J Kellner, Joung, Julia, James J Collins, Feng ; Zhang, Wang, Xiaoxia, Zhong, Minjie, Liu, Yue, Ma, Pengfei, Dang, Lei, Meng, Qing, Wan, Wanying, Ma, Xiaowei, Liu, Jing, Guohua ; Yang, Julia Joung, Ladha, Alim, Saito, Makoto, Kim, Nam-Gyun, Ann E Woolley, Segel, Michael, Barretto, P J Robert, Ranu, Antonija, Rhiannon K Macrae, Faure, Cheri M Guilhem ; Ackerman, Myhrvold, Cameron, Thakku, G Shiv, Catherine A Freije, Hayden C Metsky, David K Yang, Simon H Ye, Chloe K Boehm, Kosoko-Thoroddsen, F Tinna-Solveig, Jared Kehe, Scalable and robust SARS-CoV-2 testing in an academic center. Bruch, Richard, Baaske, Johannes, Chatelle, Claire, Meirich, Maren, Madlener, Sibylle, Weber, Wilfried, Dincer, Can, Urban, Gerald A.Field-deployable viral diagnostics using CRISPR-Cas13. journal": "Nature Biotechnology", "year": "2020", "volumes": "38", "first page": "927", "last page": "931"}]} G ANOVA Test Results We report the p-values from one-way ANOVA tests across disciplines for each model and task: â€¢ Reference Generation task: Claude-3.5-Sonnet (p&lt;.0001), DeepSeek-V3(p&lt;.0001), GPT-4o(p&lt;.0001), Qwen-2.5-72B (p&lt;.0001), Llama-3.2-3B (p=0.065</p>
<p>Qwen2.5-72B (p&lt;.001Abstract Writing task (NLI scores): Claude-3.5-Sonnet(p&lt;.0001), DeepSeek-V3(p&lt;0.05). Llama-3.2-3B (p&lt;.0001</p>
<p>Claude-3.5-Sonnet(p&lt;.0001), DeepSeek-V3(p&lt;.0001). Qwen-2.5- 72B(p&lt;.0001Review Composition (Reference accuracy. Llama-3.2-3B(p&lt;.001</p>
<p>DeepSeek-V3 (p=0.23), GPT-4o. Claude-3.5-Sonnet (p=0.46). Review Composition (KPR metric. p=0.18), Qwen-2.5-72B (p=0.10), and Llama-3.2-3B (p&lt;0.001</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3110 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3110</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3110</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-2a3b9412c096458af219d11d7c2d16433255d0d9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2a3b9412c096458af219d11d7c2d16433255d0d9" target="_blank">OccamNet: A Fast Neural Model for Symbolic Regression at Scale</a></p>
                <p><strong>Paper TL;DR:</strong> OccamNet is introduced, a neural network model that finds interpretable, compact, and sparse symbolic fits to data, a la Occam's razor, and defines a probability distribution over functions with efficient sampling and function evaluation.</p>
                <p><strong>Paper Abstract:</strong> Neural networks' expressiveness comes at the cost of complex, black-box models that often extrapolate poorly beyond the domain of the training dataset, conflicting with the goal of finding compact analytic expressions to describe scientific data. We introduce OccamNet, a neural network model that finds interpretable, compact, and sparse symbolic fits to data, \`a la Occam's razor. Our model defines a probability distribution over functions with efficient sampling and function evaluation. We train by sampling functions and biasing the probability mass toward better fitting solutions, backpropagating using cross-entropy matching in a reinforcement-learning loss. OccamNet can identify symbolic fits for a variety of problems, including analytic and non-analytic functions, implicit functions, and simple image classification, and can outperform state-of-the-art symbolic regression methods on real-world regression datasets. Our method requires a minimal memory footprint, fits complicated functions in minutes on a single CPU, and scales on a GPU.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3110",
    "paper_id": "paper-2a3b9412c096458af219d11d7c2d16433255d0d9",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0061285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>OccamNet: A Fast Neural Model for Symbolic Regression at Scale</h1>
<p>Owen Dugan<em>, Rumen Dangovski</em>, Allan Costa<em>, Samuel Kim, Pawan Goyal, Joseph Jacobson, Marin Soljačić<br>Massachusetts Institute of Technology<br></em> denotes equal contribution</p>
<p>November 29, 2023</p>
<h4>Abstract</h4>
<p>Neural networks' expressiveness comes at the cost of complex, black-box models that often extrapolate poorly beyond the domain of the training dataset, conflicting with the goal of finding compact analytic expressions to describe scientific data. We introduce OccamNet, a neural network model that finds interpretable, compact, and sparse symbolic fits to data, à la Occam's razor. Our model defines a probability distribution over functions with efficient sampling and function evaluation. We train by sampling functions and biasing the probability mass toward better fitting solutions, backpropagating using cross-entropy matching in a reinforcement-learning loss. OccamNet can identify symbolic fits for a variety of problems, including analytic and non-analytic functions, implicit functions, and simple image classification, and can outperform state-of-the-art symbolic regression methods on real-world regression datasets. Our method requires a minimal memory footprint, fits complicated functions in minutes on a single CPU, and scales on a GPU.</p>
<h2>1 Introduction</h2>
<p>Deep learning has revolutionized a variety of complex tasks, ranging from language modeling to computer vision [1]. Key to this success is designing a large search space in which many local minima sufficiently approximate given data [2]. This requires large, complex models, which often conflicts with the goals of sparsity and interpretability, making neural nets not optimally suited for a myriad of physical and computational problems with compact and interpretable underlying mathematical structures [3]. Neural networks also might not preserve desired physical properties (e.g., time invariance) and are typically unable to generalize much beyond observed data.</p>
<p>In contrast, Evolutionary Algorithms (EAs), in particular genetic programming, can find interpretable, compact models that explain observed data [4-6]. EAs have been employed as an alternative to gradient descent for optimizing neural networks in what is known as neuroevolution [7-9]. Recently, evolutionary strategies that model a probability distribution over parameters, updating this distribution according to their own best samples (i.e., selecting the fittest), were found advantageous for optimization on high-dimensional spaces, including neural networks' hyperparameters [10, 11].</p>
<p>A number of evolution-inspired, probability-based models have been explored for Symbolic Regression [12]. Along these lines, Petersen et al. [13] explore deep symbolic regression by using an RNN to define a probability distribution over a space of expressions and sample from it using autoregressive expression generation. More recently, Biggio et al. [14] have pretrained Transformer models that receive input-output pairs as input and return functional forms that could fit the data. In the related field of program synthesis, probabilistic program induction using domain-specific languages [15-17] has proven successful. Balog et al. [18] first train a machine learning model to predict a DSL based on input-output pairs and then use methods from satisfiability modulo theory [19] to search the space of programs built using the predicted DSL.</p>
<p>One approach to symbolic regression which can integrate well with deep learning is the Neural Arithmetic Logic Unit (NALU) and related models [20, 21], which provide neural inductive bias for arithmetic in neural</p>
<p>networks by shaping a neural network towards a gating interpretation of the linear layers. Neural Turing Machines [22, 23] and their stable versions [24] can also discover interpretable programs, simulated by neural networks connected to external memory, via observations of input-output pairs. Another option is Equation Learner (EQL) Networks [25-27], which identify symbolic fits to data by training a neural network with symbolic activation functions, such as multiplication or trigonometric functions. However, these methods require strong regularization to be interpretable. NALUs and to a lesser extent EQL Networks can also only use a restricted set of differentiable primitive functions, and Neural Turing Machines do not include the concept of a "primitive." Additionally, these methods often converge to local minima and often converge to uninterpretable models unless they are carefully regularized for sparsity.</p>
<p>In this paper, we consider a mixed approach of connectionist and sample-based optimization for symbolic regression. We propose a neural network architecture, OccamNet, which preserves key advantages of EQL networks and other neural-integrable symbolic regression frameworks while addressing many of these architectures' limitations. Inspired by neuroevolution, our architecture uses a neural network to model a probability distribution over functions. We optimize the model by sampling to compute a reinforcementlearning loss, tunable for different tasks, based on the training method presented in Risk-Seeking Policy Gradients [13]. Our method handles non-differentiable and implicit functions, converges to sparse, interpretable symbolic expressions, and can work across a wide range of symbolic regression problems. Further, OccamNet consistently outperforms other symbolic regression algorithms in testing on real-world regression datasets. We also introduce a number of strategies to induce compactness and simplicity a la Occam's Razor.</p>
<p>The main goal of this study is not to replace existing symbolic regression methods, but rather to create a novel hybrid approach that combines the strengths of neural networks and evolutionary algorithms. Our proposed OccamNet method consistently achieves state-of-the-art performance across a wide range of tasks, including a diverse range of synthetic functions, simple programs, raw data classification, and real-world tabular tasks. Additionally, we show how to connect OccamNet to state-of-the-art pretrained vision models, such as ResNets [28]. OccamNet has also shown promise in discovering quantitative and formal laws in social sciences, indicating its potential to aid scientific research [29]. By striking a delicate balance between expressiveness and interpretability, OccamNet presents a versatile and powerful solution for symbolic regression challenges.</p>
<h1>2 Model Architecture</h1>
<p>In Figure 1 we sketch the OccamNet architecture and the method for training it, before following with a more detailed description. We can view OccamNet as a fully-connected feed-forward network (a stack of fully connected linear layers with non-linearities) with two key unique features. First, the parameters of the linear layer are substituted with a learned probability distribution associated with the neurons from the preceding layer for each neuron within the layer. Second, the non-linearities form a collection of symbolic expressions. Thus we obtain a collection of "symbolic layers" that form OccamNet (Figure 1a). Figure 1b shows a variety of symbolic expressions, representing paths within OccamNet from sampling each probability distribution independently. Figure 1c shows OccamNet's training objective, which increases the probability of the paths that are closest to the ground truth. Below we formalize OccamNet in detail.</p>
<h3>2.1 Layer structure</h3>
<p>A dataset $\mathcal{D}=\left{\left(\vec{x}<em p="p">{p}, \vec{y}</em>\right)\right}<em p="p">{p=1}^{|\mathcal{D}|}$ consists of pairs of inputs $\vec{x}</em>}$ and targets $\vec{y<em p="p">{p}=\vec{f}^{<em>}\left(\vec{x}<em _0_="(0)">{p}\right)=\left[f</em>^{</em>}\left(\vec{x}</em>^{}\right), \ldots, f_{(v-1)<em>}\left(\vec{x}<em _i_="(i)">{p}\right)\right]^{\top}$. Our goal is to compose either $f</em>^{</em>}(\cdot)$ or an approximation of $f_{(i)}^{*}(\cdot)$ using a predefined collection of $N$ primitive functions $\boldsymbol{\Phi}=\left{\phi_{i}(\cdot, \ldots, \cdot)\right}_{i=1}^{N}$. Note that primitives can be repeated, their arity (number of arguments) is not restricted to one, and they may operate over different domains. The concept of a set of primitives $\Phi$ is similar to that of DSL, domain-specific languages [30].</p>
<p>To solve this problem, we follow a similar approach as in EQL networks [26, 25, 27], in which the primitives act as activation functions on the nodes of a neural network. Specifically, each hidden layer consists of an arguments sublayer and an images sublayer, as shown in Figure 2a. We use this notation because the arguments sublayer holds the inputs, or arguments, to the activation functions and the images sublayer holds the outputs, or images, of the activation functions. The primitives are stacked in the images sublayer and act</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: OccamNet architecture and training. a. OccamNet is a stack of "symbolic layers" each described by a collection of learned distributions (over the neurons from the previous layer) for each neuron within the layer, as well as non-linearities that are collections of symbolic expressions. b. By sampling from each distribution independently, we are able to sample paths from OccamNet that represent symbolic expressions, ready for evaluation. c. We evaluate each expression by feeding the observations' support data and comparing the outputs with the ground truth. The probability of the best paths is increased and the process is repeated until convergence.</p>
<p>as activation functions for their respective nodes. Each primitive takes in nodes from the arguments sublayer. Additionally, we use skip connections similar to those in DenseNet [31] and ResNet [28], concatenating image states with those of subsequent layers.</p>
<p>Next, we introduce a probabilistic modification of the network: instead of computing the inputs to the arguments sublayers using dense feed-forward layers, we compute them probabilistically and sample through the network. This enables many key advantages: it enforces sparsity and interpretability without requiring regularization, it allows the model to avoid backpropagating through the activation functions, thereby allowing non-differentiable and fast-growing functions in the primitives, and it helps our model avoid premature convergence to local minima.</p>
<p>Because they behave probabilistically, we call nodes in the arguments sublayer <em>P-nodes</em>. Figure 2 highlights this sublayer structure, while the methods section describes the complete mathematical formalism behind it.</p>
<h3>2.2 Temperature-controlled connectivity</h3>
<p>Instead of dense linear layers, we use <em>T-softmax layers</em>. For any temperature <em>T</em> &gt; 0, we define a <em>T</em>-softmax layer as a standard <em>T</em>-controlled softmax layer with weighted edges connecting an images sublayer and the subsequent arguments sublayer, in which each P-node from the arguments sublayer probabilistically samples a single edge between itself and a node in the images sublayer. Each node's sampling distribution is given by</p>
<p>$$\mathbf{p}^{(l,i)}(T_l) = \text{softmax}(\mathbf{w}^{(l,i)};T_l),$$</p>
<p>where <strong>w</strong>(<em>l,i</em>) and <strong>p</strong>(<em>l,i</em>) are the weights and probabilities for edges leading to the <em>i</em>th P-node of the <em>l</em>th layer and <em>T<sub>l</sub></em> is the fixed temperature for the <em>l</em>th layer. Selecting these edges for all <em>T</em>-softmax layers produces a sparse directed acyclic graph (DAG) specifying a function <em>f̂</em>, as seen in Figure 2b. While controlling the temperature adjusts the entropy of the distributions over nodes, OccamNet automatically enforces sparsity by sampling a single input edge to each P-node. Adjusting the temperature has no impact on sparsity, but it allows for balancing exploration and exploitation during training.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) A two-output network model with depth ( L = 2 ), ( \vec{x} = [x_0, x_1] ), user-selected constants ( \mathcal{C} = [1, \pi] ), and set of primitive functions ( \Phi = (+(\cdot, \cdot), \sin(\cdot), \exp(\cdot), \times(\cdot, \cdot)) ). Boxed in blue are the arguments sublayers (composed of P-nodes). For each arguments sublayer, the associated image sublayer (composed of the basis functions from ( \Phi )) is boxed in green and to the right of the corresponding arguments sublayer. Together, these two sublayers define a single hidden layer of our model. The input layer can be thought of as an image layer and the output layer can be thought as an arguments layer. (b) An example of function-specifying directed acyclic graphs (DAGs) that can be sampled from the network in (a). These DAGs represent the functions ( y_0 = \exp[\sin(x_0 + \pi)] ) and ( y_1 = \sin(\exp(x_0) \sin(x_1)) ).</p>
<h3>2.3 A neural network as a probability distribution over functions</h3>
<p>Through the temperature-controlled connectivity described above, OccamNet can be sampled to produce DAGs corresponding to functions ( \vec{f} ). Based on the weights of OccamNet, some DAGs may be sampled with higher or lower probability. In this way, OccamNet can be considered as representing a probability distribution over the set of all possible DAGs, or equivalently over all possible functions sample-able from OccamNet.</p>
<p>Let ( \mathbf{W} = { \mathbf{w}^{(l,i)}; 1 \leq l \leq L, 1 \leq i \leq N } ). The probability of the model sampling ( f_{(i)} ) as its ( i )th output, ( q_i(f_{(i)} | \mathbf{W}) ), is the product of the probabilities of the edges of ( f_{(i)} )'s DAG. Similarly, ( q(\vec{f} | \mathbf{W}) ), the probability of the model sampling ( \vec{f} ), is given by the product of ( \vec{f} )'s edges, or ( q(\vec{f} | \mathbf{W}) = \prod_{i=0}^{v-1} q_i(f_{(i)} | \mathbf{W}) ). For example, in Figure 2b, the probabilities of sampling the DAG shown is given by the product of the probabilities sampling each of the edges shown.</p>
<p>In practice, we compute an approximation of this probability which we denote ( q_{apx} ), as described in Methods Section 6.1.3. We find that OccamNet performs well with this approximation. For all other sections of this paper, unless explicitly mentioned, we use ( q ) to mean ( q_{apx} ).</p>
<p>We initialize the network with weights ( \mathbf{W}<em apx="apx">i ) such that ( q</em>}(\vec{f<em apx="apx">1 | \mathbf{W}_i) = q</em>}(\vec{f<em _Phi="\Phi">2 | \mathbf{W}_i) ) for all ( \vec{f}_1 ) and ( \vec{f}_2 ) in ( \mathcal{F}</em>}^L ). After training (Section 3), the network has weights ( \mathbf{W<em apx="apx">l ). The network then selects the function ( \vec{f}_l ) with the highest probability ( q</em>_l) ). We discuss our algorithms for initialization and function selection in the Methods section. A key benefit of OccamNet is that, unlike other approaches such as Petersen et al. [13], it allows for efficiently identifying the function with the highest probability.}(\vec{f}_l | \mathbf{W</p>
<h2>3 Training</h2>
<p>To express a wide range of functions, we include non-differentiable and fast-growing primitives. Additionally, in symbolic regression, we are interested in finding global minima. To address these constraints, we implement a loss function and training method that combine gradient-based optimization and sampling-based strategies for efficient global exploration of the possible functions. Our loss function and training procedure are closely related to those proposed by Petersen et al. [13], differing mainly in the fitness function and regularization terms.</p>
<p>Consider a mini-batch ( \mathcal{M} = (X, Y) ) and a sampled function from the network ( \vec{f}(\cdot) \sim q(\cdot | \mathbf{W}) ). We compute the fitness of each ( f_{(i)}(\cdot) ) with respect to a training pair ( (\vec{x}, \vec{y}) ) by evaluating</p>
<p>$$k_i(f_{(i)}(\vec{x}), \vec{y}) = (2\pi\sigma^2)^{-1/2} \exp\left(-\left[f_{(i)}(\vec{x}) - (\vec{y})_i\right]^2/(2\sigma^2)\right),$$</p>
<p>which measures how close ( f_{(i)}(\vec{x}) ) is to the target ( (\vec{y})<em _i_="(i)">i ). The total fitness is determined by summing over the entire mini-batch: ( K_i(\mathcal{M}, f</em>) ).}) = \sum_{(\vec{x}, \vec{y}) \in \mathcal{M}} k_i(f_{(i)}(\vec{x}), \vec{y</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Experiment on analytic functions. <strong>a.</strong> A sketch of the function $\sum_{n=1}^{3} \sin(nx)$ as an example of the analytics functions we consider in our work. <strong>b.</strong> Success rate (out of 10 trials) for each of the five methods considered: OccamNet, Eureqa, Eplex, AI Feynman 2.0 (AIF) and Deep Symbolic Regression (DSR) (at the top). Training time for the methods (at the bottom). Eureqa almost always finishes much more quickly than the other methods, so we do not provide training times for Eureqa. We enumerate the functions to ease the discussion. <strong>c.</strong> The "worst-case" performance for each methods, showing the minimal success rate across the six tasks.</p>
<p>We then define the loss function</p>
<p>$$H_{q_i}[f_{(i)}, \mathbf{W}, \mathcal{M}] = -K_i \left( \mathcal{M}, f_{(i)} \right) \cdot \log \left[ q_i (f_{(i)} | \mathbf{W}) \right]. \tag{1}$$</p>
<p>as in Petersen et al. [13]. As in [13], we train the network by sampling functions, selecting the number $\lambda$ of functions with the highest fitness for each output, and performing a gradient step based on these highest-fitness functions using the loss defined in Equation 1. In practice, $\lambda$ is a critical hyperparameter to tune as it adjusts the balance between updating toward higher-fitness functions and receiving information about all sampled functions.</p>
<p>To improve implicit function fitting, we implement regularization terms that punish trivial solutions by reducing the fitness $K$, as discussed in the Methods (Section 6.1.10). We also introduce regularization to restrict OccamNet to solutions that preserve units (Section 6.1.12).</p>
<p>OccamNet can also be trained to find recurrent functions, as discussed in the Methods (Section 6.1.9).</p>
<h2>4 Results</h2>
<p>To empirically validate our model, we first develop a diverse collection of benchmarks in four categories: <em>Analytic Functions</em>, simple, smooth functions; <em>Implicit Functions</em>, functions specifying an implicit relationship between inputs; <em>Non-Analytic Functions</em>, discontinuous and/or non-differentiable functions; <em>Image/Pattern Recognition</em>, patterns explained by analytic expressions. We then test OccamNet's performance and ability to scale on real-world symbolic regression datasets. The purpose of these experiments is to demonstrate that OccamNet can perform competitively with other symbolic regression frameworks in a diverse range of applications.</p>
<p>We compare OccamNet with several other symbolic regression methods: Eureqa [4], a genetic algorithm with Epsilon-Lexicase (Eplex) selection [32], AI Feynman 2.0 (AIF) [5, 33], and Deep Symbolic Regression (DSR) [13]. We do not compare to Transformer-based models such as Biggio et al. [14] because, unlike our method, these methods utilize a prespecified and immutable set of primitive functions which are not always sufficiently general to fit our experiments. The results are shown in Tables 1, 2, and 3, and we discuss them below. More details about the experimental setup are given in the methods section.</p>
<h1>4.1 Analytic functions</h1>
<p>In Figure 3 and Table 1 (in the Methods) we present our results on analytic functions. Figure 3a presents an analytic function that is particularly challenging for Eureqa. Figure 3b shows that OccamNet gives competitive success rate to state-of-the-art symbolic regression methods. For all methods besides OccamNet, there is at least one function for which the method gets zero accuracy; in contrast, OccamNet gets non-zero accuracy on every single considered function (Figure 3c).</p>
<p>We highlight the large success rate for function 4, which we originally speculated could easily trick the network with the local minimum $f(x) \approx x+1$ for large enough $x$. In contrast, as with the difficulties faced by AI Feynman 2.0, we find that OccamNet often failed to converge for function 5 because it approximated the factor $x_{0}^{2}\left(x_{0}+1\right)$ to $x_{0}^{3}$; even when convergence did occur, it required a relatively large number of steps for the network to resolve this additional constant factor. Notably, Eureqa and Eplex had difficulty finding function 3 .</p>
<p>AI Feynman 2.0 consistently identifies many of the functions, but it struggles with function 5 and is also generally much slower than other approaches. Eplex also performs well on most functions and is fast. However, like Eureqa, Eplex struggles with functions 3 and 6 . We suspect that this is because evolutionary approaches require a larger sample size than OccamNet's training procedure to adequately explore the search space. DSR consistently identifies many of the functions and is very fast. However, DSR struggles to fit Equation 6, which we suspect is because such an equation is complex but can be simplified using feature reuse. OccamNet's architecture allows such feature reuse, demonstrating an advantage of OccamNet's inductive biases.</p>
<h3>4.2 Non-analytic functions</h3>
<p>In Figure 4 and Table 2 (in the Methods) we benchmark the ability to find several non-differentiable, potentially recursive functions. From our experiments, we highlight both the network's fast convergence to the correct functional form and the discovery of the correct recurrence depth of the final expression. This is pronounced for function 7 in, which is a challenging chaotic series on which Eureqa and Eplex struggle. Interestinly, Eplex fails to identify the simpler functions 1-3 correctly. We suspect that this may be because, for these experiments, we restrict both OccamNet and Eplex to smaller expression depths. Although OccamNet is able to identify the correct functions with small expression depth, we suspect that Eplex often identifies expressions by producing more complex equivalents to the correct program and so cannot identify the correct function when restricted to simpler expressions.</p>
<p>We also investigated the usage of primitives such as MAX and MIN to sort numbers (function 4), obtaining relatively well-behaved final solutions: the few solutions that did not converge fail only in deciding the second component, $y_{2}$, of the output vector. Finally, we introduced binary operators and discrete input sets for testing function 5, a simple 4-bit Linear Feedback Shift Register (LFSR), the function $\left(x_{0}, x_{1}, x_{2}, x_{3}\right) \rightarrow\left(x_{0}+x_{3}\right.$ $\bmod 2, x_{0}, x_{1}, x_{2}$ ), which converges fast with a high success rate.</p>
<p>We do not compare to AI Feynman 2.0 in these experiments because AI Feynman does not support the required primitive functions.</p>
<h3>4.3 Implicit Functions and Image Recognition</h3>
<p>Figure 5a and Table 3 show OccamNet's performance on implicit functions. OccamNet demonstrates an advantage on challenging implicit functions. Notably, Eureqa is unsuccessful in fitting $m_{1} v_{1}-m_{2} v_{2}=0$ (conservation of momentum). Note that we only compare OccamNet to Eureqa for Implicit Functions because none of the other methods include the regularization that would be necessary to fit such functions.</p>
<p>Figure 5b and Table 3 demonstrate applications of OccamNet in image recognition, a domain that are not natural for standard symbolic regression baselines, but is somewhat more natural for OccamNet due to its interpretation as a feed-forward neural network.</p>
<p>We train OccamNet to classify MNIST [35] ${ }^{1}$ in a binary setting between the digits 0 and 7 (MNIST Binary). For this high-dimensional task, we implement OccamNet on an Nvidia V100 GPU, yielding a sizable 8 x speed increase compared to a CPU. For MNIST Binary, one of the successful functional</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Experiments on non-analytic functions. a. Two prominent examples of non-analytic functions: The challenging recursion $g(x)=x^{2}$ if $x&lt;2$, else $x / 2, y(x)=g^{\circ 4}(x)=g(g(g(g(x))))$ (top) and a sorting circuit of three numbers (bottom). b. Success rate (out of 10 trials) and training time for OccamNet and Eplex. We enumerate the functions to ease the discussion.</p>
<p>fits that OccamNet finds is $y_{0}(\vec{x})=\tanh\left(10(\max(x_{25,15},x_{26,19})+\tanh(x_{15,15})+2x_{25,10}+2x_{25,13})\right)$ and $y_{1}(\vec{x})=\tanh\left(10\tanh\left(10\left(x_{18,8}+x_{20,6}\right)\right)\right)$. The model learns to incorporate pixels into the functional fit that are indicative of the class: here $x_{18,8}$ and $x_{20,6}$ are indicative of the digit 7. These observations hold when we further benchmark the integration of OccamNet with deep feature extractors. We extract features from ImageNet [36] images using a ResNet 50 model, pre-trained on ImageNet [28]. For simplicity, we select two classes, "minivan" and "porcupine" (ImageNet Binary). OccamNet significantly improves its accuracy by backpropagating through our model using a standard cross-entropy signal. We either freeze the ResNet weights (Backprop OccamNet) or finetune ResNet through OccamNet (Finetune ResNet). In both cases, the converged OccamNet represents simple rules, $\left(y_{0}(\vec{x})=x_{1838}, y_{1}(\vec{x})=x_{1557}\right)$, suggesting that replacing the head in deep neural networks with OccamNet might be promising.</p>
<h3>4.4 Real-world regression datasets</h3>
<p>We also test OccamNet's ability to fit real-world datasets, selecting 15 datasets with 1667 or fewer datapoints from the Penn Machine Learning Benchmarks (PMLB) regression datasets [37]. These are real-world datasets, and based on their names, we infer that many are from social science, suggesting that they are inherently noisy and likely to follow no known symbolic law. Additionally, 1/3 of the datasets we choose have feature sizes of 10 or greater. These factors make the PMLB datasets challenging symbolic regression tasks. We again compare OccamNet to Eplex and AI Feynman 2.0.4</p>
<p>We test OccamNet twice. For the first test, "OccamNet-Small," we test exactly 1,000,000 functions, the same number as we test for Eplex. For the second test, "OccamNet-GPU," we exploit our architecture's integration with the deep learning framework by running OccamNet on an Nvidia V100 GPU and testing a much larger number of functions. We allow AIF to run for approximately as long or longer than OccamNet for each dataset.</p>
<p>As discussed in the SM, we perform grid search on hyperparameters and identify the fits with the best training, validation, and testing Mean Squared Error (MSE) losses. The raw data from these experiments are shown in the SM.</p>
<p>Figure 6 shows the relative performance of OccamNet-CPU, OccamNet-GPU, and baselines according to</p>
<p><sup>2</sup>The Creative Commons Attribution (CC BY) License</p>
<p><sup>3</sup>Creative Commons Attribution 4.0 International License</p>
<p><sup>4</sup>AIF's regression algorithm examines all possible feature subsets, the number of which grows exponentially with the number of features. Accordingly, we only test the datasets with ten or fewer features. AI Feynman 2.0 failed to run on a few datasets. All remaining datasets are included in tables and figures.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Experiments on implicit functions and standard vision benchmarks. <strong>a.</strong> Examples of implicit functions' loci (left) and the corresponding success rate on a suite of implicit functions (right). <strong>b.</strong> Examples of image recognition tasks (left) and the best accuracy from 10 trials for both OccamNet and the baseline. The baseline for MNIST Binary/Trinary and ImageNet Binary is the HeuristicLab symbolic regression algorithm [34]. The baseline for Backprop OccamNet and Finetune ResNet is a feed-forward neural network with the same number of parameters as OccamNet.</p>
<p>several metrics. As shown in Figure 6a-c, overall, Eplex outperforms OccamNet-CPU in training and testing MSE loss, but OccamNet-CPU outperforms Eplex in validation loss. We speculate that OccamNet-CPU's performance drop between the validation and testing datasets being larger than Eplex's performance drop results from overfitting from the larger set of hyperparameter combinations used by OccamNet-CPU (details in the SM).</p>
<p>Additionally, OccamNet-CPU runs faster than Eplex in nearly all datasets tested, often by an order of magnitude (Figure 6d). Furthermore, OccamNet is highly parallel and can easily scale on a GPU. Thus, a major advantage of OccamNet is its speed and scalability (see Section 4.5 for a further discussion of OccamNet's scaling). Comparing OccamNet-GPU and Eplex demonstrates that OccamNet continues to improve when testing more functions. The testing MSE is where OccamNet-GPU performs worst in comparison to Eplex (see Figure 6e), but it still outperforms Eplex at 10 out of 15 of the datasets while running more than nine times faster on average. Thus OccamNet's speed and scalability can be exploited to greatly increase its accuracy at symbolic regression. This demonstrates that OccamNet is a powerful alternative to genetic algorithms for interpretable data modeling.</p>
<p>Additionally, OccamNet outperforms AIF for training, validation, and testing MSE, while running faster. OccamNet-CPU achieves a lower training and validation MSE than AIF for every dataset tested. For training loss, OccamNet-CPU performs better than AIF in 4 out of 7 datasets (Figure 6f). Additionally, unlike OccamNet, AIF performs polynomial fitting, giving it an additional advantage. However, the datasets we test are likely a worst-case for AIF; the datasets are small, have no known underlying formula, and we normalize the data prior to training, meaning that AIF will likely struggle not to overfit with its neural network and will also be unlikely to identify graph modularities.</p>
<h3>4.5 Scaling on real-world regression datasets</h3>
<p>As discussed in Section 4.4, OccamNet-CPU runs far more quickly than Eplex on the same hardware, meaning that it can scale to testing far more functions per epoch than Eplex in the same runtime. To explore this advantage, we compare OccamNet running on an Nvidia V100 GPU (OccamNet-GPU) against Eplex while varying the number of functions sampled per epoch for each method. Since Eplex is not designed to scale on</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Bar charts showing the relative performance between OccamNet-CPU, OccamNet-GPU, and two baseline methods, Eplex and AIF. The x-axis is the dataset involved. The y-axis is the relative performance according to the given metric: the MSE on the training, validation, or testing set or the training time. To compute this relative performance, we divide the higher (worse) performance value by the lower (better) performance value for each dataset. The green bars represent datasets where OccamNet has a lower (better) performance value than the comparison baseline method, and the red bars represent the datasets where the comparison method has a better performance than OccamNet.
a GPU, we run Eplex on a CPU as before. We benchmark both methods on the same 15 PMLB datasets (see the methods section for more details).</p>
<p>We include and discuss the complete results of this experiment in Appendix C. In this section, we highlight key results. Figure 7 shows that OccamNet-GPU is often more than an order of magnitude faster than Eplex. Eplex scales quadratically with the number of functions, whereas OccamNet's runtime asymptotes to linear growth. However, the V100 GPU's extreme parallelism initially suppresses OccamNet-GPU's linear time complexity, demonstrating an advantage of OccamNet's ability to scale on a GPU.</p>
<p>In all of the 15 datasets, OccamNet-GPU's training loss decreases with larger runtimes, demonstrating that OccamNet can utilize the greater number of sampled functions that its efficient scaling allows. Additionally, for 11 of the training datasets, the OccamNet-GPU best fit has a MSE that is lower than or equal to the Eplex best fit MSE. Interestingly, OccamNet-GPU's validation and testing loss do not always show such a clear trend of improvement with increasing sample size. Given that the training loss does improve, we suspect that this is a case of overfitting. OccamNet-GPU's validation loss does decrease with increasing number of functions sampled for most of the datasets.</p>
<h1>5 Discussion</h1>
<p>Since our experimental settings did not require very large depths, we have not tested the limits of OccamNetGPU in terms of depth rigorously (preliminary results on increasing the depth for pattern recognition are in the SM). We expect increasing depth to yield significant complications as the search space grows exponentially. We recognize the need to create symbolic regression benchmarks that would require expressions that are large in depth. We believe that other contributions to symbolic regression would also benefit from such benchmarks. Another direction where OccamNet might be improved is low-level optimization that would make the method more efficient to train. For example, in our PMLB experiments, we estimate that OccamNet performs $&gt;8 \mathrm{x}$ as many computations as necessary. Eplex may also benefit from optimization. Finally, similarly to other symbolic regression methods, OccamNet requires a specified set of primitives to fit a dataset. While it is a notable advantage of OccamNet to have non-differentiable primitives, further work needs to be done to explore optimization at a meta level that identifies appropriate primitives for the datasets of interest without having them provided ahead of time.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: <em>Left:</em> The run time for OccamNet-GPU or Eplex as a function of the number of functions sampled per epoch. Each curve represents one of the 15 datasets. <em>Right:</em> Gradual modularity with training. Dark blue is the probability of the correct function. Light blue is the probability of a suboptimal fit with a high probability early in training. Red corresponds to the number of samples of the correct function. The insets zoom in on the curves around the epoch where the correct function is first sampled.</p>
<p>OccamNet's learning procedure allows it to combine partial solutions into better results. For example in Figure 7, the correct function's probability increases monotonically by more than 100 times <em>before being sampled</em> because OccamNet samples similar approximate solutions.</p>
<p>OccamNet successfully fits many implicit functions that other neurosymbolic architectures struggle to fit because of the non-differentiable regularization terms required to avoid trivial solutions. Although Eureqa also fits many of these equations, we find that it sometimes requires the data to be ordered by some latent variable and struggles when the dataset is very small. This is likely because Eureqa numerically evaluates implicit derivatives from the dataset [38], which can be noisy when the data is sparse. While Schmidt and Lipson [38] propose methods for analyzing unordered data, it is unclear whether these methods have been implemented in Eureqa. Thus, OccamNet seems to shine in its ability to fit unordered and small datasets described by implicit equations (e.g., momentum conservation in line 5 in Table 3).</p>
<p>To our knowledge, a unique advantage of our method compared to other symbolic regression approaches is that OccamNet represents complete analytic expressions with a single forward pass. This allows sizable gains when using an AI accelerator, as demonstrated by our experiments on a V100 GPU (Figure 6). Furthermore, because of this property, OccamNet can be easily integrated with components from the standard deep learning toolkit. For example, lines 9-10 in Table 3 demonstrate integrating OccamNet with other neural networks and optimizing both together, which is not possible with Eureqa. We also conjecture that such integration with autoregressive approaches such as DSR [13] might be challenging as the memory and latency would increase.</p>
<p>An advantage of OccamNet over transformer-based approaches to symbolic regression is that OccamNet can find fits to data regardless of the primitive functions it is given, whereas transformer-based models [14] can only fit functions that contain a certain set of primitive functions chosen at pretraining time. Thus, although transformer-based approaches may outperform OccamNet for functions similar to their training distribution, OccamNet and other similar approaches are more flexible and broadly applicable than transformer-based models. As discussed above, this is the reason that we do not compare against transformer-based methods in our experiments.</p>
<h3>Acknowledgements</h3>
<p>We would like to thank Isaac Chuang, Thiago Bergamaschi, Kristian Georgiev, Andrew Ma, Peter Lu, Evan Vogelbaum, Laura Zharmukhametova, Momchil Tomov, Rumen Hristov, Charlotte Loh, Ileana Rugina and Lay Jain for fruitful discussions.</p>
<p>Research was sponsored in part by the United States Air Force Research Laboratory and was accomplished</p>
<p>under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.</p>
<p>This material is based upon work supported in part by the U.S. Army Research Office through the Institute for Soldier Nanotechnologies at MIT, under Collaborative Agreement Number W911NF-18-2-0048.</p>
<p>Additionally, Owen Dugan would like to thank the United States Department of Defence for sponsoring his research and for allowing him to attend the Research Science Institute for free.</p>
<h1>6 Methods</h1>
<p>We divide our methods section into two parts. In Section 6.1, we provide a more detailed description of OccamNet, and in 6.2 we fully describe the setup for all of our experiments.</p>
<h3>6.1 Complete Model Description</h3>
<p>We divide this section as follows:</p>
<ol>
<li>In Section 6.1.1, we present additional materials that support the figures from the main text.</li>
<li>In Section 6.1.2, we describe of OccamNet's sampling process.</li>
<li>In Section 6.1.3, we describe OccamNet's probability distribution.</li>
<li>In Section 6.1.4, we describe OccamNet's initialization process.</li>
<li>In Section 6.1.5, we describe OccamNet's function selection.</li>
<li>In Section 6.1.6, we describe OccamNet's loss function.</li>
<li>In Section 6.1.7, we describe OccamNet's outer training loop.</li>
<li>In Section 6.1.8, we describe OccamNet's two-step training method for fitting constants.</li>
<li>In Section 6.1.9, we describe OccamNet's handling of recurrence.</li>
<li>In Section 6.1.10, we describe OccamNet's regularization for fitting implicit functions.</li>
<li>In Section 6.1.11, we describe OccamNet's procedure for handling functions with undefined outputs.</li>
<li>In Section 6.1.12, we describe OccamNet's method for regularizing to respect units.</li>
</ol>
<h3>6.1.1 Supporting Materials for the Main Figures</h3>
<p>Tables 1, 2, and 3 present our experiments in a tabular format.</p>
<h3>6.1.2 Sampling from OccamNet</h3>
<p>In this section, we more carefully describe OccamNet's sampling process. As described in the main text, we start from a predefined collection of $N$ primitive functions $\boldsymbol{\Phi}=\left{\phi_{i}(\cdot)\right}_{i=1}^{N}$. Each neural network layer is defined by two sublayers, the arguments and image sublayers. For a network of depth $L$, each of these sublayers is reproduced $L$ times. Now let us introduce their corresponding hidden states: for $1 \leq l \leq L$, each $l$ 'th arguments sublayer defines a hidden state vector $\widetilde{\mathbf{h}}^{(l)}$, and each $l$ 'th image sublayer defines a hidden state $\mathbf{h}^{(l)}$, as follows:</p>
<p>$$
\widetilde{\mathbf{h}}^{(l)}=\left[\widetilde{h}<em M="M">{1}^{(l)}, \ldots, \widetilde{h}</em>\right]
$$}^{(l)}\right], \quad \mathbf{h}^{(l)}=\left[h_{1}^{(l)}, \ldots, h_{N}^{(l)</p>
<p>Table 1: Analytic Functions. The proportion of 10 trials that converge to the correct analytic function for OccamNet, Eureqa, Eplex, AI Feynman 2.0, and Deep Symbolic Regression (DSR). sec. is the average number of seconds for convergence. Eureqa almost always finishes much more quickly than the other methods, so we do not provide training times for Eureqa.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Analytic Functions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Targets</td>
<td style="text-align: center;">OccamNet</td>
<td style="text-align: center;">sec.</td>
<td style="text-align: center;">Eureqa</td>
<td style="text-align: center;">Eplex</td>
<td style="text-align: center;">sec.</td>
<td style="text-align: center;">AI Feynman</td>
<td style="text-align: center;">sec.</td>
<td style="text-align: center;">DSR</td>
<td style="text-align: center;">sec.</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2 x^{2}+3 x$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\sin (3 x+2)$</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">620</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\sum_{n=1}^{3} \sin (n x)$</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">190</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">815</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\left(x^{2}+x\right) /(x+2)$</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">807</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$x_{0}^{2}\left(x_{0}+1\right) / x_{1}^{5}$</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1918</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$x_{0}^{2} / 2+\left(x_{1}+1\right)^{2} / 2$</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">3237</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3935</td>
</tr>
</tbody>
</table>
<p>where</p>
<p>$$
M=\sum_{0 \leq k \leq N} \alpha\left(\phi_{k}\right)
$$</p>
<p>and $\alpha(\phi)$ is the arity of function $\phi(\cdot, \ldots, \cdot)$. We also define $\mathbf{h}^{(0)}$ to be the input layer (an image sublayer) and $\widetilde{\mathbf{h}}^{(l+1)}$ to be the output layer (an arguments sublayer). These image and arguments sublayer vectors are related through the primitive functions</p>
<p>$$
h_{i}^{(l)}=\phi_{i}\left(\widetilde{h}<em j_alpha_left_phi__i="j+\alpha\left(\phi_{i">{j+1}^{(l)}, \ldots, \widetilde{h}</em>\right)
$$}\right)}^{(l)}\right), \quad j=\sum_{0 \leq k&lt;i} \alpha\left(\phi_{k</p>
<p>This formally expresses how the arguments connect to the images in any given layer, visualized as the bold edges between sublayers in Figure 1 in the main paper. To complete the architecture and connect the images from layer $l$ to the arguments of layer $(l+1)$, we sample from the softmax of the weights ${ }^{5}$ :</p>
<p>$$
\widetilde{\mathbf{h}}^{(l+1)}=\left[\begin{array}{c}
\widetilde{h}<em M__l_1="M_{l+1">{1}^{(l+1)} \
\vdots \
\widetilde{h}</em>
\end{array}\right] \equiv \operatorname{SAMPLE}\left(\left[\begin{array}{c}
\operatorname{softmax}\left(\mathbf{w}}}^{(l+1)<em M__l_1="M_{l+1">{1}^{(l)} ; T^{(l)}\right) \
\vdots \
\operatorname{softmax}\left(\mathbf{w}</em>\right]
\end{array}\right)\left[\begin{array}{c}
h_{1}^{(l)} \
\vdots \
h_{N_{l}}^{(l)}
\end{array}\right]\right.
$$}}^{(l)} ; T^{(l)</p>
<p>where the SAMPLE function samples a one-hot row vector for each row based on the categorical probability distribution defined by $\operatorname{softmax}(\mathbf{w} ; T)^{\top}$. Here the hidden states $\mathbf{h}^{(l)}$ and $\widetilde{\mathbf{h}}^{(l+1)}$ have $N_{l}$ and $M_{l+1}$ coordinates, respectively, and the vectors $\mathbf{w}_{i}^{(l)}$ represent the $i$ th row of the weights for the $l$ th layer. In practice, we set $T^{(l)}$ to a fixed, typically small, number. The last layer is usually set to a higher temperature to allow more compositionality. These sampled edges are encoded as sparse matrices, through which a forward pass evaluates $\widetilde{f}$.</p>
<p>It is also possible to implement OccamNet without the sampling part of the propagation. In this case, the softmax of the weight matrices is treated as the weights of linear layers, and we minimize the MSE loss between the outputs and the desired outputs. In practice, however, we find that this approach leads to solutions which are less sparse, which makes this approach less interpretable and often converge to suboptimal local minima.</p>
<p>As shown in Figure 8, we use skip connections similar to those in DenseNet [31] and ResNet [28], concatenating each image layer with prior image layers. In particular, such a network now has argument layers computed as</p>
<p>$$
\widetilde{\mathbf{h}}^{(l+1)}=\left[\begin{array}{c}
\widetilde{h}<em M__l_1="M_{l+1">{1}^{(l+1)} \
\vdots \
\widetilde{h}</em>
\end{array}\right] \equiv \operatorname{SAMPLE}\left(\left[\begin{array}{c}
\operatorname{softmax}\left(\mathbf{w}}}^{(l+1)<em M__l_1="M_{l+1">{1}^{(l)} ; T^{(l)}\right) \
\vdots \
\operatorname{softmax}\left(\mathbf{w}</em>\right]
\end{array}\right] \operatorname{CONCAT}\left(\mathbf{h}^{(0)}, \mathbf{h}^{(1)}, \ldots, \mathbf{h}^{(l)}\right)
$$}}^{(l)} ; T^{(l)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Non-analytic Functions. The proportion of 10 trials that converge to the correct function for OccamNet, Eureqa, and Eplex. sec. is the average number of seconds for convergence. Eureqa almost always finishes much more quickly than OccamNet and Eplex, so we do not provide training times for Eureqa. *For program #6, Eplex fits $y_{1}$ every time and never fits $y_{0}$ correctly, so we give it a score of 0.5 .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Non-analytic Functions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Targets</td>
<td style="text-align: center;">OccamNet</td>
<td style="text-align: center;">sec.</td>
<td style="text-align: center;">Eureqa</td>
<td style="text-align: center;">Eplex</td>
<td style="text-align: center;">sec.</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$3 x$ if $x&gt;0$, else $x$</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$x^{2}$ if $x&gt;0$, else $-x$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$x$ if $x&gt;0$, else $\sin (x)$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">47</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\operatorname{SORT}\left(x_{0}, x_{1}, x_{2}\right)$</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">191</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$4 \operatorname{LFSR}\left(x_{0}, x_{1}, x_{2}, x_{3}\right)$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">262</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$y_{0}(\vec{x})=x_{1}$ if $x_{0}&lt;2$, <br> else $-x_{1}$ <br> $y_{1}(\vec{x})=x_{0}$ if $x_{1}&lt;0$, <br> else $x_{1}^{2}$</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">157</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">${ }^{*} 0.5$</td>
<td style="text-align: center;">121</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$g(x)=x^{2}$ if $x&lt;2$, <br> else $x / 2$ <br> $y(x)=g^{\circ 4}(x)$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">189</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$g(x)=x+2$ if $x&lt;2$, <br> else $x-1$ <br> $y(x)=g^{\circ 2}(x)$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">116</td>
</tr>
</tbody>
</table>
<p>where now each vector $\mathbf{w}<em i="0">{i}^{(l+1)}$ has $\sum</em>$. Skip connections yield several desirable properties: (i) The depth of equations is not fixed, lifting the requirement that the number of layers of the solution be known in advance. (ii) The network can find compact solutions as it considers all levels of composition. This promotes solution sparsity and interpretability. (iii) Primitives in shallow layers can be reused, analogous to feature reuse in DenseNet. (iv) Subsequent layers may behave as higher-order corrections to the solutions found in early layers. Additionally, if we implement OccamNet without sampling, shallow layers are trained before or alongside the subsequent layers due to more direct supervision because gradients can propagate to shallow layers more easily to avoid exploding or vanishing gradients.}^{l} N_{i}$ components instead of $N_{l</p>
<p>From Equation (3), we see that $M_{l+1}=M=\sum_{0 \leq k \leq N} \alpha\left(\phi_{k}\right)$. If no skip connections are used, $N_{l}=N=$ $|\boldsymbol{\Phi}|$. If skip connections are used, however, $N_{l}$ grows as $l$ increases. We demonstrate how the scaling grows as follows. Let $u$ be the number of inputs and $v$ be the number of outputs. When learning connections from images to arguments at layer $l(1 \leq l \leq L)$, there will be skip connections from the images of the previous $l$ layers $0,1, \ldots, l-1$. Hence the $i$ th layer has an image size of $u+i N$, as shown in Figure 8. We learn linear layers from these images to arguments, and the number of arguments is always $M$. Thus, in total, we have the following number of parameters:</p>
<p>$$
v(u+(L+1) N)+M \sum_{i=0}^{L-1}(u+i N) \in O\left(N M L^{2}\right)
$$</p>
<p>Note that in the above discussion we assume that $M$ remains constant. However, to be able to represent all functions up to a particular depth, we must repeat primitives in earlier layers, causing $M$ to grow exponentially. For small numbers of layers, this is not problematic. If a larger expression depth is required, one can avoid primitives and increase the number of layers beyond what is necessary. This makes additional copies of each primitive available for use without requiring an exponential growth in the layer size.</p>
<h1>6.1.3 OccamNet's Probability Distribution</h1>
<p>OccamNet parametrizes not only the probability of sampling a given function $\vec{f}=\left(f_{(0)}, \ldots, f_{(v-1)}\right)^{\top}$ but also the probability of sampling each $f_{(i)}$ independently of the other components of $\vec{f}$. As discussed in the</p>
<p>Table 3: Implicit Functions: The proportion of 10 trials that converge to the correct implicit function for OccamNet and Eureqa. Image Recognition: The best accuracy from 10 trials for both OccamNet and the baseline. The baseline above the mid-line is HeuristicLab [34], and the baseline below the mid-line is a feed-forward neural network with the same number of parameters as OccamNet. sec. is the average number of seconds for convergence. The baselines almost always finish much more quickly than OccamNet, so we do not provide baseline training times.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Implicit Functions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Image Recognition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Target</td>
<td style="text-align: center;">OccamNet</td>
<td style="text-align: center;">sec.</td>
<td style="text-align: center;">Eureqa</td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Target</td>
<td style="text-align: center;">OccamNet</td>
<td style="text-align: center;">sec.</td>
<td style="text-align: center;">Baseline</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$x_{0} x_{1}=1$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">294</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">MNIST Binary</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">92.8</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$x_{0}^{2}+x_{1}^{2}=1$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">153</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">MNIST Trinary</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">81.2</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$x_{0} / \cos \left(x_{1}\right)=1$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">ImageNet Binary</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">78.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$x_{1} / x_{0}=1$</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Backprop OccamNet</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">97.7</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$m_{1} v_{1}-m_{2} v_{2}=0$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Finetune ResNet</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">95.4</td>
</tr>
</tbody>
</table>
<p>main text, the probability of the model sampling $f_{(i)}$ as its $i$ th output, $q_{i}\left(f_{(i)} \mid \mathbf{W}\right)$, is the product of the probabilities of the edges of $f_{(i)}$ 's DAG. Similarly, $q(\vec{f} \mid \mathbf{W})$, the probability of the model sampling $\vec{f}$, is given by the product of $\vec{f}$ 's edges, or $q(\vec{f} \mid \mathbf{W})=\prod_{i=0}^{c-1} q_{i}\left(f_{(i)} \mid \mathbf{W}\right)$.</p>
<p>Because $q(\cdot \mid \cdot)$ is a probability distribution, we have $\sum_{\vec{f} \in \mathcal{F}<em _Phi="\Phi">{\Phi}^{L}} q(\vec{f} \mid \mathbf{W})=1$ and $q(\vec{f} \mid \mathbf{W}) \geq 0$ for all $\vec{f}$ in $\mathcal{F}</em>$.}^{L}$. Similar results hold for the probability distributions of each component $f_{(i)</p>
<p>OccamNet's sampling process involves independently sampling connections from each layer. Although each of OccamNet's layers represents an independent probability distribution, when sampling a function, the layers do not act independently. This is because the samples from layers closer to the outputs inform which of the sampled connections from previous layers are used. In particular, the full DAG that OccamNet samples has many disconnected components, and all components of the DAG which are not connected to any of the output nodes are effectively trimmed (See Figure 9). This is advantageous as it allows OccamNet to produce very different distributions of functions for different choices of connections in the final few layers, thereby allowing OccamNet to explore multiple classes of functions simultaneously.</p>
<p>As discussed in the main text, $q(\vec{f} \mid \mathbf{W})$ is the product of the probabilities of the sampled connections in $\vec{f}$ 's DAG which are connected to the output nodes. However, in practice, we compute probabilities of functions in a feed-forward manner. This computation underestimates some probabilities; it actually computes an estimate $q_{a p x}(\vec{f} \mid \mathbf{W})$ of $q(\vec{f} \mid \mathbf{W})$.</p>
<p>To compute the probability of a given function, we assign each image and argument node a probability given this function's DAG. We denote the probability of the $i$ 'th node of the $l$ 'th image layer with $p_{i}^{(l)}$ and the probability of the $i$ 'th node of the $l$ 'th argument layer with $\tilde{p}_{i}^{(l)}$.</p>
<p>We propagate probabilities as follows. If the $i$ 'th image node in layer $l$ is connected to the $j$ 'th argument node in layer $l+1$, the probability of the $j$ 'th argument node in layer $l$ is</p>
<p>$$
\tilde{p}<em i="i">{j}^{(l+1)}=p</em>\right)
$$}^{(l)} \cdot p_{i}^{(l, j)}\left(T^{(l)</p>
<p>The $i$ th image node of the $l$ th layer then has probability given by</p>
<p>$$
p_{i}^{(l)}=\prod_{k=n+1}^{n+\alpha\left(\phi_{i}\right)} \tilde{p}<em j="1">{k}^{(l)}, \quad n=\sum</em>\right)
$$}^{i-1} \alpha\left(\phi_{j</p>
<p>where $\alpha(f)$ denotes the number of inputs to $f$. Finally, to calculate the probability of a function, we multiply the probabilities of the output nodes.</p>
<p>This algorithm computes function probabilities correctly unless a function's DAG has multiple nodes connecting to the same earlier node in the DAG. In this case, the probability of the earlier node is included multiple times in the final function probability, producing an estimate that is below the true probability of sampling the function.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Skip connections. Nodes are color coded with lines indicating the origin of the reused neurons.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: A demonstration of the dropped connections from sampled paths in OccamNet. All red paths are dropped from the final symbolic form of the sampled function because they are not directly connected to the outputs. These paths are unnecessarily computed during OccamNet's training process, leading to potential slowdowns in training.</p>
<p>In practice, we find that this biased evaluation of probabilities does not substantially affect OccamNet training. Note that when we equalize all functions to have the same probability (Section 6.1.4) or sample the highest probability function (Section 6.1.5), we do so with respect to the probability estimate ( q_{npx} ), not with respect to ( q ). In this paper, we use ( q ) to mean ( q_{npx} ) unless otherwise specified.</p>
<h3>6.1.4 Initialization</h3>
<p>When beginning this project, we originally initialized all model weights to 0. However, this initializes complex functions, which have DAGs with many more edges than simple functions, to low probabilities. As a result, we found in practice that the network sometimes struggled to converge to complex functions with high fitness ( K(\mathcal{M}, f) ) because their initial low probabilities meant that they were sampled far less often than simple functions. This is because even if complex functions have a higher probability increase than simple functions when they are sampled, the initial low probabilities caused the complex functions to be sampled far less and to have an overall lower expected probability increase.</p>
<p>To address this issue, we now use a second initialization algorithm, which initializes all functions to equal probability. This initialization algorithm iterates through the layers of the network. In practice, to balance</p>
<p>effects discussed at the end of this section, we initialize to weights interpolated between 0 and the algorithm discussed below. More details are given at the end of this section.</p>
<p>The algorithm to initialize all functions with equal probability establishes as an invariant that, after assigning the weights up to the $l$ th layer, all paths leading to a given node in the $l$ th argument layer have equal probabilities. Then, each argument layer node has a unique corresponding probability, the probability of all paths up to that node. We denote the probability of the $i$ th node in the $l$ th argument sublayer as $\bar{p}<em i="i">{i}^{(l)}$, because it is the probability of any path leading to the $i$ th node in the $l$ th argument sublayer. Because each argument layer node has a corresponding probability, each image layer node must also have a unique corresponding probability, which, for the $i$ th node in the $l$ th image sublayer, we denote as $p</em>$ because this is the probability of any path leading to the $i$ th node in the $l$ th image sublayer. These image layer probabilities are given by}^{(l)}$. Again, we use the notation $p_{i}^{(l)</p>
<p>$$
p_{i}^{(l)}=\prod_{k=v+1}^{n+\alpha\left(\phi_{i}\right)} \bar{p}<em j="1">{k}^{(l)}, \quad n=\sum</em>\right)
$$}^{i-1} \alpha\left(\phi_{j</p>
<p>Our algorithm starts with input layer, or the 0th image layer. Paths to any node in the input layer have no edges so they all have probability 1 . Thus, we initialize $p_{i}^{(0)}=1$ for all $i$. As the algorithm iterates through all subsequent $T$-Softmax layers, the invariant established above provides a system of linear equations involving the desired connection probabilities, which the algorithm solves. The algorithm groups the previous image layer according to the node probabilities, obtaining a set of ordered pairs $\left{\left(p^{\prime}{ }<em a="a">{a}^{(l)}, n</em>\right)\right}}^{(l)<em a="a">{i=a}^{k}$ representing $n</em>}^{(l)}$ nodes with probability $p^{\prime}{ <em i="i">{a}^{(l)}$ in the $l$ th layer. Note that if two image nodes have the same probability $p</em>}^{(l)}=p_{j}^{(l)}$, then the edges between any argument node in the next layer and the two image nodes must have the same probability in order to satisfy the algorithm's invariant: $p_{i}^{(l, k)}=p_{j}^{(l, k)}$. Then, we define $p^{\prime}{ <em a="a">{a}^{(l, i)}$ as the probability of the edges between the image nodes with probability $p^{\prime}{ }</em>}^{(l)}$ and the $i$ th argument $P$-node of the $l$ th layer. The probabilities of the edges to a given $P$-node sum to 1 , so for each $j$, we must have $\sum_{a} n_{a}^{(l)} p^{\prime}{ <em a="a">{a}^{(l, i)}=1$. Further, the algorithm requires that the probability of a path to a $P$-node through a given connection is the same as the probability of a path to that $P$-node through any other connection. The probability of a path to the $i$ th $P$-node through a connection with probability $p^{\prime}{ }</em>}^{(l, i)}$ is $p^{\prime}{ <em a="a">{a}^{(l)} p^{\prime}{ }</em>}^{(l, i)}$, so we obtain the equations $p^{\prime}{ <em 0="0">{0}^{(l)} p^{\prime}{ }</em>}^{(l, i)}=p^{\prime}{ <em a="a">{a}^{(l)} p^{\prime}{ }</em>$, for all $a$ and $i$. These two constraints give the vector equation}^{(l, i)</p>
<p>$$
\left[\begin{array}{ccccc}
n_{0}^{(l)} &amp; n_{1}^{(l)} &amp; n_{2}^{(l)} &amp; \cdots &amp; n_{k}^{(l)} \
p_{0}^{\prime(l)} &amp; -p_{1}^{\prime(l)} &amp; 0 &amp; \cdots &amp; 0 \
p_{0}^{\prime(l)} &amp; 0 &amp; -p_{2}^{\prime(l)} &amp; \cdots &amp; 0 \
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \
p_{0}^{\prime(l)} &amp; 0 &amp; 0 &amp; \cdots &amp; -p_{k}^{\prime(l)}
\end{array}\right]\left[\begin{array}{c}
p_{0}^{\prime(l, j)} \
p_{0}^{\prime(l, j)} \
p_{2}^{\prime(l, j)} \
\vdots \
p_{k}^{\prime(l, j)}
\end{array}\right]=\left[\begin{array}{c}
1 \
0 \
0 \
\vdots \
0
\end{array}\right]
$$</p>
<p>for all $1 \leq j \leq M$. The algorithm then solves for each $p^{\prime}{ }<em a="a">{a}^{(l, j)}$.
After determining the desired probability of each connection of the $l$ th layer, the algorithm computes the SPL weights $\mathbf{w}^{\prime(l, j)}$ that produce the probabilities $p^{\prime}{ }</em>\right)$ :}^{(l, j)}$. Since there are infinitely many possible weights that produce the correct probabilities, the algorithm sets $w_{0}^{\prime(l, j)}=0$. Then, the algorithm uses the softmax definition of the edge probabilities to determine the required value of $\sum_{n=1}^{k} \exp \left(w_{n}^{\prime(l, j)} / T^{(l)</p>
<p>$$
\begin{aligned}
p_{0}^{\prime(l, j)} &amp; =\frac{\exp \left(w_{0}^{\prime(l, j)} / T^{(l)}\right)}{\sum_{n=1}^{k} \exp \left(w_{n}^{\prime(l, j)} / T^{(l)}\right)} \
&amp; =\frac{1}{\sum_{n=1}^{k} \exp \left(w_{n}^{\prime(l, j)} / T^{(l)}\right)}
\end{aligned}
$$</p>
<p>so</p>
<p>$$
\sum_{a=1}^{k} \exp \left(w_{a}^{\prime(l, j)} / T\right)=1 / p_{0}^{\prime(l, j)}
$$</p>
<p>Substituting this equation into the expression for the other probabilities gives</p>
<p>$$
\begin{aligned}
p_{a}^{\prime(l, j)} &amp; =\exp \left(w_{a}^{\prime(l, j)} / T^{(l)}\right) /\left(\sum_{n=1}^{k} \exp \left(w_{n}^{\prime(l, j)} / T^{(l)}\right)\right) \
&amp; =p_{0}^{\prime(l, j)} \exp \left(w_{a}^{\prime(l, j)} / T^{(l)}\right)
\end{aligned}
$$</p>
<p>Solving for $w_{a}^{\prime(l, j)}$ gives</p>
<p>$$
w_{a}^{\prime(l, j)}=T^{(l)} \log \left(p_{a}^{\prime(l, j)} / p_{0}^{\prime(l, j)}\right)
$$</p>
<p>which the algorithm uses to compute $w_{i}^{\prime(l, j)}$.
After determining the weights $w_{i}^{\prime(l, j)}$ the algorithm assigns them to the corresponding $w_{a}^{(l, j)}$. In particular, if the $i$ th image node has probability $p_{a}^{\prime(l)}$, the weights of edges to the $i$ th node are given by $w_{i}^{(l, j)}=w_{k}^{\prime(l, j)}$, for all $j$. The algorithm then determines the values of $\widetilde{p}<em j="j">{j}^{(l+1)}$, given by $\widetilde{p}</em>$ using Equation 8 and repeats the above process for subsequent layers until it reaches the end of the network.}^{(l+1)}=p_{i}^{(l)} p_{i}^{(l, j)}$. Finally, the algorithm determines $p_{i}^{(l+1)</p>
<p>In summary, the algorithm involves the following steps:</p>
<ol>
<li>Set $l=0$.</li>
<li>Set $p_{i}^{(l)}=1$.</li>
<li>Increment $l$ by 1 .</li>
<li>Compute $\left{\left(p_{a}^{\prime(l)}, n_{a}^{(l)}\right)\right}<em a="a">{i=a}^{k}$ and use Equation 9 to compute $p</em>$.}^{\prime(l, j)</li>
<li>Set $w_{a}^{\prime(l, j)}$ according to Equation 10 .</li>
<li>If $l&lt;L+1$, Compute $\widetilde{p}<em i="i">{i}^{(l+1)}$ and $p</em>$.}^{(l+1)</li>
<li>Return to step 3 until $l=L+1$.</li>
</ol>
<p>This algorithm efficiently equalizes the probabilities of all functions in the network. In practice, however, we find that perfect equalization of functions causes activation functions with two inputs to be highly explored. This is because there are many more possible functions containing activation functions with two inputs than with one input. Additionally, as mentioned in Section 6.1.3, in this section we have implicitly been using the approximate probability $\tilde{q}$. This probability underestimates many functions that include activation functions with two or more inputs because these functions are those which can use a node multiple times in their DAG. As a result, although all functions will have an equal $\tilde{q}$, some functions with multiple inputs will have larger $q$ than other functions, and $q$ is what determines the probability of being sampled. In practice, therefore, we find that a balance between initializing all weights to one and initializing all functions to equal probability is most effective for exploring all types of functions.</p>
<p>To implement this balance, we create an equalization hyperparameter, $E$. If $E=0$, we initialize all weights to 1 as in the original OccamNet architecture. If $E \neq 0$, we use the algorithm presented above to initialize the weights and then divide all of the weights by $E$. For $E&gt;1$, this has the effect of initializing weights between the two initialization approaches. In practice, we find that values of $E=1$ and $E=5$ are most effective for exploring all types of functions (See Section 6.2.2).</p>
<h1>6.1.5 Function Selection</h1>
<p>As discussed in the main text, after training using a sampling strategy, the network selects the function $\tilde{f}$ with the highest probability $q(\tilde{f} \mid \mathbf{W})$.</p>
<p>We develop a dynamic programming algorithm that determines the DAG with the highest probability. The algorithm steps sequentially through each argument layer, and at each argument layer it determines the maximum probability path to each argument node. Knowing the maximum probability paths to the previous</p>
<p>argument layer nodes allows the algorithm to easily determine the maximum probability paths to the next argument layer.</p>
<p>As with the network initialization algorithm, the function selection algorithm associates the $i$ th $P$-node of the $l$ th argument sublayer with a probability, $\widetilde{p}<em i="i">{i}^{(l)}$, which represents the highest probability path to that node. Similarly, we let $p</em>}^{(l)}$ represent the assigned probability of the $i$ th node of the $l$ th image sublayer, defined as the highest probability path to a given image node. $p_{i}^{(l)}$ can once again be determined from $\widetilde{p<em i="i">{i}^{(l)}$ using Equation 8. Further, the algorithm associates each node with a function, $\widetilde{f}</em>}^{(l)}$ for argument nodes and $f_{i}^{(l)}$ for image nodes, which represents the highest probability function to the corresponding node. Thus, $\widetilde{f<em i="i">{i}^{(l)}$ has probability $\widetilde{p}</em>$ using}^{(l)}$, and $f_{i}^{(l)}$ has probability $p_{i}^{(l)}$. Further, $f_{i}^{(l)}$ is determined from $\widetilde{f}_{i}^{(l)</p>
<p>$$
f_{i}^{(l)}(\vec{x})=\phi_{i}\left(\widetilde{f}<em n_alpha_left_phi__j="n+\alpha\left(\phi_{j">{n+1}^{(l)}(\vec{x}), \ldots, \widetilde{f}</em>\right)
$$}\right)}^{(l)}(\vec{x})\right), \quad n=\sum_{j=1}^{i-1} \alpha\left(\phi_{j</p>
<p>The algorithm iterates through the networks layers. At the $l$ th layer, it determines the maximum probability path to each argument node, computing</p>
<p>$$
\begin{aligned}
&amp; \widetilde{p}<em 0="0">{i}^{(l+1)}=\operatorname{MAX}\left(p</em>\right) \
&amp; \widetilde{f}}^{(l)} p_{0}^{(l, i)}, \ldots, p_{N}^{(l)} p_{N}^{(l, i)<em 0="0">{i}^{(l+1)}= \begin{cases}f</em> \
f_{1}^{(l)} &amp; \text { if } p_{i}^{(l+1)}=p_{1}^{(l)} p_{1}^{(l, i)} \
\vdots &amp; \vdots \
f_{N}^{(l)} &amp; \text { if } p_{i}^{(l+1)}=p_{N}^{(l)} p_{N}^{(l, i)}\end{cases}
\end{aligned}
$$}^{(l)} &amp; \text { if } p_{i}^{(l+1)}=p_{0}^{(l)} p_{0}^{(l, i)</p>
<p>Next, it determines the maximum probability path up to each image node, computing $p_{i}^{(l+1)}$ and $f_{i}^{(l+1)}$ using Equations 8 and 11, respectively. The algorithm repeats this process until it reaches the output layer, at which point it returns $\widetilde{f}<em 1="1">{\max }=\left[\widetilde{f}</em>}^{(L)}, \ldots, \widetilde{f<em _max="\max">{N}^{(L)}\right]^{\top}$ and $p</em>$.}=\prod_{i=1}^{N} \widetilde{p}_{i}^{(L)</p>
<p>An advantage of this process is that identifying the highest probability function has the same computational complexity as sampling functions. In particular, the complexity at each layer is $O\left(M N_{i}\right)$, leading to an overall complexity of $O\left(N M L^{2}\right)$ if skip connections are included.</p>
<h1>6.1.6 Loss Function and its Gradient</h1>
<p>We train our network on mini-batches of data to provide flexibility for devices with various memory constraints. Consider a mini-batch $\mathcal{M}=(X, Y)$, and a sampled function from the network $\widetilde{f}(\cdot) \sim q(\cdot \mid \mathbf{W})$. We compute the fitness of each $f_{(i)}(\cdot)$ with respect to a training pair $(\vec{x}, \vec{y})$ by evaluating the likelihood</p>
<p>$$
k_{i}\left(f_{(i)}(\vec{x}), \vec{y}\right)=\left(2 \pi \sigma^{2}\right)^{-1 / 2} \exp \left(-\left[f_{(i)}(\vec{x})-(\vec{y})_{i}\right]^{2} /\left(2 \sigma^{2}\right)\right)
$$</p>
<p>which is a Normal distribution with mean $(\vec{y})<em _i_="(i)">{i}$ and variance $\sigma^{2}$, and measures how close $f</em>)}(\vec{x})$ is to the target $(\vec{y<em i="i">{i}$. The fitness can be also viewed as a Bayesian posterior with a noninformative prior. The total fitness is determined by summing over the entire mini-batch: $K</em>\right)$.}\left(\mathcal{M}, f_{(i)}\right)=\sum_{(\vec{x}, \vec{y}) \in \mathcal{M}} k_{i}\left(f_{(i)}(\vec{x}), \vec{y</p>
<p>The variance $\sigma^{2}$ of $k_{i}\left(f_{(i)}(\vec{x}), \vec{y}\right)$ characterizes the fitness function's smoothness. As $\sigma^{2} \rightarrow 0$, the fitness is a delta function with nonzero fitness for some $(\vec{x}, \vec{y})$ only if $f_{(i)}(\vec{x})=(\vec{y})<em _i_="(i)">{i}$. Similarly, a large variance characterizes a fitness in which potentially many solutions provide accurate approximations, increasing the risk of convergence to local minima. In the former case, learning becomes harder as few $f</em>$ be a network hyperparameter, tuned for the tradeoff between ease of learning and solution optimality for different tasks.}(\cdot)$ out of exponentially many sampleable functions result in any signal, whereas in the latter case learning might not converge to the optimal solution. We let $\sigma^{2</p>
<p>Similar to Petersen et al. [13], we use a loss function for backpropagating on the weights of $q(\cdot \mid \mathbf{W})$ :</p>
<p>$$
H_{q_{i}}\left[f_{(i)}, \mathbf{W}, \mathcal{M}\right]=-K_{i}\left(\mathcal{M}, f_{(i)}\right) \cdot \log \left[q_{i}\left(f_{(i)} \mid \mathbf{W}\right)\right]
$$</p>
<p>We can interpret (12) as the cross-entropy of the posterior for the target and the probability of the sampled function $f_{(i)}$. If the sampled function $f_{(i)}$ is close to $f_{(i)}^{*}$, then $K_{i}\left(\mathcal{M}, f_{(i)}\right)$ will be large, and the gradient update below will also be large:</p>
<p>$$
\nabla_{\mathbf{W}} H_{q_{i}}\left[f_{(i)}, \mathbf{W}, \mathcal{M}\right]=-\frac{\nabla_{\mathbf{W} q_{i}\left(f_{(i)} \mid \mathbf{W}\right)}}{q_{i}\left(f_{(i)} \mid \mathbf{W}\right)} K_{i}\left(\mathcal{M}, f_{(i)}\right)
$$</p>
<p>The first term on the right-hand side (RHS) of update (13) increases the probability of the function $f_{(i)}$. The second term on the RHS is maximal when $f_{(i)}(\vec{x})=f_{(i)}^{<em>}(\vec{x})$. Importantly, the second term approaches zero as $f_{(i)}$ deviates from $f_{(i)}^{</em>}$. If the sampled function is far from the target, then the probability update is suppressed by $K_{i}\left(\mathcal{M}, f_{(i)}\right)$. Therefore, we only optimize the probability for functions close to the target. Note that in (13) we backpropagate only through the probability of the function $f_{(i)}$ given by $q_{i}\left(f_{(i)} \mid \mathbf{W}\right)$, whose value does not depend on the primitives in $\boldsymbol{\Phi}$, implying that the primitives can be non-differentiable. This is particularly useful for applications requiring non-differentiable primitive functions. Furthermore, this loss function allows non-differentiable regularization terms, which greatly expands the regularization possibilities.</p>
<h1>6.1.7 Sample-based Training</h1>
<p>We use a sampling-based strategy to update our model, explained below without constant fitting for simplicity. This training procedure was first proposed in Risk-Seeking Policy Gradients [13]. We denote $\mathbf{W}^{(t)}$ as the set of weights at training step $t$, and we fix two hyperparameters: $R$, the number of functions to sample at each training step, and $\lambda$, or the truncation parameter, which defines the number of the $R$ paths chosen for optimization via (13). We initialize $\mathbf{W}^{(0)}$ as described in Section 2.2. We then proceed as follows:</p>
<ol>
<li>Sample $R$ functions $\vec{f}<em R="R">{1}, \ldots, \vec{f}</em>} \sim q\left(\cdot \mid \mathbf{W}^{(t)}\right)$. We denote the $j$ th output of $\vec{f<em i_j_="i(j)">{i}$ as $f</em>$.</li>
<li>For each output $j$, sort $f_{i(j)}$ from greatest to least value of $K_{j}\left(\mathcal{M}, f_{i(j)}\right)$ and select the top $\lambda$ functions, yielding a total of $v \lambda$ selected functions $g_{1, j}, \ldots, g_{\lambda, j}$. The total loss is then given by $\sum_{i=1}^{\lambda} \sum_{j=0}^{v-1} H_{q_{j}}\left[g_{i, j}, \mathbf{W}, \mathcal{M}\right]$, which yields the training step gradient update:</li>
</ol>
<p>$$
-\sum_{i=1}^{\lambda} \sum_{j=0}^{v-1} \frac{\nabla_{\mathbf{W} q_{j}\left(g_{i, j} \mid \mathbf{W}\right)}}{q_{j}\left(g_{i, j} \mid \mathbf{W}\right)} K_{j}\left(\mathcal{M}, g_{i, j}\right)
$$</p>
<p>Notice that through (14) we have arrived at a modified REINFORCE update [39], where the policy is $q_{i}(\cdot \mid \cdot)$ and the regret is the fitness $K_{i}(\cdot, \cdot)$.
3. Perform the gradient step (14) on $\mathbf{W}^{(t)}$ for all selected paths to obtain $\mathbf{W}^{(t+1)}$. In practice, we find that the Adam algorithm [40] works well for this step.
4. Set $t=t+1$ and repeat from Step 1 until a stop criterion is met.</p>
<p>Note that Equations (13) and (14) represent different objective functions - we use (14). The benefit of using Equation (14) is that accumulating over the top $v \lambda$ best fits to the target allows for explorations of function compositions that contain desired components but are not fully developed. In practice, we find that reweighting the importance of the top- $v \lambda$ routes, substituting $K_{j}^{\prime}\left(\mathcal{M}, g_{i, j}\right)=K_{j}\left(\mathcal{M}, g_{i, j}\right) / i$, improves convergence speed by biasing updates towards the best routes.</p>
<h3>6.1.8 Constant Fitting</h3>
<p>Thus far, our method works for functions with constants known a priori. Examples of such functions include $x^{2}$ or $x+\pi$ if $(\cdot)^{2}$ and $\pi$ are provided respectively as primitives and constants ahead of time. In some cases, however, we may wish to fit functions that involve constants that are not known a priori. To fit such undetermined constants, we use activation functions with unspecified constants, such as $x^{c}$ and $c \cdot x(c$ is undefined). We then combine the training process described in Section 6.1.7 with a constant fitting training process.</p>
<p>The two-step training process works as follows: We first sample a batch $\mathcal{M}$ and a function batch $\left(\tilde{f_{1}}, \ldots, \tilde{f_{R}}\right)$. Next, for each function $\tilde{f_{i}}$, we fit the unspecified constants to $\mathcal{M}$ in $\tilde{f_{i}}$ using gradient descent. Any other constant optimization method would also work. Finally, we update the network weights according to Section 6.1.7, using the fitness $K$ of the constant-fitted function batch. To increase training speed, we store each function's fitted constants for reuse.</p>
<h1>6.1.9 Recurrence</h1>
<p>OccamNet can also be trained to find recurrence relations, which is of particular interest for programs that rely on FOR or WHILE loops. To find such recurrence relations, we assume a maximal recursion $D$. We use the following notation for recurring functions: $f^{\circ(n+1)}(x) \equiv f^{\circ n}(f(x))$, with base case $f^{\circ 1}(x) \equiv f(x)$.</p>
<p>To augment the training algorithm, we first sample $\left(\tilde{f_{1}}, \ldots, \tilde{f_{R}}\right) \sim q\left(\cdot \mid \mathbf{W}^{(t)}\right)$. For each $\tilde{f_{i}}$, we compute its recurrence to depth $D$ as follows $\left(\tilde{f}<em i="i">{i}^{\circ 1}, \tilde{f}</em>}^{\circ 2}, \ldots, \tilde{f<em j="j">{i}^{\circ D}\right)$, obtaining a collection of $R D$ functions. Training then continues as usual; we compute the corresponding $K</em>\right)$, select the best $v \lambda$, and update the weights. It is important to note that we consider all depths up to $D$ since our maximal recurrence depth might be larger than the one for the target function.}\left(\mathcal{M}, \tilde{f}_{i(j)}^{\circ n</p>
<p>Note that we do not change the network architecture to accommodate for recurrence depth $D&gt;1$. As described in the main text, we can efficiently use the network architecture to evaluate a sampled function $\tilde{f}(\vec{x})$ for a given batch of $\vec{x}$. To incorporate recurrence, we take the output of this forward pass and feed it again to the network $D$ times, similar to what is typically done for recurrent neural networks. The resulting outputs are evaluations $\left(\tilde{f}<em i="i">{i}^{\circ 1}(\vec{x}), \tilde{f}</em>$.}^{\circ 2}(\vec{x}), \ldots, \tilde{f}_{i}^{\circ D}(\vec{x})\right)$ for a given batch of $\vec{x</p>
<h3>6.1.10 Regularization</h3>
<p>As discussed in the main text, to improve implicit function fitting, we implement a regularized loss function,</p>
<p>$$
K_{i}^{\prime}(\mathcal{M}, f)=K_{i}(\mathcal{M}, f)-s \cdot r[f]
$$</p>
<p>for some regularization function $r$, where $s=n(\mathcal{M}) / \sqrt{2 \pi \sigma^{2}}$ is the maximum possible value of $K_{i}(\mathcal{M}, f)$. We define</p>
<p>$$
r[f]=w_{\phi} \cdot \phi[f]+w_{\psi} \cdot \psi[f]+w_{\xi} \cdot \xi[f]+w_{\gamma} \cdot \gamma[f]
$$</p>
<p>where $\phi[f]$ measures trivial operations, $\psi[f]$ measures trivial approximations, $\xi[f]$ measures the number of constants in $f, \gamma[f]$ measures the number of activation functions in $f$, and $w_{\phi}, w_{\psi}, w_{\xi}$, and $w_{\gamma}$, are weights for their respective regularization terms. We now discuss each of these regularization terms in more detail.</p>
<p>The $\phi[f]$ Regularization Term: The $\phi[f]$ term measures whether the unsimplified form of $f$ contains trivial operations, by which we mean operations that simplify to 0,1 , or the identity. For example, division is a trivial operation in $x / x$, because the expression simplifies to 1 . Similarly, $1 \cdot x, x^{1}$, and $x^{0}$ are all trivial operations. We punish these trivial operations because they produce constant outputs without adding meaning to an expression.</p>
<p>To detect trivial operations, we employ two procedures. The first uses the SymPy package [41] to simplify $f$. If the simplified expression is different from the original expression, then there are trivial operations in $f$, and this procedure returns 1. Otherwise the first procedure returns 0 . Unfortunately, the SymPy $==$ function to test if functions are equal often incorrectly indicates that nontrivial functions are trivial. For example, SymPy's simplify function, which we use to test if a function can be simplified, converts $x+x$ to $2 \cdot x$, and the $==$ function states that $x+x \neq 2 \cdot x$. To combat this, we develop a new function, sympyEquals which corrects for these issues with $==$. The sympyEquals is equivalent to $==$, except that it does not take the order of terms into account, and it does not mark expressions such as $x+x$ and $x \cdot x$ as unsimplified. We find that this greatly improves implicit function fitting.</p>
<p>The constant fitting procedure often produces functions that only differ from a trivial operation because of imperfect constant fitting, such as $f\left(x_{0}\right)=x_{0}^{0.0001}$, which is likely meant to represent $x_{0}^{0}$. SymPy, however, will not mark this function as trivial. The second procedure addresses this issue by counting the constant activations, such as $x_{0}^{0.0001}, 1.001 \cdot x_{0}$, and $x_{0}+0.001$, which approximate trivial operations. For the activation function $f(x)=x+c$, if the fitted $c$ satisfies $-0.1&lt;c&lt;0.1$, the procedure adds 1 to its counter. Similarly,</p>
<p>Table 4: Primitive functions tested for clustering</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Primitive Function</th>
<th style="text-align: center;">Cluster Points</th>
<th style="text-align: right;">Cluster Tolerance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$(\cdot)^{2}$</td>
<td style="text-align: center;">${0}$</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">$(\cdot)^{3}$</td>
<td style="text-align: center;">${0}$</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">$\sin (\cdot)$</td>
<td style="text-align: center;">${1,-1}$</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">$\cos (\cdot)$</td>
<td style="text-align: center;">${1,-1}$</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">$(\cdot)^{c}$</td>
<td style="text-align: center;">${1}$</td>
<td style="text-align: right;">0.5</td>
</tr>
</tbody>
</table>
<p>for the activation functions $f(x)=c x$ and $f(x)=x^{c}$, if the fitted $c$ satisfies $-0.1&lt;c&lt;0.1$ or $0.9&lt;c&lt;1.1$, the procedure adds 1 to its counter. We select these ranges to capture instances of imperfect constant fitting without labeling legitimate solutions as trivial. After checking all activation functions used, the procedure returns the counter.</p>
<p>The $\phi[f]$ term returns the sum of the outputs of the first and second procedures. We find that a weight of $w_{\phi} \approx 0.7$ for $\phi[f]$ is most effective in our loss function. This value of $w_{\phi}$ ensures that most trivial $f$ have $K_{i}(\mathcal{M}, f)-s \cdot w_{\phi} \cdot \phi[f]&lt;0$, thus actively reducing the weights corresponding to functions with trivial operations, without over punishing functions and hindering learning.</p>
<p>The $\psi[f]$ Regularization Term: When punishing trivial operations using the $\phi$ term, we find that the network discovers many nontrivial operations which very closely approximate trivial operations by exploiting portions of functions with near-zero derivatives, which can be used to artificially compress data. For example, $\cos (x / 2)$ closely approximates 1 if $-1&lt;x&lt;1$. Unfortunately, it is often difficult to determine if a function approximates a trivial function simply from its symbolic representation. This issue is also identified in [38].</p>
<p>To detect these trivial function approximations, we develop an approach that analyzes the activation functions' outputs during the forward pass. The $\psi[f]$ term counts the number of activation functions which, during a forward pass, the network identifies as possibly approximating trivial solutions, as well as a metric for how close to trivial these functions are. For each primitive function, the network stores values around which outputs of that function often cluster artificially. Table 4 lists the primitives which the network tests for clustering.</p>
<p>The procedure for determining $\psi$ is as follows. The algorithm begins with a counter of 0 . During the forward pass, if the network reaches a primitive function $\phi$ listed in Table 4, the algorithm tests each ordered tuple $(\phi, a, \delta)$ from Table 4, where $a$ is the point tested for clustering and $\delta$ is the cluster tolerance. If the mean of all the outputs of the primitive function, $\bar{y}$, for a given batch satisfies $|\bar{y}-a|&lt;\delta$, the algorithm adds $\min (5,0.1 /|\bar{y}-a|)$ to the counter. These expressions increase with the severity of clustered data; the more closely the outputs are clustered, the higher the punishment term. The minimum term ensures that $\psi[f]$ is never infinite.</p>
<p>We also test for the approximation $\sin (x) \approx x$ by testing the inputs and outputs of the sine primitive function. If the inputs and outputs $x$ and $y$ of the sine primitive satisfy $\overline{|y-x|}&lt;0.1$, the algorithm adds $\min (5,0.05 / \overline{|y-x|})$ to the counter. In the future, we plan to consider more approximations similar to the small angle approximation.
$\psi[f]$ should not artificially punish functions involving the primitives listed in Table 4 that are not trivial approximations because no proper use of these primitive functions will always produce outputs very close to the clustering points. Because $\psi[f]$ flags functions based on their batch outputs, each batch will likely give different outcomes. This allows $\psi[f]$ to better discriminate between trivial function approximations and nontrivial operations: $\psi[f]$ should flag trivial function approximations often, but it should only flag nontrivial operations rarely when the inputs statistically fluctuate to produce clustered outputs. In practice, we find that a weight of $w_{\psi} \approx 0.3$ for $\psi[f]$ is most effective in our loss function.</p>
<p>The $\xi[f]$ Regularization Term: When our network converges to the correct solution, it may converge to a more complicated expression equivalent to the desired expression. To promote simpler expressions, we slightly punish functions based on their complexity. The $\xi[f]$ term counts the number of activation functions used to produce $f$, which serves as a measure of $f$ 's complexity. We find that a small weight of $w_{\xi} \approx 0.1$ for $\xi[f]$ is most effective in our loss function. This small value has little significance when distinguishing between a function that fits a dataset well and a function that does not, but it is enough to promote simpler functions</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ we define for any $\mathbf{z}=\left[z_{1}, \ldots, z_{N_{l}}\right]$ the softmax function as follows $\operatorname{softmax}(\mathbf{z} ; T):=\left[\frac{\exp \left(z_{1} / T\right)}{\sum_{i=1}^{N_{l}} \exp \left(z_{i} / T\right)}, \ldots, \frac{\exp \left(z_{N_{l}} / T\right)}{\sum_{i=1}^{N_{l}} \exp \left(z_{i} / T\right)}\right]$&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
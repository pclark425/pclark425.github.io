<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2246 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2246</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2246</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-281886443</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.05692v1.pdf" target="_blank">Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies</a></p>
                <p><strong>Paper Abstract:</strong> A prevailing approach for learning visuomotor policies is to employ reinforcement learning to map high-dimensional visual observations directly to action commands. However, the combination of high-dimensional visual inputs and agile maneuver outputs leads to long-standing challenges, including low sample efficiency and significant sim-to-real gaps. To address these issues, we propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a novel framework designed to improve the sample efficiency and asymptotic performance of visuomotor policy learning. OMC-RL explicitly decouples the learning process into two stages: an upstream representation learning stage and a downstream policy learning stage. In the upstream stage, a masked Transformer module is trained with temporal modeling and contrastive learning to extract temporally-aware and task-relevant representations from sequential visual inputs. After training, the learned encoder is frozen and used to extract visual representations from consecutive frames, while the Transformer module is discarded. In the downstream stage, an oracle teacher policy with privileged access to global state information supervises the agent during early training to provide informative guidance and accelerate early policy learning. This guidance is gradually reduced to allow independent exploration as training progresses. Extensive experiments in simulated and real-world environments demonstrate that OMC-RL achieves superior sample efficiency and asymptotic policy performance, while also improving generalization across diverse and perceptually complex scenarios.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2246.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2246.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OMC-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle-Guided Masked Contrastive Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage visuomotor learning framework that decouples upstream masked contrastive representation learning (latent masked reconstruction with a Transformer) from downstream oracle-guided policy learning (KL imitation from a privileged-state oracle). The Transformer is used only during pretraining for temporal context and discarded at deployment; the frozen encoder provides temporally-aware, task-relevant latent features for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OMC-RL (Oracle-Guided Masked Contrastive Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level masked feature reconstruction with temporal context (i.e., reconstruct masked latent embeddings rather than pixels)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit stochastic masking of sequence positions + contrastive learning (InfoNCE) over reconstructed latent tokens; temporal attention inside Transformer encourages task-relevant temporal patterns</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visuomotor autonomous drone navigation (first-person monocular RGB sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — tested with texture variations, obstacle color changes, hue/brightness shifts, random masking of inputs, occlusions and illumination changes in sim and real-world</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simulated (trained in box scene, evaluated without finetuning): Furniture: NE 1.49 m, OS 92.5%, SR 91.5%, SPL 0.88, CR 2.5%, TTS 502; Barrier: NE 1.62 m, OS 86.0%, SR 84.5%, SPL 0.80, CR 5.5%, TTS 525; Tree: NE 2.01 m, OS 80.5%, SR 78.5%, SPL 0.73, CR 8.0%, TTS 516. Real-world deployment: success rates 70% (normal), 65% (color variations), 55% (illumination disturbance).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Upstream encoder: lightweight CNN (two conv layers, then FC) producing d=384 latent; Transformer encoder used during pretraining (single-head attention, L blocks) with batch size 32, sequence length T=16, masking prob ρ_m=0.5; projection MLP used. Learning rates: encoder/projector 1e-3, Transformer 2e-3 with warm-up 6000 steps; key encoder updated with momentum m=0.05. Downstream PPO uses policy nets with two FC layers (256 units). No FLOPs, total parameter counts, or wall-clock training time reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms baselines trained under identical encoder capacity: In Furniture scenario SR: OMC-RL 91.5% vs CURL 72.5% (Δ+19.0pp), NPE 17.5% (Δ+74.0pp), PPO 12.0% (Δ+79.5pp). Similar relative advantages hold across Barrier/Tree scenes (e.g., Tree SR: OMC-RL 78.5% vs CURL 0.0% in that scenario; Table I). Real-world: qualitatively more robust than CURL under visual disturbances per attention and t-SNE analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Explicit transfer evaluation: encoder and policy trained in box-based environment, evaluated zero-shot in furniture/barrier/tree scenes. OMC-RL maintains high success rates across domains (Furniture SR 91.5%, Barrier 84.5%, Tree 78.5%) demonstrating strong sim-to-sim and sim-to-real transfer when compared to baselines which drop substantially (CURL and PPO drop more). Real-world success rates (70%/65%/55%) show partial sim-to-real transfer under disturbances.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not framed as multi-task RL, but evaluated across multiple distinct environments (easy/medium/hard). Uses a single shared encoder frozen at downstream time and a single policy per task; shows consistent performance across tasks/environments without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades if masking probability is too high (e.g., ρ_m = 0.7 or 0.9) or too low (ρ_m = 0.1 or 0.3); removing oracle guidance significantly reduces performance in complex scenes; introducing Transformer into both query and key branches (dual-Transformer) hurts downstream performance due to overreliance on temporal module during pretraining; real-world SR falls under heavy illumination shifts (55%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Reported ablations: masking probability sweep (ρ_m ∈ {0.1,0.3,0.5,0.7,0.9}) — best at 0.5; sequential-input vs frame-wise contrastive (CURL-cons leads to instability and collapse at high-frequency sequential sampling); Transformer-asymmetry ablation (query-only Transformer best; dual-Transformer degrades); projection head φ ablation (removal degrades performance); oracle teacher removal (our w/o OT causes notable drops especially in Barrier/Tree); decay schedule ablation (linear annealing of KL coefficient α outperforming fixed and exponential schedules). Quantitative specifics: see Table I and Fig.10–13 for relative metric drops (examples in text).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitatively faster convergence than baselines (plotted training curves show OMC-RL achieves higher episodic rewards earlier). Oracle guidance and frozen pretrained encoder both cited as key contributors to improved sample efficiency; exact sample-step counts to reach thresholds are not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Zero-shot evaluation across three held-out environments and real-world tests; attention visualizations (Grad-CAM) show OMC-RL focuses on task-relevant regions under disturbances; t-SNE of features under brightness/hue/masking shows OMC-RL features remain clustered and aligned with original inputs while CURL features shift/dispersion increases — evidence of invariance and improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>OMC-RL performs masked reconstruction at the latent-feature level (reconstruct masked latent tokens) and optimizes InfoNCE over latent similarity; no pixel-level reconstruction metrics (MSE/PSNR/SSIM) reported because raw-pixel reconstruction is not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper analyzes task-relevance via Grad-CAM attention maps (shows OMC-RL attends to salient task-relevant regions even under color/illumination shifts) and t-SNE of feature embeddings under input perturbations (shows stronger feature invariance and clustering vs CURL). The method's masked contrastive objective is explicitly designed to encourage retention of semantically relevant, temporally-consistent features.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No dynamic adjustment of abstraction level at runtime; the Transformer used for higher-level temporal abstraction is discarded after pretraining and the frozen CNN encoder (latent-level features) is used during deployment — so abstraction level is static at deployment (latent features only).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Explicit treatment: KL imitation term D_KL(π_oracle || π_agent) is weighted by α which is linearly annealed from 0.95 to 0 over 10k steps to move from heavy oracle imitation (exploitation of privileged policy) to independent exploration; ablation shows linear schedule outperforms fixed and exponential decays.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Provides a generalization bound (Theorem 1) linking contrastive population/empirical risk to representation approximation error (norm between learned z_i and optimal z*_i) and covering numbers; no empirical mutual information or rate-distortion experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Authors argue and empirically support that reconstructing latent features (not pixels) preserves task-relevant information while avoiding costly pixel-reconstruction; no pixel-fidelity metrics provided; comparison to pixel-level reconstruction literature is conceptual rather than quantitative.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2246.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2246.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Masked Contrastive Representation (component)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Temporal Contrastive Representation Learning (Transformer + InfoNCE on masked latent tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An upstream pretraining module that masks tokens in a sequence of latent visual embeddings, reconstructs them via a Transformer, and applies an InfoNCE contrastive loss between reconstructed (query) embeddings and non-masked (key) embeddings to learn temporally-aware, task-relevant features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Masked temporal contrastive learning (Transformer-assisted latent masked reconstruction + InfoNCE)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-level (masked latent tokens within temporally stacked frames)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit stochastic masking of sequence positions (Bernoulli mask) combined with contrastive loss that enforces similarity of reconstructed latent to original key — Transformer attention implicitly focuses on temporally-predictive components</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>pretraining for visuomotor drone navigation encoder (then frozen for downstream RL)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>trained with trajectories that include diverse visual appearances; downstream evaluation includes color/hue/illumination disturbances and occlusions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When paired with downstream policy, encoders pretrained with masked contrastive learning at ρ_m=0.5 yield best downstream SR (see OMC-RL metrics); explicit encoder-only metrics not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Pretraining uses sequences T=16, latent dim d=384, batch size 32; Transformer optimized with separate LR 2e-3 and warm-up 6000 steps. The Transformer is discarded at inference to reduce inference cost; no parameter count or FLOPs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Directly compared to CURL (frame-wise contrastive) and to variants where CURL uses sequential inputs (CURL-cons) — masked temporal contrastive with masking outperforms these, with CURL suffering instability when trained on high-frequency sequential inputs. Ablation showed masked approach is superior to plain sequential contrastive without masking.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Encoders pretrained with masked contrastive generalize better when frozen and used downstream: zero-shot transfer to furniture/barrier/tree scenes shows strong performance vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated across formally distinct tasks, but encoder transferred across multiple environment types without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Excessive masking (high ρ_m) destroys too much information and harms downstream performance; too little masking yields insufficient reconstruction signal; training CURL on sequential frames without masking can collapse or be unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Masking probability sweep (best ρ_m=0.5); projection head φ ablation (removing projection degrades performance); Transformer asymmetry (query-only best vs dual-Transformer worse).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pretrained encoder reduces downstream training instability and accelerates policy learning (qualitative), enabling faster convergence in RL compared to end-to-end training baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Pretrained masked-contrastive encoder yields features that remain clustered under input perturbations and supports higher downstream generalization, as evidenced by t-SNE and sim->real results.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Reconstruction target is latent embeddings (no pixel MSE reported). Contrastive InfoNCE ensures reconstructed latent is similar to corresponding non-masked key.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Masking + temporal modeling encourages the encoder to preserve temporally predictive and semantically relevant latent structure; Grad-CAM and t-SNE analyses on downstream show attention on salient task-relevant regions and stability under corruption.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Transformer provides higher-level temporal abstraction during pretraining but is removed at inference; no runtime dynamic switching.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Indirect effect: better latent features reduce exploration required to learn policy; explicit exploration/exploitation schedule handled in downstream KL decay.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>The paper provides a generalization bound that links representation approximation error to contrastive population/empirical risk but does not compute mutual information or compression rates.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Authors argue latent reconstruction avoids unnecessary pixel-level fidelity and yields representations better suited for control — supported by empirical downstream improvements versus pixel-augmentation/frame-wise contrastive baselines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2246.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2246.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CURL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Unsupervised Representations for Reinforcement Learning (CURL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior contrastive RL approach that applies instance-level contrastive learning to image observations by maximizing agreement between augmented views of the same frame; used here as a baseline for pixel/frame-wise contrastive representation learning without explicit temporal modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Curl: Contrastive Unsupervised Representations for Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CURL (contrastive instance-level representation for RL)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>frame-level / image-instance level (contrastive between augmented views of the same pixel image)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>contrastive positive/negative sampling over augmented views (no explicit masking or temporal attention)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visuomotor navigation baselines (same drone navigation tasks as OMC-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>tested under same distractors — color/texture/illumination perturbations; CURL shows greater sensitivity in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simulated Furniture: SR 72.5%, OS 75.0%, NE 2.07 m; Barrier: SR 30.0%, OS 34.0%, NE 5.07 m; Tree: SR 0.0%, OS 12.0%, NE 7.36 m (Table I). Real-world: frequent failures under visual disturbances relative to OMC-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Uses the same CNN encoder capacity as OMC-RL encoder in experiments; no Transformer pretraining; identical RL hyperparameters for fairness; exact FLOPs/params not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperforms OMC-RL particularly in temporally-demanding or high-variability scenes: OMC-RL SR advantage +19.0pp in Furniture and larger gaps in Barrier/Tree; CURL's lack of temporal modeling and masked reconstruction is cited as cause.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>CURL encoder (pretrain-and-freeze) transfers worse to held-out environments and real-world: larger performance degradation in outdoor/visual-diverse scenes compared to OMC-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Single-task per environment evaluation; poorer cross-environment generalization indicates less robust shared representation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Attention misplacements under visual disturbances (Grad-CAM), feature shift/dispersion under perturbations (t-SNE), fails in hard Tree environment and many real-world disturbed scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Authors experimented with a variant CURL-cons that uses sequential frames rather than random frames; training CURL-cons at high-frequency sequential sampling caused instability and encoder collapse, showing naive sequential application is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Slower convergence and weaker asymptotic performance than OMC-RL and NPE in training curves (Fig.4). Exact sample counts to reach thresholds not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Quantitative and visual analysis shows CURL features shift under perturbations and attention maps misplace focus under disturbances, leading to worse sim-to-real robustness than OMC-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>No reconstruction is performed; contrastive loss operates on projected features from individual frames.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Empirical analyses (Grad-CAM, t-SNE) indicate CURL focuses less consistently on task-relevant regions and produces less invariant features under perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No mechanism for dynamic abstraction; operates at frame-level instance contrastive abstraction only.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No explicit oracle-guided imitation component; standard RL training only.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided in this paper's evaluation beyond performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>CURL uses augmented pixel views to learn invariances; authors argue temporal modeling + masked latent reconstruction (OMC-RL) yields better task-relevant representations than frame-wise contrastive approaches like CURL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2246.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2246.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Teacher Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Privileged-State Oracle Teacher Policy (π_o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-based expert policy pretrained with privileged global state and depth inputs (full state s_t) used to supply dense supervision to the visuomotor agent during early training via a KL-loss (learning-by-cheating); its influence is annealed out over training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Privileged-state oracle policy (PPO-trained state-based expert)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>full-state geometric/pose & depth-level privileged representation (high-abstraction privileged state)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>oracle inputs include depth sequence, velocities, orientation, and relative positions — explicit privileged features hand-selected by state definition</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>training-time guidance for visuomotor autonomous drone navigation</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Oracle has privileged access to depth and full state and thus is unaffected by visual distractors that affect pixel-based encoders</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Oracle serves as an upper bound: fastest convergence and highest asymptotic performance in training curves; exact numeric upper-bound values not tabulated but qualitatively highest in Fig.4.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Oracle network consists of depth-image encoder plus FC head; trained with PPO. No parameter/FLOP/time reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Adding oracle guidance to baselines (e.g., NPE w/ OT) improves their sample efficiency and early performance but OMC-RL + OT still outperforms these combinations, showing synergy between robust representations and privileged guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Oracle used only during training (privileged). Policies trained with oracle guidance generalize better in complex scenes than policies trained without, but direct transfer of the oracle itself is not applicable (oracle relies on privileged sensors).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not explicitly evaluated as multi-task; oracle guidance improved performance particularly on harder environments (Barrier/Tree) when included during training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Over-reliance on oracle guidance can limit late-stage exploration if KL weight α not decayed properly (fixed α harmed performance); removal of oracle guidance hurts complex-scene performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation removing oracle (our w/o OT) shows slight drop in easy scene but significant drops in barrier/tree; replacing NPE's non-expert teacher with oracle (NPE w/ OT) improves NPE but remains inferior to OMC-RL variations, indicating combined benefits of representation and oracle supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Oracle guidance substantially accelerates early-stage learning; α annealed from 0.95→0 over 10k steps in experiments to balance imitation and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Oracle removal experiments indicate that oracle guidance contributes to better generalization in visually complex environments by providing stronger action priors during early training.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (oracle does not reconstruct visuals).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Oracle operates on hand-selected privileged features (depth, relative positions) making its actions inherently tied to task-relevant geometric information; used as ground-truth behavioral signal.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Oracle is only used during training and its influence is dynamically decreased via linear annealing of KL coefficient α; the agent transitions from imitating high-abstraction privileged policy to exploiting its learned latent visual policy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Directly used to bias early exploitation (imitation) with a schedule to enable later exploration; linear annealing demonstrated superior performance over fixed or exponential decay.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No explicit information-theoretic metrics reported for oracle guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Oracle demonstrates that privileged geometric/depth abstractions can provide strong action guidance that pixel-only encoders lack early in training; combined with good latent encoders this improves sample efficiency and asymptotic performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2246.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2246.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent World Models (category)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent World Models / Model-based Latent Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a separate class of representation learning approaches in related work: models that learn transition dynamics in latent space and enable planning (e.g., model-based RL / world models), contrasted with auxiliary self-supervised methods like contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>latent world model (general category)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-dynamics level (learns transition dynamics and predictions in compact latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>usually learned via predictive losses (next-state prediction), reconstruction, or contrastive alignment; not used directly in this work</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>referenced as applied in RL for planning in latent space (general RL), not evaluated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>not directly evaluated here; world models can be sensitive to pixel-level distractors unless latent focuses on task-relevant features</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No performance metrics in this paper — cited as related work (refs [19]-[21])</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>World-model approaches are contrasted conceptually: authors position their method within auxiliary self-supervised (contrastive) approaches rather than world models; no empirical comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated within this paper; world models are discussed as an alternative that can aid planning and sample-efficiency in other literature.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here; world models in literature sometimes support multi-task transfer via shared dynamics but that is outside this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper does not present empirical failure modes for world models; only notes the two broad families of representation learning (auxiliary vs world modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>None within this work specific to world models.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Mentioned as a route to improve sample efficiency in prior work but not measured here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not evaluated in this paper with respect to world models.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper motivates extracting task-relevant features and contrasts their approach (masked contrastive latent features) with other directions including world models that model transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Conceptually possible in world-model literature but not implemented or analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not addressed for world models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided for world models here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2246.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2246.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pixel-level reconstruction methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pixel-level reconstruction (auto-encoder / reconstruction auxiliary losses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of auxiliary representation learning methods that train encoders via pixel reconstruction losses (reconstruct raw input frames) and are contrasted with instance-level contrastive and latent masked reconstruction approaches in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pixel-level reconstruction (general category, e.g., autoencoders)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>pixel-level reconstruction (low-level visual fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>reconstruction loss forces encoder to preserve all input information, not selectively task-relevant features unless combined with task weighting</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>referenced in representation-learning-for-RL literature (e.g., pixel reconstruction methods for visual RL)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Such methods often retain task-irrelevant visual details (distractors) unless regularized; paper argues latent masked contrastive is more task-focused</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No experimental numbers in this paper — mentioned as prior approaches (refs [18], [36], [37]).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Pixel-reconstruction is typically higher compute and memory compared to latent-only contrastive losses; specific numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Authors argue pixel-level reconstruction preserves too much task-irrelevant information and can be less sample-efficient for downstream control compared to their masked-latent contrastive method; no direct empirical pixel-reconstruction baseline numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated here for pixel-level reconstruction baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tendency to preserve irrelevant pixel-level details and produce representations that overfit visual appearance; not directly quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not directly ablated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Paper positions pixel-reconstruction as less sample-efficient relative to contrastive/latent approaches conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Argued to be weaker under domain shift because pixel-level encoders must preserve non-semantic image statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper contrasts pixel reconstruction with masked-latent contrastive learning to motivate selective retention of task-relevant features.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curl: Contrastive Unsupervised Representations for Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Learning visual representation for autonomous drone navigation via a contrastive world model <em>(Rating: 2)</em></li>
                <li>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model <em>(Rating: 2)</em></li>
                <li>Solar: Deep structured representations for model-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Decoupling representation learning from reinforcement learning <em>(Rating: 2)</em></li>
                <li>Contrastive learning for enhancing robust scene transfer in vision-based agile flight <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2246",
    "paper_id": "paper-281886443",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "OMC-RL",
            "name_full": "Oracle-Guided Masked Contrastive Reinforcement Learning",
            "brief_description": "A two-stage visuomotor learning framework that decouples upstream masked contrastive representation learning (latent masked reconstruction with a Transformer) from downstream oracle-guided policy learning (KL imitation from a privileged-state oracle). The Transformer is used only during pretraining for temporal context and discarded at deployment; the frozen encoder provides temporally-aware, task-relevant latent features for RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OMC-RL (Oracle-Guided Masked Contrastive Reinforcement Learning)",
            "abstraction_level": "latent-level masked feature reconstruction with temporal context (i.e., reconstruct masked latent embeddings rather than pixels)",
            "feature_selection_mechanism": "explicit stochastic masking of sequence positions + contrastive learning (InfoNCE) over reconstructed latent tokens; temporal attention inside Transformer encourages task-relevant temporal patterns",
            "task_domain": "visuomotor autonomous drone navigation (first-person monocular RGB sequences)",
            "distractor_presence": "yes — tested with texture variations, obstacle color changes, hue/brightness shifts, random masking of inputs, occlusions and illumination changes in sim and real-world",
            "performance_metrics": "Simulated (trained in box scene, evaluated without finetuning): Furniture: NE 1.49 m, OS 92.5%, SR 91.5%, SPL 0.88, CR 2.5%, TTS 502; Barrier: NE 1.62 m, OS 86.0%, SR 84.5%, SPL 0.80, CR 5.5%, TTS 525; Tree: NE 2.01 m, OS 80.5%, SR 78.5%, SPL 0.73, CR 8.0%, TTS 516. Real-world deployment: success rates 70% (normal), 65% (color variations), 55% (illumination disturbance).",
            "computational_cost_details": "Upstream encoder: lightweight CNN (two conv layers, then FC) producing d=384 latent; Transformer encoder used during pretraining (single-head attention, L blocks) with batch size 32, sequence length T=16, masking prob ρ_m=0.5; projection MLP used. Learning rates: encoder/projector 1e-3, Transformer 2e-3 with warm-up 6000 steps; key encoder updated with momentum m=0.05. Downstream PPO uses policy nets with two FC layers (256 units). No FLOPs, total parameter counts, or wall-clock training time reported.",
            "comparison_to_baselines": "Outperforms baselines trained under identical encoder capacity: In Furniture scenario SR: OMC-RL 91.5% vs CURL 72.5% (Δ+19.0pp), NPE 17.5% (Δ+74.0pp), PPO 12.0% (Δ+79.5pp). Similar relative advantages hold across Barrier/Tree scenes (e.g., Tree SR: OMC-RL 78.5% vs CURL 0.0% in that scenario; Table I). Real-world: qualitatively more robust than CURL under visual disturbances per attention and t-SNE analyses.",
            "transfer_learning_results": "Explicit transfer evaluation: encoder and policy trained in box-based environment, evaluated zero-shot in furniture/barrier/tree scenes. OMC-RL maintains high success rates across domains (Furniture SR 91.5%, Barrier 84.5%, Tree 78.5%) demonstrating strong sim-to-sim and sim-to-real transfer when compared to baselines which drop substantially (CURL and PPO drop more). Real-world success rates (70%/65%/55%) show partial sim-to-real transfer under disturbances.",
            "multi_task_performance": "Not framed as multi-task RL, but evaluated across multiple distinct environments (easy/medium/hard). Uses a single shared encoder frozen at downstream time and a single policy per task; shows consistent performance across tasks/environments without task-specific fine-tuning.",
            "failure_modes": "Performance degrades if masking probability is too high (e.g., ρ_m = 0.7 or 0.9) or too low (ρ_m = 0.1 or 0.3); removing oracle guidance significantly reduces performance in complex scenes; introducing Transformer into both query and key branches (dual-Transformer) hurts downstream performance due to overreliance on temporal module during pretraining; real-world SR falls under heavy illumination shifts (55%).",
            "ablation_studies": "Reported ablations: masking probability sweep (ρ_m ∈ {0.1,0.3,0.5,0.7,0.9}) — best at 0.5; sequential-input vs frame-wise contrastive (CURL-cons leads to instability and collapse at high-frequency sequential sampling); Transformer-asymmetry ablation (query-only Transformer best; dual-Transformer degrades); projection head φ ablation (removal degrades performance); oracle teacher removal (our w/o OT causes notable drops especially in Barrier/Tree); decay schedule ablation (linear annealing of KL coefficient α outperforming fixed and exponential schedules). Quantitative specifics: see Table I and Fig.10–13 for relative metric drops (examples in text).",
            "sample_efficiency": "Qualitatively faster convergence than baselines (plotted training curves show OMC-RL achieves higher episodic rewards earlier). Oracle guidance and frozen pretrained encoder both cited as key contributors to improved sample efficiency; exact sample-step counts to reach thresholds are not numerically reported.",
            "generalization_analysis": "Zero-shot evaluation across three held-out environments and real-world tests; attention visualizations (Grad-CAM) show OMC-RL focuses on task-relevant regions under disturbances; t-SNE of features under brightness/hue/masking shows OMC-RL features remain clustered and aligned with original inputs while CURL features shift/dispersion increases — evidence of invariance and improved generalization.",
            "reconstruction_quality": "OMC-RL performs masked reconstruction at the latent-feature level (reconstruct masked latent tokens) and optimizes InfoNCE over latent similarity; no pixel-level reconstruction metrics (MSE/PSNR/SSIM) reported because raw-pixel reconstruction is not performed.",
            "task_relevance_analysis": "Paper analyzes task-relevance via Grad-CAM attention maps (shows OMC-RL attends to salient task-relevant regions even under color/illumination shifts) and t-SNE of feature embeddings under input perturbations (shows stronger feature invariance and clustering vs CURL). The method's masked contrastive objective is explicitly designed to encourage retention of semantically relevant, temporally-consistent features.",
            "dynamic_abstraction": "No dynamic adjustment of abstraction level at runtime; the Transformer used for higher-level temporal abstraction is discarded after pretraining and the frozen CNN encoder (latent-level features) is used during deployment — so abstraction level is static at deployment (latent features only).",
            "exploration_vs_exploitation": "Explicit treatment: KL imitation term D_KL(π_oracle || π_agent) is weighted by α which is linearly annealed from 0.95 to 0 over 10k steps to move from heavy oracle imitation (exploitation of privileged policy) to independent exploration; ablation shows linear schedule outperforms fixed and exponential decays.",
            "information_theoretic_analysis": "Provides a generalization bound (Theorem 1) linking contrastive population/empirical risk to representation approximation error (norm between learned z_i and optimal z*_i) and covering numbers; no empirical mutual information or rate-distortion experiments reported.",
            "pixel_fidelity_benefits": "Authors argue and empirically support that reconstructing latent features (not pixels) preserves task-relevant information while avoiding costly pixel-reconstruction; no pixel-fidelity metrics provided; comparison to pixel-level reconstruction literature is conceptual rather than quantitative.",
            "uuid": "e2246.0"
        },
        {
            "name_short": "Masked Contrastive Representation (component)",
            "name_full": "Masked Temporal Contrastive Representation Learning (Transformer + InfoNCE on masked latent tokens)",
            "brief_description": "An upstream pretraining module that masks tokens in a sequence of latent visual embeddings, reconstructs them via a Transformer, and applies an InfoNCE contrastive loss between reconstructed (query) embeddings and non-masked (key) embeddings to learn temporally-aware, task-relevant features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Masked temporal contrastive learning (Transformer-assisted latent masked reconstruction + InfoNCE)",
            "abstraction_level": "latent-level (masked latent tokens within temporally stacked frames)",
            "feature_selection_mechanism": "explicit stochastic masking of sequence positions (Bernoulli mask) combined with contrastive loss that enforces similarity of reconstructed latent to original key — Transformer attention implicitly focuses on temporally-predictive components",
            "task_domain": "pretraining for visuomotor drone navigation encoder (then frozen for downstream RL)",
            "distractor_presence": "trained with trajectories that include diverse visual appearances; downstream evaluation includes color/hue/illumination disturbances and occlusions",
            "performance_metrics": "When paired with downstream policy, encoders pretrained with masked contrastive learning at ρ_m=0.5 yield best downstream SR (see OMC-RL metrics); explicit encoder-only metrics not reported.",
            "computational_cost_details": "Pretraining uses sequences T=16, latent dim d=384, batch size 32; Transformer optimized with separate LR 2e-3 and warm-up 6000 steps. The Transformer is discarded at inference to reduce inference cost; no parameter count or FLOPs reported.",
            "comparison_to_baselines": "Directly compared to CURL (frame-wise contrastive) and to variants where CURL uses sequential inputs (CURL-cons) — masked temporal contrastive with masking outperforms these, with CURL suffering instability when trained on high-frequency sequential inputs. Ablation showed masked approach is superior to plain sequential contrastive without masking.",
            "transfer_learning_results": "Encoders pretrained with masked contrastive generalize better when frozen and used downstream: zero-shot transfer to furniture/barrier/tree scenes shows strong performance vs baselines.",
            "multi_task_performance": "Not evaluated across formally distinct tasks, but encoder transferred across multiple environment types without fine-tuning.",
            "failure_modes": "Excessive masking (high ρ_m) destroys too much information and harms downstream performance; too little masking yields insufficient reconstruction signal; training CURL on sequential frames without masking can collapse or be unstable.",
            "ablation_studies": "Masking probability sweep (best ρ_m=0.5); projection head φ ablation (removing projection degrades performance); Transformer asymmetry (query-only best vs dual-Transformer worse).",
            "sample_efficiency": "Pretrained encoder reduces downstream training instability and accelerates policy learning (qualitative), enabling faster convergence in RL compared to end-to-end training baselines.",
            "generalization_analysis": "Pretrained masked-contrastive encoder yields features that remain clustered under input perturbations and supports higher downstream generalization, as evidenced by t-SNE and sim-&gt;real results.",
            "reconstruction_quality": "Reconstruction target is latent embeddings (no pixel MSE reported). Contrastive InfoNCE ensures reconstructed latent is similar to corresponding non-masked key.",
            "task_relevance_analysis": "Masking + temporal modeling encourages the encoder to preserve temporally predictive and semantically relevant latent structure; Grad-CAM and t-SNE analyses on downstream show attention on salient task-relevant regions and stability under corruption.",
            "dynamic_abstraction": "Transformer provides higher-level temporal abstraction during pretraining but is removed at inference; no runtime dynamic switching.",
            "exploration_vs_exploitation": "Indirect effect: better latent features reduce exploration required to learn policy; explicit exploration/exploitation schedule handled in downstream KL decay.",
            "information_theoretic_analysis": "The paper provides a generalization bound that links representation approximation error to contrastive population/empirical risk but does not compute mutual information or compression rates.",
            "pixel_fidelity_benefits": "Authors argue latent reconstruction avoids unnecessary pixel-level fidelity and yields representations better suited for control — supported by empirical downstream improvements versus pixel-augmentation/frame-wise contrastive baselines.",
            "uuid": "e2246.1"
        },
        {
            "name_short": "CURL",
            "name_full": "Contrastive Unsupervised Representations for Reinforcement Learning (CURL)",
            "brief_description": "A prior contrastive RL approach that applies instance-level contrastive learning to image observations by maximizing agreement between augmented views of the same frame; used here as a baseline for pixel/frame-wise contrastive representation learning without explicit temporal modeling.",
            "citation_title": "Curl: Contrastive Unsupervised Representations for Reinforcement Learning",
            "mention_or_use": "use",
            "model_name": "CURL (contrastive instance-level representation for RL)",
            "abstraction_level": "frame-level / image-instance level (contrastive between augmented views of the same pixel image)",
            "feature_selection_mechanism": "contrastive positive/negative sampling over augmented views (no explicit masking or temporal attention)",
            "task_domain": "visuomotor navigation baselines (same drone navigation tasks as OMC-RL)",
            "distractor_presence": "tested under same distractors — color/texture/illumination perturbations; CURL shows greater sensitivity in experiments",
            "performance_metrics": "Simulated Furniture: SR 72.5%, OS 75.0%, NE 2.07 m; Barrier: SR 30.0%, OS 34.0%, NE 5.07 m; Tree: SR 0.0%, OS 12.0%, NE 7.36 m (Table I). Real-world: frequent failures under visual disturbances relative to OMC-RL.",
            "computational_cost_details": "Uses the same CNN encoder capacity as OMC-RL encoder in experiments; no Transformer pretraining; identical RL hyperparameters for fairness; exact FLOPs/params not reported.",
            "comparison_to_baselines": "Underperforms OMC-RL particularly in temporally-demanding or high-variability scenes: OMC-RL SR advantage +19.0pp in Furniture and larger gaps in Barrier/Tree; CURL's lack of temporal modeling and masked reconstruction is cited as cause.",
            "transfer_learning_results": "CURL encoder (pretrain-and-freeze) transfers worse to held-out environments and real-world: larger performance degradation in outdoor/visual-diverse scenes compared to OMC-RL.",
            "multi_task_performance": "Single-task per environment evaluation; poorer cross-environment generalization indicates less robust shared representation.",
            "failure_modes": "Attention misplacements under visual disturbances (Grad-CAM), feature shift/dispersion under perturbations (t-SNE), fails in hard Tree environment and many real-world disturbed scenarios.",
            "ablation_studies": "Authors experimented with a variant CURL-cons that uses sequential frames rather than random frames; training CURL-cons at high-frequency sequential sampling caused instability and encoder collapse, showing naive sequential application is insufficient.",
            "sample_efficiency": "Slower convergence and weaker asymptotic performance than OMC-RL and NPE in training curves (Fig.4). Exact sample counts to reach thresholds not provided.",
            "generalization_analysis": "Quantitative and visual analysis shows CURL features shift under perturbations and attention maps misplace focus under disturbances, leading to worse sim-to-real robustness than OMC-RL.",
            "reconstruction_quality": "No reconstruction is performed; contrastive loss operates on projected features from individual frames.",
            "task_relevance_analysis": "Empirical analyses (Grad-CAM, t-SNE) indicate CURL focuses less consistently on task-relevant regions and produces less invariant features under perturbation.",
            "dynamic_abstraction": "No mechanism for dynamic abstraction; operates at frame-level instance contrastive abstraction only.",
            "exploration_vs_exploitation": "No explicit oracle-guided imitation component; standard RL training only.",
            "information_theoretic_analysis": "Not provided in this paper's evaluation beyond performance comparisons.",
            "pixel_fidelity_benefits": "CURL uses augmented pixel views to learn invariances; authors argue temporal modeling + masked latent reconstruction (OMC-RL) yields better task-relevant representations than frame-wise contrastive approaches like CURL.",
            "uuid": "e2246.2"
        },
        {
            "name_short": "Oracle Teacher Policy",
            "name_full": "Privileged-State Oracle Teacher Policy (π_o)",
            "brief_description": "A state-based expert policy pretrained with privileged global state and depth inputs (full state s_t) used to supply dense supervision to the visuomotor agent during early training via a KL-loss (learning-by-cheating); its influence is annealed out over training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Privileged-state oracle policy (PPO-trained state-based expert)",
            "abstraction_level": "full-state geometric/pose & depth-level privileged representation (high-abstraction privileged state)",
            "feature_selection_mechanism": "oracle inputs include depth sequence, velocities, orientation, and relative positions — explicit privileged features hand-selected by state definition",
            "task_domain": "training-time guidance for visuomotor autonomous drone navigation",
            "distractor_presence": "Oracle has privileged access to depth and full state and thus is unaffected by visual distractors that affect pixel-based encoders",
            "performance_metrics": "Oracle serves as an upper bound: fastest convergence and highest asymptotic performance in training curves; exact numeric upper-bound values not tabulated but qualitatively highest in Fig.4.",
            "computational_cost_details": "Oracle network consists of depth-image encoder plus FC head; trained with PPO. No parameter/FLOP/time reported.",
            "comparison_to_baselines": "Adding oracle guidance to baselines (e.g., NPE w/ OT) improves their sample efficiency and early performance but OMC-RL + OT still outperforms these combinations, showing synergy between robust representations and privileged guidance.",
            "transfer_learning_results": "Oracle used only during training (privileged). Policies trained with oracle guidance generalize better in complex scenes than policies trained without, but direct transfer of the oracle itself is not applicable (oracle relies on privileged sensors).",
            "multi_task_performance": "Not explicitly evaluated as multi-task; oracle guidance improved performance particularly on harder environments (Barrier/Tree) when included during training.",
            "failure_modes": "Over-reliance on oracle guidance can limit late-stage exploration if KL weight α not decayed properly (fixed α harmed performance); removal of oracle guidance hurts complex-scene performance.",
            "ablation_studies": "Ablation removing oracle (our w/o OT) shows slight drop in easy scene but significant drops in barrier/tree; replacing NPE's non-expert teacher with oracle (NPE w/ OT) improves NPE but remains inferior to OMC-RL variations, indicating combined benefits of representation and oracle supervision.",
            "sample_efficiency": "Oracle guidance substantially accelerates early-stage learning; α annealed from 0.95→0 over 10k steps in experiments to balance imitation and exploration.",
            "generalization_analysis": "Oracle removal experiments indicate that oracle guidance contributes to better generalization in visually complex environments by providing stronger action priors during early training.",
            "reconstruction_quality": "Not applicable (oracle does not reconstruct visuals).",
            "task_relevance_analysis": "Oracle operates on hand-selected privileged features (depth, relative positions) making its actions inherently tied to task-relevant geometric information; used as ground-truth behavioral signal.",
            "dynamic_abstraction": "Oracle is only used during training and its influence is dynamically decreased via linear annealing of KL coefficient α; the agent transitions from imitating high-abstraction privileged policy to exploiting its learned latent visual policy.",
            "exploration_vs_exploitation": "Directly used to bias early exploitation (imitation) with a schedule to enable later exploration; linear annealing demonstrated superior performance over fixed or exponential decay.",
            "information_theoretic_analysis": "No explicit information-theoretic metrics reported for oracle guidance.",
            "pixel_fidelity_benefits": "Oracle demonstrates that privileged geometric/depth abstractions can provide strong action guidance that pixel-only encoders lack early in training; combined with good latent encoders this improves sample efficiency and asymptotic performance.",
            "uuid": "e2246.3"
        },
        {
            "name_short": "Latent World Models (category)",
            "name_full": "Latent World Models / Model-based Latent Dynamics",
            "brief_description": "Mentioned as a separate class of representation learning approaches in related work: models that learn transition dynamics in latent space and enable planning (e.g., model-based RL / world models), contrasted with auxiliary self-supervised methods like contrastive learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "latent world model (general category)",
            "abstraction_level": "latent-dynamics level (learns transition dynamics and predictions in compact latent space)",
            "feature_selection_mechanism": "usually learned via predictive losses (next-state prediction), reconstruction, or contrastive alignment; not used directly in this work",
            "task_domain": "referenced as applied in RL for planning in latent space (general RL), not evaluated in this paper",
            "distractor_presence": "not directly evaluated here; world models can be sensitive to pixel-level distractors unless latent focuses on task-relevant features",
            "performance_metrics": "No performance metrics in this paper — cited as related work (refs [19]-[21])",
            "computational_cost_details": "Not specified in this paper.",
            "comparison_to_baselines": "World-model approaches are contrasted conceptually: authors position their method within auxiliary self-supervised (contrastive) approaches rather than world models; no empirical comparison provided.",
            "transfer_learning_results": "Not evaluated within this paper; world models are discussed as an alternative that can aid planning and sample-efficiency in other literature.",
            "multi_task_performance": "Not evaluated here; world models in literature sometimes support multi-task transfer via shared dynamics but that is outside this paper's experiments.",
            "failure_modes": "Paper does not present empirical failure modes for world models; only notes the two broad families of representation learning (auxiliary vs world modeling).",
            "ablation_studies": "None within this work specific to world models.",
            "sample_efficiency": "Mentioned as a route to improve sample efficiency in prior work but not measured here.",
            "generalization_analysis": "Not evaluated in this paper with respect to world models.",
            "reconstruction_quality": "Not applicable here.",
            "task_relevance_analysis": "Paper motivates extracting task-relevant features and contrasts their approach (masked contrastive latent features) with other directions including world models that model transitions.",
            "dynamic_abstraction": "Conceptually possible in world-model literature but not implemented or analyzed here.",
            "exploration_vs_exploitation": "Not addressed for world models in this paper.",
            "information_theoretic_analysis": "Not provided for world models here.",
            "uuid": "e2246.4"
        },
        {
            "name_short": "Pixel-level reconstruction methods",
            "name_full": "Pixel-level reconstruction (auto-encoder / reconstruction auxiliary losses)",
            "brief_description": "Class of auxiliary representation learning methods that train encoders via pixel reconstruction losses (reconstruct raw input frames) and are contrasted with instance-level contrastive and latent masked reconstruction approaches in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "pixel-level reconstruction (general category, e.g., autoencoders)",
            "abstraction_level": "pixel-level reconstruction (low-level visual fidelity)",
            "feature_selection_mechanism": "reconstruction loss forces encoder to preserve all input information, not selectively task-relevant features unless combined with task weighting",
            "task_domain": "referenced in representation-learning-for-RL literature (e.g., pixel reconstruction methods for visual RL)",
            "distractor_presence": "Such methods often retain task-irrelevant visual details (distractors) unless regularized; paper argues latent masked contrastive is more task-focused",
            "performance_metrics": "No experimental numbers in this paper — mentioned as prior approaches (refs [18], [36], [37]).",
            "computational_cost_details": "Pixel-reconstruction is typically higher compute and memory compared to latent-only contrastive losses; specific numbers not provided.",
            "comparison_to_baselines": "Authors argue pixel-level reconstruction preserves too much task-irrelevant information and can be less sample-efficient for downstream control compared to their masked-latent contrastive method; no direct empirical pixel-reconstruction baseline numbers provided in this paper.",
            "transfer_learning_results": "Not evaluated here for pixel-level reconstruction baselines.",
            "multi_task_performance": "Not evaluated here.",
            "failure_modes": "Tendency to preserve irrelevant pixel-level details and produce representations that overfit visual appearance; not directly quantified here.",
            "ablation_studies": "Not directly ablated in this work.",
            "sample_efficiency": "Paper positions pixel-reconstruction as less sample-efficient relative to contrastive/latent approaches conceptually.",
            "generalization_analysis": "Argued to be weaker under domain shift because pixel-level encoders must preserve non-semantic image statistics.",
            "reconstruction_quality": "Not reported in this work.",
            "task_relevance_analysis": "Paper contrasts pixel reconstruction with masked-latent contrastive learning to motivate selective retention of task-relevant features.",
            "dynamic_abstraction": "No.",
            "exploration_vs_exploitation": "No.",
            "information_theoretic_analysis": "No.",
            "uuid": "e2246.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curl: Contrastive Unsupervised Representations for Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Learning visual representation for autonomous drone navigation via a contrastive world model",
            "rating": 2
        },
        {
            "paper_title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "rating": 2
        },
        {
            "paper_title": "Solar: Deep structured representations for model-based reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Decoupling representation learning from reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Contrastive learning for enhancing robust scene transfer in vision-based agile flight",
            "rating": 1
        }
    ],
    "cost": 0.020271499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies
7 Oct 2025</p>
<p>Student Member, IEEEYuhang Zhang 
Student Member, IEEEJiaping Xiao 
Member, IEEEChao Yan 
Member, IEEEMir Feroskhan 
Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies
7 Oct 2025537AFF0B1D63853BFDF8418EDF42B858arXiv:2510.05692v1[cs.RO]autonomous robotvisuomotor policyreinforcement learningmasked contrastive learninglearning-by-cheating
A prevailing approach for learning visuomotor policies is to employ reinforcement learning to map high-dimensional visual observations directly to action commands.However, the combination of high-dimensional visual inputs and agile maneuver outputs leads to long-standing challenges, including low sample efficiency and significant sim-to-real gaps.To address these issues, we propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a novel framework designed to improve the sample efficiency and asymptotic performance of visuomotor policy learning.OMC-RL explicitly decouples the learning process into two stages: an upstream representation learning stage and a downstream policy learning stage.In the upstream stage, a masked Transformer module is trained with temporal modeling and contrastive learning to extract temporally-aware and task-relevant representations from sequential visual inputs.After training, the learned encoder is frozen and used to extract visual representations from consecutive frames, while the Transformer module is discarded.In the downstream stage, an oracle teacher policy with privileged access to global state information supervises the agent during early training to provide informative guidance and accelerate early policy learning.This guidance is gradually reduced to allow independent exploration as training progresses.Extensive experiments in simulated and real-world environments demonstrate that OMC-RL achieves superior sample efficiency and asymptotic policy performance, while also improving generalization across diverse and perceptually complex scenarios.</p>
<p>I. INTRODUCTION</p>
<p>First-person-view navigation for autonomous robots represents a challenging task that emulates human behavioral paradigms [1], with wide-ranging applications across critical domains, including autonomous driving [2], field exploration [3], and urban surveillance [4].Skilled human operators demonstrate the ability to perform agile and precise maneuvers using only delayed and occasionally occluded visual feedback from onboard monocular cameras, without compromising safety.This remarkable capability is attributed to the human brain's ability to distill abstract perceptual signals into compact, low-dimensional representations that serve as internal interpretations for guiding action.Given the complexity and unstructured nature of real-world environments, autonomous robotic systems must be equipped with a feature extractor capable of filtering out noise and redundancy from highdimensional visual feeds, and extracting effective representa-tions for decision-making.</p>
<p>Traditional methods typically follow a mapping-planning paradigm [5], including Simultaneous Localization and Mapping (SLAM) [6] and Structure from Motion (SfM) [7].These pipelines heavily rely on sophisticated hand-crafted features for keypoint detection and matching.Although effective, their reliance on feature engineering limits flexibility and incurs high computational cost.Moreover, their performance degrades significantly in textureless or visually ambiguous environments.Deep learning (DL) [8] offers an alternative paradigm by leveraging deep neural networks (DNNs) to extract task-relevant features directly from raw sensory observations and map them to action commands.In contrast to heuristic feature engineering, DNN-based encoders extract representations directly from data, allowing the model to adapt to diverse input distributions without manual intervention.However, training visuomotor policies for autonomous robot navigation with DL often demands expert-annotated action datasets as supervision signals, which are expensive to collect, particularly in real-world settings [9].</p>
<p>Recently, reinforcement learning (RL) has achieved expertlevel performance, even exceeding human professionals in domains such as Go [10], video games [11], and Mahjong [12].By integrating RL algorithms with high-capacity DNNs, deep reinforcement learning (DRL) is increasingly adopted to address complex robotic perception and control tasks [13]- [15].Specifically, in the domain of autonomous robot navigation where visuomotor policies take a sequence of image frames as input, existing DRL algorithms suffer from two major limitations: (i) the curse of dimensionality.The input data is extremely high-dimensional, making it difficult for the policy network to directly learn effective mappings from observations to actions, which often leads to unstable learning dynamics and poor generalization; (ii) sample inefficiency.Due to the trial-and-error nature of RL and the complexity of visual navigation tasks, training typically requires a large number of rollout trajectories, reflecting low sample efficiency and incurring substantial computational cost.These limitations jointly affect both the efficiency and effectiveness of DRL algorithms.</p>
<p>To address the aforementioned challenges, recent studies have explored representation learning, which trains a feature encoder to compress high-dimensional visual inputs into compact representations before feeding them into the downstream policy network.However, it remains non-trivial to design representation learning methods that are both effective and sample-efficient in DRL settings.Existing approaches generally fall into two categories: one category introduces auxiliary self-supervised losses to guide encoder learning [16]- [18], and the other learns a world model that captures transition dynamics and enables planning in the latent space [19]- [21].</p>
<p>As a prominent self-supervised strategy, contrastive learning has recently emerged as a powerful tool in representation learning [22]- [24].Unlike class-level supervised learning approaches, it operates at the instance level and is primarily designed to learn generalizable representations through pretext tasks.Specifically, during pre-training, each input image is subjected to data augmentation techniques such as random cropping or color jittering, resulting in two distinct views of the same instance.These views are then passed through a shared feature encoder to produce latent representations.The fundamental objective of contrastive learning is to draw representations of the same instance closer in the feature space while enforcing dissimilarity with the rest in the batch, typically enforced via a contrastive loss [22].Encouraged by its notable performance in the computer vision domain, contrastive learning has been extended to RL settings, commonly as an auxiliary loss [16], [25].A prominent example is CURL [25], which stands for contrastive unsupervised representations for RL.It demonstrates that pixel-based RL can rival the performance of state-based RL counterparts.</p>
<p>Further explorations have applied contrastive learning to autonomous robot navigation tasks [26]- [28], where it has shown promise in improving robustness to visual interference and enhancing sample efficiency.Nonetheless, most existing approaches neglect the strong temporal correlations across consecutive frames.In visuomotor policy learning, understanding such temporal coherence is crucial.Intuitively, a representation encoder should not only process the current observation accurately, but also encode contextual relationships across adjacent frames.In addition, jointly optimizing the encoder and policy network often leads to domain-specific overfitting, thereby hindering generalization to unseen environments.</p>
<p>Encouraged by prior advances, this paper proposes a novel framework named oracle-guided masked contrastive RL (OMC-RL) to enhance the sample efficiency and asymptotic performance of visuomotor policy learning.As illustrated in Fig. 1, OMC-RL differs from vanilla teacher-student policy learning and contrastive learning by integrating masked temporal contrastive learning with oracle-guided supervision, leading to joint improvements in both feature encoding and downstream decision-making.Specifically, we decouple the feature encoder from the RL policy to allow stable representation learning and independent control over encoder optimization.For upstream feature extraction, unlike previous approaches that utilize DNNs as pixel encoders [25], we incorporate a Transformer encoder [29] as an auxiliary module to model the temporal correlation across sequential observations.Specifically, the latent representations of certain frames are randomly masked and subsequently reconstructed by the Transformer module, encouraging the model to capture temporally consistent representations.A contrastive loss is subsequently designed to jointly train both the DNN encoder and Transformer, encouraging the reconstructed features to be similar to the unmasked ones while distinct from others.For downstream policy learning, we introduce an oracle teacher with privileged access to complete environment states, which is pretrained to generate expert actions.A Kullback-Leibler (KL) divergence loss is designed between the oracle and agent policies to guide policy learning.We conduct extensive experiments in simulated and real-world environments to validate the effectiveness of OMC-RL.Simulation experiments show that OMC-RL consistently outperforms all learning-and planningbased baselines across diverse scenarios.Moreover, real-world in-flight experiments demonstrate that OMC-RL outperforms the state-of-the-art baseline, further validating the advantages of the proposed framework in practical deployment.</p>
<p>The contributions of this paper are summarized below.1) We propose a masked contrastive learning method for robot navigation to address the curse of dimensionality and sample inefficiency arising from high-dimensional visual inputs.The encoder is decoupled from the policy network to enable stable and task-agnostic representation learning.By reconstructing masked latent features from sequential inputs, the method improves temporal awareness and enhances robustness to domain shift.2) We introduce an oracle-guided visuomotor policy learning framework, where an expert policy with privileged access to global environment states serves as a trainingtime oracle.By developing a learning-by-cheating strategy based on KL divergence, the agent is empowered to leverage dense supervision from the oracle, thereby improving sample efficiency and asymptotic performance.3) We demonstrate that OMC-RL achieves strong simulation performance compared with various baselines.More importantly, we show that it surpasses the state-of-the-art baseline in real-world in-flight experiments and maintains robust performance under diverse visual disturbances.The remainder of this paper is organized as follows.Section II reviews prior work on visuomotor policy learning, representation learning for RL, and sample-efficient RL.Section III introduces the preliminaries and formalizes the problem.Section IV details the methodology of the proposed framework.Section V describes the experimental setup for evaluating the proposed method.Section VI presents the results and analysis in simulation and physical environments.Finally, Section VII concludes the paper and outlines future research directions.</p>
<p>II. RELATED WORK</p>
<p>This section reviews related work in three key areas: visuomotor policy learning from high-dimensional observations, representation learning for RL, and methods for improving sample efficiency in RL.</p>
<p>A. Visuomotor Policy Learning</p>
<p>At the core of visuomotor policy learning lies the challenge of approximating the mapping from high-dimensional visual observations to control actions.This problem has been widely addressed using DL [9], [30], [31], where large-scale realworld datasets, such as crash records [30] or urban traffic footage [9], are used to train a DNN to perform this mapping from pixel-level inputs to action commands.However, these DL-based approaches suffer from limited generalization capacity and brittleness to distributional shifts, making them highly sensitive to unseen scenarios.</p>
<p>To overcome these drawbacks, DRL [13], [32]- [35] has emerged as an alternative, offering a trial-and-error learning paradigm that enables agents to generalize through interaction.Thanks to its exploratory nature and ability to learn from feedback rewards, DRL exhibits greater robustness when deployed in previously unseen environments.For instance, Kaufmann et al. [13] proposed a DRL-based drone racing framework that learns agile visuomotor control policies directly from raw pixel input, achieving performance surpassing champions.Zhang et al. [32] further enhanced DRL for visuomotor learning by introducing differentiable physics, enabling more efficient and accurate policy optimization through low-variance gradients.</p>
<p>Distinct from previous efforts that directly optimize over raw pixels, our approach focuses on extracting compact and task-relevant representations from high-dimensional sequential visual inputs.This design aims to improve training efficiency and enable better transferability, especially under challenging real-world conditions.</p>
<p>B. Representation Learning for RL</p>
<p>Learning effective representations is crucial for visual RL tasks.Existing approaches can be broadly categorized into two types: auxiliary self-supervised learning [16]- [18] and latent world modeling for planning [19]- [21].Our work complements the first category.</p>
<p>Among auxiliary self-supervised methods, one strategy is pixel-level reconstruction [18], [36], [37], where the encoder is trained to reconstruct raw input observations, thereby encouraging the extraction of informative features that preserve task-relevant information.Another strategy is instance-level contrastive learning [26]- [28], [38], [39], which aims to produce similar representations for positive pairs and dissimilar ones for negatives, thereby enhancing discriminative capacity.For instance, Xing et al. [27] introduced an adaptive multipair contrastive learning method into DRL for visuomotor policy learning.Positive pairs were constructed from spatially adjacent frames along the flight track, while distant frames were treated as negatives, enabling local continuity in the learned representation.Zhang et al. [26] further proposed a cross-modal contrastive learning approach to improve domain transfer of visuomotor policies.The contrastive objective was designed to extract depth-aware features from monocular images by aligning RGB-depth pairs, resulting in consistent navigation performance under visual interference and environmental changes.</p>
<p>Different from prior works that utilize multi-layer perceptrons or DNNs to interpret pixels, our work extends instancelevel contrastive learning by incorporating Transformer [29] as a powerful sequential modeling backbone to process consecutive frames.It allows the encoder to model temporal dependencies explicitly, which enhances context understanding and supports temporally consistent decision making.</p>
<p>C. Sample-Efficient RL</p>
<p>Improving sample efficiency remains a long-standing challenge in visual RL, particularly in complex real-world robotics tasks such as visuomotor policy learning.Low sample efficiency often leads to prohibitive training time, excessive trialand-error interactions, and poor transferability [40].</p>
<p>Existing methods can be broadly categorized into two directions: input-level optimization [25], [41], [42] and policy-level guidance [1], [33], [43], [44].The former direction focuses on reducing the burden of policy optimization by learning low-dimensional, compact visual representations from highdimensional observations (e.g., CURL [25]).These approaches align with representation learning techniques discussed in the previous section.Moreover, recent efforts have incorporated data augmentation [41], [42] into the RL pipeline, improving generalization from limited data and enhancing resilience to visual variations.The latter direction introduces prior demonstrations to assist policy learning.Notable progress has been made in methods like deep Q-learning from demonstrations (DQfD) [43], where the replay buffer is enriched with prior demonstrations to accelerate early-stage learning.Beyond fixed demonstrations, recent studies [1], [33], [44] explore the use of adaptive guidance by dynamically updating prior policies during training.For instance, Wu et al. [1] proposed a human-guided RL framework for autonomous navigation, where human feedback is used to interactively update a prior policy.This strategy improved sample efficiency by enabling online correction without extensive manual demonstrations.Zhang et al. [33] introduced a non-expert policy-guided DRL framework, in which a suboptimal planner provided reference actions in the early training stage.The guidance was gradually decayed as train progressed to allow for independent exploration, resulting in enhanced data efficiency with reduced cost.In contrast to these methods, our work combines compact representation learning with privileged policy guidance in a unified framework, improving sample efficiency and realworld applicability under constrained perception.</p>
<p>III. PRELIMINARIES AND PROBLEM FORMULATION</p>
<p>This section presents the preliminaries of the proposed framework, including the problem formulation and the contrastive learning objective for visuomotor policy learning.</p>
<p>A. Problem Formulation</p>
<p>Given that visuomotor policy learning for robots is inherently subject to partial observability due to the limited field of view of the onboard monocular camera, we formulate the task as a partially observable Markov decision process (POMDP).The problem is defined by the tuple (O, A, P, R, γ), where O denotes the observation space of high-dimensional image collections captured by the camera, and A is the action space available to the agent.At each timestep t, the agent observes an image o t ∈ O and executes a corresponding action a t ∈ A, which jointly determine the evolution of its interaction with the environment.The transition dynamics model P specifies the conditional distribution over the next observation o t+1 given the current input o t and action a t , that is, P = Pr(o t+1 | o t , a t ).The reward function is defined as R : O × A → R, which maps an observation-action pair (o t , a t ) to a scalar reward, i.e., r t = R(o t , a t ) ∈ R. Finally, γ ∈ (0, 1] denotes the discount factor that balances short-term and long-term rewards.</p>
<p>In DRL, DNNs are employed to process high-dimensional observations and learn sequential decision-making policies through trial-and-error interactions.Since the agent cannot directly access the underlying environment state s t , it must instead utilize a history of recent visual observations to ensure informative action execution.Let h t = (o t−L+1 , o t−L+2 , . . ., o t ) denote this temporal history.To extract actionable representations from raw inputs, we adopt a two-stage architecture: a DNN encoder f θ , parameterized by θ, first maps the observation history h t into a compact latent feature, which is then passed to a policy network π ψ , parameterized by ψ, to generate the control action, i.e., a t = π ψ (f θ (h t )).The learning objective is to obtain an optimal visuomotor policy π * ψ :
f θ (h t ) → A that maximizes the expected cumulative discounted reward, E [ ∞ t=0 γ t r t ].
During deployment, the agent follows the learned policy
π * ψ = arg max π ψ E o∼P, a∼π ψ [R(o, a)],
where decisions are made iteratively by encoding the current observation history h t and sampling actions from the policy.</p>
<p>B. Contrastive Learning</p>
<p>Contrastive learning is a self-supervised representation learning paradigm that formulates instance-level discrimination as a classification task: each sample is treated as its own class and must be distinguished from all other instances in the dataset, without requiring any manual annotations.Given a batch of RGB images I = {I 1 , I 2 , . . ., I N } sampled from the replay buffer, each image I i ∈ I, is processed by two DNN encoders: a query encoder f q and a key encoder f k .These encoders map the inputs into a shared latent space, producing embeddings q i = f q (I i ) and k i = f k (I i ).The learning objective encourages the representation of each positive pair (q i , k i ) to be similar, while ensuring dissimilarity to the remaining keys in K{k i }.This is achieved using the InfoNCE loss [25]:
L InfoNCE = −E I log exp(sim(q i , k i )/τ ) N j=1 exp(sim(q i , k j )/τ ) ,(1)
where sim(•, •) denotes a similarity metric (e.g., cosine similarity), and τ is a temperature hyperparameter.To enhance temporal sensitivity, we adopt a sequence-based encoding scheme where L visual observations are temporally stacked and encoded into a unified latent representation before applying the contrastive loss.This promotes the learning of temporally consistent features that are beneficial for downstream visuomotor policy learning.</p>
<p>IV. METHODOLOGY</p>
<p>This section details the proposed framework, including masked contrastive learning, oracle policy learning, and the two-stage visuomotor policy training procedure.</p>
<p>A. Overview</p>
<p>We present a comprehensive framework for learning robust visuomotor policies under partial observability, formulated as a two-stage learning process.The full training pipeline is illustrated in Fig. 2. Specifically, we decouple the visuomotor policy learning into two complementary components: (i) an upstream representation learning module based on masked contrastive learning, which extracts informative latent features from high-dimensional visual observations; and (ii) a downstream policy learning module based on a learning-by-cheating strategy, guided by privileged supervision from an oracle  1) An oracle teacher policy is first trained using privileged depth inputs and full-state information, providing expert action distributions for downstream supervision.This oracle network supervises the student policy through a learning-by-cheating strategy.Specifically, the agent policy is optimized via KL-divergence against the oracle policy distribution to enable efficient visuomotor policy learning.
SG Stop Gradient EMA Encoder k f θ CNN Encoder f θ Projection Layer ϕ CNN Encoder Oracle Policy ( | ) o t a s ψ π ] [ , , , t t t t p ∆ v ω θ ( ) ] [ , , , g t t t t p ∆ v ω θ ( ) (k k m m θ θ θ ← ⋅ + − ⋅ SG ( ) (1 ) k k m m ω ω ω ← ⋅ + − ⋅ SG Agent Policy ( | )
teacher policy.In the following subsections, the upstream representation learning is detailed in Section IV-B, and the downstream policy learning is presented in Section IV-C, including the oracle teacher policy and the learning-by-cheating strategy.</p>
<p>The complete algorithm and the theoretical justification are summarized in Section IV-D.</p>
<p>B. Upstream Masked Contrastive Representation Learning</p>
<p>To extract generalizable visual features, we decouple representation learning from policy optimization in RL [45], allowing the encoder to be trained via unsupervised objectives independently of reward signals.However, when decoupling is applied directly, RL algorithms typically sample uncorrelated mini-batches for policy optimization and discard collected trajectories after each update.This process disrupts the inherent temporal coherence among observations, potentially hindering stable learning.However, for visuomotor agents, sequential context is critical for informative decision-making due to the partially observable nature of image inputs.To address this, we store full trajectories and exploit the temporal continuity embedded in these sequences to enhance representation learning.Motivated by the success of Transformer architectures in modeling sequence data across domains such as translation [46], [47], we propose a masked contrastive learning framework based on the Transformer architecture to learn temporally consistent representations.</p>
<p>Specifically, at each iteration, a stack of L consecutive images h t ∈ H, where H represents the set of all h t , is first processed by the DNN encoder
f θ : H → R d to obtain a compact d-dimensional latent representation z t = f θ (h t ).
Unlike conventional approaches such as CURL [25], which directly calculates the contrastive loss on these latent features, we introduce an auxiliary transformation pipeline to further process the representation.Specifically, the representation z t is first passed through a non-linear projection module φ, and then encoded by a Transformer module ξ to jointly model temporal dependencies and contextual structure.The final output from ξ(φ(z t )) is used as the anchor embedding for contrastive learning.Subsequently, we will proceed to detail the auxiliary architecture, including data collection and model structure.</p>
<p>Data Collection.We collect our training dataset by recording temporally ordered visual observations from drone flights under manual control, random exploration, and pretrained policies.These sequences are saved chronologically and used for offline training.At each training step, we sample T sequential observations denoted by H = (h 1 , h 2 , . . ., h T ) from the dataset.To enable masked representation learning as described in [46], we define a binary masking vector M = (M 1 , M 2 , . . ., M T ) ∈ {0, 1} T , where each M i is independently sampled from a Bernoulli distribution with masking probability ϱ m ∈ [0, 1].If M i = 1, the corresponding input h i is altered via one of the following stochastic operations: with probability 80%, h i is replaced with a zero tensor; with probability 10%, it is substituted with a randomly sampled input h j from the dataset where j ̸ = i; and with probability 10%, it remains the same.We denote the processed observation as h m i , and obtain the final masked input via:
hm i = M i •h m i +(1−M i )•h i .
The masked sequence is defined as H m = ( hm 1 , hm 2 , . . ., hm T ) and passed through the encoder f θ to generate latent embeddings Ẑ = (z 1 , z 2 , . . ., z T ), where
z i = f θ ( hm i ), hm i ∈ H m . Model Structure.
We adopt a lightweight DNN encoder f θ to extract compact features from consecutive observations.The encoder consists of two convolutional layers with 32 output channels: the first uses a 3 × 3 kernel with stride 2 for spatial downsampling, and the second applies a 3 × 3 kernel with stride 1 for feature refinement.Each convolutional layer is followed by a ReLU activation.The output is then flattened and passed through a fully connected layer, followed by layer normalization and a tanh activation, producing a bounded ddimensional latent representation.</p>
<p>We introduce a projection module φ that applies a nonlinear transformation to the latent features produced by the encoder before contrastive loss computation.This nonlinear mapping aims to project the features into a space where they are more linearly separable for the contrastive objective [22].The module operates on the masked feature sequence Ẑ = (z 1 , z 2 , . . ., z T ).Each z i is passed through a two-layer multilayer perceptron with ReLU activation.Specifically, we compute the projected representation as follows:
φ(z i ) = W (2) • ReLU(W (1) z i + b (1) ) + b (2) ,(2)
where z i ∈ Ẑ, W (1) , W (2) and b (1) , b (2) are learnable weights and biases.The resulting transformed sequence Ẑ(0) = (φ(z 1 ), . . ., φ(z T )) is subsequently passed to the Transformer encoder for temporal modeling.</p>
<p>To reconstruct the masked latent representations rather than raw pixel observations, we employ a Transformer encoder module ξ that refines the partially corrupted embeddings φ(z i ) ∈ Ẑ(0) into temporally contextualized representations.The architecture of ξ follows the encoder-only architecture of the standard Transformer [29] with single-head attention and is composed of L identical blocks.Each block contains a selfattention layer Attn(•) and a feedforward transformation F (•), equipped with residual connections and layer normalization.</p>
<p>Prior to entering the first block, positional encodings are added to each token to incorporate sequential structure, i.e., zi = φ(z i ) + p i , where p i denotes the sinusoidal positional embedding following the standard Transformer setting.At each block l ∈ {1, . . ., L}, the attention mechanism operates on the entire input sequence from the previous block to output Z(l) = (z
(l) 1 , . . . , z(l) T ), where z(l) i
is the result of self-attention at position i in layer l.Query, key, and value vectors are computed as q
(l) i = W (l) Q z(l−1) i , k (l) j = W (l) K z(l−1) j , and v (l) j = W (l) V z(l−1) j , where W (l) Q , W (l) K , W (l)
V are learnable projection matrices for the l-th block.Here, z(l) i can be computed as follows:
z(l) i = Attn(z (l−1) i , Z(l−1) ) = T j=1 α (l) ij v (l) j , α (l) ij = exp (q (l) i ) ⊤ k (l) j / √ d T j ′ =1 exp (q (l) i ) ⊤ k (l) j ′ / √ d .(3)
Then, the output z(l)</p>
<p>i is passed through F(•) consisting of a two-layer MLP with ReLU activation:
z (l) i = F(z (l) i ) = W (l) 2 • ReLU(W (l) 1 z(l) i + b (l) 1 ) + b (l) 2 ,(4)
where all
W (l) 1 , W (l) 2 and b (l) 1 , b(l)
2 are learnable parameters.By repeating the aforementioned operations for L times, we obtain the final output sequence Z (L) = (z
(L) 1 , z (L) 2 , . . . , z (L)
T ), which serves as the input to the contrastive learning objective.</p>
<p>Training Procedure.Here we detail the optimization procedure of f θ , φ, and ξ.Specifically, we employ a query encoder f θ and a key encoder f θ k with identical architecture but separate parameters, along with projection heads φ and φ k of the same configuration.The query embeddings q i are obtained from the masked representations Z (L) produced by the Transformer module ξ, while the key set K = (k 1 , k 2 , . . ., k T ) is constructed from the non-masked inputs H, where
k i = φ k (f θ k (h i )), h i ∈ H.
The key network parameters are updated using a momentum update rule [25]: both the encoder parameters θ k and the projector parameters
ω k of φ k are updated as θ k ← m • SG(θ) + (1 − m) • θ k and ω k ← m • SG(ω) + (1 − m) • ω k , where m ∈ [0, 1]
is the momentum coefficient and SG(•) denotes the stop-gradient operation [48].</p>
<p>The contrastive loss is computed over the masked positions, encouraging each reconstructed representation q i (i.e., z
(L) i ∈ Z (L)
) to be similar to its corresponding positive key k i while dissimilar to the rest of the batch.This formulation aims to enforce the encoder f θ to preserve the most representative and semantically relevant features in the original pixel space.Only by retaining globally consistent patterns can the model accurately reconstruct missing information from partially observed inputs.Note that the loss in OMC-RL follows the same InfoNCE formulation as in Eq. ( 1), with the only difference that q i and k i are derived from masked reconstruction.Formally, the loss is defined as:
L cl (H; θ, φ, ξ) = −E qi M i • log exp (sim(q i , k i )/τ ) T j=1 exp (sim(q i , k j )/τ )
.</p>
<p>(5) After training, we discard ξ and retain only f θ and φ, whose parameters are frozen and used as the visual backbone for downstream RL policy optimization, thereby reducing computational overhead.</p>
<p>C. Downstream Oracle-Guided Policy Learning</p>
<p>Oracle Policy Learning.To provide additional guidance for training the visuomotor policy, we first pretrain an oracle teacher policy, denoted as π o ψ .The oracle teacher π o ψ is a statebased policy with access to the full global state information s t , which is defined as follows:
s t = I d t , v t , ω t , θ t , ∆p t ,(6)
where ψ can be trained using various model-free RL algorithms [49]- [51].In this work, we adopt Proximal Policy Optimization (PPO) [51] for its stability and effectiveness in continuous control tasks, which are critical properties for agile and precise visuomotor policy learning.After training, it serves as guidance for subsequent visuomotor policy learning via a learning-by-cheating strategy.
I d t = {I d t−
Learning-by-Cheating Strategy.After upstream representation learning, the frozen f θ and φ are used to process sequential RGB inputs for downstream RL policy learning.Given the current observation history h t composed of L stacked RGB frames, the d-dimensional latent visual representation is computed as: z t = φ(f θ (h t )), z t ∈ R d .Unlike the oracle teacher policy π o ψ , which operates on the privileged state s t = (I d t , v t , ω t , θ t , ∆p t ), the RL policy π ψ is trained using a partial observation
o t = (z t , v t , ω t , θ t , ∆p (g) t )
, where ∆p (g) t ∈ R 3 represents the relative position between the agent and the goal.The control output is then predicted by passing o t through the actor network ϕ a : u t = ϕ a (o t ).</p>
<p>We adopt PPO [51] as the RL algorithm to optimize π ψ .Given a trajectory T sampled from rollouts, the loss is defined as:
L rl (T ; ϕ a , ϕ c ) = − E t min r t Ât , clip(r t , 1 − ϵ, 1 + ϵ) Ât + E t V (o t ) − Vt 2 ,(7)
where ϕ a and ϕ c are the parameters of the actor and critic networks, respectively, t indexes timesteps sampled from T , r t denotes the probability ratio between the new and old policies, Ât is the estimated advantage, V (o t ) is the predicted state value, and Vt is the Monte Carlo return computed from observed rewards.</p>
<p>To leverage the oracle teacher policy π o ψ for guiding the training, we adopt the learning-by-cheating paradigm, in which the agent is supervised by an oracle policy that has privileged access to full-state information unavailable during deployment.This allows for stronger and more informative guidance throughout training.Specifically, we introduce an additional KL-divergence term between the action distributions of π ψ and π o ψ .Let π ψ (•|o t ) and π o ψ (•|s t ) denote the stochastic policies, the KL divergence is defined as:
D KL (π o ψ ∥ π ψ ) = E a∼π o ψ (•|st) log π o ψ (a|s t ) π ψ (a|o t ) ,(8)
which penalizes deviations from the oracle under the privileged state distribution.To ensure stability, this term is scaled by a decaying coefficient α and a fixed scaling constant β following [33], leading to the final RL policy objective:
L π ψ (T ; ϕ a , ϕ c ) = (1 − α)L rl (T ; ϕ a , ϕ c ) + αβD KL (π o ψ ∥ π ψ ).(9)
The coefficient α decays over time to allow the learned policy to gradually diverge from the oracle and explore its own strategies.Finally, the overall learning objective of our framework is formulated as:
min θ,φ,ξ,ϕa,ϕc L cl (H; θ, φ, ξ) + L π ψ (T ; ϕ a , ϕ c ).(10)</p>
<p>D. Summary and Theoretical Justification</p>
<p>Algorithm Summary.We summarize the overall training pipeline of OMC-RL in Algorithm 1, which integrates upstream masked contrastive representation learning and downstream policy learning under oracle guidance.In the first stage, the agent learns robust visual representations through masked contrastive learning.In the second stage, a visuomotor policy is optimized using a learning-by-cheating strategy, with supervision provided by an oracle teacher policy that has access to privileged information.</p>
<p>Theoretical Justification.The objective of masked contrastive learning is to discover a representation space Z that captures the structural granularity and alignment among samples.Assuming the existence of an optimal representation Z ⋆ , we can define the expected population risk under Z ⋆ as:
R E (θ, φ, ξ) = E hi [L cl (h i , z ⋆ i ; θ, φ, ξ)].(11)
In practice, the corresponding empirical risk over N samples is given by:
R(θ, φ, ξ) = 1 N N i=1 L cl (h i , z ⋆ i ; θ, φ, ξ). (12)
Theorem 1.The generalization error of masked contrastive learning is bounded by the discrepancy between the learned representation space Z and the optimal representation space Z ⋆ , i.e., Encode:
d z = 1 n n i=1 ∥z i − z ⋆ i ∥ 2 . (13)z i = f θ ( hm i ), q i = ξ(φ(z i ))7
Compute keys:
k j = φ k (f θ k (h j )), h j ∈ H 8
Compute L cl (H; θ, φ, ξ) via Eq.( 5) Compute L rl (T ; ϕ a , ϕ c ) via Eq.( 7)
9 θ ← θ − λ θ ∇ θ L cl (H; θ, φ, ξ) 10 φ ← φ − λ φ ∇ φ L cl (H; θ, φ, ξ) 11 ξ ← ξ − λ ξ ∇ ξ L cl (H; θ, φ, ξ) 12 θ k ← m • SG(θ) + (1 − m) • θ k 13 ω k ← m • SG(ω) + (1 − m) • ω k20 Compute D KL (π o ψ ∥ π ψ ) via Eq. (8)
21</p>
<p>Compute L π ψ (T ; ϕ a , ϕ c ) via Eq.( 9)
22 ϕ a ← ϕ a − λ ϕa ∇ ϕa L π ψ (T ; ϕ a , ϕ c ) 23 ϕ c ← ϕ c − λ ϕc ∇ ϕc L π ψ (T ; ϕ a , ϕ c ) 24 end
Proof.Assume the contrastive loss L cl (h i , z ⋆ i ; θ, φ, ξ) is bounded in [a, b] and is λ-Lipschitz continuous with respect to z i .Let N E , N P , and N T denote the covering numbers of the encoder, projection head, and transformer module spaces, respectively.Then, with probability at least 1 − δ, we have:
|R E (θ, φ, ξ) − R(θ, φ, ξ)| ≤ |R E (θ, φ, ξ) − R(θ, φ, ξ)| Hoeffding's inequality + | R(θ, φ, ξ) − R(θ, φ, ξ)| Lipschitz continuous ≤ |a − b| log(2N E • N P • N T /δ) 2n + λ • 1 n n i=1 ∥z i − z ⋆ i ∥ 2 .
Based on Theorem 1, the generalization error of masked contrastive learning can be effectively reduced when the learned representation space Z better approximates the optimal space Z ⋆ .By employing a masking strategy, OMC-RL encourages the model to capture the intrinsic structure of the data, thus providing a theoretical justification for its effectiveness.</p>
<p>V. EXPERIMENTAL SETUP</p>
<p>This section details the experimental setup for evaluating OMC-RL.We describe the simulation environments, evaluation metrics, baseline methods, and key implementation configurations used throughout our experiments.</p>
<p>A. Simulation Setup</p>
<p>We construct our simulation environments in Unity to systematically evaluate the performance and generalization capability of our proposed method under diverse conditions.To this end, we design four distinct scenarios with various complexities, as illustrated in Fig. 3.The first is a structured indoor environment populated with identical box-shaped obstacles.The second is a furniture-filled indoor scene containing a variety of objects with different shapes, sizes, and materials.The third scenario introduces additional structural complexity by incorporating irregular barriers.The fourth setting is an outdoor forest environment densely populated with trees of varying geometries and textures, presenting significant visual diversity and occlusion.These environments provide a gradient of perceptual and navigational difficulty.For a fair comparison of sample efficiency, asymptotic performance, and generalization, all methods, including OMC-RL and baselines, are trained exclusively in the box-based environment and are evaluated in the remaining scenes without fine-tuning.</p>
<p>B. Metrics</p>
<p>We evaluate the performance of our method using several widely adopted metrics [52].</p>
<p>• Navigation Error (NE), defined as the average Euclidean distance between the drone's terminal position and the goal (m).• Oracle Success Rate (OS), which measures the percentage of episodes where any location along the trajectory falls within a fixed success threshold ε of 0.5 m from the goal, as specified in our setup (%).• Success Rate (SR), defined as the percentage of episodes in which the drone reaches the goal (%).• Success weighted by Path Length (SPL), the primary metric reflecting both task completion and navigation efficiency, calculated as SPL = 1</p>
<p>M M i=1</p>
<p>Ii•ℓi max(di,ℓi) , where I i ∈ {0, 1} indicates success, ℓ i is the optimal path length, and d i is the executed trajectory length.</p>
<p>• Collision Rate (CR), defined as the percentage of episodes in which the drone collides with obstacles (%).• Time to Success (TTS), defined as the average number of steps taken in successful episodes.Note that these metrics are applied exclusively in simulation, where an episode is considered successful only if the drone reaches the goal precisely.For real-world experiments, only the success rate is reported, and an episode is deemed successful if the drone arrives within a 0.5 m radius of the goal to ensure safety.</p>
<p>C. Baselines</p>
<p>To comprehensively validate the performance of OMC-RL, we compare it with several representative baselines across RL and traditional planning-based approaches.</p>
<p>• NPE [33]: An improved RL baseline that combines non-optimal demonstrations IL to address the trade-off between sample efficiency and asymptotic performance.We use it to benchmark the effectiveness of our oracleguided IL formulation.</p>
<p>• CURL [25]: A classical contrastive RL framework that learns compact visual representations by maximizing similarity between augmented views of the same observation.</p>
<p>It relies mainly on local image augmentations and CNN encoders without modeling temporal relationships.</p>
<p>• PPO [51]: A widely used model-free RL baseline.We adopt its end-to-end variant that directly maps monocular RGB inputs to continuous action commands without any auxiliary supervision or representation pretraining.• Hybrid APF [53]: A traditional path planning baseline based on the artificial potential field (APF), augmented with A* search to improve global feasibility.This method represents classical non-learning approaches.</p>
<p>D. Parameter and Architecture Configuration</p>
<p>Upstream Masked Contrastive Representation Learning.The encoder is trained using a batch size of 32 over sequences of length T = 16, where each observation h t is formed by stacking L = 3 consecutive RGB frames.Each frame is randomly cropped from 224 × 224 to 192 × 192 before being fed into the encoder.The output latent representation dimension is d = 384.We use a masking probability ϱ m = 0.5, and set the temperature coefficient in the contrastive loss of Eq. ( 5) to τ = 0.07.The encoder f θ and projection head φ are optimized using Adam with learning rates λ θ = λ φ = 1 × 10 −3 , while the Transformer module ξ is trained with a separate learning rate λ ξ = 2 × 10 −3 .A warm-up and inverse square root decay schedule is applied to the Transformer for training stabilization, with a warm-up horizon of 6000 steps.The architecture details of the encoder, projection head, and Transformer are provided in Section IV-B.The momentum coefficient used to update the key encoder and key projection head is set to m = 0.05.Downstream Oracle-Guided Policy Learning.The RL policy is optimized with a batch size of 1024, buffer size of 10240, and a linearly decaying learning rate initialized at 3 × 10 −4 .The policy network consists of two fully connected layers with 256 hidden units each, and layer normalization is applied to all input features.The clipping parameter is set to ϵ = 0.2, and other hyperparameters follow the standard PPO [51] style, including a GAE parameter of 0.95, time horizon of 128, and three optimization epochs per iteration, etc.The oracle-guided KL regularization coefficient α is linearly annealed from 0.95 to 0 every 10000 steps, with the scaling factor β fixed at 1.0.</p>
<p>To encourage goal reaching, safety, and efficiency, the reward function consists of three components.A terminal reward of +10 is given upon reaching the goal, while collisions incur a penalty of −1.To discourage overly long trajectories, each action step receives a penalty of − 1 Hmax , where H max = 5000 is the maximum episode length.Additionally, a reward of 0.1 × (d init − d t ) is applied at each timestep, where d init and d t represent the initial and current distances to the goal, respectively.Note that all baselines adopt the same DNN architecture as f θ to ensure fair visual representation capacity.For baselines involving overlapping training settings, such as the feature encoder used in CURL [25], and the RL optimization parameters in NPE [33], we apply identical hyperparameter settings to ensure fair comparison.</p>
<p>VI. RESULTS AND ANALYSIS</p>
<p>This section presents the experimental results and key analyses that validate the effectiveness of OMC-RL.We first evaluate its performance across a series of simulated environments and compare it against various baselines.We then assess its real-world generalizability through physical deployment on a quadrotor platform.Finally, we conduct ablation studies to examine the contribution of each core component.</p>
<p>A. Comparison Results in Simulation</p>
<p>All learning-based baselines are trained exclusively in the box-based environment.Their reward curves during training are illustrated in Fig. 4.After training, each baseline is evaluated in the remaining three environments without any finetuning.In each environment, we conduct 200 testing episodes to assess generalization.For the non-learning Hybrid APF, the policy is directly applied to all evaluation scenarios.Note that we refer to OMC-RL as Ours in all results.Oracle serves as an upper bound with the fastest convergence and the highest asymptotic performance, while our method achieves comparable results and consistently outperforms all other baselines.NPE benefits from imitation of suboptimal demonstrations, outperforming CURL in both sample efficiency and asymptotic performance.PPO, as a vanilla baseline, exhibits the weakest performance.</p>
<p>(1) Training Results.As shown in Fig. 4, our method achieves the fastest convergence and strongest asymptotic performance, consistently outperforming all baselines.This advantage can be attributed to two key factors: (i) the visual encoder is pretrained and frozen, which reduces training instability and computational cost; and (ii) the introduction of oracle guidance during early training is conducive to acquiring informative priors, substantially accelerating learning.Among the remaining baselines, NPE benefits from suboptimal demonstrations that enhance initial exploration, but its reliance on imperfect guidance ultimately limits overall performance.CURL converges more slowly and achieves weaker asymptotic performance compared to NPE.It also underperforms our method, which can be attributed to its inability to model temporal relationships across observations.PPO, as the vanilla baseline, performs significantly inferior than all baselines.</p>
<p>(2) Evaluation Results.Quantitative results across all metrics are summarized in Table I.The results show that our method maintains consistently strong generalization across all evaluation environments despite relying solely on sequences of monocular RGB frames.This generalization capability stems from two key design choices: (i) the pretrained and frozen visual encoder facilitates feature transfer by learning generalizable visual representations; and (ii) our method explicitly models temporal relationships across sequential observations, which are critical for visuomotor policy learning under partial observability.In contrast, although CURL also follows a pretrain-and-freeze paradigm, its focus on frame-wise representation learning without temporal modeling limits its generalization, particularly in the outdoor environment with high visual variability.NPE exhibits noticeable performance drops in evaluation scenarios.This is expected, as IL approaches inherently rely on prior demonstrations, making them prone to overfitting and less adaptable to unseen settings.While NPE partially alleviates this by transitioning from IL to exploration, its end-to-end training setup with RGB observations, where the encoder and policy are jointly optimized, still leads to overfitting within the training domain.PPO, as the vanilla RL baseline, performs the worst among all learning-based methods.The Hybrid APF baseline demonstrates competitive results in structured environments such as the furniture and tree scenes.However, its reliance on hand-crafted potential fields makes it susceptible to local minima, especially in irregular barrier layouts.In such settings, potential field approximations become unreliable, often resulting in unstable trajectories.These results demonstrate that masked contrastive learning and oracle guidance jointly enhance both navigation performance and generalization to unseen environments.</p>
<p>Representative flight trajectories are visualized in Fig. 5, further demonstrating the effectiveness of our approach.Our method consistently outperforms all baselines across evaluation scenarios, generating smooth and coherent flight trajectories.This demonstrates the advantage of incorporating oracle guidance, as leveraging complete environmental state information enables the policy to perform more effective and goal-directed maneuvers.</p>
<p>B. Real-World Experiments</p>
<p>To further evaluate the effectiveness and generalizability of OMC-RL, we conduct real-world experiments in two indoor environments.The real-world deployment setup is illustrated in Fig. 6.The first is a simple indoor scene containing multiple box-shaped obstacles with an identical appearance.The second is a more complex setting that introduces additional perceptual challenges, including potted plants and obstacles with diverse textures and colors, which increase scene diversity and pose greater sim-to-real domain shifts for visual perception.In addition, to simulate abrupt illumination changes often encountered in real deployments, we manually switch ambient lighting on and off during flight to test robustness against visual perturbations.The drone used for testing is a DJI Tello Edu, a lightweight quadrotor equipped with a forward-facing monocular RGB camera that streams 720p video at 30 FPS and provides a field of view (FOV) of 82.6 • .It receives localization updates at 120 Hz via the OptiTrack motion capture system.Start and goal positions are manually specified before each flight.Obstacles are perceived using the onboard RGB stream, which is resized and processed through the pretrained feature encoder to extract compact representations.These representations, along with other state information, are then fed into the downstream policy network to generate continuous control commands.The entire model is exported in ONNX format and deployed for real-time inference without extra fine-tuning.</p>
<p>We evaluate the model under 20 trials per environment, across varying start-goal configurations.The method achieves a success rate of 70% in the normal scene, 65% under color variations, and 55% under illumination disturbance.In   To further validate the effectiveness of OMC-RL in realworld scenarios, we conduct qualitative analyses on visual attention and feature consistency under various disturbances.As illustrated in Fig. 8, we visualize the attention heatmaps generated by OMC-RL and CURL using Grad-CAM [54] across three physical scenarios.Compared to CURL, OMC-RL consistently focuses on salient objects of interest, even in the presence of color variations and illumination changes, indicating improved robustness to perceptual noise and stronger task awareness.We further assess the representation stability of OMC-RL by analyzing the encoder's response to disturbed inputs.Specifically, we collect FOV images from successful trajectories and apply random changes in brightness and hue, as well as random masking over inputs.These modified observations are passed through the feature encoders of OMC-RL and CURL, and the resulting features are projected using t-SNE [55], as shown in Fig. 9.The features produced by OMC-RL remain densely clustered and well-aligned with those from the original inputs, indicating strong invariance and continuity in its learned representations.In contrast, features from CURL exhibit noticeable shift and dispersion under the same conditions, reflecting higher sensitivity to domain shifts and reduced generalization.</p>
<p>C. Ablation Studies</p>
<p>To better interpret the contributions of each component in OMC-RL, we conduct a series of ablation studies and analyses.Note that all ablation results are reported across evaluation environments, as our primary focus lies in assessing the model's generalizability beyond the training domain.Fig. 8: Attention visualization using Grad-CAM [54] in physical environments.Our method consistently focuses on taskrelevant regions across normal, color-shifted, and illuminationvaried scenes, whereas CURL shows misplaced attention, especially under visual disturbances.</p>
<p>Brightness</p>
<p>Hue Mask CURL Ours Fig. 9: t-SNE [55] visualization of learned features under input disturbances.Our method maintains compact and consistent feature clusters under brightness, hue, and masking conditions, while CURL exhibits scattered and misaligned representations.(1) Masking Probability.We investigate the impact of varying the masking probability ϱ m on downstream navigation performance.Specifically, we pretrain separate encoders under different ϱ m ∈ {0.1, 0.3, 0.5, 0.7, 0.9}, and pair each encoder with a newly initialized policy network trained in the box-based environment.The resulting policies are then tested in evaluation environments.The corresponding metrics are summarized in Fig. 10.As shown, setting ϱ m = 0.5 achieves the best overall performance.A high masking ratio (e.g., ϱ m = 0.9 or ϱ m = 0.7) leads to a significant performance drop, even underperforming CURL, as excessive masking removes most spatial and semantic cues from the input, impairing the model's ability to infer latent structure and recover useful features from corrupted inputs.Similarly, a low masking ratio (e.g., ϱ m = 0.1 or ϱ m = 0.3) also hampers generalization.According to Eq. ( 5), insufficient masked positions result in sparse training signals, making it difficult for the model to learn useful temporal correlations and reconstruct representations.These results confirm the utility of our masked contrastive learning framework.Properly tuned masking probability encourages the encoder to extract consistent and temporally predictive features, while both under-masking and over-masking degrade downstream navigation performance due to insufficient supervisory signals or excessive information loss.(2) Sequential Input In our method, T temporally consecutive frames are sampled from H and processed by the Transformer, where contrastive pairs are constructed through masking.In contrast, CURL forms contrastive pairs by augmenting individual frames independently sampled from the dataset.To investigate the effect of sequential input, we modify CURL to operate on consecutive frames rather than random ones, as CURL-cons.Empirically, we observe that training CURL-cons with high-frequency sequential leads severe instability and encoder collapse.Lowering the sampling frequency improves training stability but still in significantly degraded performance, as shown in Table II.These findings suggest that simply applying sequential input introduces visually redundant highly correlated that are difficult to differentiate using the effectiveof contrastive learning and generalization.The results further confirm that the performance gains of our method are not solely due to temporal continuity, but also depend on the use of masking during sequence modeling.</p>
<p>(3) Transformer Module.Our method adopts an asymmetric design in which only the query encoder is equipped with a Transformer module, while the key branch remains purely convolutional.Although this avoids additional computational overhead, we further investigate whether this asymmetry is indeed beneficial through a targeted ablation.We introduce an additional Transformer module ξ k for the key encoder, denoted as our-dualT.The parameters are updated using momentum contrastive learning following the paradigm in Section IV-B.As shown in Fig. 11, this design leads to a drop in performance.We attribute this to a mismatch between the contrastive learning objective and downstream usage: during policy learning, only the CNN encoder output is used to generate actions without the Transformer module.Introducing temporal modeling on both branches of masked contrastive  12: Ablation study on the non-linear projection module φ.We visualize downstream navigation performance using bar plots.Results demonstrate that removing the projection head leads to consistent degradation compared to the full model.While adding φ to CURL improves its performance, both CURL variants remain inferior to our approach.These results highlight the importance of φ in learning representations.learning encourages the model to rely heavily on the Transformer layers, thereby weakening the CNN encoder's ability to encode temporal dependencies independently.This limits the utility of the learned representations once the Transformer is removed, and ultimately impairs policy generalization.</p>
<p>(4) Non-Projection Module.Our method includes a nonlinear projection module φ prior to the contrastive loss.To assess its contribution to visuomotor policy learning, we conduct an ablation in which φ is removed while keeping all other components unchanged, denoted as ours w/o φ.The evaluation results, shown in Fig. 12, indicate that removing φ leads to degraded policy performance.This probably stems from the reduced representational capacity of the feature encoder, as direct optimization without non-linear transformation can restrict the encoder from producing discriminative embeddings better for policy learning.While ours w/o φ underperforms the full model, it still exceeds our CURL baseline.Note that the vanilla CURL does not include the non-linear projection module.Here, a CURL variant incorporating φ is used and denoted as CURL w/ φ, as explained in Section V-C.To further examine the effect of φ, we compare both CURL variants and observe that adding the projection head consistently improves performance.All aforementioned findings are aligned with the conclusion reported in [22].However, both CURL variants   inferior to proposed method.These results utility of the non-linear projection φ and highlight the masking and temporal correlation in learning visuomotor representations.</p>
<p>(5) Oracle Teacher.In our method, an oracle teacher policy with access to full global state information is introduced to guide the early stage of downstream policy training.To evaluate the effectiveness of this guidance, we implement an ablation variant in which the oracle guidance is removed, denoted as our w/o OT.Furthermore, we replace the nonexpert policy in the NPE baseline with the oracle teacher, denoted as NPE w/ OT.As summarized in Fig. 13, our w/o OT results in only a slight performance drop in the easy furniture environment.However, its performance degrades significantly in the more challenging barrier and tree settings.This demonstrates the importance of the oracle teacher, which leverages privileged information to generate ideal action commands.In mediated environments, the agent can still compensate through environmental exploration.However, as the scene becomes more complex and the visual appearance becomes more diverse and perceptually demanding, the absence of oracle guidance leads to more noticeable performance degradation.Results also show that NPE w/ OT outperforms the original NPE but remains inferior to both our w/o OT and our full model.These findings further confirm the benefit of oracle guidance and emphasize the significance of learning generalized representations for visuomotor policy learning.</p>
<p>(6) Decay Schedule.In our method, a decaying coefficient α is introduced in Eq. ( 9) to balance the RL objective and the KL divergence term.This coefficient is linearly annealed throughout training, allowing the policy to gradually transi-tion from imitating the oracle to exploring the environment independently.To assess the impact of the decay schedule, we compare three variants: a fixed coefficient setting (denoted as our-fixed), an exponential decay setting (our-exp), and our proposed linear decay schedule.For our-exp, α is decayed by a factor of 0.95 every 1000 steps.As shown in Table III, the linear decay consistently outperforms the other two variants.Our-fixed prevents the agent from sufficiently reducing its reliance on the oracle, thereby limiting late-stage exploration.Our-exp reduces α too rapidly in the early stage, resulting in insufficient imitation and limiting the agent's ability to acquire meaningful guidance.In the later stage, the decay slows down excessively, hindering the agent's shift toward independent exploration.The linear schedule, by contrast, enables a smooth and progressive shift from imitation to exploration, facilitating better policy adaptation across environments.</p>
<p>VII. CONCLUSION</p>
<p>In this work, we propose OMC-RL, a novel framework that combines masked contrastive learning with oracle-guided RL to improve the sample efficiency and asymptotic performance of visuomotor policies for drones.OMC-RL decouples upstream representation learning from downstream policy learning.For upstream, a Transformer-integrated module combining sequence modeling and masked contrastive learning is employed to extract temporally-aware and taskrelevant representations from sequential visual inputs.After training, the Transformer component is discarded, while the visual encoder is frozen and used for downstream policy learning.During this stage, an oracle teacher policy, which has access to full global state information, supervises the agent in the early stage and gradually reduces its impact as the agent transitions to independent exploration.Extensive experiments in both simulated and real-world environments demonstrate that OMC-RL effectively improves sample efficiency and asymptotic performance.Moreover, it enhances policy generalization under various and challenging perceptual conditions.In future work, we plan to extend OMC-RL to other robotic vision tasks.Owing to its decoupled training of the feature encoder, the framework can capture task-agnostic visual features that support the learning of transferable policies across diverse settings.Another promising direction is to extend OMC-RL to multi-modal or instruction-guided policy learning scenarios, where grounding visual observations to semantic goals remains a major challenge.</p>
<p>Fig. 1 :
1
Fig. 1: Illustration of different learning paradigms for goalconditioned representation and policy learning.(a) In vanilla teacher-student policy learning, the agent learns from an oracle teacher via policy distillation to better identify and reach the target through supervised imitation.(b) In vanilla contrastive learning, given an observation set H, positive pairs (red arrows) formed between aggregated features are aligned, while all other negatives (blue arrows) are pushed apart.(c) Oracleguided masked contrastive learning (ours) combines masked contrastive representation learning with oracle-supervised policy learning to jointly improve both feature encoding and downstream decision-making.</p>
<p>Fig. 2 :
2
Fig.2: The framework of OMC-RL.(a) Upstream Masked Contrastive Representation Learning: A masked contrastive learning module is used to learn compact and task-relevant visual representations from sequential RGB inputs.The masked and original inputs are processed through CNN encoders and projection layers, while the masked branch is further processed by an auxiliary transformer module to compute the contrastive loss L cl .After training, the CNN encoder is frozen and the transformer module is discarded.(b) Downstream Oracle-Guided Policy Learning: An oracle teacher policy is first trained using privileged depth inputs and full-state information, providing expert action distributions for downstream supervision.This oracle network supervises the student policy through a learning-by-cheating strategy.Specifically, the agent policy is optimized via KL-divergence against the oracle policy distribution to enable efficient visuomotor policy learning.</p>
<p>Algorithm 1 : 4 5 Construct
145
Training Pipeline of OMC-RL Input: RGB sequence dataset H, rollout trajectory T , and oracle teacher π o ψ Output: Optimized visuomotor policy π ψ 1 // Upstream Training 2 foreach iteration do 3 Sample sequence H = (h 1 , . . ., h T ) Sample mask M ∼ Bernoulli(ϱ m ) H m = ( hm 1 , . . ., hm T ) 6</p>
<p>Fig. 3 :
3
Fig. 3: Simulation environments with increasing complexity used to evaluate OMC-RL.</p>
<p>Fig. 4 :
4
Fig. 4: Training curves of episodic reward for all learningbased baselines.All results are averaged over three random seeds, with shaded regions indicating confidence intervals.Oracle serves as an upper bound with the fastest convergence and the highest asymptotic performance, while our method achieves comparable results and consistently outperforms all other baselines.NPE benefits from imitation of suboptimal demonstrations, outperforming CURL in both sample efficiency and asymptotic performance.PPO, as a vanilla baseline, exhibits the weakest performance.</p>
<p>Fig. 5 :<em>Fig. 6 :
56
Fig. 5: Qualitative trajectory comparisons of baselines in three evaluation environments.The results demonstrate that the oracle and OMC-RL consistently generate smooth and efficient trajectories.In contrast, other baselines often produce suboptimal or erratic trajectories and tend to fail in environments with irregular layouts and complex textures.TABLE I: Performance of Baselines in Simulation Environments Method Furniture Scenario (Easy) Barrier Scenario (Medium) Tree Scenario (Hard) NE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓ NE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓ NE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓ Ours 1.49 92.5 91.5 0.88 2.5 502 1.62 86.0 84.5 0.80 5.5 525 2.01 80.5 78.5 0.73 8.0 516 NPE [33] 6.08 21.5 17.5 0.12 80.5 564 7.59 9.5 0.0 0.00 94.5 -7.83 9.0 0.0 0.00 97.0 -CURL [25] 2.07 75.0 72.5 0.68 22.0 589 5.07 34.0 30.0 0.26 62.0 603 7.36 12.0 0.0 0.00 79.5 -PPO [51] 6.90 17.5 12.0 0.08 87.0 596 7.36 12.0 0.0 0.00 90.0 -7.78 9.5 0.0 0.00 98.0 -Hybrid APF [53] 2.56 73.5 72.0 0.69 7.0 741 5.23 31.0 28.5 0.25 17.5 789 3.04 71.5 69.5 0.66 10.5 753 </em>Bold values indicate the best results."-" indicates zero success.</p>
<p>Visual comparison of flight trajectories in three physical scenarios.CURL Velocity [m/s] Ours Velocity [m/s] (b) Velocity-annotated trajectories highlighting movement patterns of OMC-RL and CURL.</p>
<p>Fig. 7 :
7
Fig.7: Real-world trajectory evaluation under visual domain shifts.Our method consistently produces smooth and successful trajectories across all scenarios, demonstrating strong robustness and generalization.In contrast, CURL frequently fails when facing visual disturbances such as obstacle color variation, texture inconsistency, and sudden illumination shifts.</p>
<p>Fig. 10 :
10
Fig. 10: Ablation study on masking probability ϱ m .We evaluate downstream navigation performance with encoders pretrained under different masking probabilities.Navigation metrics are visualized as line plots across three simulation environments.A masking ratio (ϱ m = 0.5) achieves the best overall performance.In contrast, overly low or high values lead to performance degradation due to insufficient use of the Transformer's reconstruction capability or excessive information loss.</p>
<p>Fig. 11 :
11
Fig. 11: Ablation study on the Transformer module design.We compare our setup (Transformer on query encoder only) with a dual-Transformer variant where both query and key encoders use Transformers.Radar plots visualize normalized navigation metrics, where higher values indicate better performance.The dual-Transformer design degrades performance due to overreliance on temporal modeling, weakening the CNN encoder's standalone capability during downstream policy learning.</p>
<p>Fig.12: Ablation study on the non-linear projection module φ.We visualize downstream navigation performance using bar plots.Results demonstrate that removing the projection head leads to consistent degradation compared to the full model.While adding φ to CURL improves its performance, both CURL variants remain inferior to our approach.These results highlight the importance of φ in learning representations.</p>
<p>Fig. 13 :
13
Fig.13: Ablation Study on Oracle Teacher (OT).The heatmap visualizes the normalized performance of all metrics, where higher values indicate better performance.Removing the oracle teacher leads to moderate drops in the furniture scene, but causes significant degradation in more complex settings.This highlights the oracle's contribution in guiding policy training.Adding OT to the NPE baseline yields improvements over NPE, but still falls short of our method.These results highlight the benefit of oracle guidance for learning effective policies.</p>
<p>L+1 , I d t−L+2 , ..., I d t } denotes a sequence of depth images with I d t ∈ R H×W , v t ∈ R 3 and ω t ∈ R 3 are drone's linear and angular velocities, respectively, θ t ∈ R 3 represents the drone's orientation, and ∆p t ∈ R 6 represents the relative positions between the drone and surrounding objects, including obstacles and the goal.The oracle teacher π o ψ maps this global state s t to a control command u t as:π o ψ : s t → u t ∈ R 3, where u t = (v x , v y , ω z ) includes the linear velocities in the x and y directions and the angular velocity around the z-axis.The network architecture of π o ψ consists of a visual encoder f o θ followed by a fully-connected layer φ o .The encoderf o θ : I d t → R d maps the sequential depth images I d t into a d-dimensional feature representation z d t = f o θ (I d t ).This embedding is then concatenated with the remaining state information and passed into the fully connected module φ o to produce the control output: u o</p>
<p>t = φ o z d t ; v t ; ω t ; ∆p t .The oracle teacher π o</p>
<p>TABLE II
II: Ablation on Sequential Input for ContrastiveRepresentation LearningEnvironmentMethodNE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓Furniture (Easy)CURL CURL-cons 5.2.07 75.0 72.5 0.68 22.0 589</p>
<p>TABLE III :
III
Ablation on Decay Strategies for Downstream Policy Learning
MethodFurniture Scenario (Easy)Barrier Scenario (Medium)Tree Scenario (Hard)NE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓ NE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓ NE↓ OS↑ SR↑ SPL↑ CR↓ TTS↓Ours1.49 91.50.882.55021.62 86.0 84.50.805.55252.01 80.5 78.50.738.0516Ours-fixed 84.50.755171.84 79.0 76.00.699.55382.69 72.5 71.00.63528Ours-exp1.65 84.50.775.55121.90 81.0 78.50.715332.56 75.5 72.50.6513.0</p>
<p>Human-guided reinforcement learning with sim-to-real transfer for autonomous navigation. J Wu, Y Zhou, H Yang, Z Huang, C Lv, IEEE on Analysis and Machine Intelligence. 45122023</p>
<p>Fear-neuro-inspired reinforcement learning for safe autonomous driving. X He, J Wu, Z Huang, Z Hu, J Wang, A Sangiovanni-Vincentelli, C Lv, IEEE Transactions on Pattern Analysis and Machine Intelligence. 202346</p>
<p>Ood-control: generalizing control in unseen environments. N Ye, Z Zeng, J Zhou, L Zhu, Y Duan, Y Wu, J Wu, H Zeng, Q Gu, X Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 46112024</p>
<p>Vision-based learning for drones: A survey. J Xiao, R Zhang, Y Zhang, M Feroskhan, IEEE Transactions on Neural Networks and Learning Systems. 2025</p>
<p>Gsplanner: A gaussian-splatting-based planning framework for active highfidelity reconstruction. R Jin, Y Gao, Y Wang, Y Wu, H Lu, C Xu, F Gao, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE202411</p>
<p>Airslam: An efficient and illumination-robust point-line visual slam system. K Xu, Y Hao, S Yuan, C Wang, L Xie, IEEE Transactions on Robotics. 2025</p>
<p>Tc-sfm: Robust track-community-based structure-from-motion. L Wang, L Ge, S Luo, Z Yan, Z Cui, J Feng, IEEE Transactions on Image Processing. 332024</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>Dronet: Learning to fly by driving. A Loquercio, A I Maqueda, C R Del-Blanco, D Scaramuzza, IEEE Robotics and Automation Letters. 322018</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, Science. 36264192018</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 57577822019</p>
<p>J Li, S Koyamada, Q Ye, G Liu, C Wang, R Yang, L Zhao, T Qin, T.-Y Liu, H.-W Hon, arXiv:2003.13590Suphx: Mastering mahjong with deep reinforcement learning. 2020arXiv preprint</p>
<p>Champion-level drone racing using deep reinforcement learning. E Kaufmann, L Bauersfeld, A Loquercio, M Müller, V Koltun, D Scaramuzza, Nature. 62079762023</p>
<p>Learning multi-pursuit evasion for safe targeted navigation of drones. J Xiao, M Feroskhan, IEEE Transactions on Artificial Intelligence. 2024</p>
<p>Selective imitation enhanced deep reinforcement learning for aav navigation and obstacle avoidance with sparse rewards. C Yan, Y Sun, Y Jiang, X Xiang, M Chen, IEEE Transactions on Intelligent Transportation Systems. 2025</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. D Yarats, A Zhang, I Kostrikov, B Amos, J Pineau, R Fergus, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)202135681</p>
<p>Reinforcement learning with prototypical representations. D Yarats, R Fergus, A Lazaric, L Pinto, International Conference on Machine Learning (ICML). PMLR202111931</p>
<p>Mono-cameraonly target chasing for a drone in a dense environment by cross-modal learning. S Yoo, S Jung, Y Lee, D Shim, H J Kim, IEEE Robotics and Automation Letters. 2024</p>
<p>Solar: Deep structured representations for model-based reinforcement learning. M Zhang, S Vikram, L Smith, P Abbeel, M Johnson, S Levine, International Conference on Machine Learning (ICML). </p>
<p>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. A X Lee, A Nagabandi, P Abbeel, S Levine, Advances in Neural Information Processing Systems (NeurIPS). 202033</p>
<p>Learning visual representation for autonomous drone navigation via a contrastive world model. J Zhao, Y Wang, Z Cai, N Liu, K Wu, Y Wang, IEEE Transactions on Artificial Intelligence. 532023</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International Conference on Machine Learning (ICML). PMLR2020</p>
<p>Improving contrastive learning on imbalanced data via open-world sampling. Z Jiang, T Chen, T Chen, Z Wang, Advances in Neural Information Processing Systems (NeurIPS). 202134</p>
<p>Semisupervised contrastive learning with similarity co-calibration. Y Zhang, X Zhang, J Li, R C Qiu, H Xu, Q Tian, IEEE Transactions on Multimedia. 252022</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, P Abbeel, International Conference on Machine Learning (ICML). PMLR2020</p>
<p>Learning cross-modal visuomotor policies for autonomous drone navigation. Y Zhang, J Xiao, M Feroskhan, IEEE Robotics and Automation Letters. 2025</p>
<p>Contrastive learning for enhancing robust scene transfer in vision-based agile flight. J Xing, L Bauersfeld, Y Song, C Xing, D Scaramuzza, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Learning deep sensorimotor policies for vision-based autonomous drone racing. J Fu, Y Song, Y Wu, F Yu, D Scaramuzza, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). 201730</p>
<p>Learning to fly by crashing. D Gandhi, L Pinto, A Gupta, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2017</p>
<p>Learning to fly by myself: A selfsupervised cnn-based approach for autonomous navigation. A Kouris, C.-S Bouganis, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018</p>
<p>Learning vision-based agile flight via differentiable physics. Y Zhang, Y Hu, Y Song, D Zou, W Lin, Nature Machine Intelligence. 2025</p>
<p>Npe-drl: Enhancing perception constrained obstacle avoidance with non-expert policy guided reinforcement learning. Y Zhang, C Yan, J Xiao, M Feroskhan, IEEE Transactions on Artificial Intelligence. 2024</p>
<p>Collaborative target search with a visual drone swarm: An adaptive curriculum embedded multistage reinforcement learning approach. J Xiao, P Pisutsin, M Feroskhan, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Collision-avoiding flocking with multiple fixed-wing uavs in obstacle-cluttered environments: A task-specific curriculum-based madrl approach. C Yan, C Wang, X Xiang, K H Low, X Wang, X Xu, L Shen, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Learning visuomotor policies for aerial navigation using cross-modal representations. R Bonatti, R Madaan, V Vineet, S Scherer, A Kapoor, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2020</p>
<p>Increasing the efficiency of policy learning for autonomous vehicles by multi-task representation learning. E Kargar, V Kyrki, IEEE Transactions on Intelligent Vehicles. 732022</p>
<p>Depth-cuprl: Depth-imaged contrastive unsupervised prioritized representations in reinforcement learning for mapless navigation of unmanned aerial vehicles. J C De Jesus, V A Kich, A H Kolling, R B Grando, R S Guerra, P L Drews, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE202210</p>
<p>Bevnav: Robot autonomous navigation via spatial-temporal contrastive learning in bird'seye view. J Jiang, Y Yang, Y Deng, C Ma, J Zhang, IEEE Robotics and Automation Letters. 2024</p>
<p>Sample efficient reinforcement learning with reinforce. J Zhang, J Kim, B O'donoghue, S Boyd, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)202135895</p>
<p>Image is you need: Regularizing deep reinforcement learning from pixels. D Yarats, I Kostrikov, R Fergus, International Conference on Learning Representations (ICLR). 2021</p>
<p>Reinforcement learning with augmented data. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, A Srinivas, Advances in Neural Information Processing Systems (NeurIPS). 202033</p>
<p>Deep q-learning from demonstrations. T Hester, M Vecerik, O Pietquin, M Lanctot, T Schaul, B Piot, D Horgan, J Quan, A Sendonaris, I Osband, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)201832</p>
<p>Safety-aware human-inthe-loop reinforcement learning with shared control for autonomous driving. W Huang, H Liu, Z Huang, C Lv, IEEE Transactions on Intelligent Transportation Systems. 2024</p>
<p>Decoupling representation learning from reinforcement learning. A Stooke, K Lee, P Abbeel, M Laskin, International Conference on Machine Learning (ICML). PMLR2021</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterHuman Language Technologies (NAACL-HLT2019</p>
<p>Pre-training with whole word masking for chinese bert. Y Cui, W Che, T Liu, B Qin, Z Yang, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 292021</p>
<p>Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 51875402015</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International Conference on Machine Learning (ICML). Pmlr2018</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017arXiv preprint</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N Sünderhauf, I Reid, S Gould, A Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>An improved artificial potential field method for path planning and formation control of the multi-uav systems. Z Pan, C Zhang, Y Xia, H Xiong, X Shao, IEEE Transactions on Circuits and Systems II: Express Briefs. 6932021</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, Proceedings of the IEEE International Conference on Computer Vision (ICCV. the IEEE International Conference on Computer Vision (ICCV2017</p>
<p>Currently, he is pursuing his Ph.D. degree at the School of Mechanical &amp; Aerospace Engineering at NTU, Singapore. His research primarily focuses on unmanned aerial vehicles, deep reinforcement learning, and vision-and-language navigation. L Van Der Maaten, G Hinton, Chao Yan (member, IEEE) received the B.E. degree in electrical engineering and automation from China University of Mining and Technology. Harbin, China; Beijing, China; Singapore; Xuzhou, China; Changsha, China; Singapore; Nanjing, China; Singapore; Melbourne, FL2008. 2023. 2024. 2021 to 2022. 2011. 20169from Harbin Engineering University ; Mechanical &amp; Aerospace Engineering at Nanyang Technological University (NTU) ; and control from Beihang University ; Nanyang Technological University ; Nanjing University of Aeronautics and Astronautics ; Mir Feroskhan (member, IEEE) received B.E. degree (Hons.) in aerospace engineering from Nanyang Technological University ; Florida Institute of TechnologyHis research interests include deep reinforcement learning and coordination control of UAV swarms. He is currently an assistant professor with the School of Mechanical &amp; Aerospace Engineering at NTU. His research interests include nonlinear control systems, multi-agent systems, flight dynamics and control, and aerial robotics</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2172 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2172</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2172</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-276482776</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.14499v1.pdf" target="_blank">MLGym: A New Framework and Benchmark for Advancing AI Research Agents</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2172.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2172.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLGym agent (SWE-Agent harness)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLGym Agentic Harness (SWE-Agent based LLM research agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven agentic harness used in MLGym that executes shell commands, uses tool-augmented interfaces (ACI), iteratively runs experiments, inspects results, and submits trained artifacts; it is the experimental agent used to study automated ML research workflows in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLGym Agent (SWE-Agent harness with LLM backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>tool-augmented large language model agent (LLM agent + ACI)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>machine learning research / AI research automation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates code, experimental configurations, hyperparameter choices, model training commands, strategies/heuristics for algorithmic tasks, and written research artifacts (iterative proposals and code edits)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Automated evaluation scripts per task (read-only) invoked by 'validate' and 'submit' commands; validate runs the evaluation script to return test-set metrics without terminating the run, submit runs final evaluation and terminates; agents can run training scripts, read validation outputs, and iterate based on these automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Capability-level taxonomy defined in paper (Level 0–5, where novelty corresponds to Level 3+), comparison to baseline and SOTA on per-task metrics, and whether result is 'publication-worthy' or establishes new SOTA; within the benchmark novelty is operationalized by beating baselines or SOTA where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>On MLGym-Bench the agent (using frontier LLM backbones) typically achieves baseline improvements (Level 1) mainly via hyperparameter and training changes rather than inventing novel algorithms; aggregate performance measured via per-task metrics and AUP (area under performance-profile) shows variation by backbone (OpenAI O1-preview highest AUP, Gemini-1.5-Pro cost-effective). Performance is substantially better on familiar/supervised tasks (e.g., CIFAR-10, Fashion-MNIST, House Prices) and much worse on novel/harder tasks requiring algorithmic innovation (Language Modeling, RL tasks like Breakout, MetaMaze).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is fully automated and reliable for checking metric values on held-out test sets (the environment runs evaluation scripts and returns numeric scores); however agents frequently trigger 'Evaluation Error' (missing or malformed submission artifact) — Evaluation Error accounted for ~75% of termination error types in runs, indicating frequent failures in producing valid, evaluable outputs rather than failures of the evaluation script itself.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>No numeric false positive rate reported; qualitative evidence: many termination errors are 'Evaluation Error' (invalid submissions) implying agents often produce outputs that cannot be accepted as valid; methods that do not outperform baseline are marked infeasible and scored conservatively (infeasible penalty = (1+ε)×baseline ratio, ε=0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>No numeric false negative rate reported; not measured explicitly in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation reliability is high for familiar tasks with clear test sets and read-only evaluation scripts; for novel or open-ended tasks (e.g., algorithmic innovation, RL, large language-model training) agents more often fail to produce valid submissions and show lower validated performance — i.e., novelty correlates with lower validated performance and more invalid submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Evidence of asymmetry: agents can generate incremental, incrementalizable outputs (hyperparameter tuning, engineering changes) that validate well, but they rarely generate truly novel hypotheses/algorithms (Level 3+) and thus do not produce validated breakthroughs. The paper reports agents usually find better hyperparameters but 'do not generate novel hypotheses, algorithms, architectures, or substantial improvements.'</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Agents perform worse on tasks that demand out-of-distribution generalization or algorithmic creativity (Language Modeling and RL tasks had highest failure rates). The paper reports models failing entirely on some tasks (e.g., Llama-3.1-405b-instruct and GPT-4o failing to produce any valid solution for certain tasks), indicating degraded OOD performance relative to in-distribution supervised problems.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No explicit calibration metrics (confidence vs. accuracy) reported for agent predictions/decisions; calibration quality of model confidence is not analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation requires running the same task-specific evaluation scripts and can be compute-intensive (training timeouts per task exist); cost vs. performance analysis shows OpenAI O1-Preview is the most computationally expensive backbone while Gemini-1.5-Pro is roughly ~9× cheaper than O1 and achieves ~99% of O1's AUP, so validation (and generation) cost vary strongly with backbone choice.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Memory Module (store/retrieve best configs), validate/submit iterative feedback loop, tool-augmented ACI, read-only evaluation scripts to prevent cheating, structured agent scaffolding (SWE-Agent), cost-aware experimental budgets; paper also references search/MCTS and tree-search augmentations (from related works) as promising gap-closing approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MLGym agentic harness (SWE-Agent) can autonomously run experiments and improve baselines primarily via optimization (hyperparameters), with reliable automated validation via read-only evaluation scripts; however agents fail to produce novel, publishable scientific contributions, show worse validated performance on novel/harder tasks (RL, language-model training), and frequently produce invalid submissions that trigger evaluation errors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2172.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLGym Memory Module (research logs storage and retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured memory component that lets an agent persist experimental results, good configurations and findings (with embeddings and tags) and retrieve top-k relevant past entries by cosine similarity to inform later experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLGym Memory Module</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval / external memory augmentation for LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>machine learning research workflows / long-horizon experiment management</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate scientific claims itself; it stores and retrieves agent-generated artifacts and textual summaries that influence subsequent generation (e.g., reusing best training configurations, reproducing experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Improves validation indirectly by enabling the agent to recall prior validated configurations and re-run/iterate from them; memory entries include tags and embeddings and retrieved items are used before making new submissions or continuing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Memory tags (3-gram matching) and cosine similarity retrieval; novelty not directly measured by the module itself — it supports long-horizon novelty by preventing forgetting of earlier successful experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Empirically shown to improve long-horizon task performance: agents with memory could retrieve best training configs and achieve superior results compared to agents without memory (figures and qualitative description in paper), enabling sustained progress over longer trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Memory improves the agent's ability to reproduce previously validated configurations and thereby improves the chance of producing valid submissions; exact numeric validation improvement not provided but qualitative examples and figures (Figure 11/12) show positive impact.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Memory primarily aids repeated or long-horizon tasks (where validation outcomes had been previously observed); it reduces catastrophic forgetting so validated performance on later steps improves when prior successful configurations are relevant; for truly novel tasks the memory has limited direct effect.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Memory reduces asymmetry by preserving and enabling reuse of validated results, narrowing the gap between exploration (generation) and reliable submission (validation) across long runs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Memory helps for long-horizon OOD style search by letting agent re-use past successful strategies, but no explicit metrics for OOD performance change provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Memory operations (embedding store and cosine similarity retrieval) are lightweight relative to training/validation runs; cost reported qualitatively as small compared to training timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Persistent storage of validated configurations, retrieval by similarity, inclusion of memory state in system prompt (tags + number of records) to make agent aware of stored knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A small structured memory that stores validated configurations and results materially helps agents on long-horizon experimental tasks by enabling them to retrieve and reuse the best-known configurations, thereby improving validated outcomes and reducing forgetting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2172.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Validate/Submit pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLGym Validate and Submit Evaluation Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The built-in MLGym mechanism by which agents can run task-specific read-only evaluation scripts: 'validate' to get current test-set metrics without terminating the run, and 'submit' to run final evaluation and terminate; these enable iterative experiment-feedback loops and standardized grading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLGym validate/submit evaluation commands + read-only task evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automated evaluation pipeline / grading interface</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>machine learning experiments and automated scientific evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate outputs, but assesses generated outputs (models, code, strategies) by executing predefined evaluation scripts and returning metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-authored read-only evaluation scripts executed in the environment; 'validate' runs this script multiple times during an agent's trajectory to provide feedback, 'submit' runs final evaluation and logs metrics; evaluation scripts vary by task (accuracy, R2, BLEU, reward, wall-clock time for 3-SAT, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty is indirectly measured by outperforming baselines or achieving SOTA metrics defined in the task; evaluation scripts include safeguards (read-only) to prevent agents from changing scoring logic.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Enables agents to iteratively improve generation by giving immediate metric feedback; paper reports that use of validate increases agents' ability to explore (Best Attempt vs Best Submission distinction) but many invalid final submissions still occur.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Highly reliable at computing numerical metrics when submission artifacts are properly formatted; large fraction of termination errors are 'Evaluation Error' indicating many generated artifacts are malformed rather than being wrong per metric.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not quantified; read-only evaluation minimizes false positives caused by agents manipulating evaluation logic, but invalid submission formats lead to many evaluation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>When agents tackle novel tasks, they more frequently produce artifacts that fail evaluation (malformed outputs) and thus the validate/submit pipeline exposes these failures; for familiar tasks validation is straightforward and reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>The pipeline exposes a practical asymmetry: generation can produce candidate results but often fails to produce properly formed evaluable artifacts that validation requires; thus validation often fails due to format/artefact issues even when the underlying approach could be valid.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Evaluation scripts are task-specific and may not capture cross-domain novelty; OOD artifacts may not conform to required submission format, increasing evaluation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation cost equals cost of executing evaluation scripts (can be expensive for training-heavy tasks); overall pipeline enforces task-specific timeouts to limit runaway compute.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Read-only evaluation scripts, iterative 'validate' feedback, and task timeouts; scoring rules for infeasible methods (penalty multiplier) to avoid rewarding invalid / non-submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A robust automated validate/submit pipeline enables iterative improvement and objective scoring, but agents frequently fail to produce correctly formatted submissions or valid artifacts for novel/harder tasks, causing many evaluation errors rather than metric-level failures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2172.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Performance profiles / AUP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance Profiles and AUP (Area Under Performance Profile) scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation methodology adapted from optimization and AutoML literature used by MLGym to compare agents across heterogeneous tasks and metrics by computing performance ratios and integrating over thresholds (AUP).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Performance profile curves and AUP aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation / benchmarking methodology</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>benchmarking of AI research agents across diverse ML tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A (evaluation metric rather than generator)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Aggregates per-task numeric metrics into performance profiles by comparing each method's metric to best method on that task, then integrates the profile curve to compute AUP; supports both 'Best Attempt' (validate calls) and 'Best Submission' (final submits) aggregations to separate exploration ceiling from submission reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Does not directly measure novelty; measures relative performance across tasks and can show whether methods approach best-known solutions (SOTA) on tasks — indirectly signals novelty if a method outperforms baselines and existing methods across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used to report aggregate generation/solution quality across tasks: OpenAI O1-Preview had highest AUP; Gemini-1.5-Pro was cost-effective and achieved ~99% of O1's AUP; detailed per-task metrics show agents often improve baselines but rarely produce novel algorithmic breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>AUP summarizes validated performance; paper reports separate AUPs for Best Attempt and Best Submission where Best Attempt captures exploratory ceiling and Best Submission captures agent's ability to submit best result reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not directly applicable; AUP handles infeasible methods by applying a conservative penalty (infeasible score set to (1+ε)×baseline ratio with ε=0.05) to avoid overly rewarding invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>AUP distinctions (Best Attempt vs Best Submission) reveal that agents may reach high-performance attempts (exploration ceiling) but fail to reliably submit them; this gap is larger on more novel or complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>AUP captures asymmetry by showing higher Best Attempt AUP than Best Submission AUP for some models, indicating agents can reach good solutions but fail to reliably package/submit them.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performance profiles reveal poor relative performance on tasks requiring algorithmic creativity; the profiles quantitatively show methods being further from the best on novel/harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Computation to produce AUP is negligible compared to training/validation runs; however, obtaining the underlying validated metrics can be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Separate Best Attempt vs Best Submission evaluation highlights deployment/packaging reliability issues; conservative scoring of infeasible methods; inversion/normalization for metrics with different directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Performance-profiles and AUP provide a principled, domain-agnostic aggregation of validated agent performance and reveal a consistent pattern: agents can explore to promising attempts but are less reliable at finalizing and submitting those attempts, especially on novel tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2172.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Frontier LLMs (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frontier Large Language Models evaluated in MLGym (OpenAI O1-preview, Gemini-1.5-Pro, Claude-3.5-Sonnet, Llama-3.1-405B-Instruct, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of state-of-the-art LLM backbones used as the agent's base models in experiments, compared under a common SWE-Agent scaffolding across MLGym-Bench tasks for generation and validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI O1-preview; Gemini-1.5-Pro; Claude-3.5-sonnet; Llama-3.1-405b-instruct; GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general AI agent backbones for ML research automation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generate code edits, experiment commands, model configurations, strategy code (game theory tasks), model training scripts, and literature summaries when tool-augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same automated MLGym evaluation pipeline (validate/submit and task-specific read-only evaluation scripts) used to test outputs produced by these LLM-driven agents; performance aggregated via AUP and per-task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by ability to exceed baseline metrics and approach/achieve SOTA across tasks; the paper also uses capability levels to classify novelty (agents reaching Level 2+ would imply SOTA or novel contributions).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Aggregate results: OpenAI O1-preview achieved highest overall AUP (best aggregate), Gemini-1.5-Pro and Claude-3.5-Sonnet close behind; Gemini offered best cost-performance tradeoff (~9× cheaper than O1 while reaching ~99% of its AUP). Llama-3.1 and GPT-4o failed to produce valid solutions for some challenging tasks (e.g., language modeling, breakout). Generally strong on supervised tasks, weak on novel algorithmic/RL tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation pipeline successfully measured model outputs; many runs experienced Evaluation Errors (invalid submissions) rather than metric-level failures; evaluation error was the dominant termination cause (~75%). Completion/failure differed by model: GPT-4o highest failure rate; Gemini and O1-preview best completion rates though O1-preview had high incompletion rate but produced at least one valid solution per task.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically per model; qualitative: models occasionally produce malformed or invalid submission artifacts leading to evaluation errors rather than incorrectly validated but false-positive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>All LLM backbones exhibit degraded validated performance on tasks requiring novel algorithmic invention or large-scale training (language modeling, RL), while performing relatively well on familiar supervised tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Observed: some models (e.g., O1-preview) can explore and reach high-performing attempts but do not always finalize and submit the best attempt reliably (Best Attempt AUP > Best Submission AUP), indicating a practical generation-vs-submission gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Out-of-distribution or novel tasks (algorithmic innovation, large LM training, complex RL) show worse performance and higher failure rates; Llama/GPT-4o failed entirely on some OOD-hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>No calibration data reported for model confidences.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>OpenAI O1-preview highest cost; Gemini-1.5-Pro and Claude-3.5-Sonnet more cost-effective; cost-vs-AUP pareto frontier provided in paper (Figure 3) showing tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Agent scaffolding (SWE-Agent), memory module, validate/submit iterative feedback, tool documentation in system prompt, cost/time budgets, and literature-search tools; related works suggest search (MCTS, tree search) and execution feedback as additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Frontier LLM backbones, when run within a common agent harness, improve baselines primarily via engineering and hyperparameter changes; they differ substantially in cost-effectiveness and reliability, and all show substantially lower validated performance on novel algorithmic or RL tasks compared to familiar supervised problems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2172.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ScienceAgentBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related benchmark (cited) that contains data-driven scientific discovery tasks extracted from peer-reviewed publications and evaluates agents using Python-based evaluation environments, specialized metrics and intermediate criteria to control for data contamination and agent shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ScienceAgentBench (benchmark and evaluation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmarking framework (automated evaluation scripts + tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>data-driven scientific discovery (bioinformatics, computational chemistry, GIS, neuroscience)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Used to evaluate agents that generate data-driven discoveries, modeling pipelines, and visualizations for domain-specific discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Python-based evaluation environments, end-result metrics, intermediate evaluation criteria, and special metrics to detect data contamination and agent shortcutting; comparisons made against pure LLM prompting and tool-augmented agents.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Tasks are cherry-picked from peer-reviewed publications; novelty assessed by achieving better predictive results or recreating/expanding published findings and by human/expert-based comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Paper reports that execution feedback is necessary for agents to generate useful solutions and that pure LLM prompting underperforms; exact numeric results live in that cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Rigorous per-task automated metrics are used; ScienceAgentBench emphasizes controlling for contamination and shortcuts to get faithful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported in this paper (see the original ScienceAgentBench for details).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>ScienceAgentBench highlights the importance of execution feedback and careful contamination controls to ensure that apparent novel findings are not artifacts; novelty without execution feedback is unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Cited work indicates agents need execution feedback to produce valid outputs, suggesting a generation-validation gap if feedback is absent.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not detailed here; refer to original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Depends on domain/task; original benchmark uses Python environments and can require domain-specific compute.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Execution feedback, specialized metrics to detect contamination/shortcuts, and read-only evaluation scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ScienceAgentBench demonstrates that execution feedback and rigorous evaluation controls are necessary for agents to produce useful, verifiable data-driven scientific discoveries, highlighting the risk of false positives when feedback and contamination controls are absent.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2172.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DISCOVERYWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related simulated virtual discovery environment where agents must form hypotheses, design/run experiments, analyze results and act on conclusions across many scientific domains; used to study general discovery skills rather than domain-specific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DISCOVERYWORLD (virtual discovery environment)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>simulated discovery environment (game-like simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific discovery (proteomics, chemistry, archaeology, physics, agriculture, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Agents generate hypotheses, experimental actions, and interpretations in a simulated discovery game-world.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task success is measured by agent ability to form hypotheses, design and conduct experiments, and derive correct conclusions within the simulation; environment has constrained objects/actions so evaluation is domain-general within the simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Focuses on general discovery skills rather than domain-expert novelty; novelty operationalized as successful hypothesis-driven discovery in the simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>The original work highlights that tasks require multiple steps (120 tasks in the cited work) and supports assessment of discovery skills; MLGym references it as an example but notes DISCOVERYWORLD has a limited action/object set.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Simulation provides built-in evaluation of actions and outcomes; MLGym contrasts its approach as more open-ended and ML-task focused with real evaluation scripts per task.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Within a constrained simulator, novelty can be measured and validated; but generalization to real scientific novelty is limited by simulator constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Simulator enables clear validation of generated hypotheses/actions, reducing asymmetry within the simulated domain, but may not capture real-world validation difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Simulation cost depends on engine complexity; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Constrained simulation with explicit actions and evaluation allows automated validation of generated discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DISCOVERYWORLD demonstrates a path to automatically evaluate hypothesis generation and experimental design in a simulated environment, but its constrained action/object set limits applicability to real scientific novelty validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2172.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2172.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RE-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RE-Bench (Research Engineering Benchmark) / Evaluating frontier AI R&D capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related benchmark of diverse and challenging ML tasks that includes timed sessions comparing ML experts versus LLM agents; used to study agent performance under time budgets and human-agent comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RE-Bench (Research Engineering Benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / human-vs-agent timed evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>machine learning research tasks and engineering</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Evaluates agents that generate experimental code, model improvements and engineering solutions for ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Timed sessions and automated evaluation scripts per task; direct comparison of agents to human experts given identical time budgets (2h, 8h, 32h comparisons cited).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by improvement over baseline and/or human expert performance under time budgets; novelty operationalized as surpassing human expert returns within given time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Paper cites RE-Bench report that agents achieved 4× higher scores than human experts in 2-hour sessions, but humans scaled better with time and outperformed agents at 8h and 32h budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Automated metrics and direct head-to-head comparisons provide quantitative validation; indicates agents can be competitive under constrained budgets but have inferior returns-to-time compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>RE-Bench evidence suggests agents can find strong solutions quickly (familiar or bounded optimization tasks) but struggle to continue improving with larger time budgets (which is important for deep, novel research).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Shows a performance/time asymmetry: agents may rapidly generate validated gains early but humans achieve better long-term returns; this is a practical generation-validation/time-budget gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Timed human-vs-agent experiments highlight cost as part of evaluation; exact compute/cost numbers in the original RE-Bench source.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Timed human comparison, human-in-the-loop evaluation, and inclusion of complex tasks that require extended experimentation to reveal long-term returns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RE-Bench indicates agents can outperform human experts under tight time budgets but humans scale better with additional time, highlighting limits of current automated research agents for sustained, novel research progress.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery <em>(Rating: 2)</em></li>
                <li>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents <em>(Rating: 2)</em></li>
                <li>SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering <em>(Rating: 2)</em></li>
                <li>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation <em>(Rating: 2)</em></li>
                <li>Evaluating frontier ai r&d capabilities of language model agents against human experts <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models <em>(Rating: 2)</em></li>
                <li>ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code <em>(Rating: 1)</em></li>
                <li>AutoML, NAS and automated ML optimization (survey references cited) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2172",
    "paper_id": "paper-276482776",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "MLGym agent (SWE-Agent harness)",
            "name_full": "MLGym Agentic Harness (SWE-Agent based LLM research agent)",
            "brief_description": "An LLM-driven agentic harness used in MLGym that executes shell commands, uses tool-augmented interfaces (ACI), iteratively runs experiments, inspects results, and submits trained artifacts; it is the experimental agent used to study automated ML research workflows in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MLGym Agent (SWE-Agent harness with LLM backbones)",
            "system_type": "tool-augmented large language model agent (LLM agent + ACI)",
            "domain": "machine learning research / AI research automation",
            "generation_capability": "generates code, experimental configurations, hyperparameter choices, model training commands, strategies/heuristics for algorithmic tasks, and written research artifacts (iterative proposals and code edits)",
            "validation_method": "Automated evaluation scripts per task (read-only) invoked by 'validate' and 'submit' commands; validate runs the evaluation script to return test-set metrics without terminating the run, submit runs final evaluation and terminates; agents can run training scripts, read validation outputs, and iterate based on these automated metrics.",
            "novelty_measure": "Capability-level taxonomy defined in paper (Level 0–5, where novelty corresponds to Level 3+), comparison to baseline and SOTA on per-task metrics, and whether result is 'publication-worthy' or establishes new SOTA; within the benchmark novelty is operationalized by beating baselines or SOTA where applicable.",
            "generation_performance": "On MLGym-Bench the agent (using frontier LLM backbones) typically achieves baseline improvements (Level 1) mainly via hyperparameter and training changes rather than inventing novel algorithms; aggregate performance measured via per-task metrics and AUP (area under performance-profile) shows variation by backbone (OpenAI O1-preview highest AUP, Gemini-1.5-Pro cost-effective). Performance is substantially better on familiar/supervised tasks (e.g., CIFAR-10, Fashion-MNIST, House Prices) and much worse on novel/harder tasks requiring algorithmic innovation (Language Modeling, RL tasks like Breakout, MetaMaze).",
            "validation_performance": "Validation is fully automated and reliable for checking metric values on held-out test sets (the environment runs evaluation scripts and returns numeric scores); however agents frequently trigger 'Evaluation Error' (missing or malformed submission artifact) — Evaluation Error accounted for ~75% of termination error types in runs, indicating frequent failures in producing valid, evaluable outputs rather than failures of the evaluation script itself.",
            "false_positive_rate": "No numeric false positive rate reported; qualitative evidence: many termination errors are 'Evaluation Error' (invalid submissions) implying agents often produce outputs that cannot be accepted as valid; methods that do not outperform baseline are marked infeasible and scored conservatively (infeasible penalty = (1+ε)×baseline ratio, ε=0.05).",
            "false_negative_rate": "No numeric false negative rate reported; not measured explicitly in the paper.",
            "novelty_effect_on_validation": "Validation reliability is high for familiar tasks with clear test sets and read-only evaluation scripts; for novel or open-ended tasks (e.g., algorithmic innovation, RL, large language-model training) agents more often fail to produce valid submissions and show lower validated performance — i.e., novelty correlates with lower validated performance and more invalid submissions.",
            "generation_validation_asymmetry": "Evidence of asymmetry: agents can generate incremental, incrementalizable outputs (hyperparameter tuning, engineering changes) that validate well, but they rarely generate truly novel hypotheses/algorithms (Level 3+) and thus do not produce validated breakthroughs. The paper reports agents usually find better hyperparameters but 'do not generate novel hypotheses, algorithms, architectures, or substantial improvements.'",
            "out_of_distribution_performance": "Agents perform worse on tasks that demand out-of-distribution generalization or algorithmic creativity (Language Modeling and RL tasks had highest failure rates). The paper reports models failing entirely on some tasks (e.g., Llama-3.1-405b-instruct and GPT-4o failing to produce any valid solution for certain tasks), indicating degraded OOD performance relative to in-distribution supervised problems.",
            "calibration_quality": "No explicit calibration metrics (confidence vs. accuracy) reported for agent predictions/decisions; calibration quality of model confidence is not analyzed in the paper.",
            "validation_computational_cost": "Validation requires running the same task-specific evaluation scripts and can be compute-intensive (training timeouts per task exist); cost vs. performance analysis shows OpenAI O1-Preview is the most computationally expensive backbone while Gemini-1.5-Pro is roughly ~9× cheaper than O1 and achieves ~99% of O1's AUP, so validation (and generation) cost vary strongly with backbone choice.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Memory Module (store/retrieve best configs), validate/submit iterative feedback loop, tool-augmented ACI, read-only evaluation scripts to prevent cheating, structured agent scaffolding (SWE-Agent), cost-aware experimental budgets; paper also references search/MCTS and tree-search augmentations (from related works) as promising gap-closing approaches.",
            "evidence_type": "mixed",
            "key_findings": "MLGym agentic harness (SWE-Agent) can autonomously run experiments and improve baselines primarily via optimization (hyperparameters), with reliable automated validation via read-only evaluation scripts; however agents fail to produce novel, publishable scientific contributions, show worse validated performance on novel/harder tasks (RL, language-model training), and frequently produce invalid submissions that trigger evaluation errors.",
            "uuid": "e2172.0"
        },
        {
            "name_short": "Memory Module",
            "name_full": "MLGym Memory Module (research logs storage and retrieval)",
            "brief_description": "A structured memory component that lets an agent persist experimental results, good configurations and findings (with embeddings and tags) and retrieve top-k relevant past entries by cosine similarity to inform later experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MLGym Memory Module",
            "system_type": "retrieval / external memory augmentation for LLM agents",
            "domain": "machine learning research workflows / long-horizon experiment management",
            "generation_capability": "does not generate scientific claims itself; it stores and retrieves agent-generated artifacts and textual summaries that influence subsequent generation (e.g., reusing best training configurations, reproducing experiments).",
            "validation_method": "Improves validation indirectly by enabling the agent to recall prior validated configurations and re-run/iterate from them; memory entries include tags and embeddings and retrieved items are used before making new submissions or continuing experiments.",
            "novelty_measure": "Memory tags (3-gram matching) and cosine similarity retrieval; novelty not directly measured by the module itself — it supports long-horizon novelty by preventing forgetting of earlier successful experiments.",
            "generation_performance": "Empirically shown to improve long-horizon task performance: agents with memory could retrieve best training configs and achieve superior results compared to agents without memory (figures and qualitative description in paper), enabling sustained progress over longer trajectories.",
            "validation_performance": "Memory improves the agent's ability to reproduce previously validated configurations and thereby improves the chance of producing valid submissions; exact numeric validation improvement not provided but qualitative examples and figures (Figure 11/12) show positive impact.",
            "false_positive_rate": "Not reported.",
            "false_negative_rate": "Not reported.",
            "novelty_effect_on_validation": "Memory primarily aids repeated or long-horizon tasks (where validation outcomes had been previously observed); it reduces catastrophic forgetting so validated performance on later steps improves when prior successful configurations are relevant; for truly novel tasks the memory has limited direct effect.",
            "generation_validation_asymmetry": "Memory reduces asymmetry by preserving and enabling reuse of validated results, narrowing the gap between exploration (generation) and reliable submission (validation) across long runs.",
            "out_of_distribution_performance": "Memory helps for long-horizon OOD style search by letting agent re-use past successful strategies, but no explicit metrics for OOD performance change provided.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Memory operations (embedding store and cosine similarity retrieval) are lightweight relative to training/validation runs; cost reported qualitatively as small compared to training timeouts.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Persistent storage of validated configurations, retrieval by similarity, inclusion of memory state in system prompt (tags + number of records) to make agent aware of stored knowledge.",
            "evidence_type": "supports",
            "key_findings": "A small structured memory that stores validated configurations and results materially helps agents on long-horizon experimental tasks by enabling them to retrieve and reuse the best-known configurations, thereby improving validated outcomes and reducing forgetting.",
            "uuid": "e2172.1"
        },
        {
            "name_short": "Validate/Submit pipeline",
            "name_full": "MLGym Validate and Submit Evaluation Pipeline",
            "brief_description": "The built-in MLGym mechanism by which agents can run task-specific read-only evaluation scripts: 'validate' to get current test-set metrics without terminating the run, and 'submit' to run final evaluation and terminate; these enable iterative experiment-feedback loops and standardized grading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MLGym validate/submit evaluation commands + read-only task evaluation scripts",
            "system_type": "automated evaluation pipeline / grading interface",
            "domain": "machine learning experiments and automated scientific evaluation",
            "generation_capability": "does not generate outputs, but assesses generated outputs (models, code, strategies) by executing predefined evaluation scripts and returning metrics.",
            "validation_method": "Task-authored read-only evaluation scripts executed in the environment; 'validate' runs this script multiple times during an agent's trajectory to provide feedback, 'submit' runs final evaluation and logs metrics; evaluation scripts vary by task (accuracy, R2, BLEU, reward, wall-clock time for 3-SAT, etc.).",
            "novelty_measure": "Novelty is indirectly measured by outperforming baselines or achieving SOTA metrics defined in the task; evaluation scripts include safeguards (read-only) to prevent agents from changing scoring logic.",
            "generation_performance": "Enables agents to iteratively improve generation by giving immediate metric feedback; paper reports that use of validate increases agents' ability to explore (Best Attempt vs Best Submission distinction) but many invalid final submissions still occur.",
            "validation_performance": "Highly reliable at computing numerical metrics when submission artifacts are properly formatted; large fraction of termination errors are 'Evaluation Error' indicating many generated artifacts are malformed rather than being wrong per metric.",
            "false_positive_rate": "Not quantified; read-only evaluation minimizes false positives caused by agents manipulating evaluation logic, but invalid submission formats lead to many evaluation errors.",
            "false_negative_rate": "Not quantified.",
            "novelty_effect_on_validation": "When agents tackle novel tasks, they more frequently produce artifacts that fail evaluation (malformed outputs) and thus the validate/submit pipeline exposes these failures; for familiar tasks validation is straightforward and reliable.",
            "generation_validation_asymmetry": "The pipeline exposes a practical asymmetry: generation can produce candidate results but often fails to produce properly formed evaluable artifacts that validation requires; thus validation often fails due to format/artefact issues even when the underlying approach could be valid.",
            "out_of_distribution_performance": "Evaluation scripts are task-specific and may not capture cross-domain novelty; OOD artifacts may not conform to required submission format, increasing evaluation errors.",
            "calibration_quality": "Not applicable.",
            "validation_computational_cost": "Validation cost equals cost of executing evaluation scripts (can be expensive for training-heavy tasks); overall pipeline enforces task-specific timeouts to limit runaway compute.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Read-only evaluation scripts, iterative 'validate' feedback, and task timeouts; scoring rules for infeasible methods (penalty multiplier) to avoid rewarding invalid / non-submissions.",
            "evidence_type": "supports",
            "key_findings": "A robust automated validate/submit pipeline enables iterative improvement and objective scoring, but agents frequently fail to produce correctly formatted submissions or valid artifacts for novel/harder tasks, causing many evaluation errors rather than metric-level failures.",
            "uuid": "e2172.2"
        },
        {
            "name_short": "Performance profiles / AUP",
            "name_full": "Performance Profiles and AUP (Area Under Performance Profile) scoring",
            "brief_description": "An aggregation methodology adapted from optimization and AutoML literature used by MLGym to compare agents across heterogeneous tasks and metrics by computing performance ratios and integrating over thresholds (AUP).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Performance profile curves and AUP aggregation",
            "system_type": "evaluation / benchmarking methodology",
            "domain": "benchmarking of AI research agents across diverse ML tasks",
            "generation_capability": "N/A (evaluation metric rather than generator)",
            "validation_method": "Aggregates per-task numeric metrics into performance profiles by comparing each method's metric to best method on that task, then integrates the profile curve to compute AUP; supports both 'Best Attempt' (validate calls) and 'Best Submission' (final submits) aggregations to separate exploration ceiling from submission reliability.",
            "novelty_measure": "Does not directly measure novelty; measures relative performance across tasks and can show whether methods approach best-known solutions (SOTA) on tasks — indirectly signals novelty if a method outperforms baselines and existing methods across tasks.",
            "generation_performance": "Used to report aggregate generation/solution quality across tasks: OpenAI O1-Preview had highest AUP; Gemini-1.5-Pro was cost-effective and achieved ~99% of O1's AUP; detailed per-task metrics show agents often improve baselines but rarely produce novel algorithmic breakthroughs.",
            "validation_performance": "AUP summarizes validated performance; paper reports separate AUPs for Best Attempt and Best Submission where Best Attempt captures exploratory ceiling and Best Submission captures agent's ability to submit best result reliably.",
            "false_positive_rate": "Not directly applicable; AUP handles infeasible methods by applying a conservative penalty (infeasible score set to (1+ε)×baseline ratio with ε=0.05) to avoid overly rewarding invalid outputs.",
            "false_negative_rate": "Not applicable.",
            "novelty_effect_on_validation": "AUP distinctions (Best Attempt vs Best Submission) reveal that agents may reach high-performance attempts (exploration ceiling) but fail to reliably submit them; this gap is larger on more novel or complex tasks.",
            "generation_validation_asymmetry": "AUP captures asymmetry by showing higher Best Attempt AUP than Best Submission AUP for some models, indicating agents can reach good solutions but fail to reliably package/submit them.",
            "out_of_distribution_performance": "Performance profiles reveal poor relative performance on tasks requiring algorithmic creativity; the profiles quantitatively show methods being further from the best on novel/harder tasks.",
            "calibration_quality": "Not applicable.",
            "validation_computational_cost": "Computation to produce AUP is negligible compared to training/validation runs; however, obtaining the underlying validated metrics can be computationally expensive.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Separate Best Attempt vs Best Submission evaluation highlights deployment/packaging reliability issues; conservative scoring of infeasible methods; inversion/normalization for metrics with different directions.",
            "evidence_type": "supports",
            "key_findings": "Performance-profiles and AUP provide a principled, domain-agnostic aggregation of validated agent performance and reveal a consistent pattern: agents can explore to promising attempts but are less reliable at finalizing and submitting those attempts, especially on novel tasks.",
            "uuid": "e2172.3"
        },
        {
            "name_short": "Frontier LLMs (evaluated)",
            "name_full": "Frontier Large Language Models evaluated in MLGym (OpenAI O1-preview, Gemini-1.5-Pro, Claude-3.5-Sonnet, Llama-3.1-405B-Instruct, GPT-4o)",
            "brief_description": "A set of state-of-the-art LLM backbones used as the agent's base models in experiments, compared under a common SWE-Agent scaffolding across MLGym-Bench tasks for generation and validation performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "OpenAI O1-preview; Gemini-1.5-Pro; Claude-3.5-sonnet; Llama-3.1-405b-instruct; GPT-4o",
            "system_type": "large language models (LLMs)",
            "domain": "general AI agent backbones for ML research automation",
            "generation_capability": "Generate code edits, experiment commands, model configurations, strategy code (game theory tasks), model training scripts, and literature summaries when tool-augmented.",
            "validation_method": "Same automated MLGym evaluation pipeline (validate/submit and task-specific read-only evaluation scripts) used to test outputs produced by these LLM-driven agents; performance aggregated via AUP and per-task metrics.",
            "novelty_measure": "Measured by ability to exceed baseline metrics and approach/achieve SOTA across tasks; the paper also uses capability levels to classify novelty (agents reaching Level 2+ would imply SOTA or novel contributions).",
            "generation_performance": "Aggregate results: OpenAI O1-preview achieved highest overall AUP (best aggregate), Gemini-1.5-Pro and Claude-3.5-Sonnet close behind; Gemini offered best cost-performance tradeoff (~9× cheaper than O1 while reaching ~99% of its AUP). Llama-3.1 and GPT-4o failed to produce valid solutions for some challenging tasks (e.g., language modeling, breakout). Generally strong on supervised tasks, weak on novel algorithmic/RL tasks.",
            "validation_performance": "Validation pipeline successfully measured model outputs; many runs experienced Evaluation Errors (invalid submissions) rather than metric-level failures; evaluation error was the dominant termination cause (~75%). Completion/failure differed by model: GPT-4o highest failure rate; Gemini and O1-preview best completion rates though O1-preview had high incompletion rate but produced at least one valid solution per task.",
            "false_positive_rate": "Not reported numerically per model; qualitative: models occasionally produce malformed or invalid submission artifacts leading to evaluation errors rather than incorrectly validated but false-positive outputs.",
            "false_negative_rate": "Not reported.",
            "novelty_effect_on_validation": "All LLM backbones exhibit degraded validated performance on tasks requiring novel algorithmic invention or large-scale training (language modeling, RL), while performing relatively well on familiar supervised tasks.",
            "generation_validation_asymmetry": "Observed: some models (e.g., O1-preview) can explore and reach high-performing attempts but do not always finalize and submit the best attempt reliably (Best Attempt AUP &gt; Best Submission AUP), indicating a practical generation-vs-submission gap.",
            "out_of_distribution_performance": "Out-of-distribution or novel tasks (algorithmic innovation, large LM training, complex RL) show worse performance and higher failure rates; Llama/GPT-4o failed entirely on some OOD-hard tasks.",
            "calibration_quality": "No calibration data reported for model confidences.",
            "validation_computational_cost": "OpenAI O1-preview highest cost; Gemini-1.5-Pro and Claude-3.5-Sonnet more cost-effective; cost-vs-AUP pareto frontier provided in paper (Figure 3) showing tradeoffs.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Agent scaffolding (SWE-Agent), memory module, validate/submit iterative feedback, tool documentation in system prompt, cost/time budgets, and literature-search tools; related works suggest search (MCTS, tree search) and execution feedback as additional mechanisms.",
            "evidence_type": "mixed",
            "key_findings": "Frontier LLM backbones, when run within a common agent harness, improve baselines primarily via engineering and hyperparameter changes; they differ substantially in cost-effectiveness and reliability, and all show substantially lower validated performance on novel algorithmic or RL tasks compared to familiar supervised problems.",
            "uuid": "e2172.4"
        },
        {
            "name_short": "ScienceAgentBench",
            "name_full": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
            "brief_description": "A related benchmark (cited) that contains data-driven scientific discovery tasks extracted from peer-reviewed publications and evaluates agents using Python-based evaluation environments, specialized metrics and intermediate criteria to control for data contamination and agent shortcuts.",
            "citation_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
            "mention_or_use": "mention",
            "system_name": "ScienceAgentBench (benchmark and evaluation framework)",
            "system_type": "benchmarking framework (automated evaluation scripts + tasks)",
            "domain": "data-driven scientific discovery (bioinformatics, computational chemistry, GIS, neuroscience)",
            "generation_capability": "Used to evaluate agents that generate data-driven discoveries, modeling pipelines, and visualizations for domain-specific discovery tasks.",
            "validation_method": "Python-based evaluation environments, end-result metrics, intermediate evaluation criteria, and special metrics to detect data contamination and agent shortcutting; comparisons made against pure LLM prompting and tool-augmented agents.",
            "novelty_measure": "Tasks are cherry-picked from peer-reviewed publications; novelty assessed by achieving better predictive results or recreating/expanding published findings and by human/expert-based comparison.",
            "generation_performance": "Paper reports that execution feedback is necessary for agents to generate useful solutions and that pure LLM prompting underperforms; exact numeric results live in that cited work.",
            "validation_performance": "Rigorous per-task automated metrics are used; ScienceAgentBench emphasizes controlling for contamination and shortcuts to get faithful validation.",
            "false_positive_rate": "Not reported in this paper (see the original ScienceAgentBench for details).",
            "false_negative_rate": "Not reported here.",
            "novelty_effect_on_validation": "ScienceAgentBench highlights the importance of execution feedback and careful contamination controls to ensure that apparent novel findings are not artifacts; novelty without execution feedback is unreliable.",
            "generation_validation_asymmetry": "Cited work indicates agents need execution feedback to produce valid outputs, suggesting a generation-validation gap if feedback is absent.",
            "out_of_distribution_performance": "Not detailed here; refer to original paper.",
            "calibration_quality": "Not reported here.",
            "validation_computational_cost": "Depends on domain/task; original benchmark uses Python environments and can require domain-specific compute.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Execution feedback, specialized metrics to detect contamination/shortcuts, and read-only evaluation scripts.",
            "evidence_type": "supports",
            "key_findings": "ScienceAgentBench demonstrates that execution feedback and rigorous evaluation controls are necessary for agents to produce useful, verifiable data-driven scientific discoveries, highlighting the risk of false positives when feedback and contamination controls are absent.",
            "uuid": "e2172.5"
        },
        {
            "name_short": "DISCOVERYWORLD",
            "name_full": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
            "brief_description": "A related simulated virtual discovery environment where agents must form hypotheses, design/run experiments, analyze results and act on conclusions across many scientific domains; used to study general discovery skills rather than domain-specific methods.",
            "citation_title": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
            "mention_or_use": "mention",
            "system_name": "DISCOVERYWORLD (virtual discovery environment)",
            "system_type": "simulated discovery environment (game-like simulator)",
            "domain": "general scientific discovery (proteomics, chemistry, archaeology, physics, agriculture, etc.)",
            "generation_capability": "Agents generate hypotheses, experimental actions, and interpretations in a simulated discovery game-world.",
            "validation_method": "Task success is measured by agent ability to form hypotheses, design and conduct experiments, and derive correct conclusions within the simulation; environment has constrained objects/actions so evaluation is domain-general within the simulation.",
            "novelty_measure": "Focuses on general discovery skills rather than domain-expert novelty; novelty operationalized as successful hypothesis-driven discovery in the simulation tasks.",
            "generation_performance": "The original work highlights that tasks require multiple steps (120 tasks in the cited work) and supports assessment of discovery skills; MLGym references it as an example but notes DISCOVERYWORLD has a limited action/object set.",
            "validation_performance": "Simulation provides built-in evaluation of actions and outcomes; MLGym contrasts its approach as more open-ended and ML-task focused with real evaluation scripts per task.",
            "false_positive_rate": "Not reported here.",
            "false_negative_rate": "Not reported here.",
            "novelty_effect_on_validation": "Within a constrained simulator, novelty can be measured and validated; but generalization to real scientific novelty is limited by simulator constraints.",
            "generation_validation_asymmetry": "Simulator enables clear validation of generated hypotheses/actions, reducing asymmetry within the simulated domain, but may not capture real-world validation difficulty.",
            "out_of_distribution_performance": "Not discussed here.",
            "calibration_quality": "Not reported here.",
            "validation_computational_cost": "Simulation cost depends on engine complexity; not quantified in this paper.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Constrained simulation with explicit actions and evaluation allows automated validation of generated discoveries.",
            "evidence_type": "neutral",
            "key_findings": "DISCOVERYWORLD demonstrates a path to automatically evaluate hypothesis generation and experimental design in a simulated environment, but its constrained action/object set limits applicability to real scientific novelty validation.",
            "uuid": "e2172.6"
        },
        {
            "name_short": "RE-Bench",
            "name_full": "RE-Bench (Research Engineering Benchmark) / Evaluating frontier AI R&D capabilities",
            "brief_description": "A related benchmark of diverse and challenging ML tasks that includes timed sessions comparing ML experts versus LLM agents; used to study agent performance under time budgets and human-agent comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "RE-Bench (Research Engineering Benchmark)",
            "system_type": "benchmark / human-vs-agent timed evaluation framework",
            "domain": "machine learning research tasks and engineering",
            "generation_capability": "Evaluates agents that generate experimental code, model improvements and engineering solutions for ML tasks.",
            "validation_method": "Timed sessions and automated evaluation scripts per task; direct comparison of agents to human experts given identical time budgets (2h, 8h, 32h comparisons cited).",
            "novelty_measure": "Measured by improvement over baseline and/or human expert performance under time budgets; novelty operationalized as surpassing human expert returns within given time constraints.",
            "generation_performance": "Paper cites RE-Bench report that agents achieved 4× higher scores than human experts in 2-hour sessions, but humans scaled better with time and outperformed agents at 8h and 32h budgets.",
            "validation_performance": "Automated metrics and direct head-to-head comparisons provide quantitative validation; indicates agents can be competitive under constrained budgets but have inferior returns-to-time compared to humans.",
            "false_positive_rate": "Not reported here.",
            "false_negative_rate": "Not reported here.",
            "novelty_effect_on_validation": "RE-Bench evidence suggests agents can find strong solutions quickly (familiar or bounded optimization tasks) but struggle to continue improving with larger time budgets (which is important for deep, novel research).",
            "generation_validation_asymmetry": "Shows a performance/time asymmetry: agents may rapidly generate validated gains early but humans achieve better long-term returns; this is a practical generation-validation/time-budget gap.",
            "out_of_distribution_performance": "Not detailed here.",
            "calibration_quality": "Not reported here.",
            "validation_computational_cost": "Timed human-vs-agent experiments highlight cost as part of evaluation; exact compute/cost numbers in the original RE-Bench source.",
            "human_validation_required": null,
            "gap_closing_mechanisms": "Timed human comparison, human-in-the-loop evaluation, and inclusion of complex tasks that require extended experimentation to reveal long-term returns.",
            "evidence_type": "supports",
            "key_findings": "RE-Bench indicates agents can outperform human experts under tight time budgets but humans scale better with additional time, highlighting limits of current automated research agents for sustained, novel research progress.",
            "uuid": "e2172.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
            "rating": 2
        },
        {
            "paper_title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
            "rating": 2
        },
        {
            "paper_title": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
            "rating": 2
        },
        {
            "paper_title": "Evaluating frontier ai r&d capabilities of language model agents against human experts",
            "rating": 2
        },
        {
            "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code",
            "rating": 1
        },
        {
            "paper_title": "AutoML, NAS and automated ML optimization (survey references cited)",
            "rating": 1
        }
    ],
    "cost": 0.02567875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MLGym: A New Framework and Benchmark for Advancing AI Research Agents
February 21, 2025</p>
<p>Deepak Nathani dnathani@ucsb.edu 
University of California
Santa Barbara</p>
<p>Lovish Madaan 
University College London</p>
<p>GenAI at Meta</p>
<p>Nicholas Roberts 
University of Wisconsin-Madison</p>
<p>Nikolay Bashlykov 
GenAI at Meta</p>
<p>Ajay Menon 
GenAI at Meta</p>
<p>Vincent Moens 
Amar Budhiraja 
GenAI at Meta</p>
<p>Despoina Magka 
Vladislav Vorotilov 
GenAI at Meta</p>
<p>Gaurav Chaurasia 
GenAI at Meta</p>
<p>Dieuwke Hupkes 
GenAI at Meta</p>
<p>Ricardo Silveira Cabral 
GenAI at Meta</p>
<p>Tatiana Shavrina 
GenAI at Meta</p>
<p>Jakob Foerster 
Yoram Bachrach 
William Yang Wang 
University of California
Santa Barbara</p>
<p>Roberta Raileanu raileanu@meta.com 
University College London</p>
<p>GenAI at Meta</p>
<p>University of Oxford</p>
<p>MLGym: A New Framework and Benchmark for Advancing AI Research Agents
February 21, 20256F0DAC48F6316D2F899D79B538E5A128arXiv:2502.14499v1[cs.CL]
We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks.This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents.MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory.Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task.We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet,Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5Pro.Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks.We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements.We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</p>
<p>Introduction</p>
<p>Accelerating scientific discovery has been a long-standing ambition in artificial intelligence (AI) research, with early initiatives like the Oak Ridge Applied Artificial Intelligence Project in 1979 exploring (Team, 1985;Emrich et al., 1988;Johnson and Schaffer, 1994).More recent explorations enabled by advances in foundation models (Achiam et al., 2023;Anthropic, 2024;Team et al., 2024;Dubey et al., 2024) provide a proof-of-concept of a fully automated pipeline for end-to-end paper generation (Lu et al., 2024).In the future, we envision AI Research Agents capable of independently conducting literature search, generating scientific hypotheses, designing experiments, implementing new methods, analyzing results, disseminating findings by writing scientific papers, and applying this research in products, thus assisting with all parts of the research process.Such agents should be capable of both working fully autonomously, or be guided by human supervision, taking into account feedback from users.This vision stems from the recognition that AI, with its capacity to process vast datasets and discern complex patterns, could accelerate scientific breakthroughs in areas such as drug discovery and materials science by identifying promising drug candidates or predicting the properties of novel materials (Hessler and Baringhaus, 2018;Schneider et al., 2020;Guo et al., 2021).Unlike traditional methods, AI agents can reveal hidden interdisciplinary relationships by analyzing vast knowledge graphs, leading to novel insights and solutions for complex challenges like climate modeling.By automating laborious tasks and exploring unconventional avenues, AI agents can liberate scientists to focus on higher-level cognitive activities, ultimately driving innovation and expanding the frontiers of knowledge.Machine learning (ML) research, with its emphasis on empirical validation and systematic experimentation in simulation, presents an ideal testbed for exploring and improving the utlity of LLMs for advancing scientific research.</p>
<p>However, the scientific method inherently relies on empirical validation, rigorous evaluation, and standardized benchmarks to ensure the reliability and reproducibility of findings.While significant progress has been made in developing AI agents for various domains (Yang et al., 2024;Wu et al., 2024;Ma et al., 2024;Deng et al., 2023;Wang et al., 2023), we currently lack comprehensive frameworks and benchmarks specifically designed to assess their capabilities in conducting open-ended AI research tasks in diverse domains.This absence of standardized evaluation tools hinders our ability to objectively measure progress and identify areas for improvement in this emerging field.et al., 2022a) literature, to more fairly assess the relative performance of LLM agents across tasks with their own distinct performance metrics.</p>
<p>To summarize our contributions, we (i) introduce MLGym, the first Gym environment for evaluating and developing AI Research Agents, (ii) release MLGym-Bench, a suite of diverse open-ended AI research tasks for evaluating LLM agents, (iii) propose a new evaluation metric for comparing multiple agents on a variety of tasks, and (iv) extensively evaluate frontier LLMs on MLGym-Bench.Finally, MLGym makes it easy for researchers and developers to integrate and evaluate new tasks, agents, or models.</p>
<p>In the rest of the paper, we discuss related LLM agent frameworks and benchmarks, provide an overview of the MLGym framework, introduce the mechanics behind MLGym-Bench and its evaluation, present our experimental setup and results, and conclude with a discussion of limitations and future extensions.</p>
<p>Capability Levels for AI Research Agents</p>
<p>We propose a hierarchical framework to categorize the capabilities of LLM agents for accelerating AI research.This framework consists of six levels, each representing a distinct degree of autonomy and scientific contribution.</p>
<p>Level 0: Reproduction At this level, LLM agents can reproduce existing research papers either with or without access to the original code.This level demonstrates a basic understanding of the research domain and the ability to replicate established results.</p>
<p>Level 1: Baseline Improvement At Level 1, LLM agents can improve performance on a benchmark given a baseline code that is not state-of-the-art (SOTA).This level indicates the ability to analyze and optimize existing solutions, even if they are not the most advanced.</p>
<p>Level 2: SOTA Achievement At Level 2, LLM agents can achieve SOTA performance on a benchmark given only a task description and access to the published literature before the invention of the SOTA approach, but no access to the SOTA paper or code.This level demonstrates the ability to come up with a solution to an open research problem which is as good as the one found by humans.</p>
<p>Level 3: Novel Scientific Contribution At Level 3, LLM agents can make a novel scientific contribution, such as coming up with a new method that establishes a new SOTA on multiple benchmarks, and is worthy of publication at a top ML conference such as NeurIPS.</p>
<p>Level 4: Groundbreaking Scientific Contribution At Level 4, LLM agents can identify key research questions, directions, solutions, and make a notable scientific contribution worthy of being published as an oral or best paper award at a prestigious ML conference such as NeurIPS.</p>
<p>Level 5: Long-Term Research Agenda At Level 5, LLM agents can pursue a long-term research agenda, coming up with the research questions, directions, and solutions, continuously producing scientific discoveries over the span of weeks, months, or years.LLMs at this level should be capable of paradigm-shifting research breakthroughs worthy of prizes such as Nobel or Turing.</p>
<p>By defining these capability levels, we provide a framework for evaluating frontier AI Research Agents.</p>
<p>MLGym-Bench focuses on Level 1: Baseline Improvement of the categorisation defined above.</p>
<p>Related Work</p>
<p>AI Research Frameworks and Benchmarks</p>
<p>Table 1 shows a comparison between MLGym and MLGym-Bench with other related LLM agent frameworks and benchmarks.Below, we expand on the differences between MLGym and these works.</p>
<p>First, MLGym is the first framework for AI Research Agents that provides a Gym interface, making it easy to integrate and train these agents using RL algoritms.MLGym-Bench is also the first benchmark to include tasks that require research on algorithms in multiple domains such as RL, game theory, or SAT.  1 Comparison of MLGym and MLGym-Bench with other related LLM agent frameworks and benchmarks.Algorithmic Tasks refers to the inclusion of tasks that require coming up with new algorithms such as reinforcement learning, game theory or SAT problems.Open-ended Research refers to the inclusion of tasks that are not fully solved by the research community and where multiple new solutions could be discovered such as language modeling, game theory or SAT problems.Flexible Artifacts refers to the allowance of different research artifacts such as model weights, reinforcement learning algorithms, or code capturing an agent's strategy.</p>
<p>Second, MLGym-Bench encompasses a wide range of open-ended AI research tasks, covering supervised learning, language modeling, reinforcement learning, game theory and SAT.In contrast, SWE-Bench/SWE-Agent (Yang et al., 2024) focuses on solving Github issues so the code changes either fix the code or not (as opposed to optmization tasks with finer-grained metrics, such as a loss metric in a supervised learning problem).Similarly, MLE-Bench (Chan et al., 2024) includes narrowly scoped machine learning tasks from Kaggle competitions.While these tasks have a spectrum of quality levels, they tend to be already solved by current state-of-the-art methods.On the other hand, MLAgentBench (Huang et al., 2024) contains both ML-specialized tasks (regression, classification, code speed improvements) and tasks focused on recent research challenges (e.g.CLRS reasoning corpus (Veličković et al., 2022), BabyLM challenge (Oba et al., 2023)).RE-bench (METR, 2024) also consists of broadly scoped ML engineering tasks which are hard to saturate and reward increasingly sophisticated approaches.ScienceAgentBench (Chen et al., 2024) incorporates data-driven scientific discovery tasks extracted from peer-reviewed publications, but which are so specific that they resemble Kaggle competition rather than open research questions.</p>
<p>Third, MLGym allows for flexible evaluation artifacts: it is sufficient to provide python code that the agent can call to examine the quality of its current solution, such as a model checkpoint or an RL algorithm.In contrast, MLE-Bench requires a CSV file to be submitted for grading each question and SWE-Bench/Agent require evaluating a piece of code through a collection of unit tests.MLAgentBench, RE-Bench and ScienceAgentBench provide Python scripts to compute the evaluation scores.</p>
<p>Finally, MLGym enables easy evaluation of both models and agents.To facilitate model evaluation, MLGym provides a default agentic harness that can be used out-of-the-box to evaluate any base model.</p>
<p>LLM Agents</p>
<p>Research on tool-augmented LLMs (Schick et al., 2023) has inspired a new research agenda of "agentic" LLMs (Kaddour et al., 2023;Wang et al., 2024a), where LLMs interact with an external environment.Existing work explores teaching LLMs to use tools or APIs (Schick et al., 2023;Qin et al., 2023), navigate the web (Nakano et al., 2022;Deng et al., 2023;Zhou et al., 2023), interface with operating systems (Wu et al., 2024), play games (Paglieri et al., 2024;Wang et al., 2023), or interact with other simulated (Wang et al., 2024b;Lin et al., 2023) or physical worlds (Zhang et al., 2024a).Evaluating agentic LLMs typically involves designing controlled environments, providing suitable tools, defining tasks and goals, and establishing quantitative metrics to measure the system's performance.</p>
<p>Building on these directions, Yoran et al. (2024) introduce AssistantBench, emphasizing the complexity of open-web navigation and showcasing how current systems struggle with realistic, time-consuming tasks such as monitoring real-estate markets or identifying nearby businesses.Meanwhile, Kapoor et al. (2024) highlight the importance of standardized evaluation protocols that consider both accuracy and cost, warning against overfitting and advocating for more reproducible benchmarks.Extending these concerns to multi-dimensional environments, Liu et al. (2023) propose AgentBench-a suite of eight interactive settings that test agents' capacity for reasoning, decision-making, and long-term instruction following.Similarly, Mialon et al. (2023) focus on holistic planning skills through GAIA, a benchmark designed to assess performance on real-world questions requiring robust tool-use and multimodal reasoning, revealing substantial gaps between human-level proficiency and current LLMs.Finally, Trivedi et al. (2024) emphasize the necessity of sophisticated tool integration with AppWorld, an interactive environment where agents must operate diverse applications via APIs and generate complex code in an iterative fashion.Collectively, these works underscore not only the breadth of agentic LLM capabilities but also the pressing need for systematic, multifaceted benchmarks that capture complex tasks with verifiable results and foster reproducible progress in the field.However, none of these works focuses on evaluating or developing LLM agents for open-ended AI research tasks.</p>
<p>Agents for Software Engineering and Data Science</p>
<p>In line with the principle of reproducibility and verifiability, software engineering tasks provide a testbed for LLM agents, where tasks can be tightly scoped and outcomes rigorously measured.Recent work has explored how agents can tackle code-level challenges in controlled settings that permit systematic evaluation.As discussed above, Yang et al. (2024) introduce SWE-agent, which operates within a constrained agentcomputer interface to facilitate file creation, repository navigation, and code testing-thereby enhancing both traceability and reproducibility on benchmarks such as SWE-bench and HumanEvalFix.Similarly, Wang et al. (2024c) describe OpenHands, a platform that restricts agent interactions to sandboxed environments for safer command execution and verifiable web browsing, and in doing so provides a standardized foundation for benchmarking.Magentic-One (Fourney et al., 2024) is another agentic system competent in software engineering but also augmented with web navigation capabilities, as demonstrated by its strong performance on the GAIA, AssistantBench and WebArena (Zhou et al., 2023) agentic benchmarks.On the other hand, Zhang et al. (2024b) achieve competitive perforemance on SWE-bench with AutoCodeRover, which, unlike the agentic approaches, solves Github issues by combining LLM-based programming with program representation as an abstract syntax tree.</p>
<p>Towards the goal of automating data science work, Li et al. (2024) introduce AutoKaggle, a multi-agent human-assisting system, and Grosnit et al. (2024) present AgentK v1.0, an end-to-end autonomous data science agent; both of these systems perform well on Kaggle competition data.Still within the realm of data science work, Lei et al. (2024) build Spider 2.0, a challenging benchmark and code agent framework for automating text-to-SQL workflows.Going one step further, Cao et al. (2024) introduce Spider 2-V, an autonomous multimodal agent coupled with a benchmark focusing on the automation of enterprise data science and engineering workflows.</p>
<p>More search-oriented approaches include SWE-Search (Antoniades et al., 2024), a multi-agent framework that marries Monte Carlo Tree Search (MCTS) with iterative refinement, enabling agents to continuously evaluate and improve their approaches to repository-level tasks.In a similar vein, Koh et al. (2024b) explore tree search for LLM agents and show that equipping LLM agents with best-first search boosts performane for the WebArena and VisualWebArena (Koh et al., 2024a) agentic benchmarks.Also on augmenting LLM agents with search, Yu et al. (2025) propose MCTS-based test-time search and self-learning techniques that yield better performance on VisualWebArena.Finally, Xia et al. (2024) demonstrate that even relatively simple approaches can excel when thoroughly monitored: an 'agentless' system follows a three-step process and outperforms more complex agent-based methods on SWE-bench Lite, underscoring the value of constrained, verifiable environments in driving reproducible gains for autonomous SWE agents.</p>
<p>Agents for Scientific Research</p>
<p>Controlled SWE contexts build the foundation for more complex automation while maintaining a reproducible and verifiable approach.However, just the software foundations alone are not sufficient to address the remaining gaps towards the goal of science acceleration.Going from the limited environments and well-defined tasks with metrics towards a less-defined area of open-ended questions, there are substantial efforts needed to boost the capabilities of research agents.For instance, coming up with automatable criteria to gauge scientific novelty or constructing theories inheriting the automated findings from heterogeneous disciplines are examples of areas that could use more refinement and experimentation.</p>
<p>Nevertheless, the first steps on this path can be started now -in the field of ML research and data science -since these areas represent for us a scientific playground with tasks that are both well-defined and have formal criteria of verifiability (benchmarks and tests), falsifiability (ablation studies and tests for data leakage, memorization, out of domain generalization, etc) and reproducibility.</p>
<p>Data Science</p>
<p>Many recent works approach both classic data science tasks and real-life repository-based tasks as a testbed for agents with a known test set and metrics.While based on similar grounds, the works differ in the resulting levels of autonomy of the agents.For instance, ML-Bench (Tang et al., 2024) focuses on explicit tasks within existing GitHub repositories -evaluating agents in code-centric setups without delving into open-ended objectives.By contrast, Data Interpreter (Hong et al., 2024) extends agent testing to broader data science problems, spanning coding tasks, mathematical reasoning, and a limited suite of open-ended applications (e.g., OCR, web search, and mini-game generation), thus reflecting a more flexible approach to autonomy.The agentic benchmark SUPER (Bogin et al., 2024) raises the bar by requiring the agent to formulate the task itself and iterate on NLP-related data and tasks within research repositories, thereby emphasizing self-directed problem-solving.</p>
<p>AI Research</p>
<p>The presence of models and simulations in machine learning itself inevitably leads to the fact that this area also becomes the object of automation.Having an agent formulating a task itself and approaching openended tasks naturally leads to automatic agentic enhancement of the machine learning methods themselves.AutoML (Eggensperger et al., 2019;Lindauer and Hutter, 2020;Tornede et al., 2023) and NAS (Elsken et al., 2019;Nasir et al., 2024) approaches have been previously paving the foundations of ML automation within environments with built-in restrictions (an explicit set of methods, definition of the search space and strategy), while the agentic approach can propose open-ended solutions without said specifications.</p>
<p>For example, MLAgentBench (Huang et al., 2024) consists of an environment for agents to solve 13 complex tasks ranging from improving image classification to language modeling, with the current state-of-the-art LLMs achieving 0% success rate for the most difficult of these tasks.The proposed pipelines for agents in the environment include designing and running experiments, analyzing the results, and iterating towards improving the defined metrics.Similarly, RE-Bench (Research Engineering Benchmark) (METR, 2024) is a set of 7 diverse and challenging ML tasks with the methodological addition of real human experts involvement and progress comparison: timed sessions for ML experts vs LLM agents.Authors state that the best agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment.However, humans currently display better returns to increased time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top agent when both are given 32 total hours.MLE-bench (Chan et al., 2024) focuses on Kaggle tasks as a source for agentic evaluations.Agents are evaluated across well-defined metrics, datasets, and real competition result distribution.The attempts are limited to 24 hours.However, in contrast with MLGym, all these works contain a more narrow set of domains that do not assess algorithmic reasoning capabilities.Moreover, some of them do not provide a standardized agentic harness to allow for model evaluation, but they vary both the harnesses (also known as scaffolds) and the LLMs when comparing performances.While our work focuses on creating an evaluation framework with objective and standardized evaluation metrics, other recent works focus on developing an agentic harness for the more subjective task of generating papers based on end-to-end experimental cycles (Lu et al., 2024).</p>
<p>Scientific Discovery</p>
<p>Several recent works have approached scientific automation with LLM agents targeting the process of scientific discovery.DiscoveryWorld (Jansen et al., 2024) is a benchmark for scientific agents being evaluated in a game-like virtual discovery environment.120 tasks require an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions -for areas like proteomics, chemistry, archeology, physics, agriculture, rocket science, linguistics, or epidemiology.The custom simulation engine only supports a limited list of objects and 14 possible actions.A distinctive feature of the work is also that it focuses on general discovery skills rather than task-specific solution, and the assessment, space of objects and actions is common to all scientific domains.ScienceAgentBench (Chen et al., 2024), however, approaches differently the similar task of creating a discovery-based agentic benchmark: the tasks are based on 44 cherry-picked peer-reviewed publications that include data-driven discovery tasks with well-defined metrics.The scientific areas covered include bioinformatics, computational chemistry, geographical information science, and neuroscience yielding 102 tasks of various types, such as data processing, modeling or visualization.Each task is defined by Python-based evaluation environment, end result metrics and intermediate evaluation criteria.Special metrics control data contamination and agent shortcut issues.Comparing different baselines, including pure LLMs with prompting, authors state that execution feedback is necessary for agents to generate useful solutions.</p>
<p>The idea of execution feedback and iterative improvement for research tasks has been proposed in Re-searchAgent (Baek et al., 2024).Agentic concept-based approach with literature-based discovery shows great improvement for end-to-end iterative solution generation, also supported by knowledge-based vs random facts ablations.The agent is evaluated solely with subjective human preference annotation and automatic human preference evals.While covering structured aspects of end-to-end experimental pipeline (problem clarity, feasibility, significance, relevance, originality, method generalizability, innovativeness, experiment reproducibility, validity, etc), relying solely on human judgment without supporting it with objective metrics is insufficient, as Si et al. (2024) shows.</p>
<p>MLGym</p>
<p>An LLM agent can perform ML research/development by interacting with a shell environment through a sequence of commands.Given a task description, some starter code and access to its action and observation history, the LLM generates appropriate shell commands to accomplish research objectives like generating ideas, processing data, implementing new methods, training and evaluating models, analyzing the results, and reasoning about what experiments to run next.The agent is iteratively prompted to take actions based on the task description and execution feedback from previous commands, allowing it to develop and self-refine the solutions in-context.</p>
<p>The MLGym framework provides a unified framework for evaluating and developing agents and models for AI research tasks.We take inspiration from long existing field of RL and build a Gym (Brockman et al., 2016) environment that can execute shell commands in a local docker machine shell.MLGym provides access to four core components: Agents, Environment, Datasets, and Tasks.MLGym's modular design allows one to easily utilize and extend the library.For example, researchers can easily implement other agentic harnesses to improve performance, they can expand the environment by adding more tools for an agent, add more datasets within a given task (e.g., if the task is image classification they could add ImageNet in addition to Cifar-10), and they can even add more tasks to the MLGym benchmark.Below, we discuss each component in detail.</p>
<p>Agents</p>
<p>The Agent class provided by MLGym acts as a wrapper around a base LLM and provides functionality for integrating various base models, history processors, and cost management.Moreover, unlike other frameworks (Huang et al., 2024;Yang et al., 2024), MLGym separates the agent from the environment, allowing for easy integration of external agents.This also enables one to fairly compare different base models given the same agentic harness without the need of implementing their own agentic orchestration.</p>
<p>The agent is expected to take the history of all prior observations and actions as input and return the next action to take.The provided action is then passed to the environment, which executes the command and returns the next observation based on the command output.The agent can execute any bash command in the environment.In addition, it has access to a set of tools (i.e., bash scripts such as editing a file) that it can use similarly to any other bash command.MLGym provides an agent adapted from SWE-Agent (Yang et al., 2024) as a default agentic harness.We describe the design and configuration of the tools in Section 3.5.The full system prompt used can be found in Listing 1.</p>
<p>Environment</p>
<p>MLGym environments are designed as Gymnasium (gym) environments (Towers et al., 2024).The environment component is responsible for initializing a shell environment in a local docker machine, with all the required tools, installing task-specific python dependencies, copying all the necessary data and code in a separate agent workspace and managing interactions between the LLM agent and the system.Moreover, to support open-ended research tasks and make the environment safe and flexible, MLGym environment also manages permissions for various files and directories.Specifically, when running in a docker container, due to various security concerns associated with using a root user, we create a non-root user named "agent" and set the appropriate permissions for the working directory.</p>
<p>In this work, we make a conscious decision to decouple tools and ACI as defined in SWE-Agent (Yang et al., 2024) 1 .Note that this ensures that the agent and environment are not tightly coupled, allowing for easier implementation of other agentic architectures.Practically, this means that when the environment is initialized, it also initializes the tools in the working environment and a tool documentation is prepared which can be added to the LLM agent's prompt.More details about the tools are provided in Section 3.5.</p>
<p>Datasets</p>
<p>MLGym provides a simple abstraction for defining datasets through configuration files.It supports both locally stored and Hugging Face datasets.We decouple the dataset definition from the task definition, so that a single dataset can be used in multiple tasks.Similarly, a single task can have more than one dataset so that the agent's code can be evaluated across all of them to demonstrate the generality of the implemented method.</p>
<p>Moreover, if the dataset files are stored locally, the environment automatically copies the relevant files to the agent workspace with read-only permissions.This ensures that the agent cannot change the dataset files, which is important for reproducibility and cheating prevention.</p>
<p>If the dataset is stored in Hugging Face, the agent is given the dataset URL through the starter code or in the prompt and asked to utilize it.Note that if the LLM agent fails to follow instructions or uses a different dataset, the evaluation code will not work or result in performance issues.</p>
<p>Tasks</p>
<p>We provide an easy abstraction to define any ML research task using configuration files.Each task can incorporate one or more datasets, custom evaluation scripts (with read-only access), task-specific conda environment, optional starter code, training timeouts, and memory management settings.This provides a flexible framework for defining diverse open-ended ML research tasks covering a wide range of difficulty.For example, one can define an easier version of a task by providing a baseline code and a harder version by providing no starter code or one with bugs, thus creating a natural curriculum.</p>
<p>Evaluation is a critical component for any ML task.Every task requires a different evaluation protocol; thus, Kaggle-style evaluation as done in MLE-Bench (Chan et al., 2024) where the agent is expected to submit a CSV file is not feasible for every problem.For example, in reinforcement learning settings, the evaluation artifact is a set of models trained on a set of pre-defined random seeds, which is then used to get a mean reward across a set of environment seeds.Similarly for Game Theoretic tasks, it can be a Python file with a strategy function which will be evaluated against a fixed set of strategy functions.Since we aim to evaluate the agent on open-ended and diverse tasks, it is not possible to convert all submissions to a CSV format.To ensure extensibility to such open-ended tasks, the task definition is expected to provide an evaluation script and submission artifact instructions.The LLM agent can then be prompted to follow the submission instructions and write the appropriate code.Moreover, the evaluation script is read-only for the LM agent, so while it can inspect the evaluation format, it cannot modify the script to change the evaluation logic.All our design decisions for the Agent, Environment, Dataset, and Tasks are meant to reduce overhead on the developers' and researchers' side and enhance reproducibility in this newly emerging area.</p>
<p>Tools and ACI</p>
<p>Augmenting LLM agents with the ability of using external tools is a critical component for making progress on knowledge-intensive tasks.In this work, we extend the ACI (agent-computer interface) first introduced in SWE-Agent (Yang et al., 2024) with some additional features required for an ML research agent.Specifically, we extend the commands for search, navigation, file viewer, file editor and context management with our permission management system and introduce new commands for literature search and a memory module.For example, if the agent tries to open a file without read permission, the file viewer tool will generate textual feedback for the agent.Similarly, if agent tries to edit the evaluation script (which is marked as read-only), the edit tools will output a feedback string instead of failing silently.Literature search and the ability to maintain a experimental log in it's memory are crucial for the agent to surpass SOTA solutions on open-ended research tasks.</p>
<p>Similar to SWE-Agent, tools are defined as bash or python scripts and are made available as bash commands in the environment.</p>
<p>All tool documentation is provided to the agent in the system prompt.See Table 2</p>
<p>Validation and Submit</p>
<p>We provide two commands to the agent to validate the submission and submit the results.Both the validate and submit commands are used to run the evaluation script and give the agent feedback on its current score on the test set.However, while the submit command is a terminal action, i.e., the agent's trajectory is terminated, and the evaluation script is executed to log the final scores, the validate command can be used as many times as needed during the run to get the current performance on the test set.</p>
<p>Addition of a validation command helps the agent to continuously improve its performance on the test set.</p>
<p>Literature Search and PDF Parser</p>
<p>We provide the agent with two tools to find and extract knowledge from external sources.The Literature Search tool allows the agent to query the Semantic Scholar API to find research papers about a given query that have open-access PDFs available, and the PDF Parsing tool allows the agent to download PDFs and convert them into a text-based representation.The paper contents can be stored in the context window as well as the Memory Module for longer-term tasks.Combined, these two tools allow the agent to find and analyze research papers as part of its workflow.See Table 2 for more information about these tools and how they are called.</p>
<p>Memory Module -Research Logs</p>
<p>We introduce the Memory Module for MLGym, an important tool to improve the performance of agents on long-horizon AI research tasks.The Memory Module enables the agent to persistently store critical findings and successful training configurations using a structured memory system, overcoming the challenge of limited context retention in long tasks.During our experiments, we observed that when the agent has access to the memory module, it can retrieve the best training configuration from memory and continue to iterate on it (see Figure 11 and Figure 12).Without the memory module, the agent's trajectory can become longer than the model's context length, thus not being able to retrieve the best configuration, effectively forgetting older experiments and only being able to locally iterate on recent configurations.</p>
<p>The module is equipped with two core functions: memory_write and memory_read.The memory_write function allows the agent to store key insights and effective configurations by saving text data along with its corresponding embeddings and tags in JSON format.In contrast, the memory_read method retrieves the top-k most relevant stored entries based on cosine similarity with a given query, allowing the agent to review past knowledge and iterate from previously successful configurations.</p>
<p>Empirical results demonstrate the positive impact of the Memory Module on long-horizon tasks.Agents equipped with the Memory Module were able to sustain progress over extended sequences of trials, reusing optimal configurations and findings to achieve superior results compared to agents limited by fixed context windows.To further enhance its capabilities, we added the state of the memory to the system prompt (memory tags and number of records) so that the agent is aware of the type of data stored.Tags from a memory record are extracted by identifying the 3-gram most closely matching to the memory record.</p>
<p>This module significantly reduces the limitations of constrained context length, allowing agents to operate effectively in long experimental settings.However, it is an early version and there are many ways to improve the module.For example, one possible direction would be to introduce a more structured memory format, such as hierarchical or relational models, allowing for precise storage and retrieval of information and enabling more complex reasoning over stored knowledge.Another is to incorporate memory operations directly into the model's training or fine-tuning process to allow the agent to natively utilize stored knowledge for improved performance.Or using a sub-agent that will automatically manage the memory by selecting important insights, removing unnecessary entries, and updating the memory.Each of these directions would require extensive experimentation and rigorous testing to ensure robustness and scalability.</p>
<p>For all the experiments presented in this paper, the agent only uses the SWE-Agent tools and validation command.</p>
<p>MLGym-Bench</p>
<p>The primary motivation behind our benchmark is to challenge models across different aspects of machine learning, including data handling, model architecture, and strategic decision-making.By incorporating tasks from data science, game theory, computer vision, natural language processing, and reinforcement learning, the benchmark aims to provide a varied and comprehensive agent evaluation testbed.</p>
<p>The tasks included in the benchmark are carefully selected to represent real-world challenges, ensuring that models are tested on their ability to generalize and perform effectively across various scenarios.Each task is accompanied by standardized evaluation scripts and baseline implementations, providing a clear reference point for performance assessment and comparison.</p>
<p>The benchmark suite is structured into four main categories, each focusing on a specific domain of machine learning: Data Science, Game Theory, Computer Vision, Natural Language Processing, and Reinforcement Learning.Below we describe each of the tasks in the benchmark.</p>
<p>Data Science</p>
<p>House Price Prediction (Kaggle, 2016) In the House Price Prediction task, the goal is to predict housing prices using the Kaggle House Price dataset.This task evaluates models based on their ability to accurately predict prices from various features, using RMSE and R2 as performance metrics.The baseline for this task is a simple Ridge Regression model with minimal feature engineering.</p>
<p>3-SAT</p>
<p>3-SAT (Cook, 1971) In the 3-SAT task, the LLM agent is given a DPLL code and is prompted to optimize the variable selection heuristic.The associated DPLL code is stored in a read-only file, and the agent can inspect it to structure its heuristic function code, however, it cannot modify it.A simple random selection heuristic is used as a baseline and starter code for the LLM agent.The performance is measured by the total wall-clock time taken to solve a set of 100 generated 3-SAT instances.The instances are genereted using the algorithm described in Selsam et al. (2018).</p>
<p>Game Theory</p>
<p>We consider several tasks related to making strategic choices in iterated games, considering multiple well-known games.Specifically, we consider the task of producing code for a strategy for playing in a repeated two-player game.In each such task we provide an opponent strategy, in the form of an opponent bot for playing the game, and ask the agent to produce code for a strategy for best-responding to this opponent, i.e. provide code for a strategy that maximizes the score against that opponent.We very briefly review game theory terminology, with various textbooks covering this topic in more detail (Fudenberg and Tirole, 1991).</p>
<p>In a two-player normal form game G, players select actions simultaneously, with the outcome determined by the choices of both players.Let A 1 = {a 1 1 , . . ., a 1 k } be the (pure) strategies available to player 1 and let A 2 = {a 2 1 , . . ., a 2 m } be the strategies available to player 2. Denote the set of strategy profiles, consisting of a strategy choice for both players as A = A 1 × A 2 .The utility of the players depends on the actions selected by both for them, i.e. the payoffs are u : A → R n , where u(a) = (u 1 (a), u 2 (a)) for a ∈ A, and where each player i tries to maximize their individual utility u i .A mixed strategy is a probability distribution ∆ over pure strategies.Given a mixed strategy profile σ = (σ 1 , σ 2 ) the expected utility of u i of player i is
u i (σ 1 , σ 2 ) = (a1,a2)∈A σ 1 (a 1 )σ 2 (a 2 )u i (a 1 , a 2 ).
A repeated game consists of k rounds in which the players play the same underlying normal form game.The history at the j + 1'th round consists of the actions (pure strategies) chosen by both players in each of the rounds 1 to j.We denote by H the set of all possible such histories, so a strategy in a repeated game is a function a i : H → ∆(A), i.e. a function that takes the history of actions chosen in the previous round and provides a distribution over the actions the agents would take in the next round.In our tasks, a strategy in the repeated game is expressed as a piece of code that takes in the history (actions of both players in the previous rounds), and outputs an action for the next round (where the code may make some random choices, hence yielding a distribution over the selected next round actions).Given an opponent strategy a 2 , the goal of our agent is to produce a strategy that best responds to the opponent and produces a the maximal payoff, i.e arg max a1 u 1 (a 1 , a 2 ).Note that in this equation a 2 is a given opponent strategy expressed as a piece of code that takes the history over the previous rounds and selects an action for the next round (possibly making some random choices), and that the goal of an agent is to produce a 1 as a piece of code capturing the strategy of the first player.The agent optimization goal is selecting the code a 1 so as to maximize player 1's expected payoff u 1 against the fixed opponent a 2 .</p>
<p>We consider the repeated version of prominent games, which we briefly discuss here: iterated Prisoner's Dilemma (Flood, 1958;Fudenberg and Tirole, 1991;Axelrod, 1980), Battle of the Sexes (Cooper et al., 1989;Luce and Raiffa, 2012) and Colonel Blotto (Roberson, 2006).As our goals was to highlight how our agent framework could be used to solve game theoretic tasks, rather than providing a rigorous evaluation and analysis of many game theoretic environments, we only included few games.However, additional games could easily be added in.</p>
<p>Prisonner's Dilemma (Axelrod, 1980).In this game, two players each have two options: cooperate or defect.When both cooperate, they receive a moderate reward.If one defects while the other cooperates, the defector gets a high reward while the cooperator gets a low payoff.If both defect, they both receive a low payoff.Due to the structure of payoffs, although mutual cooperation yields the best collective outcome, individual incentives often push towards defection.We included a repeated game, consisting of k = 20 rounds of the game.In the repeated version, players remember previous interactions and can adjust their strategies based on the history consisting of the past outcomes.Repeating the stage game multiple times allows for the development of trust and cooperation, as players recognize that consistent cooperation can lead to better long-term benefits than short-term defection (Axelrod, 1980).As our opponent strategy we provided a simple model which randomizes between cooperation, defection, or actions chosen based only on the last round of the interaction.(Cooper et al., 1989).This is a simple game illustrating coordination challenges between two participants with different preferences.In the game, two participants have to agree on a venue (for instance where to go to spend an evening).There are two possible venues, and both players would rather make the same choice rather than making different choices.The strategic dilemma arises because as each player wants to coordinate their choice with the other, but they have a different ranking over the venues (one prefers the first venue and the other prefers the second venue).Similarly to the iterated Prisoner's Dilemma, we have used a repeated game with k = 20 rounds and used a simple opponent that makes random choices using the information from the last round.</p>
<p>Battle of Sexes</p>
<p>Colonel Blotto Game (Roberson, 2006).This game is a model of strategic allocation of limited resources under competition.Two players ("Colonels") must simultaneously distribute their resources (such as troops) over several alternative locations ("battlefields").The player who allocates more resources to a battlefield wins that battlefield.The overall winner is the player who wins the most battlefields.The key challenge arises from the fact that players must make their allocations without knowing how their opponent will distribute their resources.This yields an environment where players try and anticipate their opponent's moves to decide how to best allocate their own resources in order to maximize their chances of winning.A key insight from the game is the importance of diversification and unpredictability: it is harder to exploit an opponent who spreads resources across multiple battlefields and varies their strategy.Our target opponent used a very simple random allocation rule (re-normalizing to the overall budget of resources).</p>
<p>It is important to note that in all the game theoretic tasks, the agent is allowed to look at the opponent's strategy, and thus these tasks measure code understanding and the LLM's capabilities to exploit the opponent's strategy.In the future, we plan to add tasks where the opponent's strategy is not provided to the agent, and agent is pitted against multiple opponents in a round robin fashion, similar to the setup used in Axelrod's original Prisoner's Dilemma tournament.</p>
<p>Computer Vision</p>
<p>Image Classification (CIFAR-10) (Krizhevsky et al., 2009) The Image Classification CIFAR-10 task involves classifying images into one of ten classes using the CIFAR-10 dataset.This task tests the ability of models to learn visual patterns and features, with a baseline accuracy of 49.71% encouraging improvements Image Classification (Fashion MNIST) (Xiao et al., 2017) The Image Classification Fashion MNIST task involves classifying fashion items into predefined categories using the Fashion MNIST dataset.The agent is provided with a simple two layer CNN as a baseline and it has to optimize for the accuracy on the test set.</p>
<p>The agent can optimize the model architecture and the hyper-parameters for the training.(Lin et al., 2014) For the image captioning task, the agent has to write the modeling code and come up with a good architecture and training setup for the image-text pairs in the MS-COCO dataset.We provide a baseline code for training to the agent which uses an image encoder and text decoder.We use the MS-COCO training and validation sets after removing all images containing humans.The agent has to optimize for the BLEU scores (Papineni et al., 2002) computed over the model-generated captions and ground truth captions for a given image.</p>
<p>Image captioning (MS-COCO)</p>
<p>Natural Language Processing</p>
<p>For language, we test the agent's ability to understand and modify training setup for both Natural Language Understanding (NLU) and Natural Language Generation (NLG) as detailed below.</p>
<p>Natural Language Inference (Williams et al., 2018) In this task, the agent starts from a pre-trained BERT model (Devlin, 2018) and we provide the baseline code to fine-tune on the training set of the MNLI benchmark to the agent.The agent is expected to come up with good hyper-parameters and fine-tuning strategy to optimize the test set accuracy on MNLI.</p>
<p>Language Modeling (Jordan et al., 2024) In the Language Modeling task, the agent is expected to train a language model for next token prediction using a smaller version of the FineWeb (Penedo et al., 2024) dataset.The LLM Agent is provided with the dataset and the NanoGPT (Jordan et al., 2024) codebase as a baseline and starting point.We use version #8 from modded-nanogpt3 as the starting point.The training and validation sets contain 1.773B and 100M tokens, respectively.The perfomance metric is the perplexity of the trained model on the validation set.</p>
<p>Reinforcement Learning</p>
<p>MetaMaze Navigation (Miconi et al., 2020) The MetaMaze Navigation task simulates a grid-world environment where agents must navigate using local observations and reach the goal location.(Brockman et al., 2016) We use the continuous version of the Mountain Car environment introduced in Brockman et al. (2016), where the task is to learn a policy that drives a car up a steep hill in a continuous control environment.</p>
<p>Mountain Car Continuous</p>
<p>Breakout MinAtar (Young and Tian, 2019) The Breakout MinAtar task involves playing the arcade game Breakout in a simulated environment.This environment was introduced in Young and Tian (2019) and is a popular benchmark for evaluating reinforcement learning agents.</p>
<p>For all the RL tasks, we use the environments from the Gymnax library (Lange, 2022) and the PPO algorithm from Gymnax-blines4 as a baseline and starting code for the LLM agent.</p>
<p>Experimental Setup</p>
<p>Agent and Models</p>
<p>For our experiments, we utilize a SWE-Agent based model adapted specifically for the MLGYM environment.SWE-Agent follows a simple ReAct-style thought and action loop (Yao et al., 2023), where the agent is prompted with the ACI documentation, the task and dataset description, as well as lightweight generic instructions to act as a ML researcher.The agent is configured to use a single command per step, and is not allowed to use any interactive session commands (e.g., python REPL, vim).</p>
<p>We use a set of 5 state-of-the-art models for our experiments, OpenAI O1-preview, Gemini 1.5 Pro, Claude-3.5-sonnet-20241022(refered to as Claude-3.5-sonnet in the paper), Llama-3-405b-instruct, and GPT-4o.All the models are used with temperature=0.0 and top-p=0.95,with the exception for OpenAI O1-preview, which doesn't support changing the decoding parameters and has a default temperature=1.0.</p>
<p>Environment Configuration</p>
<p>The MLGYM environment is configured with several key parameters to facilitate effective interaction between the agent and the tasks:</p>
<p>• Window Configuration: The environment uses a window size of 1000 lines with an overlap of 2 lines, allowing the agent to effectively navigate and edit large files while maintaining context.</p>
<p>• Context Management: A processor maintains a rolling window with the five most recent interactions (action and observation), helping the agent maintain context about the most recent interactions while keeping the input size manageable.</p>
<p>• Command Interface: The environment provides a set of specialized commands beyond standard bash operations, including file navigation commands (goto, scroll_up, scroll_down), file editing commands (edit, insert) with linting support, file and directory search commands (search_file, search_dir, find_file), and evaluation commands (validate, submit).</p>
<p>A single agent run is limited to 50 steps (i.e.interactions with the environment), after which the agent is terminated and the last codebase state is autosubmitted.Moreover, to control the runtime of the agent and prevent it from simply increasing the number of parameters in the model, we set a task specific timeout for the training commands.</p>
<p>In the next section, we discuss the evaluation metrics used in our experiments.</p>
<p>Evaluation</p>
<p>In order to compare agents on MLGym, we aggregate the scores of each method-an agent architecture paired with a backbone model-across our tasks.There are many ways one can aggregate scores.Common options include computing the average score across tasks for each method or by computing the average ranking of each method across tasks.While simple, these approaches can weight metrics in undesirable ways and disproportionately penalize certain methods.Averaging across different metrics may unfairly weight the metrics differently based on their relative scales, and averaging ranks can disproportionately penalize methods that effectively solve a task but are tied with other methods that also solve the task.Rather than naive averaging of scores or rankings, we employ performance profile curves (Dolan and Moré, 2002), which allow us to compare relative performance gains across both methods and tasks.Performance profiles were originally developed to compare optimization techniques across a set of optimization problems.Since then, they have been used by the AutoML community to compare AutoML methods across diverse domains, each with their own domain-specific metrics (Tu et al., 2022;Roberts et al., 2022b).</p>
<p>One challenge when using performance profiles is that they produce a curve for each method (where a higher curve is better), rather than a direct ranking of methods.To address this, the AutoML Decathlon (Roberts et al., 2022a) competition introduced the AUP score, which computes the area under the performance profile curve for each method, where a higher value constitutes better performance.Variants of the AUP score have since been used to score the AutoML Cup5 and MLCommons AlgoPerf (Dahl et al., 2023) competitions.Next, we define performance profiles, the AUP score, and the details of their usage within MLGym.</p>
<p>Performance Profiles and the AUP Score</p>
<p>For a given method m, its performance profile curve is defined as
ρ m (τ ) = 1 |T | |{t ∈ T : log 10 r t,m ≤ τ }| r t,m = ℓ t,m min{ℓ t,m : m ∈ M } (1)
where M is the set of all methods, P is the set of tasks, ℓ t,m is the performance metric for a method m on task t, and r t,m is a quantity called the performance ratio.</p>
<p>Importantly, this definition assumes that the performance metric for each task, ℓ p,• , must be defined such that lower scores are better-we discuss our modification to this definition to support other scores in Section 6.2.</p>
<p>Performance profiles are parameterized by a threshold, τ , on the distance between the method m and the best scoring methods on each of the tasks.At a given threshold τ , performance profiles compute the proportion of tasks for which the method m is within τ of the best method for each task.</p>
<p>In order to derive a final score for each method m ∈ M , we compute the AUP score as
AUP m = τmax 1 ρ m (τ )dτ, (2)
where τ max is the minimum τ for which ρ m (τ ) = 1 for all m ∈ M .</p>
<p>Usage in MLGym</p>
<p>In the context of MLGym, a method is defined as a combination of an agent scaffolding and a backbone model.Since, in this work we use a single agent scaffolding (SWE-Agent), we are comparing the performance of different backbone models.Moreover, we adapt performance profiles and AUP scores to handle various edge cases introduced by our MLGym tasks.</p>
<p>• Metric Direction Handling.For metrics where higher values are better (e.g., accuracy, R2), we invert the performance ratio calculation and use the maximum score instead of the minimum:
r t,m = max{ℓ t,m : m ∈ M } ℓ t,m .
(3)</p>
<p>• Infeasible Method In order to be counted as a feasible method, an agent should produce at least one valid solution and beat the baseline, methods must outperform the baseline.Methods that don't produce any valid solution or underperform are marked as Infeasible.The score of an infeasible method is set to (1 + ε) × r t,m baseline , where r t,m baseline is the score obtained by the baseline method on task t.We set the value of ε = 0.05.</p>
<p>We report the metrics across 4 independent runs for each model on each task.Finally, since the LM agent can use the validate command to check the performance without ending the run, we maintain two separate sets of performance profiles and AUP scores for each model.</p>
<ol>
<li>Best Submission Profiles, ρ bs m (τ )@4, are computed using the best final submission across 4 runs.A submission is classified as a final submission in two cases: if the agent uses the submit command, or if the agent terminates without submitting and the last codebase state is used to evaluate the performance.</li>
</ol>
<p>Best Attempt Profiles, ρ ba</p>
<p>m (τ )@4, which are computed using the best attempt observed across 4 runs.Any valid call to the validate command is considered an attempt.</p>
<p>The resulting AUP scores provide complementary information:</p>
<p>• AUP bs m @4 indicates the model's ability to consistently submit its best attempt as the final solution.Note that to do this, the LM agent has to be able to keep an internal state of the best attempt and recover from any mistakes made after the best attempt was made.</p>
<p>• AUP ba m @4 captures the model's exploration capability and is an indicator of the ceiling of the model's performance.</p>
<p>Apart from the AUP scores and performance profiles, we also report the raw performance scores for each model on each task.Similar to performance profiles, we categorize the raw scores in two sets: Best Submission@4 and Best Attempt@4.</p>
<p>Results</p>
<p>AUP Scores and Performance Profiles</p>
<p>As detailed in the Section 6, we evaluate the performance of each model in the SWE-Agent based agent scaffolding using Performance Profiles and Area Under the Performance Profile (AUP) score.</p>
<p>P(ratio )</p>
<p>Best Attempt Profile@4 0.0 0.3 0.6 0.9 1.2</p>
<p>Best Submission Profile@4 Llama GPT-4o Claude Gemini O1-preview</p>
<p>Figure 2 Performance profiles comparing Best Attempt@4 and Best Submission@4 across all models and tasks.The x-axis shows the performance ratio threshold τ and the y-axis shows the fraction of tasks where a model achieves performance within τ of the best model.</p>
<p>Moreover, since our agent can log the performance of intermediate steps, we categorize the performance of each model using two categories: Best Submission and Best Attempt.Best Submission indicates the LLM agent's capability to produce a valid final solution for a task as well as the ability to remember to fall back to the best intermediate solution in case some experiments don't pan out.Whereas, Best Attempt indicates the potential ceiling of the LLM agent's capability to solve the given task.</p>
<p>Figure 2 shows the performance profiles for Best Attempt (on the left) and Best Submission (on the right).</p>
<p>Similarly, Table 4 shows the AUP scores for the Best Attempt and Best Submission for all models.</p>
<p>In our experiments, we found that OpenAI O1-preview is the best-performing model on aggregate across our set of tasks for both Best Attempt and Best Submission, with Gemini 1.5 Pro and Claude-3.5-Sonnetbeing close behind.</p>
<p>Model</p>
<p>Raw Performance Scores</p>
<p>To compare the performance of each model on each task, we also report aggregate metrics over 4 runs with different seeds, namely the Best Attempt@4 and Best Submission@4 in Table 5 and Table 6 respectively.</p>
<p>While OpenAI O1-Preview is not dominant in all tasks, with Gemini-1.5-Pro,Claude-3.5-Sonnet,and Llama-3.1-405b-Instructoccasionally taking the lead, it is consistently in the top performing models for most tasks and thus takes the top spot in the AUP scores and performance profiles.This shows that the performance profile is a good metric to compare the performance of different models on a set of tasks with a diverse set of metrics.</p>
<p>We also find that Llama-3.1-405b-Instruct and GPT-4o are the only models that fail to produce any valid solution for the Language Modeling and Breakout tasks, respectively.Table 6 Best Submission@4 scores for all models.Best scores are highlighted in blue .Note: ∞ indicates that the model was not able to produce even a single valid solution for submission or validation.</p>
<p>Computational Cost</p>
<p>As discussed in Kapoor et al. (2024), it is important to also consider the pareto curve of performance vs cost for a more comprehensive evaluation of the agents' capabilities and their computational cost.In this work, we do not compare different agent scaffoldings; however, the pareto curve can still be useful to choose the most balanced model for a set of tasks.Figure 3 shows the Best Attempt AUP@4 vs Average Cost for all models.We use Best Attempt AUP scores to for this plot to highlight the maximum performance achievable by each model for a given cost.</p>
<p>According to results discussed in Section 7.1, OpenAI O1-Preview is the best-performing model, however, it is also the most computationally expensive by a wide margin.In contrast, Gemini-1.5-Proand Claude-3.5-Sonnetare much more cost-effective while still reaching high performance not too far from OpenAI O1's, with Gemini-1.5-Probeing the most cost-effective.</p>
<p>Gemini-1.5-Pro is cheaper than both GPT-4o and Llama-3.1-405b-Instruct and provides massive performance gains relative to them.GPT-4o is one of the cheapest models to run but performs significantly worse than the top models, Claude-3.5-Sonnet,Gemini-1.5-Pro,or OpenAI O1-Preview.Overall, Gemini-1.5-Prostrikes the best balance between performance and cost on MLGym-Bench, being the cheapest model to run (approximately 9× cheaper than OpenAI's O1) while achieving 99% of OpenAI O1's AUP (which is the top performing model).</p>
<p>The API pricing for OpenAI O1-preview, GPT-4o, Claude-3.5-Sonnet,and Gemini-1.5-Prowas taken from their respective price pages and for Llama-3.1-405b-instruct was taken from together.ai.For details on API pricing, tokens spent, and context length please consult Table 8 7.4 Agent Behavior Analysis</p>
<p>Failure Mode Analysis</p>
<p>In this section we analyze the failure modes of our agents on MLGym-Bench tasks, using three key perspectives: termination error distribution, failed or incomplete run rates, and task-specific failure patterns.We collect trajectories across 11 tasks and 5 models with 4 different seeds.This results in a total of 220 trajectories with 20 and 44 trajectories for each task and model, respectively.</p>
<p>Termination Errors Figure 4 shows the distribution of different causes for termination encountered by each model during task execution, as indicated by the first word of the error message.We categorize the errors into the following types: context length exceeded, evaluation error, file permission error, cost limit exceeded, format error, and runtime error.</p>
<p>First, we observe that almost all models encounter Evaluation Error and is generally the most frequent final error, accounting for 75% of all termination errors.Evaluation Error is generally triggered by missing submission artefacts or incorrect submission format at the last step or when the submit command is issued.Gemini-1.5-Pro is the only model that does not submit any invalid solutions, with OpenAI O1-Preview and Claude-3.5-Sonnetbeing the runner ups.</p>
<p>OpenAI O1-Preview and Claude-3.5-Sonnetdemonstrate superior error handling capabilities with the lowest overall error rates.Cost Limit is the second most frequent error encountered by Claude-3.5-Sonnet,Gemini-1.5-Proand OpenAI O1-Preview, indicating that they could further improve performance if provided with more budget.However, it is interesting to note that Gemini-1.5-Pro is the most cost-effective model across all tasks but still encounters Cost Limit error most frequently among all models.</p>
<p>Failed and Incomplete Runs</p>
<p>The failed and incomplete run analysis in Figure 5 reveals significant variations in model reliability.If an agent run fails with a termination error without producing any valid intermediate submission, we mark it as failed.Whereas, if the run fails with a termination error but produces a valid intermediate submission i.e. at least one score on the test set is obtained, we mark it as incomplete.Note that the model's submission does not have to beat the baseline to be considered a valid intermediate submission.</p>
<p>We are not interested in the performance of the model's submission here, but rather the ability of the agent to produce a valid submission by following the given instructions.</p>
<p>GPT-4o exhibits the highest failure rate, while Gemini-1.5-Proand OpenAI O1-Preview achieve the best completion rates.While Claude-3.5-Sonnet is one of the top performing models across all tasks (Section 7.1), it has a high failure rate.Another interesting observation is that OpenAI O1-Preview has a high incompletion rate, but it always produces at least one valid solution for all tasks.</p>
<p>We report additional results and failure mode analysis in Section A.2.</p>
<p>Action Analysis</p>
<p>In this section, we analyze the overall action distribution, as well as across models and trajectory steps.To analyze the action distribution effectively, we group the actions according to categories defined in</p>
<p>Per-Model Action Distribution</p>
<p>Figure 7 shows the action distribution for each model.GPT-4o takes the least number of actions overall, indicating that the model either errors out or submits too early without reaching an optimal solution.This is consistent with the failure analysis shown in Figure 5.</p>
<p>Among the best-performing models, Claude-3.5-Sonnetand OpenAI O1-Preview perform the most number of actions within a run, while Gemini-1.5-Properforms the least number of actions.Consistent with the cost analysis discussed in Section 7.3, Gemini-1.5-Pro'slower trajectory length contributes to it being the most cost-effective model.</p>
<p>Per-Step Action Distribution</p>
<p>Figure 8 illustrates the distribution of actions taken by agents across trajectory steps.Initially, Bash commands are predominant, indicating that agents start by checking and setting up their environment with basic commands such as ls, pwd, cd etc.As the steps progress, Edit actions become the most frequent, reflecting the agents' focus on modifying and refining code.This is complemented by a consistent use of View commands, suggesting a pattern of iterative development where agents frequently review their changes.</p>
<p>Python and Validate commands are used steadily throughout, which indicates an iterative cycle of experiments and evaluation.Submit actions are sparse, typically appearing towards the end of the process, aligning with the finalization of tasks.However, we can observe the Submit action being used as soon as Step 5, which indicates that some models submit their solution too early and likely fail to reach an optimal solution to beat other models.</p>
<p>Interestingly, Search commands are rarely used, suggesting that agents might benefit from improved search strategies to enhance efficiency while editing code.</p>
<p>Overall, our analysis highlights a structured approach where agents begin with getting familiar with the environment and the task, conduct multiple iterations of experiments and validation, and conclude with and submission.We report additional action analysis in Section A.3.</p>
<p>Discussion and Limitations</p>
<p>Our findings highlight both the opportunities and ongoing challenges in leveraging large language models (LLMs) as agents for scientific workflows.The proposed MLGym framework and accompanying MLGym-Bench tasks demonstrate that modern LLM agents can successfully tackle a diverse array of quantitative experiments, reflecting advanced skills and domain adaptability.At the same time, our results reveal notable capability gaps, which point to several avenues for improvement:</p>
<p>• Scaling beyond ML tasks To further evaluate the agent's AI Research capabilities, it is essential to scale up the evaluation framework to accommodate large-scale domain-specific datasets, more complex tasks, as well as domains outside AI.This will enable the community to assess the robustness and generalizability of different methods, as well as identify potential limitations and areas for improvement.</p>
<p>• Interdisciplinary Ablations and Generalization Within the stage of method evaluation, one approach is to test the solutions for generalization:</p>
<p>automatically evaluating the applicability of a new method on different domains .For example, new LLM architectures like Mamba (Gu and Dao, 2024) could be automatically applied to data on DNA, chemical molecules, music generation, etc.</p>
<p>automatically running interdisciplinary and multidisciplinary ablations, where we systematically remove or modify specific components of the proposed ML system to assess their impact on performance.This will enable the community to more quickly identify the most critical factors contributing to generalization across different domains.</p>
<p>• Addressing Scientific Novelty While the agentic benchmarks have demonstrated their effectiveness in evaluating complex tasks in different areas, it is essential to acknowledge that proposed interdisciplinary extrapolation of methods is just one aspect of the broader scientific understanding of "novelty" and "discovery" (Popper, 2005;Langley, 1987).It is not yet clear if the notion of scientific novelty can be successfully automated or even formally defined in a form suitable for agents.For many scientific disciplines, development may be uneven and depend on the availability of open data, the development of the methods, metrics and definitions used.</p>
<p>• Data Openness Imperative Finally, we emphasize the importance of data openness in driving scientific progress.By making our representative 'corpus of the world' widely accessible, including scientific artifacts, reproducible code, and domain-specific data for modeling, we can facilitate collaboration and accelerate discovery.This imperative is crucial for advancing our understanding of complex systems and developing more effective solutions to real-world problems.Removing once accessible resources that have entered LLM training from public access can have an irreparable impact on the acceleration of scientific progress, as it becomes impossible to identify sources of facts, and it is impossible to attribute the out-of-distribution result from a scientific work from a hallucination or a completely new result.</p>
<p>Ethical Considerations</p>
<p>Conclusions</p>
<p>This paper presents MLGym and MLGym-Bench as initial steps toward building robust, flexible, and transparent LLM agents for AI research.As the field continues to evolve, improvements in long-context reasoning, better agent architectures, training and inference algorithms, as well as richer evaluation methodologies will be essential to fully harness LLMs' potential for scientific discovery, in general and for AI research in particular.By fostering collaboration among researchers in machine learning, scientific computing, and diverse application domains, we can move closer to a future where AI-driven agents meaningfully accelerate scientific research, all while maintaining verifiability, reproducibility, and integrity in scientific discovery.</p>
<p>Appendix</p>
<p>A Additional Results and Analysis</p>
<p>A.1 Computational Cost   These failure patterns align with the raw performance scores in Table 5 and Table 6, where we observe that tasks requiring complex architectural decisions (Language Modeling) or complex algorithms (Breakout, Meta Maze and Mountain Car Continuous).Traditional supervised learning tasks are handled more reliably across models, while the more advanced models demonstrate better error handling and completion rates overall.The replacement text is terminated by a line with only end_of_edit on it .All of the &lt; replacement text &gt; will be entered , so make sure your indentation is formatted properly .Python files will be checked for syntax errors after the edit .If the system detects a syntax error , the edit will not be executed .Simply try to edit the file again , but make sure to read the error message and modify the edit command you issue accordingly .Issuing the same command a second time will just lead to the same error message again .signature : edit &lt; start_line &gt;: &lt; end_line &gt; &lt; replacement_text &gt; end_of_edit arguments :</p>
<p>-start_line ( integer ) [ required ]: the line number to start the edit at -end_line ( integer ) [ required ]: the line number to end the edit at ( inclusive ) -r e p l a c e m en t _ t e x t ( string ) [ required ]: the text to replace the current selection with insert : docstring : inserts the given text after the specified line number in the open file .The text to insert is terminated by a line with only end_of_insert on it .All of the &lt; text_to_add &gt; will be entered , so make sure your indentation is formatted properly .Python files will be checked for syntax errors after the insertion .If the system detects a syntax error , the insertion will not be executed .Simply try to insert again , but make sure to read the error message and modify the insert command you issue accordingly .signature : insert &lt; line_number &gt; &lt; text_to_add &gt; end_of_insert arguments :</p>
<p>-line_number ( integer ) [ required ]: the line number after which to insert the text -text_to_add ( string ) [ required ]: the text to insert after the specified line If you ' d like to add the line ' print ( x ) ' you must fully write that out , with all those spaces before the code !Indentation is important and code that is not indented correctly will fail and require fixing before it can be run .</p>
<p>RESPONSE FORMAT :</p>
<p>Your shell prompt is formatted as follows : ( Open file : &lt; path &gt;) <cwd > You need to format your output using two fields ; discussion and command .Your output should always include <em>one</em> discussion and <em>one</em> command field EXACTLY as in the following example : DISCUSSION First I ' ll start by using ls to see what files are in the current directory .Then maybe we can look at some relevant files to see what they look like .''' ls -a ''' You should only include a * SINGLE * command in the command section and then wait for a response from the shell before continuing with more discussion and commands .Everything you include in the DISCUSSION section will be saved for future reference .</p>
<p>Please do not include any DISCUSSION after your action .If you ' d like to issue two commands at once , PLEASE DO NOT DO THAT !Please instead first submit just the first command , and then after receiving a response you ' ll be able to issue the second command .You ' re free to use any other bash commands you want ( e .g .find , grep , cat , ls , cd ) in addition to the special commands listed above .However , the environment does NOT support interactive session commands ( e .g .python , vim</p>
<p>) , so please do not invoke them .Your goal is to achieve the best possible score , not just to submit your first working solution .Consider strategies like validating your answer using the ' validate ' command , manually spot -checking predictions , building custom validation sets and grading functions , and comparing different algorithms .Once you have exhausted all possible solutions and cannot make progress , you can submit your final solution by using ' submit ' command .</p>
<p>INSTRUCTIONS</p>
<p>Figure 1
1
Figure 1 Diagram of MLGym, a unified framework designed to integrate diverse and open-ended AI research tasks into a single platform for developing and evaluating LLM agents on these tasks.</p>
<p>Existing works such as Huang et al. (2024); METR (2024); Chen et al. (2024) also use a script based evaluation approach, whereas MLE-Bench (Chan et al., 2024) uses a Kaggle style evaluation.</p>
<p>Figure 3
3
Figure 3Best Attempt AUP@4 vs cost for all models.The x-axis shows the API cost in USD and the y-axis shows the AUP@4 score.</p>
<p>Figure 4 Figure 5
45
Figure 4 Termination Error Distribution by model.The size of the bars corresponds to the number of times each model triggered an exit status.</p>
<p>Figure 12
12
Figure 12 Example of retrieving the best training configuration from memory and restarting exploration from it.</p>
<p>Table
BenchmarkGym Interface Algorithmic Tasks Open-Ended Research Flexible Artifacts Agentic HarnessMLGym (ours)MLE-BenchSWE-Bench/AgentMLAgentBenchRE-BenchScienceAgentBench</p>
<p>for a description of the available tools.
CategoryToolArgumentsDocumentationSWE-Agent ToolsExtended ToolsLiterature Search literature_search&lt; query &gt; [&lt; num_results &gt;]query Semantic Scholar API for papers with attached PDFsparse_pdf_url&lt; url &gt;downloads and extracts the contents of a PDF given a URLMemory Modulememory_write&lt; content_str &gt;save important results, configs or findings to memorymemory_read&lt; query_str &gt;retrieve top-2 elements from memory most similar to a query
Search search_dir &lt; search_term &gt; [&lt; dir &gt;] searches for the search term in all files in dir search_file &lt; search_term &gt; [&lt; file &gt;] searches for the search term in the given file find_file &lt; f ile_name &gt; [&lt; dir &gt;] finds all the files with the given name in dir File Viewer open &lt; path &gt; [&lt; line_number &gt;] opens the given file and goes to the line number goto &lt; line_number &gt; moves the window to show the line number scroll_down moves the window down 1000 lines scroll_up moves the window up 1000 lines File editing create &lt; filename &gt; creates a new file insert &lt; line_number &lt; text_to_add &gt; inserts the given text at line number in the open file edit &lt; start_line &gt;:&lt; end_line &lt; replacement_text &gt; replaces the given lines with the given text in the open file Evaluation validate validates the current submission file and returns the metrics on the test set submit submits the current code and terminates the session</p>
<p>Table 2
2
List of tools available to agents.Required arguments are enclosed in &lt;&gt; and optional arguments are in [].</p>
<p>Table 3
3
List of tasks included in MLGym-Bench along with their respective problem setting, domain, and datasets.</p>
<p>Table 4
4
AUP@4 scores for the best attempt and best submission across all models.Best scores are highlighted in blue .</p>
<p>Table 5
5
Best Attempt@4 scores for all models.Best scores are highlighted in blue .Note: ∞ indicates that the model was not able to produce even a single valid solution for submission or validation.
TaskMetricBaseline Llama3.1-405b-instruct GPT-4o Claude-3.5-Sonnet Gemini-1.5-Pro OpenAI o1CIFAR-10Accuracy0.4970.5280.7330.8940.7580.854Battle of SexesAverage Reward1.0231.2561.1441.4391.4431.439Prisoners DilemmaAverage Reward2.3722.5622.5822.5632.632.571BlottoAverage Reward-0.2480.0410.0470.2280.0880.247House Price PredictionR 2 Score0.880.9080.8950.9120.9080.931Fashion MNISTAccuracy0.7830.8760.9270.9450.9160.906MS-COCOBLEU Score0.2790.2940.1110.1250.1310.135MNLIValidation Accuracy0.5250.7770.8190.8300.8380.836Language ModelingValidation Loss4.673∞4.3614.4764.1663.966BreakoutAverage Score48.81758.87∞17.73571.38963.518Mountain Car Continuous Average Reward33.79418.692-216.62136.31392.51396.335Meta MazeAverage Return15.73426.7447.82348.56222.88934.9863-SAT HeuristicWall-Clock Time (s)16.15813.93613.67615.72814.3613.83</p>
<p>Table 2 :
2
Edit , View , Search , Validate and Submit .We treat validate and submit as two separate categories.Additionally, we have two open-ended categories: Python and Bash .All the actions that match the regex patterns python.<em>,deepspeed.</em>,torchrun.*areconsideredasPythonactions.These actions usually correspond to the agent attempting to run a model evaluation or training script.All other actions are grouped under Bash category, i.e. are considered as open-ended bash commands.Overall Action DistributionFigure6shows the action distribution across all runs.File commands such as Edit and View are one of the most frequently used commands with Edit accounting for 50% of the total actions.Whereas, Search commands are rarely used, accounting for only 1% of the total actions.This distribution suggests that models spend a significant portion of their time in an iterative development cycle of editing and viewing files.Additionally, we observe a trend of regular experimental evaluation and periodic validation of solution by the frequent use of Python and Validate commands.Action distribution across all runs.We group the actions into categories following the grouping defined in Table2and Section 7.4.2.
3500 40003,7082000 1750Edit Validate ViewSubmit SearchPython Bash3000150025001250Count2000Count100015001,45875010001,066835500500449250226Edit Figure 6 Claude Python Validate View Bash Submit Search 11 0 0 Figure 7 Action distribution for each model. We group O1-preview Llama Gemini GPT-4othe actions into categories following the grouping definedin Table 2 and Section 7.4.2.</p>
<p>Action distribution for each step.We group the actions into categories following the grouping defined in Table2and Section 7.4.2.
250Edit ViewSubmit SearchPython BashValidate200Count15010050015101520253035404550Step NumberFigure 8</p>
<p>AI agents proficient in tackling open research challenges like those in our benchmark could catalyze a remarkable acceleration in scientific advancement.This prospect is exhilarating yet demands a meticulous comprehension of model progress to ensure responsible and controlled deployment of such breakthroughs.MLGym-Bench, for instance, can serve as a metric for model autonomy within OpenAI's Preparedness Framework, autonomous capabilities in Anthropic's Responsible Scaling Policy, and ML R&amp;D in Google DeepMind's Frontier Safety Framework.Should AI agents become adept at autonomously conducting AI research, the positive impacts could be multifaceted, encompassing accelerated scientific progress in healthcare, climate science, and other domains, expedited safety and alignment research for models, and economic growth spurred by the development of novel products.The ability of agents to deliver high-quality research could signify a transformative stride in the economy.Nonetheless, agents capable of executing open-ended AI research tasks, such as enhancing their own training code, could augment the capabilities of cutting-edge models at a pace outstripping human researchers.If innovations outpace our ability to comprehend their ramifications, we risk developing models with catastrophic harm or misuse potential without parallel advancements in securing, aligning, and controlling such models.We believe a model proficient in solving a substantial portion of MLGym-Bench likely possesses the capacity to execute numerous open-ended AI tasks.We are open-sourcing MLGym and MLGym-Bench to foster understanding and research into the agentic capabilities of AI Research Agents and promote transparency regarding acceleration risks in frontier AI labs.In doing so, we acknowledge the limitations of MLGym-Bench and strongly encourage the development of additional evaluations of automated AI research capabilities, particularly those tailored to the workflow of researchers training frontier models.</p>
<p>Table 7
7
Table7lists the resources needed to run the agent on each task in MLGym-Bench.Each task has a set Training Timeout, which is used as the time limit for any python commands.Specifically, it is used to prevent the agent from continuously scaling the model parameters.Average agent runtime and Baseline runtime show the wall clock time for each agent run and the provided baseline code, respectively.Computational resources required for each task in MLGym-bench.
TaskTraining Timeout GPUs/Agents Average Agent Runtime Baseline Runtime (mins)CIFAR-1030m14h15Battle of Sexes30m030m5Prisoners Dilemma30m030m5Blotto30m030m5House Price Prediction30m11.5h10Fashion MNIST30m12h10MS-COCO40m17MNLI40m122Language Modeling40m24h20Breakout30m22h15Mountain Car Continuous30m22h15Meta Maze30m22h153-SAT Heuristic30m030m5</p>
<p>Table8lists the average input and output tokens and associated pricing for each model across all tasks in MLGym-Bench.We report the model pricing as listed by their respective providers.Llama3.1-405b-Instructpricing is taken from Together AI.Note that for this work, we used the open-weights model checkpoint with FP-8 precision, hosted on Meta Internal servers.Gemini-1.5-Procharges 2X for using the long-context capabilities, i.e for input and output exceeding 128K tokens.However, in our experiments, we do not observe Gemini using the long-context capabilities, so the final price is reported based on the normal pricing.
Avg. UsagePricingModelInput Output Input Output Context LengthLlama3.1-405b-instruct  *  30434825123.503.50128kClaude-3.5-Sonnet707704124153.0015.0200kGemini-1.5-Pro  †28261316331.255.002MGPT-4o26688624292.5010.0128kOpenAI O1-Preview3688986070415.060.0128k</p>
<p>Table 8
8
Model pricing, token usage and context length details.Model Pricing is in USD per 1M tokens.Number of Failed and Incomplete runs per task.The criteria for marking a run as incomplete or failed is described in Section 7.4.1Continuing the discussion from Section 7.4.1,we show the failed and incomplete runs on each task to understand the difficulty distribution of tasks.Language Modeling and all Reinforcement Learning tasks (Meta Maze, Mountain Car Continuous and Breakout) prove the most challenging, with the highest failure rates.Whereas, Fashion MNIST and Prisoner's Dilemma show the lowest failure rates, with all models producing a valid intermediate solution and a valid submission for all seeds.
Llama3.1:
*</p>
<dl>
<dt>submit : docstring : submits your current code and terminates the session signature : submit validate : docstring : validates your current submission file and returns the metrics on test set signature : validate Please note that THE EDIT and INSERT COMMANDS REQUIRES PROPER INDENTATION .</dt>
<dd>
<p>Now , you ' re going to train a model to improve performance on this task .Your terminal session has started and you ' re in the workspace root directory .You can use any bash commands or the special interface to help you .Edit all the file you need or create a new training script .Remember , YOU CAN ONLY ENTER ONE COMMAND AT A TIME .You should always wait for feedback after every command .When you ' re satisfied with all of the changes you ' ve made , you can run your training file .Your training file should include the logic for saving the prediction for the ' test ' set of the task .The submission file should be named ' submission .csv ' with the instance id and prediction column .A sample submission file is given in the workspace and you can read it to get a better understanding of the submission format .Note however that you cannot use any interactive session commands ( e .g .python , vim ) in this environment , but you can write scripts and run them .E .g .you can write a python script and then run it with ' python &lt; script_name &gt;. py '.NOTE ABOUT THE EDIT AND INSERT COMMANDs : Indentation really matters !When editing a file , make sure to insert appropriate indentation before each line !IMPORTANT TIPS : 1. Always start by trying to understand the baseline script if available .This will give you an idea of one possible solution for the task and the baseline scores that you have to beat .2. If you run a command and it doesn ' t work , try running a different command .A command that did not work once will not work the second time unless you modify it !</p>
</dd>
</dl>
<p>As of the latest release, SWE-Agent also decouples tools/ACI from the agent.
https://www.kaggle.com/datasets/yasserh/housing-prices-dataset
https://github.com/KellerJordan/modded-nanogpt
https://github.com/RobertTLange/gymnax-blines
https://2023.automl.cc/competitions/automl-cup/
https://www.together.ai/pricing
https://en.wikipedia.org/wiki/Borda_count
AcknowledgmentsWe thank Sten Sootla, Mikayel Samvelyan, Sharath Chandra Raparthy, Mike Plekhanov, and Rishi Hazra for many insightful discussions about evaluating and developing AI Research Agents.A.3 Action AnalysisExtending the results presented in Section 7.4.2, Figure10shows the action distribution on each task.The bars represent the sum of all the actions taken by all models on a particular task.We notice that RL tasks have the higest action count, while Game Theoretic tasks have the lowest action count.Algorithmic Tasks such as 3-SAT and Game Theory (Blotto, Prisonner's Dilemma and Battle of Sexes) also have the highest amount of validation actions, signifying a quick experimental cycle.Similarly, all RL tasks have the most complex codebases among all MLGym-Bench tasks and thus agent extensively use the View commands.A.4 Model RankingsTable9and Table10show each model's ranking based on Best Attempt@4 and Best Submission@4 scores respectively.The aggregate ranks are computed using the BORDA 7 count method.The aggregated rankings computed using BORDA count method align with the AUP score results as shown in Table4.However, similar to any ranking-only metric, it does not convey the relative difference between each model's performance.Table9Individual and Aggregate Ranking of models based on Best Attempt@4.We use the BORDA method to compute the aggregate ranks.Table10Individual and Aggregate Ranking of models based on Best Subimission@4.We use the BORDA method to compute the aggregate ranks.A.5 Memory UtilizationFigure11and Figure12show the agent using the memory module to store and retrieve specific experimental results and use them to submit the best possible model.7.Your each action should take less than 1800 seconds to complete .If your action doesn ' t finish within the time limit , it will be interrupted .( Current Step : 0 , Remaining Steps : 50) ( Open file : n / a ) ( Current directory : / home / agent / i m a g e C l a s s i f i c a t i o n C i f a r 1 0 ) bash -
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. AI Anthropic2023arXiv preprintThe claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1, 2024</p>
<p>Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement. Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, William Wang, 2024</p>
<p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. Robert Axelrod, ; Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Journal of conflict resolution. 2411980. April 2024Effective choice in the prisoner's dilemma</p>
<p>SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot, September 2024</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Openai gym. 2016</p>
<p>Spider2-v: How far are multimodal agents from automating data science and engineering workflows?. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu, 2024</p>
<p>Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Lilian Patwardhan, Aleksander Weng, Mądry, MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering. October 2024</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery. October 2024</p>
<p>The complexity of theorem-proving procedures. A Stephen, Cook, Proceedings of the third annual ACM symposium on Theory of computing. the third annual ACM symposium on Theory of computing1971</p>
<p>Communication in the battle of the sexes game: some experimental results. Russell Cooper, Douglas V Dejong, Robert Forsythe, Thomas W Ross, The RAND Journal of Economics. 1989</p>
<p>George Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, and Peter Mattson. Benchmarking neural network training algorithms. 062023</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su, 2023</p>
<p>Jacob Devlin, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Benchmarking optimization software with performance profiles. Elizabeth D Dolan, Jorge J Moré, 10.1007/s101070100263Mathematical Programming. 1436-4646912January 2002</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Pitfalls and best practices in algorithm configuration. Katharina Eggensperger, Marius Lindauer, Frank Hutter, Journal of Artificial Intelligence Research. 642019</p>
<p>Neural architecture search: A survey. Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, Journal of Machine Learning Research. 20552019</p>
<p>Potential applications of artificial intelligence to the field of software engineering. Emrich, Agarwal, Jairam, Oak Ridge National Lab Murthy, Tn, 1988Technical report</p>
<p>Some experimental games. M Merrill, Flood, Management Science. 511958</p>
<p>Magentic-one: A generalist multi-agent system for solving complex tasks. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Friederike Zhu, Grace Niedtner, Griffin Proebsting, Jack Bassman, Jacob Gerrits, Peter Alber, Ricky Chang, Robert Loynd, Victor West, Ahmed Dibia, Ece Awadallah, Rafah Kamar, Saleema Hosn, Amershi, 2024</p>
<p>Game theory. Drew Fudenberg, Jean Tirole, 1991MIT press</p>
<p>Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath , Shahul Hameed, Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, Jun Yao, Balazs Kegl, Haitham Bou-Ammar, and Jun Wang. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. 2024</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, 2024</p>
<p>Artificial intelligence and machine learning in design of mechanical materials. Kai Guo, Zhenze Yang, Chi-Hua Yu, Markus J Buehler, Materials Horizons. 842021</p>
<p>Artificial intelligence in drug design. Gerhard Hessler, Karl-Heinz Baringhaus, Molecules. 231025202018</p>
<p>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. Data Interpreter: An LLM Agent For Data Science. March 2024</p>
<p>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, April 2024</p>
<p>DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, June 2024</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Oak Ridge National Laboratory: the first fifty years. Leland Johnson, Daniel Schaffer, 1994Univ. of Tennessee Press</p>
<p>Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, Boza Vlado, You Jiacheng, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline. 2024</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy, arXiv:2307.10169Challenges and applications of large language models. 2023arXiv preprint</p>
<p>House prices -advanced regression techniques. Kaggle, January 24, 2025. 2016</p>
<p>Ai agents that matter. Sayash Kapoor, Benedikt Stroebl, Zachary S Siegel, Nitya Nadgir, Arvind Narayanan, 2024</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024aarXiv preprint</p>
<p>Tree search for language model agents. Jing Yu Koh, Stephen Mcaleer, Daniel Fried, Ruslan Salakhutdinov, 2024b</p>
<p>Learning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 2009</p>
<p>gymnax: A JAX-based reinforcement learning environment library. Robert Tjarko, Lange , 2022</p>
<p>Spider 2.0: Evaluating language models on real-world enterprise text-to-sql workflows. Langley ; Fangyu, Jixuan Lei, Yuxiao Chen, Ruisheng Ye, Dongchan Cao, Hongjin Shin, Zhaoqing Su, Hongcheng Suo, Wenjing Gao, Pengcheng Hu, Victor Yin, Caiming Zhong, Ruoxi Xiong, Qian Sun, Sida Liu, Tao Wang, Yu, 1987. 2024MIT pressScientific discovery: Computational explorations of the creative processes</p>
<p>Autokaggle: A multi-agent framework for autonomous data science competitions. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, Ge Zhang, 2024</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, 2023</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Best practices for scientific research on neural architecture search. Marius Lindauer, Frank Hutter, Journal of Machine Learning Research. 212432020</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. AgentBench: Evaluating LLMs as Agents. August 2023</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. August 2024</p>
<p>Games and decisions: Introduction and critical survey. Duncan Luce, Howard Raiffa, Courier Corporation. 2012</p>
<p>Sciagent: Tool-augmented language models for scientific reasoning. Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla, Weizhu Chen, 2024</p>
<p>Evaluating frontier ai r&amp;d capabilities of language model agents against human experts, 11 2024. METR</p>
<p>GAIA: A benchmark for General AI Assistants. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann Lecun, Thomas Scialom, November 2023</p>
<p>Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity. Thomas Miconi, Aditya Rawal, Jeff Clune, Kenneth O Stanley, 2020</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2022</p>
<p>Llmatic: neural architecture search via large language models and quality diversity optimization. Sam Muhammad Umair Nasir, Julian Earle, Steven Togelius, Christopher James, Cleghorn, proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2024</p>
<p>Babylm challenge: Curriculum learning based on sentence complexity approximating language acquisition. Miyu Oba, Akari Haga, Akiyo Fukatsu, Yohei Oseki, Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning2023</p>
<p>Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kuciński, Lerrel Pinto, Rob Fergus, arXiv:2411.13543Benchmarking agentic llm and vlm reasoning on games. 2024arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>The fineweb datasets: Decanting the web for the finest text data at scale. Guilherme Penedo, Hynek Kydlíček, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, 2024</p>
<p>The logic of scientific discovery. Karl Popper, 2005Routledge</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023</p>
<p>The colonel blotto game. Brian Roberson, Economic Theory. 2912006</p>
<p>Automl decathlon: Diverse tasks, modern methods, and efficiency at scale. Nicholas Roberts, Samuel Guo, Cong Xu, Ameet Talwalkar, David Lander, Lvfang Tao, Linhang Cai, Shuaicheng Niu, Jianyu Heng, Hongyang Qin, Minwen Deng, Johannes Hog, Alexander Pfefferle, Ammanaghatta Sushil, Arjun Shivakumar, Yubo Krishnakumar, Rhea Wang, Frank Sukthanker, Euxhen Hutter, Tien-Dung Hasanaj, Mikhail Le, Yuriy Khodak, Kashif Nevmyvaka, Frederic Rasul, Anderson Sala, Junhong Schneider, Evan Shen, Sparks, Proceedings of the NeurIPS 2022 Competitions Track. Marco Ciccone, Gustavo Stolovitzky, Jacob Albrecht, the NeurIPS 2022 Competitions TrackPMLR28 Nov-09 Dec 2022a220of Proceedings of Machine Learning Research</p>
<p>AutoWS-bench-101: Benchmarking automated weak supervision with 100 labels. Nicholas Roberts, Xintong Li, Tzu-Heng Huang, Dyah Adila, Spencer Schoenberg, Cheng-Yu Liu, Lauren Pick, Haotian Ma, Aws Albarghouthi, Frederic Sala, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022b</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Rethinking drug design in the artificial intelligence era. Petra Schneider, Patrick Walters, Alleyn T Plowright, Norman Sieroka, Jennifer Listgarten, Robert A GoodnowJr, Jasmin Fisher, Johanna M Jansen, José S Duca, Thomas S Rush, Nature reviews drug discovery. 1952020</p>
<p>Learning a SAT solver from single-bit supervision. Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo De Moura, David L Dill, CoRR, abs/1802.036852018</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 2024</p>
<p>Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, and Mark Gerstein. ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code. June 2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.05530Gemini Team. 1985. 2024arXiv preprintArtificial Intelligence Task Team. Artifical intelligence and nuclear power</p>
<p>Automl in the age of large language models: Current challenges, future opportunities and risks. Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne Theodorakopoulos, Tanja Tornede, Henning Wachsmuth, arXiv:2306.081072023arXiv preprint</p>
<p>Gymnasium: A standard interface for reinforcement learning environments. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Markus Krimmel, K G Arjun, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Hannah Tan, Omar G Younis, 2024</p>
<p>AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian, July 2024</p>
<p>NAS-bench-360: Benchmarking neural architecture search on diverse tasks. Renbo Tu, Nicholas Roberts, Mikhail Khodak, Junhong Shen, Frederic Sala, Ameet Talwalkar, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022</p>
<p>The clrs algorithmic reasoning benchmark. Petar Veličković, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, Charles Blundell, International Conference on Machine Learning. PMLR2022</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, 10.1007/s11704-024-40231-1Frontiers of Computer Science. 2095-2228186December 2024a</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen, User behavior simulation with large language model based agents. 2024b</p>
<p>OpenDevin: An Open Platform for AI Software Developers as Generalist Agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, H Hoang, Fuqiang Tran, Ren Li, Mingzhang Ma, Bill Zheng, Yanjun Qian, Niklas Shao, Yizhe Muennighoff, Binyuan Zhang, Junyang Hui, Robert Lin, Hao Brennan, Heng Peng, Graham Ji, Neubig, July 2024c</p>
<p>The multi-genre nli corpus. Adina Williams, Nikita Nangia, Samuel R Bowman, 2018</p>
<p>Os-copilot: Towards generalist computer agents with self-improvement. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong, arXiv:2402.074562024arXiv preprint</p>
<p>Agentless: Demystifying LLM-based Software Engineering Agents. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang, July 2024</p>
<p>Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf, 2017</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering. Karthik Narasimhan, and Ofir PressMay 2024</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, ; , Jonathan Berant, July 2024Ofir Press</p>
<p>Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. Kenny Young, Tian Tian, 2019</p>
<p>Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu, 2025</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, 2024a</p>
<p>Autocoderover: Autonomous program improvement. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury, Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis. the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis2024b</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854A realistic web environment for building autonomous agents. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
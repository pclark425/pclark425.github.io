<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-626 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-626</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-626</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-7dfe426e77f2dc220f5a3ea8213ff4b2827f8d16</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7dfe426e77f2dc220f5a3ea8213ff4b2827f8d16" target="_blank">Offspring from Reproduction Problems: What Replication Failure Teaches Us</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work presents two concrete use cases involving key techniques in the NLP domain for which it is shown that reproducing results is still difficult and that more care should be taken in interpreting results.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e626.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e626.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data Preprocessing (tokenisation, cleaning, feature encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All text and resource preprocessing steps (tokenisation, removal/normalisation of characters, rounding/encoding feature values, preprocessing external resources) that materially change datasets and feature distributions and thereby influence experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Reproducing Named Entity Recognition (NER) and WordNet similarity experiments; feature generation and dataset preparation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Tokenisation choices (e.g. removal of non-alphanumeric characters), normalization/cleanup of records, rounding and decimal precision of numeric features, encoding choices (multi-valued discrete vs boolean features), preprocessing of external resources (VIAF, GeoNames) and which records/versions of those resources are used.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Overall precision/recall/F1 for NER; dataset size (token counts); changes in evaluation scores before/after preprocessing choices.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Removing non-alphanumeric characters reduced the dataset from 12,510 to 10,442 tokens and produced a ~15 point drop in overall F-score; rounding/feature-encoding reported as having a 'significant impact' qualitatively (no precise numeric), and removing complex external-resource-based features produced modest F-score changes (see NER results).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of final precision/recall/F1 to original reported values; inspection of dataset sizes and intermediate feature outputs where available.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>NER reimplementation produced overall F1 = 49.45 versus original reported F1 = 69 (≈20 point drop), with a substantial portion of the difference attributable to preprocessing and dataset differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Papers often omit exact preprocessing steps (tokeniser, character normalisation), versions and preprocessing of external resources, and rounding/encoding details, preventing exact replication of feature distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Document and publish exact preprocessing code and scripts; publish intermediate outputs (tokenized data, feature files); include information on rounding/encoding and external resource preprocessing; use experiment/workflow archives or repositories.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>When preprocessing choices were altered experimentally: tokenisation change produced ~15 point drop in F1; removing window-based complex features improved F1 by 3.84 points; changing CV splitting (see Experimental setup) improved F1 by 8.57 points. Publishing intermediate outputs enabled targeted diagnosis in WordNet experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10-fold cross-validation (NER); specific preprocessing variants were run as separate experimental conditions (counts not enumerated)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Preprocessing choices can produce very large changes in outcomes (e.g., ~15 F1 points) and must be documented and shared; publishing intermediate preprocessing outputs is critical to enable reproduction and diagnosis.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e626.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e626.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental setup</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental Setup (cross-validation, PoS restrictions, gold standard, ranking coefficients)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Choices in the experimental protocol such as how data are split for cross-validation, part-of-speech (PoS) tag restrictions, which gold standard is used, and which ranking/correlation coefficient and tie-handling are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Comparative evaluation of WordNet similarity measures and NER systems under different experimental configurations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Cross-validation split strategy (record-level vs sentence-level), PoS-tag restrictions (nouns-only vs all PoS), choice of gold standard (Rubenstein & Goodenough vs Miller & Charles variants), ranking coefficient (Spearman's rho vs Kendall's tau), and tie treatment in ranking coefficient implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Spearman's correlation coefficient (ρ), Kendall's tau (τ), ranking position variance, standard deviation of F-score across CV folds.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Similarity-measure results varied across experimental setups: measures showed variation up to 0.44 points in Spearman ρ and up to 0.60 in Kendall τ (individual reported ranges: ρ from 0.06–0.42 across measures; τ from 0.05–0.60). For NER, different cross-validation folds produced standard deviation up to 25 points in F-score across 10 splits; changing the fold-creation method increased overall F1 by 8.57 points (to 58.02).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Correlation coefficients (Spearman ρ, Kendall τ), ranking position changes (1..12), F1/precision/recall comparisons across different splits and config choices, standard deviation across CV folds.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Which WordNet similarity measure 'performs best' depended on evaluation choices; ranking positions of measures changed widely (examples: 'path' ranked between 1 and 8 across conditions). NER results were fragile to CV splitting method (up to 25-point F1 spread across folds).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Papers commonly omit details on how folds are constructed, PoS restrictions, which gold standard version was used, and exact implementation/treatment of ties in ranking metrics—these omissions change conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Systematically vary and report experimental setup choices; include exact fold-generation code and seed; report and justify PoS restrictions and gold-standard selection; publish intermediate rankings and scores so that tie-handling can be inspected; prefer and explicitly state ranking coefficient (authors argued Spearman ρ preferable here).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Explicitly switching ranking coefficients and tie-handling changed measured correlations and rankings; changing CV split method recovered ~8.57 F1 points, showing that reporting and controlling these choices materially affects conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Multiple configuration permutations (WordNet versions × gold standards × PoS restrictions × measure configs) and 10-fold CV for NER (analysis compared the 10 splits and alternative fold-generation methods).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Experimental-setup choices (CV split, PoS restrictions, gold standard, ranking metric/tie handling) can flip comparative conclusions (which method is best) and produce large numeric variability, so these choices must be explicitly reported and systematically tested.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e626.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e626.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Versioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Resource and Software Versioning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences in versions of lexical resources (WordNet), software libraries (WordNet::Similarity versions, Mallet, Stanford NER), and external resources (VIAF, GeoNames) that affect outputs even when following the same described method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Computational Linguistics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Replication of WordNet similarity experiments and NER experiments across different resource and software versions</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Different WordNet releases (e.g., 2.1 vs 3.0), different WordNet::Similarity versions (1.02, 2.05), updates in external resources (VIAF, GeoNames), updates in toolkits (Mallet, CRF implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Changes in Spearman ρ and Kendall τ correlations, changes in ranking positions of measures across versions.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Switching WordNet/WordNet::Similarity versions contributed to the observed min/max ranges in ranking correlations (part of the up-to-0.44 ρ and up-to-0.60 τ variation); no single version was universally best—ranking and scores shifted when versions changed.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Compare correlations and rankings when running identical code with different resource/software versions.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Differences in versions produced measurable differences in reported correlation scores and relative rankings; the study could not declare a single superior version/configuration without version-locked reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Original publications often fail to report exact versions of resources and tools, and resources/tools are updated over time, which hampers exact replication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Record and publish exact versions used for every resource and software package; provide versioned snapshots or containerized execution environments; use experiment databases to store full setups.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Access to the same versions (and intermediate outputs) enabled partial replication and diagnosis of variation; the paper advocates version-locking and sharing but does not provide quantitative improvement numbers beyond enabling replication diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Comparisons included two WordNet versions and multiple WordNet::Similarity versions across measures (counts vary per experiment); exact run counts per version not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Version differences in resources and tools are a concrete source of variability that change numeric outcomes and relative rankings, so precise version reporting and sharing is necessary for reproducibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e626.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e626.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>System output</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System Output and Intermediate Artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Availability and inspection of intermediate outputs (per-pair similarity scores, token-level features, feature files) that allow tracing differences between implementations and diagnosing sources of variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Debugging and reproducing WordNet similarity and NER experiments via intermediate outputs</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Absence of intermediate outputs (e.g., per-pair similarity scores, token-based feature files), inconsistent feature representations, rounding differences, and undocumented tie-breaking in output processing.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Direct comparison of per-item similarity scores, inspection of ranks, effect on aggregate correlation metrics when intermediate outputs differ.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Replication was only possible because Pedersen provided the per-pair similarity scores; tie-handling in the Spearman implementation (assigning lowest rank to ties rather than mean) caused an unjustified drop in measured correlation for some measures (qualitative effect reported).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Equality/comparison of intermediate outputs (per-pair scores), then recomputation of ranking coefficients and final metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>When intermediate outputs were provided, discrepancies in final aggregated metrics could be traced to specific implementation choices (e.g., tie handling), enabling targeted fixes; without intermediate outputs, replication attempts failed or were inconclusive.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Authors rarely publish intermediate artifacts; lack of these artifacts prevents stepwise comparison and identification of divergent behavior between implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Publish intermediate outputs (similarity scores, tokenised feature vectors, counts), include detailed logging of processing steps, and make scripts that generate intermediates available.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Availability of intermediate outputs directly enabled replication and diagnosis in the WordNet experiments; the paper gives qualitative evidence but no aggregated numeric improvement beyond enabling successful debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Publishing intermediate outputs is critical: it allows researchers to locate exactly where implementations diverge (e.g., tie handling) and is often a precondition for successful replication.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e626.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e626.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>System variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inherent System Variation and Stochasticity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Non-deterministic behavior arising from algorithmic choices (random seeds, tie-breaking, stochastic training procedures) and variability due to unstable dataset splits, which together produce run-to-run variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Machine Learning</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessing run-to-run variability in ML-based NER and deterministic/stochastic components in WordNet similarity evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Randomness in ML algorithms (e.g., coin flips for tie-breaking), differences in CRF implementations and parameter settings, random cross-validation splits, and unstable small datasets where individual items have large influence.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation of F-score across CV folds, ranges/min-max of ranking correlation coefficients (Spearman ρ and Kendall τ), variation in ranking positions for measures.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Standard deviations up to 25 F1 points across 10 CV splits were observed for NER; similarity-measure correlation ranges up to ~0.44 (Spearman) and ~0.60 (Kendall) across configuration/system variations; ranking positions of measures shifted widely across conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Compute mean and standard deviation across repeated CV folds; compare ranking variability across repeated configurations and implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>High run-to-run variability and sensitivity to splits/implementation choices caused large differences in reported performance and sometimes changed the comparative ordering of methods; the NER reimplementation was ~20 F1 points lower than reported and CV variability could explain substantial parts of the instability.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Non-determinism and unreported random seeds, undocumented tie-breaking or implementation differences, and small, fragile datasets where single items carry large weight.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Perform multiple independent runs (e.g., multiple CV randomisations) and report averages and standard deviations; fix random seeds where appropriate; report tie-breaking behavior and implementation details; use larger/more diverse evaluation sets or stratified fold generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>The paper demonstrates that different fold-generation strategies change F1 by 8.57 points and that running and inspecting multiple folds reveals up to 25-point variability; averaging or reporting ranges is advocated though no specific numeric reduction from averaging is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>10-fold CV with analysis of variation across the 10 folds; multiple configuration permutations for WordNet experiments (exact counts variable).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inherent stochasticity and unstable experimental choices can produce very large variability (up to ~25 F1 points or large correlation swings), so multiple runs, explicit seed control, and reporting of variance/ranges are necessary to interpret results reliably.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Consequences of variability in classifier performance estimates <em>(Rating: 2)</em></li>
                <li>Experiment databases <em>(Rating: 2)</em></li>
                <li>The case for open computer programs <em>(Rating: 2)</em></li>
                <li>Replication of defect prediction studies: problems, pitfalls and recommendations <em>(Rating: 2)</em></li>
                <li>On reproducibility and traceability of simulations <em>(Rating: 2)</em></li>
                <li>Replicability is not reproducibility: nor is it good science <em>(Rating: 1)</em></li>
                <li>A note on rigour and replicability <em>(Rating: 1)</em></li>
                <li>Intricacies of Collins' parsing model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-626",
    "paper_id": "paper-7dfe426e77f2dc220f5a3ea8213ff4b2827f8d16",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Preprocessing",
            "name_full": "Data Preprocessing (tokenisation, cleaning, feature encoding)",
            "brief_description": "All text and resource preprocessing steps (tokenisation, removal/normalisation of characters, rounding/encoding feature values, preprocessing external resources) that materially change datasets and feature distributions and thereby influence experimental outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing",
            "experimental_task": "Reproducing Named Entity Recognition (NER) and WordNet similarity experiments; feature generation and dataset preparation",
            "variability_sources": "Tokenisation choices (e.g. removal of non-alphanumeric characters), normalization/cleanup of records, rounding and decimal precision of numeric features, encoding choices (multi-valued discrete vs boolean features), preprocessing of external resources (VIAF, GeoNames) and which records/versions of those resources are used.",
            "variability_measured": true,
            "variability_metrics": "Overall precision/recall/F1 for NER; dataset size (token counts); changes in evaluation scores before/after preprocessing choices.",
            "variability_results": "Removing non-alphanumeric characters reduced the dataset from 12,510 to 10,442 tokens and produced a ~15 point drop in overall F-score; rounding/feature-encoding reported as having a 'significant impact' qualitatively (no precise numeric), and removing complex external-resource-based features produced modest F-score changes (see NER results).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of final precision/recall/F1 to original reported values; inspection of dataset sizes and intermediate feature outputs where available.",
            "reproducibility_results": "NER reimplementation produced overall F1 = 49.45 versus original reported F1 = 69 (≈20 point drop), with a substantial portion of the difference attributable to preprocessing and dataset differences.",
            "reproducibility_challenges": "Papers often omit exact preprocessing steps (tokeniser, character normalisation), versions and preprocessing of external resources, and rounding/encoding details, preventing exact replication of feature distributions.",
            "mitigation_methods": "Document and publish exact preprocessing code and scripts; publish intermediate outputs (tokenized data, feature files); include information on rounding/encoding and external resource preprocessing; use experiment/workflow archives or repositories.",
            "mitigation_effectiveness": "When preprocessing choices were altered experimentally: tokenisation change produced ~15 point drop in F1; removing window-based complex features improved F1 by 3.84 points; changing CV splitting (see Experimental setup) improved F1 by 8.57 points. Publishing intermediate outputs enabled targeted diagnosis in WordNet experiments.",
            "comparison_with_without_controls": true,
            "number_of_runs": "10-fold cross-validation (NER); specific preprocessing variants were run as separate experimental conditions (counts not enumerated)",
            "key_findings": "Preprocessing choices can produce very large changes in outcomes (e.g., ~15 F1 points) and must be documented and shared; publishing intermediate preprocessing outputs is critical to enable reproduction and diagnosis.",
            "uuid": "e626.0"
        },
        {
            "name_short": "Experimental setup",
            "name_full": "Experimental Setup (cross-validation, PoS restrictions, gold standard, ranking coefficients)",
            "brief_description": "Choices in the experimental protocol such as how data are split for cross-validation, part-of-speech (PoS) tag restrictions, which gold standard is used, and which ranking/correlation coefficient and tie-handling are applied.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing",
            "experimental_task": "Comparative evaluation of WordNet similarity measures and NER systems under different experimental configurations",
            "variability_sources": "Cross-validation split strategy (record-level vs sentence-level), PoS-tag restrictions (nouns-only vs all PoS), choice of gold standard (Rubenstein & Goodenough vs Miller & Charles variants), ranking coefficient (Spearman's rho vs Kendall's tau), and tie treatment in ranking coefficient implementation.",
            "variability_measured": true,
            "variability_metrics": "Spearman's correlation coefficient (ρ), Kendall's tau (τ), ranking position variance, standard deviation of F-score across CV folds.",
            "variability_results": "Similarity-measure results varied across experimental setups: measures showed variation up to 0.44 points in Spearman ρ and up to 0.60 in Kendall τ (individual reported ranges: ρ from 0.06–0.42 across measures; τ from 0.05–0.60). For NER, different cross-validation folds produced standard deviation up to 25 points in F-score across 10 splits; changing the fold-creation method increased overall F1 by 8.57 points (to 58.02).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Correlation coefficients (Spearman ρ, Kendall τ), ranking position changes (1..12), F1/precision/recall comparisons across different splits and config choices, standard deviation across CV folds.",
            "reproducibility_results": "Which WordNet similarity measure 'performs best' depended on evaluation choices; ranking positions of measures changed widely (examples: 'path' ranked between 1 and 8 across conditions). NER results were fragile to CV splitting method (up to 25-point F1 spread across folds).",
            "reproducibility_challenges": "Papers commonly omit details on how folds are constructed, PoS restrictions, which gold standard version was used, and exact implementation/treatment of ties in ranking metrics—these omissions change conclusions.",
            "mitigation_methods": "Systematically vary and report experimental setup choices; include exact fold-generation code and seed; report and justify PoS restrictions and gold-standard selection; publish intermediate rankings and scores so that tie-handling can be inspected; prefer and explicitly state ranking coefficient (authors argued Spearman ρ preferable here).",
            "mitigation_effectiveness": "Explicitly switching ranking coefficients and tie-handling changed measured correlations and rankings; changing CV split method recovered ~8.57 F1 points, showing that reporting and controlling these choices materially affects conclusions.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Multiple configuration permutations (WordNet versions × gold standards × PoS restrictions × measure configs) and 10-fold CV for NER (analysis compared the 10 splits and alternative fold-generation methods).",
            "key_findings": "Experimental-setup choices (CV split, PoS restrictions, gold standard, ranking metric/tie handling) can flip comparative conclusions (which method is best) and produce large numeric variability, so these choices must be explicitly reported and systematically tested.",
            "uuid": "e626.1"
        },
        {
            "name_short": "Versioning",
            "name_full": "Resource and Software Versioning",
            "brief_description": "Differences in versions of lexical resources (WordNet), software libraries (WordNet::Similarity versions, Mallet, Stanford NER), and external resources (VIAF, GeoNames) that affect outputs even when following the same described method.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Computational Linguistics",
            "experimental_task": "Replication of WordNet similarity experiments and NER experiments across different resource and software versions",
            "variability_sources": "Different WordNet releases (e.g., 2.1 vs 3.0), different WordNet::Similarity versions (1.02, 2.05), updates in external resources (VIAF, GeoNames), updates in toolkits (Mallet, CRF implementations).",
            "variability_measured": true,
            "variability_metrics": "Changes in Spearman ρ and Kendall τ correlations, changes in ranking positions of measures across versions.",
            "variability_results": "Switching WordNet/WordNet::Similarity versions contributed to the observed min/max ranges in ranking correlations (part of the up-to-0.44 ρ and up-to-0.60 τ variation); no single version was universally best—ranking and scores shifted when versions changed.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Compare correlations and rankings when running identical code with different resource/software versions.",
            "reproducibility_results": "Differences in versions produced measurable differences in reported correlation scores and relative rankings; the study could not declare a single superior version/configuration without version-locked reporting.",
            "reproducibility_challenges": "Original publications often fail to report exact versions of resources and tools, and resources/tools are updated over time, which hampers exact replication.",
            "mitigation_methods": "Record and publish exact versions used for every resource and software package; provide versioned snapshots or containerized execution environments; use experiment databases to store full setups.",
            "mitigation_effectiveness": "Access to the same versions (and intermediate outputs) enabled partial replication and diagnosis of variation; the paper advocates version-locking and sharing but does not provide quantitative improvement numbers beyond enabling replication diagnostics.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Comparisons included two WordNet versions and multiple WordNet::Similarity versions across measures (counts vary per experiment); exact run counts per version not enumerated.",
            "key_findings": "Version differences in resources and tools are a concrete source of variability that change numeric outcomes and relative rankings, so precise version reporting and sharing is necessary for reproducibility.",
            "uuid": "e626.2"
        },
        {
            "name_short": "System output",
            "name_full": "System Output and Intermediate Artifacts",
            "brief_description": "Availability and inspection of intermediate outputs (per-pair similarity scores, token-level features, feature files) that allow tracing differences between implementations and diagnosing sources of variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing",
            "experimental_task": "Debugging and reproducing WordNet similarity and NER experiments via intermediate outputs",
            "variability_sources": "Absence of intermediate outputs (e.g., per-pair similarity scores, token-based feature files), inconsistent feature representations, rounding differences, and undocumented tie-breaking in output processing.",
            "variability_measured": true,
            "variability_metrics": "Direct comparison of per-item similarity scores, inspection of ranks, effect on aggregate correlation metrics when intermediate outputs differ.",
            "variability_results": "Replication was only possible because Pedersen provided the per-pair similarity scores; tie-handling in the Spearman implementation (assigning lowest rank to ties rather than mean) caused an unjustified drop in measured correlation for some measures (qualitative effect reported).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Equality/comparison of intermediate outputs (per-pair scores), then recomputation of ranking coefficients and final metrics.",
            "reproducibility_results": "When intermediate outputs were provided, discrepancies in final aggregated metrics could be traced to specific implementation choices (e.g., tie handling), enabling targeted fixes; without intermediate outputs, replication attempts failed or were inconclusive.",
            "reproducibility_challenges": "Authors rarely publish intermediate artifacts; lack of these artifacts prevents stepwise comparison and identification of divergent behavior between implementations.",
            "mitigation_methods": "Publish intermediate outputs (similarity scores, tokenised feature vectors, counts), include detailed logging of processing steps, and make scripts that generate intermediates available.",
            "mitigation_effectiveness": "Availability of intermediate outputs directly enabled replication and diagnosis in the WordNet experiments; the paper gives qualitative evidence but no aggregated numeric improvement beyond enabling successful debugging.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Publishing intermediate outputs is critical: it allows researchers to locate exactly where implementations diverge (e.g., tie handling) and is often a precondition for successful replication.",
            "uuid": "e626.3"
        },
        {
            "name_short": "System variation",
            "name_full": "Inherent System Variation and Stochasticity",
            "brief_description": "Non-deterministic behavior arising from algorithmic choices (random seeds, tie-breaking, stochastic training procedures) and variability due to unstable dataset splits, which together produce run-to-run variability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Machine Learning",
            "experimental_task": "Assessing run-to-run variability in ML-based NER and deterministic/stochastic components in WordNet similarity evaluations",
            "variability_sources": "Randomness in ML algorithms (e.g., coin flips for tie-breaking), differences in CRF implementations and parameter settings, random cross-validation splits, and unstable small datasets where individual items have large influence.",
            "variability_measured": true,
            "variability_metrics": "Standard deviation of F-score across CV folds, ranges/min-max of ranking correlation coefficients (Spearman ρ and Kendall τ), variation in ranking positions for measures.",
            "variability_results": "Standard deviations up to 25 F1 points across 10 CV splits were observed for NER; similarity-measure correlation ranges up to ~0.44 (Spearman) and ~0.60 (Kendall) across configuration/system variations; ranking positions of measures shifted widely across conditions.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Compute mean and standard deviation across repeated CV folds; compare ranking variability across repeated configurations and implementations.",
            "reproducibility_results": "High run-to-run variability and sensitivity to splits/implementation choices caused large differences in reported performance and sometimes changed the comparative ordering of methods; the NER reimplementation was ~20 F1 points lower than reported and CV variability could explain substantial parts of the instability.",
            "reproducibility_challenges": "Non-determinism and unreported random seeds, undocumented tie-breaking or implementation differences, and small, fragile datasets where single items carry large weight.",
            "mitigation_methods": "Perform multiple independent runs (e.g., multiple CV randomisations) and report averages and standard deviations; fix random seeds where appropriate; report tie-breaking behavior and implementation details; use larger/more diverse evaluation sets or stratified fold generation.",
            "mitigation_effectiveness": "The paper demonstrates that different fold-generation strategies change F1 by 8.57 points and that running and inspecting multiple folds reveals up to 25-point variability; averaging or reporting ranges is advocated though no specific numeric reduction from averaging is reported in the paper.",
            "comparison_with_without_controls": true,
            "number_of_runs": "10-fold CV with analysis of variation across the 10 folds; multiple configuration permutations for WordNet experiments (exact counts variable).",
            "key_findings": "Inherent stochasticity and unstable experimental choices can produce very large variability (up to ~25 F1 points or large correlation swings), so multiple runs, explicit seed control, and reporting of variance/ranges are necessary to interpret results reliably.",
            "uuid": "e626.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Consequences of variability in classifier performance estimates",
            "rating": 2
        },
        {
            "paper_title": "Experiment databases",
            "rating": 2
        },
        {
            "paper_title": "The case for open computer programs",
            "rating": 2
        },
        {
            "paper_title": "Replication of defect prediction studies: problems, pitfalls and recommendations",
            "rating": 2
        },
        {
            "paper_title": "On reproducibility and traceability of simulations",
            "rating": 2
        },
        {
            "paper_title": "Replicability is not reproducibility: nor is it good science",
            "rating": 1
        },
        {
            "paper_title": "A note on rigour and replicability",
            "rating": 1
        },
        {
            "paper_title": "Intricacies of Collins' parsing model",
            "rating": 1
        }
    ],
    "cost": 0.01596475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Offspring from Reproduction Problems: What Replication Failure Teaches Us</h1>
<p>Antske Fokkens and Marieke van Erp<br>The Network Institute<br>VU University Amsterdam<br>Amsterdam, The Netherlands<br>{a.s.fokkens,m.g.j.van.erp}@vu.nl<br>Ted Pedersen<br>Dept. of Computer Science<br>University of Minnesota<br>Duluth, MN 55812 USA<br>tpederse@d.umn.edu</p>
<p>Piek Vossen<br>The Network Institute<br>VU University Amsterdam<br>Amsterdam, The Netherlands<br>piek.vossen@vu.nl</p>
<h2>Nuno Freire</h2>
<p>The European Library
The Hague, The Netherlands
nfreire@gmail.com</p>
<h2>Abstract</h2>
<p>Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.</p>
<h2>1 Introduction</h2>
<p>Research is a collaborative effort to increase knowledge. While it includes validating previous approaches, our experience is that most research output in our field focuses on presenting new approaches, and to a somewhat lesser extent building upon existing work.</p>
<p>In this paper, we argue that the value of research that attempts to replicate previous approaches goes beyond simply validating what is already known. It is also an essential aspect for building upon existing approaches. Especially when validation
fails or variations in results are found, systematic testing helps to obtain a clearer picture of both the approach itself and of the meaning of state-of-theart results leading to a better insight into the quality of new approaches in relation to previous work.</p>
<p>We support our claims by presenting two use cases that aim to reproduce results of previous work in two key NLP technologies: measuring WordNet similarity and Named Entity Recognition (NER). Besides highlighting the difficulty of repeating other researchers' work, new insights about the approaches emerged that were not presented in the original papers. This last point shows that reproducing results is not merely part of good practice in science, but also an essential part in gaining a better understanding of the methods we use. Likewise, the problems we face in reproducing previous results are not merely frustrating inconveniences, but also pointers to research questions that deserve deeper investigation.</p>
<p>We investigated five aspects that cause experimental variation that are not typically described in publications: preprocessing (e.g. tokenisation), experimental setup (e.g. splitting data for cross-validation), versioning (e.g. which version of WordNet), system output (e.g. the exact features used for individual tokens in NER), and system variation (e.g. treatment of ties).</p>
<p>As such, reproduction provides a platform for systematically testing individual aspects of an approach that contribute to a given result. What is the influence of the size of the dataset, for example? How does using a different dataset affect the results? What is a reasonable divergence between different runs of the same experiment? Finding answers to these questions enables us to better interpret our state-of-the-art results.</p>
<p>Moreover, the experiments in this paper show that even while strictly trying to replicate a previous experiment, results may vary up to a point where they lead to different answers to the main question addressed by the experiment. The WordNet similarity experiment use case compares the performance of different similarity measures. We will show that the answer as to which measure works best changes depending on factors such as the gold standard used, the strategy towards part-of-speech or the ranking coefficient, all aspects that are typically not addressed in the literature.</p>
<p>The main contributions of this paper are the following:</p>
<p>1) An in-depth analysis of two reproduction use cases in NLP
2) New insights into the state-of-the-art results for WordNet similarities and NER, found because of problems in reproducing prior research
3) A categorisation of aspects influencing reproduction of experiments and suggestions on testing their influence systematically</p>
<p>The code, data and experimental setup for the WordNet experiments are available at http://github.com/antske/ WordNetSimilarity, and for the NER experiments at http://github.com/Mvanerp/ NER. The experiments presented in this paper have been repeated by colleagues not involved in the development of the software using the code included in these repositories. The remainder of this paper is structured as follows. In Section 2, previous work is discussed. Sections 3 and 4 describe our real-world use cases. In Section 5, we present our observations, followed by a more general discussion in Section 6. In Section 7, we present our conclusions.</p>
<h2>2 Background</h2>
<p>This section provides a brief overview of recent work addressing reproduction and benchmark results in computer science related studies and discusses how our research fits in the overall picture.</p>
<p>Most researchers agree that validating results entails that a method should lead to the same overall conclusions rather than producing the exact same numbers (Drummond, 2009; Dalle, 2012; Buchert and Nussbaum, 2012, etc.). In other words, we should strive to reproduce the same answer to a research question by different means,
perhaps by re-implementing an algorithm or evaluating it on a new (in domain) data set. Replication has a somewhat more limited aim, and simply involves running the exact same system under the same conditions in order to get the exact same results as output.</p>
<p>According to Drummond (2009) replication is not interesting, since it does not lead to new insights. On this point we disagree with Drummond (2009) as replication allows us to: 1) validate prior research, 2) improve on prior research without having to rebuild software from scratch, and 3) compare results of reimplementations and obtain the necessary insights to perform reproduction experiments. The outcome of our use cases confirms the statement that deeper insights into an approach can be obtained when all resources are available, an observation also made by Ince et al. (2012).</p>
<p>Even if exact replication is not a goal many strive for, Ince et al. (2012) argue that insightful reproduction can be an (almost) impossible undertaking without the source code being available. Moreover, it is not always clear where replication stops and reproduction begins. Dalle (2012) distinguishes levels of reproducing results related to how close they are to the original work and how each contributes to research. In general, an increasing awareness of the importance of reproduction research and open code and data can be observed based on publications in high-profile journals (e.g. Nature (Ince et al., 2012)) and initiatives such as myExperiment. ${ }^{1}$</p>
<p>Howison and Herbsleb (2013) point out that, even though this is important, often not enough (academic) credit is gained from making resources available. What is worse, the same holds for research that investigates existing methods rather than introducing new ones, as illustrated by the question that is found on many review forms 'how novel is the presented approach?'. On the other hand, initiatives for journals addressing exactly this issue (Neylon et al., 2012) and tracks focusing on results verification at conferences such as VLDB $^{2}$ show that this opinion is not universal.</p>
<p>A handful of use cases on reproducing or replicating results have been published. Louridas and Gousios (2012) present a use case revealing that source code alone is not enough for reproducing</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>results, a point that is also made by Mende (2010) who provides an overview of all information required to replicate results.</p>
<p>The experiments in this paper provide use cases that confirm the points brought out in the literature mentioned above. This includes both observations that a detailed level of information is required for truly insightful reproduction research as well as the claim that such research leads to better understanding of our techniques. Furthermore, the work in this paper relates to Bikel (2004)'s work. He provides all information needed in addition to Collins (1999) to replicate Collins' benchmark results. Our work is similar in that we also aim to fill in the blanks needed to replicate results. It must be noted, however, that the use cases in this paper have a significantly smaller scale than Bikel's.</p>
<p>Our research distinguishes itself from previous work, because it links the challenges of reproduction to what they mean for reported results beyond validation. Ruml (2010) mentions variations in outcome as a reason not to emphasise comparisons to benchmarks. Vanschoren et al. (2012) propose to use experimental databases to systematically test variations for machine learning, but neither links the two issues together. Raeder et al. (2010) come closest to our work in a critical study on the evaluation of machine learning. They show that choices in the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known.</p>
<h2>3 WordNet Similarity Measures</h2>
<p>Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)'s set (henceforth " $m c$-set") of 30 word pairs according to their semantic relatedness with several WordNet similarity measures.</p>
<p>Each measure ranks the $m c$-set of word pairs and these outputs are compared to Miller and</p>
<p>Charles (1991)'s gold standard based on human rankings using the Spearman's Correlation Coefficient (Spearman, 1904, $\rho$ ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Goodenough (1965) (rg-set) which is a superset of Miller and Charles's set.</p>
<h3>3.1 Replication Attempts</h3>
<p>This research emerged from a project running a similar experiment for Dutch on Cornetto (Vossen et al., 2013). First, an attempt was made to reproduce the results reported in Patwardhan and Pedersen (2006) and Pedersen (2010) on the English WordNet using their WordNet::Similarity web-interface. ${ }^{3}$ Results differed from those reported in the aforementioned works, even when using the same versions as the original, WordNet::Similarity-1.02 and WordNet 2.1 (Patwardhan and Pedersen, 2006) and WordNet::Similarity-2.05 and WordNet 3.0 (Pedersen, 2010), respectively. ${ }^{4}$</p>
<p>The fact that results of similarity measures on WordNet can differ even while the same software and same versions are used indicates that properties which are not addressed in the literature may influence the output of similarity measures. We therefore conducted a range of experiments that, in addition to searching for the right settings to replicate results of previous research, address the following questions:</p>
<p>1) Which properties have an impact on the performance of WordNet similarity measures?
2) How much does the performance of individual measures vary?
3) How do commonly used measures compare when the variation of their performance are taken into account?</p>
<h3>3.2 Methodology and first observations</h3>
<p>The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup),</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively.</p>
<p>Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the $m c$-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coefficient were compared.</p>
<p>Differences in results turned out to be related to variations in the experimental setup. First, we made different assumptions on the restriction of part-of-speech tags (henceforth "PoS-tag") considered in the comparison. Miller and Charles (1991) do not discuss how they deal with words with more than one PoS-tag in their study. Pedersen therefore included all senses with any PoStag in his study. The first replication attempt had restricted PoS-tags to nouns based on the idea that most items are nouns and subjects would be primed to primarily think of the noun senses. Both assumptions are reasonable. Pos-tags were not restricted in the second replication attempt, but because of a bug in the code only the first identified PoS-tag ("noun" in all cases) was considered. We therefore mistakenly assumed that PoS-tag restrictions did not matter until we compared individual scores between Pedersen and the replication attempts.</p>
<p>Second, there are two gold standards for the Miller and Charles (1991) set: one has the scores assigned during the original experiment run by Rubenstein and Goodenough (1965), the other has the scores assigned during Miller and Charles (1991)'s own experiment. The ranking correlation between the two sets is high, but they are not identical. Again, there is no reason why one gold standard would be a better choice than the other, but in order to replicate results, it must be known which of the two was used. Third, results changed because of differences in the treatment of ties while calculating Spearman $\rho$. The influence of the exact gold standard and calculation of Spearman $\rho$ could only be found because Pedersen could pro-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">measure</th>
<th style="text-align: center;">Spearman $\rho$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kendall $\tau$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ranking <br> variation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">min</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path based similarity</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">path</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$1-8$</td>
</tr>
<tr>
<td style="text-align: center;">wup</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">$1-6$</td>
</tr>
<tr>
<td style="text-align: center;">lch</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$1-7$</td>
</tr>
<tr>
<td style="text-align: center;">path based information content</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">res</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">$4-11$</td>
</tr>
<tr>
<td style="text-align: center;">lin</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">$6-10$</td>
</tr>
<tr>
<td style="text-align: center;">jcn</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">$5,7-11$</td>
</tr>
<tr>
<td style="text-align: center;">path based relatedness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">hso</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">$1-3,5-10$</td>
</tr>
<tr>
<td style="text-align: center;">dictionary and corpus based relatedness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">vpairs</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$7-11$</td>
</tr>
<tr>
<td style="text-align: center;">vector</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">$1,2,4,6-11$</td>
</tr>
<tr>
<td style="text-align: center;">lesk</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">$-0.02$</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">$1-8,11,12$</td>
</tr>
</tbody>
</table>
<p>Table 1: Variation WordNet measures' results
vide the output of the similarity measures he used to calculate the coefficient. It is unlikely we would have been able to replicate his results at all without the output of this intermediate step. Finally, results for lch, lesk and wup changed according to measure specific configuration settings such as including a PoS-tag specific root node or turning on normalisation.</p>
<p>In the second stage of this research, we ran experiments that systematically manipulate the influential factors described above. In this experiment, we included both the $m c$-set and the complete $r g$ set. The implementation of Spearman $\rho$ used in Pedersen (2010) assigned the lowest number in ranking to ties rather than the mean, resulting in an unjustified drop in results for scores that lead to many ties. We therefore experimented with a different correlation measure, Kendall tau coefficient (Kendall, 1938, $\tau$ ) rather than two versions of Spearman $\rho$.</p>
<h3>3.3 Variation per measure</h3>
<p>All measures varied in their performance. The complete outcome of our experiments (both the similarity measures assigned to each pair as well as the output of the ranking coefficients) are included in the data set provided at http://github.com/antske/ WordNetSimilarity. Table 1 presents an overview of the main point we wish to make through this experiment: the minimal and maximal results according to both ranking coefficients. Results for similarity measures varied from 0.060.42 points for Spearman $\rho$ and from $0.05-0.60$ points for Kendall $\tau$. The last column indicates the variation of performance of a measure</p>
<p>compared to the other measures, where 1 is the best performing measure and 12 is the worst. ${ }^{5}$ For instance, path has been best performing measure, second best, eighth best and all positions in between, vector has ranked first, second and fourth, but also occupied all positions from six to eleven.</p>
<p>In principle, it is to be expected that numbers are not exactly the same while evaluating against a different data set (the $m c$-set versus the $r g$-set), taking a different set of synsets to evaluate on (changing PoS-tag restrictions) or changing configuration settings that influence the similarity score. However, a variation of up to 0.44 points in Spearman $\rho$ and 0.60 in Kendall $\tau^{6}$ leads to the question of how indicative these results really are. A more serious problem is the fact that the comparative performance of individual measure changes. Which measure performs best depends on the evaluation set, ranking coefficient, PoS-tag restrictions and configuration settings. This means that the answer to the question of which similarity measure is best to mimic human similarity scores depends on aspects that are often not even mentioned, let alone systematically compared.</p>
<h3>3.4 Variation per category</h3>
<p>For each influential category of experimental variation, we compared the variation in Spearman $\rho$ and Kendall $\tau$, while similarity measure and other influential categories were kept stable. The categories we varied include WordNet and WordNet::Similarity version, the gold standard used to evaluate, restrictions on PoS-tags, and measure specific configurations. Table 2 presents the maximum variation found across measures for each category. The last column indicates how often the ranking of a specific measure changed as the category changed, e.g. did the measure ranking third using specific configurations, PoS-tag restrictions and a specific gold standard using WordNet 2.1 still rank third when WordNet 3.0 was used instead? The number in parentheses next to the 'different ranks' in the table presents the total number of scores investigated. Note that this number changes for each category, because we com-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Variations per category
pared two WordNet versions (WN version), three gold standard and PoS-tag restriction variations and configuration only for the subset of scores where configuration matters.</p>
<p>There are no definite statements to make as to which version (Patwardhan and Pedersen (2006) vs Pedersen (2010)), PoS-tag restriction or configuration gives the best results. Likewise, while most measures do better on the smaller data set, some achieve their highest results on the full set. This is partially due to the fact that ranking coefficients are sensitive to outliers. In several cases where PoS-tag restrictions led to different results, only one pair received a different score. For instance, path assigns a relatively high score to the pair chord-smile when verbs are included, because the hierarchy of verbs in WordNet is relatively flat. This effect is not observed in wup and lch which correct for the depth of the hierarchy. On the other hand, res, lin and jcn score better on the same set when verbs are considered, because they cannot detect any relatedness for the pair crane-implement when restricted to nouns.</p>
<p>On top of the variations presented above, we notice a discrepancy between the two coefficients. Kendall $\tau$ generally leads to lower coefficiency scores than Spearman $\rho$. Moreover, they each give different relative indications: where lesk achieves its highest Spearman $\rho$, it has an extremely low Kendall $\tau$ of 0.01 . Spearman $\rho$ uses the difference in rank as its basis to calculate a correlation, where Kendall $\tau$ uses the number of items with the correct rank. The low Kendall $\tau$ for lesk is the result of three pairs receiving a score that is too high. Other pairs that get a relatively accurate score are pushed one place down in rank. Because only items that receive the exact same rank help to increase $\tau$, such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman $\rho$ is therefore preferable over Kendall $\tau$. We included $\tau$, because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both $\rho$ and $\tau$ are com-</p>
<p>monly used coefficients.
Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results vary enough to change the identity of the measure that yields the best performance. Table 1 reveals a wide variation in ranking relative to alternative approaches. Results in Table 2 show that it is common for the ranking of a score to change due to variations that are not at the core of the method.</p>
<p>This study shows that it is far from clear how different WordNet similarity measures relate to each other. In fact, we do not know how we can obtain the best results. This is particularly challenging, because the 'best results' may depend on the intended use of the similarity scores (Meng et al., 2013). This is also the reason why we presented the maximum variation observed, rather than the average or typical variation (mostly below 0.10 points). The experiments presented in this paper resulted in a vast amount of data. An elaborate analysis of this data is needed to get a better understanding of how measures work and why results vary to such an extent. We leave this investigation to future work. If there is one takehome message from this experiment, it is that one should experiment with parameters such as restrictions on PoS-tags or configurations and determine which score to use depending on what it is used for, rather than picking something that did best in a study using different data for a different task and may have used a different version of WordNet.</p>
<h2>4 Reproducing a NER method</h2>
<p>Freire et al. (2012) describe an approach to classifying named entities in the cultural heritage domain. The approach is based on the assumption that domain knowledge, encoded in complex features, can aid a machine learning algorithm in NER tasks when only little training data is available. These features include information about person and organisation names, locations, as well as PoS-tags. Additionally, some general features
are used such as a window of three preceding and two following tokens, token length and capitalisation information. Experiments are run in a 10 -fold cross-validation setup using an open source machine learning toolkit (McCallum, 2002).</p>
<h3>4.1 Reproducing NER Experiments</h3>
<p>This experiment can be seen as a real-world case of the sad tale of the Zigglebottom tagger (Pedersen, 2008). The (fictional) Zigglebottom tagger is a tagger with spectacular results that looks like it will solve some major problems in your system. However, the code is not available and a new implementation does not yield the same results. The original authors cannot provide the necessary details to reproduce their results, because most of the work has been done by a PhD student who has finished and moved on to something else. In the end, the newly implemented Zigglebottom tagger is not used, because it does not lead to the promised better results and all effort went to waste.</p>
<p>Van Erp was interested in the NER approach presented in Freire et al. (2012). Unfortunately, the code could not be made available, so she decided to reimplement the approach. Despite feedback from Freire about particular details of the system, results remained 20 points below those reported in Freire et al. (2012) in overall F-score (Van Erp and Van der Meij, 2013).</p>
<p>The reimplementation process involved choices about seemingly small details such as rounding to how many decimals, how to tokenise or how much data cleanup to perform (normalisation of non-alphanumeric characters for example). Trying different parameter combinations for feature generation and the algorithm never yielded the exact same results as Freire et al. (2012). The results of the best run in our first reproduction attempt, together with the original results from Freire et al. (2012) are presented in Table 3. Van Erp and Van der Meij (2013) provide an overview of the implementation efforts.</p>
<h3>4.2 Following up from reproduction</h3>
<p>Since the experiments in Van Erp and Van der Meij (2013) introduce several new research questions regarding the influence of data cleaning and the limitations of the dataset, we performed some additional experiments.</p>
<p>First, we varied the tokenisation, removing nonalphanumeric characters from the data set. This yielded a significantly smaller data set $(10,442$</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">(Freire et al., 2012) results</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Van Erp and Van der Meij's replication results</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">$\mathrm{F}_{\beta=1}$</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">$\mathrm{F}_{\beta=1}$</td>
</tr>
<tr>
<td style="text-align: left;">LOC (388)</td>
<td style="text-align: center;">$92 \%$</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">$77.80 \%$</td>
<td style="text-align: center;">$39.18 \%$</td>
<td style="text-align: center;">52.05</td>
</tr>
<tr>
<td style="text-align: left;">ORG (157)</td>
<td style="text-align: center;">$90 \%$</td>
<td style="text-align: center;">$57 \%$</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">$65.75 \%$</td>
<td style="text-align: center;">$30.57 \%$</td>
<td style="text-align: center;">41.74</td>
</tr>
<tr>
<td style="text-align: left;">PER (614)</td>
<td style="text-align: center;">$91 \%$</td>
<td style="text-align: center;">$56 \%$</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">$73.33 \%$</td>
<td style="text-align: center;">$37.62 \%$</td>
<td style="text-align: center;">49.73</td>
</tr>
<tr>
<td style="text-align: left;">Overall $(1,159)$</td>
<td style="text-align: center;">$91 \%$</td>
<td style="text-align: center;">$55 \%$</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">$73.33 \%$</td>
<td style="text-align: center;">$37.19 \%$</td>
<td style="text-align: center;">49.45</td>
</tr>
</tbody>
</table>
<p>Table 3: Precision, recall and $\mathrm{F}_{\beta=1}$ scores for the original experiments from Freire et al. 2012 and our replication of their approach as presented in Van Erp and Van der Meij (2013)
tokens vs 12,510 ), and a 15 point drop in overall F-score. Then, we investigated whether variation in the cross-validation splits made any difference as we noticed that some NEs were only present in particular fields in the data, which can have a significant impact on a small dataset. We inspected the difference between different crossvalidation folds by computing the standard deviations of the scores and found deviations of up to 25 points in F-score between the 10 splits. In the general setup, database records were randomly distributed over the folds and cut off to balance the fold sizes. In a different approach to dividing the data by distributing individual sentences from the records over the folds, performance increases by 8.57 points in overall F-score to 58.02 . This is not what was done in the original Freire et al. (2012) paper, but shows that the results obtained with this dataset are quite fragile.</p>
<p>As we worried about the complexity of the feature set relative to the size of the data set, we deviated somewhat from Freire et al. (2012)'s experiments in that we switched some features on and off. Removal of complex features pertaining to the window around the focus token improved our results by 3.84 points in overall F-score to 53.39. The complex features based on VIAF, ${ }^{7}$ GeoNames $^{8}$ and WordNet do contribute to the classification in the Mallet setup as removing them and only using the focus token, window and generic features causes a slight drop in overall F-score from 49.45 to 47.25 .</p>
<p>When training the Stanford NER system (Finkel et al., 2005) on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also Van Erp and Van der Meij (2013)), our F-scores come very close to those reported by Freire et al. (2012), but mostly with a higher recall and lower precision. It is puzzling that the Stanford system obtains such high</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>results with only very simple features, whereas for Mallet the complex features show improvement over simpler features. This leads to questions about the differences between the CRF implementations and the influence of their parameters, which we hope to investigate in future work.</p>
<h3>4.3 Reproduction difficulties explained</h3>
<p>Several reasons may be the cause of why we fail to reproduce results. As mentioned, not all resources and data were available for this experiment, thus causing us to navigate in the dark as we could not reverse-engineer intermediate steps, but only compare to the final precision, recall and F-scores.</p>
<p>The experiments follow a general machine learning setup consisting roughly of four steps: preprocess data, generate features, train model and test model. The novelty and replication problems lie in the first three steps. How the data was preprocessed is a major factor here. The data set consisted of XML files marked up with inline named entity tags. In order to generate machine learning features, this data has to be tokenised, possibly cleaned up and the named entity markup had to be converted to a token-based scheme. Each of these steps can be carried out in several ways, and choices made here can have great influence on the rest of the pipeline.</p>
<p>Similar choices have to be made for preprocessing external resources. From the descriptions in the original paper, it is unclear how records in VIAF and GeoNames were preprocessed, or even which versions of these resources were used. Preprocessing and calculating occurrence statistics over VIAF takes 30 hours for each run. It is thus not feasible to identify the main potential variations without the original data to verify this prepatory step.</p>
<p>Numbers had to be rounded when generating the features, leading to the question of how many decimals are required to be discriminative without creating an overly sparse dataset. Freire recalls that encoding features as multi-value discrete fea-</p>
<p>tures versus several boolean features can have significant impact. These settings are not mentioned in the paper, making reproduction very difficult.</p>
<p>As the project in which the original research was performed has ended, and there is no central repository where such information can be retrieved, we are left to wonder how to reuse this approach in order to further domain-specific NER.</p>
<h2>5 Observations</h2>
<p>In this section, we generalise the observations from our use cases to the main categories that can influence reproduction.</p>
<p>Despite our efforts to describe our systems as clearly as possible, details that can make a tremendous difference are often omitted in papers. It will be no surprise to researchers in the field that preprocessing of data can make or break an experiment.</p>
<p>The choice of which steps we perform, and how each of these steps is carried out exactly are part of our experimental setup. A major difference in the results for the NER experiments was caused by variations in the way in which we split the data for cross-validation.</p>
<p>As we fine-tune our techniques, software gets updated, data sets are extended or annotation bugs are fixed. In the WordNet experiment, we found that there were two different gold standard data sets. There are also different versions of WordNet, and the WordNet::Similarity packages. Similarly for the NER experiment, GeoNames, VIAF and Mallet are updated regularly. It is therefore critical to pay attention to versioning.</p>
<p>Our experiments often consist of several different steps whose outputs may be difficult to retrace. In order to check the output of a reproduction experiment at every step of the way, system output of experiments, including intermediate steps, is vital. The WordNet replication was only possible, because Pedersen could provide the similarity scores of each word pair. This enabled us to compare the intermediate output and identify the source of differences in output.</p>
<p>Lastly, there may be inherent system variations in the techniques used. Machine learning algorithms may for instance use coin flips in case of a tie. This was not observed in our experiments, but such variations may be determined by running an experiment several times and taking the average over the different runs (cf. Raeder et al. (2010)).</p>
<p>All together, these observations show that sharing data and software play a key role in gaining insight into how our methods work. Vanschoren et al. (2012) propose a setup that allows researchers to provide their full experimental setup, which should include exact steps followed in preprocessing the data, documentation of the experimental setup, exact versions of the software and resources used and experimental output. Having access to such a setup allows other researchers to validate research, but also tweak the approach to investigate system variation, systematically test the approach in order to learn its limitations and strengths and ultimately improve on it.</p>
<h2>6 Discussion</h2>
<p>Many of the aspects addressed in the previous section such as preprocessing are typically only mentioned in passing, or not at all. There is often not enough space to capture all details, and they are generally not the core of the research described. Still, our use cases have shown that they can have a tremendous impact on reproduction, and can even lead to different conclusions. This leads to serious questions on how we can interpret our results and how we can compare the performance of different methods. Is an improvement of a few per cent really due to the novelty of the approach if larger variations are found when the data is split differently? Is a method that does not quite achieve the highest reported state-of-the-art result truly less good? What does a state-of-the-art result mean if it is only tested on one data set?</p>
<p>If one really wants to know whether a result is better or worse than the state-of-the-art, the range of variation within the state-of-the-art must be known. Systematic experiments such as the ones we carried out for WordNet similarity and NER, can help determine this range. For results that fall within the range, it holds that they can only be judged by evaluations going beyond comparing performance numbers, i.e. an evaluation of how the approach achieves a given result and how that relates to alternative approaches.</p>
<p>Naturally, our use cases do not represent the entire gamut of research methodologies and problems in the NLP community. However, they do represent two core technologies and our observations align with previous literature on replication and reproduction.</p>
<p>Despite the systematic variation we employed</p>
<p>in our experiments, they do not answer all questions that the problems in reproduction evoked. For the WordNet experiments, deeper analysis is required to gain full understanding of how individual influential aspects interact with each measurement. For the NER experiments, we are yet to identify the cause of our failure to reproduce.</p>
<p>The considerable time investment required for such experiments forms a challenge. Due to pressure to publish or other time limitations, they cannot be carried out for each evaluation. Therefore, it is important to share our experiments, so that other researchers (or students) can take this up. This could be stimulated by instituting reproduction tracks in conferences, thus rewarding systematic investigation of research approaches. It can also be aided by adopting initiatives that enable authors to easily include data, code and/or workflows with their publications such as the PLOS/figshare collaboration. ${ }^{9}$ We already do a similar thing for our research problems by organising challenges or shared tasks, why not extend this to systematic testing of our approaches?</p>
<h2>7 Conclusion</h2>
<p>We have presented two reproduction use cases for the NLP domain. We show that repeating other researchers' experiments can lead to new research questions and provide new insights into and better understanding of the investigated techniques.</p>
<p>Our WordNet experiments show that the performance of similarity measures can be influenced by the PoS-tags considered, measure specific variations, the rank coefficient and the gold standard used for comparison. We not only find that such variations lead to different numbers, but also different rankings of the individual measures, i.e. these aspects lead to a different answer to the question as to which measure performs best. We did not succeed in reproducing the NER results of Freire et al. (2012), showing the complexity of what seems a straightforward reproduction case based on a system description and training data only. Our analyses show that it is still an open question whether additional complex features improve domain specific NER and that this may partially depend on the CRF implementation.</p>
<p>Some observations go beyond our use cases. In particular, the fact that results vary significantly</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>because of details that are not made explicit in our publications. Systematic testing can provide an indication of this variation. We have classified relevant aspects in five categories occurring across subdisciplines of NLP: preprocessing, experimental setup, versioning, system output, and system variation.</p>
<p>We believe that knowing the influence of different aspects in our experimental workflow can help increase our understanding of the robustness of the approach at hand and will help understand the meaning of the state-of-the-art better. Some techniques are reused so often (the papers introducing WordNet similarity measures have around 1,0002,000 citations each as of February 2013, for example) that knowing their strengths and weaknesses is essential for optimising their use.</p>
<p>As mentioned many times before, sharing is key to facilitating reuse, even if the code is imperfect and contains hacks and possibly bugs. In the end, the same holds for software as for documentation: it is like sex: if it is good, it is very good and if it is bad, it is better than nothing! ${ }^{10}$ But most of all: when reproduction fails, regardless of whether original code or a reimplementation was used, valuable insights can emerge from investigating the cause of this failure. So don't let your failing reimplementations of the Zigglebottom tagger collect dusk on a shelf while others reimplement their own failing Zigglebottoms. As a community, we need to know where our approaches fail, as much -if not more-as where they succeed.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the anonymous reviewers for their eye to detail and useful comments to make this a better paper. We furthermore thank Ruben Izquierdo, Lourens van der Meij, Christoph Zwirello, Rebecca Dridan and the Semantic Web Group at VU University for their help and useful feedback. The research leading to this paper was supported by the European Union's 7th Framework Programme via the NewsReader Project (ICT-316404), the Agora project, by NWO CATCH programme, grant 640.004.801, and the BiographyNed project, a joint project with Huygens/ING Institute of the Dutch Academy of Sciences funded by the Netherlands eScience Center (http://esciencecenter.nl/).</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>References</h2>
<p>Stanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 805810, Acapulco, August.</p>
<p>Daniel M. Bikel. 2004. Intricacies of Collins' parsing model. Computational Linguistics, 30(4):479-511.</p>
<p>Tomasz Buchert and Lucas Nussbaum. 2012. Leveraging business workflows in distributed systems research for the orchestration of reproducible and scalable experiments. In Anne Etien, editor, 9ème édition de la conférence MAnifestation des JEunes Chercheurs en Sciences et Technologies de l'Information et de la Communication - MajecSTIC 2012 (2012).</p>
<p>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):1347.</p>
<p>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Phd dissertation, University of Pennsylvania.</p>
<p>Irene Cramer. 2008. How well do semantic relatedness measures perform? a meta-study. In Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1, pages 59-70.</p>
<p>Olivier Dalle. 2012. On reproducibility and traceability of simulations. In WSC-Winter Simulation Conference-2012.</p>
<p>Chris Drummond. 2009. Replicability is not reproducibility: nor is it good science. In Proceedings of the Twenty-Sixth International Conference on Machine Learning: Workshop on Evaluation Methods for Machine Learning IV.</p>
<p>Jenny Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363-370, Ann Arbor, USA.</p>
<p>Nuno Freire, José Borbinha, and Pável Calado. 2012. An approach for named entity recognition in poorly structured data. In Proceedings of ESWC 2012.</p>
<p>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In C. Fellbaum, editor, WordNet: An electronic lexical database, pages 305-332. MIT Press.</p>
<p>James Howison and James D. Herbsleb. 2013. Sharing the spoils: incentives and collaboration in scientific software development. In Proceedings of the 2013 conference on Computer Supported Cooperative Work, pages 459-470.</p>
<p>Darrel C. Ince, Leslie Hatton, and John GrahamCumming. 2012. The case for open computer programs. Nature, 482(7386):485-488.</p>
<p>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING X), pages 19-33, Taiwan.</p>
<p>Maurice Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1-2):81-93.</p>
<p>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In C. Fellbaum, editor, WordNet: An electronic lexical database, pages 265-283. MIT Press.</p>
<p>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296-304, Madison, USA.</p>
<p>Panos Louridas and Georgios Gousios. 2012. A note on rigour and replicability. SIGSOFT Softw. Eng. Notes, 37(5):1-4.</p>
<p>Andrew K. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet. cs.umass.edu.</p>
<p>Thilo Mende. 2010. Replication of defect prediction studies: problems, pitfalls and recommendations. In Proceedings of the 6th International Conference on Predictive Models in Software Engineering. ACM.</p>
<p>Lingling Meng, Runqing Huang, and Junzhong Gu. 2013. A review of semantic similarity measures in wordnet. International Journal of Hybrid Information Technology, 6(1):1-12.</p>
<p>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1-28.</p>
<p>Cameron Neylon, Jan Aerts, C Titus Brown, Simon J Coles, Les Hatton, Daniel Lemire, K Jarrod Millman, Peter Murray-Rust, Fernando Perez, Neil Saunders, Nigam Shah, Arfon Smith, Gaël Varoquaux, and Egon Willighagen. 2012. Changing computational research. the challenges ahead. Source Code for Biology and Medicine, 7(2).</p>
<p>Siddharth Patwardhan and Ted Pedersen. 2006. Using wordnet based context vectors to estimate the semantic relatedness of concepts. In Proceedings of the EACL 2006 Workshop Making Sense of Sense Bringing Computational Linguistics and Psycholinguistics Together, pages 1-8, Trento, Italy.</p>
<p>Ted Pedersen. 2008. Empiricism is not a matter of faith. Computational Linguistics, 34(3):465-470.</p>
<p>Ted Pedersen. 2010. Information content measures of semantic similarity perform better without sensetagged text. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010), pages 329-332, Los Angeles, USA.</p>
<p>Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla. 2010. Consequences of variability in classifier performance estimates. In Proceedings of ICDM'2010.</p>
<p>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pages 448-453, Montreal, Canada.</p>
<p>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627-633.</p>
<p>Wheeler Ruml. 2010. The logic of benchmarking: A case against state-of-the-art performance. In Proceedings of the Third Annual Symposium on Combinatorial Search (SOCS-10).</p>
<p>Charles Spearman. 1904. Proof and measurement of association between two things. American Journal of Psychology, 15:72—101.</p>
<p>Marieke Van Erp and Lourens Van der Meij. 2013. Reusable research? a case study in named entity recognition. CLTL 2013-01, Computational Lexicology \&amp; Terminology Lab, VU University Amsterdam.</p>
<p>Joaquin Vanschoren, Hendrik Blockeel, Bernhard Pfahringer, and Geoffrey Holmes. 2012. Experiment databases. Machine Learning, 87(2):127-158.</p>
<p>Piek Vossen, Isa Maks, Roxane Segers, Hennie van der Vliet, Marie-Francine Moens, Katja Hofmann, Erik Tjong Kim Sang, and Maarten de Rijke. 2013. Cornetto: a Combinatorial Lexical Semantic Database for Dutch. In Peter Spyns and Jan Odijk, editors, Essential Speech and Language Technology for Dutch Results by the STEVIN-programme, number XVII in Theory and Applications of Natural Language Processing, chapter 10. Springer.</p>
<p>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 133-138, Las Cruces, USA.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ http://blogs.plos.org/plos/2013/01/ easier-access-to-plos-data/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{10}$ The documentation variant of this quote is attributed to Dick Brandon.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
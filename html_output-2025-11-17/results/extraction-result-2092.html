<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2092 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2092</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2092</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-278739756</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12565v2.pdf" target="_blank">mCLM: A Modular Chemical Language Model that Generates Functional and Makeable Molecules</a></p>
                <p><strong>Paper Abstract:</strong> Despite their ability to understand chemical knowledge, large language models (LLMs) remain limited in their capacity to propose novel molecules with desired functions (e.g., drug-like properties). In addition, the molecules that LLMs propose can often be challenging to make, and are almost never compatible with automated synthesis approaches. To better enable the discovery of functional small molecules, LLMs need to learn a new molecular language that is more effective in predicting properties and inherently synced with automated synthesis technology. Current molecule LLMs are limited by representing molecules based on atoms. In this paper, we argue that just like tokenizing texts into meaning-bearing (sub-)word tokens instead of characters, molecules should be tokenized at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis. This motivates us to propose mCLM, a modular Chemical-Language Model that comprises a bilingual language model that understands both natural language descriptions of functions and molecular blocks. mCLM front-loads synthesizability considerations while improving the predicted functions of molecules in a principled manner. mCLM, with only 3B parameters, achieves improvements in synthetic accessibility relative to 7 other leading generative AI methods including GPT-5. When tested on 122 out-of-distribution medicines using only building blocks/tokens that are compatible with automated modular synthesis, mCLM outperforms all baselines in property scores and synthetic accessibility. mCLM can also reason on multiple functions and iteratively self-improve to rescue drug candidates that failed late in clinical trials ("fallen angels").</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2092.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2092.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mCLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>modular Chemical Language Model (mCLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal autoregressive Transformer that tokenizes molecules into synthesis-robot-friendly building blocks (GNN-encoded) and interleaves them with natural language tokens to generate novel, function-infused, and makeable small molecules; includes an iterative self-refinement (reasoning) loop to improve properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>mCLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multimodal autoregressive transformer (LLM) with GNN-based block embeddings (hybrid LLM+GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry; drug discovery; materials science</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>molecular structures (sequences of building-block tokens), molecule edits/proposals targeting ADMET/properties</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel / out-of-distribution in many tests — model often generates molecules containing building blocks not seen in the 1,000-block training vocabulary (paper reports generalization to unseen blocks in 120/122 and 426/430 FDA tests).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive next-token prediction over a bilingual vocabulary (natural language tokens + GNN-encoded synthesis-friendly molecular building-block tokens); generation proceeds by recombination of building-block tokens (recombination of learned features) and iterative, property-targeted edits (reasoning loop).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>multi-stage computational validation: (1) RDKit syntactic validity checking; (2) heuristic Synthetic Accessibility (SA) score (Ertl & Schuffenhauer) as a quick proxy; (3) Allchemy retrosynthesis software to search for concrete synthetic routes (rigorous, computationally expensive); (4) ensemble oracle models (FARM, ChemBERTA-2, GNN stacked) for ADMET/property prediction (AUC-selected tasks) used to evaluate property changes.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported generation metrics (sampled & evaluated): SA (lower = easier) mean ≈ 2.43 (Table 2 / Table 3), RDKit validity = 100.0%, Allchemy-synthesizability = 98.23%, Makeability (valid × synthesizability) = 98.23% (200 sampled molecules were evaluated per-model with Allchemy under 30 min/molecule limit). On FDA-editing tasks (122 synthesis-guaranteed drugs) mCLM achieved measurable improvements across the 6 ADMET properties compared to the original drugs and outperformed listed baselines in property scores + synthetic accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Property-oracle validation: the paper used an ensemble oracle (see separate entry) with AUCs ranging from 0.84 to 0.99 on the selected ADMET tasks; Allchemy retrosynthesis found routes for ≈98.1% of FDA-approved molecules and ≈98.2% of mCLM-generated molecules in the sampled retrosynthesis experiments. The paper reports mCLM produces larger average property improvements than baselines on tested datasets, but does not give absolute ground-truth experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported explicitly. Indirectly: RDKit validity = 100% (no syntactic invalids in reported sample); Allchemy failed to find routes for ≈1.8% of mCLM-valid molecules in the sampled retrosynthesis experiments (i.e., 100% valid but 98.23% synthesizable → 1.77% valid molecules judged unsynthesizable by Allchemy). The authors do not report a formal false-positive rate (invalid outputs incorrectly labeled valid by validators) or uncertainty in these rates.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported. The paper does not provide an estimate of cases where the validator (e.g., Allchemy) reports 'no route' but the molecule is actually synthesizable in practice (false negatives for the retrosynthesis tool).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper reports successful generalization to out-of-distribution/unseen blocks (e.g., improvements on 122/430 FDA drugs whose blocks were largely unseen during training) and claims consistent property improvements and high synthesizability in those OOD tests, but it does not provide a continuous quantitative curve showing validation performance as a function of novelty; detailed degradation vs novelty is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Yes — the paper explicitly compares generation capability (ability to propose property-improved molecules) to validation capability: mCLM-generated molecules were validated via RDKit, SA, and Allchemy and then scored by oracle property models. Results show an asymmetry: many prior atom-level generators produce molecules that are syntactically invalid or unsynthesizable (lower validity and synthesizability), whereas mCLM narrows that gap by building synthesizability into the generation tokenization step. The paper demonstrates that mCLM's generation quality (in terms of makeability) exceeds baselines (e.g., mCLM makeability ≈98.2% vs MoleculeSTM ≈85.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported — mCLM training/inference as described does not provide calibrated uncertainty estimates for its molecular proposals; no predictive posterior or calibrated confidence scores are reported for generated molecules or property predictions beyond the use of separate oracle models whose thresholds are set on validation data.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; no calibration metrics (e.g., expected calibration error) are provided for generation confidence or for the model's property-confidence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Paper reports OOD experiments: (a) 122 FDA-approved drugs composed of synthesis-guaranteed blocks (most contained unseen blocks relative to training) where mCLM improved all 6 ADMET properties vs baseline drugs; (b) larger 430-drug test (non-synthesis-guaranteed) where mCLM improved 5/6 properties; no numeric per-task OOD AUC curves provided, but aggregate property improvement and SA/Allchemy metrics indicate maintained high synthesizability and property gains on OOD examples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the paper relies on proxies: RDKit syntactic validity (structural correctness), Ertl Synthetic Accessibility (SA) score as a quick heuristic, and Allchemy retrosynthesis as a stronger computational proxy for real-world makeability; property validation uses ensemble ML oracles (FARM, ChemBERTA-2, GNN stacked) trained and validated on TDC benchmark splits.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Not quantified. The paper recommends human-in-the-loop and an autonomous lab with human oversight; suggests human validation is especially required for late-stage/novel candidates and physical experiments, but no numeric frequency is given.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (drug discovery / materials) — the domain is experimental and empirical rather than formally axiomatized, which the authors argue contributes to a generation-validation gap that needs experimental/retrosynthetic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Design-time mitigation: tokenize molecules into synthesis-guaranteed, function-infused building blocks (ensures generated sequences map to automatable synthesis steps); conservative synthesis-guaranteed tokenizer (permits only predefined automated disconnections: amide coupling, Suzuki, Buchwald-Hartwig) with rule-based fallback; use of GNN-encoded block embeddings and ensemble property oracles; computationally intensive retrosynthetic checking with Allchemy as a post-generation gating step; iterative self-refinement focusing on properties. Effectiveness: these strategies yield higher validity, SA, and Allchemy-synthesizability than baselines (reported makeability ≈98.2% vs baselines ≈84–85%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper cites and demonstrates that atom-level/SMILES-based generators frequently propose syntactically invalid or non-synthesizable molecules; empirical comparison shows baselines (e.g., MoleculeSTM, FineMolTex) have lower RDKit validity (≈93.8–94.2%) and lower Allchemy-synthesizability/makeability (e.g., MoleculeSTM makeability ≈85.4%) compared to mCLM (100% valid, ≈98.2% synthesizable). The authors argue this demonstrates generation outpacing practical validation (synthesizability) when synthesis constraints are not built into generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No direct contradiction; Allchemy is able to find synthetic routes for a very high fraction of FDA-approved molecules (98.1%), indicating that retrosynthesis tools can validate many complex molecules — but the paper still shows generator designs that ignore synthesis constraints produce many proposals that fail validation. The paper does not present evidence that validation methods routinely exceed generation ability for novel chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Qualitative: validation (Allchemy retrosynthesis) is computationally expensive — authors allocated up to 30 minutes per molecule on a supercomputing cluster for retrosynthetic search; generation (mCLM inference) is not precisely timed in the paper but is orders of magnitude faster (near real-time/inference-time). The paper therefore indicates validation cost >> generation cost (validation on the order of many minutes per molecule vs milliseconds-to-seconds for generation), but no single numeric ratio is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2092.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2092.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Allchemy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Allchemy retrosynthesis software (state-of-the-art retrosynthesis engine referenced by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational retrosynthesis engine used in this paper as a rigorous (but computationally expensive) validator to determine whether a generated molecule has a plausible synthetic route according to available reaction knowledge and starting materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Allchemy</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrosynthesis planning/search software (rule + data-driven retrosynthesis engine)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry; synthetic planning; cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic route predictions; binary synthesizability decision (route found / not found)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>NA (validation tool rather than generative novelty engine)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>search over retrosynthetic disconnection rules, reaction databases and heuristics to construct synthetic routes (detailed algorithmic internals not described in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as a validation oracle — declares a molecule 'synthesizable' when a retrosynthetic route is found under allotted compute/time and substrate price constraints (authors used generous settings: 30 min per molecule, no substrate price limit for sampled experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (tool is used for validation). In the paper Allchemy found synthetic routes for ≈98.1% of FDA-approved molecules and ≈98.2% of mCLM-generated molecules in sampled experiments (200 molecules per-model).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported success-finding-route rates: ~98.1% on FDA-approved molecules (as a check) and ~98.2% for mCLM-generated sample; detailed accuracy, precision, or benchmarking of Allchemy vs ground-truth experimental syntheses is not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported. Paper does not quantify cases where Allchemy finds a route but the molecule is not practically synthesizable in the lab (i.e., false positives), nor does it quantify false negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not explicitly reported. Allchemy was used on both in-distribution (FDA drugs) and model-generated OOD molecules; it successfully found routes for the vast majority in both cases according to reported percentages, but no systematic breakdown versus novelty is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The paper uses Allchemy to demonstrate a validation bottleneck (expensive, slower) compared to generation, but Allchemy itself is not compared against other validation engines in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Allchemy successfully found routes for a high fraction of OOD/generated molecules in sampled experiments (~98% success), but systematic OOD analysis by difficulty/novel chemistry type is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Allchemy is treated as a near-gold-standard computational retrosynthesis validator in the paper (stronger than heuristic SA score), rather than a proxy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (synthetic chemistry planning) — not formally axiomatized; relies on reaction knowledge and heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Allchemy is used as a gate to filter generated molecules before claiming makeability; authors advocate for integrating synthesis-aware tokenization into generation to reduce the need for expensive retrosynthetic checking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper emphasizes that atom-level generators produce many molecules that fail synthesizability checks, and uses Allchemy to quantify synthesizability — showing a lower makeability for baselines vs mCLM.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Allchemy finds routes for nearly all FDA drugs and nearly all mCLM outputs in sampled tests, indicating that computational retrosynthesis can often validate complex molecules; but the paper does not interpret this as negating the generation-validation gap because Allchemy is expensive and generation without synthesis constraints still produces many invalid/unsynthesizable candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Validation cost is high: the paper used up to 30 minutes per molecule on a supercomputing cluster (no substrate price limit) for exhaustive Allchemy retrosynthetic search; generation (inference) cost is not specified but is much lower, indicating validation is orders of magnitude more compute-intensive than generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2092.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2092.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble oracle models (FARM + ChemBERTA-2 + GNN stacking)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble stacking model built from FARM (SMILES-based BERT with functional-group awareness), ChemBERTA-2 (large transformer on SMILES), and a GNN; used to generate high-quality synthetic labels for ADMET property tasks and as a property-evaluation oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ensemble Oracle (FARM + ChemBERTA-2 + GNN, stacked)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>ensemble of pretrained deep models (transformers + GNN) with a meta-learner</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cheminformatics; ADMET property prediction for drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>binary or probabilistic ADMET property predictions (AMES, BBBP, CYP3A4, DILI, HIA, PGP)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution for the selected TDC tasks (paper selected tasks where standard models achieve AUC > 0.80); used as synthetic-label generator rather than novelty engine.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>feature extraction from each base model (FARM, ChemBERTA-2, GNN) followed by concatenation and a fully connected meta-learner (stacking). Thresholds are chosen to maximize F1 on validation splits.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated on Therapeutics Data Commons (TDC) scaffold-based splits; selection criterion: only retain ADMET tasks where standard models achieve AUC > 0.80. Performance summarized in paper Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (oracle for validation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported ensemble AUCs (Table 7): AMES 0.89, Pgp 0.91, DILI 0.84, BBBP 0.93, CYP3A4 0.89, HIA 0.99. Thresholds selected to maximize F1 on validation sets; these models were used to label synthetic training data and to evaluate property changes for generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported; thresholds chosen to maximize F1 imply a balance of precision/recall, but numeric FPRs are not listed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper restricts tasks to those with predictable signals (AUC > 0.80) to reduce oracle error; generalization to molecules far outside TDC chemical space is not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The ensemble is used as a property-evaluation oracle to validate mCLM's proposals. The paper acknowledges the danger of oracle models being out-of-distribution and propagating errors, and therefore selects only reliably predictable tasks and uses stacking to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described; ensemble outputs are thresholded to binary labels using validation-maximized F1; probabilistic outputs exist but calibration analyses are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; the ensemble was trained/validated on TDC scaffold splits to encourage generalization, but explicit OOD performance on novel generated molecules is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>The ensemble itself is a learned proxy for wet-lab property endpoints; the authors explicitly note that these are proxies and prefer tasks with good baseline AUCs to keep label quality high.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (ADMET prediction using statistical models) — inherently probabilistic and empirical.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Selection of only those ADMET tasks with reliable predictability (AUC > 0.80) and stacking multiple model types to improve label quality; thresholds tuned on validation to control precision/recall trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper notes oracle models can be OOD for generated molecules and low-quality oracles can propagate errors; hence they used conservative task selection and ensemble stacking to mitigate this generation-to-oracle mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High reported ensemble AUCs for selected tasks indicate the oracles are reasonably reliable for those ADMET endpoints on TDC splits, mitigating some generation-validation mismatch for those tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Oracle prediction is relatively cheap (standard ML inference) compared to Allchemy retrosynthesis; exact compute costs are not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2092.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2092.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines (MoleculeSTM, FineMolTex, GPT-4o, GPT-5, Gemini-2.5-F, LDMol, Claude-3.5-H)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative baseline molecule-generation systems and large language models referenced and compared in the study</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of prior generative systems (specialized molecule editors/generators and general multimodal LLMs) used as baselines for molecule editing and generation; many are text-based or SMILES-based and do not enforce synthesis guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Baselines (MoleculeSTM, FineMolTex, GPT-4o, GPT-5, Gemini-2.5-F, LDMol, Claude-3.5-H)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>varied — text-guided molecule editors (auto-regressive / diffusion), general large multimodal LLMs, text-to-molecule diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational chemistry; molecule generation; generative AI</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>molecular structures (SMILES or other molecular representations); property-targeted edits</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>range from in-distribution edits to moderately novel molecules; however they often produce a higher fraction of syntactically invalid or unsynthesizable outputs compared to mCLM according to the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>varied — autoregressive editing (MoleculeSTM), diffusion-based generation (LDMol), large pretrained LLM decoding (GPT-4o, GPT-5, Claude-3.5-H, Gemini), and other model-specific mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In the paper, baselines are validated with the same pipeline: RDKit validity, SA score, and Allchemy retrosynthesis on sampled outputs; property changes evaluated with the same oracle ensemble where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Selected reported numbers (Table 2): MoleculeSTM: SA ≈ 2.649, RDKit Validity ≈ 93.8%, Allchemy Synthesizability ≈ 91.03%, Makeability ≈ 85.39%. FineMolTex: SA ≈ 2.589, Validity ≈ 94.2%, Synthesizability ≈ 90.15%, Makeability ≈ 84.96%. The paper states average property improvements of these baselines fall behind mCLM.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Baseline-generated molecules have lower RDKit validity and lower Allchemy-synthesizability/makeability than mCLM in the reported samples (see above). Property improvement performance is reported as lower on average than mCLM, but detailed per-task numeric comparisons are provided in paper tables (no ground-truth wet-lab validation).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported. RDKit validity percentages imply an invalid-output rate (e.g., MoleculeSTM ~6.2% invalid syntactic outputs), but explicit false-positive rates for validators are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Baselines tend to produce more invalid/unsynthesizable outputs when not constrained by synthesis-aware tokenization; the paper does not provide a numerical novelty-vs-performance curve for baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper compares baseline generation to validation (RDKit/SA/Allchemy) and reports that lack of synthesis constraints in baseline generation leads to lower makeability and more syntactic errors compared to mCLM which integrates synthesis constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported for baselines in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Baselines were allowed to generate without synthesis guarantees; in OOD tests they produced lower makeability and sometimes invalid molecules (examples: MoleculeSTM generated syntactically incorrect molecules in some fallen-angel experiments). Specific OOD metrics beyond those in tables are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Same proxies used as for mCLM: RDKit validity, SA score, Allchemy retrosynthesis for sampled molecules; property oracles used to estimate ADMET changes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (drug discovery); methods are heuristic/data-driven and not formally verified.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Baselines generally do not include synthesis-guaranteed tokenization; the paper does not report additional gap-mitigation strategies applied to these baselines, though some (like LDMol) use structure-aware latent spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Baseline results demonstrate lower RDKit validity and lower Allchemy-synthesizability/makeability (see numbers above), which the authors use to support the claim that generation methods without synthesis constraints create a practical gap between in-silico generation and real-world makeability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Not presented in this paper; some baselines do produce valid/synthesizable structures for many cases but on average underperform mCLM in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported per-baseline; Allchemy validation cost applies equally to baseline outputs when tested (30 minutes per molecule allotted in experiments). Generation cost per baseline model is not specified in the paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2092.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2092.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDKit + SA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDKit syntactic validity checker and Ertl Synthetic Accessibility (SA) score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lightweight cheminformatics tools used as fast, low-cost proxies for molecule correctness (RDKit structural validity) and synthetic accessibility (Ertl SA score) prior to heavier retrosynthetic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RDKit validity check + Ertl SA score</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>cheminformatics toolkit (rule-based syntax checks) + heuristic scoring function</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry / cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>binary syntactic validity flag (RDKit); continuous SA score (heuristic estimate of synthetic accessibility; lower is easier)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>NA (validation/proxy tools, not generative)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Used as first-pass validation: RDKit to check whether SMILES/structures are syntactically correct; SA score as a heuristic to rank likely synthesizability before running Allchemy retrosynthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported sample metrics: mCLM RDKit validity = 100.0% and mean SA ~2.41; baseline models lower validity (e.g., MoleculeSTM ~93.8% validity) and slightly worse SA scores. SA and RDKit provide cheap, automated signals but are recognized as imperfect proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported. RDKit can miss chemically invalid but syntactically correct structures, and SA is heuristic — paper does not provide explicit false-positive/negative calibration for these proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not analyzed quantitatively; proxies are expected to be less informative for highly novel chemistries, but the paper does not quantify this.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used as a cheap filter prior to Allchemy; paper demonstrates that relying only on these proxies is insufficient for strong synthesis claims and thus uses Allchemy as a more rigorous validator.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified specifically; authors use SA + RDKit as initial checks across in-distribution and OOD tests but caution that stronger validation is required for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — RDKit validity and SA score are the primary lightweight proxies used in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Serve as fast filters to triage generated molecules before expensive Allchemy runs; the authors still treat them as proxies and favor Allchemy for rigorous claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper argues that reliance on cheap proxies (RDKit/SA) alone can overestimate practical makeability, motivating the use of synthesis-aware generation (mCLM) and expensive retrosynthetic checking.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Computer-designed repurposing of chemical wastes into drugs <em>(Rating: 2)</em></li>
                <li>Automated iterative c(sp3)-C bond formation <em>(Rating: 2)</em></li>
                <li>Artificial intelligence for retrosynthetic planning needs both data and expert knowledge <em>(Rating: 2)</em></li>
                <li>Ldmol: A text-to-molecule diffusion model with structurally informative latent space surpasses ar models <em>(Rating: 2)</em></li>
                <li>DrugAssist: A large language model for molecule optimization <em>(Rating: 2)</em></li>
                <li>MoleculeSTM <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2092",
    "paper_id": "paper-278739756",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "mCLM",
            "name_full": "modular Chemical Language Model (mCLM)",
            "brief_description": "A multimodal autoregressive Transformer that tokenizes molecules into synthesis-robot-friendly building blocks (GNN-encoded) and interleaves them with natural language tokens to generate novel, function-infused, and makeable small molecules; includes an iterative self-refinement (reasoning) loop to improve properties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "mCLM",
            "system_type": "multimodal autoregressive transformer (LLM) with GNN-based block embeddings (hybrid LLM+GNN)",
            "scientific_domain": "chemistry; drug discovery; materials science",
            "output_type": "molecular structures (sequences of building-block tokens), molecule edits/proposals targeting ADMET/properties",
            "novelty_level": "moderately novel / out-of-distribution in many tests — model often generates molecules containing building blocks not seen in the 1,000-block training vocabulary (paper reports generalization to unseen blocks in 120/122 and 426/430 FDA tests).",
            "generation_method": "autoregressive next-token prediction over a bilingual vocabulary (natural language tokens + GNN-encoded synthesis-friendly molecular building-block tokens); generation proceeds by recombination of building-block tokens (recombination of learned features) and iterative, property-targeted edits (reasoning loop).",
            "validation_method": "multi-stage computational validation: (1) RDKit syntactic validity checking; (2) heuristic Synthetic Accessibility (SA) score (Ertl & Schuffenhauer) as a quick proxy; (3) Allchemy retrosynthesis software to search for concrete synthetic routes (rigorous, computationally expensive); (4) ensemble oracle models (FARM, ChemBERTA-2, GNN stacked) for ADMET/property prediction (AUC-selected tasks) used to evaluate property changes.",
            "generation_performance": "Reported generation metrics (sampled & evaluated): SA (lower = easier) mean ≈ 2.43 (Table 2 / Table 3), RDKit validity = 100.0%, Allchemy-synthesizability = 98.23%, Makeability (valid × synthesizability) = 98.23% (200 sampled molecules were evaluated per-model with Allchemy under 30 min/molecule limit). On FDA-editing tasks (122 synthesis-guaranteed drugs) mCLM achieved measurable improvements across the 6 ADMET properties compared to the original drugs and outperformed listed baselines in property scores + synthetic accessibility.",
            "validation_performance": "Property-oracle validation: the paper used an ensemble oracle (see separate entry) with AUCs ranging from 0.84 to 0.99 on the selected ADMET tasks; Allchemy retrosynthesis found routes for ≈98.1% of FDA-approved molecules and ≈98.2% of mCLM-generated molecules in the sampled retrosynthesis experiments. The paper reports mCLM produces larger average property improvements than baselines on tested datasets, but does not give absolute ground-truth experimental validation.",
            "false_positive_rate": "Not reported explicitly. Indirectly: RDKit validity = 100% (no syntactic invalids in reported sample); Allchemy failed to find routes for ≈1.8% of mCLM-valid molecules in the sampled retrosynthesis experiments (i.e., 100% valid but 98.23% synthesizable → 1.77% valid molecules judged unsynthesizable by Allchemy). The authors do not report a formal false-positive rate (invalid outputs incorrectly labeled valid by validators) or uncertainty in these rates.",
            "false_negative_rate": "Not reported. The paper does not provide an estimate of cases where the validator (e.g., Allchemy) reports 'no route' but the molecule is actually synthesizable in practice (false negatives for the retrosynthesis tool).",
            "performance_vs_novelty": "Paper reports successful generalization to out-of-distribution/unseen blocks (e.g., improvements on 122/430 FDA drugs whose blocks were largely unseen during training) and claims consistent property improvements and high synthesizability in those OOD tests, but it does not provide a continuous quantitative curve showing validation performance as a function of novelty; detailed degradation vs novelty is not reported.",
            "generation_validation_comparison": "Yes — the paper explicitly compares generation capability (ability to propose property-improved molecules) to validation capability: mCLM-generated molecules were validated via RDKit, SA, and Allchemy and then scored by oracle property models. Results show an asymmetry: many prior atom-level generators produce molecules that are syntactically invalid or unsynthesizable (lower validity and synthesizability), whereas mCLM narrows that gap by building synthesizability into the generation tokenization step. The paper demonstrates that mCLM's generation quality (in terms of makeability) exceeds baselines (e.g., mCLM makeability ≈98.2% vs MoleculeSTM ≈85.4%).",
            "uncertainty_quantification": "Not reported — mCLM training/inference as described does not provide calibrated uncertainty estimates for its molecular proposals; no predictive posterior or calibrated confidence scores are reported for generated molecules or property predictions beyond the use of separate oracle models whose thresholds are set on validation data.",
            "calibration_quality": "Not reported; no calibration metrics (e.g., expected calibration error) are provided for generation confidence or for the model's property-confidence outputs.",
            "out_of_distribution_performance": "Paper reports OOD experiments: (a) 122 FDA-approved drugs composed of synthesis-guaranteed blocks (most contained unseen blocks relative to training) where mCLM improved all 6 ADMET properties vs baseline drugs; (b) larger 430-drug test (non-synthesis-guaranteed) where mCLM improved 5/6 properties; no numeric per-task OOD AUC curves provided, but aggregate property improvement and SA/Allchemy metrics indicate maintained high synthesizability and property gains on OOD examples.",
            "validation_proxy_metrics": "Yes — the paper relies on proxies: RDKit syntactic validity (structural correctness), Ertl Synthetic Accessibility (SA) score as a quick heuristic, and Allchemy retrosynthesis as a stronger computational proxy for real-world makeability; property validation uses ensemble ML oracles (FARM, ChemBERTA-2, GNN stacked) trained and validated on TDC benchmark splits.",
            "human_validation_required": true,
            "human_validation_frequency": "Not quantified. The paper recommends human-in-the-loop and an autonomous lab with human oversight; suggests human validation is especially required for late-stage/novel candidates and physical experiments, but no numeric frequency is given.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (drug discovery / materials) — the domain is experimental and empirical rather than formally axiomatized, which the authors argue contributes to a generation-validation gap that needs experimental/retrosynthetic validation.",
            "gap_mitigation_strategies": "Design-time mitigation: tokenize molecules into synthesis-guaranteed, function-infused building blocks (ensures generated sequences map to automatable synthesis steps); conservative synthesis-guaranteed tokenizer (permits only predefined automated disconnections: amide coupling, Suzuki, Buchwald-Hartwig) with rule-based fallback; use of GNN-encoded block embeddings and ensemble property oracles; computationally intensive retrosynthetic checking with Allchemy as a post-generation gating step; iterative self-refinement focusing on properties. Effectiveness: these strategies yield higher validity, SA, and Allchemy-synthesizability than baselines (reported makeability ≈98.2% vs baselines ≈84–85%).",
            "evidence_supporting_gap": "Paper cites and demonstrates that atom-level/SMILES-based generators frequently propose syntactically invalid or non-synthesizable molecules; empirical comparison shows baselines (e.g., MoleculeSTM, FineMolTex) have lower RDKit validity (≈93.8–94.2%) and lower Allchemy-synthesizability/makeability (e.g., MoleculeSTM makeability ≈85.4%) compared to mCLM (100% valid, ≈98.2% synthesizable). The authors argue this demonstrates generation outpacing practical validation (synthesizability) when synthesis constraints are not built into generation.",
            "evidence_contradicting_gap": "No direct contradiction; Allchemy is able to find synthetic routes for a very high fraction of FDA-approved molecules (98.1%), indicating that retrosynthesis tools can validate many complex molecules — but the paper still shows generator designs that ignore synthesis constraints produce many proposals that fail validation. The paper does not present evidence that validation methods routinely exceed generation ability for novel chemistry.",
            "computational_cost_ratio": "Qualitative: validation (Allchemy retrosynthesis) is computationally expensive — authors allocated up to 30 minutes per molecule on a supercomputing cluster for retrosynthetic search; generation (mCLM inference) is not precisely timed in the paper but is orders of magnitude faster (near real-time/inference-time). The paper therefore indicates validation cost &gt;&gt; generation cost (validation on the order of many minutes per molecule vs milliseconds-to-seconds for generation), but no single numeric ratio is provided.",
            "uuid": "e2092.0"
        },
        {
            "name_short": "Allchemy",
            "name_full": "Allchemy retrosynthesis software (state-of-the-art retrosynthesis engine referenced by authors)",
            "brief_description": "A computational retrosynthesis engine used in this paper as a rigorous (but computationally expensive) validator to determine whether a generated molecule has a plausible synthetic route according to available reaction knowledge and starting materials.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Allchemy",
            "system_type": "retrosynthesis planning/search software (rule + data-driven retrosynthesis engine)",
            "scientific_domain": "chemistry; synthetic planning; cheminformatics",
            "output_type": "synthetic route predictions; binary synthesizability decision (route found / not found)",
            "novelty_level": "NA (validation tool rather than generative novelty engine)",
            "generation_method": "search over retrosynthetic disconnection rules, reaction databases and heuristics to construct synthetic routes (detailed algorithmic internals not described in paper).",
            "validation_method": "Used as a validation oracle — declares a molecule 'synthesizable' when a retrosynthetic route is found under allotted compute/time and substrate price constraints (authors used generous settings: 30 min per molecule, no substrate price limit for sampled experiments).",
            "generation_performance": "Not applicable (tool is used for validation). In the paper Allchemy found synthetic routes for ≈98.1% of FDA-approved molecules and ≈98.2% of mCLM-generated molecules in sampled experiments (200 molecules per-model).",
            "validation_performance": "Reported success-finding-route rates: ~98.1% on FDA-approved molecules (as a check) and ~98.2% for mCLM-generated sample; detailed accuracy, precision, or benchmarking of Allchemy vs ground-truth experimental syntheses is not reported in this paper.",
            "false_positive_rate": "Not reported. Paper does not quantify cases where Allchemy finds a route but the molecule is not practically synthesizable in the lab (i.e., false positives), nor does it quantify false negatives.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Not explicitly reported. Allchemy was used on both in-distribution (FDA drugs) and model-generated OOD molecules; it successfully found routes for the vast majority in both cases according to reported percentages, but no systematic breakdown versus novelty is provided.",
            "generation_validation_comparison": "The paper uses Allchemy to demonstrate a validation bottleneck (expensive, slower) compared to generation, but Allchemy itself is not compared against other validation engines in detail here.",
            "uncertainty_quantification": "Not reported in this paper.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Allchemy successfully found routes for a high fraction of OOD/generated molecules in sampled experiments (~98% success), but systematic OOD analysis by difficulty/novel chemistry type is not provided.",
            "validation_proxy_metrics": "Allchemy is treated as a near-gold-standard computational retrosynthesis validator in the paper (stronger than heuristic SA score), rather than a proxy metric.",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (synthetic chemistry planning) — not formally axiomatized; relies on reaction knowledge and heuristics.",
            "gap_mitigation_strategies": "Allchemy is used as a gate to filter generated molecules before claiming makeability; authors advocate for integrating synthesis-aware tokenization into generation to reduce the need for expensive retrosynthetic checking.",
            "evidence_supporting_gap": "Paper emphasizes that atom-level generators produce many molecules that fail synthesizability checks, and uses Allchemy to quantify synthesizability — showing a lower makeability for baselines vs mCLM.",
            "evidence_contradicting_gap": "Allchemy finds routes for nearly all FDA drugs and nearly all mCLM outputs in sampled tests, indicating that computational retrosynthesis can often validate complex molecules; but the paper does not interpret this as negating the generation-validation gap because Allchemy is expensive and generation without synthesis constraints still produces many invalid/unsynthesizable candidates.",
            "computational_cost_ratio": "Validation cost is high: the paper used up to 30 minutes per molecule on a supercomputing cluster (no substrate price limit) for exhaustive Allchemy retrosynthetic search; generation (inference) cost is not specified but is much lower, indicating validation is orders of magnitude more compute-intensive than generation.",
            "uuid": "e2092.1"
        },
        {
            "name_short": "Oracle Ensemble",
            "name_full": "Ensemble oracle models (FARM + ChemBERTA-2 + GNN stacking)",
            "brief_description": "An ensemble stacking model built from FARM (SMILES-based BERT with functional-group awareness), ChemBERTA-2 (large transformer on SMILES), and a GNN; used to generate high-quality synthetic labels for ADMET property tasks and as a property-evaluation oracle.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Ensemble Oracle (FARM + ChemBERTA-2 + GNN, stacked)",
            "system_type": "ensemble of pretrained deep models (transformers + GNN) with a meta-learner",
            "scientific_domain": "cheminformatics; ADMET property prediction for drug discovery",
            "output_type": "binary or probabilistic ADMET property predictions (AMES, BBBP, CYP3A4, DILI, HIA, PGP)",
            "novelty_level": "in-distribution for the selected TDC tasks (paper selected tasks where standard models achieve AUC &gt; 0.80); used as synthetic-label generator rather than novelty engine.",
            "generation_method": "feature extraction from each base model (FARM, ChemBERTA-2, GNN) followed by concatenation and a fully connected meta-learner (stacking). Thresholds are chosen to maximize F1 on validation splits.",
            "validation_method": "Validated on Therapeutics Data Commons (TDC) scaffold-based splits; selection criterion: only retain ADMET tasks where standard models achieve AUC &gt; 0.80. Performance summarized in paper Table 7.",
            "generation_performance": "Not applicable (oracle for validation).",
            "validation_performance": "Reported ensemble AUCs (Table 7): AMES 0.89, Pgp 0.91, DILI 0.84, BBBP 0.93, CYP3A4 0.89, HIA 0.99. Thresholds selected to maximize F1 on validation sets; these models were used to label synthetic training data and to evaluate property changes for generated molecules.",
            "false_positive_rate": "Not explicitly reported; thresholds chosen to maximize F1 imply a balance of precision/recall, but numeric FPRs are not listed in the paper.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Paper restricts tasks to those with predictable signals (AUC &gt; 0.80) to reduce oracle error; generalization to molecules far outside TDC chemical space is not quantified here.",
            "generation_validation_comparison": "The ensemble is used as a property-evaluation oracle to validate mCLM's proposals. The paper acknowledges the danger of oracle models being out-of-distribution and propagating errors, and therefore selects only reliably predictable tasks and uses stacking to improve robustness.",
            "uncertainty_quantification": "Not described; ensemble outputs are thresholded to binary labels using validation-maximized F1; probabilistic outputs exist but calibration analyses are not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not quantified; the ensemble was trained/validated on TDC scaffold splits to encourage generalization, but explicit OOD performance on novel generated molecules is not provided.",
            "validation_proxy_metrics": "The ensemble itself is a learned proxy for wet-lab property endpoints; the authors explicitly note that these are proxies and prefer tasks with good baseline AUCs to keep label quality high.",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (ADMET prediction using statistical models) — inherently probabilistic and empirical.",
            "gap_mitigation_strategies": "Selection of only those ADMET tasks with reliable predictability (AUC &gt; 0.80) and stacking multiple model types to improve label quality; thresholds tuned on validation to control precision/recall trade-offs.",
            "evidence_supporting_gap": "Paper notes oracle models can be OOD for generated molecules and low-quality oracles can propagate errors; hence they used conservative task selection and ensemble stacking to mitigate this generation-to-oracle mismatch.",
            "evidence_contradicting_gap": "High reported ensemble AUCs for selected tasks indicate the oracles are reasonably reliable for those ADMET endpoints on TDC splits, mitigating some generation-validation mismatch for those tasks.",
            "computational_cost_ratio": "Oracle prediction is relatively cheap (standard ML inference) compared to Allchemy retrosynthesis; exact compute costs are not provided.",
            "uuid": "e2092.2"
        },
        {
            "name_short": "Baselines (MoleculeSTM, FineMolTex, GPT-4o, GPT-5, Gemini-2.5-F, LDMol, Claude-3.5-H)",
            "name_full": "Representative baseline molecule-generation systems and large language models referenced and compared in the study",
            "brief_description": "A set of prior generative systems (specialized molecule editors/generators and general multimodal LLMs) used as baselines for molecule editing and generation; many are text-based or SMILES-based and do not enforce synthesis guarantees.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Baselines (MoleculeSTM, FineMolTex, GPT-4o, GPT-5, Gemini-2.5-F, LDMol, Claude-3.5-H)",
            "system_type": "varied — text-guided molecule editors (auto-regressive / diffusion), general large multimodal LLMs, text-to-molecule diffusion models",
            "scientific_domain": "computational chemistry; molecule generation; generative AI",
            "output_type": "molecular structures (SMILES or other molecular representations); property-targeted edits",
            "novelty_level": "range from in-distribution edits to moderately novel molecules; however they often produce a higher fraction of syntactically invalid or unsynthesizable outputs compared to mCLM according to the paper's experiments.",
            "generation_method": "varied — autoregressive editing (MoleculeSTM), diffusion-based generation (LDMol), large pretrained LLM decoding (GPT-4o, GPT-5, Claude-3.5-H, Gemini), and other model-specific mechanisms.",
            "validation_method": "In the paper, baselines are validated with the same pipeline: RDKit validity, SA score, and Allchemy retrosynthesis on sampled outputs; property changes evaluated with the same oracle ensemble where applicable.",
            "generation_performance": "Selected reported numbers (Table 2): MoleculeSTM: SA ≈ 2.649, RDKit Validity ≈ 93.8%, Allchemy Synthesizability ≈ 91.03%, Makeability ≈ 85.39%. FineMolTex: SA ≈ 2.589, Validity ≈ 94.2%, Synthesizability ≈ 90.15%, Makeability ≈ 84.96%. The paper states average property improvements of these baselines fall behind mCLM.",
            "validation_performance": "Baseline-generated molecules have lower RDKit validity and lower Allchemy-synthesizability/makeability than mCLM in the reported samples (see above). Property improvement performance is reported as lower on average than mCLM, but detailed per-task numeric comparisons are provided in paper tables (no ground-truth wet-lab validation).",
            "false_positive_rate": "Not explicitly reported. RDKit validity percentages imply an invalid-output rate (e.g., MoleculeSTM ~6.2% invalid syntactic outputs), but explicit false-positive rates for validators are not given.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Baselines tend to produce more invalid/unsynthesizable outputs when not constrained by synthesis-aware tokenization; the paper does not provide a numerical novelty-vs-performance curve for baselines.",
            "generation_validation_comparison": "Paper compares baseline generation to validation (RDKit/SA/Allchemy) and reports that lack of synthesis constraints in baseline generation leads to lower makeability and more syntactic errors compared to mCLM which integrates synthesis constraints.",
            "uncertainty_quantification": "Not reported for baselines in this paper.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Baselines were allowed to generate without synthesis guarantees; in OOD tests they produced lower makeability and sometimes invalid molecules (examples: MoleculeSTM generated syntactically incorrect molecules in some fallen-angel experiments). Specific OOD metrics beyond those in tables are not provided.",
            "validation_proxy_metrics": "Same proxies used as for mCLM: RDKit validity, SA score, Allchemy retrosynthesis for sampled molecules; property oracles used to estimate ADMET changes.",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (drug discovery); methods are heuristic/data-driven and not formally verified.",
            "gap_mitigation_strategies": "Baselines generally do not include synthesis-guaranteed tokenization; the paper does not report additional gap-mitigation strategies applied to these baselines, though some (like LDMol) use structure-aware latent spaces.",
            "evidence_supporting_gap": "Baseline results demonstrate lower RDKit validity and lower Allchemy-synthesizability/makeability (see numbers above), which the authors use to support the claim that generation methods without synthesis constraints create a practical gap between in-silico generation and real-world makeability.",
            "evidence_contradicting_gap": "Not presented in this paper; some baselines do produce valid/synthesizable structures for many cases but on average underperform mCLM in the authors' experiments.",
            "computational_cost_ratio": "Not reported per-baseline; Allchemy validation cost applies equally to baseline outputs when tested (30 minutes per molecule allotted in experiments). Generation cost per baseline model is not specified in the paper.",
            "uuid": "e2092.3"
        },
        {
            "name_short": "RDKit + SA",
            "name_full": "RDKit syntactic validity checker and Ertl Synthetic Accessibility (SA) score",
            "brief_description": "Lightweight cheminformatics tools used as fast, low-cost proxies for molecule correctness (RDKit structural validity) and synthetic accessibility (Ertl SA score) prior to heavier retrosynthetic validation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "RDKit validity check + Ertl SA score",
            "system_type": "cheminformatics toolkit (rule-based syntax checks) + heuristic scoring function",
            "scientific_domain": "chemistry / cheminformatics",
            "output_type": "binary syntactic validity flag (RDKit); continuous SA score (heuristic estimate of synthetic accessibility; lower is easier)",
            "novelty_level": "NA (validation/proxy tools, not generative)",
            "generation_method": "N/A",
            "validation_method": "Used as first-pass validation: RDKit to check whether SMILES/structures are syntactically correct; SA score as a heuristic to rank likely synthesizability before running Allchemy retrosynthesis.",
            "generation_performance": "N/A",
            "validation_performance": "Reported sample metrics: mCLM RDKit validity = 100.0% and mean SA ~2.41; baseline models lower validity (e.g., MoleculeSTM ~93.8% validity) and slightly worse SA scores. SA and RDKit provide cheap, automated signals but are recognized as imperfect proxies.",
            "false_positive_rate": "Not reported. RDKit can miss chemically invalid but syntactically correct structures, and SA is heuristic — paper does not provide explicit false-positive/negative calibration for these proxies.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Not analyzed quantitatively; proxies are expected to be less informative for highly novel chemistries, but the paper does not quantify this.",
            "generation_validation_comparison": "Used as a cheap filter prior to Allchemy; paper demonstrates that relying only on these proxies is insufficient for strong synthesis claims and thus uses Allchemy as a more rigorous validator.",
            "uncertainty_quantification": "Not provided.",
            "calibration_quality": "Not provided.",
            "out_of_distribution_performance": "Not quantified specifically; authors use SA + RDKit as initial checks across in-distribution and OOD tests but caution that stronger validation is required for novel outputs.",
            "validation_proxy_metrics": "Yes — RDKit validity and SA score are the primary lightweight proxies used in the study.",
            "human_validation_required": null,
            "human_validation_frequency": null,
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / heuristic",
            "gap_mitigation_strategies": "Serve as fast filters to triage generated molecules before expensive Allchemy runs; the authors still treat them as proxies and favor Allchemy for rigorous claims.",
            "evidence_supporting_gap": "Paper argues that reliance on cheap proxies (RDKit/SA) alone can overestimate practical makeability, motivating the use of synthesis-aware generation (mCLM) and expensive retrosynthetic checking.",
            "evidence_contradicting_gap": "Not provided.",
            "uuid": "e2092.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Computer-designed repurposing of chemical wastes into drugs",
            "rating": 2
        },
        {
            "paper_title": "Automated iterative c(sp3)-C bond formation",
            "rating": 2
        },
        {
            "paper_title": "Artificial intelligence for retrosynthetic planning needs both data and expert knowledge",
            "rating": 2
        },
        {
            "paper_title": "Ldmol: A text-to-molecule diffusion model with structurally informative latent space surpasses ar models",
            "rating": 2
        },
        {
            "paper_title": "DrugAssist: A large language model for molecule optimization",
            "rating": 2
        },
        {
            "paper_title": "MoleculeSTM",
            "rating": 1
        }
    ],
    "cost": 0.02729475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MCLM: A MODULAR CHEMICAL LANGUAGE MODEL THAT GENERATES FUNCTIONAL AND MAKEABLE MOLECULES
12 Oct 2025</p>
<p>Carl Edwards 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Chi Han chihan3@illinois.edu 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Gawon Lee 
Department of Chemistry
University of Illinois Urbana-Champaign</p>
<p>Thao Nguyen 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Sara Szymkuć 
Allchemy Inc</p>
<p>Chetan Kumar 
Department of Chemistry
University of Illinois Urbana-Champaign</p>
<p>Bowen Jin 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Jiawei Han 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Ying Diao 
Department of Chemical and Biomolecular Engineering
University of Illinois Urbana-Champaign</p>
<p>Ge Liu 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Hao Peng 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>Bartosz A Grzybowski 
Ulsan National Institute of Science and Technology</p>
<p>Institute of Organic Chemistry
Polish Academy of Sciences</p>
<p>Martin D Burke mdburke@illinois.edu 
Department of Chemistry
University of Illinois Urbana-Champaign</p>
<p>Heng Ji hengji@illinois.edu 
Siebel School of Computing and Data Science
University of Illinois Urbana-Champaign</p>
<p>MCLM: A MODULAR CHEMICAL LANGUAGE MODEL THAT GENERATES FUNCTIONAL AND MAKEABLE MOLECULES
12 Oct 20257B86FBDF47550C182D3B6A0BCED5879BarXiv:2505.12565v2[cs.AI]
Despite their ability to understand chemical knowledge, large language models (LLMs) remain limited in their capacity to propose novel molecules with desired functions (e.g., drug-like properties).In addition, the molecules that LLMs propose can often be challenging to make, and are almost never compatible with automated synthesis approaches.To better enable the discovery of functional small molecules, LLMs need to learn a new molecular language that is more effective in predicting properties and inherently synced with automated synthesis technology.Current molecule LLMs are limited by representing molecules based on atoms.In this paper, we argue that just like tokenizing texts into meaning-bearing (sub-)word tokens instead of characters, molecules should be tokenized at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis.This motivates us to propose mCLM, a modular Chemical-Language Model that comprises a bilingual language model that understands both natural language descriptions of functions and molecular blocks.mCLM front-loads synthesizability considerations while improving the predicted functions of molecules in a principled manner.Experiments on 430 FDA-approved drugs showed that mCLM is capable of significantly improving chemical functions critical to determining drug potentials.mCLM, with only 3B parameters, also achieves improvements in synthetic accessibility relative to 7 other leading generative AI methods including GPT-5.When tested on 122 out-of-distribution medicines using only building blocks/tokens that are compatible with automated modular synthesis, mCLM outperforms all baselines in property scores and synthetic accessibility.mCLM can also reason on multiple functions and iteratively self-improve to rescue drug candidates that failed late in clinical trials ("fallen angels"). 1* Equal contribution.</p>
<p>INTRODUCTION</p>
<p>Small molecules-the class of chemical matter primarily built from carbon atoms bonded togethercan perform a wide range of important functions in human society (Zhang et al., 2025).These include essentials like promoting health by acting as medicines (Zheng et al., 2024;Edwards et al., 2024b;Singhal et al., 2023;Thirunavukarasu et al., 2023;Xiao et al., 2024a), converting energy by functioning as key components in solar cells (Nguyen et al., 2024b;Lv et al., 2021;Li et al., 2023b; Natural language Peptides Small molecules 4 -( N -M e t h y l p i p e r a z i n y l ) m e t h y l b e n z o i c a c i d g r o u p a c t s a s a c t i v a t i o n l o o p b i n d e r a n d p r o m o t e s s o l u b i l i t y a n d o r a l b i o a c t i v i t y i n i m a t i n i b .</p>
<p>4-(N-Methylpiperazinyl)methyl benzoic acid group acts as activation loop binder and promotes solubility and oral bioactivity in imatinib.Figure 1: mCLM adopts a modular chemical vocabulary, which uses synthesis robot-friendly molecular building blocks as tokens together with natural language tokens.Compared with using natural language names, SMILES strings, or holistic embeddings for whole molecules, this building block level of tokenization better enables the prediction of compounds with improved properties, and guarantees automated synthesizability a priori, thus building a direct link between the digital and physical worlds.This approach stands to substantially enable AI-guided discovery of new small molecules with targeted functions.2024b; Si et al., 2024), and achieving sustainability by serving as inherently recyclable products.These functions also include many nice-to-haves that drive substantial economic growth, including colorants, flavorings, perfumes, cosmetics, coatings, quantum dots and insect repellants.</p>
<p>The traditional approach for small molecule synthesis is highly artisanal, and thus slow and expensive:</p>
<p>(1) It is unfriendly to automation: machines are not good at performing thousands of reaction types, with each run under thousands of possible conditions and using millions of possible starting materials.</p>
<p>(2) It leads to an undemocratized landscape in drug discovery.With development costs averaging around $1.3 billion per drug, only economically advanced countries can afford to invest in such highrisk research (Kneller, 2010).It also excludes potential breakthroughs from the developing world and inhibits research that could prevent diseases prevalent there.Moreover, participation in the process of molecular innovation currently requires access to highly trained experts in chemical synthesis.As a result, there are no effective drugs for many diseases such as Parkinson and Amyotrophic Lateral Sclerosis.(3) Most importantly, there are many instances of known commercial drugs or materials that have well-documented limitations that have remained unaddressed.For example, Imatinib is an important anticancer drug that works well in most parts of the body, but it only poorly penetrates the blood-brain barrier (Takayama et al., 2002;Senior, 2003;Coutre et al., 2004;Isobe et al., 2009).This means it may effectively treat cancer at its primary site, yet fail to prevent fatal brain metastasis.In such cases, more blood-brain-barrier penetrant molecules that retain all of the other favorable properties of the current drug are desired, but making such selective modifications in functional properties can be very challenging.As another example in the materials domain, 350+ million people in Asia and the Pacific have only limited access to electricity, and 150 million people still have no access at all.Organic photovoltaic (OPV) molecules are expected to be more economical and environment-friendly alternatives to current solar cells.However, current commercialized OPV devices either achieve less than 10% energy conversion efficiency-significantly lower than traditional silicon solar cells-or have stability far short of 10 years (Solak and Irmak, 2023).</p>
<p>An alternative block chemistry approach for small molecule synthesis has recently emerged (Gillis and Burke, 2007;Woerly et al., 2014;Li et al., 2015a;Lehmann et al., 2018;Trobe and Burke, 2018;Blair et al., 2022a;Angello et al., 2022;Wang et al., 2024;Strieth-Kalthoff et al., 2024a).Block chemistry is iterative carbon-carbon bond-forming chemistry that machines can do.It iteratively assembles small molecules from prefabricated building blocks using chemistry that is simple and general and thus readily automated.Akin to automated DNA, RNA, and peptide synthesis platforms, a major strength of this block-based approach is that it can access billions of novel small molecules with high degrees of functional potential using only a few automation-friendly reactions and a bounded set of pre-fabricated function-infused building blocks.We specifically recognized that this block-based approach provided an opportunity to create a new modular language for chemistry.The idea that molecules and their synthesis may be best understood from the perspective of a "chemical language" dates back to 2014 (Cadeddu et al., 2014), where structural fragments and functional groups play the roles of "chemical words".This view is supported by multiple observations aligned with natural language: (1) they can be decomposed and reassembled, (2) they exhibit ambiguity-the same building block can perform vastly different functions depending on the chemical context, and (3) they possess significant diversity, as many different structures can lead to the same function.This linguistic parallel suggests the potential to train a large language model specifically for molecules by linearizing their structures into modular sequences.</p>
<p>A common representation for such an approach is the Simplified Molecular Input Line Entry System (SMILES; Weininger, 1988), where atoms are denoted by one-or two-character symbols (e.g., C for carbon, Br for bromine, and F for fluorine), rings are represented with numbers, and branches are indicated using parentheses.However, such atom-level tokenization strategies (e.g., SMILES (Weininger, 1988) or SELFIES (Krenn et al., 2020)) resemble character-level natural language models, which struggle to generalize effectively.Unlike proteins, which have a fixed vocabulary of 20 amino acids, small molecules exhibit an open vocabulary when each atom is treated as a token, as illustrated in Figure 1.It also causes severe restrictions in practicality because many of the new structures proposed by LLMs based on atom-level tokenization are not practically synthesizable in the laboratory.This approach has created a major gap between what is now possible in silico and what is possible in the physical world.</p>
<p>Furthermore, the SMILES representation can obscure critical structure information: two atoms that are direct neighbors in the molecular graph may be distantly separated in the SMILES string.Even with recent efforts to align SMILES and natural language description (Edwards et al., 2022a;Pei et al., 2024;Ahmad et al., 2022), integrate chemical properties and functional groups (Nguyen et al., 2025) into SMILES, incorporate 3D geometric information (Fu et al., 2025;Li et al., 2025a), and employ graph neural networks to capture molecular graphs and chemical reaction contexts (Wang et al., 2022a), these representations still fail to encapsulate functional knowledge that is often described only in natural language literature because the inherent properties and functions of molecules are hidden in their structure, composition, and interaction.Therefore, our goal is not to make all drug-like molecules, rather, our goal is to figure out how to make the right ones, better, faster, stronger.Unlike machines, human scientists are inherently "multilingual," seamlessly navigating diverse modalities-from natural language and scientific figures in literature to complex scientific data such as molecular structures in knowledge bases.In contrast, most prior work on molecule discovery trains large language models on a single modality.Moreover, Human scientists "think before they talk," grounding their reasoning in deeply reflective and deliberate reflection and critical evaluation to generate new hypotheses.Current models lack this critical thinking capacity, limiting their ability to contribute meaningfully to discovery.In particular, the human body is a highly complex, interconnected system, and drug discovery is essentially a multi-objective optimization problem.It involves balancing factors such as drug absorption, first-pass metabolism, bioavailability, distribution, protein binding, and clearance.However, improving one property often comes at the expense of another.Many promising drug candidates have failed in the final stages of FDA approval due to an unacceptably high risk of drug-induced liver injury.</p>
<p>Against this backdrop, we argue that there is a correctable fundamental mismatch between the way LLMs work and the way chemists traditionally synthesize and study small molecules.Reducing tokenization granularity to the level of individual letters is counterproductive, as it complicates meaning extraction and increases the likelihood of generative AI hallucinate words that don't exist.Analogous limitations are inherently linked to atom-based tokenization.And as a result, unfortunately, much of the generative AI research for scientific discovery within the computer science community does not extend to hypothesis verification in the physical world.To bridge this gap, in this paper, we propose a novel LLM by drawing inspiration from the scientific discovery process itself.We aim to develop a science-inspired large language model that follow three principles: (1) "Observe" -acquire, represent and integrate knowledge from multiple data modalities; (2) "Think" -think critically to generate hypotheses; and (3) "Propose" -verify hypotheses through the Physical World.We aim to teach computers to speak two complementary languages: one that represents molecular building blocks (i.e., subgraph structures) indicative of specific functions and compatible with automated modular assembly, and another that describes these functions in natural language (Figure 2).Unlike existing approaches that add such knowledge as a post hoc step, we develop a function-and synthesis-aware modular chemical language model (mCLM).Inspired by bilingual speakers who frequently "code-switch" (naturally and often switch between their two languages within the same  It is a multimodal chemical-language model that tokenizes molecules into synthesis robot-friendly building blocks, thus creating a direct link between the digital and physical worlds.After being trained on datasets consisting of properties, functions and synthesis data, the mCLM can conduct critical chemical reasoning through an iterative refinement process.message; Poplack, 2013), we propose a novel neural encoder that integrates molecular structure and natural language.mCLM incorporates both function-and synthesis-related knowledge into the small molecule tokenization process a priori.First, we tokenize small molecules at the level of building blocks (graph substructures) that are able to predict function and are, by design, flexible, multi-scale, fully compatible with automated modular small molecule synthesis.We use graph neural networks to encode each building block.We then extract natural language sentences from the literature that describe the molecule's functions and chemical reactions and synthesis constraints of various molecules, and seamlessly insert the encoded building blocks alongside the corresponding entity names to form the training data, as illustrated in Figure 2. By reasoning on such functional building blocks, mCLM guarantees to generate efficiently synthesizable molecules thanks to recent progress in block-based chemistry, while also improving the functions of molecules in a principled manner.During model inference, mCLM enables these functional modules to be predictably and automatically assembled into molecules with desired functions.</p>
<p>Compared to previous work, mCLM has multiple potential advantages of encoding and generating molecules at a modular level: 1. Synthesis efficiency: proposed molecules can be faster and more broadly makeable because the process is, by design, simple, iterative, general, and machine-friendly.This can enable rapid iterative drug and material development via seamless integration with automated lab experiments.2. Multimodal understanding: resembling the mechanism of natural language tokenization, molecular modularization provides a more natural interface to align with word representations to form a more powerful multimodal representation.3. Deliberate reasoning: leveraging LLMs' instruction following capability and wide-scale pretraining, mCLM is able to iteratively refine molecules based on knowledge of molecular functions.</p>
<p>THE MODULAR CHEMICAL LANGUAGE MODEL</p>
<p>In natural language modeling, tokenization identifies common substrings (such as words and subwords), which carry richer semantic information than sequences of characters.Similarly in nature, most small molecules are composed primarily of connected building blocks (Lehmann et al., 2018;Trobe and Burke, 2018).There is likewise a high degree of inherent modularity in many medicines and materials (Ertl and Schuhmann, 2019;Arkan et al., 2020;Andrews et al., 1984;Vitaku et al., 2014).To leverage the phenomena in chemistry and borrow the spirit from natural language modeling, we propose mCLM, a multimodal model that jointly encodes and understands natural language and molecules based on synthesis-friendly building blocks instead of atoms.In Section 2.1, we introduce the concept of molecular building blocks as a chemical "vocabulary" and describe the tokenization process to obtain the library of building blocks.Then in Section 2.2 we describe the mCLM architecture and its training.Finally, we introduce the reasoning mechanism of mCLM which refines molecule design over multiple iterations in Section 2.3.</p>
<p>A FUNCTION-INFUSED AND SYNTHESIS-FRIENDLY VOCABULARY</p>
<p>In this work, we propose to leverage a chemical vocabulary V of synthesis-friendly building blocks.This approach guarantees capacity for automated iterative assembly first reported in 2015 (Li et al., 2015b) and, since then, demonstrated in multiple experimental campaigns including Strieth-Kalthoff et al. (2024b) and Angello et al. (2024).The blocks can be chemically connected using predefined synthesis rules in a short period.Briefly, akin to language models developed for peptides/proteins, small molecules can be assembled automatically from makeable building blocks.These building blocks are often highly associated with chemical functions, such as binding to protein targets, modulating enzyme activity, or affecting involved metabolic processes.This will enable rapid and iterative proposal of new small molecules, automated synthesis of those small molecules, and generation of the corresponding functional data on demand.In contrast to SMILES strings which break molecule structure during graph linearization (e.g., separating physically adjacent subgraphs such as the two carbons in molecule C(N)C), our representation is linear in the physical world.During dataset construction, we combine two tokenization strategies to balance coverage and synthetic feasibility: a synthesis-guaranteed tokenizer and a rule-based tokenizer (Figure 3).The synthesisguaranteed tokenizer disconnects the molecule only at bonds that can be formed by a predetermined small set of reactions that can be performed in an automated manner.Specifically, it only permits bond disconnections that correspond to reactions compatible with state-of-the-art automated synthesis platforms.These three bonds are: amide coupling, Suzuki-Miyaura coupling, and Buchwald-Hartwig coupling (see Figure 3 tokenization rules; Tyrikos-Ergas et al., 2025).The disconnection rules are adapted from well-established principles in block-based and automated chemistry (Blair et al., 2022b;Li et al., 2015b).Qualitative examples of tokenization are included in the appendix F, and the full rules and tokenizer code will be released with documentation.Importantly, the tokenizer is modular: as new automated reactions are validated in practice, their corresponding disconnection rules can be readily incorporated, expanding the makeable chemical space without requiring fundamental architectural changes.Unlike traditional retrosynthesis tools that prioritize maximum coverage or human-designed heuristics, this tokenizer is guided by the operational constraints of block chemistry.It also ensures that resulting blocks are free from functional group conflicts that would interfere with downstream synthesis.This conservative approach leads to a set of molecular tokens that are guaranteed to work under defined synthesis protocols, enabling seamless transition from modelgenerated output to real-world synthesis.When the synthesis-guaranteed tokenizer cannot fully cover a molecule-due to chemical incompatibilities or reaction constraints-we fall back on a rule-based tokenizer to ensure coverage of a larger variety of molecules.The rule-based tokenizer also breaks molecules along the same automated machine-friendly bonds as the above tokenizer.However, no other rules are applied beyond specifying a minimum size for blocks.This rule-based tokenizer is used only during training to support learning on diverse molecular structures while maintaining consistency with synthesis logic.We note that the vocabulary growth is analogous to the shift from character-level to subword-level tokenization in natural language, where larger but semantically meaningful tokens significantly improved performance.</p>
<p>CHEMICAL-LANGUAGE MODELING</p>
<p>Figure 2 illustrates the architecture of mCLM.The model is a sequential generative model that processes molecule and text sequences in a unified manner.It adopts a Transformer architecture, which is well-suited for handling sequential data and allows for using pre-trained language models as a backbone.After tokenizing the molecules as mentioned in the last section, we encode each building block using graph neural networks (GNNs; Edwards et al., 2024a;Sprueill et al., 2024;Gasteiger et al., 2021) before passing them through an adaptor module.These representations are then concatenated with natural language embeddings at positions where molecule entity names appear.This results in a form of "code-switched" language which blends molecular structures with natural language descriptions.The feature sequence is then fed into a Transformer decoder-only architecture, which predicts the next token based on previous tokens.This allows for pre-training the model on a large corpus of multi-modal data, enabling it to learn and "talk about" the relationships between the different modalities and their respective representations.</p>
<p>We train mCLM on top of open-source pretrained large language models (we adopt Qwen2.5 Yang et al. (2024) as the starting point), allowing us to leverage their natural language understanding and reasoning capabilities without incurring the computational cost of training from scratch.As the training objective, we adopt a unified categorical cross-entropy (CCE) loss applied to both natural language and molecular tokens:
L = H(P (x), P θ (x)), logit P θ (v | x 1•••i−1 ) = c ⊤ i e v , v ∈ V natural language c ⊤ i f ψ GNN ϕ (v) , v ∈ V molecular building block
Specifically, the cross-entropy loss is computed between the ground truth distribution P (x) and the model distribution P θ (x) over the combined vocabulary of natural language and molecular building blocks.The model generates logits for the next token v by computing the dot product between the contextual representation c i with token embeddings.c i is produced by the Transformer given the previous tokens
x 1•••i−1 = [X 1 , • • • , X i−1 ]
).For natural language tokens, the embedding e v is directly taken from the pretrained natural language model.The embedding for molecular building blocks is computed by passing the building block's graph through a GNN, followed by a linear adapter function f ψ to project it into the same embedding space.This formulation enables joint training over both modalities using a single loss function.We train on approximately 1 million examples consisting of the 1,000 most frequent molecular building blocks, sampled from a new dataset we constructed.This dataset contains over 36 million molecular instructions, 1,000 tasks, and 8 million molecules.More details on the training procedure are described in Appendix E. Notably, the architecture of mCLM is compatible with larger vocabularies, and we anticipate future scaling studies as computational resources permit.</p>
<p>CRITICAL CHEMICAL REASONING</p>
<p>Chemical reasoning over molecules often involves optimizing multiple functions, such as toxicity, bioactivity, and binding affinity.Therefore, it is not a straightforward task to propose an ideal molecule structure, especially with only a single attempt.Optimizing one function may lead to trade-offs in others.For example, many drugs with higher potency were rejected due to increased toxicity to patients.To address this, we propose a reasoning process that allows the model to refine its own generated molecules and iteratively improve their desired functions.At each iteration, we evaluate the properties and identify one that still requires improvement, and mCLM proposes a modification of the molecule targeting this property.This process is repeated until a maximum number of iterations is reached.This process is summarized in Appendix Algorithm 1.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>CREATING ORACLE MODELS FOR EVALUATION</p>
<p>To evaluate the performance of our generated molecules, we construct oracle models focused on Absorption, Distribution, Metabolism, Excretion, Toxicity (ADMET) property prediction.We select 6 tasks from the Therapeutics Data Commons (TDC) benchmark (Huang et al., 2021): AMES (mutagenicity), BBBP (blood-brain barrier permeability), CYP3A4 inhibition (metabolism), DILI (drug-induced liver injury), HIA (human intestinal absorption), and PGP (P-glycoprotein substrate classification).The detailed training procedure is described in Appendix D.2.3.</p>
<p>IMPROVING FDA-APPROVED DRUGS WITH OUT-OF-VOCABULARY BLOCKS</p>
<p>In practice, real-world drug discovery is often driven by the optimization of known molecules, as existing approved drugs offer a more direct and promising path to safe and effective new drugs.Therefore, we evaluate mCLM's drug proposal capability in improving FDA-approved drugs.Specifically, we apply the mCLM to improve the 6 properties for all FDA-approved drugs consisting of synthesis-guaranteed blocks.This amounts to 122 molecular structures and 153 unseen blocks.We confine the output vocabulary of mCLM to 582 synthesis-guaranteed blocks.Most of these drugs (120/122) contain blocks that were not present in the 1,000-block training vocabulary, presenting an opportunity to examine how the mCLM works on out-of-distribution molecules.Results in Table 1 show that improvements are achieved for all properties2 .This shows that mCLM is not limited to a fixed vocabulary but can generalize beyond its training coverage.We also compare our approach against a wide range of strong text-based molecule editing baselines, including MoleculeSTM (Liu et al., 2022), FineMolTex (Li et al., 2025b), GPT-4o, GPT-5, Gemini-2.5-Flash(Gemini-2.5-F),LDMol (Chang and Ye, 2024), and Claude 3.5 Haiku (Claude-3.5-H).Despite the fact that all of the baseline models are allowed to generate without synthesis guarantees, their average improvements fall behind mCLM.</p>
<p>Table 1: Average pharmacokinetic and toxicity properties of FDA drugs composed of synthesisguaranteed blocks, as well as their proposed modifications.(↓: lower is better, ↑: higher is better).Green = better than FDA, Red = worse, Light green bold = best overall per column.The key to expediting the drug creation process is to discover potent molecular candidates that are simultaneously synthesis-friendly.While mCLM shows strong property editing results, its key benefit lies in its synthesis-friendly nature.We assessed the synthesizability of generated molecules by computing synthetic accessibility (SA scores) (Ertl and Schuffenhauer, 2009) as a quick heuristic (see Table 3).Then, as a more rigorous assessment, we consider Allchemy, which is the stateof-the-art retrosynthesis software (Wołos et al., 2022;Strieth-Kalthoff et al., 2024c).Allchemy is computationally expensive, but it evaluates synthesizability to the best of publicly available human chemical knowledge.For example, it finds synthetic routes for 98.1% of the FDA-approved molecules.</p>
<p>We select the top 3 models by SA score (mCLM, MolSTM, and FineMolTex) and randomly sample 200 generated molecules from each to be assessed by Allchemy on a supercomputing cluster.</p>
<p>LIBRARY DESIGN: IDENTIFYING FUNCTIONALLY-IMPORTANT BLOCKS</p>
<p>As a by-product, mCLM is also able to identify blocks which are preferred for certain functions.This can help inform virtual screening campaign design (e.g., which 10 blocks should we use to get the best chemical space for BBBP?), and it can also be useful for stimulating scientific inquiry.As an example, we calculated the most frequent modifications preferred by the mCLM to improve DILI in FDA drugs (Figure 4).Notably, modification 3 resembles a newly discovered modification for reducing amphotericin B toxicity by human scientists in Maji et al. (2023).</p>
<p>CASE STUDY: MULTI-STEP REASONING TO RESURRECT THE "FALLEN ANGELS"</p>
<p>There are many new drug candidates that almost reach FDA approval but fall short for various reasons when tested in clinical trials.For example, Evobrutinib is a Bruton's tyrosine kinsase (BTK) inhibitor that went through clinical trial as a drug for relapsing Multiple Sclerosis.However, the FDA placed a partial clinical hold on Phase III trials in April 2023 after two patients showed signs of drug-induced liver injury (Montalban et al., 2024).TNG348 is a USP1 inhibitor designed for treating BRCA1/2-mutant and HRD cancers, but it failed in phase 1/2 clinical trials due to liver abnormalities (Inc, 2024;Simoneau et al., 2025).These "fallen angels" represent tremendous opportunities for impactful engagement of the AI/chemistry interface, because much is known about the strengths of each of these small molecules, and it is also known why they fell short.Fixing such fallen angels is a high leverage opportunity for the function-infused mCLM to contribute.</p>
<p>Figure 5 shows an application of the mCLM, without synthesis restrictions on the vocabulary, to these two molecules.For both, the initial step is to optimize DILI, the reason the drugs failed in clinical trials.Following that, the mCLM fixes other properties which were made worse in the previous attempt (PGP for Evobrutinib and BBBP for TNG248).For good measure, another property of each molecule is then improved.Notably, at each step, the mCLM only makes minor modifications to each drug of roughly 1 building block.While the mCLM shows promising results for repairing these drugs, it is worth noting that drug discovery is a many-objective optimization problem.While we are able to generate molecules with improved toxicity relative to Evobrutinib and TNG348, as well as other properties, yet other important properties may still have been compromised.Future work may want to investigate longer reasoning chains across a wider variety of properties.For comparison, we show a (less-successful) version of this experiment using MoleculeSTM A.3.</p>
<p>RELATED WORK</p>
<p>Protein language models (Lin et al., 2023;Hayes et al., 2025;Wang et al., 2025b;Madani et al., 2023;Xiao et al., 2024b;Su et al., 2024;Buehler and Buehler, 2024;Wang et al., 2025a;Zhu et al., 2024;Ferruz et al., 2022) have been highly successful because proteins/peptides are tokenized at the level of amino acid building blocks (which are akin to the words in a sentence).This makes it possible to associate the sequences of these building blocks with the fold and function of proteins, and then the corresponding building blocks can be assembled into targeted sequences.</p>
<p>Inspired from the successes of protein LLMs, LLMs have been increasingly applied in computational chemistry (Zhang et al., 2024a;b;Fang et al., 2023;Livne et al., 2024;Pei et al., 2023;Zhao et al., 2024;2023;Yu et al., 2024;Ye et al., 2025;Liu et al., 2023b;Li et al., 2024c;Liu et al., 2024a;Taylor et al., 2022).Recent work has further explored cross-modal modeling of molecule and language (Edwards et al., 2022b;Liu et al., 2022;Li et al., 2025b;Chang and Ye, 2024).Generalpurpose LLMs such as GPT-4o (OpenAI, 2025a), GPT-5 (OpenAI, 2025b), Gemini-2.5-Flash(Google DeepMind, 2025), and Claude-3.5-H(Anthropic, 2025) show multimodal reasoning capabilities.However, most molecular language models still rely on atom-level (Wang et al., 2019;Ahmad et al., 2022;Zhou et al., 2023;Xia et al., 2023) or Byte-Pair Encoding on SMILES (Chang and Ye, 2024), and thus they often produce invalid tokens, misalign with chemical structures, and generate unsynthesizable molecules.To address these limitations, other approaches incorporate functional groups and fragments into molecular representations (Li et al., 2023a;Nguyen et al., 2024c;Han et al., 2023;Jin et al., 2024;Nguyen et al., 2024a;Wang et al., 2023;Zhang et al., 2020;2021), capturing both atomic-and group-level information.However, these methods tend to break molecules at bonds that are difficult or even impossible to form through chemical reactions.In contrast, our mCLM decomposes small molecules into function-infused and synthesis-friendly building blocks and integrates synthesis constraints early in training, akin to the way peptide/protein LLMs tokenize at the amino acid level.</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>We have developed a function-and synthesis-aware modular Chemical-Language Model (mCLM), as the first attempt to jointly model natural language sequences with modular chemical language.By design, the mCLM only generates chemical building blocks that can be iteratively assembled on robotic small molecule synthesis platforms, enabling the rapid creation of novel molecules with desired functions, all accessible by non-specialists.In the future we aim to incorporate richer multimodal knowledge related to physical and chemical properties from 2D/3D molecular structures, protein, cell lines and nucleic acid sequences to further enable the mCLM to reason on biological activity, protein docking knowledge, and individuals' genetic profiles.We plan to extend chemical reasoning to additional aspects such as filling in unspoken knowledge gaps, thinking outside of the box, System 2 thinking for counterfactual reasoning and plausibility prediction, and resolving conflicting claims.We also plan to leverage more physical constraints from simulation tools and chemical and reaction knowledge bases.In the long term, we envision the mCLM as part of a comprehensive, multi-agent, human-in-the-loop autonomous laboratory, structured around iterative cycles of reasoning, proposal, synthesis, physical testing, feedback, and reasoning to enable neverending self-improvement and co-evolvement with human scientists.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>In section 3.1 and D.2.3, we describe the process and resources of developing oracle models for evaluation.The synthesis-guaranteed tokenizer is described in further detail in Section F. Sections C and D explain the data collection process and the statistics of the training dataset.Section G lists all source datasets.Model training details are described in Section E. Evaluation data from FDAapproved drugs are described in Section 3.2, which also lists the baselines we used for comparison.</p>
<p>The full data and resources will be released upon publication.</p>
<p>A ADDITIONAL RESULTS</p>
<p>A.1 SYNTHESIZABILITY</p>
<p>For the mCLM, all starting materials are the core building blocks, and the same collection of building blocks is used redundantly to make all the proposed structures.For this analysis, we assume that all the building blocks required for the mCLM approach are available.Given a sequence of blocks A -B -C -D in our methodology, we can assemble them with the following steps:</p>
<p>• Coupling reaction (A and B)</p>
<p>• Deprotect reaction (AB)</p>
<p>• Coupling reaction (AB and C)</p>
<p>• Deprotect reaction (ABC)</p>
<p>• Coupling reaction (ABC and D)</p>
<p>Scaling this formula to n building blocks gives a requirement of 2n − 3 reactions (synthetic steps).</p>
<p>On the other hand, each molecule generated by MoleculeSTM is a unique problem for synthesis, and thus cannot leverage the strengths of the block-based approach.Each generated target requires a customized synthesis solution.</p>
<p>However, it is important to consider an external metric for verifying the synthesis capabilities of the mCLM.In table 3, we compute validity and synthetic accessibility scores for all baselines, both overall and in terms of specific property modifications.We find that mCLM outperforms all baselines (lower SA is better).Next, we select the best 3 models and consider a computationally expensive but rigorous approach.We leverage Allchemy, which is the state-of-the-art retrosynthesis software, to quantitatively measure this.We randomly sampled a representative set of 200 molecules generated by each baseline (and the original FDA molecules).</p>
<p>We gave each molecule 30 minutes on a supercomputing cluster (which is quite exhaustive) and didn't apply a price limit for substrates (so this is really quite generous for starting materials).Scores, as reported in Table 2, show much stronger overall performance of the mCLM.Here, validity is the percent of syntactically correct molecules as measured by RDKit, synthesizability is the percent of valid molecules where Allchemy could find a synthetic path, and Makeability is the product of those two metrics.Makeability represents the percent of generated molecules which could be made in the lab.</p>
<p>A.2 EXPANDED TESTING WITHOUT SYNTHESIS GUARANTEES</p>
<p>As a large-scale test, we apply the mCLM to improve all FDA-approved drugs consisting of at least 3 blocks (without synthesis guarantees).Note: this is just a confirmatory test, since the mCLM is intended to be used on the distribution of synthesis-guaranteed molecules (as shown in the main paper).This amounts to 430 molecular structures and 796 unseen blocks.Since most of these molecules (426/430) contain blocks that were not present in the 1,000 blocks used for training, this presents an opportunity to find how the mCLM performs out-ofdistribution.Results in Table 4 show that improvement is achieved for 5/6 properties, even though the model has not seen almost half of the blocks in its vocabulary during training.Specifically, the strictest synthesis-guaranteed tokenizer covers 26.7% of compounds in our corpus.For the remainder, a rule-based tokenizer is used.Despite this fallback, mCLM demonstrates strong generalization, as it successfully incorporated GNN-derived features for the unseen modules.As a baseline comparison, we repeat the fallen angels reasoning experiment using MoleculeSTM (Figure 6).Even though the steps 1 and 2 of MoleculeSTM's modification of TNG348 showed comparable property changes to mCLM, MoleculeSTM encounters molecule validity problems: it generates a syntactically incorrect molecule on step 1 of Evobrutinib and on step 3 of TNG348.</p>
<p>Training Corpus</p>
<blockquote>
<p>is a tyrosine kinase inhibitor that specifically targets the BCR-ABL protein ...</p>
</blockquote>
<p>Imatinib &lt; Molecule property/ function description</p>
<p>Molecule proposal</p>
<p>To enhance selectivity and potency against resistant cancer cell lines, we can use this molecule: &lt; &gt; Synthesis prediction A last-step synthesis reaction of imatinib is: &lt; &gt;</p>
<p>Molecule optimization</p>
<p>Molecule Tokenization</p>
<p>Synthesis-Aware Tokenizer Flexible Tokenizer</p>
<p>Fully Synthesizable Vocab Large-scale Pretraining Vocab</p>
<p>Retrieval-based molecule generation for flexible inference-time vocabulary</p>
<p>Function-aware Vocabulary Selection</p>
<p>C PRETRAINING DATA MIXTURE</p>
<p>To train the model, our goal is to cover a wide variety of different molecular functions and classes of molecules.</p>
<p>To do so, we consider a wide variety of data sources.We use three existing instruction datasets: SMolInstruct (Yu et al., 2024), Tulu-3 (Lambert et al., 2024), and the Biomolecular text portion of Mol-Instructions (Fang et al., 2023).SMolInstruct is used to cover the standard set of chemistry LLM tasks.We supplement this with the non-overlapping portion of Mol-Instructions.Finally, we include Tulu-3 to preserve general reasoning capability.</p>
<p>C.1 PROPERTY-BASED SYNTHETIC INSTRUCTION DATA GENERATION FOR CONTRASTIVE LEARNING</p>
<p>Additionally, we augment this data mixture with a significant amount of synthetic data created using realworld property datasets.Given our datasets, we create additional question datapoints for both regression and classification tasks, such as "Is this molecule <SMILES> ... </SMILES> blood brain barrier permeable?Yes".We also consider the opposite direction: property to molecule and multiple properties to molecule, and unconditional molecule generation.The templates used for constructing this data were created using GPT-4o; 50 were originally generated and then bad templates were removed by hand.Further, we augmented each data point with a description of the property, also written by GPT-4o.</p>
<p>In addition to these tasks, we also consider the molecule optimization task as described in DrugAssist (Ye et al., 2025) (e.g., given molecule A, improve property X).However, their approach has fundamental limitations due to the reliance on oracle models.These models may be out-of-distribution for molecules within our dataset, and low-quality models may propagate errors into our training data.To address this issue, we consider "activity cliffs" (Stumpfe et al., 2019;Zhang et al., 2023) within existing property datasets to train the model for molecule optimization.Activity cliffs are two molecules which are structurally similar but have large differences in a given property.We opt to base our definition of activity cliff on Murcko scaffolds (Bemis and Murcko, 1996).This methodology, which is widely used in medicinal chemistry, extracts the core subgraph, or scaffold, of a molecule.We found that this approach was much more computationally feasible on large-scale molecule datasets than approaches like fingerprint similarity (Bajusz et al., 2015).</p>
<p>Given nominal data (e.g., toxic, kinase inhibitor, banana smell), we look for a pair of molecules that have the same scaffold but only one molecule has the desired property.Given numerical data (e.g., solubility, power conversion efficiency, HOMO-LUMO gap), we instead look for molecules which have a property difference of 1 standard deviation.This forms the positive-negative same-scaffold portion of our data.We also consider pairs of molecules that have different scaffolds but the same property.Here, our goal is to teach the model to consider function over structure.As an example, we may ask "Propose a molecule with a different structure than  5 gives a breakdown data quantity by instruction type for each subset of the data.Table 6 gives a breakdown of the molecule data for each subset of the data.Note that not all molecules in the "Full Data" can be represented with blocks, so we have a subset for "Only Blocks" molecules, and then molecules consisting of the most frequent "Top 100k" blocks, "Top 10k" blocks, and "Top 1000" blocks.Scaffolds are the number of unique Murcko scaffolds (Bemis and Murcko, 1996)."Caps" are end blocks, while "Mid" are middle blocks."Synthesis-aware" is the number of synthesis-guaranteed blocks out of "Total".Data splits are subdivided into their original source: our constructed instruction data or molecule instructions from SMolInstruct.To train our modular chemical language model (mCLM), we generate synthetic data that reflect realistic scenarios encountered in molecular design.These include tasks relevant to drug discovery and organic photovoltaic (OPV) materials design.Our goal is to equip the model with the ability to understand and reason over molecular representations and natural language prompts across a diverse set of tasks.</p>
<p>Drug Discovery Tasks: In the context of drug discovery, chemists often engage in iterative and multi-objective optimization processes, where molecules are evaluated, modified, or generated based on various physicochemical and pharmacological properties.These tasks typically involve querying for specific properties, modifying structures to meet certain design criteria, or generating novel candidates that satisfy given constraints.We consider the following tasks that a chemist may perform:</p>
<p>• Text-to-Molecule Generation: Generate a molecule from a textual description of its structure or properties.</p>
<p>• Property Prediction: Given a molecule and a property of interest, predict whether the molecule possesses the property (binary classification) or the quantitative value of the property (regression).</p>
<p>• Molecular Optimization: Modify a given molecule to satisfy or improve a specific property.</p>
<p>• Scaffold-Constrained Generation: Given a scaffold and a target property, generate a molecule that satisfies both the structural constraint and the property constraint.</p>
<p>OPV Material Design Tasks: To support broader applications of mCLM beyond drug discovery, we also incorporate tasks relevant to organic photovoltaic (OPV) material design.OPVs are an emerging class of lightweight, flexible materials used for solar energy harvesting, where the power conversion efficiency (PCE) is the primary performance metric.A typical OPV device is composed of a donor molecule and an acceptor molecule.The donor is responsible for absorbing sunlight and generating excitons (electron-hole pairs), while the acceptor facilitates charge separation and electron transport.The chemical compatibility and electronic alignment between the donor and acceptor molecules critically influence the resulting PCE.To enable learning in this domain, we define the following OPV-specific tasks</p>
<p>• PCE Prediction: Predict the PCE of a given donor-acceptor pair.</p>
<p>• Donor/Acceptor Completion: Given a donor (or acceptor) and a target PCE, generate the complementary component (acceptor or donor) that achieves the desired performance.</p>
<p>• Constrained Completion with Scaffold: Generate donor or acceptor molecules that match a given scaffold and achieve a target PCE.</p>
<p>D.2 INSTRUCTION TUNING DATA GENERATION D.2.1 PROMPT-ANSWER TEMPLATES</p>
<p>To generate instructional data, we construct a pool of question and answer templates for each task.These templates include multiple paraphrased variants to introduce linguistic diversity and improve the model's generalization capability.During data generation, a question template is randomly sampled from the question set and populated with sample-specific content.Likewise, a corresponding answer template is sampled from the answer set to form a complete prompt-response pair.For example:</p>
<p>D.2.2 LABEL SOURCES FOR INSTRUCTION TUNING DATA</p>
<p>To enable diverse and meaningful pretraining for mCLM, we incorporate both experimentally derived and model-generated labels, covering a broad spectrum of molecular properties critical to chemistry, pharmacology, and materials science.This dual-labeling strategy allows the model to learn from abundant low-level molecular descriptors while also reasoning over high-level functional and biological endpoints.</p>
<p>Low-Level Molecular Properties from ChEMBL: For foundational chemical descriptors, we leverage the ChEMBL25 database-a comprehensive bioactivity resource containing approximately 2 million compounds with rich structural and physicochemical annotations.ChEMBL25 serves as an abundant and reliable source of labels for low-level properties that are widely used in cheminformatics pipelines.From this corpus, we select a core set of descriptors that are most informative for molecular design: Hydrogen bond acceptors (HBA), Hydrogen bond donors (HBD), LogP (octanol-water partition coefficient), Molecular weight (MolWt), Number of aromatic rings, Number of rotatable bonds, Topological polar surface area (TPSA).These descriptors are inexpensive to compute and provide critical insights into molecular solubility, permeability, and synthetic feasibility-making them essential for early-stage screening and property-based filtering.</p>
<p>High-Level Molecular Properties via Oracle Labeling: In addition to low-level properties, we aim to expose the model to high-level functional endpoints that capture complex biological phenomena.Such endpoints are central to pharmacokinetics, drug safety, and efficacy, but they are rarely available in large quantities due to the high cost of experimental validation.Consequently, labeled datasets for these tasks are limited in size and diversity.</p>
<p>To address this challenge, we employ the oracle ensemble models to generate synthetic labels for a curated set of ADMET tasks.</p>
<p>D.2.3 ENSEMBLE ORACLE MODEL:</p>
<p>To generate high-quality synthetic labels for downstream tasks, we construct oracle models focused on ADMET property prediction.We select tasks from the Therapeutics Data Commons (TDC) benchmark3 using the following criteria:</p>
<p>• Relevance to Drug Discovery: The task must reflect a critical aspect of drug efficacy or toxicity.</p>
<p>• Predictability: The task must be reliably predictable using existing models.Specifically, we evaluate all 22 ADMET-related classification tasks in TDC and retain only those where standard models achieve an area under the ROC curve (AUC) greater than 0.80.This ensures the synthetic labels are sufficiently accurate for training purposes.</p>
<p>Based on these criteria, we select six tasks:</p>
<p>• AMES (mutagenicity),</p>
<p>• BBBP (blood-brain barrier permeability),</p>
<p>• CYP3A4 inhibition (metabolism),</p>
<p>• DILI (drug-induced liver injury),</p>
<p>• HIA (human intestinal absorption),</p>
<p>• PGP (P-glycoprotein substrate classification).</p>
<p>TDC provides predefined scaffold-based data splits with an 8:1:1 ratio for train, validation, and test sets.This splitting strategy ensures that structurally dissimilar compounds are separated across subsets, encouraging generalization to novel scaffolds.</p>
<p>Although TDC provides leaderboards for these tasks, many top-performing entries lack reproducible code or working implementations.For instance, the authors of one of the top submissions explicitly acknowledge on GitHub that their code is not runnable.4Therefore, we opt to use robust foundation models-FARM, ChemBERTA-2, and a GNN-for ensemble learning.</p>
<p>• FARM (Nguyen et al., 2024a): A SMILES-based BERT model trained with functional group-aware tokenization.</p>
<p>• ChemBERTA-2 (Ahmad et al., 2022): A large-scale transformer model trained on millions of canonical SMILES sequences.</p>
<p>• GNN (Edwards et al., 2024a): A graph neural network trained on molecular graphs with atom-and bond-level features.</p>
<p>To build the ensemble, we use each model as a feature extractor.The extracted features are concatenated and passed through a fully connected layer for final prediction.This ensemble approach is stacking, where multiple base learners feed into a meta-learner.For each task, we select a threshold that maximizes the F1 score on the validation set.This threshold is then used to binarize the predicted logits into class labels.The performance of our ensemble model across the selected tasks is summarized in Table 7.</p>
<p>E TRAINING PROCEDURE</p>
<p>We employed Qwen2.5-3B(Yang et al., 2024) as the starting LLM for building the mCLM.Generally, we followed the training procedure from LLaVa (Liu et al., 2023a;2024b;Li et al., 2024a).We used a two-layer MLP with PReLU activation (He et al., 2015) as an adapter into the LLM input/output from the GNN.We selected an initial learning rate of 1e-5 for the full model and 1e-6 for the adaptor and LM heads.Further, we used a cosine annealing schedule with a minimum of 1e-6 and 2000 linear warmup steps; AdamW (Loshchilov and Hutter, 2017) optimizer was employed.The model was trained on 4 A100 80GB GPUs in bfloat16 precision.</p>
<p>We found that the model learned the molecule tokens much slower than the text (there is usually a 10x difference in loss value).Molecule tokens are rarer and show up less in the training data.Because of this, we decided to separate the molecule classifier head and the language classifier head.We used a standard autoregressive language modeling loss for both, and we averaged these two losses for the final loss value.The main part of our training experiments focused on minimizing the molecule loss, since the text loss was easy to optimize.Further, we found PEFT (Hu et al., 2021) was not sufficient to adapt to molecules, so full finetuning was required.Roughly 10-50 examples from each synthetic (data source, task) pair were put into a validation set.</p>
<p>To initialize the GNN weights, we employed the MolCLR (Wang et al., 2022b) unsupervised contrastive learning technique.We used AugliChem (Magar et al., 2022) for the augmentations: random atom masking, random bond deletion, and motif removal.One of these augmentation was selected uniformly at random for each data point.The GNN was initialized using a batch size of 128 and lr of 1e-4 with a cosine schedule.The model was trained on all 800k blocks in the full data until convergence on a validation set.We tested embedding dimensions between 16 and 4,096 and found 128 dimensions to be sufficient while minimizing total memory cost.This was necessary because we stored the entire embedding matrix in GPU memory, which was much faster, but consumed about 20GB VRAM.Doing so allowed us to train without the GNN during our pretraining process, which is considerably more efficient.We note the GNN can then be finetuned along with the rest of the mCLM during finetuning to new types of molecules or specific tasks.While we did consider a sampled softmax to train the mCLM, we found this to limit the learning of the model.</p>
<p>For training the mCLM, we used two stages for pretraining.First, we trained for 1 epoch with everything frozen except the adaptor, to allow the adaptor to adjust to the LLM representation space.For the second stage, we trained for 5 epochs with only the GNN embeddings frozen.As discussed in the training data mixture section C, we used the most frequent 1000 building blocks as our vocabulary.</p>
<p>After pretraining, we finetune the mCLM to standardize it's outputs for our experiments.During pretraining, we train for robustness by using a wide variety of responses (e.g., for BBBP prediction we might respond "It is restricted from entering the central nervous system" instead of 'No').For finetuning, we train with standardized responses for our desired tasks (e.g., "Generate a molecule that has higher blood brain barrier permeability than [MOL] ... [/MOL].","[MOL] ... [/MOL]".Due to our downstream tasks, we finetuned exclusively on the molecule optimization task for 5 epochs over 100k examples for each property.We trained using the same procedure as the pretraining stage, but we selected the best model using validation loss.</p>
<p>F TOKENIZER DETAILS F.1 SYNTHESIS-GUARANTEED TOKENIZER DETAILS</p>
<p>The synthesis-guaranteed tokenizer disconnects the molecule only at bonds that can be formed by a predetermined small set of reactions, preferably only those that can be performed in an automated manner.For instance, if amide bond formation is defined as available, the tokenizer will be able to disconnect amide bonds in the molecule of interest.Up to this point, the protocol is synonymous with classical computational retrosynthesis, but there is a fundamental difference.Namely, the sets of reactions suitable for automated synthesis is very limited -in fact, state-of-the-art synthesis machines utilize only three types of disconnections (amide bond formation, as well as Suzuki and Buchwald-Hartwig couplings).This places very stringent requirements on the groups that can be present in the disconnected blocks -for instance, when the disconnection (say, Buchwald-Hartwig coupling) yields an amine functionality on one of the blocks, this block cannot contain any groups that during the anticipated uses of this block would present a synthetic incompatibility.In the most trivial case, the block cannot contain another unprotected amine because after the Buchwald-Hartwig disconnection, block would feature two amines which, in turn, would present competing reactive sites (in the Buchwald-Hartwig synthesis but also in the formation of amide bonds).Therefore, the tokenizer performs retrosynthetic operations while simultaneously checking if they do not lead to blocks with functional groups presenting competing reactivities.Only disconnections avoiding such problems are allowed.This then guarantees that when the corresponding blocks are used to make other molecules, they give only the selective synthesis outcomes.</p>
<p>F.2 MOLECULE TOKEN EXAMPLES</p>
<p>As discussed in E, we pretrained our GNN encoder using MolCLR (Wang et al., 2022b) on all 800k blocks in our full pretraining dataset.</p>
<p>Figure 2 :
2
Figure2: An overview of the mCLM.It is a multimodal chemical-language model that tokenizes molecules into synthesis robot-friendly building blocks, thus creating a direct link between the digital and physical worlds.After being trained on datasets consisting of properties, functions and synthesis data, the mCLM can conduct critical chemical reasoning through an iterative refinement process.message;Poplack, 2013), we propose a novel neural encoder that integrates molecular structure and natural language.mCLM incorporates both function-and synthesis-related knowledge into the small molecule tokenization process a priori.First, we tokenize small molecules at the level of building blocks (graph substructures) that are able to predict function and are, by design, flexible, multi-scale, fully compatible with automated modular small molecule synthesis.We use graph neural networks to encode each building block.We then extract natural language sentences from the literature that describe the molecule's functions and chemical reactions and synthesis constraints of various molecules, and seamlessly insert the encoded building blocks alongside the corresponding entity names to form the training data, as illustrated in Figure2.By reasoning on such functional building blocks, mCLM guarantees to generate efficiently synthesizable molecules thanks to recent progress in block-based chemistry, while also improving the functions of molecules in a principled manner.During model inference, mCLM enables these functional modules to be predictably and automatically assembled into molecules with desired functions.</p>
<p>Figure 3 :
3
Figure 3: An overview of the tokenization process.A functional molecule is first processed by the synthesis-guaranteed tokenizer to produce a set of building blocks compatible with automated modular synthesis.These blocks are then evaluated via a structure coverage check to determine whether they fully reconstruct the original molecule.If coverage is complete, the blocks are used directly for pretraining.Otherwise, the molecule is reprocessed using a rule-based tokenizer to ensure full representation for training purposes.</p>
<p>Figure 4 :
4
Figure 4: Four of the most frequent mCLM-proposed modifications to improve DILI in FDA-approved drugs.</p>
<p>Figure 5 :
5
Figure 5: Examples of fallen angel property modification.</p>
<p>Figure 7 :
7
Figure 7: Overview of Data Creation.</p>
<p>shows six examples of random blocks and their 5 nearest neighbors in our dataset.The similarity of the blocks and their neighbors shows the high quality representations which were obtained.</p>
<p>Figure 8 :
8
Figure 8: Example token from pretraining dataset.Similarity above each molecule is ECFP4 Fingerprint Tanimoto similarity from the ground truth molecule.</p>
<p>Figure 9 :
9
Figure 9: Example token from pretraining dataset.Similarity above each molecule is ECFP4 Fingerprint Tanimoto similarity from the ground truth molecule.</p>
<p>Figure 10 :
10
Figure 10: Example token from pretraining dataset.Similarity above each molecule is ECFP4 Fingerprint Tanimoto similarity from the ground truth molecule.</p>
<p>Figure 11 :
11
Figure 11: Example token from pretraining dataset.Similarity above each molecule is ECFP4 Fingerprint Tanimoto similarity from the ground truth molecule.</p>
<p>Figure 12 :
12
Figure 12: Example token from pretraining dataset.Similarity above each molecule is ECFP4 Fingerprint Tanimoto similarity from the ground truth molecule.</p>
<p>Figure 13 :
13
Figure 13: Example token from pretraining dataset.Similarity above each molecule is ECFP4 Fingerprint Tanimoto similarity from the ground truth molecule.</p>
<p>Table 2 :
2
(Ertl et al., 2009)lity (SA)(Ertl et al., 2009), validity, and retrosynthetic results across baselines.Synthesizability indicates a retrosynthetic route was found.Makeability =Valid × Synth.As shown in Table2, molecules proposed by mCLM are 100% valid (syntactically correct) and 98.2% synthesizable, which is superior even to the FDA drugs.In contrast, MoleculeSTM outputs are valid only 93.9% of the time, and among those, only 90.3% are predicted to be synthesizable using exhaustive retrosynthesis search.Out of MoleculeSTM-generated molecules, only 85.4% can be made (93.8 × 0.91 = 85.4).
ModelSA Validity (%) Synthesizability (%) Makeability (%)FDA2.70100.098.1198.11MoleculeSTM 2.6493.8091.0385.39FineMolTex2.5894.2090.1584.96mCLM (Ours) 2.43100.098.2398.23</p>
<p>Table 3 :
3
Synthetic Accessibility (SA) scores with RDKit validity percentages across datasets.
DatasetFDA SA Valid (%) SA Valid (%) SA Valid (%) SA Valid (%) SA Valid (%) SA Valid (%) SA Valid (%) SA Valid (%) SA Valid (%) mCLM MoleculeSTM FineMolTex GPT-4o GPT-5 Gemini-2.5-F LDMol Claude-3.5-HAMES2.70100.02.39100.02.6494.262.5294.263.0690.982.9872.132.9289.342.8590.163.1293.44BBBP2.70100.02.44100.02.6793.442.6195.902.6788.522.8778.692.6990.162.8994.262.9093.44CYP3A4 2.70100.02.40100.02.6095.902.6394.262.7089.342.8385.252.7593.442.7493.442.6988.52DILI2.70100.02.38100.02.6294.262.5993.442.7393.442.7581.152.7090.162.8087.702.7988.52HIA2.70100.02.42100.02.7391.802.5993.442.6887.702.8273.772.7794.262.8090.162.8691.80PGP2.70100.02.45100.02.6393.442.5994.262.8190.162.8586.072.7982.792.8793.442.7590.16Mean2.70100.02.41100.02.6493.852.5994.262.7890.022.8579.512.7790.032.8391.532.8590.98A.1.1 MORE INFORMATION ON RETROSYNTHESIS EXPERIMENTS</p>
<p>Table 4 :
4
Average pharmacokinetic and toxicity properties of FDA drugs with 3 or more blocks and their proposed modifications.Note, these molecules do not have synthesis-guarantees. (↓: lower is better, ↑: higher is better.
PropertyAMES Mut. (↓) BBBP (↑) CYP3A4 Inhib. (↓) DILI (↓) HIA (↑) PGP (↓) Mean Improv.FDA Drug59.537.62.066.293.266.00.00 %mCLM54.041.41.255.597.668.012.87 %
A.3 APPLYING MOLECULESTM TO THE "FALLEN ANGELS"</p>
<p>(Zipf, 1936)OL] that still demonstrates anticoagulant properties.[MOL]... [/MOL]".For a select number of property types, we use oracle models instead of ground truth data.Please see Appendix D.2.3 for more information.C.2 DATA FILTERINGDue to the large size of our dataset, we filter our data to only keep the most frequent 100k (50k end blocks and 50k mid blocks), 10k (5k and 5k), and 1000 blocks (500 and 500) for experiments.This is because there are far fewer molecule tokens compared to text (each training sample only has up to about 10).Due to Zipf's law(Zipf, 1936), in the full dataset, a single molecule token may only appear in one example in the entire training corpus.Without considerably scaling the compute budget, understanding this additional block is difficult.Overall, this filtering enables us to learn more efficiently and requires less memory resources for training the model, and allows us to better test the model architecture.Please see the section on training details for more information in Appendix E and see Appendix G for source property datasets.Table</p>
<p>Table 5 :
5
Dataset categories and their respective sample counts.
Data TypeFull Data Only BlocksTop 100kTop 10k Top 1000All36,641,17020,910,431 14,823,280 6,195,903 1,054,124Existing Data Sources3,117,8411,208,3651,049,473537,674149,994SMolInstruct2,124,738215,26256,37014,2422,300Tulu-3939,343939,343939,343469,67293,934Mol-Instructions Biomol. Text53,76053,76053,76053,76053,760Synthetic Real Data (Ours)21,290,3539,294,4347,711,548 3,960,633705,210Classification5,612,2153,771,0383,192,583 1,337,619223,341Molecule Generation535,317316,630245,034150,00127,393Positive Negative Same Scaffold3,795,820113,39065,56616,2862,007Positive Positive Different Scaffold660,925477,976256,435191,72033,579Property to Molecule6,676,9573,125,0972,878,012 1,639,322307,780Multi-Property to Molecule572,505556,107533,647423,08978,793Scaffold+Property to Molecule705,833405,497226,69680,44813,078Regression2,730,781528,699313,575122,14819,239Synthetic Oracle Data (Ours)12,232,97610,407,6326,062,259 1,697,596198,920Classification2,077,3271,778,8741,058,754299,44435,168Positive Negative Same Scaffold72,76358,64525,0354,935469Scaffold+Property to Molecule1,169,407925,055453,087113,04913,642Property to Molecule13,25713,2548,8419062Regression8,900,2227,631,8044,516,542 1,279,262149,639D SYNTHETIC DATA GENERATIOND.1 TASK FORMULATION FOR MOLECULAR DESIGN</p>
<p>Table 6 :
6
Breakdown of molecule data by dataset category.
MoleculesMolecule TokensSubsetSourceTotal Tokenized Untokenized ScaffoldsCapMidTotal Synthesis-AwareSMolInstruct 1,951,205 1,566,030385,175510,464Full DataOur data6,160,565 5,270,982864,212 1,109,562Total7,994,305 6,787,8791,178,957 1,537,424SMolInstruct459,910459,9100145,614 157,20459,681 216,88550,788Only BlocksOurs3,830,543 3,830,5430705,641 531,472 175,903 707,375189,770Total4,220,604 4,220,6040799,267 598,470 203,810 802,280214,431SMolInstruct146,079146,079049,74322,87114,80937,68010,930Top 100kOurs2,345,519 2,345,5190429,96049,94149,78099,72127,823Total2,458,034 2,458,0340457,03250,00050,000 100,00028,220SMolInstruct37,68637,686012,3734,1302,5346,6642,618Top 10kOurs821,238821,2380147,5245,0004,9989,9983,468Total848,674848,6740153,1445,0005,00010,0003,487SMolInstruct4,8674,86701391477359836420Top 1000Ours109,554109,554019,6395005001,000432Total112,657112,657020,1015005001,000432</p>
<p>•</p>
<p>Question Template: "Given [a molecule] and [a property of interest], modify the molecule to achieve [desired property value]."•Answer Template: "The [property] of [molecule] is [value]."Thistemplating strategy allows us to produce a large number of diverse, semantically equivalent training instances that support instruction tuning across multiple molecular design tasks.</p>
<p>Table 7 :
7
Performance (AUC) of individual models and the ensemble across six selected ADMET tasks.
ModelAMES Pgp DILI BBBP CYP3A4 HIAFARM (Nguyen et al., 2024a)0.880.89 0.790.940.880.92GNN (Edwards et al., 2024a)0.750.78 0.860.790.800.81ChemBERTa-2 (Ahmad et al., 2022)0.860.89 0.810.930.860.99Ensemble0.890.91 0.840.930.890.99
We also test a different distribution of 430 non-synthesis-guaranteed FDA drugs in Appendix A.2.
https://tdcommons.ai/benchmark/admet_group/overview/
https://github.com/maplightrx/MapLight-TDC
B CRITICAL CHEMICAL REASONING ALGORITHMEvaluate M ′ with respect to desired functions 12: end whileG SOURCE DATASETS AND DATABASES• Leffingwell odors(Sanchez-Lengeling et al., 2019)• BACE(Wu et al., 2018)• Flashpoint(Sun et al., 2020)• MUV(Wu et al., 2018)• Tox21(Wu et al., 2018)• AMES(Huang et al., 2021)• Bioavailability(Huang et al., 2021)• Caco2(Huang et al., 2021)• Carcinogens(Huang et al., 2021)• cav3_t(Huang et al., 2021)• choline_transporter(Huang et al., 2021)• clearance hepatocyte(Huang et al., 2021)• CYP1A2(Huang et al., 2021)• CYP2C9(Huang et al., 2021)• CYP2D6(Huang et al., 2021)• CYP3A4(Huang et al., 2021)• DILI (drug induced liver toxicity)(Huang et al., 2021)• Half_life(Huang et al., 2021)• hERG(Huang et al., 2021)• HIA (human intestinal absorption)(Huang et al., 2021)• Hydration free energy(Huang et al., 2021)• kcnq2(Huang et al., 2021)• LD50(Huang et al., 2021)• Lipophilicity(Wu et al., 2018)• m1_muscarinic(Huang et al., 2021)• OPV data(Nguyen et al., 2024b;Lopez et al., 2016)• orexin_receptor(Huang et al., 2021)• PAMPA_NCATS(Huang et al., 2021)• Broccatelli(Broccatelli et al., 2022)• potassium_ion_channel(Huang et al., 2021)• PPBR (Plasma Protein Binding Rate)(Huang et al., 2021)• Pubchem logP(Kim et al., 2023)• SARSCoV2_3CL(Huang et al., 2021)• SARSCoV2_vitro(Huang et al., 2021)• serine_threonine_kinase_33(Huang et al., 2021)• Skin Reaction(Huang et al., 2021)• Solubility_AqSolDB(Huang et al., 2021)• tyrosyl-dna_phosphodiesterase(Huang et al., 2021)• VDss (volume of distribution at steady state)(Huang et al., 2021)• Molecule Property Cliff Datasets (30+ datasets)(Van Tilborg et al., 2022)• Chemical Function (CheF)(Kosonocky et al., 2023)• ChemFOnt: the chemical functional ontology resource(Wishart et al., 2023)• Pubchem properties(Kim et al., 2023)• FreeSolv(Wu et al., 2018)• QM8(Wu et al., 2018)• QM9(Wu et al., 2018)• Thermosol(Wu et al., 2018)• ESOL (Estimated SOLubility)(Wu et al., 2018)• Lipo(Wu et al., 2018)• BBBP(Gaulton et al., 2012)• ClinTox(Wu et al., 2018)• HIV(Wu et al., 2018)• SIDER(Wu et al., 2018)• Forward synthesis (USPTO)(Yu et al., 2024)• Retrosynthesis(Yu et al., 2024)• CheBI-20(Edwards et al., 2021)• L+M-24(Edwards et al., 2024c)• HBA(Gaulton et al., 2012)• HBD(Gaulton et al., 2012)• MolWt(Gaulton et al., 2012)• NumAromaticRings(Gaulton et al., 2012)• rotatable_bonds(Gaulton et al., 2012)• TPSA (topological polar surface area)(Gaulton et al., 2012)
Walid Ahmad, Elana Simon, arXiv:2209.01712Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. 2022arXiv preprint</p>
<p>Functional group contributions to drug-receptor interactions. Pr Andrews, Craik, Martin, Journal of medicinal chemistry. 27121984</p>
<p>Closed-loop optimization of general reaction conditions for heteroaryl suzuki-miyaura coupling. H Nicholas, Angello, Wiktor Vandana Rathore, Agnieszka Beker, Edward R Wołos, Rafał Jira, Tony C Roszak, Charles M Wu, Alán Schroeder, Bartosz A Aspuru-Guzik, Grzybowski, 10.1126/science.adc8743Science. 3786618Oct 2022</p>
<p>Closed-loop transfer enables artificial intelligence to yield chemical knowledge. David M Nicholas H Angello, Changhyun Friday, Seungjoo Hwang, Austin H Yi, Tiara C Cheng, Edward R Torres-Flores, Wesley Jira, Alán Wang, Martin D Aspuru-Guzik, Burke, Nature. 63380292024</p>
<p>Claude-3.5-H large language model. Anthropic, 2025</p>
<p>Effect of functional groups of self assembled monolayer molecules on the performance of inverted perovskite solar cell. Emre Arkan, Eyup Yalcin, Muhittin Unal, M Zeliha Yigit Arkan, Mustafa Can, Cem Tozlu, Serafettin Demic, Materials Chemistry and Physics. 2541234352020</p>
<p>Why is tanimoto index an appropriate choice for fingerprintbased similarity calculations. Dávid Bajusz, Anita Rácz, Károly Héberger, Journal of cheminformatics. 72015</p>
<p>The properties of known drugs. 1. molecular frameworks. W Guy, Mark A Bemis, Murcko, Journal of medicinal chemistry. 39151996</p>
<p>Automated iterative csp3-c bond formation. J Daniel, Sriyankari Blair, Melanie Chitti, David M Trobe, Hannah M Kostyra, Richard L Haley, Steve G Hansen, Toby J Ballmer, Wesley Woods, Vikram Wang, Mubayi, 10.1038/s41586-022-04491-wNature. 6047904Feb 2022a</p>
<p>Automated iterative c sp 3-c bond formation. J Daniel, Sriyankari Blair, Melanie Chitti, David M Trobe, Kostyra, M S Hannah, Richard L Haley, Steve G Hansen, Toby J Ballmer, Wesley Woods, Vikram Wang, Mubayi, Nature. 60479042022b</p>
<p>Benchmarking accuracy and generalizability of four graph neural networks using large in vitro adme datasets from different chemical spaces. Fabio Broccatelli, Richard Trager, Michael Reutlinger, George Karypis, Mufei Li, Molecular Informatics. 41821003212022</p>
<p>X-lora: Mixture of low-rank adapter experts, a flexible framework for large language models with applications in protein mechanics and molecular design. L Eric, Markus J Buehler, Buehler, APL Machine Learning. 222024</p>
<p>Organic chemistry as a language and the implications of chemical linguistics for structural and retrosynthetic analyses. Andrea Cadeddu, Elizabeth K Wylie, Janusz Jurczak, Matthew Wampler-Doty, Bartosz A Grzybowski, 10.1002/anie.201403708Angewandte Chemie International Edition. 53312014</p>
<p>Ldmol: A text-to-molecule diffusion model with structurally informative latent space surpasses ar models. Jinho Chang, Jong Chul, Ye , arXiv:2405.178292024arXiv preprint</p>
<p>Pharmacokinetics and cellular uptake of imatinib and its main metabolite cgp74588. Karl-Anton Philipp Le Coutre, Stefan Kreuzer, Pursche, Malte Von Bonin, Gökben Leopold, Bernd Baskaynak, Gerhard Dörken, Oliver G Ehninger, Andreas Ottmann, Martin Jenke, Eberhard Bornhäuser, Schleyer, 10.1007/s00280-003-0741-6Cancer Chemotherapy and Pharmacology. 2004</p>
<p>Text2Mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, 10.18653/v1/2021.emnlp-main.47Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, Proc. The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP2022). The 2022 Conference on Empirical Methods in Natural Language essing (EMNLP2022)2022a</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>Carl Edwards, Ziqing Lu, Ehsan Hajiramezanali, Tommaso Biancalani, Ji Heng, Gabriele Scalia, arXiv:2411.00737Molcaparena: A comprehensive captioning benchmark on language-enhanced molecular property prediction. 2024aarXiv preprint</p>
<p>Synergpt: In-context learning for personalized drug synergy prediction and drug design. Carl Edwards, Aakanksha Naik, Tushar Khot, Martin Burke, Ji Heng, Tom Hope, Proc. 1st Conference on Language Modeling (COLM2024). 1st Conference on Language Modeling (COLM2024)2024b</p>
<p>Carl Edwards, Qingyun Wang, Lawrence Zhao, Heng Ji, 10.18653/v1/2024.langmol-1.1Proceedings of the 1st Workshop on Language + Molecules (L+M 2024). Carl Edwards, Qingyun Wang, Manling Li, Lawrence Zhao, Tom Hope, Heng Ji, the 1st Workshop on Language + Molecules (L+M 2024)Bangkok, ThailandAssociation for Computational Linguistics2024cL+M-24: Building a dataset for Lan-guage+Molecules @ ACL 2024</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, Journal of cheminformatics. 112009</p>
<p>A systematic cheminformatics analysis of functional groups occurring in natural products. Peter Ertl, Tim Schuhmann, Journal of natural products. 8252019</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Journal of cheminformatics. 112009</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen, abs/2306.080182023ArXiv preprint</p>
<p>Protgpt2 is a deep unsupervised language model for protein design. Noelia Ferruz, Steffen Schmidt, H " Birte, Ocker, Nature communications. 13143482022</p>
<p>Fragment and geometry aware tokenization of molecules for structure-based drug design using language models. Cong Fu, Xiner Li, Blake Olson, Heng Ji, Shuiwang Ji, Proc. The Thirteenth International Conference on Learning Representations (ICLR2025). The Thirteenth International Conference on Learning Representations (ICLR2025)2025</p>
<p>Gemnet: Universal directional graph neural networks for molecules. Johannes Gasteiger, Florian Becker, Stephan Günnemann, Advances in Neural Information Processing Systems. 202134</p>
<p>Chembl: a large-scale bioactivity database for drug discovery. Anna Gaulton, Louisa J Bellis, Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun Mcglinchey, David Michalovich, Bissan Al-Lazikani, Nucleic acids research. 40D12012</p>
<p>A simple and modular strategy for small molecule synthesis: Iterative suzuki−miyaura coupling of b-protected haloboronic acid building blocks. P Eric, Martin D Gillis, Burke, Journal of the American Chemical Society. 0002-7863129212007</p>
<p>Google Deepmind, Gemini-2.5-F large language model. 2025</p>
<p>Himgnn: a novel hierarchical molecular graph representation learning framework for property prediction. Shen Han, Haitao Fu, Yuyang Wu, Ganglan Zhao, Zhenyu Song, Feng Huang, Zhongfei Zhang, Shichao Liu, Wen Zhang, Briefings in Bioinformatics. 2453052023</p>
<p>Simulating 500 million years of evolution with a language model. Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Jonathan Vincent Q Tran, Marius Deaton, Wiggert, Science. 38767362025</p>
<p>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, Marinka Zitnik, arXiv:2102.09548Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. 2021arXiv preprint</p>
<p>Tango therapeutics announces discontinuation of tng348 program. May 2024Tango Therapeutics Inc</p>
<p>Central nervous system is a sanctuary site for chronic myelogenous leukaemia treated with imatinib mesylate. Y Isobe, K Sugimoto, A Masuda, Y Hamano, K Oshimi, 10.1111/j.1445-5994.2009.01947.xInternal medicine journal. 2009</p>
<p>Large language models on graphs: A comprehensive survey. Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, IEEE Transactions on Knowledge and Data Engineering. 2024</p>
<p>Pubchem 2023 update. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic Acids Research. 51D12023</p>
<p>The importance of new companies for drug discovery: origins of a decade of new drugs. R Kneller, In Nat Rev Drug Discov. 92010. 2010</p>
<p>Mining patents with large language models demonstrates congruence of functional labels and chemical structures. Claus O Clayton W Kosonocky, Edward M Wilke, Andrew D Marcotte, Ellington, arXiv:2309.087652023arXiv preprint</p>
<p>Self-referencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Pascal Friederich, Alan Aspuru-Guzik, Machine Learning: Science and Technology. 14450242020</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, arXiv:2411.15124Pushing frontiers in open language model post-training. 20243arXiv preprint</p>
<p>Towards the generalized iterative synthesis of small molecules. Jonathan W Lehmann, Daniel J Blair, Martin D Burke, 10.1038/s41570-018-0115Nature Reviews Chemistry. 22Feb 2018</p>
<p>Fg-bert: a generalized and self-supervised functional group-based molecular representation learning framework for properties prediction. Biaoshun Li, Mujie Lin, Tiegen Chen, Ling Wang, Briefings in Bioinformatics. 2463982023a</p>
<p>Llava-next: What else influences visual instruction tuning beyond data?. Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, Chunyuan Li, May 2024a</p>
<p>Machine learning-assisted low-dimensional electrocatalysts design for hydrogen evolution reaction. Jin Li, Naiteng Wu, Jian Zhang, Honghui Wu, Kunming Pan, Yingxue Wang, Guilong Liu, Xianming Liu, Zhenpeng Yao, Qiaobao Zhang, 10.1007/s40820-023-01192-5Nano-Micro Letters. 2023b</p>
<p>Machine learning-assisted property prediction of solid-state electrolyte. Jin Li, Meisa Zhou, Honghui Wu, Lifei Wang, Jian Zhang, Naiteng Wu, Kunming Pan, Guilong Liu, Yinggan Zhang, Jiajia Han, Xianming Liu, Xiang Chen, Jiayu Wan, Qiaobao Zhang, 10.1002/aenm.202304480Advanced Energy Materials. 2024b</p>
<p>Synthesis of many different types of organic small molecules using one automated process. Junqi Li, Steven G Ballmer, Eric P Gillis, Seiko Fujii, Michael J Schmidt, Andrea M E Palazzolo, Jonathan W Lehmann, Greg F Morehouse, Martin D Burke, Science. 0036-807534762272015a</p>
<p>Synthesis of many different types of organic small molecules using one automated process. Junqi Li, Steven G Ballmer, Eric P Gillis, Seiko Fujii, Michael J Schmidt, Andrea Me Palazzolo, Jonathan W Lehmann, Greg F Morehouse, Martin D Burke, Science. 34762272015b</p>
<p>Towards 3d molecule-text interpretation in language models. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, Qi Tian, abs/2401.13923ArXiv preprint. 2024c</p>
<p>Learning to generate 3d molecules via language models with geometry-aware tokenization. Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Ji Heng, Shuiwang Ji, Proc. 2025 International Conference on Machine Learning (ICML2025). 2025 International Conference on Machine Learning (ICML2025)2025a</p>
<p>Advancing molecular graph-text pre-training via finegrained alignment. Yibo Li, Yuan Fang, Mengmei Zhang, Chuan Shi, Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V2025b2</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Science. 37966372023</p>
<p>Multimodal large language models for inverse molecular design with retrosynthetic planning. Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, Jie Chen, arXiv:2410.042232024aarXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023a</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. January 2024b</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar, abs/2212.107892022ArXiv preprint</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua, arXiv:2310.127982023barXiv preprint</p>
<p>Alán Aspuru-Guzik, et al. nach0: multimodal natural and chemical languages foundation model. Micha Livne, Zulfat Miftahutdinov, Elena Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, Anthony Costa, Alex Aliper, Chemical Science. 15222024</p>
<p>Edward O Steven A Lopez, Gregor N Pyzer-Knapp, Trevor Simm, Kewei Lutzow, Li, Johannes Laszlo R Seress, Alán Hachmann, Aspuru-Guzik, arXiv:1711.05101Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. 2016. 20173arXiv preprintThe harvard organic photovoltaic dataset. Scientific data</p>
<p>Machine learning: An advanced platform for materials development and state prediction in lithium-ion batteries. Chade Lv, Xin Zhou, Lixiang Zhong, Chunshuang Yan, M Srinivasan, Z Seh, Chuntai Liu, Hongge Pan, Shuzhou Li, Yonggang Wen, Qingyu Yan, 10.1002/adma.202101474Advances in Materials. 2021</p>
<p>Large language models generate functional protein sequences across diverse families. Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher, Nature Biotechnology. 4182023</p>
<p>Auglichem: data augmentation library of chemical structures for machine learning. Rishikesh Magar, Yuyang Wang, Cooper Lorsung, Chen Liang, Hariharan Ramasubramanian, Peiyuan Li, Amir Barati, Farimani , 10.1088/2632-2153/ac9c84Machine Learning: Science and Technology. 3445015nov 2022</p>
<p>Tuning sterol extraction kinetics yields a renal-sparing polyene antifungal. Arun Maji, Corinne P Soutar, Jiabao Zhang, Agnieszka Lewandowska, Brice E Uno, Su Yan, Yogesh Shelke, Ganesh Murhade, Evgeny Nimerovsky, Collin G Borcik, Nature. 62379892023</p>
<p>Efficacy and safety results after &gt;3.5 years of treatment with the bruton's tyrosine kinase inhibitor evobrutinib in relapsing multiple sclerosis: Long-term follow-up of a phase ii randomised clinical trial with a cerebrospinal fluid sub-study. Karolina Xavier Montalban, Jens Piasecka-Stryczynska, Pascal Kuhle, Douglas L Benkert, Martin S Arnold, Andrea Weber, Hans Seitzinger, Jamie Guehring, Davorka Shaw, Yann Tomic, Danielle E Hyvert, Martin Harlow, Jerry S Dyroff, Wolinsky, 10.1177/1352458524123478338436271Multiple Sclerosis Journal. 304-52024</p>
<p>Thao Nguyen, Kuan-Hao Huang, Ge Liu, Martin D Burke, Ying Diao, Heng Ji, arXiv:2410.02082Farm: Functional groupaware representations for small molecules. 2024aarXiv preprint</p>
<p>Glad: Synergizing molecular graphs and language descriptors for enhanced power conversion efficiency prediction in organic photovoltaic devices. Thao Nguyen, Tiara Torres-Flores, Changhyun Hwang, Carl Edwards, Ying Diao, Heng Ji, Proc. 33rd ACM International Conference on Information and Knowledge Management (CIKM 2024). 33rd ACM International Conference on Information and Knowledge Management (CIKM 2024)2024b</p>
<p>Glad: Synergizing molecular graphs and language descriptors for enhanced power conversion efficiency prediction in organic photovoltaic devices. Thao Nguyen, Tiara Torres-Flores, Changhyun Hwang, Carl Edwards, Ying Diao, Heng Ji, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024c</p>
<p>Farm: Functional groupaware representations for small molecules. Thao Nguyen, Kuan-Hao Huang, Ge Liu, Martin D Burke, Ying Diao, Heng Ji, Proc. NAACL2025 Workshop on AI and Scientific Discovery: Directions and Opportunities. NAACL2025 Workshop on AI and Scientific Discovery: Directions and Opportunities2025</p>
<p>GPT-4o: Large language model. 2025aOpenAI</p>
<p>GPT-5 large language model. 2025bOpenAI</p>
<p>Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, arXiv:2310.07276Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. 2023arXiv preprint</p>
<p>Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan, ArXiv preprint, abs/2402.17810. 2024</p>
<p>sometimes i'll start a sentence in spanish y termino en español": Toward a typology of code-switching. Shana Poplack, Linguistics. 51s12013</p>
<p>Benjamin Sanchez-Lengeling, Jennifer N Wei, Brian K Lee, Richard C Gerkin, Alexander B Al'an Aspuru-Guzik, Wiltschko, arXiv:1910.10685Machine learning for scent: Learning generalizable perceptual representations of small molecules. 2019arXiv preprint</p>
<p>Gleevec does not cross blood-brain barrier. The Lancet. Oncology. K Senior, 10.1016/s1470-2045(03)01050-72003</p>
<p>Data-based prediction of redox potentials via introducing chemical features into the transformer architecture. Zhan Si, Deguang Liu, Wan Nie, Jingjing Hu, Wei Wang, Tingting Jiang, Haizhu Yu, Yao Fu, 10.1021/acs.jcim.4c01299Journal of Chemical Information and Modeling. 2024</p>
<p>Characterization of tng348: a selective, allosteric usp1 inhibitor that synergizes with parp inhibitors in tumors with homologous recombination deficiency. Antoine Simoneau, Charlotte B Pratt, Hsin-Jung Wu, Charlotte Grace Shreya S Rajeswaran, Sirimas Comer, Wenhai Sudsakorn, Shangtao Zhang, Liu, Ashley H Samuel R Meier, Choi, Molecular Cancer Therapeutics. 2025</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Advances in organic photovoltaic cells: a comprehensive review of materials, technologies, and performance. E Solak, E Irmak, 10.1039/d3ra01454aRSC Advances2023</p>
<p>Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback. Carl Henry W Sprueill, Khushbu Edwards, Agarwal, Udishnu Mariefel V Olarte, Conrad Sanyal, Hongbin Johnston, Heng Liu, Sutanay Ji, Choudhury, abs/2402.109802024ArXiv preprint</p>
<p>Delocalized, asynchronous, closed-loop discovery of organic laser emitters. Felix Strieth-Kalthoff, Han Hao, Vandana Rathore, Joshua Derasp, Théophile Gaudin, Martin Nicholas H Angello, Ekaterina Seifrid, Mason Trushina, Junliang Guy, Xun Liu, Masashi Tang, Wesley Mamada, Tuul Wang, Cyrille Tsagaantsooj, Robert Lavigne, Tony C Pollice, Kazuhiro Wu, Leticia Hotta, Shangyu Bodo, Mohammad Li, Agnieszka Haddadnia, Rafał Wołos, Cher Tian Roszak, Carlota Ser, Riley J Bozal-Ginesta, Jenya Hickman, Andrés Vestfrid, Elena L Aguilar-Granda, Klimareva, Wenduan Ralph C Sigerson, Daniel Hou, Slawomir Gahler, Adrian Lach, Oleg Warzybok, Simon Borodin, Benjamin Rohrbach, Chihaya Sanchez-Lengeling, Adachi, A Bartosz, Leroy Grzybowski, Jason E Cronin, Martin D Hein, Alán Burke, Aspuru-Guzik, Science. 38466979227May 2024a</p>
<p>Delocalized, asynchronous, closed-loop discovery of organic laser emitters. Felix Strieth-Kalthoff, Han Hao, Vandana Rathore, Joshua Derasp, Théophile Gaudin, Martin Nicholas H Angello, Ekaterina Seifrid, Mason Trushina, Junliang Guy, Liu, Science. 384669792272024b</p>
<p>Alán Aspuru-Guzik, Frank Glorius, and Bartosz A Grzybowski. Artificial intelligence for retrosynthetic planning needs both data and expert knowledge. Felix Strieth-Kalthoff, Sara Szymkuc, Karol Molga, Journal of the American Chemical Society. 146162024c</p>
<p>Evolving concept of activity cliffs. Dagmar Stumpfe, Huabin Hu, Jurgen Bajorath, ACS omega. 4112019</p>
<p>Saprot: Protein language modeling with structure-aware vocabulary. Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan, ICLR2024</p>
<p>Assessing graph-based deep learning models for predicting flash point. Xiaoyu Sun, Nathaniel J Krakauer, Alexander Politowicz, Wei-Ting Chen, Qiying Li, Zuoyi Li, Xianjia Shao, Alfred Sunaryo, Mingren Shen, James Wang, Dane Morgan, 10.1002/minf.201900101Mol. Inf. 3961900101feb 2020</p>
<p>Imatinib mesylate has limited activity against the central nervous system involvement of philadelphia chromosome-positive acute lymphoblastic leukaemia due to poor penetration into cerebrospinal fluid. N Takayama, N Sato, S O'brien, Y Ikeda, S Okamoto, 10.1046/j.1365-2141.2002.03881.xBritish journal of haematology. 2002</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Large language models in medicine. A Thirunavukarasu, D Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, D Ting, 10.1038/s41591-023-02448-8Nature Network Boston. 2023</p>
<p>The molecular industrial revolution: Automated synthesis of small molecules. Melanie Trobe, Martin D Burke, 10.1002/anie.201710482Angewandte Chemie International Edition. 5716Mar 2018</p>
<p>Automated iterative nc and cc bond formation. Theodore Tyrikos-Ergas, Sevasti Agiakloglou, Antonio J Laporte, Wesley Wang, Chieh-Kai Chan, Clare E Wells, Christopher K Rakowski, Rachel I Hammond, Jia Qiu, Jonathan D Raymond, 2025ChemRxiv</p>
<p>Exposing the limitations of molecular machine learning with activity cliffs. Derek Van Tilborg, Alisa Alenicheva, Francesca Grisoni, Journal of chemical information and modeling. 62232022</p>
<p>Analysis of the structural diversity, substitution patterns, and frequency of nitrogen heterocycles among u.s. fda approved pharmaceuticals. Edon Vitaku, David T Smith, Jon T Njardarson, 10.1021/jm501100bJournal of Medicinal Chemistry. 2014</p>
<p>Chemicalreaction-aware molecule representation learning. Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, Martin Burke, Proc. The International Conference on Learning Representations (ICLR2022). The International Conference on Learning Representations (ICLR2022)2022a</p>
<p>Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei, Xiang Pang, Li Yu, Peter Zhiping Xiao, Zhang, arXiv:2508.14765Pepthink-r1: Llm for interpretable cyclic peptide optimization with cot sft and reinforcement learning. 2025aarXiv preprint</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang, Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. the 10th ACM international conference on bioinformatics, computational biology and health informatics2019</p>
<p>Rapid automated iterative small-molecule synthesis. Wesley Wang, Nicholas H Angello, Daniel J Blair, Theodore Tyrikos-Ergas, William H Krueger, N S Kameron, Antonio J Medine, Joshua M Laporte, Martin D Berger, Burke, Nat. Synth. 38May 2024</p>
<p>DPLM-2: A multimodal diffusion protein language model. Xinyou Wang, Zaixiang Zheng, Y E Fei, Dongyu Xue, Shujian Huang, Quanquan Gu, The Thirteenth International Conference on Learning Representations. 2025b</p>
<p>Motif-based graph representation learning with application to chemical molecules. Yifei Wang, Shiyang Chen, Guobin Chen, Ethan Shurberg, Hang Liu, Pengyu Hong, Informatics. MDPI2023108</p>
<p>Molecular contrastive learning of representations via graph neural networks. Yuyang Wang, Jianren Wang, Zhonglin Cao, Amir Barati, Farimani , Nature Machine Intelligence. 432022b</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Chemfont: the chemical functional ontology resource. Sagan David S Wishart, Harrison Girod, Eponine Peters, Juan Oler, Zachary Jovel, Ralph Budinski, Vicki W Milford, Zinat Lui, Robert Sayeeda, Mah, Nucleic Acids Research. 51D12023</p>
<p>Synthesis of most polyene natural product motifs using just 12 building blocks and one coupling reaction. Jahnabi Eric M Woerly, Martin D Roy, Burke, Nature Chemistry. 1755-434962014</p>
<p>Computer-designed repurposing of chemical wastes into drugs. Agnieszka Wołos, Dominik Koszelewski, Rafał Roszak, Sara Szymkuć, Martyna Moskal, Ryszard Ostaszewski, T Brenden, Josef M Herrera, Gordon Maier, Jonathon Brezicki, Samuel, Nature. 60479072022</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, Vijay Pande, Chemical science. 922018</p>
<p>Mole-bert: Rethinking pre-training graph neural networks for molecules. Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, Stan Z Li, The Eleventh International Conference on Learning Representations. 2023</p>
<p>A comprehensive survey of large language models and multimodal large language models in medicine. Information Fusion. Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tianqi Liu, Zhipeng Li, Xin Liu, Xiaoxuan Huang, 2024a102888</p>
<p>Proteingpt: Multimodal llm for protein property prediction and structure understanding. Yijia Xiao, Edward Sun, Yiqiao Jin, Qifan Wang, Wei Wang, arXiv:2408.113632024b</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Drugassist: A large language model for molecule optimization. Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, Briefings in Bioinformatics. 261e6932025</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024arXiv preprint</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, arXiv:2402.06852A chemical large language model. 2024aarXiv preprint</p>
<p>Shichang Zhang, Ziniu Hu, Arjun Subramonian, Yizhou Sun, arXiv:2012.12533Motif-driven contrastive learning of graph representations. 2020arXiv preprint</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, Yuqing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K Joshi, Simon V Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi Jaakkola, Connor W Coley, Xiaoning Qian, Xiaofeng Qian, Tess Smidt, and Shuiwang Ji. 2025Foundations and Trends in Machine Learning</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, arXiv:2406.108332024barXiv preprint</p>
<p>Motif-based graph self-supervised learning for molecular property prediction. Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Chee-Kong Lee, Advances in Neural Information Processing Systems. 202134</p>
<p>Activity cliff prediction: Dataset and benchmark. Ziqiao Zhang, Bangyi Zhao, Ailin Xie, Yatao Bian, Shuigeng Zhou, arXiv:2302.075412023arXiv preprint</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, Qi Liu, Advances in neural information processing systems. 202336</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Dialogue foundation model for chemistry. 2024arXiv preprint</p>
<p>Large language models in drug discovery and development: From disease mechanisms to clinical trials. Yizhen Zheng, Yee Huan, Maddie Koh, Li Yang, Lauren T Li, Geoffrey I May, Shirui Webb, George Pan, Church, 2024</p>
<p>Uni-mol: A universal 3d molecular representation learning framework. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Cd-gpt: a biological foundation model bridging the gap between molecular sequences through central dogma. Xiao Zhu, Chenchen Qin, Fang Wang, Fan Yang, Bing He, Yu Zhao, Jianhua Yao, bioRxiv. 2024</p>
<p>The psycho-biology of language: An introduction to dynamic philology. George Kingsley, Zipf , 1936Routledge</p>            </div>
        </div>

    </div>
</body>
</html>
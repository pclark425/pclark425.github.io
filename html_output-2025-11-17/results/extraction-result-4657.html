<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4657 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4657</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4657</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269149341</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.09982v2.pdf" target="_blank">Memory Sharing for Large Language Model based Agents</a></p>
                <p><strong>Paper Abstract:</strong> The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions. This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process. This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories. The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence. Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever. We evaluated our framework across three distinct domains involving specialized tasks of agents. The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4657.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4657.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Sharing (MS) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that enables multiple LLM-based agents to share, store, retrieve and continuously train a retriever on Prompt-Answer (PA) "memories" (natural-language PA pairs) to improve In-Context Learning for open-ended tasks via a shared memory pool, LLM-based scoring, and continuous retriever training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory Sharing (MS) framework (multi-agent system)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-agent system where each agent is an LLM-based generator; interactions are recorded as Prompt-Answer (PA) pairs. PA pairs that pass an LLM-driven rubric are stored in a shared memory pool; a dense retriever fetches relevant PA pairs to augment future prompts. The retriever is continuously trained on newly added memories using LLM-evaluated positive/negative labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented shared memory (PA pair vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memories are stored as natural-language Prompt-Answer (PA) pairs in a shared memory pool (vector index). A dense retriever (cosine-similarity over embeddings; initial BM25 candidates for training) retrieves top-n similar PA pairs given a query; selected PA pairs are concatenated to the query as in-context examples. New PA pairs that meet scoring thresholds are appended to the pool and used to further train the retriever online.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-ended text generation and problem-solving across three domains</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents address open-ended tasks in three domains: Literary Creation (sonnets, limericks, Wuyanlvshi), Unconventional Logic Problem-solving (puzzles, riddles, puns), and Plan Generation (study, travel, fitness plans). Each task requires producing creative or executable free-text outputs that satisfy a prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Internal datasets (nine agent-specific datasets; total 1000 QA pairs; 20% seed memories, 40% used to generate live memories, 40% test set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper reports systematic improvements when retrieved PA-pair memories are included; example (three-shot retrieval, Domain-pool, backbone gpt-3.5-turbo, measured by BERTScore): Limerick 0.87, Wuyanlvshi 0.72, Sonnet 0.53, Puzzle 0.60, Pun 0.70, Riddle 0.88, Fitness 0.65, Study 0.63, Travel 0.71 (BERTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Zero-shot baseline is reported qualitatively as lower (paper states other few-shot/memory strategies outperform zero-shot), but precise per-agent zero-shot numerical values are not given in the provided excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Retrieval-augmented ICL using shared PA-pair memories improves agent outputs on open-ended tasks relative to zero-shot. Three-shot (retrieving three memories) often yielded the best performance for most agents. Domain-specific memory pools (Domain-pool) outperform a single combined cross-domain pool (Single-pool) for most agents (Single-pool reduced performance except for one agent 'Study'). Performance generally increases as more high-quality memories are added (20%→100%), though some agents plateau when new memories are not more suitable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Initial memory scarcity requires an interactive bootstrapping process; memories are single-interaction PA pairs and may omit multi-step context; cross-domain pooled memories can harm performance for some agents (homologous/domain-specific memories often more helpful); performance plateaus if newly added memories are not more relevant; reliance on LLMs for scoring introduces potential bias and requires manual rubric review.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Sharing self-generated high-quality PA pairs among multiple agents yields collective improvement (collective intelligence). Continuous retriever training on newly added memories increases retrieval relevance over time. Domain-specific memory pools are generally more effective than a combined single pool. Retrieving a small set (e.g., three) of suitable memories often gives the largest gains for open-ended generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4657.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4657.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literary Creation agents (Limerick, Wuyanlvshi, Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three LLM-based agents specialized in poetry: Limerick, Wuyanlvshi (classical Chinese form), and Sonnet; they use the MS shared PA-pair memory pool to improve creative text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Literary Creation agents (Limerick, Wuyanlvshi, Sonnet)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Each agent is an LLM fine-tuned/prompted for a poetry form; backbones used in experiments include gpt-3.5-turbo, gpt-4o (closed-source), and open-mistral-7b (open-source). They generate poetry fulfilling descriptive constraints provided in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>shared retrieval-augmented in-context memory (PA pair store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Agents retrieve PA-pair memories from the Domain-pool (literary domain) and concatenate them with the query as few-shot examples (zero/one/two/three-shot retrieval were tested). High-scoring PA pairs from any agent are added to the shared pool and used to further train the dense retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Creative poetry generation (form-constrained generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate poetry matching the prompt's form and content constraints (e.g., craft a sonnet exploring a theme, create a Wuyanlvshi). Evaluation uses BERTScore to measure similarity/quality relative to ground-truth poetic responses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Internal Literary Creation dataset (subset of the 1000-pair dataset; 20% seed memories, 40% for live memory generation, 40% test set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Three-shot, Domain-pool, gpt-3.5-turbo (BERTScore): Limerick 0.87; Wuyanlvshi 0.72; Sonnet 0.53 (examples from Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Zero-shot baseline reported as lower in the paper (numerical zero-shot scores not provided in excerpt); authors note that three-shot with open-source LLM can surpass closed-source zero-shot for some literary tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Shared domain memories improve poetry generation quality. Domain-pool (domain-specific memories) typically yields higher performance than Single-pool for these agents. Gains in Literary Creation were sometimes smaller than in other domains, possibly due to heterogeneity of language/style in stored memories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Differences in language/style of stored memories can reduce benefit (e.g., literary language mismatch). Single pooled cross-domain memories sometimes reduce performance. The paper notes that literary agents showed smaller improvements, requiring further study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Domain-aligned PA-pair memories are particularly helpful for creative generation; even open-source LLMs can outperform closed-source zero-shot when supplied with relevant shared memories; memory diversity helps, but style/domain alignment matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4657.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4657.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unconventional Logic Problem-solving agents (Puzzle, Pun, Riddle)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three agents specialized in solving unconventional logic problems (puzzles, pun-based answers, riddles) that use MS shared memories to improve reasoning and solution generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Unconventional Logic Problem-solving agents (Puzzle, Pun, Riddle)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agents tasked with producing correct, concise solutions or explanations for logical puzzles, riddles, and puns; evaluated using BERTScore against curated 'standard' answers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>shared retrieval-augmented PA-pair memory pool (episodic-like external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Agents retrieve similar past Prompt-Answer pairs (PA pairs) from a Domain-pool for logic tasks; retrieved examples are appended to the query as in-context demonstrations; high-quality new PA pairs are scored and written back to the pool.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Solve unconventional logic problems (puzzles, puns, riddles)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a logical puzzle or riddle prompt, generate a reasonable solution or explanation. The dataset includes items like classic puzzles and explanatory answers; metric used is BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Internal Logic dataset (subset of the 1000-pair dataset; Puzzle dataset from Liu et al., 2023c referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Three-shot, Domain-pool, gpt-3.5-turbo (BERTScore examples): Puzzle 0.60; Pun 0.70; Riddle 0.88 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Zero-shot baseline reported as lower qualitatively; exact numerical zero-shot scores not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Memory retrieval substantially helps reasoning for these tasks; three-shot retrieval often produced best results. Domain-pool memory better than Single-pool for these agents in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cross-domain (Single-pool) memories can reduce effectiveness; memory quality matters (poor or irrelevant memories do not help and can stagnate improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Retrieval of exemplars (PA pairs) helps LLMs generalize to new logic problems via ICL; continuous updating of the retriever with newly validated PA pairs improves future retrievals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4657.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4657.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlanAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan Generation agents (Fitness, Study, Travel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three agents specialized in producing actionable plans (study plans, travel itineraries, fitness plans) that leverage the MS framework's shared memories to improve plan quality and executability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Plan Generation agents (Fitness, Study, Travel)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agents that generate stepwise actionable plans for practical goals; backbones used include gpt-3.5-turbo, gpt-4o, and open-mistral-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>shared retrieval-augmented PA-pair memory pool (external plan exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Agents retrieve previously generated plan PA pairs from the Domain-pool and use them as in-context examples to produce improved and more executable plans; newly validated PA pairs are appended to the shared pool and used to further train the retriever.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Generate executable plans (study, travel, fitness)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce step-by-step, realistic plans for a stated objective (e.g., 'Start learning Python for data analysis'). Evaluation via BERTScore against curated standard answers.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Internal Plan Generation dataset (subset of the 1000-pair dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Three-shot, Domain-pool, gpt-3.5-turbo (BERTScore examples): Fitness 0.65; Study 0.63; Travel 0.71 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Zero-shot baseline described as inferior; numerical zero-shot results are not provided explicitly in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Shared domain memories improve plan generation quality; Domain-pool generally outperformed Single-pool. Increasing the number of high-quality memories tends to improve performance until a plateau.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance plateaus when newly added memories are not more relevant; Single-pool (cross-domain) mixing sometimes reduces agent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Storing and reusing successful plan PA pairs helps agents produce more actionable and higher-quality plans; continuous retriever training on validated memories helps maintain retrieval relevance as the pool grows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory Sharing for Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memgpt: Towards llms as operating systems. <em>(Rating: 2)</em></li>
                <li>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. <em>(Rating: 2)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior. <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Learning to retrieve in-context examples for large language. <em>(Rating: 2)</em></li>
                <li>Learning to retrieve prompts for in-context learning. <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4657",
    "paper_id": "paper-269149341",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "MS",
            "name_full": "Memory Sharing (MS) framework",
            "brief_description": "A framework that enables multiple LLM-based agents to share, store, retrieve and continuously train a retriever on Prompt-Answer (PA) \"memories\" (natural-language PA pairs) to improve In-Context Learning for open-ended tasks via a shared memory pool, LLM-based scoring, and continuous retriever training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Memory Sharing (MS) framework (multi-agent system)",
            "agent_description": "A multi-agent system where each agent is an LLM-based generator; interactions are recorded as Prompt-Answer (PA) pairs. PA pairs that pass an LLM-driven rubric are stored in a shared memory pool; a dense retriever fetches relevant PA pairs to augment future prompts. The retriever is continuously trained on newly added memories using LLM-evaluated positive/negative labeling.",
            "memory_type": "external retrieval-augmented shared memory (PA pair vector store)",
            "memory_description": "Memories are stored as natural-language Prompt-Answer (PA) pairs in a shared memory pool (vector index). A dense retriever (cosine-similarity over embeddings; initial BM25 candidates for training) retrieves top-n similar PA pairs given a query; selected PA pairs are concatenated to the query as in-context examples. New PA pairs that meet scoring thresholds are appended to the pool and used to further train the retriever online.",
            "task_name": "Open-ended text generation and problem-solving across three domains",
            "task_description": "Agents address open-ended tasks in three domains: Literary Creation (sonnets, limericks, Wuyanlvshi), Unconventional Logic Problem-solving (puzzles, riddles, puns), and Plan Generation (study, travel, fitness plans). Each task requires producing creative or executable free-text outputs that satisfy a prompt.",
            "benchmark_name": "Internal datasets (nine agent-specific datasets; total 1000 QA pairs; 20% seed memories, 40% used to generate live memories, 40% test set)",
            "performance_with_memory": "Paper reports systematic improvements when retrieved PA-pair memories are included; example (three-shot retrieval, Domain-pool, backbone gpt-3.5-turbo, measured by BERTScore): Limerick 0.87, Wuyanlvshi 0.72, Sonnet 0.53, Puzzle 0.60, Pun 0.70, Riddle 0.88, Fitness 0.65, Study 0.63, Travel 0.71 (BERTScore).",
            "performance_without_memory": "Zero-shot baseline is reported qualitatively as lower (paper states other few-shot/memory strategies outperform zero-shot), but precise per-agent zero-shot numerical values are not given in the provided excerpt.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Retrieval-augmented ICL using shared PA-pair memories improves agent outputs on open-ended tasks relative to zero-shot. Three-shot (retrieving three memories) often yielded the best performance for most agents. Domain-specific memory pools (Domain-pool) outperform a single combined cross-domain pool (Single-pool) for most agents (Single-pool reduced performance except for one agent 'Study'). Performance generally increases as more high-quality memories are added (20%→100%), though some agents plateau when new memories are not more suitable.",
            "limitations_or_challenges": "Initial memory scarcity requires an interactive bootstrapping process; memories are single-interaction PA pairs and may omit multi-step context; cross-domain pooled memories can harm performance for some agents (homologous/domain-specific memories often more helpful); performance plateaus if newly added memories are not more relevant; reliance on LLMs for scoring introduces potential bias and requires manual rubric review.",
            "key_insights": "Sharing self-generated high-quality PA pairs among multiple agents yields collective improvement (collective intelligence). Continuous retriever training on newly added memories increases retrieval relevance over time. Domain-specific memory pools are generally more effective than a combined single pool. Retrieving a small set (e.g., three) of suitable memories often gives the largest gains for open-ended generation tasks.",
            "uuid": "e4657.0",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LitAgents",
            "name_full": "Literary Creation agents (Limerick, Wuyanlvshi, Sonnet)",
            "brief_description": "Three LLM-based agents specialized in poetry: Limerick, Wuyanlvshi (classical Chinese form), and Sonnet; they use the MS shared PA-pair memory pool to improve creative text generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Literary Creation agents (Limerick, Wuyanlvshi, Sonnet)",
            "agent_description": "Each agent is an LLM fine-tuned/prompted for a poetry form; backbones used in experiments include gpt-3.5-turbo, gpt-4o (closed-source), and open-mistral-7b (open-source). They generate poetry fulfilling descriptive constraints provided in prompts.",
            "memory_type": "shared retrieval-augmented in-context memory (PA pair store)",
            "memory_description": "Agents retrieve PA-pair memories from the Domain-pool (literary domain) and concatenate them with the query as few-shot examples (zero/one/two/three-shot retrieval were tested). High-scoring PA pairs from any agent are added to the shared pool and used to further train the dense retriever.",
            "task_name": "Creative poetry generation (form-constrained generation)",
            "task_description": "Generate poetry matching the prompt's form and content constraints (e.g., craft a sonnet exploring a theme, create a Wuyanlvshi). Evaluation uses BERTScore to measure similarity/quality relative to ground-truth poetic responses.",
            "benchmark_name": "Internal Literary Creation dataset (subset of the 1000-pair dataset; 20% seed memories, 40% for live memory generation, 40% test set)",
            "performance_with_memory": "Three-shot, Domain-pool, gpt-3.5-turbo (BERTScore): Limerick 0.87; Wuyanlvshi 0.72; Sonnet 0.53 (examples from Table 1).",
            "performance_without_memory": "Zero-shot baseline reported as lower in the paper (numerical zero-shot scores not provided in excerpt); authors note that three-shot with open-source LLM can surpass closed-source zero-shot for some literary tasks.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Shared domain memories improve poetry generation quality. Domain-pool (domain-specific memories) typically yields higher performance than Single-pool for these agents. Gains in Literary Creation were sometimes smaller than in other domains, possibly due to heterogeneity of language/style in stored memories.",
            "limitations_or_challenges": "Differences in language/style of stored memories can reduce benefit (e.g., literary language mismatch). Single pooled cross-domain memories sometimes reduce performance. The paper notes that literary agents showed smaller improvements, requiring further study.",
            "key_insights": "Domain-aligned PA-pair memories are particularly helpful for creative generation; even open-source LLMs can outperform closed-source zero-shot when supplied with relevant shared memories; memory diversity helps, but style/domain alignment matters.",
            "uuid": "e4657.1",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LogicAgents",
            "name_full": "Unconventional Logic Problem-solving agents (Puzzle, Pun, Riddle)",
            "brief_description": "Three agents specialized in solving unconventional logic problems (puzzles, pun-based answers, riddles) that use MS shared memories to improve reasoning and solution generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Unconventional Logic Problem-solving agents (Puzzle, Pun, Riddle)",
            "agent_description": "LLM-based agents tasked with producing correct, concise solutions or explanations for logical puzzles, riddles, and puns; evaluated using BERTScore against curated 'standard' answers.",
            "memory_type": "shared retrieval-augmented PA-pair memory pool (episodic-like external memory)",
            "memory_description": "Agents retrieve similar past Prompt-Answer pairs (PA pairs) from a Domain-pool for logic tasks; retrieved examples are appended to the query as in-context demonstrations; high-quality new PA pairs are scored and written back to the pool.",
            "task_name": "Solve unconventional logic problems (puzzles, puns, riddles)",
            "task_description": "Given a logical puzzle or riddle prompt, generate a reasonable solution or explanation. The dataset includes items like classic puzzles and explanatory answers; metric used is BERTScore.",
            "benchmark_name": "Internal Logic dataset (subset of the 1000-pair dataset; Puzzle dataset from Liu et al., 2023c referenced)",
            "performance_with_memory": "Three-shot, Domain-pool, gpt-3.5-turbo (BERTScore examples): Puzzle 0.60; Pun 0.70; Riddle 0.88 (Table 1).",
            "performance_without_memory": "Zero-shot baseline reported as lower qualitatively; exact numerical zero-shot scores not provided in the excerpt.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Memory retrieval substantially helps reasoning for these tasks; three-shot retrieval often produced best results. Domain-pool memory better than Single-pool for these agents in most cases.",
            "limitations_or_challenges": "Cross-domain (Single-pool) memories can reduce effectiveness; memory quality matters (poor or irrelevant memories do not help and can stagnate improvements).",
            "key_insights": "Retrieval of exemplars (PA pairs) helps LLMs generalize to new logic problems via ICL; continuous updating of the retriever with newly validated PA pairs improves future retrievals.",
            "uuid": "e4657.2",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PlanAgents",
            "name_full": "Plan Generation agents (Fitness, Study, Travel)",
            "brief_description": "Three agents specialized in producing actionable plans (study plans, travel itineraries, fitness plans) that leverage the MS framework's shared memories to improve plan quality and executability.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Plan Generation agents (Fitness, Study, Travel)",
            "agent_description": "LLM-based agents that generate stepwise actionable plans for practical goals; backbones used include gpt-3.5-turbo, gpt-4o, and open-mistral-7b.",
            "memory_type": "shared retrieval-augmented PA-pair memory pool (external plan exemplars)",
            "memory_description": "Agents retrieve previously generated plan PA pairs from the Domain-pool and use them as in-context examples to produce improved and more executable plans; newly validated PA pairs are appended to the shared pool and used to further train the retriever.",
            "task_name": "Generate executable plans (study, travel, fitness)",
            "task_description": "Produce step-by-step, realistic plans for a stated objective (e.g., 'Start learning Python for data analysis'). Evaluation via BERTScore against curated standard answers.",
            "benchmark_name": "Internal Plan Generation dataset (subset of the 1000-pair dataset)",
            "performance_with_memory": "Three-shot, Domain-pool, gpt-3.5-turbo (BERTScore examples): Fitness 0.65; Study 0.63; Travel 0.71 (Table 1).",
            "performance_without_memory": "Zero-shot baseline described as inferior; numerical zero-shot results are not provided explicitly in the excerpt.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Shared domain memories improve plan generation quality; Domain-pool generally outperformed Single-pool. Increasing the number of high-quality memories tends to improve performance until a plateau.",
            "limitations_or_challenges": "Performance plateaus when newly added memories are not more relevant; Single-pool (cross-domain) mixing sometimes reduces agent performance.",
            "key_insights": "Storing and reusing successful plan PA pairs helps agents produce more actionable and higher-quality plans; continuous retriever training on validated memories helps maintain retrieval relevance as the pool grows.",
            "uuid": "e4657.3",
            "source_info": {
                "paper_title": "Memory Sharing for Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memgpt: Towards llms as operating systems.",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Memochat: Tuning llms to use memos for consistent long-range open-domain conversation.",
            "rating": 2,
            "sanitized_title": "memochat_tuning_llms_to_use_memos_for_consistent_longrange_opendomain_conversation"
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory.",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_enviroments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior.",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models.",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Learning to retrieve in-context examples for large language.",
            "rating": 2,
            "sanitized_title": "learning_to_retrieve_incontext_examples_for_large_language"
        },
        {
            "paper_title": "Learning to retrieve prompts for in-context learning.",
            "rating": 1,
            "sanitized_title": "learning_to_retrieve_prompts_for_incontext_learning"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01396425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Memory Sharing for Large Language Model based Agents
5 Jul 2024</p>
<p>Hang Gao h.gao@rutgers.edu 
Department of Computer Science
Rutgers University New Brunswick
NJ, US</p>
<p>Yongfeng Zhang yongfeng.zhang@rutgers.edu 
Department of Computer Science
Rutgers University New Brunswick
NJ, US</p>
<p>Memory Sharing for Large Language Model based Agents
5 Jul 202497F07E6CB38132506F9C4EE357A4448EarXiv:2404.09982v2[cs.CL]
The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions.This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process.This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories.The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence.Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever.We evaluated our framework across three distinct domains involving specialized tasks of agents.The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.</p>
<p>Introduction</p>
<p>The emergence of Large Language Models (LLMs) has brought about significant transformations in machine learning and conversational AI, while the advent of In-Context Learning (ICL) (Brown et al., 2020) signifies a more subtle evolution.ICL facilitates dynamic and intuitive interactions between LLMs and users, enabling LLMs to perform tasks using few-shot examples without necessitating any updates to the model parameters.This greatly expands the capabilities of LLM-based agents.</p>
<p>Figure 1: The Memory Sharing framework.Whenever a new Prompt-Answer(PA) pair is generated, it will be considered to be added to the memory pool and train the retriever.</p>
<p>Initially, ICL was introduced to facilitate LLMbased agents in achieving better performance with few-shot examples (Brown et al., 2020), and subsequently extended across various domains (Ahmed and Devanbu, 2022;Izacard et al., 2023).Following this, the proposal of Chain-of-Thought (CoT) prompting significantly augmented the proficiency of LLM-based agents in executing arithmetic tasks (Wei et al., 2022).Building upon this foundation, innovative methodologies such as PAL (Gao et al., 2023) and the integration of LLMs with symbolic solvers (He-Yueya et al., 2023) have been developed to further enhance agent capabilities in tackling reasoning tasks.Recent works has also developed agent which can continuously acquire diverse skills and make novel discoveries (Wang et al., 2023).While as the areas of questions continue to expand, especially for the open-ended questions, aiming to enable the agent to make more desired answers through ICL, it is particularly important to enrich the diversity of examples, since the agents can learn more information from different angle.</p>
<p>By combining Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) with ICL, for a particular question, the number of relevant examples available to agents has increased significantly.And subsequently facilitated more effective generation in open-domain queries (Mao et al., 2021).In recent developments, self-learning techniques have been also integrated with the retrieval mechanism within ICL to refine model performance in text generation tasks, through the retrieval of examples with the most analogous patterns (Rubin et al., 2022;Wang et al., 2024).Although for a certain question, the number of relevant examples available to agents has increased significantly through RAG, it also means that this depends heavily on the quality of the external database at the same time.Sometimes, it is also very likely that a suitable external database for some problems cannot be find.</p>
<p>As consequence, given the diversity of examples needed to help agents better answer the open-ended questions in the ICL and diminishing the dependence on external database, there is a critical need to get continuously generated high quality examples.To further minimize the dependence of LLMbased agents on external data and enhance their performance, this paper introduces the Memory Sharing (MS) framework.The MS framework is designed to enable multiple agents to share memories, where the interaction and shared memories among multiple agents enhance memory diversity.The collective self-enhancement achieved through multi-agent interaction represents a progression from individual intelligence to collective intelligence.Additionally, we have developed an interactive learning method that facilitates rapid growth and dynamic updating of memory through multiagent interactions.Consequently, the diversity and rapid expansion of memory effectively improve the agents' ability to respond to open-ended questions.</p>
<p>Specifically, within the MS framework, the input and output of an agent in a single interaction are conceptualized as a Prompt-Answer (PA) pair, also considered as a memory, and the shared memory pool is composed of memories from different agents.This framework introduces an innovative real-time memory storage and retrieval mechanism, aimed at enhancing the shared memory pool by receiving PA pairs from different agents.During the storage phase, each PA pair undergoes rigorous evaluation by a dedicated LLM evaluator to determine its suitability for inclusion in the memory pool.The newly added memories then serve as references for the agents' subsequent performance.The retrieval phase is coordinated by an autonomous learning retriever, calibrated to ensure the inclusion of particularly relevant memories in prompts, thereby enhancing the agents' understanding of the query's essence.Figure .1 illustrates the MS framework.Similar to human self-learning mechanisms, incorporating self-generated memories into prompts significantly improves the agents' comprehension of query meanings.Moreover, continuously adding new memories to the pool not only enriches it but also refines the retriever's performance in selecting relevant memories.Our empirical evidence demonstrates that this approach greatly assists LLM-based agents in generating outputs that better align with user expectations.</p>
<p>We evaluate the MS framework through three divergent domains where each domain involved the participation of three agents, and our finding suggests that incremental additions to the memory pool have led to enhancements in the precision and relevance of outputs.This research delineates the MS framework's capacity to mitigate the inherent constraints associated with ICL, thereby underscoring its potential applicability and effectiveness.</p>
<p>Overall, our main contributions can be summarized as follows:</p>
<ol>
<li>
<p>Constructing PA pairs from the answers generated by multiple agents and storing them as "memories" in shared memory pool.The diversity of group memories from different agents, along with the real-time dynamic growth of memories, significantly aids in improving the subsequent behavior of the agents.Additionally, the shared memories in the memory pool are used to enhance the performance of the retriever.</p>
</li>
<li>
<p>Addressing the problem of memory scarcity (data scarcity) by proposing an interactive learning method that allows different agents to rapidly grow their memories through interactive prompt and answer, thereby achieving collective enhancement quickly.</p>
</li>
<li>
<p>Conducting extensive experiments on various types of open-ended tasks to verify the effectiveness of the proposed MS framework.The experimental results show that MS not only help the agents get more expected answers, but also continuously input high quality memories into the memory pool and establish a reliable database for agents.</p>
</li>
</ol>
<p>In the following, Section 2 delineates relevant works.An exhaustive elucidation of the MS framework, inclusive of its conceptual underpinnings and operational methodologies, is presented in Section 3. Section 4 provides empirical validation of the framework's enhanced capability to address openended questions.The conclusion, presented in the Section 5, not only summarizes the findings but also explores prospective avenues for future development of the MS framework, which may better help improve the LLM-based agents.</p>
<p>Related Work</p>
<p>The Memory Mechanism in Agents</p>
<p>Equipping agents with memory mechanisms to enhance their abilities has attracted the attention of researchers.Memory can play an important role in helping agents remember conversation information, maintain behavioral consistency, and accumulate experience.Generative agents enhanced with memory features can store vast experience records, facilitating deeper self understanding (Park et al., 2023), while VOYAGER has developed a skill library that evolves by incorporating successful action programs, optimizing task resolution (Wang et al., 2023).In the case of Ghost in the Minecraft, a text-based memory system supports LLMs in maintaining reference plans for efficient plan formulation when similar objectives arise (Zhu et al., 2023).Later, based on the concept of "memorizationretrieval-response", Memochat was proposed for maintaining consistent long-range open-domain conversation (Lu et al., 2023).With the emergence of MemGPT (Packer et al., 2023), a new memory hierarchy was developed to process long texts and maintain the long-term memory.And the TiM make LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream to a reality (Liu et al., 2023a).Also, through maintain agents' own reflective text in an episodic memory buffer and implementing the exemplar memory, the Reflexion (Shinn et al., 2023) and SYNAPSE (Zheng et al., 2023) successfully induce better decision-making and generalize successful trajectories to new task respectively.Our MS framework is primarily designed to enable multiple agents to share memories, facilitating collective self-enhancement through inter-agent interactions.In previously mentioned agents equipped with memory mechanisms, the memory mainly ensures conversational consistency and stores past experiences to achieve individual enhancement.However, our MS framework achieves collective enhancement through shared memories, providing a pathway for the evolution from individual intelligence to collective intelligence.</p>
<p>In-Context Learning</p>
<p>ICL enhances the problem-solving capabilities of LLMs by incorporating few-shot examples into prompts (Brown et al., 2020;Levine et al., 2021;Zhou et al., 2022;Liu et al., 2023b;White et al., 2023).Research has demonstrated that ICL can foster creative learning in LLMs to a certain extent (Swanson et al., 2021).By redesigning inputs, LLMs become more adept at handling logical challenges (Wiegreffe et al., 2022;Wu et al., 2022).Crowdsourced instructions also contribute to improved performance in LLMs (Mishra et al., 2022).Additionally, elucidating the relationship between examples and tasks has been shown to be highly beneficial for LLMs (Lampinen et al., 2022), while the CoT (Wei et al., 2022) and PAL (Gao et al., 2023), enhance LLMs' performance in complex reasoning tasks by introducing intermediate reasoning steps.However, when dealing with openended questions, agents still face two primary challenges: insufficient problem descriptions, which impair the agents' comprehension, and the lack of external knowledge bases and available reference materials.Our MS framework addresses these challenges by converting high-quality content generated by various LLM-based agents into shared memories, providing agents with useful reference examples, thereby improving their performance in open-ended questions.</p>
<p>Retrieval Augmented Generation</p>
<p>RAG (Lewis et al., 2020;Ram et al., 2023;Shi et al., 2023) is a method that enhances LLMs' ability to generate accurate and timely content by integrating retrieval techniques such as BM25 (Luo et al., 2023;Liu et al., 2022) or SBERT (Reimers and Gurevych, 2019).Using dense retrievers combined with contrastive learning for feedback can effectively enhance the performance (Rubin et al., 2022).Furthermore, iteratively training the retriever with contrastive learning can further improve its performance (Wang et al., 2024).However, the retrievers in the aforementioned studies are typically trained only once before deployment, making it challenging to adapt to newly generated data.In contrast, the retriever in our MS framework undergoes continuous training.Whenever new memories are added to the memory pool, they are used for further training of the retriever.This continuous updating and evolution process ensures that the quality of the retrieved memories gradually improves over time.And most importantly, with the help of MS framework, the dependence on external databases has been greatly reduced.1) + (2) The retriever take the original query from agent as the input, retrieve the suitable memories from the memory pool and concatenate them to the query to form the prompt.</p>
<p>(3) The Agent (Sonnet) takes the prompt and makes an answer, pack them as (Prompt, Answer).( 4) Scorer generates a score according to the designed rubric for (Prompt, Answer), while (Prompt, Answer) pairs with high scores will be added into the Memory Pool and also be sent to train the Retriever.</p>
<p>All agents share the same Memory Pool; they can write memories into the pool and retrieve memories from the pool so that they can share memories with each other.</p>
<p>The Memory Sharing framework</p>
<p>In this section, we provide an in-depth description of our innovative Memory Sharing (MS) framework.MS is a framework designed to enhance the performance of multiple LLM-based agents through shared memories, while preserving the original creativity and versatility of the agents.Fig. 2 show how an agent work within the MS framework.Different agents interact through a Prompt-Answer format, where the results of these interactions are evaluated.High-quality interactions are transformed into memories and stored in a shared memory pool accessible by all agents.</p>
<p>Additionally, new memories are used to train and improve the memory retriever.During subsequent interactions, these stored memories are retrieved to enhance the performance of agents.The underlying idea is intuitive: it represents a crucial step from individual intelligence towards a more powerful collective intelligence.Similar to a group of individuals engaging in prompt and answer interactions, the valuable content is recorded and shared, serving as a reference for future questions.Consequently, after several rounds of interactions, the quality of answers to related questions typically improves.This collective memory-sharing mechanism, enriched by the diversity of memories contributed by different agents, provides more effective assistance in addressing open-ended questions.The main principles and technologies of the MS framework are detailed in three sections below.</p>
<p>Memory Generation</p>
<p>A memory is essentially a Prompt-Answer (PA) pair.In some special cases, it is permissible for a PA pair to lack a prompt, typically applicable in initial scenarios.These PA pairs are stored in natural language, which serves as the shared memories.</p>
<p>On one hand, these shared memories can be used to improve the response quality of different agents; on the other hand, they are generated by various agents and stored in the shared memory pool.The dynamic expansion of the shared memory pool ensures a continuous influx of new memories, thereby enriching the knowledge base of all agents.In addressing open-ended questions, these shared memories provide agents with a broader perspective and deeper understanding, which is crucial for generating high quality answers.Fig. 3 shows how the Agent-Puzzle use the memory to make a prompt and get the answer through one-shot learning.</p>
<p>After each interaction, the PA pair is scored.If the score of PA pair exceeds a preset threshold, the answer and its corresponding prompt are packaged as a useful memory and stored in the memory.When scoring the answers, we established different scoring criteria for various domains and topics, delegating the scoring tasks to the LLM itself.To facilitate the LLM's understanding of these criteria, they were autonomously generated by the LLM, She walked out of the elevator door on the sixth floor and never came back, why?What comes once in a minute, twice in a moment, but never in a thousand years?-&gt;The letter m.</p>
<p>What comes once in a minute, twice in a moment, but never in a thousand years?-&gt;The letter m.Now, learning from above question and answer, what is the answer of the question: She walked out of the elevator door on the sixth floor and never came back, why?</p>
<p>She unfortunately met with an accident or encountered an insurmountable obstacle that prevented her return.based on the reasonable assumption that the LLM can better grasp criteria it designed.Therefore, these scoring criteria are established prior to the deployment of the framework to ensure consistency in the LLM's scoring process, thereby guaranteeing fair evaluation of memories from different agents.Before implementation, these criteria undergo a manual review phase, assessing the relevance of potential memories to the current focal task and their relevance within the domain to ensure their utility.Manual review, rather than delegation to the LLM, is employed to provide additional precision and special consideration to align with the agents' specific needs, particularly in potential or special application scenarios.The manual review of these scoring criteria requires minimal workload.</p>
<p>Query</p>
<p>Memory Retrieval and Training</p>
<p>Prior to the operational deployment of the MS, a small subset of instances was manually archived within the memory pool.These instances fulfill a dual purpose: firstly, they provide a diversified array of memories upon which each agent may experiment with novel prompts in the face of new queries; secondly, they constitute the preliminary training corpus for our retriever.This foundational training regimen mirrors the methodology by which subsequently archived memories will be assimilated into our model in real time, thereby facilitating the model's ongoing adaptive learning and optimization.During the answering phase, an agent retrieves memories from the shared memory pool based on the question with the help of a dense retriever, which are more similar to the target question in terms of cosine similarity.These retrieved memories, combined with question, form a prompt that is submitted to the agent, which then generates an answer.The memories extracted from the shared memory are used as context to enhance the quality of the agent's response, a typical ICL method that usually improves the answer quality.</p>
<p>Memory Train.Whenever a new memory, denoted as (X, Y ), is added into the memory pool, it will also be used to train our retriever, which help the retriever to continuously update itself and continuously adapt to new memory.Based on the new generated memory (X, Y ), the classical method BM25 ascertain the most pertinent top-n candidate pairs {(x i , y i )} n i=1 , sourced from the diverse and extensive memory pool, labeled as C.Each candidate within C will undergo a evaluation process utilizing the comprehensive scoring capabilities of LLMs.The scoring mechanism employed is defined by the following equation:
p(x i , y i ) = P(¬Y | (x i , y i ), X), i ∈ {1, ..., n}
(1) This equation seeks to determine, given a inputoutput pair (x i , y i ) in C as a condition, the probability that the response generated for the input in the new memory contradicts the output in the new memory.This grading part serves as a preparatory step for the subsequent labeling of each candidate example.It is noteworthy that making ¬Y as the result part is trying to make sure that the memory that the retriever gets from other agents is of reference value, but it does not have to be the most relevant to the current question, which means that it can help the current agent to learn from new examples.This approach diverges from a simplistic reliance on Y as the outcome, which tends to restrict the retrieval process to memory previously stored by the current agent.</p>
<p>Within the defined set C = {(x i , y i )} n i=1 , each candidate now is ascribed a score.We sort them from the lowest to the highest score and we select v memory in total to label.The top v 2 candidates (lowest score) in C are identified as being the pair with the reference value to (X, Y ) and accordingly, their labels are set to positive.Conversely, the bottom v 2 candidates are deemed as the least reference value to (X, Y ), and their labels are thus designated as negative.Those labeled data will be used to minimize the following function:
loss(x, y) = − 1 v v i=1 [y i • log( 1 1 + e −x i )+ (1 − y i ) • log(1 − 1 1 + e −x i )]
(2)</p>
<p>It enhances the predictive accuracy, which is es-pecially critical in handling imbalanced memory pool.This strategic choice underscores our model's preparation to extract meaningful insights from various memory, advancing our overarching goal of developing a robust and adaptable MS framework.</p>
<p>Interactive Learning</p>
<p>As described in sections 3.1 and 3.2, the memories stored within the memory pool effectively help agents improve their response quality.Moreover, the number of memories in memory pool is dynamically expanding.Over time, the increasing number of memories in memory pool enhances the assistance provided to the agents.However, the initial state of a newly created multi-agent system and its corresponding memory pool lacks any memories.How can it assist the agents from the outset?And how can we validate the effectiveness of this mechanism?To address this, we propose a rapid interactive learning method that enables multiple agents to engage in interactive prompt and answer.High-quality PA pair is stored as memory in the memory pool, facilitating self-learning and self-enhancement within the multi-agent system.Initially, a small set of answers (e.g., 100 records, theoretically even one record can initiate the process) is placed in the memory as the initial set.Agents then engage in prompt and answer based on this initial memory set, rapidly expanding the memory pool.Specially, give a standard answer, we ask the agent to give a corresponding question based on the standard answer.Then, we give this question back to agents again and ask them to answer this question.This is also the way how we construct our dataset and our initial memory pool.We measure the effectiveness of the MS framework by calculating the average quality of answers generated by the agents at different stages.</p>
<p>4 Experiments The evaluation of memory impact begins with the implementation of different retrieval strategies, including zero-shot, one-shot, two-shot, and threeshot learning.Subsequently, quantitative and qualitative analysis are conducted.In qualitative analysis, one question is that the memories of agents in the same domain can make up for the lack of diversity, so if the memories of agents in all domains are placed in the same memory pool, from the perspective of diversity, it will definitely increase further, but will the memory pool still provide positive help to the agents' answers?Aiming to solve this, our study uses two distinct types of memory pools, Domain-pool and Single-pool.The Domain-pool means a dedicated memory pool is allocated for each domain and is shared for all agents with this domain, aiming at enhancing the integration of domain-specific memories.Conversely, the second pool integrates agents from all domains into a unified memory pool, facilitating the analysis of cross-domain memory utilization.Although not all the memories are very related to one single query, the memory comes from other unrelated agents in other domains may help the agents better understand the query from different angle.</p>
<p>In terms of the quantitative analysis, we measure the performance of each agent whenever the same proportion of new memory is added to the memory pool.There are five phase in total-20%, 40%, 60%, 80%, 100%.At each phase, an evaluation of agent performance was conducted to ascertain improvements or regressions.This dualfaceted approach enabled a thorough exploration of the naunced impacts and applicability of authentic memories across diverse domains.</p>
<p>Experiment Analysis</p>
<p>Before the experiment, none of the agents have a suitable database for reference.While after the interactive learning stage, a continuously expanding memory pool with high quality memories is successfully be a database for agent to refer.The MS framework help the agents get rid of the dependence on external databases, and multiple agents can interactively expand the memory pool.The principal outcomes of our experiments are presented in Table .1,which shows the performance of each agent under various learning strategies within the MS framework.Compared to Zero-shot learning, other three learning strategies all help agents achieve better performance, which means that the memory from other agents can help current agent get desired answers, rather than interfering with the agent's learning ability.Also, given the performance of all agents get improved after utilizing the shared memories, our previous hypothesis that the MS framework could enhance collective intelligence through multi-agent interactions, thereby advancing from individual to collective intelligence, has been confirmed.And for most agents, they achieve the best performance when they retrieve and utilize three memories from the memory pool.It worth noting that, for all the agents in the domains-Literary Creation and Plan Generation, the performance of them under the three-shot learn-ing with a open-source LLM successfully surpass themselves with close-source LLM under the zeroshot learning, which proves the potential power of the shared memories.Besides, when under the same learning strategy, the superior performance of the closed-source LLM over the open-source LLM can be attributed to the former's enhanced understanding and reasoning capabilities.Specifically, in the Literary Creation domain, the improvement of those three agents' performance are not as significant as other agents, which may resulted in the language used is different when storing memory.This deserves further study in future work.</p>
<p>Table .2compares the scenario where all agents use the Domain-pool or Single-pool under the threeshot learning strategy, since the prior experiments showed us that most agents achieve the best performance under the three-shot learning.Excluding Agent-Study, all other agents exhibited diminished performance with the Single-pool.Although the Single-pool can further enrich the diversity of shared memories, for the agents, homologous memories can better help them get more reliable answers, no matter what the agent's backbone is.</p>
<p>One follow up question is, even memory types are homogeneous, whether an excessive accumulation of memories will impede the agent's output quality.Figure.4shows the variations in performance across individual agents consequent to the integration of different ratios of newly generated memories into the pool.It is clear that as more and more high-quality memories are added to the  memory pool, the performance of most agents is getting better and better, especially for the Agent-Limerick.For several agents, there is no further change of their performance as the shared memories increases further in the later stages.We assume this is since those newly added shared memories are not more suitable than the previous ones.If the memory pool can be further expanded, this stagnation may be broken.</p>
<p>Conclusions</p>
<p>We introduce a novel MS framework that enables real-time memory sharing among multiple agents through memory storage and retrieval.The findings indicate that the continuously growing shared memory enhances the ability of LLM-based agents to understand the nuances of problems, leading to higher-quality responses to open-ended queries.Furthermore, these shared memories are utilized for iterative training and improving the retriever, ensuring that the retriever consistently identifies and selects the most relevant memories as the memory pool dynamically expands.Regarding future research directions, there is substantial work to be done in the areas of memory sharing and interactive learning among multiple agents.Introducing more agents based on different LLMs (e.g., GPT-4, LLaMA-3, Claude-2) to comprehensively explore and evaluate the benefits of memory sharing and interactive learning for agents is a topic worthy of long-term investigation, as it will guide us from individual intelligence to collective intelligence.Additionally, in some open-ended problem domains, exploring the use of self-generated memories by these agents for the retraining and fine-tuning of LLMs is also an interesting area to study.</p>
<p>Limitations</p>
<p>In our work, the shared memory collected and used by agents is generated through one interaction, which means that the question part is the query and the answer part is the output.However, sometimes users will ask seemingly unrelated questions first, which may not be very helpful, but sometimes they are some preliminary preparations for answering the following questions.How to integrate those "unrelated" questions and answers with the final query to form a memory with more information could be a new direction of developing the MS.</p>
<p>A Appendix</p>
<p>A.1 Rubrics and Prompt for scoring Memory</p>
<p>In order to judge whether a memory can be added into the memory pool, we set three scoring rubrics for three domains respectively.For Single Pool, we set up a set of rubrics from a global perspective.</p>
<p>A The output shows some creativity or insights but to a lesser extent, offering added value to the answer.1-7 points: The output is standard, with minimal to no creativity or insightful additions.0 points: The output is entirely generic, with no attempt at creativity or providing additional insights.</p>
<p>A.1.5 Prompt for scoring Memory</p>
<p>For scoring a memory, the following instructions were provided to LLM:</p>
<p>Here is the rubrics for grading an example of In-context learning-"grading_rubric". According to the rubric, for the Question-"question" and Answer-"answer", give me a score of pair (question, answer) if I want to use it as a prompt in In-context learning later.Your answer should be a number between 0-100.</p>
<p>A.2 Memories</p>
<p>This section give an detailed example about how agent answer a question with the help of shared memories and how the newly generated prompt and memories looks like.Here is the shared memory from the memory pool of the domain Literary Creation:</p>
<p>Elucidate on a mirror that reflects the soul that revealing truths, making one whole?-&gt;Thereonce was a mirror that reflects the soul; With a tale both old and new; revealing truths, making one whole; A legend through and through; In a world where magic grew.| | Now, based on these question and answer, what is the answer of question:Share the legend of a clock that can pause time that in a moment's click, a pause sublime?-&gt;The answer to the question Share the legend of a clock that can pause time that in a moment's click, a pause sublime?would be the tale of a mystical clock rumored to have the power to pause time with just a simple click.In this legend, the clock is said to possess the ability to grant individuals a moment of pause in their lives, allowing them to reflect, reset, and find serenity in the chaos of the world.Just like the mirror that reflects the soul, this clock is believed to reveal truths and make one feel whole again, offering a moment of clarity and peace in a busy and fast-paced world.</p>
<p>Figure.5 show an real example about how the</p>
<p>Agent-Sonnet make use of this shared memory to construct a new prompt with the original query to make a sonnet.</p>
<p>A.3 Datasets</p>
<p>For all those nine agents, they have their own dataset to evaluate.There are 1000 items in total for all datasets.In each datasets, the item contains a question and a answer.For each dataset, 20% will be taken randomly as a small subset of instances which was manually archived within the memory pool before the experiments.And 40% of the dataset, we will only capture the problem part and use it to generate real-time memory in agents.As the rest 40%, they will be used as the test set.</p>
<p>For the agents in the domain Literary Creation, in their datasets, the question part is a description, and the answer part is like a poetry which fullfills the requirements in the description, the question and answer in the dataset are like:</p>
<p>Tell me about a star that twinkles with a secret that shining brightly, mysteries to decrypt? -There once was a star that twinkles with a secret; With a tale both old and new; shining brightly, mysteries to decrypt; A legend through and through; In a world where magic grew.</p>
<p>For the agents in the domain Unconventional Logic Problem-solving in their datasets, the question part is a logic problem, and the answer part is a reasonable solution of that problem, the question and answer in the dataset are like:</p>
<p>Why is it better to have round manhole covers than square ones?-A square manhole cover can be turned and dropped down the diagonal of the manhole.A round manhole cover cannot be dropped down the manhole.So for safety reasons, all manhole covers should be round For the agents in the domain Plan Generation, in their datasets, the question part is a problem about how to achieve something, and the answer part is a executable plan, the question and answer in the dataset are like:</p>
<p>Start learning Python for data analysis?-Learn basics, then focus on data libraries like pandas, and practice on projects.</p>
<p>Specifically, the dataset of the Puzzle comes from Liu et al., 2023c.For other agents' datasets, the standard "answer" was systematically harvested from the internet and subsequently input to the LLM (gpt-3.5-turbo) to get the corresponding "question" part.In particular, the selection of Wuyanlvshi-renowned and historically significant poems within Chinese literature-was meticulously chosen for their fame and widespread recognition.Furthermore, the sonnets incorporated into our study originate from the "quarto" collection authored by Shakespeare in 1609.We use the following similar format of prompt to generate "question" part:</p>
<p>Here is a sonnet of shakespeare -&gt; "From fairest creatures we desire increase, That thereby beauty's rose might never die, But as the riper should by time decrease, His tender heir mught bear his memeory: But thou, contracted to thine own bright eyes, Feed'st thy light'st flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel.Thou that art now the world's fresh ornament, And only herald to the gaudy spring, Within thine own bud buriest thy content And, tender churl, makest waste in niggarding.Pity the world, or else this glutton be, To eat the world's due, by the grave and thee."According to this, can you give me a problem of it(like make a sonnot followed with some description), so the possible answer can be this sonnet .The output as a "question" from the LLM is like:</p>
<p>Craft a sonnet that explores the tension between personal vanity and the broader imperative to contribute to the continuation of beauty and life.Your poem should address how an individual's focus on their own allure can lead to a metaphorical barrenness, despite the inherent capacity for generational legacy.Conclude with a reflection on the choice between selfishness and altruism, using nature as a metaphor for this existential dilemma.</p>
<p>.</p>
<p>Step-1(Getting Memories from</p>
<p>Figure 2 :
2
Figure 2: An example of how the Agent (Sonnet) cooperates with the MS framework.(1) + (2) The retriever take the original query from agent as the input, retrieve the suitable memories from the memory pool and concatenate them to the query to form the prompt.(3) The Agent (Sonnet) takes the prompt and makes an answer, pack them as (Prompt, Answer).(4) Scorer generates a score according to the designed rubric for (Prompt, Answer), while (Prompt, Answer) pairs with high scores will be added into the Memory Pool and also be sent to train the Retriever.All agents share the same Memory Pool; they can write memories into the pool and retrieve memories from the pool so that they can share memories with each other.</p>
<p>Figure 3 :
3
Figure 3: An illustration of prompt and memory</p>
<p>Figure 4 :
4
Figure 4: Evaluating agent performance on open-ended questions using three suitable memories and Domain-pool.The backbone of each agent is gpt-3.5-turbo.</p>
<p>Memory Pool): Query Compose a sonnet that explores the theme of beauty as both a concealer and enhancer of flaws, reflecting on the beloved's ability to render their sins and vices seemingly innocuous through the sheer force of their allure.Begin by comparing the beloved's imperfections to a flaw in a rose, suggesting that even in blemish, there exists a form of beauty.Elaborate on how the beloved's misdeeds are sweetened by their charm, to the extent that even gossip and scandal become a form of flattery.Reflect on the notion that vice has found an ideal home within the beloved, where their external beauty acts as a cover that makes all imperfections appear attractive.Conclude with a caution to the beloved about the potential consequences of overrelying on their beauty and charm to excuse or obscure their faults, hinting at the possibility of losing their integrity or the genuine admiration of others if this privilege is abused."Memory Pool Elucidate on a mirror that reflects the soul that revealing truths, making one whole?-&gt;Thereonce was a mirror that reflects the soul; With a tale both old and new; revealing truths, making one whole; A legend through and through; In a world where magic grew.| Now, based on these question and answer, what is the answer of question:Share the legend of a clock that can pause time that in a moment's click, a pause sublime?-&gt;The answer to the question \"Share the legend of a clock that can pause time that in a moment's click, a pause sublime?\" would be the tale of a mystical clock rumored to have the power to pause time with just a simple click.In this legend, the clock is said to possess the ability to grant individuals a moment of pause in their lives, allowing them to reflect, reset, and find serenity in the chaos of the world.Just like the mirror that reflects the soul, this clock is believed to reveal truths and make one feel whole again, offering a moment of clarity and peace in a busy and fast-paced world.Step-2(Constructing new Prompt): Query + Memories -&gt; Prompt Elucidate on a mirror that reflects the soul that revealing truths, making one whole?-&gt;Thereonce was a mirror that reflects the soul,\nWith a tale both old and new; revealing truths, making one whole; A legend through and through; In a world where magic grew.| | Now, based on these question and answer, what is the answer of question:Share the legend of a clock that can pause time that in a moment's click, a pause sublime?-&gt;The answer to the question \"Share the legend of a clock that can pause time that in a moment's click, a pause sublime?\" would be the tale of a mystical clock rumored to have the power to pause time with just a simple click.In this legend, the clock is said to possess the ability to grant individuals a moment of pause in their lives, allowing them to reflect, reset, and find serenity in the chaos of the world.Just like the mirror that reflects the soul, this clock is believed to reveal truths and make one feel whole again, offering a moment of clarity and peace in a busy and fast-paced world.Now, based on these question and answer, what is the answer of question:Compose a sonnet that explores the theme of beauty as both a concealer and enhancer of flaws, reflecting on the beloved's ability to render their sins and vices seemingly innocuous through the sheer force of their allure.Begin by comparing the beloved's imperfections to a flaw in a rose, suggesting that even in blemish, there exists a form of beauty.Elaborate on how the beloved's misdeeds are sweetened by their charm, to the extent that even gossip and scandal become a form of flattery.Reflect on the notion that vice has found an ideal home within the beloved, where their external beauty acts as a cover that makes all imperfections appear attractive.Conclude with a caution to the beloved about the potential consequences of overrelying on their beauty and charm to excuse or obscure their faults, hinting at the possibility of losing their integrity or the genuine admiration of others if this privilege is abused.like a rose with flaws so rare; Imperfections blend with beauty fair; Their sins and vices sweetened by allure; Gossip and scandal turned to flattering allure; Vice finds a home within their charming grace; Where flaws are masked and faults find no disgrace; But caution to the beloved I impart; Overreliance may tarnish their heart; For beauty's cloak can only hide so much; Integrity and admiration as such; May wane if charm becomes their only shield; So let not beauty's power be revealed.Embrace true self; flaws and all;with pride; For in authenticity, true beauty resides.</p>
<p>Figure 5 :
5
Figure 5: Agent-Sonnet create a sonnet with using shared memories</p>
<p>Riddles, and Puns.Meanwhile, for Plan Generation, we have developed agents to create Study Plans, Travel Plans, and Fitness Plans.We use a total of nines datasets representing nine agents to evaluate.There are 1000 question-answer pairs in total, details in Appendix A.3.For each agent, a consistent, small subset of pre-provided, complete PA pairs were selected and incorporated into the memory pool for the initial phase of retriever training and prompt refinement.Subsequently, for each agent, an identical number of queries will be introduced to increment the volume of real-time memory within the memory pool.For our scoring LLM, we use gpt-3.5-turbo.As the backbones of our agents, we consider three LLMs: two closesource LLMs (gpt-3.5-turboandgpt-4o)and one open-source LLM (open-mistral-7b).We use the BERTScore(Zhang et al., 2019)as our metric to measure the performance of each agent.
4.1 Experiment DetailsWe aim to assess the efficacy of the MS frameworkin processing open-ended questions across threedomains: Literary Creation, Unconventional LogicProblem-solving, and Plan Generation. Withinthe Literary Creation domain, we have appointedthree specialized agents responsible for generating
Wuyanlvshi (a form of classical Chinese poetry, Chinese form), Limericks, and Sonnets, respectively.In the Logic Problem-solving domain, dedicated agents are tasked with addressing Puzzles,</p>
<p>Table 1 :
1
Performance across agents utilizing different amounts memory for open-ended questions execution.Each domain has its own Domain-pool shared within its three agents.
ModelLimerick Wuyanlvshi Sonnet Puzzle Pun Riddle Fitness Study Travelgpt-3.5-turbo0.870.720.530.600.700.880.650.630.71Domain-pool gpt-4o0.590.760.540.520.670.520.520.510.53open-mistral-7b0.930.660.530.500.390.370.540.460.53gpt-3.5-turbo0.600.680.490.540.700.800.620.630.58Single-poolgpt-4o0.540.760.540.500.610.510.520.560.52open-mistral-7b0.640.630.520.480.380.350.540.490.50</p>
<p>Table 2 :
2
Agent performance with Domain-pool vs. Single-pool by utilizing three suitable memories for open-ended questions.</p>
<p>models.In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1752-1767, St. Julian's, Malta.Association for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:24824-24837.Jules White, Quchen Fu, Sam Hays, Michael Sandborn,Carlos Olea, Henry Gilbert, Ashraf Elnashar, JesseSpencer-Smith, and Douglas C Schmidt. 2023. Aprompt pattern catalog to enhance prompt engineer-ing with chatgpt. arXiv preprint arXiv:2302.11382.Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,Mark Riedl, and Yejin Choi. 2022. Reframinghuman-ai collaboration for generating free-text ex-planations. In Proceedings of the 2022 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 632-658.Tongshuang Wu, Michael Terry, and Carrie Jun Cai.2022. Ai chains: Transparent and controllablehuman-ai interaction by chaining large languagemodel prompts. In Proceedings of the 2022 CHIconference on human factors in computing systems,pages 1-22.Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-berger, and Yoav Artzi. 2019. Bertscore: Evaluatingtext generation with bert. In International Confer-ence on Learning Representations.Longtao Zheng, Rundong Wang, Xinrun Wang, andBo An. 2023. Synapse: Trajectory-as-exemplarprompting with memory for computer control. InThe Twelfth International Conference on LearningRepresentations.Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,Keiran Paster, Silviu Pitis, Harris Chan, and JimmyBa. 2022. Large language models are human-levelprompt engineers. In The Eleventh InternationalConference on Learning Representations.Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-jie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu,Xiaogang Wang, et al. 2023. Ghost in the minecraft:Generally capable agents for open-world enviromentsvia large language models with text-based knowledgeand memory. arXiv preprint arXiv:2305.17144.
ZeroOneTwo Three Agent gpt-3.5-turbogpt-4o open-mistral-7b gpt-3.5-turbogpt-4o open-mistral-7b gpt-3.
Few-shot training llms for project-specific codesummarization. Toufique Ahmed, Premkumar Devanbu, Proceedings of the 37th. the 37th2022</p>
<p>IEEE/ACM International Conference on Automated Software Engineering. </p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, Journal of Machine Learning Research. 242512023</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, 10.18653/v1/2022.findings-emnlp.38Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua, arXiv:2110.04541The inductive bias of in-context learning: Rethinking pretraining example design. 2021arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>What makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, Weizhu Chen, The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2022Dee-LIO 2022Proceedings of Deep Learning Inside Out</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023aarXiv preprint</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023b</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, The Twelfth International Conference on Learning Representations. 2023c</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, Vincent Y Zhao, arXiv:2305.14128Dr. icl: Demonstration-retrieved in-context learning. 2023arXiv preprint</p>
<p>Generation-augmented retrieval for opendomain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, 10.18653/v1/2021.acl-long.316Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, 10.1162/tacl_a_00605Transactions of the Association for Computational Linguistics. 112023</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Story centaur: Large language model few shot learning as a creative writing tool. Ben Swanson, Kory Mathewson, Ben Pietrzak, Sherol Chen, Monica Dinalescu, 10.18653/v1/2021.eacl-demos.29Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. the 16th Conference of the European Chapter of the Association for Computational Linguistics: System DemonstrationsOnline. Association for Computational Linguistics2021</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Learning to retrieve in-context examples for large language. Liang Wang, Nan Yang, Furu Wei, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
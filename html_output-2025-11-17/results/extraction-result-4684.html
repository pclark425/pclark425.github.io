<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4684 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4684</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4684</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-265157947</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07687v1.pdf" target="_blank">Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks. CALM, a popular approach, leverages linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games. In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire. We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs. We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4684.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4684.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-in-the-Loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model-In-The-Loop (GPT-2 finetuned with in-game transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent architecture where a pretrained GPT-2 proposes natural-language candidate actions and a DRRN (Deep Reinforcement Relevance Network) selects among them; the GPT-2 is further finetuned during gameplay using stored in-game transitions (D+ / D-) to improve candidate suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop (GPT-2 + DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Pretrained GPT-2 (base) is first adapted on ClubFloyd human gameplay and then used to generate candidate actions; a DRRN encodes observation and candidate actions and picks the action; GPT-2 is periodically finetuned during RL using sampled in-game transitions to improve candidate generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (Base, 117M)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (10 selected interactive fiction games, e.g., Zork1, Zork3, Ludicorp, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Play text-based interactive fiction games by generating/selecting textual actions to maximize in-game cumulative reward; specifically: use GPT-2 to propose actions and DRRN to choose among them, with the LLM optionally updated from in-game transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Experience replay buffers for LM finetuning (episodic transition memory) and prioritized experience replay for DRRN; buffers are categorized (D+, D-) or uncategorized.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw transitions: context / observation-action pairs and transitions stored as (o_t, a_t, o_{t+1}, r_{t+1}) and context-action pairs ((o_{j-1}, a_{j-1}, o_j), a_j).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Transitions are appended to FIFO replay buffers during play; buffers are partitioned into positive/negative (D+, D-) according to heuristics; every k game steps a fixed-size minibatch (d_LM) is sampled and GPT-2 is finetuned for 2000 gradient steps (weighted cross-entropy).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Uniform sampling from sampled buffers or probabilistic sampling using p+ / (1 - p+) between D+ and D-; optionally reweighting examples using advantage-based weights (h(·) = exp(β A) or 1 + β A). DRRN accesses its prioritized replay for RL learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Best LM-in-the-Loop strategy (state-feature categorized, OC) achieved avg.norm ≈ 24.0% (paper reports ≈ 24.0% avg.norm vs baseline CALM 20.1%), an absolute ≈4% improvement and ≈53% relative improvement over baseline's avg.improvement; OC also accelerated convergence (~2×–3× faster to reach CALM's best). Per-game scores in Table 1 show OC often highest across many games (e.g., Zork1: 38.0 vs CALM 30.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline CALM (GPT-2 adapted on ClubFloyd but frozen during gameplay) avg.norm = 20.1% (see CALM entry); LM-in-the-Loop without categorization (UT) led to avg.norm 19.1% (worse than baseline), and simple reweighting variants produced small changes (~20.6%–20.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Comprehensive ablations comparing: Uncategorized Transitions (UT), State Feature Categorized (OC), Reward Trajectories (RT), and reweighted losses (UT_EA, UT_LA). Findings: OC (state-feature categorization: positive transitions when reward increased or location changed) provided the largest gains in both final score and speed of convergence; UT performed worse than baseline; reweighting by advantage (UT_EA, UT_LA) yielded only marginal improvements. Separate ablation removing DRRN (letting GPT-2 pick argmax) showed GPT-2 alone performed poorly, indicating DRRN handles planning.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires careful selection/categorization of in-game transitions — naive uncategorized replay can hurt performance; OC uses game-specific signals (e.g., location change) that may not generalize; transfer of an LM-in-the-Loop model across different games is inconsistent and often degrades performance; LM alone (as policy) fails in many games; automatic semantic metrics (BLEU, MAUVE) do not reliably predict transfer or performance improvements; scalability and data-efficiency remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend using targeted transition selection for LM finetuning (state-feature-based OC when available) to maximize benefit and convergence speed; use in-game transitions to reduce dependence on large human-annotated adaptation corpora (LM-in-the-Loop with 10% ClubFloyd + OC outperformed CALM with 100% ClubFloyd); keep a separate planning module (DRRN) rather than relying on LM argmax; align LM generation likelihoods with action-value signals and develop interpretable automatic metrics to identify important transitions for LM updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4684.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4684.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM (GPT-2 adaptation for action generation / baseline from prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approach that adapts GPT-2 on a corpus of human gameplay (ClubFloyd) to generate candidate actions for a downstream DRRN agent but keeps the LM frozen during RL gameplay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in textbased games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM (GPT-2 adapted on ClubFloyd + DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 is adapted offline on human-annotated ClubFloyd transcripts to bias candidate generation; during gameplay the adapted GPT-2 is frozen and provides action candidates to a DRRN which learns via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (adapted on ClubFloyd)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho benchmark (same set of games as this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Baseline for action recommendation in text games: generate candidate actions (using GPT-2 adapted on human gameplay) for DRRN to choose; evaluated on same metrics as LM-in-the-Loop.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>LM is not updated during gameplay (no in-game LM memory); DRRN still uses standard prioritized experience replay for RL but GPT-2 remains frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Offline adaptation dataset (ClubFloyd human gameplay transcripts) was used to adapt GPT-2 prior to gameplay; no in-game transition storage is used to update the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>No in-game updates to LM; adaptation done once offline on ClubFloyd dataset (~217K pairs); RL replay for DRRN operates normally but does not change LM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>At inference, GPT-2 generates candidate actions conditioned on current context; no retrieval-from-memory mechanism for LM during gameplay beyond its learned weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline reported avg.norm = 20.1% across selected games (Table 1). Per-game numbers in Table 1 (e.g., Zork1 30.7, Inhumane 24.8, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reproduces CALM as baseline and compares it to LM-in-the-Loop variants. CALM's performance degrades significantly when less ClubFloyd adaptation data is used (10% adaptation -> avg.norm 18.5); LM-in-the-Loop with OC + 10% adaptation outperformed CALM with 100% adaptation, demonstrating utility of in-game LM updates.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relies heavily on large annotated human gameplay corpus (ClubFloyd); frozen LM cannot adapt to game-specific transitions observed during RL, which may limit convergence speed and final performance if adaptation corpus mismatches target game.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Paper suggests complementing or replacing large offline adaptation with in-game LM updates to reduce dependence on annotated corpora; keep LM adaptation aligned to game-specific transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4684.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4684.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-as-Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 used directly as the policy network (argmax action selection ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation in which GPT-2 (either frozen or in-game trained) directly selects the argmax action instead of only proposing candidates to a DRRN; tests whether the LM alone can act as the policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-2 policy (LM argmax)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 (either frozen after ClubFloyd adaptation or further trained in-game) is used to pick the top-scoring/generated action directly (argmax) without a separate DRRN decision-making module.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (Base), both frozen and in-game finetuned variants evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (same selected games used in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Direct policy evaluation: have GPT-2 choose actions to play the game (no DRRN).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>No external memory mechanism beyond the LM's weights / immediate context; even when GPT-2 was finetuned with in-game transitions, there was no external episodic retrieval or replay during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>LM internal weights (post-adaptation or post in-game finetuning) and the immediate textual context; no separate stored transition memory used at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>When tested, variants included (a) frozen GPT-2 adapted on ClubFloyd (no in-game updates) and (b) GPT-2 finetuned in-game (updates to LM weights occurred during training phase), but during deployment the LM used only its learned weights and context window.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Standard causal generation from GPT-2 conditioned on the context; no external retrieval or attention over an episodic buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Poor: GPT-2 as sole policy yielded near-zero scores on most games (Table 3 shows zeros for many games; overall Norm ≈ 1.1%), irrespective of whether the LM was frozen or trained in-game.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>This ablation demonstrates that even an in-game finetuned LM does not by itself perform reasonably as a policy; DRRN contributes the majority of planning and decision-making capability. Thus, LM improvements alone (better candidate generation) are not sufficient to replace an explicit policy/critic.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>LMs (GPT-2 with or without in-game finetuning) do not reliably capture game dynamics sufficiently to act as a standalone policy; they fail on large parts of the benchmark despite adaptation and in-game updates.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Do not rely on the LM alone as the decision-making policy; combine LM candidate generation with an explicit value-based policy network (e.g., DRRN) to handle planning and value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4684.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4684.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transition Memory / Replay Buffers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-game Transition Replay Buffers and Selection Heuristics (D+, D-, UT, OC, RT, UT_EA, UT_LA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanisms to store and select in-game transitions for finetuning the LLM: uncategorized buffer (UT), state-feature oracle categorization (OC), reward-trajectory categorization (RT), and advantage-based reweighting (UT_EA, UT_LA); used to decide which transitions are most useful for in-game LM updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transition replay + selection heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>During RL, transitions (o_t, a_t, o_{t+1}, r_{t+1}) are saved in replay buffers; buffers may be split into positive (D+) and negative (D-) sets according to heuristics; batches are sampled (with probability p+) and used to finetune GPT-2 periodically.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (Base) when these buffers are used to finetune the LM</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (same games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select and store in-game experiences that are useful for finetuning the LM to produce better candidate actions and improve RL learning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Episodic experience replay for LM finetuning; explicit categorization strategies (OC, RT) and reweighting schemes (UT_EA, UT_LA).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored raw transitions: context/action/next-observation/reward tuples and concatenated context-action pairs used for causal language modeling updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Transitions appended continuously during episodes into FIFO replay buffers (max size 100K); periodic LM updates sample d_LM examples after every k game steps; positive/negative assignment depends on heuristics (OC uses state-change or reward increase; RT uses reward occurrences).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Sampling: uniform from chosen buffer with a probability p+ for D+ vs D-; alternative weighting uses advantage-based scalar h(o,a) either exponential of advantage (UT_EA) or linear-shifted (UT_LA) to scale loss contributions during LM finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Selection matters: OC (state-feature categorization) led to largest gains (avg.norm ≈ 24.0%), UT (uncategorized) performed worse than baseline (avg.norm 19.1%), RT (reward-based) and reweighting (UT_EA, UT_LA) produced only modest improvements (~20.6%–20.9%). OC also delivered ~2–3× faster convergence to baseline's best.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Direct comparison of UT, OC, RT, UT_EA, UT_LA shows OC is strongest; advantage-based reweighting does not substitute for categorical selection based on state features. Authors also varied hyperparameters p+ and k to study sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>OC needs access to game-specific state features (e.g., location change) which may not be available in all environments; naive uncategorized replay can harm LM performance; reweighting by advantage requires stable advantage estimates and did not yield large gains in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Prefer informative selection heuristics (state-feature based when available) to populate positive buffers for LM finetuning; sample with controlled p+ to favor exploratory/useful transitions; if no oracle state-features are available, reweighting by advantage is a possible but weaker alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Keep CALM and explore: Language models for action generation in textbased games. <em>(Rating: 2)</em></li>
                <li>Reading and acting while blindfolded: The need for semantics in text game agents. <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decisionmaking. <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with a natural language action space. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4684",
    "paper_id": "paper-265157947",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "LM-in-the-Loop",
            "name_full": "Language Model-In-The-Loop (GPT-2 finetuned with in-game transitions)",
            "brief_description": "An agent architecture where a pretrained GPT-2 proposes natural-language candidate actions and a DRRN (Deep Reinforcement Relevance Network) selects among them; the GPT-2 is further finetuned during gameplay using stored in-game transitions (D+ / D-) to improve candidate suggestions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop (GPT-2 + DRRN)",
            "agent_description": "Pretrained GPT-2 (base) is first adapted on ClubFloyd human gameplay and then used to generate candidate actions; a DRRN encodes observation and candidate actions and picks the action; GPT-2 is periodically finetuned during RL using sampled in-game transitions to improve candidate generation.",
            "llm_model_name": "GPT-2 (Base, 117M)",
            "game_or_benchmark_name": "Jericho (10 selected interactive fiction games, e.g., Zork1, Zork3, Ludicorp, etc.)",
            "task_description": "Play text-based interactive fiction games by generating/selecting textual actions to maximize in-game cumulative reward; specifically: use GPT-2 to propose actions and DRRN to choose among them, with the LLM optionally updated from in-game transitions.",
            "memory_used": true,
            "memory_type": "Experience replay buffers for LM finetuning (episodic transition memory) and prioritized experience replay for DRRN; buffers are categorized (D+, D-) or uncategorized.",
            "memory_representation": "Raw transitions: context / observation-action pairs and transitions stored as (o_t, a_t, o_{t+1}, r_{t+1}) and context-action pairs ((o_{j-1}, a_{j-1}, o_j), a_j).",
            "memory_update_mechanism": "Transitions are appended to FIFO replay buffers during play; buffers are partitioned into positive/negative (D+, D-) according to heuristics; every k game steps a fixed-size minibatch (d_LM) is sampled and GPT-2 is finetuned for 2000 gradient steps (weighted cross-entropy).",
            "memory_retrieval_mechanism": "Uniform sampling from sampled buffers or probabilistic sampling using p+ / (1 - p+) between D+ and D-; optionally reweighting examples using advantage-based weights (h(·) = exp(β A) or 1 + β A). DRRN accesses its prioritized replay for RL learning.",
            "performance_with_memory": "Best LM-in-the-Loop strategy (state-feature categorized, OC) achieved avg.norm ≈ 24.0% (paper reports ≈ 24.0% avg.norm vs baseline CALM 20.1%), an absolute ≈4% improvement and ≈53% relative improvement over baseline's avg.improvement; OC also accelerated convergence (~2×–3× faster to reach CALM's best). Per-game scores in Table 1 show OC often highest across many games (e.g., Zork1: 38.0 vs CALM 30.7).",
            "performance_without_memory": "Baseline CALM (GPT-2 adapted on ClubFloyd but frozen during gameplay) avg.norm = 20.1% (see CALM entry); LM-in-the-Loop without categorization (UT) led to avg.norm 19.1% (worse than baseline), and simple reweighting variants produced small changes (~20.6%–20.9%).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "Comprehensive ablations comparing: Uncategorized Transitions (UT), State Feature Categorized (OC), Reward Trajectories (RT), and reweighted losses (UT_EA, UT_LA). Findings: OC (state-feature categorization: positive transitions when reward increased or location changed) provided the largest gains in both final score and speed of convergence; UT performed worse than baseline; reweighting by advantage (UT_EA, UT_LA) yielded only marginal improvements. Separate ablation removing DRRN (letting GPT-2 pick argmax) showed GPT-2 alone performed poorly, indicating DRRN handles planning.",
            "challenges_or_limitations": "Requires careful selection/categorization of in-game transitions — naive uncategorized replay can hurt performance; OC uses game-specific signals (e.g., location change) that may not generalize; transfer of an LM-in-the-Loop model across different games is inconsistent and often degrades performance; LM alone (as policy) fails in many games; automatic semantic metrics (BLEU, MAUVE) do not reliably predict transfer or performance improvements; scalability and data-efficiency remain concerns.",
            "best_practices_or_recommendations": "Authors recommend using targeted transition selection for LM finetuning (state-feature-based OC when available) to maximize benefit and convergence speed; use in-game transitions to reduce dependence on large human-annotated adaptation corpora (LM-in-the-Loop with 10% ClubFloyd + OC outperformed CALM with 100% ClubFloyd); keep a separate planning module (DRRN) rather than relying on LM argmax; align LM generation likelihoods with action-value signals and develop interpretable automatic metrics to identify important transitions for LM updates.",
            "uuid": "e4684.0",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CALM",
            "name_full": "CALM (GPT-2 adaptation for action generation / baseline from prior work)",
            "brief_description": "Prior approach that adapts GPT-2 on a corpus of human gameplay (ClubFloyd) to generate candidate actions for a downstream DRRN agent but keeps the LM frozen during RL gameplay.",
            "citation_title": "Keep CALM and explore: Language models for action generation in textbased games.",
            "mention_or_use": "use",
            "agent_name": "CALM (GPT-2 adapted on ClubFloyd + DRRN)",
            "agent_description": "GPT-2 is adapted offline on human-annotated ClubFloyd transcripts to bias candidate generation; during gameplay the adapted GPT-2 is frozen and provides action candidates to a DRRN which learns via RL.",
            "llm_model_name": "GPT-2 (adapted on ClubFloyd)",
            "game_or_benchmark_name": "Jericho benchmark (same set of games as this paper)",
            "task_description": "Baseline for action recommendation in text games: generate candidate actions (using GPT-2 adapted on human gameplay) for DRRN to choose; evaluated on same metrics as LM-in-the-Loop.",
            "memory_used": false,
            "memory_type": "LM is not updated during gameplay (no in-game LM memory); DRRN still uses standard prioritized experience replay for RL but GPT-2 remains frozen.",
            "memory_representation": "Offline adaptation dataset (ClubFloyd human gameplay transcripts) was used to adapt GPT-2 prior to gameplay; no in-game transition storage is used to update the LM.",
            "memory_update_mechanism": "No in-game updates to LM; adaptation done once offline on ClubFloyd dataset (~217K pairs); RL replay for DRRN operates normally but does not change LM.",
            "memory_retrieval_mechanism": "At inference, GPT-2 generates candidate actions conditioned on current context; no retrieval-from-memory mechanism for LM during gameplay beyond its learned weights.",
            "performance_with_memory": "Baseline reported avg.norm = 20.1% across selected games (Table 1). Per-game numbers in Table 1 (e.g., Zork1 30.7, Inhumane 24.8, etc.).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Paper reproduces CALM as baseline and compares it to LM-in-the-Loop variants. CALM's performance degrades significantly when less ClubFloyd adaptation data is used (10% adaptation -&gt; avg.norm 18.5); LM-in-the-Loop with OC + 10% adaptation outperformed CALM with 100% adaptation, demonstrating utility of in-game LM updates.",
            "challenges_or_limitations": "Relies heavily on large annotated human gameplay corpus (ClubFloyd); frozen LM cannot adapt to game-specific transitions observed during RL, which may limit convergence speed and final performance if adaptation corpus mismatches target game.",
            "best_practices_or_recommendations": "Paper suggests complementing or replacing large offline adaptation with in-game LM updates to reduce dependence on annotated corpora; keep LM adaptation aligned to game-specific transitions.",
            "uuid": "e4684.1",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-2-as-Policy",
            "name_full": "GPT-2 used directly as the policy network (argmax action selection ablation)",
            "brief_description": "Ablation in which GPT-2 (either frozen or in-game trained) directly selects the argmax action instead of only proposing candidates to a DRRN; tests whether the LM alone can act as the policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-2 policy (LM argmax)",
            "agent_description": "GPT-2 (either frozen after ClubFloyd adaptation or further trained in-game) is used to pick the top-scoring/generated action directly (argmax) without a separate DRRN decision-making module.",
            "llm_model_name": "GPT-2 (Base), both frozen and in-game finetuned variants evaluated",
            "game_or_benchmark_name": "Jericho (same selected games used in the paper)",
            "task_description": "Direct policy evaluation: have GPT-2 choose actions to play the game (no DRRN).",
            "memory_used": false,
            "memory_type": "No external memory mechanism beyond the LM's weights / immediate context; even when GPT-2 was finetuned with in-game transitions, there was no external episodic retrieval or replay during inference.",
            "memory_representation": "LM internal weights (post-adaptation or post in-game finetuning) and the immediate textual context; no separate stored transition memory used at inference.",
            "memory_update_mechanism": "When tested, variants included (a) frozen GPT-2 adapted on ClubFloyd (no in-game updates) and (b) GPT-2 finetuned in-game (updates to LM weights occurred during training phase), but during deployment the LM used only its learned weights and context window.",
            "memory_retrieval_mechanism": "Standard causal generation from GPT-2 conditioned on the context; no external retrieval or attention over an episodic buffer.",
            "performance_with_memory": "Poor: GPT-2 as sole policy yielded near-zero scores on most games (Table 3 shows zeros for many games; overall Norm ≈ 1.1%), irrespective of whether the LM was frozen or trained in-game.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "This ablation demonstrates that even an in-game finetuned LM does not by itself perform reasonably as a policy; DRRN contributes the majority of planning and decision-making capability. Thus, LM improvements alone (better candidate generation) are not sufficient to replace an explicit policy/critic.",
            "challenges_or_limitations": "LMs (GPT-2 with or without in-game finetuning) do not reliably capture game dynamics sufficiently to act as a standalone policy; they fail on large parts of the benchmark despite adaptation and in-game updates.",
            "best_practices_or_recommendations": "Do not rely on the LM alone as the decision-making policy; combine LM candidate generation with an explicit value-based policy network (e.g., DRRN) to handle planning and value estimation.",
            "uuid": "e4684.2",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Transition Memory / Replay Buffers",
            "name_full": "In-game Transition Replay Buffers and Selection Heuristics (D+, D-, UT, OC, RT, UT_EA, UT_LA)",
            "brief_description": "Mechanisms to store and select in-game transitions for finetuning the LLM: uncategorized buffer (UT), state-feature oracle categorization (OC), reward-trajectory categorization (RT), and advantage-based reweighting (UT_EA, UT_LA); used to decide which transitions are most useful for in-game LM updates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Transition replay + selection heuristics",
            "agent_description": "During RL, transitions (o_t, a_t, o_{t+1}, r_{t+1}) are saved in replay buffers; buffers may be split into positive (D+) and negative (D-) sets according to heuristics; batches are sampled (with probability p+) and used to finetune GPT-2 periodically.",
            "llm_model_name": "GPT-2 (Base) when these buffers are used to finetune the LM",
            "game_or_benchmark_name": "Jericho (same games)",
            "task_description": "Select and store in-game experiences that are useful for finetuning the LM to produce better candidate actions and improve RL learning.",
            "memory_used": true,
            "memory_type": "Episodic experience replay for LM finetuning; explicit categorization strategies (OC, RT) and reweighting schemes (UT_EA, UT_LA).",
            "memory_representation": "Stored raw transitions: context/action/next-observation/reward tuples and concatenated context-action pairs used for causal language modeling updates.",
            "memory_update_mechanism": "Transitions appended continuously during episodes into FIFO replay buffers (max size 100K); periodic LM updates sample d_LM examples after every k game steps; positive/negative assignment depends on heuristics (OC uses state-change or reward increase; RT uses reward occurrences).",
            "memory_retrieval_mechanism": "Sampling: uniform from chosen buffer with a probability p+ for D+ vs D-; alternative weighting uses advantage-based scalar h(o,a) either exponential of advantage (UT_EA) or linear-shifted (UT_LA) to scale loss contributions during LM finetuning.",
            "performance_with_memory": "Selection matters: OC (state-feature categorization) led to largest gains (avg.norm ≈ 24.0%), UT (uncategorized) performed worse than baseline (avg.norm 19.1%), RT (reward-based) and reweighting (UT_EA, UT_LA) produced only modest improvements (~20.6%–20.9%). OC also delivered ~2–3× faster convergence to baseline's best.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "ablation_or_analysis": "Direct comparison of UT, OC, RT, UT_EA, UT_LA shows OC is strongest; advantage-based reweighting does not substitute for categorical selection based on state features. Authors also varied hyperparameters p+ and k to study sensitivity.",
            "challenges_or_limitations": "OC needs access to game-specific state features (e.g., location change) which may not be available in all environments; naive uncategorized replay can harm LM performance; reweighting by advantage requires stable advantage estimates and did not yield large gains in experiments.",
            "best_practices_or_recommendations": "Prefer informative selection heuristics (state-feature based when available) to populate positive buffers for LM finetuning; sample with controlled p+ to favor exploratory/useful transitions; if no oracle state-features are available, reweighting by advantage is a possible but weaker alternative.",
            "uuid": "e4684.3",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in textbased games.",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Reading and acting while blindfolded: The need for semantics in text game agents.",
            "rating": 2,
            "sanitized_title": "reading_and_acting_while_blindfolded_the_need_for_semantics_in_text_game_agents"
        },
        {
            "paper_title": "Pre-trained language models for interactive decisionmaking.",
            "rating": 2,
            "sanitized_title": "pretrained_language_models_for_interactive_decisionmaking"
        },
        {
            "paper_title": "Deep reinforcement learning with a natural language action space.",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_with_a_natural_language_action_space"
        }
    ],
    "cost": 0.01525525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
13 Nov 2023</p>
<p>Arjun Vaithilingam Sudhakar 
Mila -Quebec AI Institute</p>
<p>Ecole Polytechnique de Montreal</p>
<p>Prasanna Parthasarathi 
Janarthanan Rajendran 
Mila -Quebec AI Institute</p>
<p>University of Montreal</p>
<p>Sarath Chandar 
Mila -Quebec AI Institute</p>
<p>Ecole Polytechnique de Montreal</p>
<p>CIFAR AI Chair
Canada</p>
<p>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
13 Nov 2023E9CFC669378BBD22F5BE438C7D800830arXiv:2311.07687v1[cs.CL]
Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks.CALM, a popular approach, leverages linguistic priors of LLMs-GPT-2-for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions.However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games.In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire.We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs.We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.</p>
<p>Introduction</p>
<p>Large Language models (Devlin et al., 2019a;Radford et al., 2018b;Ouyang et al., 2022) (LLMs) trained on large corpora of unstructured text corpora are the state-of-the-art models in several Natural Language Understanding (NLU) benchmarks.Bender and Koller (2020) argue in their position paper that the models trained largely from static benchmarks rely to the form rather than understanding the meaning.While it is imperative to understand the learning dynamics of LLMs (Rogers et al., 2020;Webson and Pavlick, 2021), introducing novel language understanding challenges pushes the frontiers for LLMs' applications.There has been a recent interest in interactive training of large language models in situated learning environments.Bisk et al. (2020); McClelland et al. (2020) Figure 1: Sample gameplay from zork1 game in Jericho using LM for action recommendation: LM recommends action candidates based on the observation from env.The RL agent selects an action from the candidates.point out the necessity for LMs to have enhanced language understanding and meaning through interacting with the physical world.Also, Lake and Murphy (2021) argues that LMs fall short in their communicative usage, requiring reasoning over intents despite their success in static datasets.</p>
<p>Training decision making agents over textual information for playing text-based games (Hausknecht et al., 2020;Côté et al., 2018) has been a recent usecase for LLM.While decision making has been the front of text-game playing, such games introduce novel challenges for language understanding, and domain adaptation for LLMs.Yao et al. (2020) used GPT-2 (Radford et al., 2018b) to generate candidate actions for the decision making DRRN module (He et al., 2016) in Jericho benchmark of text based games.Such a set up allows for qualitatively understanding the LLMs' abilities to understand, reason, and adapt to novel situations.In a typical text-based game, as in Figure 1, an agent receives a textual observation about its environment that it has to understand and reason over the possible actions to pick one and proceed.</p>
<p>While learning from scratch is time-consuming, Yao et al. (2020) make use of linguistic priors in LLMs to prune the combinatorially large action space.The authors adapt GPT-2 for the task with a corpus of human game play on similar games-ClubFloyd.After the adaptation phase, the model remains frozen throughout the learning that happens within the game.</p>
<p>Further, Yao et al. (2020) also note that the performance on the text-based games in Jericho benchmark was sensitive to the size of the annotated human gameplay corpus; such reliance adds to the cost.On the one hand in-game transitions remain unutilized for training the LLM, and on the other there is a need to mitigate the reliance on human annotated transitions to scale applications of LLMs.Although one can make use of the transitions to train the model, the solution requires a comprehensive analysis on what such a LM-in-the-Loop training entails.Toward that, we explore LM-inthe-Loop by building over the setup in Yao et al. (2020) by training GPT-2 using in-game generated transitions.Further, we analyze such a set up along the metrics of: (1) Improvement in performance, (2) Acceleration in convergence, (3) Reliance on human annotated transitions, (4) Replacing GPT-2 as a policy network, (5) comparing reward, state based transitions selection for LM training, and (6) Generalization of LM-in-the-Loop trained LM to other games.The main findings of the approach are summarized as follows:</p>
<p>• LM-in-the-Loop reduces emphasis on human annotated transitions and enables accelerated convergence.</p>
<p>• State feature based transitions selection provided greater gains than other alternates.</p>
<p>• LM-in-the-Loop does not always transfer to other games.</p>
<p>• Although LM-in-the-Loop improved candidate suggestion, GPT-2 as policy network did faired poorly across games.</p>
<p>Related Work</p>
<p>Text Games: Jericho (Hausknecht et al., 2020) is a popular learning environment that supports 32 human-written interactive fiction games.These games are designed to be difficult for human players, serving as a more realistic training ground to evaluate language understanding agents.Compared with frameworks like TextWorld (Côté et al., 2018), these games have significantly more linguistic variety and larger action space.Jericho environment provides a smaller list of candidate actions that can be used to train reinforcement learning (RL) agents.Approaches like DRRN (He et al., 2016), TDQN (Hausknecht et al., 2020), and KGA2C (Ammanabrolu and Hausknecht, 2020) have used handicap to operate on small action space and learn only through in-game rewards.Towards using large LMs, environment provided actions are replaced with LM generated actions like with GPT-2 (Yao et al., 2020), or BERT (Singh et al., 2021).Li et al. (2022) train LMs to remember optimal trajectories to swiftly move to novel game regions.</p>
<p>Data Efficiency: LLMs (Devlin et al., 2019b;Brown et al., 2020) are pretrained with tremendous amount of unstructured text data from the web using a generic language modeling objective.Adapting the models to a downstream tasks (Khashabi et al., 2020;Rajpurkar et al., 2016;Zhang et al., 2015;Maas et al., 2011), however, has been shown to greatly affected by the quality of supervision and the size of the dataset.As reliance on annotated data makes their application hard to scale, techniques like data augmentation (Feng et al., 2021), using distilled models (Radford et al., 2018a), learning from toyish data (Wu et al., 2022) has been explored has alternatives.However, the approach of making LLMs interactive to be trained in a situated learning environment to reduce the need for annotations is only recently getting popular.</p>
<p>3 Background</p>
<p>Text Games</p>
<p>In text-based games, at each step t, a learning agent interacts with the game environment by generating a textual action a t ∈ A t that is relevant to the textual observation o t .The agent receives a scalar reward r t = R t (o t , a t ).The agent maximizes the expected cumulative rewards (r 0 , r 1 , r 2 , . . .r N ), until the end of an N -step long episode.</p>
<p>DRRN and Advantage Function</p>
<p>A popular deep RL method used in text-based games is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016).The observation (o) and actions (a) are first encoded using separate recurrent neural network encoders (such as a GRU (Chung et al., 2014)) f o and f a respectively.A decoder g then combines the representations to obtain the Q-value using a network parameterized by Φ:
Q Φ (o, a) = g(f o (o), f a (a)).(1)
The DRRN learns to estimate the Q-value through iteratively updating Φ with experience sampled from a prioritized experience replay buffer with the temporal difference (TD) loss:
L T D (Φ) = r + γ max a ′ ∈A Q Φ (o ′ , a ′ ) − Q Φ (o, a) 2 ,
(2) where r and o ′ are the reward and the observation received after taking action a upon observing o, and γ represents the discount factor.</p>
<p>Advantage function: An estimate how good an action, a, is when chosen in a state, o, is obtained by subtracting the value of the state (V (o))a weighted average of the Q-values-from the Q(o, a) of that particular action in that state.
A(o, a) = Q Φ (o, a) − V ψ (o)(3)
Q-Value estimates the expected reward after a specific action was played, whereas V ψ (o) is the parameterized estimate of the expected reward from being in o before an action was selected.</p>
<p>LLM for Action Recommendation</p>
<p>Consider a dataset D of N transitions of human gameplay across different games organized in context-action pairs as ((o j−1 , a j−1 , o j ), a j ).For example: a sample could be like, " [CLS]. . . to the north is a restaurant where the mayor ate often.to the east is the mayor's home.2020) uses ClubFloyd to adapt a pretrained GPT-2 model with causal language modeling task.The motivation is to enable the linguistic prior of GPT-2 to adapt to the games and provide better action recommendations to the DRRN.</p>
<p>Methodology</p>
<p>LM-in-the-Loop to recommend Actions</p>
<p>The game playing agent follows trajectories that are rewarded according to the rules of the game in the Jericho environment.The environment has two scenarios-with and without handicap-which correspond to whether the actions can be generated from within the possible actions suggested by the environment or without any limitations by the environment respectively.The with handicap set up evaluates the agent exclusively on planning with the actions provided, while the without handicap requires the agent in addition to understanding the observation also generate acceptable candidates.In Yao et al. (2020), the LLM is kept constant throughout the gameplay and that assumption could be only validated if Jericho games share significant similarity with the transitions in ClubFloyd.However, MAUVE-Score1 between the human transitions and the game transitions in Jericho did not overlap significantly (Table 6 in §A.4), suggesting that the adapting from in-game transitions is needed.</p>
<p>Toward that, we explore the feasibility, prospects, and challenges that entail training LM-in-theloop post finetuning with human gameplays in ClubFloyd adaptation as in Table 1.We use a similar set up for action recommendation as in Yao et al. (2020), where a pretrained GPT-2 LM is adapted with Clubfloyd dataset to recommend actions to DRRN agent.In addition to training the DRRN agent with TD-learning (Equation 2), we collect the transitions (o t , a t , o t+1 , r t+1 ) throughout the game episode, e T D , and populate them in D + and D − based on a heuristic that depends on-reward, return, and the game states.</p>
<p>First, with LM parameterized by θ and generating action candidates, we train DRRN for n RL consecutive episodes.After n RL episodes, we sample d LM sized dataset from D + , and D − with probabil- ities p + and 1 − p + respectively for 2000 gradient steps at finetuned after every k game steps.To train LM we use a weighted cross-entropy loss:
L LM (θ) = −E (at,ot)∼(D + ,D − ) log P θ (a t | o t )•h (•)
(4) Then, we plug-in back the in-game trained LM to recommend actions for the DRRN agent.The maximum buffer size of D + , D − , p + , d LM , and n RL are all game-specific hyperparameters.The h (•) is defined as a function of reward r t , or action-advantage, A(o t , a t ), or assumed 1 uniformly ∀(o, a) ∈ O × A. We evaluate different approaches based on the sampling of transitions, and the loss function (L), used for training the language model.Approaches for LM-in-the-Loop based on the construction of D, and sampling are:</p>
<p>Uncategorized Transitions (UT): In this setting the transitions stored in the buffer are not categorized by any special heuristic function.We simplify this approach by maintaining a single buffer, D in place of two.This is a weaker baseline than other heuristics to select useful transitions based on their importance.</p>
<p>State Feature Categorized (OC): In this, the transitions are labeled as useful or not based on whether an action a t resulted in reward increase or if the agent's location changed.i.e., moved from one room to another.As the location information received is an artifact of the game framework, we consider this as the Oracle.Further, we vary p + to maximize the transitions that encourage exploration to eventually result in improved performance in the game.Here, h (•) is fixed as
1 uniformly ∀(o, a) ∈ O × A.
Reward Trajectories (RT): The reward from transitions, r t , is used to categorize positive and negative trajectories.When r t &gt; 0 all transitions up until the earlier non-zero reward are considered positive and added to D + .</p>
<p>Further, we explore utilizing the return, reward, and advantage function of actions to re-weight L LM using the h (•) function over UT setting as above.We describe them as follows:</p>
<p>Weighted Cross-Entropy: In this, the transition data is kept in a single buffer D similar to in the UT setting.To finetune the language model using the weighted cross-entropy loss (Equation 4), we use the exponential weighted advantage function (Equation 3).We use two variants to the weights, wherein UT EA is non-negative using h(•) function:
h(o t , a t ) = e β•A(ot,at) ,(5)
where, β ∈ R + is a hyperparameter.The other variant, UT LA , allows for negative weights with h(•) as follows:
h(o t , a t ) = 1 + β • A(o t , a t ),(6)
where, β ∈ R + is a hyperparameter.</p>
<p>Experiments</p>
<p>We perform comprehensive experiments2 with LMin-the-loop set up to study the following questions:</p>
<ol>
<li>
<p>Does including the language model in the training loop improve performance?</p>
</li>
<li>
<p>Does LM-in-the-Loop mitigate the reliance on human gameplay transitions?</p>
</li>
<li>
<p>Should the transitions be categorized for improved learning?</p>
</li>
<li>
<p>Can we make LM itself a policy network without DRRN with LM-in-the-Loop?</p>
</li>
<li>
<p>Does training LM-in-the-Loop affect generalization to other games?</p>
</li>
</ol>
<p>Task Adaptation Dataset</p>
<p>ClubFloyd dataset (Yao et al., 2020) is a collection of crawled data from the ClubFloyd website.The dataset comprises of gameplay from experienced players; however, they may not be familiar with the particular games.The data is preprocessed and contains around 217K pairs of context an in the form of ((o j−1 , a j−1 , o j ), a j ).</p>
<p>Benchmark and the Metric</p>
<p>Jericho (Hausknecht et al., 2020) is a learning environment that supports human-written interactive fiction games as described in Figure 1.We chose 10 games based on the diversity in the challenges faced in each game such as large action space, solution length, and reward sparsity as mentioned in Hausknecht et al. (2020).We use the average of the last 100-episodes' score with standard error for individual games (Hausknecht et al., 2020) as our metric for evaluation.</p>
<p>In addition, we report the average score normalized (avg.norm) against the maximum score possible in each of the games, which estimates the human-machine gap in text-based games.Finally, we also report the relative performance percentage difference between the baseline and the best approach mentioned as ∆% in Table 1 to capture the improvement as the range of the scores in each game is different.</p>
<p>Model Details</p>
<p>Language model (GPT-2) is first finetuned on ClubFloyd dataset.</p>
<p>Given the context, (o j−1 , a j−1 , o j ), the finetuned GPT-2 proposes action candidates for DRRN to choose.Following that, every action candidate and context is encoded with a GRU.Then a decoder combines the representations to estimate the Q-value using a multilayer Perceptron (MLP) and updates the DRRN agent parameter Φ.During the training process of the DRRN agents, the context-action pairs are stored in the replay buffers.After k steps, we sample d LM sized dataset from D + , and D − with probabilities p + and 1 − p + respectively and update the language model with in-game transitions.Then, the updated language model is used to propose the action candidates.</p>
<p>The buffer size is defined as 100K for replay buffers that uses First-In-First-Out (FIFO) strategy to replace samples.To train, d LM samples are sampled uniformly at random from the two buffers D + and D − .However, the probability of choosing the buffers are defined by p + and p − (1 − p + ) respectively.The number of gradient steps for LM training is fixed at 2000 across the set ups.And, across games we experiment with the hyperparameter p + ∈ [0, 1] in 0.1 increment, and the value for LM finetuning frequency k ∈ [2k, 5k, 10k, 20k].The results tabled are estimated from 5 runs.</p>
<p>Results</p>
<p>We follow the questions enumerated in §5 to analyze the effect of in-game learning of language models for action recommendations.</p>
<p>Effect on Performance</p>
<p>To understand the effect on performance with LMin-the-Loop, we follow the experimental set up in §5.3 to evaluate on Jericho benchmark.Table 5 compares the different methods detailed in §4.1 with reproduced norm score of CALM (Yao et al., 2020) as the baseline.We see that categorizing the transitions using state features (OC) scored the highest in all tasks, suggesting that LM-in-the-Loop enables improved performance.This was also reflected in the avg.norm score with an improvement of ≈ 4% over the baseline.This is ≈ 53% more avg.improvement over the scores obtained by the baseline model.Although the performances of OC are closer to the baseline in many games, the in-game training accelerated the convergence in most games.However, the improvement with OC is, in a way, a loose upperbound to in-game learning with LMin-the-Loop, as special techniques to reweight the transitions (UT LA , and UT EA ), or reward based categorization RT only improved the avg.norm score by ≈ 0.6%.On the other hand, the avg.norm score with Uncategorized Transitions (UT)   dropped to 19.2% which is ∼ 1% below the baseline performance.The difference in performance between UT, and OC with the baseline suggests that LM-in-the-loop for action recommendation is helpful but requires careful selection of transitions for training the language model.In Figure 3, we compare the % of steps in-game learning methods took in average to achieve k% of CALM model's best performance across the games.We see that LM-in-the-Loop techniques enabled atleast 2× on average 3 acceleration in convergence, although the weaker alternatives to OC with reward based categorization, and reweighted techniques only provided meagre improvements 3 Individual comparison of each method across the games is in §B.</p>
<p>over the baseline (Table 5).This shows that the adaptation offered with the ClubFloyd dataset was insufficient, and off-the-shelf techniques can drastically accelerate convergence.</p>
<p>Emphasis on Human Annotations</p>
<p>CALM model-the baseline-uses all of the ∼ 220K transitions in the ClubFloyd dataset to adapt GPT-2 model for action recommendation.But, by using in-game transitions for LM-in-the-Loop training, the LM is provided with game specific information.So, the requirement for adapting GPT-2 with human annotated transitions should be minimal.Yao et al. (2020) show that CALM's performance decreased significantly when adaptation was done with 10% of ClubFloyd dataset.The reproduced results of CALM with 10% of adaptation data shows the avg.norm score as 18.5% across the games in Table 2. Using State features (OC) with 10% of the adaptation date achieved an average norm score of 21.8%, which was more than even using 100% of the adaptation data with CALM.Although there was a small decline in the performance of the detective game, it was insignificant because it was still within the standard error.These results suggest empirically that we can reduce the burden of collecting human-played or human-annotated data by doing in-game learning.</p>
<p>Effect of Weight Adjusted LM Loss</p>
<p>Categorization of transitions, although possible in most games, often requires game specific functions to identify what is a good and a bad transition.However, a generalized technique would be to use a notion of the usefulness of transitions that don't require game specific mechanisms.We explore reweighted cross entropy loss as in Equation 4with variations of the h(•) functions from being uniformly distributed as 1 over (o, a) ∈ O × A to using advantage function with two variations as in Equation 5and Equation 6.While UT uses vanilla cross-entropy loss to train the LM on transitions sampled from buffer D, UT EA and UT LA adjusts the experience according to the advantage, A(o, a), of the actions chosen in those observations.We use causal language modeling to train the GPT-2 LM to discourage the LM in generating a useful action in a state and discouraging the not useful.As A(o, a) ∈ [−∞, +∞], it is important to understand how it affects the language model.A negative advantage for a ′ in o ′ should discourage the LM from suggesting a ′ in o ′ .UT EA rescales the LM-loss with h(•) ∈ [0, 1), while UT LA works similar to Unlikelihood training as proposed in Welleck et al. (2019) by maintaining the same scale as A(o, a).But, from the restuls we see that the differences in reweighting did not tangible affect the performance as seen in Table 5 (Columns UT EA and UT LA ).</p>
<p>GPT-2 as Policy Network?</p>
<p>So far, we have explored the performance of LMin-the-Loop training of GPT-2 for suggesting candidate actions for the DRRN, but to disambiguate the role of GPT-2 and DRRN, we conduct an ablation experiment.Instead of providing action candidates to DRRN agent, what if GPT-2 chose the argmax action?The experiment addresses two questions:</p>
<p>(1) Is the improvement in the performance and acceleration as in §6.1 largely from the GPT-2 training?and (2) Does the max action of the LM reflect the game dynamics?</p>
<p>Games</p>
<p>Frozen LM In-game LM
Zork1 3.3 [5.7] 3.3 [5.7] Inhumane 0 [0] 0 [0] Detective 23.3 [5.7] 15 [7] Zork3 0 [0] 0 [0] Omniquest 0 [0] 1.6 [2.8] Library 0 [0] 0 [0] Balances 0 [0] 0 [0] Ludicorp 5.3 [2.0] 4.1 [2.9] Dragon 0 [0.0] 0 [0.] Ztuu 0 [0.0] 0 [0.0] Norm 1.1% 1.1%
Table 3: Irrespective of whether the LM was maintained frozen or trained with LM-in-the-Loop, GPT-2 model as policy network yielded zero in the majority of games when DRRN is not used for decision-making.</p>
<p>McClelland et al. ( 2020) motivate the set up of a language model placed in situated learning set up, where it can interact and learn from the environment.However, other than the study conducted in this work, there exists little evidence for interactive learning of an LM from the game transition.Table 3 shows the results of how domain adapted pretrained GPT-2 fairs in the ultimate goal of learning solely from interaction on the text games.We observe that the model's performance is 0 in most games when not using DRRN for decision making.Irrespective of whether the LM was kept frozen or trained with in-game transitions, there was no palpable evidence of language understanding through game semantics observed.</p>
<p>But, the possible explanation for the performance in §6.1 is that the language model learns more game specific actions, though not optimal, leading to DRRN contributing significantly to the performance observed.</p>
<p>Generalization to Other Games</p>
<p>We observed from the previous results that the agent performing well could be attributed to the actions suggested by the LM that that adapted from the transitions in-game.While that is encouraging, it also risks the generality of such an agent in being transferrable to other games.To quantify the loss in generality, we use the LM-in-the-Loop trained GPT-2 from zork1 game and continue to train with 4 different target games-zork3, Ludicorp, inhumane, and Ztuu.</p>
<p>For the settings to compare, we used using state features (OC), and Uncategorized Transitions (UT).To train the LM model, we set the buffer size as 100K in both the settings.The results tabled
0 [0] 1 [2.1]
Table 4: Transferring LM-in-the-Loop trained GPT-2 did not provide guarantee improvements over the baseline.Also, the performance remained unexplainable with A ≈ and (A × O) ≈ .</p>
<p>in Table 4 shows that in-game learning techniques suffered from extending to other games when compared with the baseline performance of CALM.As LM-in-the-Loop training performed well on these games when trained in isolation as seen from Table 5, it would be interesting to observe if that depended on some notion of similarity between the source and the target games.</p>
<p>To that end, we define two measures of similarity using the actions, A ≈ , and the action-observation combinations (O × A) ≈ with the target games.For A ≈ we populate the possible actions available from the Jericho environment on the source game, zork1, and each of the target games considered.We estimate the BLEU-2 (Papineni et al., 2002) score for every action in the source, a ∈ A s , with all actions in the target game, A t , as the reference.The average over the corpus BLEU is tabled in the A ≈ column in Table 4. But, A ≈ did not have any reasonable correlation over the performance observed.While zork3 had an expected action space similarity with the source game and did not have a significant performance drop, the counterfactual scenarios for when A ≈ is much lower in other games, the performance was mixed.Although in inhumane and Ztuu, OC performed significantly lower than CALM, in Ludicorp the performance was better than the baseline rendering the action similarity score, A ≈ , ineffective in explaining the results.Also, the MAUVE score measured between the game transitions, (O × A) ≈ , was too low suggesting a weak semantic overlap between the game spaces.If generality were to be affected with lower similarity between the source and target games measured along action, action×observations, the results were inconsistent in that regard.</p>
<p>Although Yao et al. (2021) observed that the models did not naturally respect the notion of semantics, the results that the learnability in the target games being strongly affected when the LM-in-the-Loop is adapted to zork1 does not entail the LMs being agnostic to notions of semantics.At the same time, the results we observed doesn't either sug-gest that semantics guide the results.Such mixed observations probably only suggest that notions of semantics through automatic evaluations are not the tools to interpret LMs in text games.</p>
<p>Discussion</p>
<p>The comparison of LM-in-the-Loop with baseline and their absolute performances from Table 1 shows that there is more room for improvement.Despite the LMs having strong linguistic priors from pretraining, the large action space when it comes to generative task is one of the significant challenges in adapting LMs to text-based games.Although interactive learning is promising, towards realizing interactive task solving agents, it is imperative to address the issues due to scalability and data-efficiency.The results in the paper through exploring the possibility of adapting language models for action suggestions through utilizing the ingame generated transitions opens up discussions on several key questions:</p>
<p>While there is improvement in performance, and acceleration in comparison to not learning from the game transitions, the absolute improvement with respect to the games has still a long way to go.When DRRN module was plugged out for ablation, the argmax action of LM was not even close to a reasonable performance indicating the heavy lifting in planning was from DRRN.Towards realizing LMs in situated learning environments, adapting LMs to different games is a challenging language understanding milestone.Specifically, it is important to align LM's action generation likelihood to reflect the action value function.</p>
<p>Despite the acceleration and a reduced need for human transitions to adapt LMs for action suggestion, interpreting their performance through the conventional lens of automatic semantic and syntax scores is less effective.It is, then, only imperative to make the application of LMs in text games interpretable through automatic metrics that identifies important transitions to train LM-in-the-Loop.</p>
<p>Limitations</p>
<p>The paper analyzes the possibility and challenges in LM-in-the-Loop training of GPT-2 model for action recommendation in text based games.The claims in the work can be further supported with experiments on different LLM.Similarly, the generalization experiments could have added more support to the lack of evidence with additional games.However, these are compute intensive experiments and the claims are largely made in consideration to the limitations in the set up.</p>
<p>[SEP]  northeast[SEP] . . .you are carrying nothing.you are still on the streets. . . .[SEP] northeast''.[SEP] and [CLS] are special tokens specific to LM-training.Yao et al. (</p>
<p>Figure 2 :
2
Figure 2: Training LM-in-the-Loop post-human-annotated dataset adaptation: RL agent (DRRN) picks the action recommended by the language model (at T ), which is GPT-2.The context pairs are stored in the replay buffers that are categorized by some heuristic.Then the Language model is updated with in-game transitions after k learning steps in the game.Finally, the updated language model (T + k) actions are recommended.</p>
<p>Figure 3 :
3
Figure 3: We see that LM-in-the-Loop techniques only need half of the steps to achieve the best of CALM.Whereas, using state feature based categorization (OC) achieved better acceleration and performance over the rest.</p>
<p>Figure 4 :
4
Figure 4: Comparison of learning dynamics of the different LM-in-the-Loop techniques with the baseline CALM agent across the selected 10 games in Jericho.</p>
<p>[2.7] 288.5 [1.5] 289.3 [0.2] 288.3 [1.3] 285.1 [5.6] 288.5 [1.5]
GamesCALMUTUT LAUT EARTOC∆(%)MaxScoreZork130.7 [4.8]32.6 [4.4]30.4 [8.5]35.6 [5.7]30.7 [3.8]38.0 [1.7]23%350Inhumane24.8 [2.7]21.9 [5.24] 28.9 [11]27.3 [3.1]29.1 [12.7]43.4 [3.8]75%90Detective290.9 0%360Zork30.3 [0.09]0.3 [0.14]0.4 [0.1]0.6 [0.1]0.6 [0.1]0.7 [0.2]133%7Omniquest6.7 [0.3]6.0 [0.6]6.6 [0.9]6.6 [1]6.0 [0.79]7.8 [1.7]16%50Library11.2 [1.3]9.3 [1.1]9.5 [1]10.3 [0.2]10.3 [1.8]12.1 [0.7]8%30Balances9.3 [0.2]9.6 [0.1]9.6 [0.2]9.5 [0.2]9.7 [0.2]9.7 [0.1]4%51Ludicorp10.4 [0.7]11.4 [2.6]12.5 [1.1]11.9 [2.6]11.3 [3.1]15.1 [0.8]45%150Dragon0.1 [0.06]0.1 [0.1]0.3 [0.3]0.3 [0.3]0.1 [0.12]0.3 [0.2]200%25Ztuu3.8 [0.18]4.4 [0.0]4.5 [0.2]4.4 [0.1]4.3 [0.1]4.5 [0.1]18%100Norm Score 20.1%19.1%20.6%20.9%20.7 %24.0%52.37% 100%</p>
<p>Table 1 :
1
From the results, it can be consistently seen that LM-in-the-Loop provides a performance improvement over CALM.Especially, categorizing the transitions with state features (OC) scored the highest with ∼ 53% improvement over the scores obtained by the baseline model.</p>
<p>Table 2 :
2
Using State Features (OC) achieved an average norm score of 21.8% with 10%, which was more than even with CALM using 100% of the adaptation data.
GamesCALMCALMOC100%10%10%Zork130.7 [4.8]29 [3.4]35.1 [2.3]Inhumane24.8 [2.7]15.7 [14.7] 27.5 [6.8]Detective290.9 [2.7] 289.5 [0.2] 289.6 [0.2]Zork30.3 [0.09]0.6 [0]0.7 [0.3]Omniquest 6.7 [0.3]5.9 [0.8]6.0 [1]Library11.2 [1.3]10.5 [1.5]10.2 [1.8]Balances9.3 [0.2]6.6 [3.5]8.6 [1.6]Ludicorp10.4 [0.7]10.2 [0.4]13.7 [0.4]Dragon0.1 [0.06]0.1 [0.06]0.3 [0.2]Ztuu3.8 [0.18]3.6 [0.1]4.1 [0.1]Norm20.1%18.5%21.8 %
 MAUVE-Score (Pillutla et al.,<br />
) measures semantic relatedness of an LM generated text with that of human generated text distribution using an LLM representation of the texts.
The codebase for all experiments will be released after the anonymity period.
AcknowledgementsSarath Chandar is supported by a Canada CI-FAR AI Chair and an NSERC Discovery Grant.The authors acknowledge the computational resources provided by the Digital Research Alliance of Canada and Mila Compute resources.We are thankful to Siva Reddy for their helpful feedback in this work.A AppendixA.1 Language Model SetupWe use a GPT-2 (Base)(Radford et al., 2018b)model with 12-layers, 768-hidden units, and 12attention heads with 117M parameters pre-trained on the WebText corpus.This model's implementation and pretrained weights are obtained from(Wolf et al., 2020, Huggingface).We train for 3 epochs on the ClubFloyd dataset following(Yao et al., 2020)to minimize the crossentropy loss, as shown in Table5.We use AdamW to optimize model's weights to minimize the loss, with the learning rate as 2 × 10 −6 and Adam epsilon as 1 × 10 −9 .We use a linear schedule with a warmup of 0.1 for the learning rate.Finally, we clip gradients with a maximum gradient norm of 1.Following(Yao et al., 2020)'s finetuning process, we exclude using Jericho-related transcripts by setting the flag as 1.We used random seeds to select the dataset to avoid bias in selecting data for the LM training.Model MetricFinalA.2 Reinforcement Learning Agent Setup:We train on 10 interactive fiction games from the Jericho benchmark(Hausknecht et al., 2020).The states are observations concatenated with items in possession of the player and their current location description provided by the game engine using commands inventory and look.A single game episode runs for 100 environment steps at max or gets terminated before the game is over or won.We use the look and inventory commands to add location and inventory descriptions to observations, followingHausknecht et al. (2020).We train DRRN asynchronously on 8 parallel instances of the game environment for 100, 000 steps for each game.At each step, the Q-value is estimated using the DRRN agent, and the action is selected based on the soft-exploration policy.Action's admissibility is predicted based on the textual response of the game.Then, inadmissible are filtered out using a FastText model(Joulin et al., 2017).The agent is optimized using adam optimizer with a learning rate of 10 −5 .We sample transitions of batch size 64 from priority buffer with a priority fraction of 0.5.The discount factor in determining the future reward's importance is 0.9.The size of the embedding dimension is 128, and the hidden dimension is 128.Finally, the gradient is clipped with a maximum gradient norm of 5. We train 5 separate runs for each game and report the average score.We use the average of the last 100 episode scores to calculate the final score.A.3 Software DetailsWe used PyTorch for the code implementation and Huggingface to load pre-trained language models.We used Weights &amp; Biases(Biewald, 2020)for experiment tracking and visualizations to develop insights for this paper.Finally, the seaborn package is used to generate plots.A.4 Mauve Score
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Luu, 10.48550/ARXIV.2204.01691Peter Pastor. Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng, Carolina Parada2022Do as i can. not as i say: Grounding language in robotic affordances</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2020</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Experiment tracking with weights and biases. Software available from wandb. Lukas Biewald, 2020</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, 10.18653/v1/2020.emnlp-main.703Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems. 2021</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, NIPS 2014 Workshop on Deep Learning. 2014. December 2014</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, Textworld: A learning environment for text-based games. 2018</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019a. June 2-7, 20191</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019b. June 2-7, 20191</p>
<p>A survey of data augmentation approaches for nlp. Varun Steven Y Feng, Jason Gangal, Sarath Wei, Soroush Chandar, Teruko Vosoughi, Eduard Mitamura, Hovy, 10.1609/aaai.v34i05.6297arXiv:2105.03075Proceedings of the AAAI Conference on Artificial Intelligence. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, the AAAI Conference on Artificial Intelligence2021. 202034arXiv preprintInteractive fiction games: A colossal adventure</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Bag of tricks for efficient text classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European ChapterShort Papers; Valencia, SpainAssociation for Computational Linguistics20172</p>
<p>UNIFIEDQA: Crossing format boundaries with a single QA system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Word meaning in minds and machines. M Brenden, Gregory L Lake, Murphy, 2021Psychological review</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, Yuke Zhu, arXivPre-trained language models for interactive decisionmaking. 2022</p>
<p>Learning word vectors for sentiment analysis. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USA2011Association for Computational Linguistics</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Felix James L Mcclelland, Maja Hill, Jason Rudolph, Hinrich Baldridge, Schütze, Proceedings of the National Academy of Sciences. 117422020</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Stabilizing transformers for reinforcement learning. Emilio Parisotto, H Francis Song, Jack W Rae, Razvan Pascanu, C ¸aglar Gülc ¸ehre, M Siddhant, Max Jayakumar, Raphael Jaderberg, Aidan Lopez Kaufman, Clark, CoRR, abs/1910.06764Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. 2019</p>
<p>Mauve: Measuring the gap between neural text and human text using divergence frontiers. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaid Harchaoui, 2021In NeurIPS</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018a</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2018b</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Can wikipedia help offline reinforcement learning?. Machel Reid, Yutaro Yamada, Shixiang Shane Gu, CoRR, abs/2201.121222022</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works. 20208</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, CoRR, abs/2107.084082021</p>
<p>Prompts and pre-trained language models for offline reinforcement learning. Denis Tarasov, Vladislav Kurenkov, Sergey Kolesnikov, ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. 2022</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2022</p>
<p>Do promptbased models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, arXiv:2109.012472021arXiv preprint</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, arXiv:1908.04319Neural text generation with unlikelihood training. 2019arXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Insights into pre-training via simpler synthetic tasks. Yuhuai Wu, Felix Li, Percy Liang, arXiv:2206.101392022arXiv preprint</p>
<p>Deep reinforcement learning with transformers for text adventure games. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, Chengqi Zhang, 10.1109/CoG47356.2020.92316222020 IEEE Conference on Games (CoG). 2020</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Keep CALM and explore: Language models for action generation in textbased games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in Neural Information Processing Systems. Curran Associates, Inc201528</p>            </div>
        </div>

    </div>
</body>
</html>
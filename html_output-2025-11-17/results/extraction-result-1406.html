<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1406 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1406</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1406</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-188dac491f04c56e1eb7d7b33ac6aa0b87303232</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/188dac491f04c56e1eb7d7b33ac6aa0b87303232" target="_blank">DeepMDP: Learning Continuous Latent Space Models for Representation Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states, and shows that the optimization of these objectives guarantees the quality of the latent space as a representation of the state space.</p>
                <p><strong>Paper Abstract:</strong> Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1406.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1406.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Markov Decision Process (DeepMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameterized latent-space world model trained by minimizing two tractable losses: (1) reward prediction error and (2) a distributional loss between the embedded true next-state distribution and the model's next-latent distribution (typically using the Wasserstein or a Norm-MMD metric). Provides theoretical bounds linking model/representation loss to value-function error and (with Wasserstein) to bisimulation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepMDP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent-space world model: an encoder phi maps high-dimensional observations s to continuous latent states \overline{s}; a learned deep reward network predicts \overline{R}(\overline{s},a); a learned deep transition network predicts a distribution over next latents \overline{P}(\cdot|\overline{s},a). The transition loss is a probability metric (Wasserstein-1 or more generally a Norm-MMD) between the pushed-forward true next-latent distribution phi P(\cdot|s,a) and the model's predicted latent distribution. The model is trained jointly with the encoder via expected losses under sampled state-action distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning (synthetic DonutWorld and Atari 2600 games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reward absolute error (|R - \overline{R}|) and distributional distance between next-state distributions measured by Wasserstein-1 (W_d) or members of the Norm-Maximum-Mean-Discrepancy family (e.g., Total Variation, Energy distance); local/global variants (expected under a state-action sampling distribution or sup over all states).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single scalar fidelity numbers reported; theoretical bounds relate the global/local losses L_{\overline{R}} and L_{\overline{P}} to value-function error: |Q - \bar{Q}| \le (L_{\overline{R}} + \gamma K_{\bar{V}} L_{\overline{P}})/(1-\gamma). Empirically the DeepMDP recovered the 2D latent in DonutWorld and yielded consistent performance improvements over a C51 baseline on the 60-game Atari suite (qualitative/aggregate improvement reported, no per-game numeric table provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: learned latents in DonutWorld correspond to underlying 2D agent position; latent-space heatmaps and distance visualizations show that bisimilar or value-equivalent states are merged. In Atari, representations are shown to be more task-relevant than pixel autoencoders (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of latent-space distances/heatmaps (DonutWorld), analysis via bisimulation metrics and theoretical guarantees linking embedding distances to value differences; qualitative comparisons to autoencoders.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitative: Wasserstein is expensive and has biased stochastic gradients; to mitigate, experiments assume deterministic transitions (reducing W to a pointwise distance), and training uses gradient-penalty style Lipschitz constraints for stability. No wall-clock, parameter-count, or GPU-day numbers reported. Models implemented as small feed-forward or convolutional nets (transition model choices: one-layer FC, two-layer FC, single conv layer).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No quantitative compute-speed comparison; algorithmic notes: minimizing DeepMDP losses as an auxiliary task improves final policy performance compared to baseline C51 with same backbone, implying better sample-use of shared encoder; optimization can be difficult due to competing objectives (reward vs transition) and local minima.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Empirical: learning DeepMDP as an auxiliary objective yielded nearly consistent performance improvements over the C51 baseline across 60 Atari games; in DonutWorld the latent recovered true 2D structure. No absolute numeric scores provided in-text for aggregate improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Theoretical and empirical evidence: DeepMDP losses prioritize task-relevant aspects of observations (reward and transition behaviour under chosen metric), producing representations that better approximate value functions of a broad class of policies; high fidelity in the DeepMDP (small L_{\overline{R}}, L_{\overline{P}}) directly bounds value-function error, so fidelity improvements are expected to translate into better policy/value learning. Empirically auxiliary DeepMDP training improved model-free RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Key trade-offs: (1) Wasserstein yields strong representation guarantees (connection to bisimulation) but is computationally costly and subject to biased stochastic gradients; (2) enforcing Lipschitz continuity simplifies analysis but is a strong assumption and can complicate optimization; (3) reward and transition losses can compete during training, producing local minima (e.g., trivial zero-latent solution minimizes transition loss but is poor for reward prediction); (4) choosing metrics other than Wasserstein (Norm-MMD family) can improve computational tractability or better match environment smoothness but may lose bisimulation guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predict next latent state and immediate reward (rather than pixels); use Wasserstein-1 for transition loss when representation guarantees are desired; alternately use Norm-MMD metrics (Total Variation, Energy distance) for different smoothness inductive biases; constrain model Lipschitz constants (gradient-penalty) and choose inductive transition architectures (convolutional transition model performed best in Atari experiments); train losses as expectations under sampling distributions (local losses) to scale with SGD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically against observation reconstruction, next-observation prediction, and VPN-style 'next logits' auxiliary objectives: DeepMDP's next-latent prediction objective outperformed all three as an auxiliary task for C51 on Atari. The paper also argues that standard negative log-likelihood (KL) next-state training used in many model-based works can be interpreted as training a DeepMDP (via Pinsker's inequality relating KL and TV), but the Wasserstein-based DeepMDP gives stronger representation guarantees (bisimulation link) than KL/TV alone.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations/insights: use Wasserstein transition loss when representation/bisimulation guarantees are desired; for practical optimization consider Norm-MMD alternatives if environment smoothness suggests a different value-function seminorm; use a convolutional transition network to introduce useful inductive biases in visual domains; enforce Lipschitz constraints (e.g., gradient penalty) to stabilize Wasserstein-style training; focus on local losses under policies of interest (stationary distributions) when full-state learning is infeasible. The paper stresses that there is no single universal optimum and trade-offs must be managed between fidelity (metric choice), interpretability (latent design), efficiency (metric computability), and task utility (local vs global losses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1406.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1406.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent-space models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent-space environment models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of world models that learn a low-dimensional continuous latent state and model transitions/rewards in latent space rather than in pixel/observation space; training objectives vary (pixel reconstruction, next-latent prediction, value-consistent losses).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>latent space model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder phi maps observations to continuous latents; separate networks predict reward and next-latent distribution; losses can be defined on next-latent (as in DeepMDP) or on reconstructing/predicting pixels. Models may be deterministic or probabilistic, and can use metrics like MSE, KL, or Wasserstein on predicted distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning, visual-control domains (Atari, synthetic environments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Depends on instantiation: pixel MSE or NLL for observation-based models; distributional metrics (KL, TV, Wasserstein, MMD) for latent distributions; reward L1 error.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single number â€” performance depends on loss choice and architecture; the paper reports that latent next-state prediction objectives (DeepMDP-style) recover underlying low-dimensional structure in DonutWorld and yield better downstream RL performance than pixel reconstruction in Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent models can be interpretable if latents align with true state factors (as in DonutWorld), but pixel-prediction objectives may force encoding of irrelevant perceptual details.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent visualization, heatmaps, measuring invariance to irrelevant visual factors, bisimulation analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lower than full image generative models when latent dimension is small, but cost depends on the chosen distribution metric (Wasserstein expensive; KL cheaper).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Latent modeling typically more efficient for planning/representation than pixel prediction since it avoids high-dimensional reconstruction; DeepMDP-style latent objectives outperform pixel-based auxiliary tasks empirically as representation learners in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Empirically, latent next-latent objectives aided RL on Atari more than observation-reconstruction or next-observation objectives in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Learning to predict next latent states and rewards focuses capacity on task-relevant dynamics, improving downstream value/policy learning versus full observation prediction which preserves irrelevant details.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent compression improves efficiency but may lose irrelevant details needed for other objectives; metric choice governs representation smoothness and theoretical guarantees; choosing latent dimension and transition architecture impacts both interpretability and fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choice of metric (Wasserstein vs MMD vs KL), latent dimensionality, deterministic vs probabilistic transitions, architectural inductive biases (convolutional vs FC), local vs global loss weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to pixel predictors, latent predictors (DeepMDP) provided better task-relevant representations and improved RL performance in the experiments; compared to KL/NLL training, Wasserstein yields stronger representation/bisimulation guarantees though at higher computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests: minimize reward + next-latent distributional loss (Wasserstein when bisimulation guarantees are desired); pick architecture with suitable inductive bias for domain (conv nets for images); use local losses under on-policy or replay distributions for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1406.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1406.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural world-model architecture that learns a compact latent representation (VAE) of observations and a recurrent network (MDN-RNN) to predict future latents; used for sampling imagined rollouts and evolving policies in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent world models facilitate policy evolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (VAE + RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder (VAE) compresses frames into latents; a recurrent predictive model models dynamics in latent space (often an MDN-RNN) and can be used to simulate trajectories for policy learning; reward/value components can be trained on latents or via imagination rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + recurrent predictive network)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual reinforcement learning / simulated environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically evaluated by reconstruction error (VAE), negative log-likelihood of predicted latents, and downstream policy performance when planning in imagined trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper; mentioned as prior art that uses pixel/predictive losses to learn latent models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latents sometimes correspond to semantically meaningful factors (depends on VAE training); the approach enables qualitative interpretability via latent traversals and simulated rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent traversals, visualization of imagined rollouts, inspection of VAE latents.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Moderate; requires training VAE and RNN components; cost depends on architecture and rollout horizon. Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as prior approach; DeepMDP is framed as a theoretically grounded alternative focusing on reward and distributional latent losses rather than pixel reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not measured in this paper; original work reported successful policy learning via imagined rollouts on toy environments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>World Models are useful for generating imagined trajectories for policy search, but pixel/prediction losses can encourage encoding irrelevant visual information; DeepMDP argues for task-oriented latent objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel-focused latent models may encode irrelevant perceptual details, trading representation compactness and task utility vs faithful observation reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of VAE for compression, recurrent predictive model for dynamics, optional controller trained in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>World Models emphasize generative observation reconstruction; DeepMDP emphasizes reward-and-distributional latent objectives which the authors argue yield better task-relevant representations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; cited as motivating prior work showing latent dynamics can be useful but lacking the theoretical value-function guarantees DeepMDP provides.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1406.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1406.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VPN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that predicts future abstract states and value-relevant quantities to perform planning in latent space; integrates model-based predictions with value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Value Prediction Network</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value Prediction Network (VPN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural architecture that learns abstract-state transitions and uses them to predict rewards and values for planning; training aligns transition predictions with value/logit targets rather than raw pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model / planning model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning with planning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>VPN-style losses often measure how well transitions produce correct next-state value/logit predictions (e.g., cross-entropy or KL on predicted logits), not raw pixel fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numbers in this paper; VPN-style auxiliary loss (next logits prediction) was implemented as an ablation and was found to hurt performance when used as an auxiliary task with a distributional RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Interpretable to the extent that predicted abstract states are aligned to value-relevant features; not emphasized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified here; generally via analysis of predicted value trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to other latent predictors; cost depends on the architecture of the transition and value heads.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>In this work VPN-style auxiliary losses underperformed the DeepMDP next-latent objective as an auxiliary task for C51.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used as an auxiliary objective in the paper's Atari experiments, VPN-style 'next logits' auxiliary loss degraded performance compared to the DeepMDP objective.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>VPN-style losses may be helpful when the learned model is used directly for planning, but as a mere auxiliary objective with distributional RL (C51) they prioritized signals incongruent with the distributional agent and hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Predicting value/logits can bias representation to value-specific features but may not align with distributional learning objectives; hence auxiliary VPN-style loss can be detrimental in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Training transition to predict next-state representations that produce correct Q/logit predictions (a distributional analogue of VPN was used as an ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DeepMDP next-latent prediction outperformed VPN-style next-logits prediction as an auxiliary task for the tested distributional RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests VPN-style losses are more appropriate when the learned model will be used for planning; they are not recommended as blind auxiliary losses for distributional agents in place of next-latent objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1406.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1406.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pixel/predictive models (obs recon / next obs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observation-reconstruction and next-observation prediction models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>World-model variants trained to reconstruct the current observation or predict the next observation (pixels) from latents; common in literature but can force encodings of perceptual details irrelevant to task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>observation reconstruction / next-observation predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder (autoencoder) or predictor network mapping latents to pixel reconstructions (current or next frame); loss typically pixel-wise MSE or NLL over pixels; sometimes combined with recurrent latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit observation-predictive model / latent+pixel generator</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual reinforcement learning (Atari, simulated environments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction loss (MSE or NLL) on pixels; potentially perceptual metrics if used.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically in this paper; autoencoders solved pixel reconstruction in DonutWorld but their learned latents were less task-useful than DeepMDP latents (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Low for task utility: tends to encode visual details (colors, backgrounds) that are irrelevant to dynamics or reward; latent axes may not correspond to underlying task state.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of reconstructions and latent heatmaps; comparison to DeepMDP latent maps.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Typically heavier (decoder and pixel loss) than pure latent predictors; cost scales with image size and decoder complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>In experiments, using observation reconstruction as an auxiliary objective was worse for downstream RL performance than DeepMDP next-latent objective.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Autoencoder latent representations were less effective for predicting value or supporting RL than DeepMDP latents in DonutWorld and Atari auxiliary-task comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity pixel reconstruction does not necessarily yield task-useful representations; it can distract model capacity into encoding irrelevant perceptual features.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel fidelity vs task utility: better pixel fidelity may reduce task performance due to encoding irrelevant details; computational cost higher for pixel losses.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use reconstruction / next-frame predictor as auxiliary losses; architectures include decoders and pixel-wise losses; in this paper these were used as ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DeepMDP next-latent prediction outperformed pixel reconstruction and next-observation prediction as auxiliary tasks for the tested RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>When goal is representation for control/value estimation, paper recommends next-latent/reward objectives (DeepMDP) over pixel reconstruction; pixel models may be preferable when accurate image generation is required for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1406.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1406.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KL / NLL next-state models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative log-likelihood / KL-trained next-state models (probabilistic dynamics models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Probabilistic dynamics models trained by maximizing next-state likelihood (equivalently minimizing negative log-probability or KL to the true next-state distribution); widely used in model-based deep RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KL / NLL next-state model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models (probabilistic or deterministic) that predict next observations/states and are trained via a likelihood objective (negative log-likelihood) which is a one-sample estimate of KL divergence between model and true next-state distribution; often paired with reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic next-state model / model-based RL component</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based reinforcement learning across various control tasks (Atari, continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Negative log-likelihood (NLL) on next states; KL divergence estimated via NLL; Pinsker's inequality relates KL to Total Variation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numerical fidelity reported here; paper notes that minimizing NLL/KL can be interpreted as training a DeepMDP (via Pinsker) and therefore obeys local value-difference bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural density estimators in general; interpretability depends on model class (e.g., mixture density networks yield interpretable modes).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not specified in this paper; typically examine predicted distributions or sample rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Usually cheaper than direct Wasserstein minimization, but depends on output parametric form and sampling; commonly used since NLL/KL losses are straightforward with SGD.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More commonly used in practice due to computational convenience; paper frames them as special cases of DeepMDP training (via Pinsker linking KL->TV) with corresponding local value guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used in many model-based RL works cited (Chua et al., Hafner et al., Buesing et al., etc.) with demonstrated sample-efficiency gains in some domains; specifics are in the cited literature rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Minimizing KL/NLL trains models that lower local model loss and therefore reduce value-function error per the DeepMDP framework, but may not provide bisimulation-based representation guarantees that Wasserstein gives.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>KL/NLL is computationally convenient but gives weaker representational guarantees (w.r.t. bisimulation) than Wasserstein; choice reflects trade-off between tractability and theoretical representation strength.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Parametric conditional density models for next-state predictions (e.g., Gaussians, mixture models); optimize NLL; used as in several cited model-based RL approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper notes KL/NLL training can be interpreted within DeepMDP theory (via Pinsker), but suggests exploring Norm-MMD metrics (including TV, Energy distance) as alternatives to balance computability and representation guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that KL/NLL is practical and compatible with DeepMDP local bounds, but for representation learning Wasserstein may be preferred when bisimulation-style invariance is important; Norm-MMD family offers intermediate choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Value Prediction Network <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 2)</em></li>
                <li>Model-based reinforcement learning for atari <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1406",
    "paper_id": "paper-188dac491f04c56e1eb7d7b33ac6aa0b87303232",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DeepMDP",
            "name_full": "Deep Markov Decision Process (DeepMDP)",
            "brief_description": "A parameterized latent-space world model trained by minimizing two tractable losses: (1) reward prediction error and (2) a distributional loss between the embedded true next-state distribution and the model's next-latent distribution (typically using the Wasserstein or a Norm-MMD metric). Provides theoretical bounds linking model/representation loss to value-function error and (with Wasserstein) to bisimulation metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepMDP",
            "model_description": "A latent-space world model: an encoder phi maps high-dimensional observations s to continuous latent states \\overline{s}; a learned deep reward network predicts \\overline{R}(\\overline{s},a); a learned deep transition network predicts a distribution over next latents \\overline{P}(\\cdot|\\overline{s},a). The transition loss is a probability metric (Wasserstein-1 or more generally a Norm-MMD) between the pushed-forward true next-latent distribution phi P(\\cdot|s,a) and the model's predicted latent distribution. The model is trained jointly with the encoder via expected losses under sampled state-action distributions.",
            "model_type": "latent world model",
            "task_domain": "Reinforcement learning (synthetic DonutWorld and Atari 2600 games)",
            "fidelity_metric": "Reward absolute error (|R - \\overline{R}|) and distributional distance between next-state distributions measured by Wasserstein-1 (W_d) or members of the Norm-Maximum-Mean-Discrepancy family (e.g., Total Variation, Energy distance); local/global variants (expected under a state-action sampling distribution or sup over all states).",
            "fidelity_performance": "No single scalar fidelity numbers reported; theoretical bounds relate the global/local losses L_{\\overline{R}} and L_{\\overline{P}} to value-function error: |Q - \\bar{Q}| \\le (L_{\\overline{R}} + \\gamma K_{\\bar{V}} L_{\\overline{P}})/(1-\\gamma). Empirically the DeepMDP recovered the 2D latent in DonutWorld and yielded consistent performance improvements over a C51 baseline on the 60-game Atari suite (qualitative/aggregate improvement reported, no per-game numeric table provided in main text).",
            "interpretability_assessment": "Moderately interpretable: learned latents in DonutWorld correspond to underlying 2D agent position; latent-space heatmaps and distance visualizations show that bisimilar or value-equivalent states are merged. In Atari, representations are shown to be more task-relevant than pixel autoencoders (qualitative).",
            "interpretability_method": "Visualization of latent-space distances/heatmaps (DonutWorld), analysis via bisimulation metrics and theoretical guarantees linking embedding distances to value differences; qualitative comparisons to autoencoders.",
            "computational_cost": "Qualitative: Wasserstein is expensive and has biased stochastic gradients; to mitigate, experiments assume deterministic transitions (reducing W to a pointwise distance), and training uses gradient-penalty style Lipschitz constraints for stability. No wall-clock, parameter-count, or GPU-day numbers reported. Models implemented as small feed-forward or convolutional nets (transition model choices: one-layer FC, two-layer FC, single conv layer).",
            "efficiency_comparison": "No quantitative compute-speed comparison; algorithmic notes: minimizing DeepMDP losses as an auxiliary task improves final policy performance compared to baseline C51 with same backbone, implying better sample-use of shared encoder; optimization can be difficult due to competing objectives (reward vs transition) and local minima.",
            "task_performance": "Empirical: learning DeepMDP as an auxiliary objective yielded nearly consistent performance improvements over the C51 baseline across 60 Atari games; in DonutWorld the latent recovered true 2D structure. No absolute numeric scores provided in-text for aggregate improvement.",
            "task_utility_analysis": "Theoretical and empirical evidence: DeepMDP losses prioritize task-relevant aspects of observations (reward and transition behaviour under chosen metric), producing representations that better approximate value functions of a broad class of policies; high fidelity in the DeepMDP (small L_{\\overline{R}}, L_{\\overline{P}}) directly bounds value-function error, so fidelity improvements are expected to translate into better policy/value learning. Empirically auxiliary DeepMDP training improved model-free RL performance.",
            "tradeoffs_observed": "Key trade-offs: (1) Wasserstein yields strong representation guarantees (connection to bisimulation) but is computationally costly and subject to biased stochastic gradients; (2) enforcing Lipschitz continuity simplifies analysis but is a strong assumption and can complicate optimization; (3) reward and transition losses can compete during training, producing local minima (e.g., trivial zero-latent solution minimizes transition loss but is poor for reward prediction); (4) choosing metrics other than Wasserstein (Norm-MMD family) can improve computational tractability or better match environment smoothness but may lose bisimulation guarantees.",
            "design_choices": "Predict next latent state and immediate reward (rather than pixels); use Wasserstein-1 for transition loss when representation guarantees are desired; alternately use Norm-MMD metrics (Total Variation, Energy distance) for different smoothness inductive biases; constrain model Lipschitz constants (gradient-penalty) and choose inductive transition architectures (convolutional transition model performed best in Atari experiments); train losses as expectations under sampling distributions (local losses) to scale with SGD.",
            "comparison_to_alternatives": "Compared empirically against observation reconstruction, next-observation prediction, and VPN-style 'next logits' auxiliary objectives: DeepMDP's next-latent prediction objective outperformed all three as an auxiliary task for C51 on Atari. The paper also argues that standard negative log-likelihood (KL) next-state training used in many model-based works can be interpreted as training a DeepMDP (via Pinsker's inequality relating KL and TV), but the Wasserstein-based DeepMDP gives stronger representation guarantees (bisimulation link) than KL/TV alone.",
            "optimal_configuration": "Paper recommendations/insights: use Wasserstein transition loss when representation/bisimulation guarantees are desired; for practical optimization consider Norm-MMD alternatives if environment smoothness suggests a different value-function seminorm; use a convolutional transition network to introduce useful inductive biases in visual domains; enforce Lipschitz constraints (e.g., gradient penalty) to stabilize Wasserstein-style training; focus on local losses under policies of interest (stationary distributions) when full-state learning is infeasible. The paper stresses that there is no single universal optimum and trade-offs must be managed between fidelity (metric choice), interpretability (latent design), efficiency (metric computability), and task utility (local vs global losses).",
            "uuid": "e1406.0",
            "source_info": {
                "paper_title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Latent-space models (general)",
            "name_full": "Latent-space environment models",
            "brief_description": "General class of world models that learn a low-dimensional continuous latent state and model transitions/rewards in latent space rather than in pixel/observation space; training objectives vary (pixel reconstruction, next-latent prediction, value-consistent losses).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "latent space model",
            "model_description": "Encoder phi maps observations to continuous latents; separate networks predict reward and next-latent distribution; losses can be defined on next-latent (as in DeepMDP) or on reconstructing/predicting pixels. Models may be deterministic or probabilistic, and can use metrics like MSE, KL, or Wasserstein on predicted distributions.",
            "model_type": "latent world model",
            "task_domain": "Reinforcement learning, visual-control domains (Atari, synthetic environments)",
            "fidelity_metric": "Depends on instantiation: pixel MSE or NLL for observation-based models; distributional metrics (KL, TV, Wasserstein, MMD) for latent distributions; reward L1 error.",
            "fidelity_performance": "No single number â€” performance depends on loss choice and architecture; the paper reports that latent next-state prediction objectives (DeepMDP-style) recover underlying low-dimensional structure in DonutWorld and yield better downstream RL performance than pixel reconstruction in Atari.",
            "interpretability_assessment": "Latent models can be interpretable if latents align with true state factors (as in DonutWorld), but pixel-prediction objectives may force encoding of irrelevant perceptual details.",
            "interpretability_method": "Latent visualization, heatmaps, measuring invariance to irrelevant visual factors, bisimulation analysis.",
            "computational_cost": "Lower than full image generative models when latent dimension is small, but cost depends on the chosen distribution metric (Wasserstein expensive; KL cheaper).",
            "efficiency_comparison": "Latent modeling typically more efficient for planning/representation than pixel prediction since it avoids high-dimensional reconstruction; DeepMDP-style latent objectives outperform pixel-based auxiliary tasks empirically as representation learners in this paper.",
            "task_performance": "Empirically, latent next-latent objectives aided RL on Atari more than observation-reconstruction or next-observation objectives in this work.",
            "task_utility_analysis": "Learning to predict next latent states and rewards focuses capacity on task-relevant dynamics, improving downstream value/policy learning versus full observation prediction which preserves irrelevant details.",
            "tradeoffs_observed": "Latent compression improves efficiency but may lose irrelevant details needed for other objectives; metric choice governs representation smoothness and theoretical guarantees; choosing latent dimension and transition architecture impacts both interpretability and fidelity.",
            "design_choices": "Choice of metric (Wasserstein vs MMD vs KL), latent dimensionality, deterministic vs probabilistic transitions, architectural inductive biases (convolutional vs FC), local vs global loss weighting.",
            "comparison_to_alternatives": "Compared to pixel predictors, latent predictors (DeepMDP) provided better task-relevant representations and improved RL performance in the experiments; compared to KL/NLL training, Wasserstein yields stronger representation/bisimulation guarantees though at higher computational cost.",
            "optimal_configuration": "Paper suggests: minimize reward + next-latent distributional loss (Wasserstein when bisimulation guarantees are desired); pick architecture with suitable inductive bias for domain (conv nets for images); use local losses under on-policy or replay distributions for scalability.",
            "uuid": "e1406.1",
            "source_info": {
                "paper_title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "Recurrent World Models (Ha & Schmidhuber)",
            "brief_description": "A neural world-model architecture that learns a compact latent representation (VAE) of observations and a recurrent network (MDN-RNN) to predict future latents; used for sampling imagined rollouts and evolving policies in latent space.",
            "citation_title": "Recurrent world models facilitate policy evolution",
            "mention_or_use": "mention",
            "model_name": "World Models (VAE + RNN)",
            "model_description": "Encoder (VAE) compresses frames into latents; a recurrent predictive model models dynamics in latent space (often an MDN-RNN) and can be used to simulate trajectories for policy learning; reward/value components can be trained on latents or via imagination rollouts.",
            "model_type": "latent world model (VAE + recurrent predictive network)",
            "task_domain": "Visual reinforcement learning / simulated environments",
            "fidelity_metric": "Typically evaluated by reconstruction error (VAE), negative log-likelihood of predicted latents, and downstream policy performance when planning in imagined trajectories.",
            "fidelity_performance": "Not reported in this paper; mentioned as prior art that uses pixel/predictive losses to learn latent models.",
            "interpretability_assessment": "Latents sometimes correspond to semantically meaningful factors (depends on VAE training); the approach enables qualitative interpretability via latent traversals and simulated rollouts.",
            "interpretability_method": "Latent traversals, visualization of imagined rollouts, inspection of VAE latents.",
            "computational_cost": "Moderate; requires training VAE and RNN components; cost depends on architecture and rollout horizon. Not quantified here.",
            "efficiency_comparison": "Mentioned as prior approach; DeepMDP is framed as a theoretically grounded alternative focusing on reward and distributional latent losses rather than pixel reconstruction.",
            "task_performance": "Not measured in this paper; original work reported successful policy learning via imagined rollouts on toy environments.",
            "task_utility_analysis": "World Models are useful for generating imagined trajectories for policy search, but pixel/prediction losses can encourage encoding irrelevant visual information; DeepMDP argues for task-oriented latent objectives.",
            "tradeoffs_observed": "Pixel-focused latent models may encode irrelevant perceptual details, trading representation compactness and task utility vs faithful observation reconstruction.",
            "design_choices": "Use of VAE for compression, recurrent predictive model for dynamics, optional controller trained in latent space.",
            "comparison_to_alternatives": "World Models emphasize generative observation reconstruction; DeepMDP emphasizes reward-and-distributional latent objectives which the authors argue yield better task-relevant representations.",
            "optimal_configuration": "Not specified in this paper; cited as motivating prior work showing latent dynamics can be useful but lacking the theoretical value-function guarantees DeepMDP provides.",
            "uuid": "e1406.2",
            "source_info": {
                "paper_title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "VPN",
            "name_full": "Value Prediction Network (VPN)",
            "brief_description": "A model that predicts future abstract states and value-relevant quantities to perform planning in latent space; integrates model-based predictions with value estimates.",
            "citation_title": "Value Prediction Network",
            "mention_or_use": "mention",
            "model_name": "Value Prediction Network (VPN)",
            "model_description": "A neural architecture that learns abstract-state transitions and uses them to predict rewards and values for planning; training aligns transition predictions with value/logit targets rather than raw pixels.",
            "model_type": "latent world model / planning model",
            "task_domain": "Reinforcement learning with planning",
            "fidelity_metric": "VPN-style losses often measure how well transitions produce correct next-state value/logit predictions (e.g., cross-entropy or KL on predicted logits), not raw pixel fidelity.",
            "fidelity_performance": "No numbers in this paper; VPN-style auxiliary loss (next logits prediction) was implemented as an ablation and was found to hurt performance when used as an auxiliary task with a distributional RL agent.",
            "interpretability_assessment": "Interpretable to the extent that predicted abstract states are aligned to value-relevant features; not emphasized in this paper.",
            "interpretability_method": "Not specified here; generally via analysis of predicted value trajectories.",
            "computational_cost": "Comparable to other latent predictors; cost depends on the architecture of the transition and value heads.",
            "efficiency_comparison": "In this work VPN-style auxiliary losses underperformed the DeepMDP next-latent objective as an auxiliary task for C51.",
            "task_performance": "When used as an auxiliary objective in the paper's Atari experiments, VPN-style 'next logits' auxiliary loss degraded performance compared to the DeepMDP objective.",
            "task_utility_analysis": "VPN-style losses may be helpful when the learned model is used directly for planning, but as a mere auxiliary objective with distributional RL (C51) they prioritized signals incongruent with the distributional agent and hurt performance.",
            "tradeoffs_observed": "Predicting value/logits can bias representation to value-specific features but may not align with distributional learning objectives; hence auxiliary VPN-style loss can be detrimental in some setups.",
            "design_choices": "Training transition to predict next-state representations that produce correct Q/logit predictions (a distributional analogue of VPN was used as an ablation).",
            "comparison_to_alternatives": "DeepMDP next-latent prediction outperformed VPN-style next-logits prediction as an auxiliary task for the tested distributional RL agent.",
            "optimal_configuration": "Paper suggests VPN-style losses are more appropriate when the learned model will be used for planning; they are not recommended as blind auxiliary losses for distributional agents in place of next-latent objectives.",
            "uuid": "e1406.3",
            "source_info": {
                "paper_title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Pixel/predictive models (obs recon / next obs)",
            "name_full": "Observation-reconstruction and next-observation prediction models",
            "brief_description": "World-model variants trained to reconstruct the current observation or predict the next observation (pixels) from latents; common in literature but can force encodings of perceptual details irrelevant to task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "observation reconstruction / next-observation predictor",
            "model_description": "Encoder-decoder (autoencoder) or predictor network mapping latents to pixel reconstructions (current or next frame); loss typically pixel-wise MSE or NLL over pixels; sometimes combined with recurrent latent dynamics.",
            "model_type": "explicit observation-predictive model / latent+pixel generator",
            "task_domain": "Visual reinforcement learning (Atari, simulated environments)",
            "fidelity_metric": "Reconstruction loss (MSE or NLL) on pixels; potentially perceptual metrics if used.",
            "fidelity_performance": "Not reported numerically in this paper; autoencoders solved pixel reconstruction in DonutWorld but their learned latents were less task-useful than DeepMDP latents (qualitative).",
            "interpretability_assessment": "Low for task utility: tends to encode visual details (colors, backgrounds) that are irrelevant to dynamics or reward; latent axes may not correspond to underlying task state.",
            "interpretability_method": "Visual inspection of reconstructions and latent heatmaps; comparison to DeepMDP latent maps.",
            "computational_cost": "Typically heavier (decoder and pixel loss) than pure latent predictors; cost scales with image size and decoder complexity.",
            "efficiency_comparison": "In experiments, using observation reconstruction as an auxiliary objective was worse for downstream RL performance than DeepMDP next-latent objective.",
            "task_performance": "Autoencoder latent representations were less effective for predicting value or supporting RL than DeepMDP latents in DonutWorld and Atari auxiliary-task comparisons.",
            "task_utility_analysis": "High-fidelity pixel reconstruction does not necessarily yield task-useful representations; it can distract model capacity into encoding irrelevant perceptual features.",
            "tradeoffs_observed": "Pixel fidelity vs task utility: better pixel fidelity may reduce task performance due to encoding irrelevant details; computational cost higher for pixel losses.",
            "design_choices": "Use reconstruction / next-frame predictor as auxiliary losses; architectures include decoders and pixel-wise losses; in this paper these were used as ablations.",
            "comparison_to_alternatives": "DeepMDP next-latent prediction outperformed pixel reconstruction and next-observation prediction as auxiliary tasks for the tested RL agent.",
            "optimal_configuration": "When goal is representation for control/value estimation, paper recommends next-latent/reward objectives (DeepMDP) over pixel reconstruction; pixel models may be preferable when accurate image generation is required for planning.",
            "uuid": "e1406.4",
            "source_info": {
                "paper_title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "KL / NLL next-state models",
            "name_full": "Negative log-likelihood / KL-trained next-state models (probabilistic dynamics models)",
            "brief_description": "Probabilistic dynamics models trained by maximizing next-state likelihood (equivalently minimizing negative log-probability or KL to the true next-state distribution); widely used in model-based deep RL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "KL / NLL next-state model",
            "model_description": "Models (probabilistic or deterministic) that predict next observations/states and are trained via a likelihood objective (negative log-likelihood) which is a one-sample estimate of KL divergence between model and true next-state distribution; often paired with reward models.",
            "model_type": "probabilistic next-state model / model-based RL component",
            "task_domain": "Model-based reinforcement learning across various control tasks (Atari, continuous control)",
            "fidelity_metric": "Negative log-likelihood (NLL) on next states; KL divergence estimated via NLL; Pinsker's inequality relates KL to Total Variation.",
            "fidelity_performance": "No numerical fidelity reported here; paper notes that minimizing NLL/KL can be interpreted as training a DeepMDP (via Pinsker) and therefore obeys local value-difference bounds.",
            "interpretability_assessment": "Black-box neural density estimators in general; interpretability depends on model class (e.g., mixture density networks yield interpretable modes).",
            "interpretability_method": "Not specified in this paper; typically examine predicted distributions or sample rollouts.",
            "computational_cost": "Usually cheaper than direct Wasserstein minimization, but depends on output parametric form and sampling; commonly used since NLL/KL losses are straightforward with SGD.",
            "efficiency_comparison": "More commonly used in practice due to computational convenience; paper frames them as special cases of DeepMDP training (via Pinsker linking KL-&gt;TV) with corresponding local value guarantees.",
            "task_performance": "Used in many model-based RL works cited (Chua et al., Hafner et al., Buesing et al., etc.) with demonstrated sample-efficiency gains in some domains; specifics are in the cited literature rather than in this paper.",
            "task_utility_analysis": "Minimizing KL/NLL trains models that lower local model loss and therefore reduce value-function error per the DeepMDP framework, but may not provide bisimulation-based representation guarantees that Wasserstein gives.",
            "tradeoffs_observed": "KL/NLL is computationally convenient but gives weaker representational guarantees (w.r.t. bisimulation) than Wasserstein; choice reflects trade-off between tractability and theoretical representation strength.",
            "design_choices": "Parametric conditional density models for next-state predictions (e.g., Gaussians, mixture models); optimize NLL; used as in several cited model-based RL approaches.",
            "comparison_to_alternatives": "Paper notes KL/NLL training can be interpreted within DeepMDP theory (via Pinsker), but suggests exploring Norm-MMD metrics (including TV, Energy distance) as alternatives to balance computability and representation guarantees.",
            "optimal_configuration": "Paper suggests that KL/NLL is practical and compatible with DeepMDP local bounds, but for representation learning Wasserstein may be preferred when bisimulation-style invariance is important; Norm-MMD family offers intermediate choices.",
            "uuid": "e1406.5",
            "source_info": {
                "paper_title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 2
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Value Prediction Network",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 2
        },
        {
            "paper_title": "Model-based reinforcement learning for atari",
            "rating": 1
        }
    ],
    "cost": 0.023099750000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DeepMDP: Learning Continuous Latent Space Models for Representation Learning</h1>
<p>Carles Gelada ${ }^{1}$ Saurabh Kumar ${ }^{1}$ Jacob Buckman ${ }^{2}$ Ofir Nachum ${ }^{1}$ Marc G. Bellemare ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.</p>
<h2>1. Introduction</h2>
<p>In reinforcement learning (RL), it is typical to model the environment as a Markov Decision Process (MDP). However, for many practical tasks, the state representations of these MDPs include a large amount of redundant information and task-irrelevant noise. For example, image observations from the Arcade Learning Environment (Bellemare et al., 2013) consist of 33,600-dimensional pixel arrays, yet it is intuitively clear that there exist lower-dimensional approximate representations for all games. Consider PONG; observing only the positions and velocities of the three objects in the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>frame is enough to play. Converting each frame into such a simplified state before learning a policy facilitates the learning process by reducing the redundant and irrelevant information presented to the agent. Representation learning techniques for reinforcement learning seek to improve the learning efficiency of existing RL algorithms by doing exactly this: learning a mapping from states to simplified states.</p>
<p>Prior work on representation learning, such as state aggregation with bisimulation metrics (Givan et al., 2003; Ferns et al., 2004; 2011) or feature discovery algorithms (Comanici \&amp; Precup, 2011; Mahadevan \&amp; Maggioni, 2007; Bellemare et al., 2019), has resulted in algorithms with good theoretical properties; however, these algorithms do not scale to large scale problems or are not easily combined with deep learning. On the other hand, many recentlyproposed approaches to representation learning via deep learning have strong empirical results on complex domains, but lack formal guarantees (Jaderberg et al., 2016; van den Oord et al., 2018; Fedus et al., 2019). In this work, we propose an approach to representation learning that unifies the desirable aspects of both of these categories: a deep-learning-friendly approach with theoretical guarantees.</p>
<p>We describe the DeepMDP, a latent space model of an MDP which has been trained to minimize two tractable losses: predicting the rewards and predicting the distribution of next latent states. DeepMDPs can be viewed as a formalization of recent works which use neural networks to learn latent space models of the environment (Ha \&amp; Schmidhuber, 2018; Oh et al., 2017; Hafner et al., 2018; Francois-Lavet et al., 2018), because the value functions in the DeepMDP are guaranteed to be good approximations of value functions in the original task MDP. To provide this guarantee, careful consideration of the metric between distribution is necessary. A novel analysis of Maximum Mean Discrepancy (MMD) metrics (Gretton et al., 2012) defined via a function norm allows us to provide such guarantees; this includes the Total Variation, the Wasserstein and Energy metrics. These results represent a promising first step towards principled latentspace model-based RL algorithms.</p>
<p>From the perspective of representation learning, the state of a DeepMDP can be interpreted as a representation of the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Diagram of the latent space losses. Circles denote a distribution.
original MDP's state. When the Wasserstein metric is used for the latent transition loss, analysis reveals a profound theoretical connection between DeepMDPs and bisimulation. These results provide a theoretically-grounded approach to representation learning that is salable and compatible with modern deep networks.</p>
<p>In Section 2, we review key concepts and formally define the DeepMDP. We start by studying the model-quality and representation-quality results of DeepMDPs (using the Wasserstein metric) in Sections 3 and 4. In Section 5, we investigate the connection between DeepMDPs using the Wasserstein and bisimulation. Section 6 generalizes only our model-based guarantees to metrics other than the Wasserstein; this limitation emphasizes the special role of that the Wasserstein metric plays in learning good representations. Finally, in Section 8 we consider a synthetic environment with high-dimensional observations and show that a DeepMDP learns to recover its underlying low-dimensional latent structure. We then demonstrate that learning a DeepMDP as an auxiliary task to model-free RL in the Atari 2600 environment leads to significant improvement in performance when compared to a baseline model-free method.</p>
<h2>2. Background</h2>
<h3>2.1. Markov Decision Processes</h3>
<p>Define a Markov Decision Process (MDP) in standard fashion: $\mathcal{M}=\langle\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma\rangle$ (Puterman, 1994). For simplicity of notation we will assume that $\mathcal{S}$ and $\mathcal{A}$ are discrete spaces unless otherwise stated. A policy $\pi$ defines a distribution over actions conditioned on the state, $\pi(a \mid s)$. Denote
by $\Pi$ the set of all stationary policies. The value function of a policy $\pi \in \Pi$ at a state $s$ is the expected sum of future discounted rewards by running the policy from that state. $V^{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ is defined as:</p>
<p>$$
V^{\pi}(s)=\underset{\substack{a_{t} \sim \pi\left(\cdot \mid s_{t}\right) \ s_{t+1} \sim \mathcal{P}\left(\cdot \mid s_{t}, a_{t}\right)}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t} \mathcal{R}\left(s_{t}, a_{t}\right) \mid s_{0}=s\right]
$$</p>
<p>The action value function is similarly defined:</p>
<p>$$
Q^{\pi}(s, a)=\underset{\substack{a_{t} \sim \pi\left(\cdot \mid s_{t}\right) \ s_{t+1} \sim \mathcal{P}\left(\cdot \mid s_{t}, a_{t}\right)}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t} \mathcal{R}\left(s_{t}, a_{t}\right) \mid s_{0}=s, a_{0}=a\right]
$$</p>
<p>We denote by $\mathcal{P}<em _bar_pi="\bar{\pi">{\pi}$ the action-independent transition function induced by running a policy $\pi$, $\mathcal{P}</em>}}\left(s^{\prime} \mid s\right)=\sum_{a \in \mathcal{A}} \mathcal{P}\left(s^{\prime} \mid s, a\right) \pi(a \mid s) . \quad$ Similarly $\mathcal{R<em _in="\in" _mathcal_A="\mathcal{A" a="a">{\bar{\pi}}(s)=\sum</em>(s, a) \pi(a \mid s)$. We denote $\pi^{}} \mathcal{R<em>}$ as the optimal policy in $\mathcal{M}$; i.e., the policy which maximizes expected future reward. We denote the optimal state and action value functions with respect to $\pi^{</em>}$ as $V^{<em>}, Q^{</em>}$. We denote the stationary distribution of a policy $\pi$ in $\mathcal{M}$ by $\xi_{\pi}$; i.e.,</p>
<p>$$
\xi_{\pi}(s)=\sum_{\dot{s} \in \mathcal{S}, \dot{a} \in \mathcal{A}} \mathcal{P}(s \mid \dot{s}, \dot{a}) \pi(\dot{a} \mid \dot{s}) \xi_{\pi}(\dot{s})
$$</p>
<p>We overload notation by also denoting the state-action stationary distribution as $\xi_{\pi}(s, a)=\xi_{\pi}(s) \pi(a \mid s)$. Although only non-terminating MDPs have stationary distributions, a state distribution for terminating MDPs with similar properties exists (Gelada \&amp; Bellemare, 2019).</p>
<h3>2.2. Latent Space Models</h3>
<p>For some MDP $\mathcal{M}$, let $\overline{\mathcal{M}}=\langle\overline{\mathcal{S}}, \mathcal{A}, \overline{\mathcal{R}}, \overline{\mathcal{P}}, \gamma\rangle$ be an MDP where $\overline{\mathcal{S}}$ is a continuous space with metric $d_{\overline{\mathcal{S}}}$ and a shared action space $\mathcal{A}$ between $\mathcal{M}$ and $\overline{\mathcal{M}}$. Furthermore, let $\phi$ : $\mathcal{S} \rightarrow \overline{\mathcal{S}}$ be an embedding function which connects the state spaces of these two MDPs. We refer to $(\overline{\mathcal{M}}, \phi)$ as a latent space model of $\mathcal{M}$.</p>
<p>Since $\overline{\mathcal{M}}$ is, by definition, an MDP, value functions can be defined in the standard way. We use $\bar{V}^{\bar{\pi}}, \bar{Q}^{\bar{\pi}}$ to denote the value functions of a policy $\bar{\pi} \in \bar{\Pi}$, where $\bar{\Pi}$ is the set of policies defined on the state space $\overline{\mathcal{S}}$. The transition and reward functions, $\overline{\mathcal{R}}<em _bar_pi="\bar{\pi">{\bar{\pi}}$ and $\overline{\mathcal{P}}</em>^{}}$, of a policy $\bar{\pi}$ are also defined in the standard manner. We use $\bar{\pi<em>}$ to denote the optimal policy in $\overline{\mathcal{M}}$. The corresponding optimal state and action value functions are then $\bar{V}^{</em>}, \bar{Q}^{*}$. For ease of notation, when $s \in \mathcal{S}$, we use $\bar{\pi}(\cdot \mid s):=\bar{\pi}(\cdot \mid \phi(s))$ to denote first using $\phi$ to map $s$ to the state space $\overline{\mathcal{S}}$ of $\overline{\mathcal{M}}$ and subsequently using $\bar{\pi}$ to generate the probability distribution over actions.</p>
<p>Although similar definitions of latent space models have been previously studied (Francois-Lavet et al., 2018; Zhang et al., 2018; Ha \&amp; Schmidhuber, 2018; Oh et al., 2017;</p>
<p>Hafner et al., 2018; Kaiser et al., 2019; Silver et al., 2017), the parametrizations and training objectives used to learn such models have varied widely. For example Ha \&amp; Schmidhuber (2018); Hafner et al. (2018); Kaiser et al. (2019) use pixel prediction losses to learn the latent representation while (Oh et al., 2017) chooses instead to optimize the model to predict next latent states with the same value function as the sampled next states.</p>
<p>In this work, we study the minimization of loss functions defined with respect to rewards and transitions in the latent space:</p>
<p>$$
\begin{aligned}
L_{\bar{\mathcal{R}}}(s, a) &amp; =|\mathcal{R}(s, a)-\overline{\mathcal{R}}(\phi(s), a)| \
L_{\bar{\mathcal{P}}}(s, a) &amp; =\mathcal{D}(\phi \mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}}(\cdot \mid \phi(s), a))
\end{aligned}
$$</p>
<p>where we use the shorthand notation $\phi \mathcal{P}(\cdot \mid s, a)$ to denote the probability distribution over $\overline{\mathcal{S}}$ of first sampling $s^{\prime} \sim$ $\mathcal{P}(\cdot \mid s, a)$ and then embedding $\bar{s}^{\prime}=\phi\left(s^{\prime}\right)$, and where $\mathcal{D}$ is a metric between probability distributions. To provide guarantees, $\mathcal{D}$ in Equation 2 needs to be chosen carefully. For the majority of this work, we focus on the Wasserstein metric; in Section 6, we generalize some of the results to alternative metrics from the Maximum Mean Discrepancy family. Francois-Lavet et al. (2018) and Chung et al. (2019) have considered similar latent losses, but to the best of our knowledge ours is the first theoretical analysis of these models. See Figure 1 for an illustration of how the latent space losses are constructed.</p>
<p>We use the term DeepMDP to refer to a parameterized latent space model trained via the minimization of losses consisting of $L_{\bar{\mathcal{R}}}$ and $L_{\bar{\mathcal{P}}}$ (sometimes referred to as DeepMDP losses). In Section 3, we derive theoretical guarantees of DeepMDPs when minimizing $L_{\bar{\mathcal{R}}}$ and $L_{\bar{\mathcal{P}}}$ over the whole state space. However, our principal objective is to learn DeepMDPs parameterized by deep networks, which requires DeepMDP losses in the form of expectations; we show in Section 4 that similar theoretical guarantees can be obtained in this setting.</p>
<h3>2.3. Wasserstein Metric</h3>
<p>Initially studied in the optimal transport literature (Villani, 2008), the Wasserstein-1 (which we simply refer to as the Wasserstein) metric $W_{d}(P, Q)$ between two distributions $P$ and $Q$, defined on a space with metric $d$, corresponds to the minimum cost of transforming $P$ into $Q$, where the cost of moving a particle at point $x$ to point $y$ comes from the underlying metric $d(x, y)$.
Definition 1. The Wasserstein-1 metric $W$ between distributions $P$ and $Q$ on a metric space $\langle\chi, d\rangle$ is:</p>
<p>$$
W_{d}(P, Q)=\inf <em _chi="\chi" _times="\times">{\lambda \in \Gamma(P, Q)} \int</em> d(x, y) \lambda(x, y) d x d y
$$</p>
<p>where $\Gamma(P, Q)$ denotes the set of all couplings of $P$ and $Q$.</p>
<p>When there is no ambiguity on what the underlying metric $d$ is, we will simply write $W$. The Monge-Kantorovich duality (Mueller, 1997) shows that the Wasserstein has a dual form:</p>
<p>$$
W_{d}(P, Q)=\sup <em d="d">{f \in \mathcal{F}</em> f(y)\right|
$$}}\left|\underset{x \sim P}{\mathbb{E}} f(x)-\underset{y \sim Q}{\mathbb{E}</p>
<p>where $\mathcal{F}<em d="d">{d}$ is the set of 1-Lipschitz functions under the metric $d, \mathcal{F}</em>={f:|f(x)-f(y)| \leqslant d(x, y)}$.</p>
<h3>2.4. Lipschitz Norm of Value Functions</h3>
<p>The degree to which a value function of $\overline{\mathcal{M}}, \bar{V}^{\pi}$ approximates the value function $V^{\pi}$ of $\mathcal{M}$ will depend on the Lipschitz norm of $\bar{V}^{\pi}$. In this section we define and provide conditions for value functions to be Lipschitz. ${ }^{1}$ Note that we study the Lipschitz properties of DeepMDPs $\overline{\mathcal{M}}$ (instead of a MDP $\mathcal{M}$ ) because in this work, only the Lipschiz properties of DeepMDPs are relevant; the reader should note that these results follow for any continuous MDP with a metric state space.</p>
<p>We say a policy $\bar{\pi} \in \bar{\Pi}$ is Lipschitz-valued if its value function is Lipschitz, i.e. it has Lipschitz $\bar{Q}^{\bar{\pi}}$ and $\bar{V}^{\bar{\pi}}$ functions.</p>
<p>Definition 2. Let $\overline{\mathcal{M}}$ be a DeepMDP with a metric $d_{\overline{\mathcal{S}}}$. A policy $\bar{\pi} \in \bar{\Pi}$ is $K_{\bar{V}}$-Lipschitz-valued if for all $\bar{s}<em 2="2">{1}, \bar{s}</em>$ :} \in \overline{\mathcal{S}</p>
<p>$$
\left|\bar{V}^{\bar{\pi}}\left(\bar{s}<em 2="2">{1}\right)-\bar{V}^{\bar{\pi}}\left(\bar{s}</em>}\right)\right| \leqslant K_{\bar{V}} d_{\bar{\mathcal{S}}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>\right)
$$</p>
<p>and if for all $a \in \mathcal{A}$ :</p>
<p>$$
\left|\bar{Q}^{\bar{\pi}}\left(\bar{s}<em 2="2">{1}, a\right)-\bar{Q}^{\bar{\pi}}\left(\bar{s}</em>}, a\right)\right| \leqslant K_{\bar{V}} d_{\bar{\mathcal{S}}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>\right)
$$</p>
<p>Several works have studied Lipschitz norm constraints on the transition and reward functions (Hinderer, 2005; Asadi et al., 2018) to provide conditions for value functions to be Lipschitz. Closely following their formulation, we define Lipschitz DeepMDPs as follows:
Definition 3. Let $\overline{\mathcal{M}}$ be a DeepMDP with a metric $d_{\overline{\mathcal{S}}}$. We say $\overline{\mathcal{M}}$ is $\left(K_{\overline{\mathcal{R}}}, K_{\bar{\mathcal{P}}}\right)$-Lipschitz if, for all $\bar{s}<em 2="2">{1}, \bar{s}</em>$ :} \in \overline{\mathcal{S}}$ and $a \in \mathcal{A</p>
<p>$$
\begin{aligned}
&amp; \left|\overline{\mathcal{R}}\left(\bar{s}<em 2="2">{1}, a\right)-\overline{\mathcal{R}}\left(\bar{s}</em>}, a\right)\right| \leqslant K_{\overline{\mathcal{R}}} d_{\overline{\mathcal{S}}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>\right) \
&amp; W\left(\overline{\mathcal{P}}(\cdot \mid, \bar{s}<em 2="2">{1}, a), \overline{\mathcal{P}}(\cdot \mid \bar{s}</em>}, a)\right) \leqslant K_{\bar{\mathcal{P}}} d_{\bar{\mathcal{S}}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>\right)
\end{aligned}
$$</p>
<p>From here onwards, we will we restrict our attention to the set of Lipschitz DeepMDPs for which the constant $K_{\bar{\mathcal{P}}}$ is sufficiently small, formalized in the following assumption:
Assumption 1. The Lipschitz constant $K_{\bar{P}}$ of the transition function $\overline{\mathcal{P}}$ is strictly smaller than $\frac{1}{\gamma}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>From a practical standpoint, Assumption 1 is relatively strong, but simplifies our analysis by ensuring that close states cannot have future trajectories that are "divergent." An MDP might still not exhibit divergent behaviour even when $K_{\bar{\mathcal{P}}} \geq \frac{1}{\gamma}$. In particular, when episodes terminate after a finite amount of time, Assumption 1 becomes unnecessary. We leave as future work how to improve on this assumption.</p>
<p>We describe a small set of Lipschitz-valued policies. For any policy $\bar{\pi} \in \bar{\Pi}$, we refer to the Lipschitz norm of its transition function $\overline{\mathcal{P}}<em _mathcal_P="\mathcal{P">{\bar{\pi}}$ as $K</em><em _bar_pi="\bar{\pi">{\bar{\pi}}} \geqslant W\left(\overline{\mathcal{P}}</em>}}\left(\cdot \mid \bar{s<em _bar_pi="\bar{\pi">{1}\right), \overline{\mathcal{P}}</em>}}\left(\cdot \mid \bar{s<em 1="1">{2}\right)\right)$ for all $\bar{s}</em>}, \bar{s<em _overline_mathcal_R="\overline{\mathcal{R">{2} \in \mathcal{S}$. Similarly, we denote the Lipschitz norm of the reward function as $K</em>}<em _bar_pi="\bar{\pi">{\bar{\pi}}} \geqslant\left|\overline{\mathcal{R}}</em>}}\left(\bar{s<em _bar_pi="\bar{\pi">{1}\right)-\overline{\mathcal{R}}</em>}}\left(\bar{s<em _overline_mathcal_R="\overline{\mathcal{R">{2}\right)\right|$.
Lemma 1. Let $\overline{\mathcal{M}}$ be $\left(K</em>\right)$-Lipschitz. Then,}}}, K_{\bar{\mathcal{P}}</p>
<ol>
<li>The optimal policy $\bar{\pi}^{*}$ is $\frac{K_{\overline{\mathcal{R}}}}{1-\gamma K_{\bar{\mathcal{P}}}}$-Lipschitz-valued.</li>
<li>All policies with $K_{\overline{\mathcal{P}}<em _overline_mathcal_R="\overline{\mathcal{R">{\bar{\pi}}} \leqslant \frac{1}{\gamma}$ are $\frac{K</em>}<em _bar_mathcal_P="\bar{\mathcal{P">{\bar{\pi}}}}{1-\gamma K</em>$-Lipschitzvalued.}}_{\bar{\pi}}}</li>
<li>All constant policies (i.e. $\bar{\pi}\left(a \mid \bar{s}<em 2="2">{1}\right)=\bar{\pi}\left(a \mid \bar{s}</em>}\right), \forall a \in$ $\left.\mathcal{A}, \bar{s<em 2="2">{1}, \bar{s}</em>$-Lipschitz-valued.} \in \overline{\mathcal{S}}\right)$ are $\frac{K_{\overline{\mathcal{R}}}}{1-\gamma K_{\bar{\mathcal{P}}}</li>
</ol>
<p>Proof. See Appendix A for all proofs.</p>
<p>A more general framework for understanding Lipschitz value functions is still lacking. Little prior work studying classes of Lipschitz-valued policies exists in the literature and we believe that this is an important direction for future research.</p>
<h2>3. Global DeepMDP Bounds</h2>
<p>We now present our first main contributions: concrete DeepMDP losses, and several bounds which provide us with useful guarantees when these losses are minimized. We refer to these losses as the global DeepMDP losses, to emphasize their dependence on the whole state and action space: ${ }^{2}$</p>
<p>$$
\begin{aligned}
L_{\overline{\mathcal{R}}}^{\infty} &amp; =\sup <em _bar_mathcal_P="\bar{\mathcal{P">{s \in \mathcal{S}, a \in \mathcal{A}}|\mathcal{R}(s, a)-\overline{\mathcal{R}}(\phi(s), a)| \
L</em>(\cdot \mid \phi(s), a))
\end{aligned}
$$}}}^{\infty} &amp; =\sup _{s \in \mathcal{S}, a \in \mathcal{A}} W(\phi \mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}</p>
<h3>3.1. Value Difference Bound</h3>
<p>We start by bounding the difference of the value functions $Q^{\bar{\pi}}$ and $\bar{Q}^{\bar{\pi}}$ for any policy $\bar{\pi} \in \bar{\Pi}$. Note that $Q^{\bar{\pi}}(s, a)$ is computed using $\mathcal{P}$ and $\mathcal{R}$ on $\mathcal{S}$ while $\bar{Q}^{\bar{\pi}}(\phi(s), a)$ is computed using $\overline{\mathcal{P}}$ and $\overline{\mathcal{R}}$ on $\overline{\mathcal{S}}$.
Lemma 2. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, with an embedding function $\phi$ and global loss</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>functions $L_{\overline{\mathcal{R}}}^{\infty}$ and $L_{\bar{\mathcal{P}}}^{\infty}$. For any $K_{\bar{V}}$-Lipschitz-valued policy $\bar{\pi} \in \bar{\Pi}$ the value difference can be bounded by</p>
<p>$$
\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right| \leqslant \frac{L_{\overline{\mathcal{R}}}^{\infty}+\gamma K_{\bar{V}} L_{\bar{\mathcal{P}}}^{\infty}}{1-\gamma}
$$</p>
<p>The previous result holds for all policies $\bar{\Pi} \subseteq \Pi$, a subset of all possible policies $\Pi$. The reader might ask whether this is an interesting set of policies to consider; in Section 5, we answer with a fat "yes" by characterizing this set via a connection with bisimulation.</p>
<p>A bound similar to Lemma 2 can be found in Asadi et al. (2018), who study non-latent transition models using the Wasserstein metric when there is access to an exact reward function. We also note that our results are arguably simpler, since we do not require the treatment of MDP transitions in terms of distributions over a set of deterministic components.</p>
<h3>3.2. Representation Quality Bound</h3>
<p>When a representation is used to predict the value of a policy in $\mathcal{M}$, a clear failure case is when two states with different values are collapsed to the same representation. The following result demonstrates that when the global DeepMDP losses $L_{\overline{\mathcal{R}}}^{\infty}=0$ and $L_{\bar{\mathcal{P}}}^{\infty}=0$, this failure case can never occur for the embedding function $\phi$.</p>
<p>Theorem 1. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, let $d_{\overline{\mathcal{S}}}$ be a metric in $\overline{\mathcal{S}}, \phi$ be an embedding function and $L_{\overline{\mathcal{R}}}^{\infty}$ and $L_{\bar{\mathcal{P}}}^{\infty}$ be the global loss functions. For any $K_{\bar{V}}$-Lipschitz-valued policy $\bar{\pi} \in \bar{\Pi}$ the representation $\phi$ guarantees that for all $s_{1}, s_{2} \in \mathcal{S}$ and $a \in \mathcal{A}$,</p>
<p>$$
\begin{aligned}
&amp; \left|Q^{\bar{\pi}}\left(s_{1}, a\right)-Q^{\bar{\pi}}\left(s_{2}, a\right)\right| \leqslant K_{\bar{V}} d_{\overline{\mathcal{S}}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
&amp; +2 \frac{L_{\overline{\mathcal{R}}}^{\infty}+\gamma K_{\bar{V}} L_{\bar{\mathcal{P}}}^{\infty}}{1-\gamma}
\end{aligned}
$$</p>
<p>This result justifies learning a DeepMDP and using the embedding function $\phi$ as a representation to predict values. A similar connection between the quality of representations and model based objectives in the linear setting was made by Parr et al. (2008).</p>
<h3>3.3. Suboptimality Bound</h3>
<p>For completeness, we also bound the performance loss of running the optimal policy of $\overline{\mathcal{M}}$ in $\mathcal{M}$, compared to the optimal policy $\pi^{*}$. See Theorem 5 in Appendix A.</p>
<h2>4. Local DeepMDP Bounds</h2>
<p>In large-scale tasks, data from many regions of the state space is often unavailable, ${ }^{3}$ making it infeasible to measure - let alone optimize - the global losses. Further, when the capacity of a model is limited, or when sample efficiency is a concern, it might not even be desirable to precisely learn a model of the whole state space. Interestingly, we can still provide similar guarantees based on the DeepMDP losses, as measured under an expectation over a state-action distribution, denoted here as $\xi$. We refer to these as the losses local to $\xi$. Taking $L_{\mathcal{R}}^{\xi}, L_{\mathcal{P}}^{\xi}$ to be the reward and transition losses under $\xi$, respectively, we have the following local DeepMDP losses:</p>
<p>$$
\begin{aligned}
&amp; L_{\mathcal{R}}^{\xi}=\underset{s, a \sim \xi}{\mathbb{E}}|\mathcal{R}(s, a)-\overline{\mathcal{R}}(\phi(s), a)| \
&amp; L_{\mathcal{P}}^{\xi}=\underset{s, a \sim \xi}{\mathbb{E}}[W(\phi \mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}}(\cdot \mid \phi(s), a))]
\end{aligned}
$$</p>
<p>Losses of this form are compatible with the stochastic gradient decent methods used by neural networks. Thus, study of the local losses allows us to bridge the gap between theory and practice.</p>
<h3>4.1. Value Difference Bound</h3>
<p>We provide a value function bound for the local case, analogous to Lemma 2.</p>
<p>Lemma 3. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, with an embedding function $\phi$. For any $K_{\bar{V}^{-}}$ Lipschitz-valued policy $\bar{\pi} \in \bar{\Pi}$, the expected value function difference can be bounded using the local loss functions $L_{\mathcal{R}}^{\xi_{\pi}}$ and $L_{\mathcal{P}}^{\xi_{\pi}}$ measured under $\xi_{\pi}$, the stationary state action distribution of $\bar{\pi}$.</p>
<p>$$
\underset{s, a \sim \xi_{\pi}}{\mathbb{E}}\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right| \leqslant \frac{L_{\mathcal{R}}^{\xi_{\pi}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\xi_{\pi}}}{1-\gamma}
$$</p>
<p>The provided bound guarantees that for any policy $\bar{\pi} \in \bar{\Pi}$ which visits state-action pairs $(s, a)$ where $L_{\mathcal{R}}(s, a)$ and $L_{\overline{\mathcal{P}}}(s, a)$ are small, the DeepMDP will provide accurate value functions for any states likely to be seen under the policy. ${ }^{4}$</p>
<h3>4.2. Representation Quality Bound</h3>
<p>We can also extend the local value difference bound to provide a local bound on how well the representation $\phi$ can be used to predict the value function of a policy $\bar{\pi} \in \bar{\Pi}$, analogous to Theorem 1.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A pair of bisimilar states. In the game of ASTEROIDS, the colors of the asteroids can vary randomly, but this in no way impacts gameplay.</p>
<p>Theorem 2. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, let $d_{\bar{\beta}}$ be the metric in $\overline{\mathcal{S}}$ and $\phi$ be the embedding function. Let $\bar{\pi} \in \bar{\Pi}$ be any $K_{\bar{V}}$-Lipschitz-valued policy with stationary distribution $\xi_{\bar{\pi}}$, and let $L_{\mathcal{R}}^{\xi_{\bar{\pi}}}$ and $L_{\mathcal{P}}^{\xi_{\bar{\pi}}}$ be the local loss functions. For any two states $s_{1}, s_{2} \in \mathcal{S}$, the representation $\phi$ is such that,</p>
<p>$$
\begin{aligned}
&amp; \left|V^{\bar{\pi}}\left(s_{1}\right)-V^{\bar{\pi}}\left(s_{2}\right)\right| \leqslant K_{\bar{V}} d_{\bar{\beta}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
&amp; \quad+\frac{L_{\mathcal{R}}^{\xi_{\bar{\pi}}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\xi_{\bar{\pi}}}}{1-\gamma}\left(\frac{1}{d_{\bar{\pi}}\left(s_{1}\right)}+\frac{1}{d_{\bar{\pi}}\left(s_{2}\right)}\right)
\end{aligned}
$$</p>
<p>Thus, the representation quality argument given in 3.2 holds for any two states $s_{1}$ and $s_{2}$ which are visited often by a policy $\bar{\pi}$.</p>
<h2>5. Bisimulation</h2>
<h3>5.1. Bisimulation Relations</h3>
<p>Bisimulation relations in the context of RL (Givan et al., 2003), are a formalization of behavioural equivalence between states.
Definition 4 (Givan et al. (2003)). Given an MDP $\mathcal{M}$, an equivalence relation $B$ between states is a bisimulation relation if for all states $s_{1}, s_{2} \in \mathcal{S}$ that are equivalent under $B$ (i.e. $s_{1} B s_{2}$ ), the following conditions hold for all actions $a \in \mathcal{A}$.</p>
<p>$$
\begin{aligned}
&amp; R\left(s_{1}, a\right)=R\left(s_{2}, a\right) \
&amp; \mathcal{P}\left(G \mid s_{1}, a\right)=\mathcal{P}\left(G \mid s_{2}, a\right), \forall G \in \mathcal{S} / B
\end{aligned}
$$</p>
<p>Where $\mathcal{S} / B$ denotes the partition of $\mathcal{S}$ under the relation $B$, the set of all groups of equivalent states, and where $\mathcal{P}(G \mid s, a)=\sum_{s^{\prime} \in G} \mathcal{P}\left(s^{\prime} \mid s, a\right)$.</p>
<p>Note that bisimulation relations are not unique. For example, the equality relation $=$ is always a bisimulation relation. Of particular interest is the maximal bisimulation relation $\sim$, which defines the partition $\mathcal{S} / \sim$ with the fewest elements (or equivalently, the relation that generates the largest possible groups of states). We will say that two states are bisimilar if they are equivalent under $\sim$. Essentially, two states are bisimilar if (1) they have the same immediate reward for all actions and (2) both of their distributions over next-states contain states which themselves are bisimilar. Figure 2 gives an example of states that are bisimilar in the Atari 2600 game ASTEROIDS. An important property of bisimulation relations is that any two bisimilar states $s_{1}, s_{2}$ must have the same optimal value function $Q^{<em>}\left(s_{1}, a\right)=Q^{</em>}\left(s_{2}, a\right), \forall a \in \mathcal{A}$. Bisimulation relations were first introduced for state aggregation (Givan et al., 2003), which is a form of representation learning, since merging behaviourally equivalent states does not result in the loss of information necessary for solving the MDP.</p>
<h3>5.2. Bisimulation Metrics</h3>
<p>A drawback of bisimulation relations is their all-or-nothing nature. Two states that are nearly identical, but differ slightly in their reward or transition functions, are treated as though they were just as unrelated as two states with nothing in common. Relying on the optimal transport perspective of the Wasserstein, Ferns et al. (2004) introduced bisimulation metrics, which are pseudometrics that quantify the behavioural similarity of two discrete states.</p>
<p>A pseudometric $d$ satisfies all the properties of a metric except identity of indiscernibles, $d(x, y)=0 \Leftrightarrow x=y$. A pseudometric can be used to define an equivalence relation by saying that two points are equivalent if they have zero distance; this is called the kernel of the pseudometric. Note that pseudometrics must obey the triangle inequality, which ensures the kernel satisfies the associative property. Without any changes to its definition, the Wasserstein metric can be extended to spaces $\langle\chi, d\rangle$, where $d$ is a pseudometric. Intuitively, the usage of a pseudometric in the Wasserstein can be interpreted as allowing different points $x_{1} \neq x_{2}$ in $\chi$ to be equivalent under the pseudometric (i.e. $d\left(x_{1}, x_{2}\right)=0$ ). Thus, there is no need for transportation from one to the other.</p>
<p>An extension of bisimulation metrics based on Banach fixed points by Ferns et al. (2011) which allows the metric to be defined for MDPs with discrete and continuous state spaces.</p>
<p>Definition 5 (Ferns et al. (2011)). Let $\mathcal{M}$ be an MDP and denote by $Z$ the space of pseudometrics on the space $\mathcal{S}$ s.t. $d\left(s_{1}, s_{2}\right) \in[0, \infty)$ for $d \in Z$. Define the operator $F: Z \rightarrow Z$ to be:</p>
<p>$$
\begin{aligned}
F_{d}\left(s_{1}, s_{2}\right)=\max <em 1="1">{a}(1- &amp; \left.\gamma)\left|\mathcal{R}\left(s</em>, a\right)\right|\right. \
&amp; \left.+\gamma W_{d}\left(\mathcal{P}\left(\cdot \mid s_{1}, a\right), \mathcal{P}\left(\cdot \mid s_{2}, a\right)\right)\right.
\end{aligned}
$$}, a\right)-\mathcal{R}\left(s_{2</p>
<p>Then:</p>
<ol>
<li>The operator $F$ is a contraction with a unique fixed point denoted by $\widetilde{d}$.</li>
<li>The kernel of $\widetilde{d}$ is the maximal bisimulation relation $\sim$. (i.e. $\widetilde{d}\left(s_{1}, s_{2}\right)=0 \Longleftrightarrow s_{1} \sim s_{2}$ )</li>
</ol>
<p>A useful property of bisimulation metrics is that the optimal value function difference between any two states can be upper bounded by the bisimulation metric between the two states.</p>
<p>$$
\left|V^{<em>}\left(s_{1}\right)-V^{</em>}\left(s_{2}\right)\right| \leqslant \frac{\widetilde{d}\left(s_{1}, s_{2}\right)}{1-\gamma}
$$</p>
<p>Bisimulation metrics have been used for state aggregation (Ferns et al., 2004; Ruan et al., 2015), feature discovery (Comanici \&amp; Precup, 2011) and transfer learning between MDPs (Castro \&amp; Precup, 2010), but due to their high computational cost and poor compatibility with deep networks they have not been successfully applied to large scale settings.</p>
<h3>5.3. Connection with DeepMDPs</h3>
<p>The representation $\phi$ learned by global DeepMDP losses with the Wasserstein metric can be connected to bisimulation metrics.
Theorem 3. Let $\mathcal{M}$ be an MDP and $\widetilde{\mathcal{M}}$ be a $K_{\mathcal{R}} \cdot K_{\mathcal{P}}$. Lipschitz DeepMDP with metric $d_{\mathcal{S}}$. Let $\phi$ be the embedding function and $L_{\mathcal{P}}^{\infty}$ and $L_{\mathcal{R}}^{\infty}$ be the global DeepMDP losses. The bisimulation distance in $\mathcal{M}, \widetilde{d}: \mathcal{S} \times \mathcal{S} \rightarrow \mathbb{R}^{+}$can be upperbounded by the $\ell_{2}$ distance in the embedding and the losses in the following way:</p>
<p>$$
\begin{aligned}
\widetilde{d}\left(s_{1}, s_{2}\right) \leqslant \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} &amp; d_{\mathcal{S}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
&amp; +2\left(L_{\mathcal{R}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}\right)
\end{aligned}
$$</p>
<p>This result provides a similar bound to Theorem 1, except that instead of bounding the value difference $\left|\tilde{V}^{<em>}\left(s_{1}\right)-\right.$ $\left.\tilde{V}^{</em>}\left(s_{2}\right)\right|$ the bisimulation distance $\widetilde{d}\left(s_{1}, s_{2}\right)$ is bounded. We speculate that similar results should be possible based on local DeepMDP losses, but they would require a generalization of bisimulation metrics to the local setting.</p>
<h3>5.4. Characterizing $\tilde{\Pi}$</h3>
<p>In order to better understand the set of policies $\tilde{\Pi}$ (which appears in the bounds of Sections 3 and 4), we</p>
<p>first consider the set of bisimilar policies, defined as $\widetilde{\Pi}=\left{\pi: \forall s_{1}, s_{2} \in \mathcal{S}, s_{1} \sim s_{2} \Leftrightarrow \pi\left(a \mid s_{1}\right)=\pi\left(a \mid s_{2}\right) \forall a\right}$, which contains all policies that act the same way on states that are bisimilar. Although this set excludes many policies in $\Pi$, we argue that it is adequately expressive, since any policy that acts differently on states that are bisimilar is fundamentally uninteresting. ${ }^{5}$</p>
<p>We show a connection between deep policies and bisimilar policies by proving that the set of Lipschitz-deep policies, $\Pi_{K} \subset \Pi$, approximately contains the set of Lipschitzbisimilar policies, $\widetilde{\Pi}_{K} \subset \widetilde{\Pi}$, defined as follows:</p>
<p>$$
\begin{aligned}
&amp; \widetilde{\Pi}<em 1="1">{K}=\left{\bar{\pi}: \forall s</em> \leqslant K\right} \
&amp; \widetilde{\Pi}} \neq s_{2} \in \mathcal{S}, \frac{\left|\bar{\pi}\left(a \mid s_{1}\right)-\bar{\pi}\left(a \mid s_{2}\right)\right|}{d_{\mathcal{S}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right)<em 1="1">{K}=\left{\pi: \forall s</em> \leqslant K\right}
\end{aligned}
$$} \neq s_{2} \in \mathcal{S}, \frac{\left|\pi\left(a \mid s_{1}\right)-\pi\left(a \mid s_{2}\right)\right|}{d\left(s_{1}, s_{2}\right)</p>
<p>The following theorem proves that minimizing the global DeepMDP losses ensures that for any $\widetilde{\pi} \in \widetilde{\Pi}<em C="C" K="K">{K}$, there is a deep policy $\bar{\pi} \in \widetilde{\Pi}</em>$.
Theorem 4. Let $\mathcal{M}$ be an MDP and $\overline{\mathcal{M}}$ be a $\left(K_{\overline{\mathcal{R}}}, K_{\overline{\mathcal{P}}}\right)$ Lipschitz DeepMDP, with an embedding function $\phi$ and global loss functions $L_{\overline{\mathcal{R}}}^{\infty}$ and $L_{\mathcal{P}}^{\infty}$. Denote by $\widetilde{\Pi}}$ which is close to $\widetilde{\pi}$, where the constant $C=\frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}<em K="K">{K}$ and $\widetilde{\Pi}</em>}$ the sets of Lipschitz-bisimilar and Lipschitz-deep policies. Then for any $\widetilde{\pi} \in \widetilde{\Pi<em C="C" K="K">{K}$ there exists a $\bar{\pi} \in \widetilde{\Pi}</em>$,}$ which is close to $\widetilde{\pi}$ in the sense that, for all $s \in \mathcal{S}$ and $a \in \mathcal{A</p>
<p>$$
|\widetilde{\pi}(a \mid s)-\bar{\pi}(a \mid s)| \leqslant L_{\overline{\mathcal{R}}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\overline{\mathcal{R}}}}{1-\gamma K_{\mathcal{P}}}
$$</p>
<h2>6. Beyond the Wasserstein</h2>
<p>Interestingly, value difference bounds (Lemmas 2 and 3) can be derived for many different choices of probability metric $\mathcal{D}$ (in the DeepMDP transition loss function, Equation 2). Here, we generalize the result to a family of Maximum Mean Discrepancy (MMD) metrics (Gretton et al., 2012) defined via a function norm that we denote as Norm Maximum Mean Discrepancy (Norm-MMD) metrics. Interestingly, the role of the Lipschitz norm in the value difference bounds is a consequence of using the Wasserstein; when we switch from the Wasserstein to another metric, it is replaced by a different term. We interpret these terms as different forms of smoothness of the value functions in $\overline{\mathcal{M}}$.</p>
<p>By choosing a metric whose associated smoothness corresponds well to the environment, we can potentially improve the tightness of the bounds. For example, in environments with highly non-Lipschitz dynamics, it may be impossible to</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>learn an accurate DeepMDP whose deep value function has a small Lipschitz norm. Instead, the associated smoothness of another metric might be more appropriate. Another reason to consider other metrics is computational; the Wasserstein has high computational cost and suffers from biased stochastic gradient estimates (Bikowski et al., 2018; Bellemare et al., 2017b), so minimizing a simpler metric, such as the KL, may be more convenient.</p>
<h3>6.1. Norm Maximum Mean Discrepancy Metrics</h3>
<p>MMD metrics (Gretton et al., 2012) are a family of probability metrics, each generated via a class of functions. They have also been studied by MÃ¼ller (1997) under the name of Integral Probability Metrics.
Definition 6 (Gretton et al. (2012) Definition 2). Let $P$ and $Q$ be distributions on a measurable space $\chi$ and let $\mathcal{F}_{\mathcal{D}}$ be a class of functions $f: \chi \rightarrow \mathbb{R}$. The Maximum Mean Discrepancy $\mathcal{D}$ is</p>
<p>$$
\mathcal{D}(P, Q)=\sup <em _mathcal_D="\mathcal{D">{f \in \mathcal{F}</em> f(y)\right|
$$}}}\left|\underset{x \sim P}{\mathbb{E}} f(x)-\underset{y \sim Q}{\mathbb{E}</p>
<p>When $P=Q$ it's obvious that $\mathcal{D}(P, Q)=0$ regardless of the function class $\mathcal{F}<em D="D">{\mathcal{D}}$. But the class of functions leads to MMD metrics with different behaviours and properties. Of interest to us are function classes generated via function seminorms ${ }^{6}$. Concretely, we define a Norm-MMD metric $\mathcal{D}$ to be an MMD metric generated from a function class $\mathcal{F}</em>$ of the following form:</p>
<p>$$
\mathcal{F}<em _mathcal_D="\mathcal{D">{\mathcal{D}}=\left{f:|f|</em> \leqslant 1\right}
$$}</p>
<p>where $|\cdot|<em _mathcal_D="\mathcal{D">{\mathcal{D}}$ is the associated function seminorm of $\mathcal{D}$. We will see that the family of Norm-MMDs are well suited for the task of latent space modeling. Their key property is the following: let $\mathcal{D}$ be a Norm-MMD, then for any function $f$ s.t. $|f|</em> \leqslant K$,}</p>
<p>$$
\left|\underset{x \sim P}{\mathbb{E}} f(x)-\underset{y \sim Q}{\mathbb{E}} f(y)\right| \leqslant K \cdot \mathcal{D}(P, Q)
$$</p>
<p>We now discuss three particularly interesting examples of Norm-MMD metrics.</p>
<p>Total Variation: Defined as $T V(P, Q)=\frac{1}{2} \int_{\mathbb{R}}|P(x)-$ $Q(x) \mid d x$, the Total Variation is one of the most widelystudied metrics. Pinsker's inequality (Borwein \&amp; Lewis, 2005, p.63) bounds the TV with the Kullback-Leibler (KL) divergence. The Total Variation is also the Norm-MMD generated from the set of functions with absolute value bounded by 1 (MÃ¼ller, 1997). Thus, the function norm $|f|<em _infty="\infty">{T V}=|f|</em>|f(x)|$.
Wasserstein metric: The interpretation of the Wasserstein as an MMD metrics is clear from its dual form (Equation 3),}=\sup _{x \in \chi</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Visualization of the way in which different smoothness properties on the value function are derived. The left compares two near-identical frames of PONG, (a) and (b), whose only difference is the position of the player's paddle. The plots on the right show the optimal value of the state (top) and the derivative of the optimal value (bottom) as a function of the position of the player's paddle, assuming all other features of the state are kept constant. The associated smoothness of each Norm-MMD metric is shown visually. (Note that this is for illustrative purposes only, and was not actually computed from the real game. The curve in the value function represents noisy dynamics, such as those induced by "sticky actions" (Mnih et al., 2015); if the environment were deterministic, the optimal value would be a step function.)
where the function class $\mathcal{F}_{W}$ is set of 1-Lipschitz functions,</p>
<p>$$
\mathcal{F}_{W}={f:|f(x)-f(y)| \leqslant d(x, y), \forall x, y \in \chi}
$$</p>
<p>The norm associated with the Wasserstein metric $|f|<em _infty="\infty">{W}$ is therefore the Lipschitz norm, which in turn is the the $\ell</em>$ (the derivative of $f$ ). Thus, $|f|}$ norm of $f^{\prime<em _infty="\infty">{W}=\left|f^{\prime}\right|</em>$.
Energy distance: The energy distance $E$ was first developed to compare distributions in high dimensions via a two sample test (SzÃ©kely \&amp; Rizzo, 2004; Gretton et al., 2012). It is defined as:}=$ $\sup _{x \in \chi} \frac{d f(x)}{d x</p>
<p>$$
\begin{aligned}
E(P, Q)=2 &amp; \underset{(x, y) \sim P \times Q}{\mathbb{E}}|x-y| \
&amp; -\underset{x, x^{\prime} \sim P}{\mathbb{E}}\left|x-x^{\prime}\right|-\underset{y, y^{\prime} \sim Q}{\mathbb{E}}\left|y-y^{\prime}\right|
\end{aligned}
$$</p>
<p>where $x, x^{\prime} \sim P$ denotes two independent samples of the distribution $P$. Sejdinovic et al. (2013) showed the connection between the energy distance and MMD metrics. Similarly to the Wasserstein, the Energy distance's associated seminorm is: $|f|<em 1="1">{E}=\left|f^{\prime}\right|</em>\right| d x$.}=\int_{\chi}\left|\frac{d f(x)}{d x</p>
<h3>6.2. Value Function Smoothness</h3>
<p>In the context of value functions, we interpret the function seminorms associated with Norm-MMD metrics as different forms of smoothness.</p>
<p>Definition 7. Let $\overline{\mathcal{M}}$ be a DeepMDP and let $\mathcal{D}$ be a NormMMD with associated norm $|\cdot|<em _bar_V="\bar{V">{\mathcal{D}}$. We say that a policy $\bar{\pi} \in \bar{\Pi}$ is $K</em>$-smooth-valued if:}</p>
<p>$$
\left|\bar{V}^{\bar{\pi}}\right|<em _bar_V="\bar{V">{\mathcal{D}} \leqslant K</em>
$$}</p>
<p>and if for all $a \in \mathcal{A}$ :</p>
<p>$$
\left|\bar{Q}^{\bar{\pi}}(\cdot, a)\right|<em _bar_V="\bar{V">{\mathcal{D}} \leqslant K</em>
$$}</p>
<p>For a value function $\bar{V}^{\bar{\pi}},\left|\bar{V}^{\bar{\pi}}\right|<em W="W">{F V}$ is the maximum absolute value of $\bar{V}^{\bar{\pi}}$. Both $\left|\bar{V}^{\bar{\pi}}\right|</em>\right|}$ and $\left|\bar{V}^{\bar{\pi}<em W="W">{E}$ depend on the derivative of $\bar{V}^{\bar{\pi}}$, but while $\left|\bar{V}^{\bar{\pi}}\right|</em>\right|}$ is governed by point of maximal change, $\left|\bar{V}^{\bar{\pi}<em W="W">{E}$ instead measures the amount of change over the whole state space $\bar{s}$. Thus, a value function with a small region of high derivative (and thus, large $\left|\bar{V}^{\bar{\pi}}\right|</em>$. In Figure 3 we provide an intuitive visualization of these three forms of smoothness in the game of Pong.}$ ) can still have small $\left|\bar{V}^{\bar{\pi}}\right|_{E</p>
<p>One advantage of the Total Variation is that it requires minimal assumptions on the DeepMDP. If the reward function is bounded, i.e. $|\overline{\mathcal{R}}(\bar{s}, a)| \leqslant K_{\overline{\mathcal{R}}}, \forall \bar{s} \in \overline{\mathcal{S}}, a \in \mathcal{A}$, then all policies $\bar{\pi} \in \bar{\Pi}$ are $\frac{K_{\bar{R}}}{1-\epsilon}$-smooth-valued. We leave it to future work to study value function smoothness more generally for different Norm-MMD metrics and their associated norms.</p>
<h3>6.3. Generalized Value Difference Bounds</h3>
<p>The global and local value difference results (Lemmas 2 and 3), as well as the suboptimality result Lemma 1, can easily be derived when $\mathcal{D}$ is any Norm-MMD metric. Due to the repetitiveness of these results, we don't include them in the main paper; refer to Appendix A. 6 for the full</p>
<p>statements and proofs. We leave it to future work to characterize the of policies $\bar{\Pi}$ when general (i.e. non-Wasserstein) Norm-MMD metrics are used.</p>
<p>The fact that the representation quality results (Theorems 1 and 2) and the connection with bisimulation (Theorems 3 and 4) donâ€™t generalize to Norm-MMD metrics emphasizes the special role the Wasserstein metric plays for representation learning.</p>
<h2>7 Related Work in Representation Learning</h2>
<p>State aggregation methods <em>(Abel et al., 2017; Li et al., 2006; Singh et al., 1995; Givan et al., 2003; Jiang et al., 2015; Ruan et al., 2015)</em> attempt to reduce the dimensionality of the state space by joining states together, taking the perspective that a good representation is one that reduces the total number of states without sacrificing any necessary information. Other representation learning approaches take the perspective that an optimal representation contains features that allow for the linear parametrization of the optimal value function <em>(Comanici &amp; Precup, 2011; Mahadevan &amp; Maggioni, 2007)</em>. Recently, <em>Bellemare et al. (2019); Dadashi et al. (2019)</em> approached the representation learning problem from the perspective that a good representation is one that allows the prediction via a linear map of any value function in the value function space. In contrast, we have argued that a good representation (1) allows for the parametrization of a large set of <em>interesting</em> policies and (2) allows for the good approximation of the <em>value function</em> of these policies.</p>
<p>Concurrently, a suite of methods combining model-free deep reinforcement learning with auxiliary tasks has shown large benefits on a wide variety of domains <em>(Jaderberg et al., 2016; van den Oord et al., 2018; Mirowski et al., 2017)</em>. Distributional RL <em>(Bellemare et al., 2017a)</em>, which was not initially introduced as a representation learning technique, has been shown by <em>Lyle et al. (2019)</em> to only play an auxiliary task role. Similarly, <em>(Fedus et al., 2019)</em> studied different discounting techniques by learning the spectrum of value functions for different discount values $\gamma$, and incidentally found that to be a highly useful auxiliary task. Although successful in practice, these auxiliary task methods currently lack strong theoretical justification. Our approach also proposes to minimize losses as an auxilliary task for representation learning, for a specifc choice of losses: the DeepMDP losses. We have formally justified this choice of losses, by providing theoretical guarantees on representation quality.</p>
<h2>8 Empirical Evaluation</h2>
<p>Our results depend on minimizing losses in expectation, which is the main requirement for deep networks to be applicable. Still, two main obstacles arise when turning</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Given a state in our DonutWorld environment (first row), we plot a heatmap of the distance between that latent state and each other latent state, for both autoencoder representations (second row) and DeepMDP representations (third row). More-similar latent states are represented by lighter colors.</p>
<p>these theoretical results into practical algorithms:</p>
<p>(1) Minimization of the Wasserstein <em>Arjovsky et al. (2017)</em> first proposed the use of the Wasserstein distance for Generative Adversarial Networks (GANs) via its dual formulation (see Equation 3). Their approach consists of training a network, constrained to be 1-Lipschitz, to attain the supremum of the dual. Once this supremum is attained, the Wasserstein can be minimized by differentiating through the network. Quantile regression has been proposed as an alternative solution to the minimization of the Wasserstein <em>(Dabney et al., 2018b)</em>, <em>(Dabney et al., 2018a)</em>, and has shown to perform well for Distributional RL. The reader might note that issues with the stochastic minimization of the Wasserstein distance have been found to be biased by <em>Bellemare et al. (2017b)</em> and <em>Bikowski et al. (2018)</em>. In our experiments, we circumvent these issues by assuming that both $\mathcal{P}$ and $\bar{\mathcal{P}}$ are deterministic. This reduces the Wasserstein distance $W_{d_{\mathcal{S}}}(\phi\mathcal{P}(\cdot|s,a),\bar{\mathcal{P}}(\cdot|\phi(s),a))$ to $d_{\bar{\mathcal{S}}}(\phi(\mathcal{P}(s,a)),\bar{\mathcal{P}}(\phi(s),a))$, where $\mathcal{P}(s,a)$ and $\bar{\mathcal{P}}(\bar{s},a)$ denote the deterministic transition functions.</p>
<p>(2) Control the Lipschitz constants $K_{\mathcal{R}}$ and $K_{\bar{\mathcal{P}}}$. We also turn to the field of Wasserstein GANs for approaches to con-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Due to the competition between reward and transition losses, the optimization procedure spends significant time in local minima early on in training. It eventually learns a good representation, which it then optimizes further. (Note that the curves use different scaling on the y-axis.)
strain deep networks to be Lipschitz. Originally, Arjovsky et al. (2017) used a projection step to constraint the discriminator function to be 1-Lipschitz. Gulrajani et al. (2017a) proposed using a gradient penalty, and sowed improved learning dynamics. Lipschitz continuity has also been proposed as a regularization method by Gouk et al. (2018), who provided an approach to compute an upper bound to the Lipschitz constant of neural nets. In our experiments, we follow Gulrajani et al. (2017a) and utilize the gradient penalty.</p>
<h3>8.1. DonutWorld Experiments</h3>
<p>In order to evaluate whether we can learn effective representations, we study the representations learned by DeepMDPs in a simple synthetic environment we call DonutWorld. DonutWorld consists of an agent rewarded for running clockwise around a fixed track. Staying in the center of the track results in faster movement. Observations are given in terms of $32 \times 32$ greyscale pixel arrays, but there is a simple 2D latent state space (the x-y coordinates of the agent). We investigate whether the x-y coordinates are correctly recovered when learning a two-dimensional representation.</p>
<p>This task epitomizes the low-dimensional dynamics, highdimensional observations structure typical of Atari 2600 games, while being sufficiently simple to experiment with. We implement the DeepMDP training procedure using Tensorflow and compare it to a simple autoencoder baseline. See Appendix B for a full environment specification, experimental setup, and additional experiments. Code for replicating all experiments is included in the supplementary material.</p>
<p>In order to investigate whether the learned representations learned correspond well to reality, we plot a heatmap of closeness of representation for various states. Figure 4(a) shows that the DeepMDP representations effectively recover the underlying state of the agent, i.e. its 2D position, from
the high-dimensional pixel observations. In contrast, the autoencoder representations are less meaningful, even when the autoencoder solves the task near-perfectly.</p>
<p>In Figure 4(b), we modify the environment: rather than a single track, the environment now has four identical tracks. The agent starts in one uniformly at random and cannot move between tracks. The DeepMDP hidden state correctly merges all states with indistinguishable value functions, learning a deep state representation which is almost completely invariant to which track the agent is in.</p>
<p>The DeepMDP training loss can be difficult to optimize, as illustrated in Figure 5. This is due to the tendency of the transition and reward losses to compete with one another. If the deep state representation is uniformly zero, the transition loss will be zero as well; this is an easily-discovered local optimum, and gradient descent tends to arrive at this point early on in training. Of course, an informationless representation results in a large reward loss. As training progresses, the algorithm incurs a small amount of transition loss in return for a large decrease in reward loss, resulting in a net decrease in loss.</p>
<p>In DonutWorld, which has very simple dynamics, gradient descent is able to discover a good representation after only a few thousand iterations. However, in complex environments such as Atari, it is often much more difficult to discover representations that allow us to escape the low-information local minima. Using architectures with good inductive biases can help to combat this, as shown in Section 8.3. This issue also motivates the use of auxiliary losses (such as value approximation losses or reconstruction losses), which may help guide the optimizer towards good solutions; see Appendix C.5.</p>
<h3>8.2. Atari 2600 Experiments</h3>
<p>In this section, we demonstrate practical benefits of approximately learning a DeepMDP in the Arcade Learning Environment (Bellemare et al., 2013). Our results on representation-similarity indicate that learning a DeepMDP is a principled method for learning a high-quality representation. Therefore, we minimize DeepMDP losses as an auxiliary task alongside model-free reinforcement learning, learning a single representation which is shared between both tasks. Our implementations of the proposed algorithms are based on Dopamine (Castro et al., 2018).</p>
<p>We adopt the Distributional Q-learning approach to modellfree RL; specifically, we use as a baseline the C51 agent (Bellemare et al., 2017a), which estimates probability masses on a discrete support and minimizes the KL divergence between the estimated distribution and a target distribution. C51 encodes the input frames using a convolutional neural network $\phi: \mathcal{S} \rightarrow \overline{\mathcal{S}}$, outputting a dense</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. We compare the DeepMDP agent versus the C51 agent on the 60 games from the ALE (3 seeds each). For each game, the percentage performance improvement of DeepMDP over C51 is recorded.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Performance of C51 with model-based auxiliary objectives. Three types of transition models are used for predicting next latent states: a single convolutional layer (convolutional), a single fully-connected layer (one-layer), and a two-layer fully-connected network (two-layer).</p>
<p>vector representation $\vec{s}=\phi(s)$. The C51 Q-function is a feed-forward neural network which maps $\vec{s}$ to an estimate of the reward distribution's logits.</p>
<p>To incorporate learning a DeepMDP as an auxiliary learning objective, we define a deep reward function and deep transition function. These are each implemented as a feed-forward neural network, which uses $\vec{s}$ to estimate the immediate reward and the next-state representation, respectively. The overall objective function is a simple linear combination of the standard C51 loss and the Wasserstein distance-based approximations to the local DeepMDP loss given by Equations 6 and 7. For experimental details, see Appendix C.</p>
<p>By optimizing $\phi$ to jointly minimize both C51 and DeepMDP losses, we hope to learn meaningful $\vec{s}$ that form the basis for learning good value functions. In the following subsections, we aim to answer the following questions: (1) What deep transition model architecture is conducive to learning a DeepMDP on Atari? (2) How does the learning of a DeepMDP affect the overall performance of C51 on Atari 2600 games? (2) How do the DeepMDP objectives compare with similar representation-learning approaches?</p>
<h3>8.3. Transition Model Architecture</h3>
<p>We compare the performance achieved by using different architectures for the DeepMDP transition model (see Figure 7). We experiment with a single fully-connected layer, two fully-connected layers, and a single convolutional layer (see Appendix C for more details). We find that using a convolutional transition model leads to the best DeepMDP performance, and we use this transition model architecture for the rest of the experiments in this paper. Note how the performance of the agent is highly dependent on the architecture. We hypothesize that the inductive bias provided via the model has a large effect on the learned DeepMDPs. Further exploring model architectures which provide inductive biases is a promising avenue to develop better auxiliary tasks. Particularly, we believe that exploring attention (Vaswani et al., 2017; Bahdanau et al., 2014) and relational inductive biases (Watters et al., 2017; Battaglia et al., 2016) could be useful in visual domains like Atari2600.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Using various auxiliary tasks in the Arcade Learning Environment. We compare predicting the next state's representation (Next Latent State, recommended by theoretical bounds on DeepMDPs) with reconstructing the current observation (Observation), predicting the next observation (Next Observation), and predicting the next C51 logits (Next Logits). Training curves for a baseline C51 agent are also shown.</p>
<h3>8.4. DeepMDPs as an Auxiliary Task</h3>
<p>We show that when using the best performing DeepMDP architecture described in Appendix C.2, we obtain nearly consistent performance improvements over C51 on the suite of 60 Atari 2600 games (see Figure 6).</p>
<h3>8.5. Comparison to Alternative Objectives</h3>
<p>We empirically compare the effect of the DeepMDP auxiliary objectives on the performance of a C51 agent to a variety of alternatives. In the experiments in this section, we replace the deep transition loss suggested by the DeepMDP bounds with each of the following:
(1) Observation Reconstruction: We train a state decoder to reconstruct observations $s \in \mathcal{S}$ from $\tilde{s}$. This framework is similar to (Ha \&amp; Schmidhuber, 2018), who learn a latent space representation of the environment with an autoencoder, and use it to train an RL agent.
(2) Next Observation Prediction: We train a transition model to predict next observations $s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)$ from the current state representation $\tilde{s}$. This framework is similar to modelbased RL algorithms which predict future observations (Xu et al., 2018).
(3) Next Logits Prediction: We train a transition model to predict next-state representations such that the Q-function correctly predicts the logits of $\left(s^{\prime}, a^{\prime}\right)$, where $a^{\prime}$ is the action associated with the max Q-value of $s^{\prime}$. This can be understood as a distributional analogue of the Value Prediction Network, VPN, (Oh et al., 2017). Note that this auxiliary loss is used to update only the parameters of the representation encoder and the transition model, not the Q-function.</p>
<p>Our experiments demonstrate that the deep transition loss suggested by the DeepMDP bounds (i.e. predicting the next state's representation) outperforms all three ablations (see Figure 8). Accurately modeling Atari 2600 frames, whether through observation reconstruction or next observation pre-
diction, forces the representation to encode irrelevant information with respect to the underlying task. VPN-style losses have been shown to be helpful when using the learned predictive model for planning (Oh et al., 2017); however, we find that with a distributional RL agent, using this as an auxiliary task tends to hurt performance.</p>
<h2>9. Discussion on Model-Based RL</h2>
<p>We have focused on the implications of DeepMDPs for representation learning, but our results also provide a principled basis for model-based RL - in latent space or otherwise. Although DeepMDPs are latent space models, by letting $\phi$ be the identity function, all the provided results immediately apply to the standard model-based RL setting, where the model predicts states instead of latent states. In fact, our results serve as a theoretical justification for common practices already found in the model-based deep RL literature. For example, Chua et al. (2018); Doerr et al. (2018); Hafner et al. (2018); Buesing et al. (2018); Feinberg et al. (2018); Buckman et al. (2018) train models to predict a reward and a distribution over next states, minimizing the negative log-probability of the true next state. The negative log-probability of the next state can be viewed as a one-sample estimate of the KL between the model's state distribution and the next state distribution. Due to Pinsker's inequality (which bounds the TV with the KL), and the suitability of TV as a metric (Section 6), this procedure can be interpreted as training a DeepMDP. Thus, the learned model will obey our local value difference bounds (Lemma 8) and suboptimality bounds (Theorem 6), which provide theoretical guarantees for the model.</p>
<p>Further, the suitability of Norm-MMD metrics for learning models presents a promising new research avenue for modelbased RL: to break away from the KL and explore the vast family of Norm Maximum Mean Discrepancy metrics.</p>
<h2>10. Conclusions</h2>
<p>We introduce the concept of a DeepMDP: a parameterized latent space model trained via the minimization of tractable losses. Theoretical analysis provides guarantees on the quality of the value functions of the learned model when the latent transition loss is any member of the large family of Norm Maximum Mean Discrepancy metrics. When the Wasserstein metric is used, a novel connection to bisimulation metrics guarantees the set of parametrizable policies is highly expressive. Further, it's guaranteed that two states with different values for any of those policies will never be collapsed under the representation. Together, these findings suggest that learning a DeepMDP with the Wasserstein metric is a theoretically sound approach to representation learning. Our results are corroborated by strong performance on large-scale Atari 2600 experiments, demonstrating that minimizing the DeepMDP losses can be a beneficial auxiliary task in model-free RL.</p>
<p>Using the transition and reward models of the DeepMDP for model-based RL (e.g. planning, exploration) is a promising future research direction. Additionally, extending DeepMDPs to accommodate different action spaces or time scales from the original MDPs could be a promising path towards learning hierarchical models of the environment.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to thank Philip Amortila and Robert Dadashi for invaluable feedback on the theoretical results; Pablo Samuel Castro, Doina Precup, Nicolas Le Roux, Sasha Vezhnevets, Simon Osindero, Arthur Gretton, Adrien Ali Taiga, Fabian Pedregosa and Shane Gu for useful discussions and feedback.</p>
<h2>Changes From ICML 2019 Proceedings</h2>
<p>This document represents an updated version of our work relative to the version published in ICML 2019. The major addition was the inclusion of the generalization to NormMMD metrics and associated math in Section 6. Lemma 1 also underwent minor changes to its statements and proofs. Additionally, some sections were partially rewritten, especially the discussion on bisimulation (Section 5), which was significantly expanded.</p>
<h2>References</h2>
<p>Abel, D., Hershkowitz, D. E., and Littman, M. L. Near optimal behavior via approximate state abstraction. arXiv preprint arXiv:1701.04113, 2017.</p>
<p>Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In ICML, 2017.
Asadi, K., Misra, D., and Littman, M. L. Lipschitz continuity in model-based reinforcement learning. arXiv preprint arXiv:1804.07193, 2018.</p>
<p>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Battaglia, P. W., Pascanu, R., Lai, M., Rezende, D. J., and Kavukcuoglu, K. Interaction networks for learning about objects, relations and physics. In NIPS, 2016.</p>
<p>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, June 2013.</p>
<p>Bellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In Proceedings of the International Conference on Machine Learning, 2017a.</p>
<p>Bellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., and Munos, R. The cramer distance as a solution to biased wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017b.</p>
<p>Bellemare, M. G., Dabney, W., Dadashi, R., Taiga, A. A., Castro, P. S., Roux, N. L., Schuurmans, D., Lattimore, T., and Lyle, C. A geometric perspective on optimal representations for reinforcement learning. CoRR, abs/1901.11530, 2019.</p>
<p>Bikowski, M., Sutherland, D. J., Arbel, M., and Gretton, A. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. URL https:// openreview.net/forum?id=r11UOzWCW.</p>
<p>Borwein, J. and Lewis, A. S. Convex Analysis and Nonlinear Optimization. Springer, 2005.</p>
<p>Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H. Sample-efficient reinforcement learning with stochastic ensemble value expansion. In NeurIPS, 2018.</p>
<p>Buesing, L., Weber, T., Racaniere, S., Eslami, S., Rezende, D., Reichert, D. P., Viola, F., Besse, F., Gregor, K., Hassabis, D., et al. Learning and querying fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.</p>
<p>Castro, P. and Precup, D. Using bisimulation for policy transfer in mdps. Proceeedings of the 9th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-2010), 2010.</p>
<p>Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. Dopamine: A research framework for deep reinforcement learning. arXiv, 2018.</p>
<p>Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pp. 4754-4765, 2018.</p>
<p>Chung, W., Nath, S., Joseph, A. G., and White, M. Twotimescale networks for nonlinear value function approximation. In International Conference on Learning Representations, 2019.</p>
<p>Comanici, G. and Precup, D. Basis function discovery using spectral clustering and bisimulation metrics. In AAMAS, 2011.</p>
<p>Dabney, W., Ostrovski, G., Silver, D., and Munos, R. Implicit quantile networks for distributional reinforcement learning. In ICML, 2018a.</p>
<p>Dabney, W., Rowland, M., Bellemare, M. G., and Munos, R. Distributional reinforcement learning with quantile regression. In AAAI, 2018b.</p>
<p>Dadashi, R., Taiga, A. A., Roux, N. L., Schuurmans, D., and Bellemare, M. G. The value function polytope in reinforcement learning. CoRR, abs/1901.11524, 2019.</p>
<p>Doerr, A., Daniel, C., Schiegg, M., Nguyen-Tuong, D., Schaal, S., Toussaint, M., and Trimpe, S. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.</p>
<p>Fedus, W., Gelada, C., Bengio, Y., Bellemare, M. G., and Larochelle, H. Hyperbolic discounting and learning over multiple horizons. ArXiv, abs/1902.06865, 2019.</p>
<p>Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101, 2018.</p>
<p>Ferns, N., Panangaden, P., and Precup, D. Metrics for finite markov decision processes. Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, UAI'04:162-169, 2004.</p>
<p>Ferns, N., Panangaden, P., and Precup, D. Bisimulation metrics for continuous markov decision processes. SIAM Journal on Computing, 40(6):1662-1714, 2011.</p>
<p>Francois-Lavet, V., Bengio, Y., Precup, D., and Pineau, J. Combined reinforcement learning via abstract representations. arXiv preprint arXiv:1809.04506, 2018.</p>
<p>Gelada, C. and Bellemare, M. G. Off-policy deep reinforcement learning by bootstrapping the covariate shift. CoRR, abs/1901.09455, 2019.</p>
<p>Givan, R., Dean, T., and Greig, M. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147(1-2):163-223, 2003.</p>
<p>Gouk, H., Frank, E., Pfahringer, B., and Cree, M. J. Regularisation of neural networks by enforcing lipschitz continuity. CoRR, abs/1804.04368, 2018.</p>
<p>Gretton, A., Borgwardt, K. M., Rasch, M. J., SchÃ¶lkopf, B., and Smola, A. J. A kernel two-sample test. Journal of Machine Learning Research, 13:723-773, 2012.</p>
<p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. In NIPS, 2017a.</p>
<p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767-5777, 2017b.</p>
<p>Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pp. 2455-2467, 2018.</p>
<p>Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</p>
<p>Hinderer, K. Lipschitz continuity of value functions in markovian decision processes. Math. Meth. of OR, 62: $3-22,2005$.</p>
<p>Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.</p>
<p>Jiang, N., Kulesza, A., and Singh, S. Abstraction selection in model-based reinforcement learning. In International Conference on Machine Learning, pp. 179-188, 2015.</p>
<p>Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Sepassi, R., Tucker, G., and Michalewski, H. Model-based reinforcement learning for atari. CoRR, abs/1903.00374, 2019.</p>
<p>Li, L., Walsh, T. J., and Littman, M. L. Towards a unified theory of state abstraction for mdps. In ISAIM, 2006.</p>
<p>Lyle, C., Castro, P. S., and Bellemare, M. G. A comparative analysis of expected and distributional reinforcement learning. CoRR, abs/1901.11084, 2019.</p>
<p>Mahadevan, S. and Maggioni, M. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research, 8:2169-2231, 2007.</p>
<p>Mirowski, P. W., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., Kumaran, D., and Hadsell, R. Learning to navigate in complex environments. CoRR, abs/1611.03673, 2017.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015.</p>
<p>Mueller, A. Integral probability metrics and their generating classes of functions. 1997.</p>
<p>MÃ¼ller, A. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29 (2):429-443, 1997.</p>
<p>Oh, J., Singh, S., and Lee, H. Value prediction network. In Advances in Neural Information Processing Systems, pp. 6118-6128, 2017.</p>
<p>Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., and Littman, M. L. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In ICML, 2008.</p>
<p>Pirotta, M., Restelli, M., and Bascetta, L. Policy gradient in lipschitz markov decision processes. Machine Learning, 100(2-3):255-283, 2015.</p>
<p>Puterman, M. L. Markov decision processes: Discrete stochastic dynamic programming. 1994.</p>
<p>Ruan, S. S., Comanici, G., Panangaden, P., and Precup, D. Representation discovery for mdps using bisimulation metrics. In $A A A I, 2015$.</p>
<p>Sejdinovic, D., Sriperumbudur, B. K., Gretton, A., and Fukumizu, K. Equivalence of distance-based and rkhs-based statistics in hypothesis testing. CoRR, abs/1207.6076, 2013.</p>
<p>Silver, D., van Hasselt, H. P., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert, D. P., Rabinowitz, N. C., Barreto, A., and Degris, T. The predictron: End-to-end learning and planning. In ICML, 2017.</p>
<p>Singh, S. P., Jaakkola, T., and Jordan, M. I. Reinforcement learning with soft state aggregation. In Advances in neural information processing systems, pp. 361-368, 1995.</p>
<p>SzÃ©kely, G. J. and Rizzo, M. L. Testing for equal distributions in high dimension. 2004.
van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017.</p>
<p>Villani, C. Optimal Transport: Old and New. Springer Science \&amp; Business Media, 2008, 2008.</p>
<p>Watters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P. W., and Zoran, D. Visual interaction networks. CoRR, abs/1706.01433, 2017.</p>
<p>Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. Algorithmic framework for model-based reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858, 2018.</p>
<p>Zhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M. J., and Levine, S. Solar: Deep structured latent representations for model-based reinforcement learning. arXiv preprint arXiv:1808.09105, 2018.</p>
<h1>Appendix</h1>
<h2>A. Proofs</h2>
<h2>A.1. Lipschitz MDP</h2>
<p>Lemma 1. Let $\overline{\mathcal{M}}$ be $\left(K_{\overline{\mathcal{R}}}, K_{\overline{\mathcal{P}}}\right)$-Lipschitz. Then,</p>
<ol>
<li>The optimal policy $\bar{\pi}^{*}$ is $\frac{K_{\bar{\pi}}}{1-\gamma K_{\bar{\pi}}}$-Lipschitz-valued.</li>
<li>All policies with $K_{\bar{\mathcal{P}}<em _bar_pi="\bar{\pi">{\bar{\pi}}} \leqslant \frac{1}{\gamma}$ are $\frac{K</em><em _bar_pi="\bar{\pi">{\bar{\pi}}}}{1-\gamma K</em>$-Lipschitz-valued.}_{\bar{\pi}}}</li>
<li>All constant policies (i.e. $\bar{\pi}\left(a \mid \bar{s}<em 2="2">{1}\right)=\bar{\pi}\left(a \mid \bar{s}</em>}\right), \forall a \in \mathcal{A}, \bar{s<em 2="2">{1}, \bar{s}</em>$-Lipschitz-valued.} \in \overline{\mathcal{S}}$ ) are $\frac{K_{\bar{\pi}}}{1-\gamma K_{\bar{\pi}}</li>
</ol>
<p>Proof. Start by proving 1. By induction we will show that a sequence of Q values $\bar{Q}<em _bar_Q="\bar{Q">{n}$ converging to $\bar{Q}^{<em>}$ are all Lipschitz, and that as $n \rightarrow \infty$, their Lipschitz norm goes to $\frac{K_{\bar{\pi}}}{1-\gamma K_{\bar{\pi}}}$. Let $\bar{Q}<em n_1="n+1">{0}(\bar{s}, a)=0, \forall \bar{s} \in \overline{\mathcal{S}}, a \in \mathcal{A}$ be the base case. Define $\bar{Q}</em>}(\bar{s}, a)=\overline{\mathcal{R}}(\bar{s}, a)+\gamma \mathbb{E<em a_prime="a^{\prime">{\bar{s}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \bar{s}, a)}\left[\max </em>}} \bar{Q<em n="n">{n}\left(\bar{s}^{\prime}, a^{\prime}\right)\right]$. It is a well known result that the sequence $\bar{Q}</em>^{}$ converges to $\bar{Q</em>}$. Now let $K</em>=\sup }, n<em 1="1">{a \in \mathcal{A}, \bar{s}</em>} \neq \bar{s<em n="n">{2} \in \overline{\mathcal{S}}} \frac{\left|\bar{Q}</em>}\left(\bar{s<em n="n">{1}, a\right)-\bar{Q}</em>}\left(\bar{s<em _bar_pi="\bar{\pi">{2}, a\right)\right|}{d</em>}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>}\right)}$ be the Lipschitz norm of $\bar{Q<em _bar_Q="\bar{Q">{n}$. Clearly $K</em>=0$. Then,}, 0</p>
<p>$$
\begin{aligned}
K_{\bar{Q}, n+1} &amp; =\sup <em 1="1">{a \in \mathcal{A}, \bar{s}</em>} \neq \bar{s<em n_1="n+1">{2} \in \overline{\mathcal{S}}} \frac{\left|\bar{Q}</em>}\left(\bar{s<em n_1="n+1">{1}, a\right)-\bar{Q}</em>}\left(\bar{s<em _bar_pi="\bar{\pi">{2}, a\right)\right|}{d</em>}}\left(\bar{s<em 2="2">{1}, \bar{s}</em> \
&amp; \leqslant \sup }\right)<em 1="1">{a \in \mathcal{A}, \bar{s}</em>} \neq \bar{s<em 1="1">{2} \in \overline{\mathcal{S}}} \frac{\left|\overline{\mathcal{R}}\left(\bar{s}</em>}, a\right)-\overline{\mathcal{R}}\left(\bar{s<em _bar_pi="\bar{\pi">{2}, a\right)\right|}{d</em>}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>+\gamma \sup }\right)<em 1="1">{a \in \mathcal{A}, \bar{s}</em>} \neq \bar{s<em _bar_s="\bar{s">{2} \in \overline{\mathcal{S}}} \frac{\left|\mathbb{E}</em><em 1="1">{1}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \bar{s}</em>}, a)} \bar{Q<em 1="1">{n}\left(\bar{s}</em>}^{\prime}, a\right)-\mathbb{E<em 2="2">{\bar{s}</em>}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \bar{s<em n="n">{2}, a)} \bar{Q}</em>}\left(\bar{s<em _bar_pi="\bar{\pi">{2}^{\prime}, a\right)\right|}{d</em>}}\left(\bar{s<em 2="2">{1}, \bar{s}</em> \
&amp; =K_{\overline{\mathcal{R}}}+\gamma \sup }\right)<em 1="1">{a \in \mathcal{A}, \bar{s}</em>} \neq \bar{s<em _bar_s="\bar{s">{2} \in \overline{\mathcal{S}}} \frac{\left|\mathbb{E}</em><em 1="1">{1}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \bar{s}</em>}, a)} \bar{Q<em 1="1">{n}\left(\bar{s}</em>}^{\prime}, a\right)-\mathbb{E<em 2="2">{\bar{s}</em>}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \bar{s<em n="n">{2}, a)} \bar{Q}</em>}\left(\bar{s<em _bar_pi="\bar{\pi">{2}^{\prime}, a\right)\right|}{d</em>}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>
\end{aligned}
$$}\right)</p>
<p>$$
\begin{aligned}
&amp; \leqslant K_{\overline{\mathcal{R}}}+\gamma K_{\bar{Q}, n} \sup <em 1="1">{a \in \mathcal{A}, \bar{s}</em>} \neq \bar{s<em 2="2">{2} \in \overline{\mathcal{S}}} \frac{W\left(\overline{\mathcal{P}}\left(\cdot \mid \bar{s}</em>}, a\right), \overline{\mathcal{P}}\left(\cdot \mid \bar{s<em _bar_pi="\bar{\pi">{2}, a\right)\right)}{d</em>}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>}\right)}, \text { (Using the fact that } \bar{Q<em _bar_Q="\bar{Q">{n} \text { is } K</em> \
&amp; \leqslant K_{\overline{\mathcal{R}}}+\gamma K_{\bar{Q}, n} K_{\bar{P}} \
&amp; \leqslant \sum_{i=0}^{n-1}\left(\gamma K_{\bar{P}}\right)^{i} K_{\overline{\mathcal{R}}}+\left(\gamma K_{\bar{P}}\right)^{n} K_{\bar{Q}, 0}=\sum_{i=0}^{n-1}\left(\gamma K_{\bar{P}}\right)^{i} K_{\overline{\mathcal{R}}}, \text { (by expanding the recursion) }
\end{aligned}
$$}, n} \text {-Lipschitz by induction) </p>
<p>Thus, as $n \rightarrow \infty, K_{\bar{Q}^{*}} \leqslant \frac{K_{\bar{\pi}}}{1-\gamma K_{\bar{P}}}$.
To prove 2 a similar argument can be used. The sequence $\bar{V}<em _bar_pi="\bar{\pi">{n+1}(\bar{s}, a)=\overline{\mathcal{R}}</em>}}(\bar{s})+\gamma \mathbb{E<em n="n">{\bar{s}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \bar{s}, a)}\left[\bar{V}</em>$ is also Lipschitz.}\left(\bar{s}^{\prime}\right)\right]$ converges to $\bar{Q}^{\bar{\pi}}$ and the sequence of Lipschitz norms converge to $\frac{K_{\bar{\pi} \bar{\pi}}}{1-\gamma K_{\bar{P}}}$. From there it's trivial to show that $\bar{Q}^{\bar{\pi}</p>
<p>Finally, we prove 3. Note that the transition function of a constant policy $\pi(a)$ has the following property:</p>
<p>$$
\begin{aligned}
W\left(\overline{\mathcal{P}}<em 2="2">{\bar{\pi}}\left(\cdot \mid \bar{s}</em>}\right), \overline{\mathcal{P}<em 2="2">{\bar{\pi}}\left(\cdot \mid \bar{s}</em>\right)\right) &amp; =\sup <em _bar_pi="\bar{\pi">{f \in \mathcal{F}}\left|\int\left(\overline{\mathcal{P}}</em>}}\left(\bar{s}^{\prime} \mid \bar{s<em _bar_pi="\bar{\pi">{2}\right)-\overline{\mathcal{P}}</em>}}\left(\bar{s}^{\prime} \mid \bar{s<em _in="\in" _mathcal_F="\mathcal{F" f="f">{2}\right)\right) f\left(\bar{s}^{\prime}\right) d \bar{s}^{\prime}\right| \
&amp; \leqslant \sup </em>}}\left|\int \sum_{a} \pi(a)\left(\overline{\mathcal{P}}\left(\bar{s}^{\prime} \mid \bar{s<em 2="2">{2}, a\right)-\overline{\mathcal{P}}\left(\bar{s}^{\prime} \mid \bar{s}</em>\right| \
&amp; \leqslant \sum_{a} \pi(a) \sup }, a\right)\right) f\left(\bar{s}^{\prime}\right) d \bar{s}^{\prime<em 2="2">{f \in \mathcal{F}}\left|\int\left(\overline{\mathcal{P}}\left(\bar{s}^{\prime} \mid \bar{s}</em>}, a\right)-\overline{\mathcal{P}}\left(\bar{s}^{\prime} \mid \bar{s<em a="a">{2}, a\right)\right) f\left(\bar{s}^{\prime}\right) d \bar{s}^{\prime}\right| \
&amp; \leqslant \sum</em> \
&amp; \leqslant K_{\bar{P}}
\end{aligned}
$$} \pi(a) K_{\bar{P}</p>
<p>Similarly, $\left|\overline{\mathcal{R}}<em 1="1">{\bar{\pi}}\left(\bar{s}</em>}\right)-\overline{\mathcal{R}<em 2="2">{\bar{\pi}}\left(\bar{s}</em>}\right)\right| \leqslant K_{\overline{\mathcal{R}}}$. Thus, for a constant policy $\pi$, the Lipschitz norms $K_{\overline{\mathcal{P}<em _overline_mathcal_P="\overline{\mathcal{P">{\bar{\pi}}} \leqslant K</em>}}}$ and $K_{\overline{\mathcal{R}<em _overline_mathcal_R="\overline{\mathcal{R">{\bar{\pi}}} \leqslant K</em>$. To complete the proof we can apply result 2.}}</p>
<h1>A.2. Global DeepMDP</h1>
<p>Lemma 2. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, with an embedding function $\phi$ and global loss functions $L_{\mathcal{R}}^{\mathcal{V}}$ and $L_{\mathcal{P}}^{\mathcal{V}}$. For any $K_{\bar{V}}$-Lipschitz-valued policy $\bar{\pi} \in \bar{\Pi}$ the value difference can be bounded by</p>
<p>$$
\left|Q^{\pi}(s, a)-\bar{Q}^{\pi}(\phi(s), a)\right| \leqslant \frac{L_{\mathcal{R}}^{\mathcal{V}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\mathcal{V}}}{1-\gamma}
$$</p>
<p>Proof. This is a specific case to the general Lemma 7
Theorem 1. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, let $d_{\overline{\mathcal{S}}}$ be a metric in $\overline{\mathcal{S}}, \phi$ be an embedding function and $L_{\mathcal{R}}^{\mathcal{V}}$ and $L_{\mathcal{P}}^{\mathcal{V}}$ be the global loss functions. For any $K_{\bar{V}}$-Lipschitz-valued policy $\bar{\pi} \in \bar{\Pi}$ the representation $\phi$ guarantees that for all $s_{1}, s_{2} \in \mathcal{S}$ and $a \in \mathcal{A}$,</p>
<p>$$
\begin{aligned}
\left|Q^{\pi}\left(s_{1}, a\right)-Q^{\pi}\left(s_{2}, a\right)\right| \leqslant K_{\bar{V}} d_{\overline{\mathcal{S}}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
+2 \frac{L_{\mathcal{R}}^{\mathcal{V}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\mathcal{V}}}{1-\gamma}
\end{aligned}
$$</p>
<p>Proof.</p>
<p>$$
\begin{aligned}
\left|Q^{\pi}\left(s_{1}, a\right)-Q^{\pi}\left(s_{2}, a\right)\right| &amp; \leqslant\left|\bar{Q}^{\pi}\left(s_{1}, a\right)-\bar{Q}^{\pi}\left(s_{2}, a\right)\right|+\left|Q^{\pi}\left(s_{1}, a\right)-\bar{Q}^{\pi}\left(s_{1}, a\right)\right|+\left|Q^{\pi}\left(s_{2}, a\right)-\bar{Q}^{\pi}\left(s_{2}, a\right)\right| \
&amp; \leqslant\left|\bar{Q}^{\pi}\left(s_{1}, a\right)-\bar{Q}^{\pi}\left(s_{2}, a\right)\right|+2 \frac{\left(L_{\mathcal{R}}^{\mathcal{V}}+\gamma K_{V} L_{\mathcal{P}}^{\mathcal{V}}\right)}{1-\gamma} \text { Applying Lemma } 2 \
&amp; \leqslant K_{V}\left|\phi\left(s_{1}\right)-\phi\left(s_{2}\right)\right|+2 \frac{\left(L_{\mathcal{R}}^{\mathcal{V}}+\gamma K_{V} L_{\mathcal{P}}^{\mathcal{V}}\right)}{1-\gamma} \text { Using the Lipschitz property of } \bar{Q}^{\pi}
\end{aligned}
$$</p>
<p>Theorem 5. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and a $\left(K_{\mathcal{R}}, K_{\mathcal{P}}\right)$-Lipschitz DeepMDP respectively, with an embedding function $\phi$ and global loss functions $L_{\mathcal{R}}^{\mathcal{V}}$ and $L_{\mathcal{P}}^{\mathcal{V}}$. For all $s \in \mathcal{S}$, the suboptimality of the optimal policy $\bar{\pi}^{*}$ of $\overline{\mathcal{M}}$ evaluated on $\mathcal{M}$ can be bounded by:</p>
<p>$$
V^{<em>}(s)-V^{\pi^{</em>}}(s) \leqslant 2 \frac{L_{\mathcal{R}}^{\mathcal{V}}}{1-\gamma}+2 \gamma \frac{K_{\mathcal{R}} L_{\mathcal{P}}^{\mathcal{V}}}{(1-\gamma)\left(1-\gamma K_{\mathcal{P}}\right)}
$$</p>
<p>Proof. This is just a case of the general Theorem 6 combined with the result that the optimal policy of a $\left(K_{\mathcal{R}}, K_{\mathcal{P}}\right)$-Lipschitz DeepMDP is $\frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}$-Lipschitz-valued.</p>
<h2>A.3. Local DeepMDP</h2>
<p>Lemma 3. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, with an embedding function $\phi$. For any $K_{\bar{V}}$-Lipschitzvalued policy $\bar{\pi} \in \bar{\Pi}$, the expected value function difference can be bounded using the local loss functions $L_{\mathcal{R}}^{\xi_{\pi}}$ and $L_{\mathcal{P}}^{\xi_{\pi}}$ measured under $\xi_{\pi}$, the stationary state action distribution of $\bar{\pi}$.</p>
<p>$$
\underset{s, a \sim \xi_{\pi}}{\mathbb{E}}\left|Q^{\pi}(s, a)-\bar{Q}^{\pi}(\phi(s), a)\right| \leqslant \frac{L_{\mathcal{R}}^{\xi_{\pi}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\xi_{\pi}}}{1-\gamma}
$$</p>
<p>Proof. This Lemma is just an example of the general Lemma 8
Theorem 2. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, let $d_{\overline{\mathcal{S}}}$ be the metric in $\overline{\mathcal{S}}$ and $\phi$ be the embedding function. Let $\bar{\pi} \in \bar{\Pi}$ be any $K_{\bar{V}}$-Lipschitz-valued policy with stationary distribution $\xi_{\pi}$, and let $L_{\mathcal{R}}^{\xi_{\pi}}$ and $L_{\mathcal{P}}^{\xi_{\pi}}$ be the local loss functions. For any two states $s_{1}, s_{2} \in \mathcal{S}$, the representation $\phi$ is such that,</p>
<p>$$
\begin{aligned}
\left|V^{\bar{\pi}}\left(s_{1}\right)-V^{\bar{\pi}}\left(s_{2}\right)\right| \leqslant K_{\bar{V}} d_{\overline{\mathcal{S}}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
+\frac{L_{\mathcal{R}}^{\xi_{\pi}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\xi_{\pi}}}{1-\gamma}\left(\frac{1}{d_{\pi}\left(s_{1}\right)}+\frac{1}{d_{\pi}\left(s_{2}\right)}\right)
\end{aligned}
$$</p>
<p>Proof. $\mathbb{E}<em k="k">{s \sim \xi</em>$
Using the fact that $\left|V^{\pi}(s)-\bar{V}^{\pi}(s)\right| \leqslant d_{\pi}^{-1}(s) \mathbb{E}}<em k="k">{s \sim \xi</em>(s)\right|$,}}\left|V^{\pi}(s)-\bar{V}^{\pi</p>
<p>$$
\begin{aligned}
\left|V^{\pi}\left(s_{1}\right)-V^{\pi}\left(s_{2}\right)\right| &amp; \leqslant\left|\bar{V}^{\pi}\left(s_{1}\right)-\bar{V}^{\pi}\left(s_{2}\right)\right|+d_{\pi}^{-1}\left(s_{1}\right) \underset{s \sim \xi_{k}}{\mathbb{E}}\left|V^{\pi}\left(s_{1}\right)-\bar{V}^{\pi}\left(s_{1}\right)\right|+d_{\pi}^{-1}\left(s_{2}\right) \underset{s \sim \xi_{k}}{\mathbb{E}}\left|V^{\pi}\left(s_{2}\right)-\bar{V}^{\pi}\left(s_{2}\right)\right| \
&amp; \leqslant\left|\bar{V}^{\pi}\left(s_{1}\right)-\bar{V}^{\pi}\left(s_{2}\right)\right|+\frac{L_{\mathcal{R}}^{\xi_{k}}+\gamma K_{V} L_{p}^{\xi_{k}}}{1-\gamma}\left(d_{\pi}^{-1}\left(s_{1}\right)+d_{\pi}^{-1}\left(s_{2}\right)\right), \text { Applying Lemma } 3 \
&amp; \leqslant K_{\bar{V}} d_{\bar{\mathcal{S}}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right)+\frac{L_{\mathcal{R}}^{\xi_{k}}+\gamma K_{V} L_{p}^{\xi_{k}}}{1-\gamma}\left(d_{\pi}^{-1}\left(s_{1}\right)+d_{\pi}^{-1}\left(s_{2}\right)\right)
\end{aligned}
$$</p>
<h1>A.4. Connection to Bisimulation</h1>
<p>Lemma 4. Let $\overline{\mathcal{M}}$ be a $K_{\mathcal{R}}$ - $K_{\mathcal{P}}$-Lipschitz DeepMDP with a metric in the state space $d_{\bar{\mathcal{S}}}$. Then the bisimulation metric $\widetilde{d}$ is Lipschitz s.t. $\forall \bar{s}<em 2="2">{1}, \bar{s}</em>$,} \in \overline{\mathcal{S}</p>
<p>$$
\widetilde{d}\left(\bar{s}<em 2="2">{1}, \bar{s}</em>}\right) \leqslant \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} d_{\bar{\mathcal{S}}}\left(\bar{s<em 2="2">{1}, \bar{s}</em>\right)
$$</p>
<p>Proof. We first derive a property of the Wasserstein. Let $d$ and $p$ be pseudometrics in $\chi$ s.t. $d(x, y) \leqslant K p(x, y)$ for all $x, y \in \chi$, and let $P$ and $Q$ be two distributions in $\chi$. Then $W_{d}(P, Q) \leqslant K W_{p}(P, Q)$. To prove it, first note that define the sets of $C$-Lipschitz functions for both metrics:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{F}<em C="C" p_="p,">{d, C}={f: \forall x \neq y \in \chi,|f(x)-f(y)| \leqslant C d(x, y)} \
&amp; \mathcal{F}</em>={f: \forall x \neq y \in \chi,|f(x)-f(y)| \leqslant C p(x, y)}
\end{aligned}
$$</p>
<p>Then it becomes clear that $\mathcal{F}<em K="K" p_="p,">{d, 1} \subseteq \mathcal{F}</em>$. We can now prove the property:</p>
<p>$$
\begin{aligned}
W_{d}(P, Q) &amp; =\sup <em 1="1" d_="d,">{f \in \mathcal{F}</em> f(y)\right| \
&amp; \leqslant \sup }}\left|\underset{x \sim P}{\mathbb{E}} f(x)-\underset{y \sim Q}{\mathbb{E}<em K="K" p_="p,">{f \in \mathcal{F}</em> f(y)\right| \
&amp; =\sup }}\left|\underset{x \sim P}{\mathbb{E}} f(x)-\underset{y \sim Q}{\mathbb{E}<em 1="1" p_="p,">{f \in \mathcal{F}</em> K f(y)\right| \
&amp; =K \sup }}\left|\underset{x \sim P}{\mathbb{E}} K f(x)-\underset{y \sim Q}{\mathbb{E}<em 1="1" p_="p,">{f \in \mathcal{F}</em> f(y)\right| \
&amp; =K W_{p}(P, Q)
\end{aligned}
$$}}\left|\underset{x \sim P}{\mathbb{E}} f(x)-\underset{y \sim Q}{\mathbb{E}</p>
<p>We prove the Lemma by induction. We show that a sequence of pseudometrics values $d_{n}$ converging to $\widetilde{d}$ are all Lipschitz, and that as $n \rightarrow \infty$, their Lipschitz norm goes to $\frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}$. Let $d_{0}\left(s_{1}, s_{2}\right)=0, \forall s_{1}, s_{2} \in \mathcal{S}$ be the base case. Define $d_{n+1}\left(s_{1}, s_{2}\right)=F_{d}\left(s_{1}, s_{2}\right)$ as defined in Definition 5. Ferns et al. (2011) shows that $F$ is a contraction, and that $d_{n}$ converges</p>
<p>to $\widetilde{d}$ as $n \rightarrow \infty$. Now let $K_{d, n}=\sup <em 1="1">{s</em>=0$. Then,} \neq s_{2} \in \mathcal{S}} \frac{d_{n}\left(s_{1}, s_{2}\right)}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)}$ be the Lipschitz norm of $d_{n}$. Also see that $K_{d, 0</p>
<p>$$
\begin{aligned}
K_{d, n+1} &amp; =\sup <em 1="1">{s</em> \
&amp; =\sup } \neq s_{2} \in \mathcal{S}} \frac{d_{n+1}\left(s_{1}, s_{2}\right)}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)<em 1="1">{s</em> \
&amp; \leqslant(1-\gamma) \sup } \neq s_{2} \in \mathcal{S}} \frac{F_{d_{n}}\left(s_{1}, s_{2}\right)}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)<em 1="1">{a \in \mathcal{A}, s</em>+\gamma \sup } \neq s_{2} \in \mathcal{S}} \frac{\left|\mathcal{R}\left(s_{1}, a\right)-\mathcal{R}\left(s_{2}, a\right)\right|}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)<em 1="1">{a \in \mathcal{A}, s</em> \
&amp; =(1-\gamma) K_{\mathcal{R}}+\gamma \sup } \neq s_{2} \in \mathcal{S}} \frac{W_{d_{n}}\left(\mathcal{P}\left(\cdot \mid s_{2}, a\right), \mathcal{P}\left(\cdot \mid s_{2}, a\right)\right)}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)<em 1="1">{a \in \mathcal{A}, s</em> \
&amp; \leqslant(1-\gamma) K_{\mathcal{R}}+\gamma K_{d, n} \sup } \neq s_{2} \in \mathcal{S}} \frac{W_{d_{n}}\left(\mathcal{P}\left(\cdot \mid s_{2}, a\right), \mathcal{P}\left(\cdot \mid s_{2}, a\right)\right)}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)<em 1="1">{a \in \mathcal{A}, s</em> \
&amp; \leqslant(1-\gamma) K_{\mathcal{R}}+\gamma K_{d, n} K_{\mathcal{P}} \
&amp; \leqslant(1-\gamma) \sum_{i=0}^{n-1}\left(\gamma K_{\mathcal{P}}\right)^{i} K_{\mathcal{R}}, \text { (by expanding the recursion) }
\end{aligned}
$$} \neq s_{2} \in \mathcal{S}} \frac{W_{d_{\mathcal{S}}}\left(\mathcal{P}\left(\cdot \mid s_{2}, a\right), \mathcal{P}\left(\cdot \mid s_{2}, a\right)\right)}{d_{\mathcal{S}}\left(s_{1}, s_{2}\right)}, \text { (Using the property derived above.) </p>
<p>Thus, even as $n \rightarrow \infty, K_{d, n} \leqslant \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}$.
Lemma 5. Let $\mathcal{M}$ be an MDP and $\overline{\mathcal{M}}$ be a $K_{\overline{\mathcal{R}}} K_{\overline{\mathcal{P}}}$-Lipschitz MDP with an embedding function $\phi: \mathcal{S} \rightarrow \overline{\mathcal{S}}$ and global DeepMDP losses $L_{\mathcal{P}}^{\infty}$ and $L_{\overline{\mathcal{R}}}^{\infty}$. We can extend the bisimulation metric to also measure a distance between $s \in \mathcal{S}$ and $\bar{s} \in \overline{\mathcal{S}}$ by considering an joined MDP constructed by joining $\mathcal{M}$ and $\overline{\mathcal{M}}$. When an action is taken, each state will transition according to the transition function of its corresponding MDP. Then the bisimulation metric between a state $s \in \mathcal{S}$ and it's embedded counterpart $\phi(s)$ is bounded by:</p>
<p>$$
\widetilde{d}(s, \phi(s)) \leqslant L_{\overline{\mathcal{R}}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\overline{\mathcal{R}}}}{1-\gamma K_{\overline{\mathcal{P}}}}
$$</p>
<p>Proof. First, note that</p>
<p>$$
\begin{aligned}
W_{\tilde{d}}(\phi \mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}}(\cdot \mid \phi(s), a)) &amp; =\sup <em _tilde_d="\tilde{d">{f \bar{s}</em>}}^{\prime}} \underset{\bar{s<em 1="1">{1}^{\prime} \sim \phi \overline{\mathcal{P}}(\cdot \mid s, a)}{\mathbb{E}}\left[f\left(\bar{s}</em>}^{\prime}\right)\right]-\underset{\bar{s<em 2="2">{2}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \phi(s), a)}{\mathbb{E}}\left[f\left(\bar{s}</em>\right)\right] \
&amp; \leqslant \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} \sup }^{\prime<em _tilde_d="\tilde{d">{f \bar{s}</em>}}^{\prime} \mid} \underset{\bar{s<em 1="1">{1}^{\prime} \sim \phi \overline{\mathcal{P}}(\cdot \mid s, a)}{\mathbb{E}}\left[f\left(\bar{s}</em>}^{\prime}\right)\right]-\underset{\bar{s<em 2="2">{2}^{\prime} \sim \overline{\mathcal{P}}(\cdot \mid \phi(s), a)}{\mathbb{E}}\left[f\left(\bar{s}</em> \
&amp; =\frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} W_{\ell_{2}}\left(\mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}}(\cdot \mid \phi(s), a)\right) \
&amp; \leqslant \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} L_{\mathcal{P}}^{\infty}
\end{aligned}
$$}^{\prime}\right)\right] \quad \text { (Using Theorem 4) </p>
<p>Using the triangle inequality of pseudometrics and the previous derivation:</p>
<p>$$
\begin{aligned}
\sup <em _in="\in" _mathcal_A="\mathcal{A" a="a">{s} \widetilde{d}(s, \phi(s)) &amp; =\max </em>(\cdot \mid \phi(s), a)\right)\right) \
&amp; \leqslant(1-\gamma) L_{\overline{\mathcal{R}}}^{\infty}+\gamma \max }}\left((1-\gamma)\left|\mathcal{R}(s, a)-\overline{\mathcal{R}}(\phi(s), a)\right|+\gamma W_{\tilde{d}}\left(\mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}<em _tilde_d="\tilde{d">{a \in \mathcal{A}}\left(W</em>(\cdot \mid \phi(s), a)\right)\right) \
&amp; \leqslant(1-\gamma) L_{\overline{\mathcal{R}}}^{\infty}+\gamma \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} L_{\mathcal{P}}^{\infty}+\gamma \max }}\left(\mathcal{P}(\cdot \mid s, a), \phi \mathcal{P}(\cdot \mid s, a)\right)+W_{\tilde{d}}\left(\phi \mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}<em _tilde_d="\tilde{d">{a \in \mathcal{A}} W</em>(\cdot \mid s, a)\right) \
&amp; \leqslant(1-\gamma) L_{\overline{\mathcal{R}}}^{\infty}+\gamma \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} L_{\mathcal{P}}^{\infty}+\gamma \sup _{s} \widetilde{d}\left(s^{\prime}, \phi(s)\right)
\end{aligned}
$$}}\left(\mathcal{P}(\cdot \mid s, a), \phi \mathcal{P</p>
<p>Solving for the recurrence leads to the desired result.</p>
<p>Theorem 3. Let $\mathcal{M}$ be an MDP and $\widetilde{\mathcal{M}}$ be a $K_{\mathcal{R}}-K_{\mathcal{P}}$-Lipschitz DeepMDP with metric $d_{\mathcal{S}}$. Let $\phi$ be the embedding function and $L_{\mathcal{P}}^{\infty}$ and $L_{\mathcal{R}}^{\infty}$ be the global DeepMDP losses. The bisimulation distance in $\mathcal{M}, \widetilde{d}: \mathcal{S} \times \mathcal{S} \rightarrow \mathbb{R}^{+}$can be upperbounded by the $\ell_{2}$ distance in the embedding and the losses in the following way:</p>
<p>$$
\begin{aligned}
\widetilde{d}\left(s_{1}, s_{2}\right) \leqslant \frac{(1-\gamma) K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}} d_{\mathcal{S}}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) &amp; \
&amp; +2\left(L_{\mathcal{R}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}\right)
\end{aligned}
$$</p>
<p>Proof.</p>
<p>$$
\begin{aligned}
\widetilde{d}\left(s_{1}, s_{2}\right) &amp; \leqslant \widetilde{d}\left(s_{1}, \phi\left(s_{1}\right)\right)+\widetilde{d}\left(s_{2}, \phi\left(s_{2}\right)\right)+\widetilde{d}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
&amp; \leqslant 2\left(L_{\mathcal{R}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}\right)+\widetilde{d}\left(\phi\left(s_{1}\right), \phi\left(s_{2}\right)\right) \
&amp; \leqslant 2\left(L_{\mathcal{R}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}\right)+\frac{(1-\gamma) L_{\mathcal{R}}^{\infty}}{1-\gamma K_{\mathcal{P}}}\left|\phi\left(s_{1}\right)-\phi\left(s_{2}\right)\right|
\end{aligned}
$$</p>
<p>(Using Theorem 5)</p>
<p>Completing the proof.</p>
<h1>A.5. Quality of $\widetilde{\Pi}$</h1>
<p>Lemma 6. Let $d_{f}$ and $d_{g}$ be the metrics on the space $\chi$, with the property that for some $\epsilon \geqslant 0$ it holds that $\forall x, y \in$ $\chi, d_{f}(x, y) \leqslant \epsilon+d_{g}(x, y)$. Define the sets of 1-Lipschitz functions $\mathbb{F}=\left{f:|f(x)-f(y)| \leqslant d_{f}(x, y), \forall x, y \in \chi\right}$ and $\mathbb{G}=\left{g:|g(x)-g(y)| \leqslant d_{g}(x, y), \forall x, y \in \chi\right}$. Then for any $f \in \mathbb{F}$, there exists one $g \in \mathbb{G}$ such that for all $x \in \chi$,</p>
<p>$$
|f(x)-g(x)| \leqslant \frac{\epsilon}{2}
$$</p>
<p>Proof. Define the set $\mathbb{Z}=\left{z:|z(x)-z(y)| \leqslant \epsilon+d_{g}(x, y), \forall x, y \in \chi\right}$. Then trivially, any function $f \in \mathbb{F}$ is also a member of $\mathbb{Z}$. We now show that the set $\mathbb{Z}$ can equivalently be expressed as $z(x)=g(x)+u(x)$, where $g \in \mathbb{G}$ and $u(x) \in\left(\frac{-\epsilon}{2}, \frac{\epsilon}{2}\right)$, is (non Lipschitz) bounded function.</p>
<p>$$
\begin{aligned}
|z(x)-z(y)| &amp; =|g(x)+u(x)-g(y)-u(y)| \
&amp; \leqslant|g(x)-g(y)|+|u(x)-u(y)| \
&amp; \leqslant d_{g}(x, y)+\epsilon
\end{aligned}
$$</p>
<p>Note how both inequalities are tight (there is a $g$ and $u$ for which the equality holds), together with the fact that the set $\mathbb{Z}$ is convex, it follows that any $z \in \mathbb{Z}$ must be expressible as $g(x)+u(x)$.
We now complete the proof. For any $z \in \mathbb{Z}$, there exist a $g \in \mathbb{G}$ s.t. $z(x)=g(x)+u(x)$. Then:</p>
<p>$$
|z(x)-g(x)|=|u(x)| \leqslant \frac{\epsilon}{2}
$$</p>
<p>Theorem 4. Let $\mathcal{M}$ be an MDP and $\widetilde{\mathcal{M}}$ be a ( $K_{\mathcal{R}}, K_{\mathcal{P}}$ )-Lipschitz DeepMDP, with an embedding function $\phi$ and global loss functions $L_{\mathcal{R}}^{\infty}$ and $L_{\mathcal{P}}^{\infty}$. Denote by $\widetilde{\Pi}<em K="K">{K}$ and $\widetilde{\Pi}</em>}$ the sets of Lipschitz-bisimilar and Lipschitz-deep policies. Then for any $\widetilde{\pi} \in \widetilde{\Pi<em C="C" K="K">{K}$ there exists a $\widetilde{\pi} \in \widetilde{\Pi}</em>$,}$ which is close to $\widetilde{\pi}$ in the sense that, for all $s \in \mathcal{S}$ and $a \in \mathcal{A</p>
<p>$$
|\widetilde{\pi}(a \mid s)-\bar{\pi}(a \mid s)| \leqslant L_{\mathcal{R}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}
$$</p>
<p>Proof. The proof is based on Lemma 6. Let $\chi=\mathcal{S}, d_{f}(x, y)=K \widetilde{d}(x, y), d_{g}(x, y)=K C|\phi(x)-\phi(y)|$ and $\epsilon=$ $2\left(L_{\mathcal{R}}^{\infty}+\gamma L_{\mathcal{P}}^{\infty} \frac{K_{\mathcal{R}}}{1-\gamma K_{\mathcal{P}}}\right)$. Theorem 3 can be used to show that the condition $d_{f}(x, y) \leqslant \epsilon+d_{g}(x, y)$ holds. Then the application of Lemma 6 provides the desired result.</p>
<h1>A.6. Generalized Value Difference Bounds</h1>
<p>Lemma 7. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, with an embedding function $\phi$ and global loss functions $L_{\mathcal{R}}^{\mathcal{Z}}$ and $L_{\mathcal{P}}^{\mathcal{Z}}$, where $L_{\mathcal{P}}^{\mathcal{Z}}$ uses on a Norm-MMD $\mathcal{D}$. For any $K_{\bar{V}}$-smooth-valued policy $\bar{\pi} \in \overline{\Pi}$ as in Definition 7. The value difference can be bounded by</p>
<p>$$
\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right| \leqslant \frac{L_{\mathcal{R}}^{\mathcal{Z}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\mathcal{Z}}}{1-\gamma}
$$</p>
<p>Proof. The proof consists of showing that the supremum $\sup _{s, a}\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right|$ is bounded by a recurrence relationship.</p>
<p>$$
\begin{aligned}
\sup <em _in="\in" _mathcal_S="\mathcal{S" s="s">{s \in \mathcal{S}, a \in \mathcal{A}}\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right| &amp; \leqslant \sup </em>(\phi(s), a)|+\gamma \sup }, a \in \mathcal{A}}|\mathcal{R}(s, a)-\overline{\mathcal{R}<em _mathcal_R="\mathcal{R">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}} V^{\bar{\pi}}\left(s^{\prime}\right)-\underset{\bar{s}^{\prime} \sim \mathcal{P}(\cdot \mid \phi(s), a)}{\mathbb{E}} \bar{V}^{\bar{\pi}}\left(\bar{s}^{\prime}\right)\right| \
&amp; =L</em>+\gamma \sup }}^{\mathcal{Z}<em _mathcal_R="\mathcal{R">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}}\left[V^{\bar{\pi}}\left(s^{\prime}\right)-\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)\right]+\underset{\bar{s}^{\prime} \sim \mathcal{P}(\cdot \mid \phi(s), a)}{\mathbb{E}}\left[\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)-\bar{V}^{\bar{\pi}}\left(\bar{s}^{\prime}\right)\right]\right.\right. \
&amp; \leqslant L</em>+\gamma \sup }}^{\mathcal{Z}<em _in="\in" _mathcal_S="\mathcal{S" s="s">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}}\left[V^{\bar{\pi}}\left(s^{\prime}\right)-\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)\right]\right|+\gamma \sup </em>\right)\right]\right| \
&amp; \leqslant L_{\mathcal{R}}^{\mathcal{Z}}+\gamma \sup }, a \in \mathcal{A}}\left|\underset{s^{\prime} \sim \mathcal{P}(\cdot \mid \phi(s), a)}{\mathbb{E}} \underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}}\left[\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)-\bar{V}^{\bar{\pi}}\left(\bar{s}^{\prime<em _bar_V="\bar{V">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}}\left[V^{\bar{\pi}}\left(s^{\prime}\right)-\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)\right]\right|+\gamma K</em> \sup }<em _mathcal_R="\mathcal{R">{s \in \mathcal{S}, a \in \mathcal{A}} \mathcal{D}(\phi \mathcal{P}(\cdot \mid s, a), \overline{\mathcal{P}}(\cdot \mid \phi(s), a)) \
&amp; =L</em>+\gamma \sup }}^{\mathcal{Z}<em _bar_V="\bar{V">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}}\left[V^{\bar{\pi}}\left(s^{\prime}\right)-\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)\right]\right|+\gamma K</em> \
&amp; \leqslant L_{\mathcal{R}}^{\mathcal{Z}}+\gamma \sup }} L_{\mathcal{P}}^{\mathcal{Z}} \text { Using Jensen's inequality. <em _bar_V="\bar{V">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\left[V^{\bar{\pi}}(s)-\bar{V}^{\bar{\pi}}(\phi(s))\right]\right|+\gamma K</em> \
&amp; \leqslant L_{\mathcal{R}}^{\mathcal{Z}}+\gamma \sup }} L_{\mathcal{P}}^{\mathcal{Z}<em _bar_V="\bar{V">{s \in \mathcal{S}, a \in \mathcal{A}} \underset{s^{\prime} \sim \mathcal{P}(\cdot \mid s, a)}{\mathbb{E}}\left|\left[V^{\bar{\pi}}\left(s^{\prime}\right)-\bar{V}^{\bar{\pi}}\left(\phi\left(s^{\prime}\right)\right)\right]\right|+\gamma K</em> \
&amp; \leqslant L_{\mathcal{R}}^{\mathcal{Z}}+\gamma \sup }} L_{\mathcal{P}}^{\mathcal{Z}} \text { Using Jensen's inequality. <em _bar_V="\bar{V">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\left[V^{\bar{\pi}}(s)-\bar{V}^{\bar{\pi}}(\phi(s))\right]\right|+\gamma K</em> \
&amp; \leqslant L_{\mathcal{R}}^{\mathcal{Z}}+\gamma \sup }} L_{\mathcal{P}}^{\mathcal{Z}<em _bar_V="\bar{V">{s \in \mathcal{S}, a \in \mathcal{A}}\left|\left[Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right]\right|+\gamma K</em>
\end{aligned}
$$}} L_{\mathcal{P}}^{\mathcal{Z}</p>
<p>Solving for the recurrence relation over $\sup _{s \in \mathcal{S}, a \in \mathcal{A}}\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right|$ results in the desired result.</p>
<p>Lemma 8. Let $\mathcal{M}$ and $\overline{\mathcal{M}}$ be an MDP and DeepMDP respectively, with an embedding function $\phi$ and let $\mathcal{D}$ be a Norm-MMD metric. For any $K_{\bar{V}}$-smooth-valued policy $\bar{\pi} \in \overline{\Pi}$ (as in Definition 7), let $L_{\mathcal{R}}^{\xi_{\pi}}$ and $L_{\mathcal{P}}^{\xi_{\pi}}$ be the local loss functions measured under $\xi_{\bar{\pi}}$, the stationary state action distribution of $\bar{\pi}$. Then the value difference can be bounded by:</p>
<p>$$
\underset{s, a \sim \xi_{\bar{\pi}}}{\mathbb{E}}\left|Q^{\bar{\pi}}(s, a)-\bar{Q}^{\bar{\pi}}(\phi(s), a)\right| \leqslant \quad \frac{L_{\mathcal{R}}^{\xi_{\pi}}+\gamma K_{\bar{V}} L_{\mathcal{P}}^{\xi_{\pi}}}{1-\gamma}
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ For control, searching over these policies increases the size of the search space with no benefits on the optimality of the solution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6} \mathrm{~A}$ seminorm $|\cdot|<em _mathcal_D="\mathcal{D">{\mathcal{D}}$ is a norm except that $|f|</em>=0 \Rightarrow f=0$.&#160;}<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
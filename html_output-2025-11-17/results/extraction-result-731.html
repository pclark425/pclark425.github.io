<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-731 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-731</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-731</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-70c8cf3cbe8cd7dfa33952533b81dd033afc4afd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/70c8cf3cbe8cd7dfa33952533b81dd033afc4afd" target="_blank">ChatGPT Code Detection: Techniques for Uncovering the Source of Code</a></p>
                <p><strong>Paper Venue:</strong> Applied Informatics</p>
                <p><strong>Paper TL;DR:</strong> This study uses advanced classification techniques to differentiate between code written by humans and code generated by ChatGPT, a type of LLM, using a new approach that combines powerful embedding features (black-box) with supervised learning algorithms including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting with impressive accuracy.</p>
                <p><strong>Paper Abstract:</strong> In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas such as higher education. The present paper explores this issue by using advanced classification techniques to differentiate between code written by humans and code generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well, but provide at most 85–88% accuracy. Tests on a small sample of untrained humans suggest that humans do not solve the task much better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e731.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e731.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formatting bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code formatting dependence / formatting-induced feature differences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrepancy where models exploit differences in code formatting between natural-language problem descriptions/expectations and actual code, causing degraded generalization when formatting is standardized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code-detection experimental pipeline (preprocessing / feature extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that ingests problem descriptions and code solutions (human and GPT), applies optional automatic code formatting (Black), creates embeddings or hand-crafted features, and trains classifiers to detect code origin.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper hypothesis / experimental assumption about importance of formatting</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data preprocessing / formatting script (Black formatter) and feature extraction code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>formatting bias / format-dependent feature reliance</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Models trained on unformatted code can rely heavily on formatting differences (whitespace, indentation style, trailing spaces, line breaks) as distinguishing features; when code is normalized with a formatter, many of these cues disappear, reducing model performance. The paper hypothesized formatting would be a minor cause (H2); experiments show formatting reduces accuracy notably, so reliance on formatting is a real gap between naive NL expectation and implementation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / feature space (human-designed features and some embedding artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Controlled experiment comparing classifiers trained/evaluated on unformatted vs formatted datasets (using Black formatter) and inspection of feature distributions (boxplots) and model performance tables.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified by classification metrics (accuracy, AUC, precision, recall, F1) reported for both conditions. Examples: XGB+TF-IDF accuracy decreased from 98.28% (unformatted) to 93.98% (formatted) (~4.3 percentage points); human-designed XGB dropped from 88.48% to 80.29% (~8.2 points). AUC drops were smaller (e.g., ~1 percentage point for best models).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Standardizing formatting reduces spurious stylistic signal, lowering apparent separability but improving robustness; unformatted pipelines likely overestimate detectability due to formatting cues. Quantitatively, best-model accuracy dropped ~4 percentage points after formatting, and white-box feature models degraded up to ~8 points.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently in this study across feature sets and classifiers; not a one-off — formatting alignment noticeably changed feature distributions (e.g., leading/trailing whitespace counts converged).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Models (especially using shallow human-designed features) exploit superficial formatting differences that are not semantically meaningful; formatting differences between human and AI outputs are an artifact not intrinsic to problem semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Normalize all code samples with a canonical formatter (Black) prior to feature extraction and model training; emphasize embedding-based features that capture deeper semantic patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Formatting reduced formatting-derived signal (as intended) and lowered accuracy only slightly for best embedding models (best embedding models remained high-performing: e.g., ADA/DNN and TF-IDF/XGB remained >90% even after formatting). For white-box features, mitigation reduced spurious signals but also reduced accuracy more strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / program analysis (code forensic detection)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e731.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e731.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-output mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguity between prompt specifications (natural language) and generated code outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch where different natural-language prompt formulations for ChatGPT produced different output forms (explanations vs. pure code), affecting the ability to generate usable code datasets and biasing downstream experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated code generation subsystem (ChatGPT API prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Data-generation component that sends problem-text prompts to gpt-3.5-turbo and extracts returned code for inclusion in training/evaluation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experiment protocol / prompt specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>API call scripts and prompt templates used to generate code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous prompt / specification mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some prompt variants caused ChatGPT to return natural-language explanations or mixed-format answers instead of pure code snippets; intended NL instruction 'provide Python code' did not always result in code-only outputs, meaning the natural-language prompt specification did not deterministically map to the expected code implementation output.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data generation / input prompt specification</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical observation during data generation: trying multiple prompt templates and manually inspecting outputs; tracking fraction of API responses that contained usable Python code.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured by usable-code yield per prompt and by successful-solution rate: single API generation success rate observed 21.3% (usable correct code); after five API calls per problem success rate rose to 45.6% (approximate), closer to OpenAI-reported 48.1% zero-shot figure.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduced number of usable GPT-generated samples and biased the dataset toward particular outputs; required multiple calls per problem (increased API cost/time) and may introduce selection bias (keeping only particular outputs that pass tests). Quantitatively, single-call usable-code yield (~21.3%) was much lower than multi-call yield (~45.6%), impacting sample sizes and representativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across attempted prompts during dataset creation; certain phrasing repeatedly produced non-code outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>LLM behavior sensitivity to prompt phrasing and internal instruction-following heuristics; ambiguous or under-constraining prompts allow the model to include explanatory text.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Prompt engineering to enforce code-only responses (adopted prompt: 'Question: <Coding_Task_Description> Please provide your answer as Python code. Answer:'), repeated generation attempts per problem and filtering outputs by executing test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Using the chosen prompt plus multiple generations increased the empirical success rate from ~21.3% to ~45.6%; however this required extra API calls and did not fully close the gap to published figures for single-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / data collection for empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e731.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e731.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spec-vs-solution functional gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between natural-language problem specification and generated implementations (functional correctness gap)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Many GPT-generated solutions did not satisfy the problem's test cases, revealing a gap between natural-language problem descriptions and the model-produced code implementations' correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dataset validation pipeline (test-case execution gating)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Post-processing pipeline that executes human and GPT solutions against provided problem-specific test cases to ensure syntactic and semantic correctness before inclusion in the labeled dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>problem statement / task specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>generated solution code (scripts/functions)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>functional correctness gap / specification nonconformance</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>A large fraction of GPT outputs fail to satisfy the functional requirements encoded in the problem's test cases; executing the test-suite removed many GPT samples (and some human samples) from the dataset because outputs were incorrect or non-executable.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>solution correctness verification / evaluation stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Automatic execution of attached test cases against each solution; removal of any solution that fails tests or is non-executable.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported by percent of samples removed by test-case filtering: GPT samples reduced by 72.39%, human samples reduced by 28.59%; overall dataset shrank substantially (from ~314k to ~171k after filtering), and final balanced dataset contained ~31.4k samples.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Severe reduction in usable GPT-generated data, potential sampling bias (only successful GPT outputs retained), increased difficulty in constructing balanced datasets, and possible inflation/deflation of detection model performance depending on which outputs survive filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Substantial in this dataset (majority of GPT outputs filtered out), indicating this is a common phenomenon when applying code-generation models to realistic programming tasks with strict test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>LLM limitations in reliably producing fully correct, problem-constrained code in a single shot; ambiguity or incompleteness in problem descriptions can exacerbate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Generate multiple candidate solutions per problem (multiple API calls), filter by executing test cases, and balance datasets per problem using the minimum count of valid human/GPT solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Multiple generations increased the observed success rate to ~45.6% (from ~21.3% on single generation), but test-case filtering still removed a majority of GPT responses in aggregate (72.39% removed).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>empirical ML evaluation / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e731.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e731.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Related-work reporting ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ambiguous evaluation protocol reporting in prior studies (possible train/test leakage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that some prior work reporting strong detection performance did not clearly specify whether fine-tuning and evaluation split data by problem or by sample, which can cause misleadingly optimistic results through leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evaluation methodology in related studies</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Protocols used by other researchers to fine-tune detectors and evaluate performance on code-origin classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods section / experimental protocol of cited papers</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>training and evaluation scripts (not available/unclear in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous evaluation protocol / possible data leakage between train and test</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Wang et al. reported improved AUC after fine-tuning a text detector on code, but the paper notes it is unclear whether fine-tuning and evaluation used samples from the same coding problems (sample-wise split) or held-out problems (problem-wise split). If sample-wise splits were used, near-duplicate solutions for the same problem could have leaked into both train and test sets, inflating performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental design / dataset splitting</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Critical reading of related-work methods and comparison to best-practice problem-wise splitting used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No direct re-measurement in this paper; the concern is qualitative and based on lack of reporting. The paper contrasts this with their own problem-wise 80/20 split to avoid leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potential overestimation of generalization performance in cited studies; results may not hold on unseen problems. The paper emphasizes this as a reason prior high AUC claims may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified; arises when authors do not explicitly report split strategy and when datasets contain many similar solutions per problem.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Incomplete methodological reporting in prior work and possible convenience-driven use of sample-wise splitting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt and explicitly report problem-wise train/test splits (as done in this paper), repeat experiments over multiple random seeds, and ensure no near-duplicate problem solutions cross the split.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not directly evaluated here for prior works; this paper asserts problem-wise splitting is standard practice to improve validity and reduce leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>empirical ML evaluation / experimental reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e731.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e731.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL-detector domain mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between natural-language detectors' claims and their applicability to code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural-language text detectors (e.g., DetectGPT, GPTZero) perform well on NL text but, when applied directly to code, show poor performance unless adapted or fine-tuned for code, revealing a domain transfer gap between NL descriptions of detector capability and actual implementation results on code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cross-domain detector application (text detectors applied to code)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applying detectors designed and validated on natural language text to code snippets without adaptation (zero-shot), and evaluating their performance on code-origin classification.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>tool capability claims / method description in original detector papers</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>detector application scripts (zero-shot or fine-tuned on code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>domain transfer gap / method mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Text detectors that report >90% accuracy on NL text drop to AUC ~0.40-0.50 on code when used as-is; some studies improved performance by fine-tuning on code, but generalization remains questionable. DetectGPT required code-specific modifications (DetectGPT4Code) to be marginally useful on code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model selection / adaptation stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical evaluation reported in related work: applying off-the-shelf NL detectors to code and measuring AUC/accuracy; comparisons with code-specific adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported performance metrics from cited studies (e.g., Wang et al. AUC 0.40-0.50 for text detectors on code; DetectGPT4Code AUC 0.70-0.80 on small code sets; after fine-tuning some detectors saw AUC improvements to 0.77-0.98 but with unclear generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Direct application of NL detectors to code is unreliable; without adaptation results can be near-random, leading to false confidence if NL claims are naively assumed transferable.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple cited works; a common pitfall when crossing text-to-code domains.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Different statistical and syntactic properties of code vs natural language (tokenization, structure, determinism, semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use code-aware perturbation models, code-specific embeddings, fine-tune detectors on held-out problems, or design detectors specifically for code (e.g., DetectGPT4Code modifications).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial improvements reported (DetectGPT4Code AUC 0.70-0.80 on small datasets; this paper shows embedding-based supervised classifiers achieving up to 98% accuracy), but fine-tuning must be carefully evaluated with problem-wise splits to ensure generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / NLP-to-code transfer</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e731.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e731.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probability calibration mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model probability calibration issues (overconfident / bimodal outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch where classifier predicted probabilities do not reflect empirical uncertainty because many models output probabilities very close to 0 or 1, producing unstable calibration estimates in intermediate ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Model evaluation & calibration subsystem</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Component that computes predicted class probabilities for classifiers and evaluates calibration via LOESS calibration plots and kernel density estimates of predicted probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>evaluation expectations about predicted probabilities representing reliable uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model training and probability-prediction code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>probability calibration mismatch / overconfidence</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some high-performing models (DNNs, GMMs) produce near-deterministic probabilities (close to 0 or 1) for most samples, leaving few predictions in the mid-range; LOESS-based calibration curves are therefore unstable and may give a misleading impression of calibration in intermediate probability ranges. In contrast, some models (XGB+TF-IDF / XGB+ADA) show near-perfect calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model output / evaluation stage (probability estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Calibration plots (LOESS-smoothed predicted-probability vs empirical frequency) and kernel density estimates of predicted probabilities per model.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Visual and statistical analysis: LOESS calibration curves compared to the diagonal; kernel density plots reveal whether predictions are concentrated at extremes. The paper reports that some models produce almost exclusively extreme probabilities and that calibration curves in 0.15–0.85 range are unreliable due to sparse data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Overconfident predictions reduce the usefulness of predicted probabilities for downstream decision-making (e.g., human review thresholds) and make probabilistic interpretation unsafe; models can be 'certain' while still being wrong on some samples.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in several embedding-based DNNs and GMMs in this study; not universal across models (XGB variants were well-calibrated).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model architectures and training objectives that optimize classification accuracy (often with margin) without explicit calibration, coupled with class-separable embeddings that permit confident predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Prefer models with better inherent calibration (e.g., XGB+TF-IDF), apply post-hoc calibration methods (Platt scaling, isotonic regression) or avoid relying solely on predicted probabilities where models are overconfident.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>XGB+TF-IDF and XGB+ADA demonstrated nearly perfect calibration and high accuracy in this study; for other models, calibration curves were unstable and not fully trustworthy without more mid-range predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>statistical ML evaluation / classifier calibration</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGPT Code Detection: Techniques for Uncovering the Source of Code', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DetectGPT <em>(Rating: 2)</em></li>
                <li>DetectGPT4Code <em>(Rating: 2)</em></li>
                <li>A study by Wang et al. comparing text detectors on code <em>(Rating: 1)</em></li>
                <li>Pan et al. comparison of text detectors on code <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-731",
    "paper_id": "paper-70c8cf3cbe8cd7dfa33952533b81dd033afc4afd",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Formatting bias",
            "name_full": "Code formatting dependence / formatting-induced feature differences",
            "brief_description": "Discrepancy where models exploit differences in code formatting between natural-language problem descriptions/expectations and actual code, causing degraded generalization when formatting is standardized.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Code-detection experimental pipeline (preprocessing / feature extraction)",
            "system_description": "Pipeline that ingests problem descriptions and code solutions (human and GPT), applies optional automatic code formatting (Black), creates embeddings or hand-crafted features, and trains classifiers to detect code origin.",
            "nl_description_type": "paper hypothesis / experimental assumption about importance of formatting",
            "code_implementation_type": "data preprocessing / formatting script (Black formatter) and feature extraction code",
            "gap_type": "formatting bias / format-dependent feature reliance",
            "gap_description": "Models trained on unformatted code can rely heavily on formatting differences (whitespace, indentation style, trailing spaces, line breaks) as distinguishing features; when code is normalized with a formatter, many of these cues disappear, reducing model performance. The paper hypothesized formatting would be a minor cause (H2); experiments show formatting reduces accuracy notably, so reliance on formatting is a real gap between naive NL expectation and implementation outcomes.",
            "gap_location": "data preprocessing / feature space (human-designed features and some embedding artifacts)",
            "detection_method": "Controlled experiment comparing classifiers trained/evaluated on unformatted vs formatted datasets (using Black formatter) and inspection of feature distributions (boxplots) and model performance tables.",
            "measurement_method": "Quantified by classification metrics (accuracy, AUC, precision, recall, F1) reported for both conditions. Examples: XGB+TF-IDF accuracy decreased from 98.28% (unformatted) to 93.98% (formatted) (~4.3 percentage points); human-designed XGB dropped from 88.48% to 80.29% (~8.2 points). AUC drops were smaller (e.g., ~1 percentage point for best models).",
            "impact_on_results": "Standardizing formatting reduces spurious stylistic signal, lowering apparent separability but improving robustness; unformatted pipelines likely overestimate detectability due to formatting cues. Quantitatively, best-model accuracy dropped ~4 percentage points after formatting, and white-box feature models degraded up to ~8 points.",
            "frequency_or_prevalence": "Observed consistently in this study across feature sets and classifiers; not a one-off — formatting alignment noticeably changed feature distributions (e.g., leading/trailing whitespace counts converged).",
            "root_cause": "Models (especially using shallow human-designed features) exploit superficial formatting differences that are not semantically meaningful; formatting differences between human and AI outputs are an artifact not intrinsic to problem semantics.",
            "mitigation_approach": "Normalize all code samples with a canonical formatter (Black) prior to feature extraction and model training; emphasize embedding-based features that capture deeper semantic patterns.",
            "mitigation_effectiveness": "Formatting reduced formatting-derived signal (as intended) and lowered accuracy only slightly for best embedding models (best embedding models remained high-performing: e.g., ADA/DNN and TF-IDF/XGB remained &gt;90% even after formatting). For white-box features, mitigation reduced spurious signals but also reduced accuracy more strongly.",
            "domain_or_field": "machine learning / program analysis (code forensic detection)",
            "reproducibility_impact": true,
            "uuid": "e731.0",
            "source_info": {
                "paper_title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Prompt-output mismatch",
            "name_full": "Ambiguity between prompt specifications (natural language) and generated code outputs",
            "brief_description": "Mismatch where different natural-language prompt formulations for ChatGPT produced different output forms (explanations vs. pure code), affecting the ability to generate usable code datasets and biasing downstream experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Automated code generation subsystem (ChatGPT API prompting)",
            "system_description": "Data-generation component that sends problem-text prompts to gpt-3.5-turbo and extracts returned code for inclusion in training/evaluation datasets.",
            "nl_description_type": "experiment protocol / prompt specification",
            "code_implementation_type": "API call scripts and prompt templates used to generate code",
            "gap_type": "ambiguous prompt / specification mismatch",
            "gap_description": "Some prompt variants caused ChatGPT to return natural-language explanations or mixed-format answers instead of pure code snippets; intended NL instruction 'provide Python code' did not always result in code-only outputs, meaning the natural-language prompt specification did not deterministically map to the expected code implementation output.",
            "gap_location": "data generation / input prompt specification",
            "detection_method": "Empirical observation during data generation: trying multiple prompt templates and manually inspecting outputs; tracking fraction of API responses that contained usable Python code.",
            "measurement_method": "Measured by usable-code yield per prompt and by successful-solution rate: single API generation success rate observed 21.3% (usable correct code); after five API calls per problem success rate rose to 45.6% (approximate), closer to OpenAI-reported 48.1% zero-shot figure.",
            "impact_on_results": "Reduced number of usable GPT-generated samples and biased the dataset toward particular outputs; required multiple calls per problem (increased API cost/time) and may introduce selection bias (keeping only particular outputs that pass tests). Quantitatively, single-call usable-code yield (~21.3%) was much lower than multi-call yield (~45.6%), impacting sample sizes and representativeness.",
            "frequency_or_prevalence": "Observed across attempted prompts during dataset creation; certain phrasing repeatedly produced non-code outputs.",
            "root_cause": "LLM behavior sensitivity to prompt phrasing and internal instruction-following heuristics; ambiguous or under-constraining prompts allow the model to include explanatory text.",
            "mitigation_approach": "Prompt engineering to enforce code-only responses (adopted prompt: 'Question: &lt;Coding_Task_Description&gt; Please provide your answer as Python code. Answer:'), repeated generation attempts per problem and filtering outputs by executing test cases.",
            "mitigation_effectiveness": "Using the chosen prompt plus multiple generations increased the empirical success rate from ~21.3% to ~45.6%; however this required extra API calls and did not fully close the gap to published figures for single-shot generation.",
            "domain_or_field": "machine learning / data collection for empirical evaluation",
            "reproducibility_impact": true,
            "uuid": "e731.1",
            "source_info": {
                "paper_title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Spec-vs-solution functional gap",
            "name_full": "Mismatch between natural-language problem specification and generated implementations (functional correctness gap)",
            "brief_description": "Many GPT-generated solutions did not satisfy the problem's test cases, revealing a gap between natural-language problem descriptions and the model-produced code implementations' correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Dataset validation pipeline (test-case execution gating)",
            "system_description": "Post-processing pipeline that executes human and GPT solutions against provided problem-specific test cases to ensure syntactic and semantic correctness before inclusion in the labeled dataset.",
            "nl_description_type": "problem statement / task specification",
            "code_implementation_type": "generated solution code (scripts/functions)",
            "gap_type": "functional correctness gap / specification nonconformance",
            "gap_description": "A large fraction of GPT outputs fail to satisfy the functional requirements encoded in the problem's test cases; executing the test-suite removed many GPT samples (and some human samples) from the dataset because outputs were incorrect or non-executable.",
            "gap_location": "solution correctness verification / evaluation stage",
            "detection_method": "Automatic execution of attached test cases against each solution; removal of any solution that fails tests or is non-executable.",
            "measurement_method": "Reported by percent of samples removed by test-case filtering: GPT samples reduced by 72.39%, human samples reduced by 28.59%; overall dataset shrank substantially (from ~314k to ~171k after filtering), and final balanced dataset contained ~31.4k samples.",
            "impact_on_results": "Severe reduction in usable GPT-generated data, potential sampling bias (only successful GPT outputs retained), increased difficulty in constructing balanced datasets, and possible inflation/deflation of detection model performance depending on which outputs survive filtering.",
            "frequency_or_prevalence": "Substantial in this dataset (majority of GPT outputs filtered out), indicating this is a common phenomenon when applying code-generation models to realistic programming tasks with strict test suites.",
            "root_cause": "LLM limitations in reliably producing fully correct, problem-constrained code in a single shot; ambiguity or incompleteness in problem descriptions can exacerbate failures.",
            "mitigation_approach": "Generate multiple candidate solutions per problem (multiple API calls), filter by executing test cases, and balance datasets per problem using the minimum count of valid human/GPT solutions.",
            "mitigation_effectiveness": "Multiple generations increased the observed success rate to ~45.6% (from ~21.3% on single generation), but test-case filtering still removed a majority of GPT responses in aggregate (72.39% removed).",
            "domain_or_field": "empirical ML evaluation / program synthesis",
            "reproducibility_impact": true,
            "uuid": "e731.2",
            "source_info": {
                "paper_title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Related-work reporting ambiguity",
            "name_full": "Ambiguous evaluation protocol reporting in prior studies (possible train/test leakage)",
            "brief_description": "The paper notes that some prior work reporting strong detection performance did not clearly specify whether fine-tuning and evaluation split data by problem or by sample, which can cause misleadingly optimistic results through leakage.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Evaluation methodology in related studies",
            "system_description": "Protocols used by other researchers to fine-tune detectors and evaluate performance on code-origin classification tasks.",
            "nl_description_type": "methods section / experimental protocol of cited papers",
            "code_implementation_type": "training and evaluation scripts (not available/unclear in cited works)",
            "gap_type": "ambiguous evaluation protocol / possible data leakage between train and test",
            "gap_description": "Wang et al. reported improved AUC after fine-tuning a text detector on code, but the paper notes it is unclear whether fine-tuning and evaluation used samples from the same coding problems (sample-wise split) or held-out problems (problem-wise split). If sample-wise splits were used, near-duplicate solutions for the same problem could have leaked into both train and test sets, inflating performance.",
            "gap_location": "experimental design / dataset splitting",
            "detection_method": "Critical reading of related-work methods and comparison to best-practice problem-wise splitting used in this paper.",
            "measurement_method": "No direct re-measurement in this paper; the concern is qualitative and based on lack of reporting. The paper contrasts this with their own problem-wise 80/20 split to avoid leakage.",
            "impact_on_results": "Potential overestimation of generalization performance in cited studies; results may not hold on unseen problems. The paper emphasizes this as a reason prior high AUC claims may not generalize.",
            "frequency_or_prevalence": "Not quantified; arises when authors do not explicitly report split strategy and when datasets contain many similar solutions per problem.",
            "root_cause": "Incomplete methodological reporting in prior work and possible convenience-driven use of sample-wise splitting.",
            "mitigation_approach": "Adopt and explicitly report problem-wise train/test splits (as done in this paper), repeat experiments over multiple random seeds, and ensure no near-duplicate problem solutions cross the split.",
            "mitigation_effectiveness": "Not directly evaluated here for prior works; this paper asserts problem-wise splitting is standard practice to improve validity and reduce leakage.",
            "domain_or_field": "empirical ML evaluation / experimental reproducibility",
            "reproducibility_impact": true,
            "uuid": "e731.3",
            "source_info": {
                "paper_title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "NL-detector domain mismatch",
            "name_full": "Mismatch between natural-language detectors' claims and their applicability to code",
            "brief_description": "Natural-language text detectors (e.g., DetectGPT, GPTZero) perform well on NL text but, when applied directly to code, show poor performance unless adapted or fine-tuned for code, revealing a domain transfer gap between NL descriptions of detector capability and actual implementation results on code.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Cross-domain detector application (text detectors applied to code)",
            "system_description": "Applying detectors designed and validated on natural language text to code snippets without adaptation (zero-shot), and evaluating their performance on code-origin classification.",
            "nl_description_type": "tool capability claims / method description in original detector papers",
            "code_implementation_type": "detector application scripts (zero-shot or fine-tuned on code)",
            "gap_type": "domain transfer gap / method mismatch",
            "gap_description": "Text detectors that report &gt;90% accuracy on NL text drop to AUC ~0.40-0.50 on code when used as-is; some studies improved performance by fine-tuning on code, but generalization remains questionable. DetectGPT required code-specific modifications (DetectGPT4Code) to be marginally useful on code.",
            "gap_location": "model selection / adaptation stage",
            "detection_method": "Empirical evaluation reported in related work: applying off-the-shelf NL detectors to code and measuring AUC/accuracy; comparisons with code-specific adaptations.",
            "measurement_method": "Reported performance metrics from cited studies (e.g., Wang et al. AUC 0.40-0.50 for text detectors on code; DetectGPT4Code AUC 0.70-0.80 on small code sets; after fine-tuning some detectors saw AUC improvements to 0.77-0.98 but with unclear generalization).",
            "impact_on_results": "Direct application of NL detectors to code is unreliable; without adaptation results can be near-random, leading to false confidence if NL claims are naively assumed transferable.",
            "frequency_or_prevalence": "Observed across multiple cited works; a common pitfall when crossing text-to-code domains.",
            "root_cause": "Different statistical and syntactic properties of code vs natural language (tokenization, structure, determinism, semantics).",
            "mitigation_approach": "Use code-aware perturbation models, code-specific embeddings, fine-tune detectors on held-out problems, or design detectors specifically for code (e.g., DetectGPT4Code modifications).",
            "mitigation_effectiveness": "Partial improvements reported (DetectGPT4Code AUC 0.70-0.80 on small datasets; this paper shows embedding-based supervised classifiers achieving up to 98% accuracy), but fine-tuning must be carefully evaluated with problem-wise splits to ensure generalization.",
            "domain_or_field": "machine learning / NLP-to-code transfer",
            "reproducibility_impact": true,
            "uuid": "e731.4",
            "source_info": {
                "paper_title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Probability calibration mismatch",
            "name_full": "Model probability calibration issues (overconfident / bimodal outputs)",
            "brief_description": "Mismatch where classifier predicted probabilities do not reflect empirical uncertainty because many models output probabilities very close to 0 or 1, producing unstable calibration estimates in intermediate ranges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Model evaluation & calibration subsystem",
            "system_description": "Component that computes predicted class probabilities for classifiers and evaluates calibration via LOESS calibration plots and kernel density estimates of predicted probabilities.",
            "nl_description_type": "evaluation expectations about predicted probabilities representing reliable uncertainty",
            "code_implementation_type": "model training and probability-prediction code",
            "gap_type": "probability calibration mismatch / overconfidence",
            "gap_description": "Some high-performing models (DNNs, GMMs) produce near-deterministic probabilities (close to 0 or 1) for most samples, leaving few predictions in the mid-range; LOESS-based calibration curves are therefore unstable and may give a misleading impression of calibration in intermediate probability ranges. In contrast, some models (XGB+TF-IDF / XGB+ADA) show near-perfect calibration.",
            "gap_location": "model output / evaluation stage (probability estimation)",
            "detection_method": "Calibration plots (LOESS-smoothed predicted-probability vs empirical frequency) and kernel density estimates of predicted probabilities per model.",
            "measurement_method": "Visual and statistical analysis: LOESS calibration curves compared to the diagonal; kernel density plots reveal whether predictions are concentrated at extremes. The paper reports that some models produce almost exclusively extreme probabilities and that calibration curves in 0.15–0.85 range are unreliable due to sparse data.",
            "impact_on_results": "Overconfident predictions reduce the usefulness of predicted probabilities for downstream decision-making (e.g., human review thresholds) and make probabilistic interpretation unsafe; models can be 'certain' while still being wrong on some samples.",
            "frequency_or_prevalence": "Observed in several embedding-based DNNs and GMMs in this study; not universal across models (XGB variants were well-calibrated).",
            "root_cause": "Model architectures and training objectives that optimize classification accuracy (often with margin) without explicit calibration, coupled with class-separable embeddings that permit confident predictions.",
            "mitigation_approach": "Prefer models with better inherent calibration (e.g., XGB+TF-IDF), apply post-hoc calibration methods (Platt scaling, isotonic regression) or avoid relying solely on predicted probabilities where models are overconfident.",
            "mitigation_effectiveness": "XGB+TF-IDF and XGB+ADA demonstrated nearly perfect calibration and high accuracy in this study; for other models, calibration curves were unstable and not fully trustworthy without more mid-range predictions.",
            "domain_or_field": "statistical ML evaluation / classifier calibration",
            "reproducibility_impact": true,
            "uuid": "e731.5",
            "source_info": {
                "paper_title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DetectGPT",
            "rating": 2
        },
        {
            "paper_title": "DetectGPT4Code",
            "rating": 2
        },
        {
            "paper_title": "A study by Wang et al. comparing text detectors on code",
            "rating": 1
        },
        {
            "paper_title": "Pan et al. comparison of text detectors on code",
            "rating": 1
        }
    ],
    "cost": 0.019355999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Article</h1>
<h2>ChatGPT Code Detection: Techniques for Uncovering the Source of Code</h2>
<p>Marc Oedingen ${ }^{1}$ (D), Raphael C. Engelhardt ${ }^{1}$ (D), Robin Denz ${ }^{2}$ (D), Maximilian Hammer ${ }^{1}$ (D) and Wolfgang Konen ${ }^{1}$ (D)</p>
<p>Citation: Oedingen, M.; Engelhardt, R. C.; Denz, R.; Hammer M.; Konen, W. ChatGPT Code Detection: Techniques for Uncovering the Source of Code. AI 2024, 1, 1-28. https://doi.org/
Received:
Revised:
Accepted:
Published:
Copyright: (c) 2024 by the authors. Submitted to $A I$ for possible open access publication under the terms and conditions of the Creative Commons Attri-bution (CC BY) license (https:// creativecommons.org/licenses/by/ $4.0 /$ ).
1 Cologne Institute of Computer Science, Faculty of Computer Science and Engineering Science, TH Köln, Gummersbach, Germany; (Marc.Oedingen,Raphael.Engelhardt,Wolfgang.Konen)@th-koeln.de and Maximilian.Hammer@smail.th-koeln.de
2 Department of Medical Informatics, Biometry and Epidemiology, Faculty of Medicine, Ruhr-University Bochum, Bochum, Germany; denz@amib.rub.de</p>
<ul>
<li>Correspondence: Wolfgang.konen@th-koeln.de</li>
</ul>
<h4>Abstract</h4>
<p>In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education. This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of $98 \%$. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well but provide at most $85-88 \%$ accuracy. We also show that untrained humans solve the same task not better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.</p>
<p>Keywords: AI, Machine Learning, Code Detection, ChatGPT, Large Language Models</p>
<h2>1. Introduction</h2>
<p>The recent performance of ChatGPT has astonished scholars and the general public, not only regarding the seemingly human way of using natural language but also its proficiency in programming languages. While the training data (e.g., for the Python programming language) ultimately stems from human programmers, ChatGPT has likely developed its own coding style and idiosyncrasies, as human programmers do. In this paper, we train and evaluate various machine learning (ML) models on distinguishing human-written from ChatGPT-written Python code. These models achieve very high performance even for code samples with few lines, a seemingly impossible task for humans. The following subsections present our motivation, scientific description of the task, and the elaboration of research questions.</p>
<h3>1.1. Motivation</h3>
<p>Presently, the world is looking ambivalently at the development and opportunities of powerful large language models (LLMs). On the one hand, such models can execute complex tasks and augment human productivity due to their enhanced performance in various areas [1,2]. On the other hand, these models can be misused for malicious purposes, such as generating deceptive articles or cheating in educational institutions and other competitive environments [3-7].</p>
<p>The inherently opaque nature of these black-box LLM models, combined with the difficulty of distinguishing between human- and AI-generated content, poses a problem that can make it challenging to trust these models [8]. Earlier research has extensively focused on detecting natural language (NL) text content generated by LLMs [9-11]. A recent survey [12] discusses the strengths and weaknesses of those approaches. However, detecting AI-generated code is an equally important and relatively unexplored area of research. As LLMs are utilized more often in the field of software development, the ability to distinguish between human- and AI-generated code becomes increasingly important. Thereby, it is not exclusively about the distinction but also the implications, such as the code's trustworthiness, efficiency, and security. Moreover, the rapid development and improvement of AI models may lead to an arms race where detection techniques must continuously evolve to stay ahead of the curve.</p>
<p>Earlier research showed that using LLMs to generate code can lead to security vulnerabilities, and $40 \%$ of the code fails to solve the given task [13]. Contrastingly, using LLMs can also lead to a significant increase in productivity and efficiency [2]. This dual-edged nature of LLMs necessitates a balanced approach. Harnessing the potential benefits of such models while mitigating risks is the key. Ensuring the authenticity of code is especially crucial in academic environments, where the integrity of research and educational outcomes is paramount. Fraudulent or AI-generated submissions can undermine the foundation of academic pursuits, leading to a loss of trust in research findings and educational qualifications. Moreover, in the context of examinations, robust fraud detection is essential to prevent cheating, ensuring the assessments accurately reflect the student's capabilities and do not check the non-deterministic output of a prompt due to the stochastic decision-making of LLMs based on transformer models (TM) during inference. Under the assumption that AI-generated code has a higher chance of security vulnerabilities and beyond the educational context, it can also be critical in software industries to have the ability to distinguish between human- and AI-generated code when testing an unknown piece of software. As the boundaries of what AI can achieve expand, our approach to understanding, managing, and integrating these capabilities into our societal fabric, including academic settings, will determine our success in the AI-augmented era.</p>
<h1>1.2. Problem Introduction</h1>
<p>This paper delves deep into the challenge of distinguishing between human-generated and AI-generated code, offering a comprehensive overview of state-of-the-art methods and proposing novel strategies to tackle this problem.</p>
<p>Central to our methodology is a reduction of complexity: the intricate task of differentiating between human- and AI-generated code is represented as a fundamental binary classification problem. Specifically, given a code snippet $x \in \mathcal{C}$ as input, we aim for a function $f: \mathcal{C} \rightarrow{0,1}=\mathcal{Y}$, which indicates whether the code's origin is human ${0}$ or GPT ${1}$. This allows us to use well-known and established ML models. We represent the code snippets as human-designed (white-box) and embeddings (black-box) features in order to apply ML models. The usage of embeddings requires prior tokenization, which is carried out either implicitly by the model or explicitly by us. Hence, we use embeddings to obtain constant dimensionality across all code snippets or single tokens. Figure 1 gives an overview of our approach in the form of a flowchart, where the details will be explained in the following sections.</p>
<p>For a model to truly generalize in the huge field of software development across many tasks, it requires training on vast amounts of data. However, volume alone is insufficient for a model. The data's quality is paramount, ensuring meaningful and discriminating features are present within the code snippets. An ideal dataset would consist of code snippets that satisfy a variety of test cases to guarantee their syntactic and semantic correctness for a given task. Moreover, to overcome the pitfalls of biased data, we emphasize snippets prior to the proliferation of GPT in code generation. However, a significant blocker emerges from a stark paucity of publicly available GPT-generated solutions that match our criteria</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Flowchart of our Code Detection Methodology
mentioned before. This lack underscores the necessity to find and generate solutions that can serve as fitting data sources for our classifiers.</p>
<h1>1.3. Research Questions and Contributions</h1>
<p>We formulate the following research questions based on the problem introduction and the need to obtain detection techniques for AI-generated code. In addition, we provide our hypotheses regarding the questions we want to answer with this work.
RQ1: Can we distinguish (on unseen tasks) between human- and AI-generated code?
RQ2: To what extent can we explain the difference between human- and AI-generated code?</p>
<ul>
<li>$\quad \mathbf{H}_{1}$ : There are detectable differences in style between AI-generated and humangenerated code.</li>
<li>$\quad \mathbf{H}_{2}$ : The differences are only to a minor extent attributable to code formatting: If both code snippets are formatted in the same way, there are still many detectable differences.</li>
</ul>
<p>Upon rigorous scrutiny of the posed research questions, an apparent paradox emerges. LLMs have been trained using human-generated code. Consequently, the question arises: Does AI-generated code diverge from its human counterpart? We postulate that LLMs follow learning trajectories similar to individual humans. Throughout the learning process, humans and machines are exposed to many code snippets, each encompassing distinct stylistic elements. They subsequently develop and refine their unique coding style, i.e., by using consistent variable naming conventions, commenting patterns, code formatting, or selecting specific algorithms for given scenarios [14]. Given the vast amount of code the</p>
<p>machine has seen during training, it is anticipated to adopt a more generalized coding style. Thus, identifiable discrepancies between machine-generated code snippets and individual human-authored code are to be expected.</p>
<p>The main contributions of this paper are:</p>
<ol>
<li>Several classification models are evaluated on a large corpus of code data. While the human-generated code comes from many different subjects, the AI-generated code is (currently) only produced by ChatGPT-3.5.</li>
<li>The best model-feature combinations are models operating on high dimensional vector embeddings (black-box) of the code data.</li>
<li>Formatting all snippets with the same code formatter decreases the accuracy only slightly. Thus, the format of the code is not the key feature of distinction.</li>
<li>The best models achieve classification accuracies of $98 \%$. An explainable classifier with almost $90 \%$ accuracy is obtained with the help of Bayes classification.</li>
</ol>
<h1>1.4. Structure</h1>
<p>The remainder of this article is structured as follows: Section 2 provides a comprehensive review of the existing literature, evaluating and discussing its current state. Section 3 offers an overview of the techniques and frameworks employed throughout this study, laying the foundation for understanding the subsequent sections. Section 4 details our experimental setup, outlining procedures from data collection and preprocessing to the training of models and their parameters. Section 5 presents the empirical findings of our investigations, followed by a careful analysis of the results. In Section 6, we provide a critical discussion and contextualization within the scope of alternative approaches. Finally, Section 7 synthesizes our findings and outlines potential directions for further research.</p>
<h2>2. Related Work</h2>
<p>Fraud detection is a well-established area of research in the domain of AI. However, most methodologies focus on AI-generated NL content [9,10,15,16] rather than on code [17,18]. Nevertheless, findings from text-based studies remain relevant, given the potential for cross-application and transferability of techniques.</p>
<h3>2.1. Zero-shot detection</h3>
<p>One of the most successful models for differentiating between human- and AI-generated text is DetectGPT by Mitchell et al. [9], which employs zero-shot detection, i.e., it requires neither labeled training data nor specific model training. Instead, DetectGPT follows a simple hypothesis: Minor rewrites of model-generated text tend to have lower log probability under the model than the original sample, while minor rewrites of humanwritten text may have higher or lower log probability than the original sample. DetectGPT only requires log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained LLM (e.g., T5 [19]). Applying this method to textual data of different origins, Mitchell et al. [9] report very good classification results, namely $\mathrm{AUC}=0.90-0.99$, which is considerably better than other zero-shot detection methods on the same data.</p>
<p>Yang et al. [18] developed DetectGPT4Code, an adaptation of DetectGPT for code detection that also operates as a zero-shot detector, by introducing three modifications: (1) Replacing T5 with the code-specific Incoder-6B [20] for code perturbations, addressing the need for maintaining code's syntactic and semantic integrity. (2) Employing smaller surrogate LLMs to approximate the probability distributions of closed, black-box models, like GPT-3.5 [21] or GPT-4 [22]. (3) Using fewer tokens as anchors turned out to be better than the full-length code. Preliminary experiments found that the ending tokens are more deterministic given enough preceding text and thus better indicators. Yang et al. [18] tested DetectGPT4Code on a relatively small set of 102 Python and 165 Java code snippets. Their results with $\mathrm{AUC}=0.70-0.80$ were clearly better than using plain DetectGPT $(\mathrm{AUC}=0.50-0.60)$ but still not reliable enough for practical use.</p>
<h1>2.2. Text Detectors Applied to Code</h1>
<p>Recently, several detectors like DetectGPT [9], GPTZero [23] and others [10,15,16,24] were developed that are good at distinguishing AI-generated from human-generated NL texts, often with an accuracy better than $90 \%$. This also makes it tempting to apply those NL text detectors to code snippets. As written above, Yang et al. [18] used DetectGPT as a baseline for their code detection method.</p>
<p>There are two recent works [25,26] that compare a larger variety of NL text detectors on code detection tasks: Wang et al. [25] collect a large code-related content dataset, among them 226k code snippets, and apply 6 different text detectors to it. When using the text detectors as-is, they only reach a low $\mathrm{AUC}=0.40-0.50$ on code snippets, which they consider unsuitable for reliable classification. In a second experiment, they fine-tune one of the open-source detectors (RoBERTa-QA [24]) by training it with a portion of their code data. It is unclear whether this fine-tuning and its evaluation used training and test samples originating from the same coding problem. Interestingly, after fine-tuning, they report a considerably higher $\mathrm{AUC}=0.77-0.98$. The authors conclude that "While fine-tuning can improve performance, the generalization of the model still remains a challenge".</p>
<p>Pan et al. [26] provide a similar study on a medium-size database with 5 k code snippets, testing 5 different text detectors on their ability to recognize the origin. As a special feature, they consider 13 variant prompts. They report an accuracy of $50-60 \%$ for the tested detectors, only slightly better than random choice.</p>
<p>In general, text detectors work well on NL detection tasks but are not reliable enough on code detection tasks.</p>
<h3>2.3. Embedding- and Feature-Based Methods</h3>
<p>In modern LLMs, embeddings constitute an essential component, transforming text or code into continuous representations within a dense vector space of constant dimension, where proximity indicates similarity among elements. Hoq et al. [17] use a prior term frequency-inverse document frequency (TF-IDF) [27] embedding for classic ML algorithms, code2Vec [28] and abstract syntax tree-based neural networks (ASTNN) [29] for predicting the code's origin. While TF-IDF reflects the frequency of a token in a code snippet over a collection of code snippets, code2Vec converts a code snippet, represented as an abstract syntax tree (AST), into a set of path-contexts, linking pairs of terminal nodes. Subsequently, it computes the attention weights of the paths and uses them to compute the single aggregated weighted code vector. Similarly, ASTNN parses code snippets as an AST and uses preorder traversal to segment the AST into a sequence of statement trees, which are further encoded into vectors with pre-trained embedding parameters of Word2Vec [30]. These vectors are processed through a Bidirectional Gated Recurrent Unit (Bi-GRU) [31] to model statement naturalness, with pooling of Bi-GRU hidden states to represent the code fragment. Hoq et al. [17] used $3.162 \times 10^{3}$ human- and $3 \times 10^{3}$ ChatGPT-generated code snippets in Java from a CS1 course with a total of 10 distinct problems, yielding 300 solutions per problem. Further, they select $4 \times 10^{3}$ random code snippets for training and distribute the remaining samples equally on the test- and validation set. All models achieve similar accuracies, ranging from $0.90-0.95$. However, the small number of unique problems, the large number of similar solutions, and their splitting procedure render the results challenging to generalize beyond the study's specific context, potentially limiting the applicability of the finding to broader scenarios.</p>
<p>Li et al. [32] present an interesting work where they generate features in three groups (lexical, structural layout, and semantic) for discrimination of code generated by ChatGPT from human-generated code. Based on these rich feature sets, they reach detection accuracies between $0.93-0.97$ with traditional ML classifiers like random forests (RF) or sequential minimal optimization (SMO). Limitations of the method, as mentioned by the authors, are the relatively small ChatGPT code dataset (1206 code snippets) and the lack of prompt engineering (specific prompt instructions may lead to different results).</p>
<h1>3. Algorithms and Methodology</h1>
<p>In this section, we present fundamental algorithms and describe the approaches mandatory for our methodology. We start by detailing the general prerequisites and preprocessing needed for algorithm application. Subsequently, we explicate our strategies for code detection and briefly delineate the models used for code sample classification.</p>
<h3>3.1. General Prerequisites</h3>
<p>Detecting fraudulent use of ChatGPT in software development or coding assessment scenarios requires an appropriate dataset. Coding tasks consisting of a requirement text, human solutions, and test cases are of interest. To the best of our knowledge, one of the most used methods for fraudulent usage is representing the requirement text as the prompt for ChatGPT's input and using the output's extracted code for submission. Therefore, tasks that fulfill the above criteria are sampled from several coding websites, and human solutions are used as a baseline to compare to the code from ChatGPT. The attached test cases were applied to both the human and AI solutions before comparison to guarantee that the code is not arbitrary but functional and correct. For generating code gpt-3.5-turbo [33] was used, the most commonly used AI tool for fraudulent content, which also powers the application ChatGPT.</p>
<h3>3.2. ChatGPT</h3>
<p>Fundamentally, ChatGPT is a fine-tuned sequence-to-sequence learning [34] model with an encoder-decoder structure based on a pre-trained transformer [21,35]. Due to its positional encoding and self-attention mechanism, it can process data in parallel rather than sequentially, unlike previously used models such as recurrent neural networks [36] or long-short-term memory (LSTM) models [37]. Just limited in the maximum capacity of input tokens, it is capable of capturing long-term dependencies. During inference, the decoder is detached from the encoder and is used solely to output further tokens. Interacting with the model requires the user to provide input that is then processed and passed into the decoder, which generates the output sequence token-by-token. Once a token is generated, the model incorporates this new token into the input from the preceding forward pass, continuously generating subsequent tokens until a termination criterion is met. Upon completion, the model stands by for the next user input, seamlessly integrating it with the ongoing conversation. This process effectively simulates an interactive chat with a GPT model, maintaining the flow of the conversation.</p>
<h3>3.3. Embeddings</h3>
<p>Leveraging the contextual representation of embeddings in a continuous and constant space allows ML models to perform mathematical operations and understand patterns or similarities in the data. In our context, we use the following three models to embed all code snippets:</p>
<p>TF-IDF [38] incorporates an initial step of prior tokenization of code snippets, setting the foundation to capture two primary components: (1) term frequency (TF), which is the number of times a token appears in a code snippet, and (2) inverse document frequency (IDF), which reduces the weight of tokens that are common across multiple code snippets. Formally, TF-IDF is defined as:</p>
<p>$$
\begin{gathered}
\operatorname{TF-IDF}(t, d)=\operatorname{TF}(t, d) \cdot \operatorname{IDF}(t) \
\text { with } \operatorname{TF}(t, d)=\frac{N_{t, d}}{N_{d}} \text { and } \operatorname{IDF}(t)=\log \left(\frac{N}{N_{t}}\right)
\end{gathered}
$$</p>
<p>where $N$ is the number of code snippets, $N_{t, d}$ the number of times token $t$ appears in code snippet $d, N_{d}$ the number of tokens in code snippet $d$ and $N_{t}$ the number of code snippets that include token $t$. The score emphasizes tokens that occur frequently in a particular code snippet but are less frequent in the entire collection of code snippets,</p>
<p>thereby underlining the unique relevance of those tokens to that particular code snippet.
Word2Vec [39] is a neural network-based technique used to generate dense vector representations of words in a continuous vector space. It fundamentally operates on one of two architectures: (1) Skip-gram (SG), where the model predicts the surrounding context given a word, or (2) continuous bag of words (CBOW), where the model aims to predict a target word from its surrounding context. Given a sequence of words $w_{1}, \ldots, w_{T}$, their objective is to maximize the average log probability:</p>
<p>$$
\begin{array}{ll}
\frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{c \leq j \leq c \
j \neq 0}} \begin{cases}\log \left(p\left(w_{t} \mid w_{t+j}\right)\right) &amp; \text { for SG } \
\log \left(p\left(w_{t+j} \mid w_{t}\right)\right) &amp; \text { for CBOW }\end{cases} \
\text { with } p\left(w_{\mathrm{O}} \mid w_{l}\right)=\frac{\exp \left(v_{w_{\mathrm{O}}}^{\prime} v_{w_{l}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime} v_{w_{l}}\right)}
\end{array}
$$</p>
<p>where $v_{w}$ and $v^{\prime}{ }<em i="i">{w}$ denote the input and output vector representation of word $w</em>\right)$. Training the model efficiently involves the use of hierarchical softmax and negative sampling to avoid the computational challenges of the softmax over large vocabularies [30].
OpenAI ADA [33] does not have an official paper, but we strongly suspect that a methodology related to OpenAI's paper [40] was used to train the model. In their approach, Neelakantan et al. [40] use a contrastive objective on semantically similar paired samples $\left{\left(x_{i}, y_{i}\right)\right}_{i=1}^{N}$ and in-batch negative in training. Therefore, a transformers pre-trained encoder $E$ [35], initialized with GPT models [21,41], was used to map each pairs elements to their embeddings, and calculate the cosine similarity:} \in \mathcal{V}$ in the sequence of all words in the vocabulary, $W \in \mathbb{N}$ the number of words in that vocabulary $\mathcal{V}$, and $c \in \mathbb{N}$ the size of the training context. The probability of a word given its context is calculated by the softmax function with $p\left(w_{\mathrm{O}} \mid w_{l</p>
<p>$$
\begin{aligned}
&amp; v_{x}=E\left(\left[\mathrm{SOS}<em x="x">{x} \oplus x \oplus\left[\mathrm{EOS}</em>\right)\right.\right. \
&amp; v_{y}=E\left(\left[\mathrm{SOS}<em y="y">{y} \oplus y \oplus\left[\mathrm{EOS}</em>,\right.
\end{aligned}
$$}\right)\right.\right. \quad \text { and } \quad \left.\left.\operatorname{sim}(x, y)=\frac{v_{x} \cdot v_{y}}{\left|v_{x}\right| \cdot\left|v_{y}\right|</p>
<p>where $\oplus$ denotes the operation of string concatenation and EOS, SOS special tokens, delimiting the sequences. Fine-tuning the model includes contrasting the paired samples against in-batch negatives, given by supervised training data like natural language inference (NLI) [42]. Mini-batches of $M$ samples are considered for training, which consist of $M-1$ negative samples from NLI, and one positive example $\left(x_{i}, y_{i}\right)$. Thus, the logits for one batch is a $M \times M$ matrix, where each logit is defined as $\tilde{y}=\operatorname{sim}\left(x_{i}, y_{i}\right) \cdot \exp (\tau)$, where $\tau$ is a trainable temperature parameter. The loss is calculated as the cross entropy losses across each row and column direction, where positives examples lie on the diagonal of the matrix. Currently, embeddings from ADA can be obtained by using OpenAIs API, namely text-embedding-ada-002 [33], which returns a non-variable dimension $\tilde{x} \in \mathbb{R}^{1536}$.</p>
<h1>3.4. Supervised Learning Methods</h1>
<p>Feature extraction and embedding derivation constitute integral components in distinguishing between AI-generated and human-generated code, serving as inputs for classification models. Subsequently, we list the supervised learning (SL) models employed in our analysis:</p>
<p>Logistic Regression (LR) [43] which makes a linear regression model usable for classification tasks.</p>
<p>Classification and Regression Tree (CART) [44] is a well-known form of decision trees (DTs) that offers transparent decision-making. Its simplicity, consisting of simple rules, makes it easy to use and understand.
Oblique Predictive Clustering Tree (OPCT) [45]: In contrast to regular DTs like CART, an OPCT split at a decision node is not restricted to a single feature, but rather a linear combination of features, cutting the feature space along arbitrary slanted (oblique) hyperplanes.
Random Forest (RF) [46]: A random forest is an ensemble method, i.e., the application of several DTs, and is subject to the idea of bagging. RF tend to be much more accurate than individual DTs due to their ensemble nature, usually at the price of reduced interpretability.
eXtreme Gradient Boosting (XGB) [47]: Boosting is an ensemble technique that aims to create a strong classifier from several weak classifiers. In contrast to RF with its independent trees, in boosting the weak learners are trained sequentially, with each new learner attempting to correct the errors of their predecessors. In addition to gradient boosting [48], XGB employs a more sophisticated objective function with regularization to prevent overfitting and improve computational efficiency.
Deep Neural Network (DNN) [49,50]: A feedforward neural network with multiple layers. DNNs can learn highly complex patterns and hierarchical representations, making them extremely powerful for various tasks. However, they require large amounts of data and computational resources for training and their highly non-linear nature makes them, in contrast to other methods, somewhat of a "black-box", making it difficult to interpret their predictions.</p>
<h1>3.5. Gaussian Mixture Models</h1>
<p>Beyond SL methods, we also incorporate Gaussian mixture models (GMMs). Generally, a GMM is characterized by a set of $K$ Gaussian distributions $\mathcal{N}(x \mid \mu, \sigma)$. Each distribution $k=1, \ldots, K$ has a mean vector $\vec{\mu}<em k="k">{k}$ and a covariance matrix $\Sigma</em>=1$ to ensure the probability is normalized to 1 . Further, all components $k$ are initialized with $k$-means, modeling each cluster with the corresponding Gaussian distribution. The probability density function of a GMM is defined as:}$. Additionally, there are mixing coefficients $\psi_{k}$ associated with each Gaussian component $k$, satisfying the condition $\sum_{k=1}^{K} \psi_{k</p>
<p>$$
p(\vec{x})=\sum_{k=1}^{K} \psi_{k} \mathcal{N}\left(\vec{x} \mid \vec{\mu}<em k="k">{k}, \Sigma</em>\right)
$$</p>
<p>The pre-defined clusters serve as starting point for optimizing the GMM with the expectationmaximization (EM) algorithm, which refines the model through iterative expectation and maximization steps. In the expectation step, it calculates the posterior probabilities $\hat{\gamma}<em k="k">{i k}$ of data points belonging to each Gaussian component $k$, using the current parameter estimates according to Eq. (2). Subsequently, the maximization step in Eq. (3) updates the model parameters $\left(\hat{\psi}</em>}, \hat{\mu<em k="k">{k}, \hat{\Sigma}</em>\right)$ to maximize the data likelihood:</p>
<p>$$
\hat{\gamma}<em k="k">{i k}=\frac{\hat{\psi}</em>} \mathcal{N}\left(\vec{x<em k="k">{i} \mid \hat{\mu}</em>}, \hat{\Sigma<em j="1">{k}\right)}{\sum</em>}^{K} \hat{\psi<em i="i">{j} \mathcal{N}\left(\vec{x}</em>} \mid \hat{\mu<em j="j">{j}, \hat{\Sigma}</em>
$$}\right)</p>
<p>$$
\begin{aligned}
\hat{\psi}<em i="1">{k} &amp; =\frac{1}{N} \sum</em>}^{N} \hat{\gamma<em k="k">{i k}, \quad \hat{\mu}</em>}=\frac{1}{N \hat{\psi<em i="1">{k}} \sum</em>}^{N} \hat{\gamma<em i="i">{i k} \vec{x}</em> \
\hat{\Sigma}<em k="k">{k} &amp; =\frac{1}{(N-1) \hat{\psi}</em>}} \sum_{i=1}^{N} \hat{\gamma<em i="i">{i k}\left(\vec{x}</em>}-\hat{\mu<em i="i">{k}\right)\left(\vec{x}</em>
\end{aligned}
$$}-\hat{\mu}_{k}\right)^{\top</p>
<p>The iterative repetition of this process guarantees that at least one local optimum and possibly the global optimum is always achieved.</p>
<h2>4. Experimental Setup</h2>
<p>In this section, we outline the requirements to carry out our experiments. We cover basic hard- and software components, as well as the collection and preprocessing of data to apply the methodology and models presented in the previous section.</p>
<p>For data preparation and all our experiments, we used Python version 3.10 with different packages, as delineated in our repository https://github.com/MarcOedingen/ ChatGPT-Code-Detection (accessed on July 4, 2024). Due to large amounts of code snippets, we recommend a minimum of 32 GB of RAM, especially when experimenting with Word2Vec.</p>
<h1>4.1. Data Collection</h1>
<p>As previously delineated in the general prerequisites, an ideal dataset for our intended purposes is characterized by the inclusion of three fundamental elements: (1) Problem description, (2) one or more human solutions, and (3) various test cases. This is exemplified in Figure 2. The problem description (1) should clearly contain the minimum information required to solve a programming task or to generate a solution through ChatGPT. In contrast, unclear problem descriptions may lead to solutions that overlook the main problem, thereby lowering the solution quality and potentially omitting useful solutions from the limited available samples. The attached human solutions (2) for a coding problem play an important role in the subsequent analysis and serve as referential benchmark for the output of ChatGPT. Furthermore, a set of test cases (3) facilitates the elimination of syntactically correct solutions that do not fulfill the functional requirements specified in the problem description. To this end, a controlled environment is created in which the code's functionality is rigorously tested, preventing the inclusion of snippets of code that are based on incorrect logic or could potentially produce erroneous output. Hence, we strongly focus on syntactic and executable code but ignore a possibly typical behavior of ChatGPT in case of uncertainties or wrong answers. For some problems, a function is expected to solve them, while others expect a console output. We have considered both by using either the function name or the entire script, referred to as the entry point, for the enclosed test cases.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Example of row in dataset
Programming tasks from programming competitions are particularly suitable for the above criteria; see Table 1 for the sources of the coding problems. Due to the variety and high availability of such tasks in Python, we decided to use this programming language. Thereby, we exclusively include human solutions from a period preceding the launch of ChatGPT. We used OpenAIs gpt3.5-turbo API with the default parameters to generate code.</p>
<p>The OpenAI report [22] claims that GPT-3.5 has an accuracy of $48.1 \%$ in a zero-shot evaluation for generating a correct solution on the HumanEval dataset [51]. After a single generation, our experimental verification yielded a notably lower average probability of</p>
<p>$21.3 \%$. Due to the low success rate, we conducted five distinct API calls for each collected problem. This strategy improved the accuracy rate considerably to $45.6 \%$, converging towards the accuracy reported in [22] and substantiating the theory that an increment in generation attempts correlates positively with heightened accuracy levels [51]. Furthermore, it is noteworthy that the existence of multiple AI-generated solutions for a single problem does not pose an issue, given that the majority of problems possess various human solutions; see Table 1 in column 'before pre-processing' for the number of problems $n_{\text {PROBLEMS }}$, average human solutions per problem $\bar{n}<em _SAMPLES="{SAMPLES" _text="\text">{\text {PROBLEMS }}$ and the total human samples $n</em>$.}</p>
<p>When generating code with gpt3.5-turbo, the prompt strongly influences success rate. Prompt engineering is a separate area of research that aims to utilize the intrinsic capabilities of an LLM while mitigating potential pitfalls related to unclear problem descriptions or inherent biases. During the project, we tried different prompts to increase the yield of successful solutions. Our most successful prompt, which we subsequently used, is the following: 'Question: <Coding_Task_Description> Please provide your answer as Python code. Answer:". Other prompts, i.e., "Question: <Coding_Task_Description> You are a developer writing Python code. Put all python code \$PYTHON in between [[[\$PYTHON]]]. Answer:", led to a detailed explanation of the problem and an associated solution strategy of the model, but without the solution in code.</p>
<h1>4.2. Data Preprocessing</h1>
<p>Based on impurities in both the GPT-generated and human solutions, the data must be preprocessed before it can be used as input for ML models. Hence, we first extracted the code from the GPT-generated responses and checked whether it and the human solutions can be executed, reducing the whole dataset to $3.68 \times 10^{5}$ samples. This also eliminated missing values due to miscommunication with the API, server overloads, or the absence of Python code in the answer. Further, to prevent the overpopulation of particular code snippet subsets, we removed duplicates in both classes. A duplicate is a code snippet for problem $P$ that is identical to another code snippet for the same problem $P$. This first preprocessing step leaves us with $3.14 \times 10^{5}$ samples.</p>
<p>Numerically, the largest collapse for the remaining samples, and especially for the GPT-generated code snippets, is given by the application of the test cases. This reduces the number of remaining GPT samples by $72.39 \%$ and the number of human samples by $28.59 \%$, leaving a total of $1.71 \times 10^{5}$ samples. Furthermore, we consider a balanced dataset so that our models are less likely to develop biases or favor a particular class, reducing the risk of overfitting and making the evaluation of the model's performance more straightforward. Given $n$ individual coding problems $P_{i}, i=1, \ldots, n$, with $h_{i}$ human solutions and $g_{i}$ GPT solutions, we take the minimum $k_{i}=\min \left(h_{i}, g_{i}\right)$ and choose $k_{i}$ random and distinct solutions from each of the two classes for $P_{i}$. The figures for human samples after the pre-processing procedure are listed in Table 1 in column 'after pre-processing'. Based on a balanced dataset, there are as many average GPT solutions $\bar{n}<em _SAMPLES="{SAMPLES" _text="\text">{\text {SOLUTIONS }}$ and total GPT samples $n</em>$ samples in total.}}$ as human solutions and samples for each source in the final processed dataset. Thus, the pre-processed, balanced and cleaned dataset contains $3.14 \times 10^{4</p>
<h3>4.3. Optional Code Formatting</h3>
<p>A discernible method for distinguishing between the code sources lies in the analysis of code formatting patterns. Variations in these patterns may manifest through the presence of spaces over tabs for indention purposes or the uniform application of extended line lengths. Thus, we use the Black code formatter [58], a Python code formatting tool, for both human- and GPT-generated code, standardizing all samples into a uniform formatting style in an automated way. This methodology effectively mitigates the model's tendency to focus on stylistic properties of the code. Consequently, it allows the models to emphasize more significant features beyond mere formatting. A comparison of the number of tokens for all code snippets of the unformatted and formatted datasets is shown in Figure 3.</p>
<p>Table 1. Code datasets overview: $n_{\text {PROBLEMS }}$ - number of distinct problems, $\bar{n}<em _SAMPLES="{SAMPLES" _text="\text">{\text {SOLUTIONS }}$ - average number of human solutions per problem, and $n</em>$ - total human samples per data source.}</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">before pre-processing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">after pre-processing</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">$n_{\text {PROBLEMS }}$</td>
<td style="text-align: center;">$\bar{n}_{\text {SOLUTIONS }}$</td>
<td style="text-align: center;">$n_{\text {SAMPLES }}$</td>
<td style="text-align: center;">$n_{\text {PROBLEMS }}$</td>
<td style="text-align: center;">$\bar{n}_{\text {SOLUTIONS }}$</td>
<td style="text-align: center;">$n_{\text {SAMPLES }}$</td>
</tr>
<tr>
<td style="text-align: left;">APPS [52]</td>
<td style="text-align: center;">$1.00 \times 10^{4}$</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$2.10 \times 10^{5}$</td>
<td style="text-align: center;">$1.95 \times 10^{3}$</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">$2.17 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">CCF [53]</td>
<td style="text-align: center;">$1.56 \times 10^{3}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$2.81 \times 10^{4}$</td>
<td style="text-align: center;">$4.89 \times 10^{2}$</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">$1.17 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">CC [54]</td>
<td style="text-align: center;">$8.26 \times 10^{3}$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$1.23 \times 10^{5}$</td>
<td style="text-align: center;">$3.11 \times 10^{3}$</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">$9.30 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">HAEA [55]</td>
<td style="text-align: center;">$1.49 \times 10^{3}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$2.38 \times 10^{4}$</td>
<td style="text-align: center;">$5.24 \times 10^{2}$</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">$2.05 \times 10^{3}$</td>
</tr>
<tr>
<td style="text-align: left;">HED [51]</td>
<td style="text-align: center;">$1.64 \times 10^{2}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1.64 \times 10^{2}$</td>
<td style="text-align: center;">$1.27 \times 10^{2}$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">$1.27 \times 10^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">MBPPD [56]</td>
<td style="text-align: center;">$9.74 \times 10^{2}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$9.74 \times 10^{2}$</td>
<td style="text-align: center;">$6.91 \times 10^{2}$</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">$8.40 \times 10^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">MTrajK [57]</td>
<td style="text-align: center;">$1.44 \times 10^{2}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1.44 \times 10^{2}$</td>
<td style="text-align: center;">$3.20 \times 10^{1}$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">$3.20 \times 10^{1}$</td>
</tr>
<tr>
<td style="text-align: left;">Sum (Average)</td>
<td style="text-align: center;">$\mathbf{2 . 2 6} \times \mathbf{1 0}^{\mathbf{4}}$</td>
<td style="text-align: center;">$\langle\mathbf{1 0}\rangle$</td>
<td style="text-align: center;">$\mathbf{3 . 8 6} \times \mathbf{1 0}^{\mathbf{5}}$</td>
<td style="text-align: center;">$\mathbf{7 . 0 1} \times \mathbf{1 0}^{\mathbf{3}}$</td>
<td style="text-align: center;">$\langle\mathbf{1 . 9}\rangle$</td>
<td style="text-align: center;">$\mathbf{1 . 5 7} \times \mathbf{1 0}^{\mathbf{4}}$</td>
</tr>
</tbody>
</table>
<h1>4.4. Training / Test Set Separation</h1>
<p>In dividing our dataset into training and test sets, we employed a problem-wise division, allocating $80 \%$ of the problems to the training set and $20 \%$ to the test set instead of a sample-wise approach. This decision stems from our dataset's structure, which includes multiple solutions per problem. The sample-wise approach could include similar solutions for the same problem within the training and test sets. We opted for a problem-wise split to avoid this issue and enhance the model's generalization, ensuring the model is tested on unseen problem instances. Additionally, we repeat each experiment ten times for statistical reliability, each time using another seed for a different distribution of problems into training and test sets.</p>
<h3>4.5. Modeling Parameters and Tokenization</h3>
<p>For all SL methods, we use the default parameters proposed by scikit-learn [59] for RF, GB, LR, and DT, those of xgboost [47] for XGB, those of spyct [45] for OPCT, and those of TensorFlow [60] for DNN with two notable exceptions for DNNs (1) and OPCTs (2). For DNNs (1), we configure the network architecture to $[1536,768,512,128,32,8,1]$, employing the relu activation function across all layers except for the output layer, where sigmoid was used alongside binary cross-entropy as the loss function. In response to the strong fluctuation of the OPCTs (2), we create 10 individual trees and select the best of them. Additionally, we standardized the number of Gaussian components $K=2$ for all experiments with GMMs.</p>
<p>While embedding ADA uses internal tokenization which is the cl100k_base encoding, we must explicitly tokenize the prepared formatted or unformatted code for TF-IDF and Word2Vec. We decide to use the same tokenization cl100k_base encoding implemented in the tiktoken library [33] which was uniformly applied across all code snippets. Given the fixed size of the embedding of text-embedding-ada-002 with $\bar{x} \in \mathbb{R}^{1536}$, we adopted this dimensionality for the other embeddings. For TF-IDF, we retain sklearn's default parameters, while for gensim's [61] Word2Vec, we adjusted the threshold for a word's occurrence in the vocabulary to min_count $=1$ and use CBOW as the training algorithm.</p>
<h2>5. Results</h2>
<p>In this section, we present the primary outcomes from deploying the models introduced in Section 3, operating on the pre-processed dataset as shown in Section 4. We discuss the impact of different kinds of features (human-designed vs. embeddings) and the calibration of ML models. We then put the results into perspective by comparing them to the performance of untrained humans and a Bayes classifier.</p>
<h3>5.1. Similarities between Code Snippets</h3>
<p>The representation of the code snippets as embeddings describes a context-rich and high-dimensional vector space. However, the degree of similarity among code snippets</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Distribution of the code length (number of tokens according to cl100k_base encoding) across the unformatted and formatted dataset. Values larger than the 99% quantile were removed to avoid a distorted picture.</p>
<p>Within this space remains to be determined. Based on our balanced dataset and the code's functionality, we assume that the code samples are very similar. They are potentially even more similar when a code formatter, e.g., the Black code formatter [58], is used, which presents the models with considerable challenges in distinguishing subtle differences.</p>
<p>Mathematically, similarities in high-dimensional spaces can be particularly well calculated using cosine similarity. Let $H_P, G_P \in \mathcal{C}$ be code snippets for problem $P$ originating from humans and GPT, respectively. We compute the cosine similarity equivalent to Eq. (1) as $\operatorname{sim}(H_P, G_P) = \frac{H_P \cdot G_P}{|H_P ||G_P|}$. Concerning the embeddings of all formatted and unformatted code snippets generated by ADA, the resulting distributions of cosine similarities are presented in Figure 4. This allows us to mathematically confirm our assumption that the embeddings of the codes are very similar. The cosine similarities for both the embeddings of the formatted and unformatted code samples in the ADA-case are approximately normally distributed, resulting in very similar mean and standard deviation: $\overline{x_{FORM}} = 0.859 \pm 0.065$ and $\overline{x_{UNFORM}} = 0.863 \pm 0.067$. Figure 4 also shows the TF-IDF embeddings for both datasets. In contrast to the ADA embeddings, significantly lower cosine similarities can be identified. We suspect that this discrepancy arises because TF-IDF embeddings are sparse and based on exact word matches. In contrast, ADA embeddings are dense and capture semantic relationships and context. Finally, the average cosine similarities in the TF-IDF-case are $\overline{y_{FORM}} = 0.316 \pm 0.189$ for the formatted dataset and $\overline{y_{UNFORM}} = 0.252 \pm 0.166$ for the unformatted dataset.</p>
<p>Figure 4 demonstrates that the cosine similarities of embeddings vary largely, depending on the kind of embedding. But, as the results in Section 5.3 will show, ML models can effectively detect the code's origin from those embeddings. However, embeddings are black-box in the sense that the meaning of certain embedding dimensions is not explainable to humans. Consequently, we also investigated features that can be interpreted by humans to avoid the black-box setting with embeddings.</p>
<h3>5.2. Human-designed Features (white-box)</h3>
<p>A possible and comprehensible differentiation of the code samples can be attributed to their formatting. Even if these differences are not immediately visible to the human eye,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Cosine similarity between all human and GPT code samples embedded using ADA and TFIDF, both formatted and unformatted.
they can be determined with the help of calculations. To illustrate this, we have defined the features in Table 2. We assess their applicability using the presented SL models from Section 3.4. The results for the unformatted samples are displayed in Table 3, and for the formatted samples in Table 4.</p>
<p>Table 2. Detailed description of all human-designed features.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Abbreviation</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of leading whitespaces</td>
<td style="text-align: left;">$n_{\mathrm{LW}}$</td>
<td style="text-align: left;">The code sample is divided into individual <br> lines, and the number of leading whitespaces is <br> summed up.</td>
</tr>
<tr>
<td style="text-align: left;">Number of empty lines</td>
<td style="text-align: left;">$n_{\mathrm{EL}}$</td>
<td style="text-align: left;">The code sample is divided into individual <br> lines, and the number of empty is summed up.</td>
</tr>
<tr>
<td style="text-align: left;">Number of inline whitespaces</td>
<td style="text-align: left;">$n_{\mathrm{IW}}$</td>
<td style="text-align: left;">The code sample is divided into individual <br> lines, and the number of spaces within the <br> trimmed content of each line is summed up.</td>
</tr>
<tr>
<td style="text-align: left;">Number of punctuations</td>
<td style="text-align: left;">$n_{\mathrm{PT}}$</td>
<td style="text-align: left;">The code sample is filtered using the regular <br> expression [ $\left.\wedge \backslash w \backslash s\right]$ to isolate punctuation char- <br> acters, which are then counted.</td>
</tr>
<tr>
<td style="text-align: left;">Maximum line length</td>
<td style="text-align: left;">$n_{\mathrm{ML}}$</td>
<td style="text-align: left;">The code sample is split by every new line, and <br> the maximum count of characters of a line is <br> returned.</td>
</tr>
<tr>
<td style="text-align: left;">Number of trailing whitespaces</td>
<td style="text-align: left;">$n_{\mathrm{TW}}$</td>
<td style="text-align: left;">The code sample is divided into individual <br> lines, and the number of trailing whitespaces is <br> summed up.</td>
</tr>
<tr>
<td style="text-align: left;">Number of lines with leading <br> whitespaces</td>
<td style="text-align: left;">$n_{\mathrm{LWL}}$</td>
<td style="text-align: left;">The code sample is split by every new line, and <br> all lines starting with whitespaces are summed <br> up.</td>
</tr>
</tbody>
</table>
<p>We find that the selected features capture a large proportion of the differences on the unformatted dataset. The XGB model stands out as the most effective one, achieving</p>
<p>an average accuracy of $88.48 \%$ across various problem splits. However, when assessing the formatted dataset, there is a noticeable performance drop, with the XGB model's effectiveness decreasing by approximately 8 percentage points across all metrics. While outperforming all other models on the unformatted dataset, XGB slightly trails behind the RF model in the formatted dataset, where the RF model leads with an accuracy of $80.50 \%$. This aligns with our expectations that the differences are minimized when employing formatting. While human-designed features are valuable for distinguishing unformatted code, their effectiveness diminishes significantly when formatting variations are reduced. This fact is further supported by Figure 5, which reflects the normalized values of the individual features from Table 2 over the two data sets. The figure demonstrates an alignment of the feature value distributions for humans and ChatGPT after formatting, especially evident in features such as $n_{\mathrm{LWL}}$ and $n_{\mathrm{TW}}$, which show nearly identical values post-formatting.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Box plot of human-designed features for formatted and unformatted dataset.
Despite their astonishing performance at first glance, human-designed features do not lead to near-perfect classification results. This is likely due to the small number of features and the fact that they do not capture all available information. ML models that use the much higher-dimensional (richer) embedding space address these limitations by capturing implicit patterns not readily recognizable by human-designed features.</p>
<h1>5.3. Embedding Features (black-box)</h1>
<p>ML models operating on embeddings achieve superior performance compared to the models using the human-designed features; see Table 3 and Table 4 for results on the unformatted and formatted dataset, respectively. The results on the unformatted dataset show that the highest values are achieved by XGB + TF-IDF with an accuracy greater than $98 \%$ and an astonishing AUC value of $99.84 \%$. With a small gap to XBG + TF-IDF across all metrics, RF + TF-IDF is in second place. With less than one percentage point difference to XGB + TF-IDF, DNN + ADA is in third place. An identical ranking of the top 3 models can be found on the formatted dataset. As with the human-designed features, 'formatted' shows weaker performance than 'unformatted', with a deterioration of about 4 percentage points on all metrics except AUC, which only decreased by about 1 percentage point. In a direct comparison of models based on either human-designed features or embeddings, the best-</p>
<p>Table 3. Final results - Shown are the mean $\mu \pm$ standard deviation $\sigma$ from 10 independent runs on unformatted data; each run corresponds to a different seed, i.e., different distribution of training and test samples. Boldface: column maximum of $\mu$ in the 'Human-designed (white-box)' box and for each individual embedding model in the 'Embedding (black-box)' box.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Unformatted</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy (\%)</td>
<td style="text-align: center;">Precision (\%)</td>
<td style="text-align: center;">Recall (\%)</td>
<td style="text-align: center;">F1 (\%)</td>
<td style="text-align: center;">AUC (\%)</td>
</tr>
<tr>
<td style="text-align: center;">Human-designed (white-box)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CART</td>
<td style="text-align: center;">$82.54 \pm 0.23$</td>
<td style="text-align: center;">$82.70 \pm 0.36$</td>
<td style="text-align: center;">$82.29 \pm 0.57$</td>
<td style="text-align: center;">$82.49 \pm 0.26$</td>
<td style="text-align: center;">$82.54 \pm 0.23$</td>
</tr>
<tr>
<td style="text-align: center;">DNN</td>
<td style="text-align: center;">$85.37 \pm 0.80$</td>
<td style="text-align: center;">$84.58 \pm 2.73$</td>
<td style="text-align: center;">$86.84 \pm 4.77$</td>
<td style="text-align: center;">$85.54 \pm 1.21$</td>
<td style="text-align: center;">$93.73 \pm 0.42$</td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">$79.68 \pm 0.50$</td>
<td style="text-align: center;">$74.74 \pm 0.66$</td>
<td style="text-align: center;">$89.74 \pm 0.58$</td>
<td style="text-align: center;">$81.55 \pm 0.39$</td>
<td style="text-align: center;">$89.05 \pm 0.25$</td>
</tr>
<tr>
<td style="text-align: center;">LR</td>
<td style="text-align: center;">$76.69 \pm 0.63$</td>
<td style="text-align: center;">$74.80 \pm 0.64$</td>
<td style="text-align: center;">$80.50 \pm 0.94$</td>
<td style="text-align: center;">$77.54 \pm 0.63$</td>
<td style="text-align: center;">$84.57 \pm 0.41$</td>
</tr>
<tr>
<td style="text-align: center;">OPCT</td>
<td style="text-align: center;">$84.12 \pm 0.60$</td>
<td style="text-align: center;">$80.89 \pm 1.95$</td>
<td style="text-align: center;">$89.52 \pm 2.37$</td>
<td style="text-align: center;">$84.94 \pm 0.43$</td>
<td style="text-align: center;">$89.60 \pm 0.81$</td>
</tr>
<tr>
<td style="text-align: center;">RF</td>
<td style="text-align: center;">$88.10 \pm 0.40$</td>
<td style="text-align: center;">$87.29 \pm 0.41$</td>
<td style="text-align: center;">$89.19 \pm 0.66$</td>
<td style="text-align: center;">$88.23 \pm 0.41$</td>
<td style="text-align: center;">$95.34 \pm 0.22$</td>
</tr>
<tr>
<td style="text-align: center;">XGB</td>
<td style="text-align: center;">$88.48 \pm 0.26$</td>
<td style="text-align: center;">$87.39 \pm 0.46$</td>
<td style="text-align: center;">$89.93 \pm 0.42$</td>
<td style="text-align: center;">$88.64 \pm 0.24$</td>
<td style="text-align: center;">$95.59 \pm 0.20$</td>
</tr>
<tr>
<td style="text-align: center;">Embedding (black-box)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CART</td>
<td style="text-align: center;">$80.82 \pm 0.50$</td>
<td style="text-align: center;">$80.63 \pm 0.55$</td>
<td style="text-align: center;">$81.46 \pm 0.67$</td>
<td style="text-align: center;">$79.83 \pm 1.03$</td>
<td style="text-align: center;">$80.82 \pm 0.50$</td>
</tr>
<tr>
<td style="text-align: center;">DNN</td>
<td style="text-align: center;">$97.79 \pm 0.36$</td>
<td style="text-align: center;">$97.40 \pm 0.77$</td>
<td style="text-align: center;">$98.22 \pm 0.60$</td>
<td style="text-align: center;">$97.80 \pm 0.35$</td>
<td style="text-align: center;">$99.76 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">$92.71 \pm 0.39$</td>
<td style="text-align: center;">$95.10 \pm 0.48$</td>
<td style="text-align: center;">$90.06 \pm 0.66$</td>
<td style="text-align: center;">$92.51 \pm 0.42$</td>
<td style="text-align: center;">$97.37 \pm 0.21$</td>
</tr>
<tr>
<td style="text-align: center;">LR</td>
<td style="text-align: center;">$95.87 \pm 0.13$</td>
<td style="text-align: center;">$95.93 \pm 0.13$</td>
<td style="text-align: center;">$94.60 \pm 0.24$</td>
<td style="text-align: center;">$97.30 \pm 0.25$</td>
<td style="text-align: center;">$99.06 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">OPCT</td>
<td style="text-align: center;">$95.53 \pm 0.27$</td>
<td style="text-align: center;">$95.59 \pm 0.25$</td>
<td style="text-align: center;">$94.35 \pm 0.81$</td>
<td style="text-align: center;">$96.87 \pm 0.65$</td>
<td style="text-align: center;">$97.24 \pm 0.46$</td>
</tr>
<tr>
<td style="text-align: center;">RF</td>
<td style="text-align: center;">$92.39 \pm 0.35$</td>
<td style="text-align: center;">$92.55 \pm 0.33$</td>
<td style="text-align: center;">$90.67 \pm 0.63$</td>
<td style="text-align: center;">$94.52 \pm 0.55$</td>
<td style="text-align: center;">$97.85 \pm 0.14$</td>
</tr>
<tr>
<td style="text-align: center;">XGB</td>
<td style="text-align: center;">$95.05 \pm 0.23$</td>
<td style="text-align: center;">$95.09 \pm 0.22$</td>
<td style="text-align: center;">$94.30 \pm 0.42$</td>
<td style="text-align: center;">$95.89 \pm 0.30$</td>
<td style="text-align: center;">$98.94 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;">TF-IDF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CART</td>
<td style="text-align: center;">$95.82 \pm 0.31$</td>
<td style="text-align: center;">$95.82 \pm 0.31$</td>
<td style="text-align: center;">$95.93 \pm 0.26$</td>
<td style="text-align: center;">$95.71 \pm 0.52$</td>
<td style="text-align: center;">$95.82 \pm 0.31$</td>
</tr>
<tr>
<td style="text-align: center;">DNN</td>
<td style="text-align: center;">$97.61 \pm 0.30$</td>
<td style="text-align: center;">$97.79 \pm 0.79$</td>
<td style="text-align: center;">$97.45 \pm 1.17$</td>
<td style="text-align: center;">$97.61 \pm 0.32$</td>
<td style="text-align: center;">$99.68 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">$94.60 \pm 0.32$</td>
<td style="text-align: center;">$95.24 \pm 0.26$</td>
<td style="text-align: center;">$93.89 \pm 0.60$</td>
<td style="text-align: center;">$94.56 \pm 0.34$</td>
<td style="text-align: center;">$96.18 \pm 0.26$</td>
</tr>
<tr>
<td style="text-align: center;">LR</td>
<td style="text-align: center;">$97.06 \pm 0.24$</td>
<td style="text-align: center;">$97.09 \pm 0.24$</td>
<td style="text-align: center;">$96.11 \pm 0.28$</td>
<td style="text-align: center;">$98.09 \pm 0.31$</td>
<td style="text-align: center;">$99.46 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">OPCT</td>
<td style="text-align: center;">$96.72 \pm 0.26$</td>
<td style="text-align: center;">$96.72 \pm 0.26$</td>
<td style="text-align: center;">$96.78 \pm 0.50$</td>
<td style="text-align: center;">$96.66 \pm 0.50$</td>
<td style="text-align: center;">$97.62 \pm 0.41$</td>
</tr>
<tr>
<td style="text-align: center;">RF</td>
<td style="text-align: center;">$98.04 \pm 0.11$</td>
<td style="text-align: center;">$98.04 \pm 0.11$</td>
<td style="text-align: center;">$97.77 \pm 0.18$</td>
<td style="text-align: center;">$98.31 \pm 0.23$</td>
<td style="text-align: center;">$99.72 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">XGB</td>
<td style="text-align: center;">$98.28 \pm 0.09$</td>
<td style="text-align: center;">$97.87 \pm 0.15$</td>
<td style="text-align: center;">$98.70 \pm 0.22$</td>
<td style="text-align: center;">$98.28 \pm 0.09$</td>
<td style="text-align: center;">$99.84 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">WORD2VEC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GMM</td>
<td style="text-align: center;">$93.57 \pm 0.20$</td>
<td style="text-align: center;">$92.49 \pm 0.37$</td>
<td style="text-align: center;">$94.86 \pm 0.31$</td>
<td style="text-align: center;">$93.66 \pm 0.19$</td>
<td style="text-align: center;">$94.14 \pm 0.14$</td>
</tr>
</tbody>
</table>
<p>performing models of each type show a difference of approximately 10 and 14 percentage points across all metrics except the AUC score on the unformatted and formatted dataset, respectively.</p>
<p>The disparities in performance between the white- and black-box approaches, despite employing identical models, highlight the significance of embedding techniques. While traditional feature engineering is based on domain-specific expertise or interpretability, it often cannot capture the complex details of code snippets as effectively as embedding features.</p>
<h1>5.4. Gaussian Mixture Models</h1>
<p>Although GMMs are, in principle, capable of unsupervised learning, they reach better classification accuracies in a supervised or semi-supervised setting: the class labels are provided during training, but the assignment of data points to one of the $K$ GMM components has to be found by the EM method described in Section 3.5. Our method proceeds as follows: First, we train two independent GMMs, one exclusively on human</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Distribution of the F1 Score of all considered algorithms in different conditions. In both ADA cases, the single outlier is CART.</p>
<p>Table 4. Same as Table 3 but for formatted data.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Formatted</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Accuracy (\%)</td>
<td>Precision (\%)</td>
<td>Recall (\%)</td>
<td>F1 (\%)</td>
<td>AUC (\%)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Human-designed (white-box)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CART</td>
<td>$72.74 \pm 0.58$</td>
<td>$72.94 \pm 0.54$</td>
<td>$72.31 \pm 1.24$</td>
<td>$72.62 \pm 0.72$</td>
<td>$72.75 \pm 0.57$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DNN</td>
<td>$77.51 \pm 0.89$</td>
<td>$78.15 \pm 3.65$</td>
<td>$77.18 \pm 6.72$</td>
<td>$77.31 \pm 2.33$</td>
<td>$86.69 \pm 0.30$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GMM</td>
<td>$74.09 \pm 0.65$</td>
<td>$73.37 \pm 1.78$</td>
<td>$75.83 \pm 2.45$</td>
<td>$74.53 \pm 0.53$</td>
<td>$82.17 \pm 0.50$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LR</td>
<td>$73.03 \pm 0.56$</td>
<td>$73.22 \pm 0.77$</td>
<td>$72.65 \pm 0.68$</td>
<td>$72.93 \pm 0.52$</td>
<td>$80.61 \pm 0.36$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OPCT</td>
<td>$73.63 \pm 1.50$</td>
<td>$69.81 \pm 3.70$</td>
<td>$\mathbf{8 4 . 3 8} \pm \mathbf{4 . 8 8}$</td>
<td>$76.18 \pm 0.85$</td>
<td>$80.78 \pm 1.76$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RF</td>
<td>$\mathbf{8 0 . 5 0} \pm \mathbf{0 . 4 3}$</td>
<td>$\mathbf{8 1 . 8 3} \pm \mathbf{0 . 4 9}$</td>
<td>$78.42 \pm 0.69$</td>
<td>$80.09 \pm 0.47$</td>
<td>$\mathbf{8 8 . 8 6} \pm \mathbf{0 . 3 0}$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>XGB</td>
<td>$80.29 \pm 0.54$</td>
<td>$80.57 \pm 0.62$</td>
<td>$79.84 \pm 0.69$</td>
<td>$\mathbf{8 0 . 2 0} \pm \mathbf{0 . 5 5}$</td>
<td>$88.65 \pm 0.35$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Embedding (black-box)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ADA</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CART</td>
<td>$74.33 \pm 0.58$</td>
<td>$73.89 \pm 0.58$</td>
<td>$75.19 \pm 0.67$</td>
<td>$72.63 \pm 0.68$</td>
<td>$74.33 \pm 0.58$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DNN</td>
<td>$\mathbf{9 3 . 1 4} \pm \mathbf{0 . 8 7}$</td>
<td>$\mathbf{9 2 . 4 6} \pm \mathbf{2 . 9 5}$</td>
<td>$\mathbf{9 4 . 1 2} \pm \mathbf{2 . 2 1}$</td>
<td>$\mathbf{9 3 . 2 2} \pm \mathbf{0 . 6 9}$</td>
<td>$\mathbf{9 8 . 6 6} \pm \mathbf{0 . 1 0}$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GMM</td>
<td>$86.10 \pm 0.40$</td>
<td>$88.16 \pm 0.55$</td>
<td>$83.41 \pm 0.55$</td>
<td>$85.71 \pm 0.41$</td>
<td>$93.03 \pm 0.31$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LR</td>
<td>$91.16 \pm 0.35$</td>
<td>$91.21 \pm 0.35$</td>
<td>$90.72 \pm 0.48$</td>
<td>$91.70 \pm 0.56$</td>
<td>$97.29 \pm 0.17$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OPCT</td>
<td>$89.99 \pm 0.40$</td>
<td>$90.03 \pm 0.41$</td>
<td>$89.67 \pm 1.42$</td>
<td>$90.44 \pm 1.72$</td>
<td>$94.56 \pm 0.86$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RF</td>
<td>$85.89 \pm 0.34$</td>
<td>$85.94 \pm 0.37$</td>
<td>$85.63 \pm 0.59$</td>
<td>$86.26 \pm 0.89$</td>
<td>$94.25 \pm 0.18$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>XGB</td>
<td>$89.62 \pm 0.15$</td>
<td>$89.63 \pm 0.18$</td>
<td>$89.58 \pm 0.25$</td>
<td>$89.67 \pm 0.48$</td>
<td>$96.72 \pm 0.14$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TF-IDF</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CART</td>
<td>$89.69 \pm 0.41$</td>
<td>$89.65 \pm 0.43$</td>
<td>$90.01 \pm 0.43$</td>
<td>$89.30 \pm 0.67$</td>
<td>$89.69 \pm 0.41$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DNN</td>
<td>$93.17 \pm 0.29$</td>
<td>$93.48 \pm 0.90$</td>
<td>$92.83 \pm 1.06$</td>
<td>$93.14 \pm 0.30$</td>
<td>$98.36 \pm 0.16$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GMM</td>
<td>$87.53 \pm 0.52$</td>
<td>$88.14 \pm 0.61$</td>
<td>$86.74 \pm 0.67$</td>
<td>$87.43 \pm 0.53$</td>
<td>$91.01 \pm 0.58$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LR</td>
<td>$92.50 \pm 0.36$</td>
<td>$92.52 \pm 0.37$</td>
<td>$92.32 \pm 0.37$</td>
<td>$92.72 \pm 0.58$</td>
<td>$97.95 \pm 0.22$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OPCT</td>
<td>$90.74 \pm 0.57$</td>
<td>$90.78 \pm 0.53$</td>
<td>$90.48 \pm 1.61$</td>
<td>$91.12 \pm 1.51$</td>
<td>$94.76 \pm 0.64$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RF</td>
<td>$93.36 \pm 0.37$</td>
<td>$93.29 \pm 0.38$</td>
<td>$\mathbf{9 4 . 3 3} \pm \mathbf{0 . 3 6}$</td>
<td>$92.27 \pm 0.44$</td>
<td>$98.55 \pm 0.15$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>XGB</td>
<td>$\mathbf{9 3 . 9 8} \pm \mathbf{0 . 4 4}$</td>
<td>$\mathbf{9 3 . 9 8} \pm \mathbf{0 . 4 4}$</td>
<td>$93.96 \pm 0.51$</td>
<td>$\mathbf{9 4 . 0 1} \pm \mathbf{0 . 4 8}$</td>
<td>$\mathbf{9 8 . 8 0} \pm \mathbf{0 . 1 6}$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>WORD2VEC</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GMM</td>
<td>$87.92 \pm 0.45$</td>
<td>$87.61 \pm 0.82$</td>
<td>$88.35 \pm 0.95$</td>
<td>$87.97 \pm 0.45$</td>
<td>$89.26 \pm 0.41$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>samples and the other on GPT samples. For the prediction of an embedded sample $\vec{x}$, we then calculate the likelihood:</p>
<p>$$
\begin{aligned}
p\left(\vec{x} ; \mathcal{G}<em k="1">{\mathrm{AI}}\right) &amp; =\sum</em>}^{K} \psi_{k} \mathcal{N}\left(\vec{x} \mid \vec{\mu<em k="k">{k}^{(\mathrm{AI})}, \Sigma</em>\right) \
p\left(\vec{x} ; \mathcal{G}}^{(\mathrm{AI})<em k="1">{\mathrm{HU}}\right) &amp; =\sum</em>}^{K} \psi_{k} \mathcal{N}\left(\vec{x} \mid \vec{\mu<em k="k">{k}^{(\mathrm{HU})}, \Sigma</em>\right)
\end{aligned}
$$}^{(\mathrm{HU})</p>
<p>where $p\left(\vec{x} ; \mathcal{G}<em _mathrm_HU="\mathrm{HU">{\mathrm{AI}}\right)$ and $p\left(\vec{x} ; \mathcal{G}</em>$ to the class with the higher probability.}}\right)$ denote the probability density functions of $\vec{x}$ under the GMMs, respectively, and assign $\vec{x</p>
<p>When using ADA, the embedding can be performed on individual code snippets $x$ directly before the GMM finds clusters in the embeddings $\mathcal{E}(x)$. When applying TF-IDF embedding, instead, tokenization $\mathcal{T}$ is required (to determine the number of tokens in each snippet) before the embeddings are computed on the tokenized code snippets $\mathcal{E}(\mathcal{T}(x))$. With both embeddings, GMMs achieve accuracies of over $90 \%$, outperforming any of the models based on human-designed features (see Table 3).</p>
<p>The underlying concept of GMMs is the approximation of the probability distributions of ChatGPT, which does not extend over entire snippets but rather on individual tokens used for the prediction of the following token. It appears, therefore, naturally, to apply Word2Vec for the embedding of single tokens instead of the snippet-level embedding used in ADA and TF-IDF. ${ }^{1}$ With the single-token Word2Vec embedding, GMMs reach an accuracy of $93.57 \%$.</p>
<p>To summarize, we show in Figure 6 the box plots for all our main results. Each box group contains all models trained on a particular (feature set, format)-combination. Within a certain format choice, the box plots for the embedding feature sets do not overlap with those for the human-designed feature set. This shows that the choice 'embedding vs. human-designed' is more important than the specific ML model.</p>
<h1>5.5. Model Calibration</h1>
<p>Previously, we focused on how well the considered algorithms can discriminate between code snippets generated by humans and GPT-generated code snippets. This is an important aspect when judging the performance of these algorithms. However, it also reduces the problem to one of classification, e.g., the algorithm only tells us whether a code snippet is generated by humans or by GPT. In many cases, we may also be interested in more nuanced judgments, e.g., in how likely it is that GPT or humans generated a code snippet. All the considered algorithms are theoretically able to output such predicted class probabilities. In this section, we evaluate how well these predicted probabilities correspond to the proportion of actually observed cases.</p>
<p>We mainly use calibration plots to do this for a subset of the considered algorithms. In such plots, the predicted probability of a code snippet being generated by GPT is shown on the $x$-axis, while the actually observed value ( 0 if human-generated, 1 if GPT generated) is shown on the $y$-axis. A simple version of this plot would divide the predicted probabilities into categories, for example, ten equally wide ones [62]. The proportion of code snippets labeled as GPT inside those categories should ideally be equal to the mean predicted probability inside this category. For example, in the category $10-20 \%$, the proportion of actual GPT samples should roughly equal $15 \%$. We use a smooth variant of this plot by calculating and plotting a non-parametric locally weighted regression (LOESS) instead, which does not require the use of arbitrary categories [63].</p>
<p>Figures 7 and 9 show these calibration plots for each algorithm, separately for formatted and unformatted data. Most of the considered algorithms show adequate calibration,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Calibration plots for selected classifiers trained and evaluated using unformatted test-set data. The lines were estimated using LOESS regression models (gray: individual runs, blue: mean over all runs). Dashed diagonal line: perfect calibration.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Kernel density estimates of the probabilities predicted by each considered classifier in the unformatted test set. The plot contains one line per run.
with the estimated LOESS regression being close to the line that goes through the origin. There are only minor differences between algorithms fitted on formatted vs unformatted data. For some algorithms, however (RF + ADA, RF + TF-IDF, GMM + Word2VEC, GMM + TF-IDF), there seem to be some issues with the calibration in predicted probabilities between 0.15 and 0.85 . However, one possible reason for this is not a lack of calibration but a lack of suitable data points to correctly fit the LOESS regression line. For example, the GMM-based models almost always predict only probabilities very close to 0 or 1 . This is</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p><strong>Figure 9.</strong> Calibration plots for selected classifiers trained and evaluated using <strong>formatted</strong> test-set data. The lines were estimated using LOESS regression models (gray: individual runs, blue: mean over all runs). Dashed diagonal line: perfect calibration.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p><strong>Figure 10.</strong> Kernel density estimates of the probabilities predicted by each considered classifier in the <strong>formatted</strong> test set. The plot contains one line per run.</p>
<p>Not necessarily a problem of calibration (if the algorithm is correct most of the time), but it may lead to unstable LOESS results [63].</p>
<p>We, therefore, additionally plotted kernel density estimates of the predicted probabilities for each algorithm to show the range of predictions made by each one (Figures 8 and 10). As can be seen quite clearly, most of the DNN and GMM-based models relying on embeddings generated only very few predicted probabilities between 0.1 and 0.9, making the validity of the calibration curves in these ranges questionable for those algorithms.</p>
<p>Since these models do not discriminate perfectly between the two classes, these models may not be the best choice when the main interest lies in predicting the probability of a code snippet being generated by GPT because their output always suggests certainty, even when it is wrong. On the other hand, algorithms such as XGB + TF-IDF or XGB + ADA show a nearly perfect calibration and a very high accuracy.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Performance distribution of study participants</p>
<h1>5.6. Human Agents and Bayes Classifiers</h1>
<p>To gain a better understanding about the "baseline" performance, to which we compare the more sophisticated models, we briefly present the results of untrained humans and a Bayes classifier.</p>
<h3>5.6.1. Untrained Human Agents</h3>
<p>To assess how difficult it is for humans to classify Python code snippets according to their origin, we conducted a small study with 20 participants. Using Google Forms, the individuals were asked to indicate their educational background, their experience in Python, and to self-asses their programming proficiency on a 10-point Likert scale. They were afterward asked to judge whether 20 randomly selected and ordered code snippets were written by humans or by ChatGPT. The dataset was balanced, but the participants were not told. The participants, of which $50 \%$ had a Master's degree and $40 \%$ held a PhD degree, self-rated their programming skills at an average of $6.85 \pm 1.69$ with a median of 7 and indicated an average of $5.05 \pm 4.08$ (median 5) years of Python programming experience. The result in Table 5 shows a performance slightly below random guessing, confirming this task's difficulty for untrained humans. Figure 11 shows the distribution of the participant's performances.</p>
<p>Table 5. Results of human performance on the classification task</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Accuracy (\%)</th>
<th style="text-align: left;">Precision (\%)</th>
<th style="text-align: left;">Recall (\%)</th>
<th style="text-align: left;">F1 (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$48.7 \pm 14.7$</td>
<td style="text-align: left;">$48.4 \pm 14.8$</td>
<td style="text-align: left;">$48.0 \pm 18.9$</td>
<td style="text-align: left;">$47.6 \pm 16.0$</td>
</tr>
</tbody>
</table>
<p>When comparing these results to the ones obtained by ML models, it should be taken into account that the participants were not trained on the task and are, therefore, considered zero-shot learners. How well humans can be trained in the task using labeled data remains to be investigated.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. Top 40 tokens with the largest absolute discrepancies in their log probabilities (which corresponds to the largest ratio of probabilities).</p>
<h1>5.6.2. Trained Bayes Classifier</h1>
<p>Viewed from a broader perspective, the question raised by the results from the preceding subsection is why ML models with an F1 score between $83 \%$ and $98 \%$ excel so much over (untrained) humans who show a performance close to random guessing. In the following, we investigate the possibility (which also appeared briefly in Li et al. [32]) that ML models might use subtle differences in conditional probabilities for the appearance of tokens for decision-making. Such probabilities might be complex for humans to calculate, memorize, and combine.</p>
<p>To demonstrate this, we build a simple Bayes classifier: In the following, we abbreviate $H=$ human origin, $G=$ GPT origin, and $X=$ either origin. For each token $t_{k}$ in our training dataset, we calculate the probabilities $P\left(t_{k} \mid X\right)$. We keep for reliable estimates only those tokens $t_{k}$ where the absolute frequencies $n\left(t_{k} \mid H\right)$ and $n\left(t_{k} \mid G\right)$ are both greater-equal some predefined threshold $\tau$. The set of those tokens above the threshold shall be $T$. For illustration, Figure 12 shows the tokens with the largest probability ratio.</p>
<p>Given a new code document $D$ from the test dataset, we determine the intersection $D \cap T$ of tokens and enumerate the elements in this intersection as $\left{T_{1}, \ldots, T_{K}\right}$, where $T_{k}$ means the event "Document $D$ contains token $t_{k}$ ". We assume statistical independence: $P\left(\cap_{k=1}^{K} T_{k} \mid X\right)=\prod_{k=1}^{K} P\left(T_{k} \mid X\right)$. Now we can calculate with Bayes's law the probabilities of origin:</p>
<p>$$
\begin{aligned}
P\left(H \mid \cap_{k=1}^{K} T_{k}\right) &amp; =\frac{\left(P\left(\cap_{k=1}^{K} T_{k} \mid H\right) P(H)\right)}{\left(P\left(\cap_{k=1}^{K} T_{k} \mid H\right) P(H)+P\left(\cap_{k=1}^{K} T_{k} \mid G\right) P(G)\right)} \
&amp; =\frac{\left[\prod_{k=1}^{K} P\left(T_{k} \mid H\right)\right] P(H))}{\left[\prod_{k=1}^{K} P\left(T_{k} \mid H\right)\right] P(H)+\left[\prod_{k=1}^{K} P\left(T_{k} \mid G\right)\right] P(G)}
\end{aligned}
$$</p>
<p>and, similarly, $P\left(G \mid \cap_{k=1}^{K} T_{k}\right)$. By definition, we have $P\left(H \mid \cap_{k=1}^{K} T_{k}\right)+P\left(G \mid \cap_{k=1}^{K} T_{k}\right)=1$.
The Bayes classifier classifies document $D$ as being of GPT-origin, if $P\left(G \mid \cap_{k=1}^{K} T_{k}\right)&gt;$ $P\left(H \mid \cap_{k=1}^{K} T_{k}\right)$. Eq. (4) might look complex, but it is just a simple multiply-add of probabilities, straightforward to calculate from the training dataset. It is a simple calculation for a machine but difficult for a human just looking at a code snippet.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>1 It should be noted that while this approach would technically also work for ADA, the computational overhead makes it infeasible (as a rough estimate we consider $\approx 2.6 \times 10^{6}$ tokens and an average API response time of $\approx 5$ seconds).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
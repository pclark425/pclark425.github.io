<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-378 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-378</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-378</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-198251784</p>
                <p><strong>Paper Title:</strong> Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets</p>
                <p><strong>Paper Abstract:</strong> Most machine learning algorithms require that training data are identically distributed to ensure effective learning. In biological studies, however, even small variations in the experimental setup can lead to substantial deviations. Domain adaptation offers tools to deal with this problem. It is particularly useful for cases where only a small amount of training data is available in the domain of interest, while a large amount of training data is available in a different, but relevant domain. We investigated to what extent domain adaptation was able to improve prediction accuracy for complex biological data. To that end, we used simulated data and time-lapse movies of differentiating blood stem cells in different cell cycle stages from multiple experiments and compared three commonly used domain adaptation approaches. EasyAdapt, a simple technique of structured pooling of related data sets, was able to improve accuracy when classifying the simulated data and cell cycle stages from microscopic images. Meanwhile, the technique proved robust to the potential negative impact on the classification accuracy that is common in other techniques that build models with heterogeneous data. Despite its implementation simplicity, EasyAdapt consistently produced more accurate predictions compared to conventional techniques. Domain adaptation is therefore able to substantially reduce the amount of work required to create a large amount of annotated training data in the domain of interest necessary whenever the domain changes even a little, which is common not only in biological experiments, but universally exists in almost all data collection routines.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e378.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e378.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EasyAdapt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frustratingly Easy Domain Adaptation (EasyAdapt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple feature-transformation domain adaptation technique that augments a shared feature space with domain-specific copies so a single supervised learner can use both general and domain-specific signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Frustratingly Easy Domain Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>EasyAdapt domain adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Creates an augmented feature representation by concatenating a shared (general) feature block with domain-specific feature blocks (one per source domain plus one for the target). Each input x from a domain d is mapped to a vector that contains the original features in the shared block and a copy only in the block corresponding to d (zeros elsewhere). Any supervised classifier can then be trained on the expanded feature space so that it learns both cross-domain structure and domain-specific adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (domain adaptation / feature augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computer science / natural language processing / general machine learning (text & vision domain adaptation literature)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>biological data analysis (simulated biological data; time-lapse microscopy single-cell imaging; cell-cycle stage classification)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (directly applied with domain-specific preprocessing and classifier choices)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied the EasyAdapt transformation to imaging and simulated biological feature vectors; preprocessed by centering and scaling to unit variance, applied PCA dimensionality reduction (either to explain 98% variance or keep top 16 PCs), and used off-the-shelf supervised classifiers (linear SVM, RBF-SVM, random forest) on the augmented feature vectors. Feature blocks were created per experimental replicate (domain). No changes to the core EasyAdapt mapping were necessary, but practical adaptations included feature scaling, PCA, and selection of classifier hyperparameters via grid search with cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>successful - improved target-domain classification in both simulation and real imaging data. In simulation, EasyAdapt achieved AUC=0.91 (best among compared techniques). In the imaging data across 18 experimental settings EasyAdapt ranked best or tied for best in 15 settings and second in 3 settings, showing consistent superiority especially at small-to-medium target training sizes; it also avoided negative transfer observed for some other pooling methods when magnification differed.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Increased feature-space dimensionality (grows linearly with number of domains and feature dimensionality) which can be problematic for very large feature sets or many domains; requires identification of discrete domains and a shared feature subspace; potential overfitting if domain-specific blocks are high-dimensional relative to available target training data.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of a shared feature representation across domains (same measured features), moderate number of domains and feature dimensionality, substantial labeled source data, some similarity (relatedness) between source and target distributions, and standard preprocessing (scaling, PCA) and classifier tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Need labeled source domains and at least small labeled target set; domains must be identifiable (e.g., experimental replicates); shared feature space required; computational resources to handle augmented feature dimensionality; standard ML pipeline (scaling, PCA, cross-validation) used.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within supervised learning problems that provide a common feature space across domains; the authors state the method is classifier-agnostic and general, but limited when feature dimensionality or number of domains becomes large; applicable beyond imaging to other biological and non-biological problems where domains correspond to replicates, machines, protocols, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (feature-augmentation mapping and learning paradigm), with technical/implementational know-how (preprocessing, PCA, classifier selection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e378.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e378.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain (one-hot) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain membership one-hot encoding (Domain technique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical technique that augments the feature set with binary indicator variables encoding domain membership, allowing a single model to learn domain-specific offsets while sharing feature-based structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Domain membership one-hot encoding for pooled learning</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Adds one-hot encoded binary variables indicating the domain (e.g., which experimental replicate) to each feature vector; trains a standard supervised classifier on the pooled data so the model can learn different intercepts/offsets per domain while sharing parameterization of feature effects across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (feature augmentation for multi-domain learning)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine learning practice (used in multi-domain/multi-center studies and statistical modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>biological data analysis (time-lapse microscopy single-cell classification across experimental replicates)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without major modification (applied as-is to biological imaging data)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>No fundamental change to the method; applied after standard preprocessing (centering, scaling) and PCA for dimensionality reduction before classifier training. Domain membership encoded as additional binary features corresponding to each experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful - in many settings it improved performance (ranked second best across many settings) but in at least one transfer direction (experiment 2 as target with different magnification) it produced negative transfer and performed worse than training on the small target set alone.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Negative transfer when domains differ strongly (e.g., different microscope magnification) because pooled source data can bias the learned model; relies on domain labels being informative; does not separate shared vs. domain-specific parameterization beyond intercept shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Simple implementation; ability to leverage pooled data if domains are sufficiently similar; availability of explicit domain labels (replicate identity) facilitated this encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Domain membership must be known and encoded; shared features across domains; adequate preprocessing and hyperparameter tuning; caution when domains are heterogeneous (e.g., differing magnification or acquisition protocols).</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderately general but sensitive to domain heterogeneity; applicable to many pooled-multi-site biological studies but can cause negative transfer when domain differences are large.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and practical know-how (how to encode domain membership and integrate into ML pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e378.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e378.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Combined pooling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combined (pooled training) technique</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A straightforward approach that pools labeled data from source and target domains and trains a classifier on the merged set without encoding domain membership, weighting every example equally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Combined pooled-data training</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Merge labeled examples from one or more source domains with labeled examples from the target domain into a single training set, perform standard preprocessing and train a supervised classifier on the pooled data without special domain-specific features or transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / data analysis technique (data pooling / simple transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general machine learning practice (pooling datasets across studies/domains)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>biological data analysis (cell imaging classification across experiments/replicates)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application without modification (naive pooling)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Applied standard preprocessing (centering, scaling, PCA) and trained SVMs or random forest on pooled features; no explicit domain encoding or domain-specific model components were added.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>partially successful but risky - in many cases pooling improved learning when domains were similar, but caused negative transfer (degraded performance) when source and target distributions differed strongly (e.g., different magnification in experiment 2), performing worse than target-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Sensitivity to batch effects and domain heterogeneity; large source datasets can bias the model away from the limited target examples; lack of mechanism to compensate for distributional shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>When domains are similar or differences are small, large source datasets provide useful additional labeled data; simple to implement and requires no extra domain metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires that pooled data be commensurate (shared feature space and similar distributions) for successful transfer; careful preprocessing and model selection are necessary to mitigate negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>General but brittle; applies broadly across domains but may fail when domain shifts are large; the paper recommends caution and alternative domain-adaptation methods when domain differences exist.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and practical know-how (how to pool and preprocess data for classifier training).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e378.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e378.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain adaptation (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain adaptation (a form of transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of supervised learning methods that leverage labeled data from one or more source domains to improve learning in a related but different target domain by addressing distributional shifts between domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>domain adaptation / transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>General paradigm where models trained on source-domain data are adapted to work on a target domain with limited labeled data; methods include feature-based transformations (e.g., EasyAdapt), instance weighting, parameter sharing, adversarial alignment, and multi-task regularization to reduce distribution mismatch between P_source(X) and P_target(X).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / learning paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>originally extensively developed in natural language processing and computer vision (text classification literature and visual domain adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>biological and biomedical data analysis (e.g., genomic sequence analysis, imaging, cell phenotype classification)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>analogical transfer and methodological import — techniques originally developed in text/vision are applied to biological data analysis with domain-specific preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>In this paper, general domain adaptation ideas are instantiated using EasyAdapt and other pooling/encoding strategies; in the literature other adaptations include adversarial training, residual adapters for neural nets, and joint adaptation networks, which are referenced but not implemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>generally successful when appropriately chosen: the paper demonstrates that domain-adaptation ideas (in particular EasyAdapt) can substantially reduce the amount of target labeling required and improve classification accuracy; success depends on domain relatedness and method choice.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Biological batch effects, small labeled target sets, changes in acquisition (magnification, objectives), and the need for a shared feature subspace; potential for negative transfer when domains are too different.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Underlying shared biological signal across experiments, explicit domain labels, availability of abundant source annotations, and preprocessing (scaling/PCA) enable effective adoption of domain adaptation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Knowledge of domain identities, shared feature definitions across datasets, at least some labeled target examples for supervised adaptation, and computational infrastructure to train and validate adapted models.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Broadly generalizable across data types and biological applications provided domain relationships and shared features exist; many modern domain-adaptation methods (including those for deep learning) are noted as applicable but orthogonal to the EasyAdapt approach.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>theoretical principles and explicit procedural frameworks (paradigms for how to share information across domains and mitigate distribution shift).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e378.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e378.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Widmer et al. multi-task / transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Widmer et al. multi-task/transfer learning for genomic and imaging tasks (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work applying multi-task and transfer-learning frameworks for biological sequence analysis and transferring models between 2D and 3D images to enhance learning in bioimaging and genomics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An empirical analysis of domain adaptation algorithms for genomic sequence analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>multi-task / transfer approaches for genomic sequence analysis and 2D→3D image transfer</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>Uses regularization-based multi-task learning to share information across related prediction tasks (e.g., splice site or binding site prediction across species/conditions) and transfers learned model parameters between image domains (e.g., 2D to 3D) to improve performance in tasks with limited labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / multitask learning and transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>computational biology applications and image analysis (genomics sequence analysis; 2D image analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>related biological tasks such as 3D image analysis and other genomic prediction tasks with limited labeled examples</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new biological imaging context (parameter transfer between image dimensionalities) and applied within genomics</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Not detailed in this paper (cited as related work); generally involves regularization schemes to tie parameters across tasks and specialized preprocessing for imaging domains to allow parameter transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>reported in cited literature to be beneficial for splice-site/binding-site prediction and for transferring 2D→3D image parameters, but specific metrics are in the original cited works rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Task-specific modeling challenges and domain mismatch between imaging dimensionalities or genomic conditions; need for appropriate regularization and task similarity measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of related labeled tasks, appropriate regularization frameworks, and shared representational structure across tasks/domains.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Task/domain similarity, appropriate model families (e.g., regularized linear models, SVMs), and access to labeled source tasks and some labeled target data.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Applicable across genomic prediction tasks and imaging tasks where tasks/domains share structure; cited as prior successful examples in computational biology.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and theoretical principles (multitask regularization frameworks and parameter-sharing strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Frustratingly Easy Domain Adaptation <em>(Rating: 2)</em></li>
                <li>An empirical analysis of domain adaptation algorithms for genomic sequence analysis <em>(Rating: 2)</em></li>
                <li>Regularization-based multitask learning with applications to genome biology and biological imaging <em>(Rating: 2)</em></li>
                <li>Frustratingly Easy Semi-supervised Domain Adaptation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-378",
    "paper_id": "paper-198251784",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "EasyAdapt",
            "name_full": "Frustratingly Easy Domain Adaptation (EasyAdapt)",
            "brief_description": "A simple feature-transformation domain adaptation technique that augments a shared feature space with domain-specific copies so a single supervised learner can use both general and domain-specific signals.",
            "citation_title": "Frustratingly Easy Domain Adaptation",
            "mention_or_use": "use",
            "procedure_name": "EasyAdapt domain adaptation",
            "procedure_description": "Creates an augmented feature representation by concatenating a shared (general) feature block with domain-specific feature blocks (one per source domain plus one for the target). Each input x from a domain d is mapped to a vector that contains the original features in the shared block and a copy only in the block corresponding to d (zeros elsewhere). Any supervised classifier can then be trained on the expanded feature space so that it learns both cross-domain structure and domain-specific adjustments.",
            "procedure_type": "computational method / data analysis technique (domain adaptation / feature augmentation)",
            "source_domain": "computer science / natural language processing / general machine learning (text & vision domain adaptation literature)",
            "target_domain": "biological data analysis (simulated biological data; time-lapse microscopy single-cell imaging; cell-cycle stage classification)",
            "transfer_type": "adapted/modified for new context (directly applied with domain-specific preprocessing and classifier choices)",
            "modifications_made": "Applied the EasyAdapt transformation to imaging and simulated biological feature vectors; preprocessed by centering and scaling to unit variance, applied PCA dimensionality reduction (either to explain 98% variance or keep top 16 PCs), and used off-the-shelf supervised classifiers (linear SVM, RBF-SVM, random forest) on the augmented feature vectors. Feature blocks were created per experimental replicate (domain). No changes to the core EasyAdapt mapping were necessary, but practical adaptations included feature scaling, PCA, and selection of classifier hyperparameters via grid search with cross-validation.",
            "transfer_success": "successful - improved target-domain classification in both simulation and real imaging data. In simulation, EasyAdapt achieved AUC=0.91 (best among compared techniques). In the imaging data across 18 experimental settings EasyAdapt ranked best or tied for best in 15 settings and second in 3 settings, showing consistent superiority especially at small-to-medium target training sizes; it also avoided negative transfer observed for some other pooling methods when magnification differed.",
            "barriers_encountered": "Increased feature-space dimensionality (grows linearly with number of domains and feature dimensionality) which can be problematic for very large feature sets or many domains; requires identification of discrete domains and a shared feature subspace; potential overfitting if domain-specific blocks are high-dimensional relative to available target training data.",
            "facilitating_factors": "Availability of a shared feature representation across domains (same measured features), moderate number of domains and feature dimensionality, substantial labeled source data, some similarity (relatedness) between source and target distributions, and standard preprocessing (scaling, PCA) and classifier tuning.",
            "contextual_requirements": "Need labeled source domains and at least small labeled target set; domains must be identifiable (e.g., experimental replicates); shared feature space required; computational resources to handle augmented feature dimensionality; standard ML pipeline (scaling, PCA, cross-validation) used.",
            "generalizability": "High within supervised learning problems that provide a common feature space across domains; the authors state the method is classifier-agnostic and general, but limited when feature dimensionality or number of domains becomes large; applicable beyond imaging to other biological and non-biological problems where domains correspond to replicates, machines, protocols, etc.",
            "knowledge_type": "explicit procedural steps and theoretical principles (feature-augmentation mapping and learning paradigm), with technical/implementational know-how (preprocessing, PCA, classifier selection).",
            "uuid": "e378.0",
            "source_info": {
                "paper_title": "Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Domain (one-hot) encoding",
            "name_full": "Domain membership one-hot encoding (Domain technique)",
            "brief_description": "A practical technique that augments the feature set with binary indicator variables encoding domain membership, allowing a single model to learn domain-specific offsets while sharing feature-based structure.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Domain membership one-hot encoding for pooled learning",
            "procedure_description": "Adds one-hot encoded binary variables indicating the domain (e.g., which experimental replicate) to each feature vector; trains a standard supervised classifier on the pooled data so the model can learn different intercepts/offsets per domain while sharing parameterization of feature effects across domains.",
            "procedure_type": "computational method / data analysis technique (feature augmentation for multi-domain learning)",
            "source_domain": "general machine learning practice (used in multi-domain/multi-center studies and statistical modeling)",
            "target_domain": "biological data analysis (time-lapse microscopy single-cell classification across experimental replicates)",
            "transfer_type": "direct application without major modification (applied as-is to biological imaging data)",
            "modifications_made": "No fundamental change to the method; applied after standard preprocessing (centering, scaling) and PCA for dimensionality reduction before classifier training. Domain membership encoded as additional binary features corresponding to each experiment.",
            "transfer_success": "partially successful - in many settings it improved performance (ranked second best across many settings) but in at least one transfer direction (experiment 2 as target with different magnification) it produced negative transfer and performed worse than training on the small target set alone.",
            "barriers_encountered": "Negative transfer when domains differ strongly (e.g., different microscope magnification) because pooled source data can bias the learned model; relies on domain labels being informative; does not separate shared vs. domain-specific parameterization beyond intercept shifts.",
            "facilitating_factors": "Simple implementation; ability to leverage pooled data if domains are sufficiently similar; availability of explicit domain labels (replicate identity) facilitated this encoding.",
            "contextual_requirements": "Domain membership must be known and encoded; shared features across domains; adequate preprocessing and hyperparameter tuning; caution when domains are heterogeneous (e.g., differing magnification or acquisition protocols).",
            "generalizability": "Moderately general but sensitive to domain heterogeneity; applicable to many pooled-multi-site biological studies but can cause negative transfer when domain differences are large.",
            "knowledge_type": "explicit procedural steps and practical know-how (how to encode domain membership and integrate into ML pipeline).",
            "uuid": "e378.1",
            "source_info": {
                "paper_title": "Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Combined pooling",
            "name_full": "Combined (pooled training) technique",
            "brief_description": "A straightforward approach that pools labeled data from source and target domains and trains a classifier on the merged set without encoding domain membership, weighting every example equally.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Combined pooled-data training",
            "procedure_description": "Merge labeled examples from one or more source domains with labeled examples from the target domain into a single training set, perform standard preprocessing and train a supervised classifier on the pooled data without special domain-specific features or transformations.",
            "procedure_type": "computational method / data analysis technique (data pooling / simple transfer)",
            "source_domain": "general machine learning practice (pooling datasets across studies/domains)",
            "target_domain": "biological data analysis (cell imaging classification across experiments/replicates)",
            "transfer_type": "direct application without modification (naive pooling)",
            "modifications_made": "Applied standard preprocessing (centering, scaling, PCA) and trained SVMs or random forest on pooled features; no explicit domain encoding or domain-specific model components were added.",
            "transfer_success": "partially successful but risky - in many cases pooling improved learning when domains were similar, but caused negative transfer (degraded performance) when source and target distributions differed strongly (e.g., different magnification in experiment 2), performing worse than target-only training.",
            "barriers_encountered": "Sensitivity to batch effects and domain heterogeneity; large source datasets can bias the model away from the limited target examples; lack of mechanism to compensate for distributional shifts.",
            "facilitating_factors": "When domains are similar or differences are small, large source datasets provide useful additional labeled data; simple to implement and requires no extra domain metadata.",
            "contextual_requirements": "Requires that pooled data be commensurate (shared feature space and similar distributions) for successful transfer; careful preprocessing and model selection are necessary to mitigate negative transfer.",
            "generalizability": "General but brittle; applies broadly across domains but may fail when domain shifts are large; the paper recommends caution and alternative domain-adaptation methods when domain differences exist.",
            "knowledge_type": "explicit procedural steps and practical know-how (how to pool and preprocess data for classifier training).",
            "uuid": "e378.2",
            "source_info": {
                "paper_title": "Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Domain adaptation (concept)",
            "name_full": "Domain adaptation (a form of transfer learning)",
            "brief_description": "A family of supervised learning methods that leverage labeled data from one or more source domains to improve learning in a related but different target domain by addressing distributional shifts between domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "procedure_name": "domain adaptation / transfer learning",
            "procedure_description": "General paradigm where models trained on source-domain data are adapted to work on a target domain with limited labeled data; methods include feature-based transformations (e.g., EasyAdapt), instance weighting, parameter sharing, adversarial alignment, and multi-task regularization to reduce distribution mismatch between P_source(X) and P_target(X).",
            "procedure_type": "computational method / learning paradigm",
            "source_domain": "originally extensively developed in natural language processing and computer vision (text classification literature and visual domain adaptation)",
            "target_domain": "biological and biomedical data analysis (e.g., genomic sequence analysis, imaging, cell phenotype classification)",
            "transfer_type": "analogical transfer and methodological import — techniques originally developed in text/vision are applied to biological data analysis with domain-specific preprocessing",
            "modifications_made": "In this paper, general domain adaptation ideas are instantiated using EasyAdapt and other pooling/encoding strategies; in the literature other adaptations include adversarial training, residual adapters for neural nets, and joint adaptation networks, which are referenced but not implemented here.",
            "transfer_success": "generally successful when appropriately chosen: the paper demonstrates that domain-adaptation ideas (in particular EasyAdapt) can substantially reduce the amount of target labeling required and improve classification accuracy; success depends on domain relatedness and method choice.",
            "barriers_encountered": "Biological batch effects, small labeled target sets, changes in acquisition (magnification, objectives), and the need for a shared feature subspace; potential for negative transfer when domains are too different.",
            "facilitating_factors": "Underlying shared biological signal across experiments, explicit domain labels, availability of abundant source annotations, and preprocessing (scaling/PCA) enable effective adoption of domain adaptation methods.",
            "contextual_requirements": "Knowledge of domain identities, shared feature definitions across datasets, at least some labeled target examples for supervised adaptation, and computational infrastructure to train and validate adapted models.",
            "generalizability": "Broadly generalizable across data types and biological applications provided domain relationships and shared features exist; many modern domain-adaptation methods (including those for deep learning) are noted as applicable but orthogonal to the EasyAdapt approach.",
            "knowledge_type": "theoretical principles and explicit procedural frameworks (paradigms for how to share information across domains and mitigate distribution shift).",
            "uuid": "e378.3",
            "source_info": {
                "paper_title": "Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "Widmer et al. multi-task / transfer",
            "name_full": "Widmer et al. multi-task/transfer learning for genomic and imaging tasks (cited work)",
            "brief_description": "Prior work applying multi-task and transfer-learning frameworks for biological sequence analysis and transferring models between 2D and 3D images to enhance learning in bioimaging and genomics tasks.",
            "citation_title": "An empirical analysis of domain adaptation algorithms for genomic sequence analysis",
            "mention_or_use": "mention",
            "procedure_name": "multi-task / transfer approaches for genomic sequence analysis and 2D→3D image transfer",
            "procedure_description": "Uses regularization-based multi-task learning to share information across related prediction tasks (e.g., splice site or binding site prediction across species/conditions) and transfers learned model parameters between image domains (e.g., 2D to 3D) to improve performance in tasks with limited labeled data.",
            "procedure_type": "computational method / multitask learning and transfer learning",
            "source_domain": "computational biology applications and image analysis (genomics sequence analysis; 2D image analysis)",
            "target_domain": "related biological tasks such as 3D image analysis and other genomic prediction tasks with limited labeled examples",
            "transfer_type": "adapted/modified for new biological imaging context (parameter transfer between image dimensionalities) and applied within genomics",
            "modifications_made": "Not detailed in this paper (cited as related work); generally involves regularization schemes to tie parameters across tasks and specialized preprocessing for imaging domains to allow parameter transfer.",
            "transfer_success": "reported in cited literature to be beneficial for splice-site/binding-site prediction and for transferring 2D→3D image parameters, but specific metrics are in the original cited works rather than this paper.",
            "barriers_encountered": "Task-specific modeling challenges and domain mismatch between imaging dimensionalities or genomic conditions; need for appropriate regularization and task similarity measurement.",
            "facilitating_factors": "Availability of related labeled tasks, appropriate regularization frameworks, and shared representational structure across tasks/domains.",
            "contextual_requirements": "Task/domain similarity, appropriate model families (e.g., regularized linear models, SVMs), and access to labeled source tasks and some labeled target data.",
            "generalizability": "Applicable across genomic prediction tasks and imaging tasks where tasks/domains share structure; cited as prior successful examples in computational biology.",
            "knowledge_type": "explicit procedural steps and theoretical principles (multitask regularization frameworks and parameter-sharing strategies).",
            "uuid": "e378.4",
            "source_info": {
                "paper_title": "Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Frustratingly Easy Domain Adaptation",
            "rating": 2,
            "sanitized_title": "frustratingly_easy_domain_adaptation"
        },
        {
            "paper_title": "An empirical analysis of domain adaptation algorithms for genomic sequence analysis",
            "rating": 2,
            "sanitized_title": "an_empirical_analysis_of_domain_adaptation_algorithms_for_genomic_sequence_analysis"
        },
        {
            "paper_title": "Regularization-based multitask learning with applications to genome biology and biological imaging",
            "rating": 2,
            "sanitized_title": "regularizationbased_multitask_learning_with_applications_to_genome_biology_and_biological_imaging"
        },
        {
            "paper_title": "Frustratingly Easy Semi-supervised Domain Adaptation",
            "rating": 1,
            "sanitized_title": "frustratingly_easy_semisupervised_domain_adaptation"
        }
    ],
    "cost": 0.0129785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets</p>
<p>Michael Schneider 
Institute of Computational Biology
Helmholtz Zentrum München
German Research Center for Environmental Health
85764NeuherbergGermany</p>
<p>Cancer Research UK Cambridge Institute
CambridgeUK</p>
<p>Lichao Wang lichaowang@gmail.com 
Institute of Computational Biology
Helmholtz Zentrum München
German Research Center for Environmental Health
85764NeuherbergGermany</p>
<p>Carsten Marr carsten.marr@helmholtz-muenchen.de 
Institute of Computational Biology
Helmholtz Zentrum München
German Research Center for Environmental Health
85764NeuherbergGermany</p>
<p>Evaluation of domain adaptation approaches for robust classification of heterogeneous biological data sets</p>
<p>Most machine learning algorithms require that training data are identically distributed to ensure effective learning. In biological studies, however, even small variations in the experimental setup can lead to substantial deviations. Domain adaptation offers tools to deal with this problem. It is particularly useful for cases where only a small amount of training data is available in the domain of interest, while a large amount of training data is available in a different, but relevant domain.We investigated to what extent domain adaptation was able to improve prediction accuracy for complex biological data. To that end, we used simulated data and time-lapse movies of differentiating blood stem cells in different cell cycle stages from multiple experiments and compared three commonly used domain adaptation approaches. EasyAdapt, a simple technique of structured pooling of related data sets, was able to improve accuracy when classifying the simulated data and cell cycle stages from microscopic images. Meanwhile, the technique proved robust to the potential negative impact on the classification accuracy that is common in other techniques that build models with heterogeneous data. Despite its implementation simplicity, EasyAdapt consistently produced more accurate predictions compared to conventional techniques. Domain adaptation is therefore able to substantially reduce the amount of work required to create a large amount of annotated training data in the domain of interest necessary whenever the domain changes even a little, which is common not only in biological experiments, but universally exists in almost all data collection routines.</p>
<p>Introduction</p>
<p>Over the last decade, machine learning, especially supervised learning, has become increasingly important in biological and medical research. Example applications range from protein structure prediction [1,2] and the identification of new disease subgroups from gene expression data [3,4], to the identification of cell connectivity [5] and the prediction of phenotypes from time-lapse [6] data and high throughput imaging [7]. With improving capabilities of data collection and growing computational resources, machine learning will be playing an even more important role in understanding of underlying biological processes.</p>
<p>One of the most well-known limitations of supervised learning, however, is the need for a large amount of annotated data. In biological and medical research, this requirement is often difficult to meet, as it necessitates expert knowledge and intensive manual work. With an increase in high-throughput data it becomes more and more unrealistic to annotate all observations. An appealing alternative is to combine already-annotated data from one or multiple sources in order to build a model for a new problem for which there is only little annotated data.</p>
<p>Another limitation of classic supervised learning techniques is the poor performance in dealing with data from multiple sources. A typical problem in biological research are batch effects. Batch effects describe qualitative changes in measurements because of experimental changes that are unrelated to the biological feature under investigation [8]. Typically, differences in the experimental setup, the use of different protocols, reagents or different machine settings can all lead to such effects. Conventional machine learning techniques are less effective in data with batch effects, due to differences in underlying distributions. Even in the case of an experiment being designed to be a replicate, the classifier trained with data from one experiment often tends to have lower predictive accuracy when applied to data from another replicate [9]. While it is possible to build a new model using only data from one experiment, this would mean wasting expert knowledge and involve labor-intensive annotation for each separate experiment. Consequently, it is desirable to have a model that can achieve a high performance with limited additional annotation work.</p>
<p>Domain adaptation describes the case where at least a part of the data used to train a model follows a different distribution from the data on which the model is finally applied [10]. It is closely related to the notion of transfer learning and mutlitask learning [10,11,12]. We follow Pan and Yang [11] and consider transfer learning as the more general term, with domain adaptation being one special form of transfer learning. Domain adaptation can be applied where a large number of annotated data are available in one or more domains that are not of direct interest (the source domain), while only a limited amount of annotated data is available in the domain of interest (the target domain) (Fig. 1). The idea of domain adaptation is to transfer the knowledge from the source to improve the learning in the target domain. Technically, it can be understood that the pre-trained decision boundary only requires some 'minor' tuning from a smaller amout of data to be applied to the new domain. Domain adaptation techniques have originally been developed to address text classification problems [13,14,15].</p>
<p>Domains in this context correspond to different types, styles or topics, e.g., a model trained with news articles can be adapted to classify a corpus containing fiction texts [14]. However, the concept is very broad and can be applied to any variable that is likely to lead to differences in the data distribution, e.g. different machines, protocols or reagents. Here, we consider domains representing different replicates of a biological experiment, where each replicate can be seen as a different domain. Fig. 1. Illustration of a domain adaptation classifier in the target domain that leverages knowledge from a related, but different problem in the source domain. A direct application of the source domain (left) classifier (solid line) would lead to a poor classification in the target domain (right). On the other hand, using only data available in the target domain to train a target domain classifier (dotted line) would also lead to poor performance, as the available data is not sufficient to fully learn the decision boundary. Transferring the knowledge from the source to the target domain using domain adaptation leads to an enhanced classification performance.</p>
<p>Methodology</p>
<p>Definitions</p>
<p>We define a domain D as a feature space X with the marginal probability distribution P (X) and a label space Y . A function f (·) maps x i to y i , where x i ∈ X and y i ∈ Y . We consider problems with an arbitrary number of source domains D s1 , . . . , D sm (m ≥ 1) and a single target domain D t . For a multi-class classification problem, we convert to a set of binary classification problems in a one-vs-all manner, i.e. by training a single classifier per class, with the observations of that class as the positive examples and all other observations as negative examples. The aim of domain adaptation is to use the knowledge from the source domains and limited labeling information from the target domain to effectively learn the objective predictive function f (·) for the target domain.</p>
<p>Learning techniques</p>
<p>We compare a particular domain adaptation algorithm, the EasyAdapt technique [16], with four more conventional techniques of building classifiers. We refer to these as the 'Source', 'Target', 'Combined' and 'Domain' techniques. In this study, all domains share the same feature space X. In general, the techniques require a common feature subspace across domains. The details of these techniques are outlined below and illustrated in Fig. 2. For all techniques, we assume that the number of observations in the source domains is sufficiently large to estimate a model that will generalize to unseen data from the same distribution. In the Source technique, we only use labeled data from the source domains D s1 , . . . , D sm to train the model. The model trained on the source domains is then evaluated on data from the target domain, giving an indirect measure of proximity between source and target domains. In the Target technique, we only use labeled data from the target domain D t to train the model, without considering the data from the source domains. Given enough training data in the target domain, this model should perform the best. In the Combined technique, we use labeled data from both the source and the target domains without any reference to the domain membership when training the models (where every data point is weighted equally). This is arguably one of the most common approaches in practice [17,18,19], where a typical scenario consists of a relatively large amount of labeled data from the source domains and a limited amount of data from the target domain. In the Domain technique, we slightly adapt the Combined approach. An additional set of binary variables encoding the domain membership, in the form of one-hot-encoding, is added to the existing feature set [20]. It is expected to enable the estimated function to have a different offset for each domain, while making use of all the other predictors from all domains to define the shape of the function in common. The EasyAdapt domain adaptation technique [16,21], uses a simple transformation to create a representation for the general data structure common to source and target domains and a separate representation for each domain. The transformations Φ s1 , Φ s2 , . . . , Φ sm , Φ t : X →X between the features spaces of the different domains have the following form:
Φ s1 (X Ds1 ) = X Ds1 , X Ds1 , 0 Ds2 , . . . , 0 Ds m , 0 Dt Φ s2 (X Ds2 ) = X Ds2 , 0 Ds1 , X Ds2 , . . . , 0 Ds m , 0 Dt . . . Φ t (X Dt ) = X Dt , 0 Ds1 , 0 Ds2 , . . . , 0 Ds m , X Dt 0 D d denotes a matrix
of dimensions corresponding to the dimensions of domain d filled with zeros. EasyAdapt can be applied to an arbitrary number m of source domains D s1 , . . . , D sm and a single target domain D t (see Fig. 2 for a visualization and a comparison with other techniques). Features only available in the target domain could also be incorporated by setting the relevant entries for the other domains to 0. The technique is simple and flexible and can be used with any supervised classifier. However, it is recommended that the num-ber of features per domain is not too large, because the feature space increases to R (m+2)p dimensions with p being the dimension of the shared feature space. 
Source           y s1 x s1 y s2 x s2 y s3 x s3 . . . . . .           Tar ge t y t x t Combine d               y s1 x s1 y s2 x s2 y s3 x s3 . . . . . . y t x t               Domain               y s1 x s1 1 0 0 · · · 0 y s2 x s2 0 1 0 · · · 0 y s3 x s3 0 0 1 · · · 0 . . . . . . . . . . . . . . . . . . 0 y t x t 0 0 0 0 1               Eas yAdapt               y s1 x s1 x s1 0 0 · · · 0 y s2 x s2 0 x s2 0 · · · 0 y s3 x s3 0 0 x s3 · · · 0 . . . . . . . . . . . . . . . . . . 0 y t x t 0 0 0 0 x t              </p>
<p>Results</p>
<p>Simulation study</p>
<p>In order to visualize how the different techniques work and to test their performance, we created a two dimensional artificial data set with one source domain and one target domain (each with 200 data points), where the ground truth is known (see Fig. 3A). The data was created as follows: In the source domain, we simulate the positive class by sampling 200 data points uniformly around a central point with coordinates (1.0, 0.0). The distance from the centre is sampled from a uniform distribution with mean 0.5 and a range between 0.1 and 0.9. The radial angle is uniformly distributed between 0 and 360 degrees. For the negative class, 200 data points are sampled uniformly around the same central point, but the distance from the centre is sampled from a uniform distribution with mean 0.9 and a range between 0.5 and 1.3. Again, the radial angle is uniformly distributed between 0 and 360 degrees. In order to create the data for the target domain, we translate both classes in the source domain by y = y − 0.60, where y is the horizontal coordinate in the source domain while y is the horizontal coordinate in the target domain. 15% of the data in the target domain was used for training. The remainder of data in the target domain was used for performance evaluation. Support Vector Machine (SVM) [22,23] with a radial basis function (RBF) kernel was chosen as the basic classifier for all the five learning techniques described in the previous section. Parameters were selected using a grid search with 5-fold cross-validation. From both the contour lines ( Fig. 3B-F) and the ROC curves (Fig. 3G) it is evident that the EasyAdapt technique captured the distribution of the target domain most accurately (AUC = 0.91), by leveraging information from both the source domain and the limited amount of training data from the target domain in building the classifier. Fig. 3B illustrates that due to the limited amount of training data in the target domain, the Target technique (AUC = 0.86) learned a decision boundary that was much more complicated than the underlying distribution. The Source technique (AUC = 0.55, Fig. 3C) directly applied the decision boundary learned from the source to the target domain, leading to an evident discrepancy with respect to the target domain distribution. The Combined technique (AUC = 0.64, Fig. 3D), shifts towards the target domain when building the model. Due to the comparatively large number of source domain data, however, the model is strongly biased towards the source distribution. The Domain technique (AUC = 0.89, Fig. 3E) learned a model that describes the target domain quite well, especially in regions close to the centre. In regions that were farther away, however, the contour lines were clearly distracted by source domain information. Compared with these four techniques, the EasyAdapt technique (Fig. 3F) learned a model that described the target distribution the best, by successfully integrating the information from the two domains.</p>
<p>Imaging data set</p>
<p>For a realistic evaluation case, we applied the techniques to a biological data set [25] consisting of 2888 cells with 186 cell texture and shape features from time lapse microscopy experiments, where 8 different cell cycle stages have been manually annotated. The data comes from three experiments, with 1468, 726, and 694 cells, respectively. It is important to note that the experiments differ regarding the microscope objectives and the magnification factor (10x for experiments 1 and 3, and 20x for experiment 2) used, and were conducted by different lab technicians [25]. The different techniques were trained and tested in a onevs-all manner on the 8 cell cycle stages (where each stage is treated as a separate class). We always picked two experiments to represent the source domains and the remaining experiment as the target domain. We tested all three possible combinations of two source domains and one target domain. All data from the source domains together with the data from the target train set were centered and scaled to unit variance. Subsequently, we applied a principal component analysis (PCA) to the data, (i) keeping only factors explaining 98% of variance (reducing the number of features to roughly 20-30), and (ii) keeping only the 16 highest loaded principal components. We used 4-fold cross-validation and a grid search to select parameters and subsequently evaluated performance on a test set in the target domain. The procedure was repeated 50 times for different target training set sizes of 100, 120, 150, 200, 250, 300, and 400 samples in order to obtain robust estimates for variable performance, especially when using small training set sizes. Independent of the amount of data available in the target domain, we used a fixed-sized test set with 240 samples for performance evaluation, which was randomly chosen for every iteration and for every new training set. In   order to evaluate and compare performance of techniques, we chose the microaveraged AUC. Using this metric, class imbalances were taken into account by computing cumulative values for true positives, false negatives, true negatives and false positives for every label and then computing the performance measure from the aggregated values [24]. We compared three different base classifiers, namely a linear SVM [23], an RBF kernel SVM [22], and a random forest classifier [26].</p>
<p>We found that the EasyAdapt technique is particularly robust when working with a small set of training samples in the target domain and consistently performed among the top techniques in the regime of small training set sizes (Fig. 4). As expected, with increasing training set size the Target technique catches up and for 400 training samples (the maximum training set size in the study), the performance for this technique was among the best performing techniques. In general performance improved for all techniques with increasing training set size with exception of the Source technique, which was not trained with any of the target domain data. Results from all experiments are summarised in Table 1, showing the performances of the five learning techniques across three different base classifiers, two different feature selection methods and three different target domains (each combination of a base classifier, a feature selection method and a target domain is referred to as a 'setting' below). To assess performance of the different techniques across training set sizes (Fig. 4), we measured the area under the curve for each of the 50 iterations for a given setting. This renders an aggregated performance for each train/test split across the range of training set sizes we used and gives us an estimate of performance for small to medium training set sizes. In contrast to the microaveraged AUC across different training set sizes, this measure takes into account the fact that we tested more smaller training set sizes (in the range of 100-200 samples) and is a more conservative measure than simple averaging in our case. This is achieved by weighting performance according to train set size sampling frequency. Additionally, we normalized performance, so that a perfect classifier would achieve an relative performance of 1, corresponding to an AUC of 1 for all training set sizes in the range from 100 to 400 samples. Fig. 5 shows the distribution of this performance measure for different techniques, classifiers and transfer directions. Across all settings, the EasyAdapt technique consistently showed superior performance over other techniques: Among 18 different settings, EasyAdapt ranked 15 times the best or tied for the best and 3 times as the second best. This not only demonstrates the effectiveness of knowledge transfer of EasyAdapt, but also shows its generality with respect to base classifiers and feature selection methods under different transfer situations. The second best technique was the Domain technique, with 8 times the best or tied for the best and 3 times in the second place. This indicated that in many cases the membership feature used by the Domain technique was also able to leverage some knowledge from related domains. The technique with the lowest performance was the Source technique, which ranked last in every setting. Table 1. Mean micro-averaged AUC for different classification methods, learning techniques, feature sets (see text for explanation), and target domains. The best performing technique in a row is marked in bold. Note that the performance is averaged over the full range of training set sizes and that one value in the table corresponds to an average of the performance across different training set sizes. Thus, while the average performance for the Target technique appears relatively high, it is much lower when the target training size is small. EasyAdapt, on the other hand, consistently outperforms other methods, when the target training data size is small (e.g., 100 -200 instances, see Fig. 4). In practice, it is hard to predict whether pooling of data will actually improve prediction performance or lead to negative transfer, i.e. learning in the target domain might be negatively affected by the use of additional information, if domains are too different [11,27]. An example for such negative transfer is the case of experiment 2 as the target domain. Here, both the Combined and Domain techniques performed considerably worse compared to the Target technique (see Table 1). This can probably be explained by stronger differences in distributions between experiments 1 and 3 on the one hand, and experiment 2 on the other, as experiment 2 used a different magnification. This difference can also be seen from the extremely poor performance of the Source technique for experiment 2 as the target domain. It is worth noting that the negative transfer that affected the Combined and Domain techniques with experiment 2 as target domain appears stable across different training set sizes (Fig. 4). Importantly, we do not observe such negative transfer in the case of the EasyAdapt technique. Performance of EasyAdapt was comparable or even slightly better than the Target technique when looking at experiment 2 as the target domain.</p>
<p>Method number of features</p>
<p>Discussion</p>
<p>In the present study, we investigated whether accounting for experimental variation in biological data using a domain adaptation techniques can help improve prediction performance and reduce the need for labeled data. We show that indeed, given only limited training data, the EasyAdapt domain adaptation technique boosts prediction performance both in a simulation study and a data set of imaged single cells [25] and leads to more robust predictions in the presence of experimental variation.</p>
<p>Recently, there have been a number of approaches that try to improve generalization of deep neural network performance across multiple domains. This is important, as neural networks have been known to generalize relatively poorly [28]. Often, the approach is to learn transferable representations that both identify the factors driving variation within the data and match feature distributions across domains [29,30]. Recent work has used models that are able to adapt to different domain very quickly by using an efficient parametrization of deep neural networks and adapter residual modules [31,32]. There is also interesting work combining generative adversarial networks with domain adaptation [33,34,35]. It is worth noting that the approach described in this work is orthogonal to these models, and can be used with any type of supervised machine learning algorithm, including but not limited to deep neural networks.</p>
<p>Applications of domain adaptation techniques in biological research have so far been mostly restricted to genomic sequence analysis [36,37]. Widmer et al. [38,39] used a more general multi-task learning framework in conjunction with regularization based supervised learning methods, such as SVM and logistic regression for splice-site and binding site prediction and to transfer model parameters learned on 2D images to 3D images in order to enhance learning. In contrast to [39], we do not learn domain specific differences explicitly. In practice, this information is also often hard to quantify. Here, we rather focus on the effect of training set size and the pooling of heterogeneous data without quantitative knowledge about the relationship between domains. We compare performance of the EasyAdapt technique across three different machine learning algorithms. Furthermore, we consider a range of common ways of combining information from different domains, e.g. via explicit encoding of domain membership, a procedure that is often used in practice. We demonstrate that the EasyAdapt technique is relatively robust to negative effects of data pooling.</p>
<p>Our results have implications for dealing with biological batch effects in machine learning tasks and for improving learning in settings with limited training data, if additional source data is available. The EasyAdapt technique allows the reuse of existing data sets as source data and avoids cost-intensive manual labelling of training data. Results confirm the problem that is one major motivation of this work: a model trained using data from one biological experiment is likely to have much inferior performance when applied to a different experiment, despite the experiments sharing similar experimental setups. Importantly, the EasyAdapt technique is general in that it does not change the machine learning method used and can therefore be applied to a wide set of problems. Because the feature space grows linearly in the number of domains, the approach is not applicable in cases with very large feature spaces or a large number of domains.</p>
<p>In general, classification accuracy in the transfer learning setting will be an increasing function of both the number of training samples available and the homogeneity and level of relatedness of the training samples to the test set. Given a limited set of training samples and reasonable relatedness between training and test set, transfer learning can help to improve classification accuracy. However, in the case when the relatedness between training and test set is insufficient to enable transfer, there is potential for negative impact when adding additional data from a different domain (known as negative transfer). EasyAdapt strikes a balance between improving performance in cases when additional information is available and robustness to experimental variations. Compared with classic techniques such as the Domain and Combined techniques, the EasyAdapt technique is less affected by negative transfer and for small to medium training set sizes it can improve learning in the target domain.</p>
<p>The technique is limited by the necessity to identify domains, i.e. it is necessary to have domain knowledge about potential differences in experimental conditions and fundamental differences in feature distributions that define domains. Furthermore, it requires that the domains have a shared feature subspace and are distinct [16]. Both requirements are typically fulfilled in biological data. Further research will be necessary to develop empirical measures of domain relationships that help to identify cases where the use of domain adaptation in machine learning can be particularly helpful.</p>
<p>Fig. 2 .
2Schematic overview over the different learning techniques. We denote the feature matrices with xs 1 to xs m for the m source domains and with xt for the target domain. Label vectors are denoted by ys i and yt, respectively. Single underlined zeros and ones are column vectors, while double underline indicates matrices of dimensions matching the dimensions of xi. The Domain technique is adding an additional feature encoding the domain membership in the form of a one-hot encoding, where the kth domain is encoded via a 1 at position k. The EasyAdapt technique creates both a unified representation of the data across all domains (analogously to the Combined technique) and a separate representation for each domain (diagonal entries).</p>
<p>Fig. 3 .
3Simulated data: with limited training data and sufficient domain similarity, EasyAdapt has the best classification performance on the target domain. (A) Distribution of the two classes in the source (light blue and orange symbols, right) and target domain (blue and red symbols, left). The target domain was divided into a training set and a test set. The target training set consisted of 15% randomly sampled data from the target domain. Classifiers were trained using RBF kernel SVM. (B-F) Classifiers created using Target (B), Source (C), Combined (D), Domain (E) and EasyAdapt (F). Contour lines represent different thresholds of the decision boundary of the corresponding classifier. (G) ROC curves for the different techniques.</p>
<p>Fig. 4 .
4EasyAdapt outperforms other techniques in particular for small training set sizes. Performance for (A) linear SVM, (B) radial basis function (RBF) kernel SVM, and (C) random forest classifiers for learning with experiments 1 and 3 as source domains and experiment 2 as the target domain. Performance is measured as microaveraged AUC (mean±standard deviation, n=50 iterations)[24]. We do not plot the Source technique since it is independent of the training set size.</p>
<p>Fig. 5 .
5Relative performance, measured as area under the curve for each of the 50 iterations that were used to generate the average performance lines in Fig. 4. Each data point shows performance over the range of training set sizes (100-400) for one iteration of the target domain; each box plot comprises data from 50 iterations. Performance is shown for (A) linear SVM, (B) radial basis function (RBF) kernel SVM, and (C) random forest classifiers.</p>
<p>Protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images. V Golkov, M J Skwark, A Golkov, A Dosovitskiy, T Brox, J Meiler, D Cremers, Advances in Neural Information Processing Systems. Golkov, V., Skwark, M.J., Golkov, A., Dosovitskiy, A., Brox, T., Meiler, J., Cre- mers, D.: Protein contact prediction from amino acid co-evolution using convo- lutional networks for graph-valued images. In: Advances in Neural Information Processing Systems. pp. 4222-4230 (2016)</p>
<p>Combining evolutionary information and neural networks to predict protein secondary structure. B Rost, C Sander, 10.1002/prot.340190108Proteins: Structure, Function, and Bioinformatics. 191Rost, B., Sander, C.: Combining evolutionary information and neural networks to predict protein secondary structure. Proteins: Structure, Function, and Bioinfor- matics 19(1), 55-72 (1994). https://doi.org/10.1002/prot.340190108</p>
<p>The human splicing code reveals new insights into the genetic determinants of disease. H Y Xiong, B Alipanahi, L J Lee, H Bretschneider, D Merico, R K C Yuen, Y Hua, S Gueroussov, H S Najafabadi, T R Hughes, Q Morris, Y Barash, A R Krainer, N Jojic, S W Scherer, B J Blencowe, B J Frey, 10.1126/science.1254806Science. 34762181254806Xiong, H.Y., Alipanahi, B., Lee, L.J., Bretschneider, H., Merico, D., Yuen, R.K.C., Hua, Y., Gueroussov, S., Najafabadi, H.S., Hughes, T.R., Morris, Q., Barash, Y., Krainer, A.R., Jojic, N., Scherer, S.W., Blencowe, B.J., Frey, B.J.: The human splicing code reveals new insights into the genetic determinants of disease. Science 347(6218), 1254806 (Jan 2015). https://doi.org/10.1126/science.1254806</p>
<p>Distinct types of diffuse large B-cell lymphoma identified by gene expression profiling. A A Alizadeh, M B Eisen, R E Davis, C Ma, I S Lossos, A Rosenwald, J C Boldrick, H Sabet, T Tran, X Yu, J I Powell, L Yang, G E Marti, T Moore, J Hudson, L Lu, D B Lewis, R Tibshirani, G Sherlock, W C Chan, T C Greiner, D D Weisenburger, J O Armitage, R Warnke, R Levy, W Wilson, M R Grever, J C Byrd, D Botstein, P O Brown, L M Staudt, 10.1038/35000501Nature. 4036769Alizadeh, A.A., Eisen, M.B., Davis, R.E., Ma, C., Lossos, I.S., Rosenwald, A., Boldrick, J.C., Sabet, H., Tran, T., Yu, X., Powell, J.I., Yang, L., Marti, G.E., Moore, T., Hudson, J., Lu, L., Lewis, D.B., Tibshirani, R., Sherlock, G., Chan, W.C., Greiner, T.C., Weisenburger, D.D., Armitage, J.O., Warnke, R., Levy, R., Wilson, W., Grever, M.R., Byrd, J.C., Botstein, D., Brown, P.O., Staudt, L.M.: Distinct types of diffuse large B-cell lymphoma identified by gene expression pro- filing. Nature 403(6769), 503-511 (2000). https://doi.org/10.1038/35000501</p>
<p>Connectomic reconstruction of the inner plexiform layer in the mouse retina. M Helmstaedter, K L Briggman, S C Turaga, V Jain, H S Seung, W Denk, Nature. 5007461Helmstaedter, M., Briggman, K.L., Turaga, S.C., Jain, V., Seung, H.S., Denk, W.: Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature 500(7461), 168-174 (2013)</p>
<p>Prospective identification of hematopoietic lineage choice by deep learning. F Buggenthin, F Buettner, P S Hoppe, M Endele, M Kroiss, M Strasser, M Schwarzfischer, D Loeffler, K D Kokkaliaris, O Hilsenbeck, Nature methods. 144403Buggenthin, F., Buettner, F., Hoppe, P.S., Endele, M., Kroiss, M., Strasser, M., Schwarzfischer, M., Loeffler, D., Kokkaliaris, K.D., Hilsenbeck, O., et al.: Prospec- tive identification of hematopoietic lineage choice by deep learning. Nature methods 14(4), 403 (2017)</p>
<p>Label-free cell cycle analysis for high-throughput imaging flow cytometry. T Blasi, H Hennig, H D Summers, F J Theis, J Cerveira, J O Patterson, D Davies, A Filby, A E Carpenter, P Rees, 10.1038/ncomms10256Nature Communications. 710256Blasi, T., Hennig, H., Summers, H.D., Theis, F.J., Cerveira, J., Patterson, J.O., Davies, D., Filby, A., Carpenter, A.E., Rees, P.: Label-free cell cycle analysis for high-throughput imaging flow cytometry. Nature Communications 7, 10256 (2016). https://doi.org/10.1038/ncomms10256</p>
<p>Tackling the widespread and critical impact of batch effects in high-throughput data. J T Leek, R B Scharpf, H C Bravo, D Simcha, B Langmead, W E Johnson, D Geman, K Baggerly, R A Irizarry, Nature Reviews Genetics. 1110733Leek, J.T., Scharpf, R.B., Bravo, H.C., Simcha, D., Langmead, B., Johnson, W.E., Geman, D., Baggerly, K., Irizarry, R.A.: Tackling the widespread and critical im- pact of batch effects in high-throughput data. Nature Reviews Genetics 11(10), 733 (2010)</p>
<p>Cross-study validation for the assessment of prediction algorithms. C Bernau, M Riester, A L Boulesteix, G Parmigiani, C Huttenhower, L Waldron, L Trippa, Bioinformatics. 3012Bernau, C., Riester, M., Boulesteix, A.L., Parmigiani, G., Huttenhower, C., Wal- dron, L., Trippa, L.: Cross-study validation for the assessment of prediction algo- rithms. Bioinformatics 30(12), i105-i112 (2014)</p>
<p>Learning to learn, from transfer learning to domain adaptation: A unifying perspective. N Patricia, B Caputo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPatricia, N., Caputo, B.: Learning to learn, from transfer learning to domain adap- tation: A unifying perspective. In: Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition. pp. 1442-1449 (2014)</p>
<p>A survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 2210Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22(10), 1345-1359 (2010)</p>
<p>Visual Domain Adaptation: A survey of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, 10.1109/MSP.2014.2347059IEEE Signal Processing Magazine. 323Patel, V.M., Gopalan, R., Li, R., Chellappa, R.: Visual Domain Adaptation: A survey of recent advances. IEEE Signal Processing Magazine 32(3), 53-69 (2015). https://doi.org/10.1109/MSP.2014.2347059</p>
<p>Supervised Grammar Induction Using Training Data with Limited Constituent Information. R Hwa, 10.3115/1034678.1034699Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics. the 37th Annual Meeting of the Association for Computational Linguistics on Computational LinguisticsStroudsburg, PA, USAAssociation for Computational LinguisticsHwa, R.: Supervised Grammar Induction Using Training Data with Limited Constituent Information. In: Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics. pp. 73-79. Association for Computational Linguistics, Stroudsburg, PA, USA (1999). https://doi.org/10.3115/1034678.1034699</p>
<p>Corpus variation and parser performance. D Gildea, Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing. the 2001 Conference on Empirical Methods in Natural Language ProcessingGildea, D.: Corpus variation and parser performance. In: Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing. pp. 167-202 (2001)</p>
<p>Domain adaptation for statistical classifiers. Iii Daume, H Marcu, D , Journal of artificial Intelligence research. 26Daume III, H., Marcu, D.: Domain adaptation for statistical classifiers. Journal of artificial Intelligence research 26, 101-126 (2006)</p>
<p>Iii Daumé, H , Frustratingly Easy Domain Adaptation. ACL p. 256Daumé III, H.: Frustratingly Easy Domain Adaptation. ACL p. 256 (2007)</p>
<p>Blood transcriptome based biomarkers for human circadian phase. E E Laing, C S Möller-Levet, N Poh, N Santhi, S N Archer, D J Dijk, 10.7554/eLife.20214620214Laing, E.E., Möller-Levet, C.S., Poh, N., Santhi, N., Archer, S.N., Dijk, D.J.: Blood transcriptome based biomarkers for human circadian phase. eLife 6, e20214 (2017). https://doi.org/10.7554/eLife.20214</p>
<p>Identification of breast cancer patients based on human signaling network motifs. L Chen, X Qu, M Cao, Y Zhou, W Li, B Liang, W Li, W He, C Feng, X Jia, Y He, 10.1038/srep03368Scientific Reports. 3Chen, L., Qu, X., Cao, M., Zhou, Y., Li, W., Liang, B., Li, W., He, W., Feng, C., Jia, X., He, Y.: Identification of breast cancer patients based on human signaling network motifs. Scientific Reports 3 (2013). https://doi.org/10.1038/srep03368</p>
<p>Prediction of both conserved and nonconserved microRNA targets in animals. X Wang, E Naqa, M , I , 10.1093/bioinformatics/btm595Bioinformatics. 243Wang, X., Naqa, E., M, I.: Prediction of both conserved and noncon- served microRNA targets in animals. Bioinformatics 24(3), 325-332 (2008). https://doi.org/10.1093/bioinformatics/btm595</p>
<p>A practical guide to support vector classification. C W Hsu, C C Chang, C J Lin, Hsu, C.W., Chang, C.C., Lin, C.J., et al.: A practical guide to support vector classification (2003)</p>
<p>Frustratingly Easy Semi-supervised Domain Adaptation. Iii Daumé, H Kumar, A Saha, A , Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. the 2010 Workshop on Domain Adaptation for Natural Language ProcessingAssociation for Computational LinguisticsDaumé, III, H., Kumar, A., Saha, A.: Frustratingly Easy Semi-supervised Domain Adaptation. In: Proceedings of the 2010 Workshop on Domain Adaptation for Nat- ural Language Processing. pp. 53-59. Association for Computational Linguistics (2010)</p>
<p>Support-vector networks. C Cortes, V Vapnik, 10.1007/BF00994018Machine Learning. 203Cortes, C., Vapnik, V.: Support-vector networks. Machine Learning 20(3), 273-297 (1995). https://doi.org/10.1007/BF00994018</p>
<p>A training algorithm for optimal margin classifiers. B E Boser, I M Guyon, V N Vapnik, Proceedings of the fifth annual workshop on Computational learning theory. the fifth annual workshop on Computational learning theoryACMBoser, B.E., Guyon, I.M., Vapnik, V.N.: A training algorithm for optimal margin classifiers. In: Proceedings of the fifth annual workshop on Computational learning theory. pp. 144-152. ACM (1992)</p>
<p>A systematic analysis of performance measures for classification tasks. M Sokolova, G Lapalme, 10.1016/j.ipm.2009.03.002Information Processing &amp; Management. 454Sokolova, M., Lapalme, G.: A systematic analysis of performance measures for classification tasks. Information Processing &amp; Management 45(4), 427-437 (2009). https://doi.org/10.1016/j.ipm.2009.03.002</p>
<p>CellCognition: time-resolved phenotype annotation in high-throughput live cell imaging. M Held, M H A Schmitz, B Fischer, T Walter, B Neumann, M H Olma, M Peter, J Ellenberg, D W Gerlich, 10.1038/nmeth.1486Nature Methods. 79Held, M., Schmitz, M.H.A., Fischer, B., Walter, T., Neumann, B., Olma, M.H., Peter, M., Ellenberg, J., Gerlich, D.W.: CellCognition: time-resolved phenotype annotation in high-throughput live cell imaging. Nature Methods 7(9), 747-754 (2010). https://doi.org/10.1038/nmeth.1486</p>
<p>Random Forests. L Breiman, 10.1023/A:1010933404324Machine Learning. 451Breiman, L.: Random Forests. Machine Learning 45(1), 5-32 (2001). https://doi.org/10.1023/A:1010933404324</p>
<p>To transfer or not to transfer. M T Rosenstein, Z Marx, L P Kaelbling, T G Dietterich, NIPS 2005 workshop on transfer learning. 898Rosenstein, M.T., Marx, Z., Kaelbling, L.P., Dietterich, T.G.: To transfer or not to transfer. In: NIPS 2005 workshop on transfer learning. vol. 898, pp. 1-4 (2005)</p>
<p>J Yosinski, J Clune, Y Bengio, H Lipson, How transferable are features in deep neural networks? In: Advances in Neural Information Processing Systems. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in deep neural networks? In: Advances in Neural Information Processing Systems. pp. 3320-3328 (2014)</p>
<p>Domain-Adversarial Training of Neural Networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Domain Adaptation in Computer Vision Applications. Csurka, G.ChamSpringer International PublishingGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-Adversarial Training of Neural Networks. In: Csurka, G. (ed.) Domain Adaptation in Computer Vision Applications, pp. 189-209. Springer International Publishing, Cham (2017)</p>
<p>M Long, H Zhu, J Wang, M I Jordan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Deep Transfer Learning with Joint Adaptation NetworksLong, M., Zhu, H., Wang, J., Jordan, M.I.: Deep Transfer Learning with Joint Adaptation Networks. In: Proceedings of the 34th International Conference on Machine Learning -Volume 70. pp. 2208-2217 (2017)</p>
<p>Learning multiple visual domains with residual adapters. S A Rebuffi, H Bilen, A Vedaldi, Advances in Neural Information Processing Systems. 30Rebuffi, S.A., Bilen, H., Vedaldi, A.: Learning multiple visual domains with residual adapters. In: Advances in Neural Information Processing Systems 30, pp. 506-516 (2017)</p>
<p>Efficient Parametrization of Multi-Domain Deep Neural Networks. S A Rebuffi, H Bilen, A Vedaldi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRebuffi, S.A., Bilen, H., Vedaldi, A.: Efficient Parametrization of Multi-Domain Deep Neural Networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8119-8127 (2018)</p>
<p>Simultaneous Deep Transfer Across Domains and Tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionTzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous Deep Transfer Across Domains and Tasks. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 4068-4076 (2015)</p>
<p>Adversarial Discriminative Domain Adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial Discriminative Domain Adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7167-7176 (2017)</p>
<p>M Long, Z Cao, J Wang, M I Jordan, Advances in Neural Information Processing Systems. 31Conditional Adversarial Domain AdaptationLong, M., CAO, Z., Wang, J., Jordan, M.I.: Conditional Adversarial Domain Adap- tation. In: Advances in Neural Information Processing Systems 31, pp. 1640-1650 (2018)</p>
<p>Efficient peptide-mhc-i binding prediction for alleles with few known binders. L Jacob, J P Vert, Bioinformatics. 243Jacob, L., Vert, J.P.: Efficient peptide-mhc-i binding prediction for alleles with few known binders. Bioinformatics 24(3), 358-366 (2007)</p>
<p>An empirical analysis of domain adaptation algorithms for genomic sequence analysis. G Schweikert, G Rätsch, C Widmer, B Schölkopf, Advances in Neural Information Processing Systems. Schweikert, G., Rätsch, G., Widmer, C., Schölkopf, B.: An empirical analysis of do- main adaptation algorithms for genomic sequence analysis. In: Advances in Neural Information Processing Systems. pp. 1433-1440 (2009)</p>
<p>Multitask learning in computational biology. C Widmer, G Rätsch, Proceedings of ICML Workshop on Unsupervised and Transfer Learning. ICML Workshop on Unsupervised and Transfer LearningWidmer, C., Rätsch, G.: Multitask learning in computational biology. In: Pro- ceedings of ICML Workshop on Unsupervised and Transfer Learning. pp. 207-216 (2012)</p>
<p>Regularization-based multitask learning with applications to genome biology and biological imaging. C Widmer, M Kloft, X Lou, G Rätsch, KI-Künstliche Intelligenz. 281Widmer, C., Kloft, M., Lou, X., Rätsch, G.: Regularization-based multitask learn- ing with applications to genome biology and biological imaging. KI-Künstliche Intelligenz 28(1), 29-33 (2014)</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-497 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-497</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-497</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-259108581</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.05079v3.pdf" target="_blank">Enhancing Robustness of AI Offensive Code Generators via Data Augmentation</a></p>
                <p><strong>Paper Abstract:</strong> Since manually writing software exploits for offensive security is time-consuming and requires expert knowledge, AI-base code generators are an attractive solution to enhance security analysts' productivity by automatically crafting exploits for security testing. However, the variability in the natural language and technical skills used to describe offensive code poses unique challenges to their robustness and applicability. In this work, we present a method to add perturbations to the code descriptions to create new inputs in natural language (NL) from well-intentioned developers that diverge from the original ones due to the use of new words or because they miss part of them. The goal is to analyze how and to what extent perturbations affect the performance of AI code generators in the context of offensive code. First, we show that perturbed descriptions preserve the semantics of the original, non-perturbed ones. Then, we use the method to assess the robustness of three state-of-the-art code generators against the newly perturbed inputs, showing that the performance of these AI-based solutions is highly affected by perturbations in the NL descriptions. To enhance their robustness, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the NL descriptions in the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e497.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e497.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL variability sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language variability sensitivity in AI code generators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AI code generators show substantial performance degradation when natural-language code descriptions deviate from the style or vocabulary seen in training; small lexical or informational changes in intents can produce incorrect or non-compilable code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NMT-based AI offensive code generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encoder-decoder NMT models (Seq2Seq, CodeBERT+decoder, CodeT5+) fine-tuned to translate developer-written English intents into IA-32 assembly shellcode snippets, including preprocessing (token standardization) and post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer-written NL code intents / comments describing assembly operations</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>IA-32 assembly code snippets (shellcode)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / vocabulary mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions vary in wording and specificity; models trained on a single or narrow description style fail to generalize when different but semantically-equivalent wording is used, producing syntactically or semantically incorrect code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input (NL intent) / training corpus coverage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>performance comparison on original vs. perturbed test sets (syntactic and semantic accuracy drops)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Syntactic Accuracy (NASM compilation), Semantic Accuracy (human expert manual judgement), Robust Accuracy (ROB = fraction correct before and after perturbation), and percent drops in these metrics between baseline and perturbed tests</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: on average syntactic correctness dropped ≈10% (word substitution) and ≈18% (word omission); semantic correctness dropped ≈20% (substitution) and ≈34% (omission). Specific model drops (Table 5): word substitution syntactic drops of 9% (Seq2Seq), 4% (CodeBERT), 17% (CodeT5+); semantic drops of 14%, 20%, 27%. Word omission caused larger semantic collapses (semantic correctness approximately halved for some models).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the full dataset (5,900 pairs); performance degradation occurred whenever test inputs were fully perturbed (100% perturbed test set scenario used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>high variability and ambiguity in NL intents, dataset bias toward specific description styles, and lack of exposure to varied wordings during training</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Data augmentation: inject perturbed NL descriptions (word substitution and omission) into training at varying percentages (25%, 50%, 100%); adopt constrained substitution (counter-fitted embeddings + POS + similarity threshold) to preserve semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective but partial: augmentation improved robustness (syntax and semantics) on perturbed tests—e.g., syntactic boosts up to +17% for some models with substitution and up to +27% with omission; semantic increases up to ~25% in favorable cases. However, augmented models often still underperform the original non-perturbed baseline on semantic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>automated code generation for offensive security (machine translation / ML for code)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e497.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e497.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word omission gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information-loss due to word omission in NL intents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Omitting action-, structure- or name-related tokens from NL descriptions can remove information that models are unable to infer from context, causing large drops in syntactic and semantic correctness of generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NMT-based AI offensive code generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Same experimental setup translating English intents into IA-32 assembly; perturbation applied by removing all words in one category (verbs, structure-related nouns, or name-related tokens) per intent.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer-written NL code intents with omitted tokens</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>IA-32 assembly code snippets (shellcode)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing/incomplete specification (omitted information)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When intents omit crucial tokens (e.g., register names or action verbs), models cannot reliably derive the missing semantics from prior context and often generate incorrect or placeholder tokens (e.g., not var), especially for multi-line code where pronouns or implicit references occur.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model input (NL intent) and contextual understanding module (lack thereof)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>compare model outputs and metrics between original and omission-perturbed test sets; manual inspection of failure cases showing missing operands or wrong registers</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>SYN/SEM/ROB metrics; semantic similarity checks of perturbed NL (sentence-transformer cosine) to filter only semantically-preserving omissions for evaluation; qualitative failure examples</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Severe: omission produced larger drops than substitution; Table 5 reports syntactic correctness falls of 14% (Seq2Seq), 26% (CodeBERT), 15% (CodeT5+); semantic correctness drops up to 32% for CodeT5+ and roughly halved for other models. Only ≤51% of predictions remained correct before and after omission perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Among omission types, action-related omissions preserved semantics in 94.98% of perturbations, structure-related in 85.97%, name-related in 74.63% (measured by cosine similarity >0.80). Omission perturbations were applied across the dataset (100% perturbed test cases in RQ1).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>developers omit information thinking it's implicit or redundant; models lack mechanisms to resolve cross-sentence or program-context references; training corpora often literal and explicit, so models don't learn inference of omitted details</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Data augmentation with omission perturbations in training (25/50/100%); keep omission categories constrained (one category omitted per perturbed intent) and use standardized tokenization to preserve some structure</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Improves robustness (syntactic and semantic gains for many settings). Best syntactic results for omission obtained with 100% omission-perturbed training (e.g., +11% Seq2Seq, +27% CodeBERT, +15% CodeT5+). Semantic gains are moderate (e.g., up to +16% for CodeBERT). Nonetheless, even with augmentation semantic performance on perturbed tests often remains below the non-perturbed baseline (not fully closed).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>automated code generation for offensive security (machine translation / ML for code)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e497.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e497.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word substitution gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic drift from unconstrained word substitution in NL intents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replacing words in NL intents without appropriate semantic/POS constraints can produce substitutions that change meaning or create incoherent intents, leading to incorrect code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NMT-based AI offensive code generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Word-level substitutions performed using counter-fitted embeddings and POS constraints (or without constraints for comparison) to generate perturbed NL intents for testing and augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer-written NL code intents with substituted words</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>IA-32 assembly code snippets (shellcode)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/unsafe substitution leading to semantic change</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Blind synonym replacement can substitute across POS or choose context-irrelevant synonyms (e.g., 'store'→'stock' if POS not constrained), producing NL descriptions that do not preserve the original intent and thus mislead the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data augmentation / perturbation generation stage (perturbed inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>semantic equivalence check using sentence-transformer cosine similarity and manual human survey; also comparing model performance with constrained vs unconstrained substitution</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>cosine similarity of sentence embeddings (threshold 0.80); percentage of perturbed descriptions above threshold; human confidence scores; model SYN/SEM/ROB differences between constrained/unconstrained perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Unconstrained substitution preserves semantics less often (median cosine 0.93 vs 0.97 with constraints); constrained substitutions preserved semantics for 96.20% of perturbations vs 92.90% without constraints. Model performance degraded more when substitutions altered meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>In the experiments random substitution of 10% of eligible words was applied across dataset; constrained approach preserved semantics in ~96% of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>lack of context-aware constraints in naive augmentation; programming-specific vocabulary requires careful handling (register names, labels, function names)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use counter-fitted embeddings to propose candidates, enforce POS matching, cosine-similarity threshold (0.8), and a curated programming-vocabulary whitelist to prevent substitutions of programming-specific tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>High: constraints increased semantic-preservation rates (cosine similarity median +~0.03) and improved downstream robustness gains when constrained substitutions were used for augmentation; augmentation with constrained substitution improved syntactic and semantic accuracies (see reported +% improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>data augmentation for code generation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e497.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e497.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token-overlap reliance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model reliance on literal token overlap between NL and code outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models often copy or strongly rely on tokens present in the NL intents (register names, numeric constants), and when such tokens are omitted or altered they fail because they expect direct mapping rather than reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NMT-based AI offensive code generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models fine-tuned on a dataset where NL descriptions and output assembly share many tokens (register names, values), exposing models to direct token mapping behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer-written NL intents with explicit tokens (register names, labels, constants)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>IA-32 assembly code snippets (shellcode)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation dependence on explicit tokens / inability to infer implicit references</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Because NL intents often contain tokens that appear verbatim in the target code, models learn to rely on copying these tokens; when missing (omitted) or paraphrased, the models may produce placeholders or incorrect operands instead of inferring appropriate values from context.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model translation (generation) stage; training data characteristic</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>analysis of failed outputs (e.g., generation of generic placeholders 'not var'), manual inspection of cases where prior NL context implies a register but the model outputs a placeholder</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative examples (Table 7/Table 9), counts of syntactic/semantic failures correlated with omitted name tokens, percentage of name-omission perturbations that preserve semantics (cosine similarity) vs model failure rates</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Direct cause of many failure cases under omission perturbations; major contributor to semantic accuracy collapse when name-related words are omitted (name-omission semantic-preservation only ~74.63% by cosine but models still failed often).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Token overlap is common in shellcode NL/coding pairs—dataset design includes many shared tokens (shellcode registers, labels).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>training data contains many direct overlaps; models optimize to map tokens rather than to reason or use program context to resolve implicit references</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Include perturbed training examples that remove or swap name tokens (data augmentation), standardize tokens during preprocessing (replace names by var#), and train models to handle implicit references; propose future work to leverage broader program context.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: augmentation that includes name-omission reduces failures but does not fully restore baseline semantic performance; standardization reduces variability but models still struggle with inferring missing information.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>code generation / ML for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e497.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e497.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic-equivalence detection mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between automatic semantic-equivalence checks and human judgement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic sentence-embedding-based cosine similarity (threshold 0.80) is used to filter perturbed NL descriptions that preserve semantics, but there is non-zero disagreement between the automatic threshold and human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Perturbation validation pipeline (sentence-embedding filter + human survey)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline computes sentence-transformer embeddings for original and perturbed NL descriptions and applies a cosine-similarity threshold (0.80) to select only semantically-preserving perturbations for model robustness experiments; validated via a human survey.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>original and perturbed developer-written NL intents</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>N/A (this entity concerns validation of NL perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>automated semantic-check vs human judgement mismatch / threshold sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Using a fixed embedding-similarity threshold can both exclude valid perturbed examples and include some invalid ones; the authors observed that perturbations scoring >0.8 aligned with human confidence (≈85% had human score ≥3), while those ≤0.8 mostly had low human confidence (≈71% scored 1-2).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>perturbation validation / dataset curation stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>human survey of 1,104 perturbed descriptions rated by 31 domain experts comparing model-score-based inclusion to human judgement</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>cosine similarity distributions (violin plots), percent above threshold, human Likert scores distribution, sensitivity analysis using thresholds 0.70, 0.80, 0.90 (reported coverage changes)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Filtering via threshold allowed large-scale automated experiments while retaining high semantic fidelity; but remaining small fraction of non-semantic-preserving perturbations in training/testing can affect model learning and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>With threshold 0.80: substitution w/constraints preserved semantics for 96.20% of perturbations; substitution w/o constraints 92.90%; omission action 94.98%; omission structure 85.97%; omission name 74.63%. Human validation aligned with thresholded selection in majority of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>imperfect correlation between embedding-space similarity and human concept of semantic equivalence, plus ambiguity and domain-specific vocabulary making embedding estimates noisy</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use embedding threshold (0.80) chosen via prior work and sensitivity analysis; perform human survey validation on sampled perturbations; exclude perturbed examples with cosine ≤0.80 from robustness experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>High practical effectiveness: automatic filter aligned with human judgement in a majority of cases and enabled scalable experiments; sensitivity analysis shows tradeoffs at other thresholds (0.70 nearly all preserved, 0.90 stricter but excludes more).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>NLP validation / dataset curation for ML experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e497.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e497.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distribution shift (JSD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training/test distribution divergence measured by Jensen-Shannon Divergence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perturbations change the token distribution of the NL inputs; Jensen-Shannon divergence (JSD) quantifies how perturbed test sets diverge from the original training distribution, which correlates with decreased generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Dataset distribution analysis module</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparative analysis of token distributions across original/perturbed training and test splits using Jensen-Shannon divergence to quantify distributional shifts introduced by perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer-written NL intents (original and perturbed)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>N/A (analysis of NL distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>distribution shift between training and test data</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Perturbed test sets have higher JSD relative to original training distribution (baseline JSD 0.29); perturbed test JSDs range ~0.36–0.40, indicating that perturbations materially change data distributions and hinder model generalization when training lacks similar perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data distribution / training vs. evaluation datasets</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>compute Jensen-Shannon divergence between unigram/token distributions of training and test sets under various perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>JSD numeric values: baseline OriginalTraining vs OriginalTest = 0.29; OriginalTraining vs PerturbedTest: Omitted Action 0.38, Omitted Name 0.36, Omitted Structure 0.36, Word Substitution 0.40; PerturbedTraining(50%) vs PerturbedTest JSDs ≈0.32–0.34</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Higher JSD correlates with reduced performance when training lacks similar perturbations; training on perturbed data reduces JSD and improves robustness, demonstrating distributional mismatch as a major factor in failures.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Quantified per experimental perturbation; all perturbation types increased JSD relative to baseline in the authors' dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>perturbation-induced vocabulary and token-frequency changes that were not represented in original training data</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Include perturbed examples in the training set (data augmentation at 25/50/100%) to reduce distributional gap; keep dataset size constant while increasing variety to isolate effect.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: JSD values between perturbed-training(50%) and perturbed-test decreased compared to OriginalTraining vs PerturbedTest, and models trained with perturbed data showed improved SYN/SEM/ROB on perturbed tests; however, some residual divergence and performance gap remain.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>ML robustness / dataset shift analysis</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e497.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e497.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity of pre-trained models to unseen NL perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained models (CodeBERT, CodeT5+) outperform non-pretrained Seq2Seq in baseline settings but can be more sensitive to unseen perturbations; pretraining confers advantages when fine-tuned on augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fine-tuned pre-trained transformer models for code generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pre-trained encoder/encoder-decoder (CodeBERT + decoder, CodeT5+) fine-tuned on the shellcode dataset and evaluated under perturbed NL inputs, compared to a Seq2Seq LSTM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer-written NL code intents</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>IA-32 assembly code snippets (shellcode)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>model-class sensitivity / mismatch between pretraining distribution and perturbed fine-tuning/test inputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Pre-trained models may be more sensitive to lexical/structural perturbations not seen during pretraining; without augmentation they show larger semantic drops in some perturbation settings than the non-pretrained Seq2Seq baseline. However, when trained with appropriate augmented data, pre-trained models gain larger robustness improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture and pretraining mismatch with task perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>comparative evaluation of SYN/SEM/ROB across Seq2Seq, CodeBERT, and CodeT5+ under baseline and perturbed test conditions, before and after data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>metric differentials (e.g., CodeBERT/CodeT5+ semantic drops vs Seq2Seq), and gains after augmentation: pre-trained models regained or improved advantage when fine-tuned on perturbed data (e.g., performance boost up to 10–11% vs non-pretrained after augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Pre-trained models had mixed sensitivity: they outperformed Seq2Seq in baseline but were sometimes more fragile to unseen perturbations; with augmentation they showed larger absolute gains (e.g., CodeT5+ semantic accuracy improvement up to ~25% under substitution augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across experiments comparing the three model classes in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>pretraining exposure mismatches—pretrained models encode priors that can be brittle when fine-tuned on narrow datasets without exposure to certain variations</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Fine-tune pre-trained models on augmented datasets that include the targeted perturbations (≥50% perturbed training shown effective); leverage perturbation-aware augmentation to align fine-tuning distribution with evaluation scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: pre-trained models benefited most from augmentation; when trained with perturbed data they regained and exceeded performance of non-pretrained baselines on perturbed inputs, but semantic parity with original non-perturbed baseline was not always achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>pretrained transformers for code generation / transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Robustness of AI Offensive Code Generators via Data Augmentation', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the robustness of code generation techniques: An empirical study on github copilot <em>(Rating: 2)</em></li>
                <li>ReCode: Robustness evaluation of code generation models <em>(Rating: 2)</em></li>
                <li>Synthetic and natural noise both break neural machine translation <em>(Rating: 1)</em></li>
                <li>TEAPOT: On evaluation of adversarial perturbations for sequence-to-sequence models <em>(Rating: 1)</em></li>
                <li>TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-497",
    "paper_id": "paper-259108581",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "NL variability sensitivity",
            "name_full": "Natural-language variability sensitivity in AI code generators",
            "brief_description": "AI code generators show substantial performance degradation when natural-language code descriptions deviate from the style or vocabulary seen in training; small lexical or informational changes in intents can produce incorrect or non-compilable code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NMT-based AI offensive code generation pipeline",
            "system_description": "Encoder-decoder NMT models (Seq2Seq, CodeBERT+decoder, CodeT5+) fine-tuned to translate developer-written English intents into IA-32 assembly shellcode snippets, including preprocessing (token standardization) and post-processing.",
            "nl_description_type": "developer-written NL code intents / comments describing assembly operations",
            "code_implementation_type": "IA-32 assembly code snippets (shellcode)",
            "gap_type": "ambiguous description / vocabulary mismatch",
            "gap_description": "Natural-language descriptions vary in wording and specificity; models trained on a single or narrow description style fail to generalize when different but semantically-equivalent wording is used, producing syntactically or semantically incorrect code.",
            "gap_location": "model input (NL intent) / training corpus coverage",
            "detection_method": "performance comparison on original vs. perturbed test sets (syntactic and semantic accuracy drops)",
            "measurement_method": "Syntactic Accuracy (NASM compilation), Semantic Accuracy (human expert manual judgement), Robust Accuracy (ROB = fraction correct before and after perturbation), and percent drops in these metrics between baseline and perturbed tests",
            "impact_on_results": "Substantial: on average syntactic correctness dropped ≈10% (word substitution) and ≈18% (word omission); semantic correctness dropped ≈20% (substitution) and ≈34% (omission). Specific model drops (Table 5): word substitution syntactic drops of 9% (Seq2Seq), 4% (CodeBERT), 17% (CodeT5+); semantic drops of 14%, 20%, 27%. Word omission caused larger semantic collapses (semantic correctness approximately halved for some models).",
            "frequency_or_prevalence": "Observed across the full dataset (5,900 pairs); performance degradation occurred whenever test inputs were fully perturbed (100% perturbed test set scenario used in experiments).",
            "root_cause": "high variability and ambiguity in NL intents, dataset bias toward specific description styles, and lack of exposure to varied wordings during training",
            "mitigation_approach": "Data augmentation: inject perturbed NL descriptions (word substitution and omission) into training at varying percentages (25%, 50%, 100%); adopt constrained substitution (counter-fitted embeddings + POS + similarity threshold) to preserve semantics.",
            "mitigation_effectiveness": "Effective but partial: augmentation improved robustness (syntax and semantics) on perturbed tests—e.g., syntactic boosts up to +17% for some models with substitution and up to +27% with omission; semantic increases up to ~25% in favorable cases. However, augmented models often still underperform the original non-perturbed baseline on semantic accuracy.",
            "domain_or_field": "automated code generation for offensive security (machine translation / ML for code)",
            "reproducibility_impact": true,
            "uuid": "e497.0",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Word omission gap",
            "name_full": "Information-loss due to word omission in NL intents",
            "brief_description": "Omitting action-, structure- or name-related tokens from NL descriptions can remove information that models are unable to infer from context, causing large drops in syntactic and semantic correctness of generated code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NMT-based AI offensive code generation pipeline",
            "system_description": "Same experimental setup translating English intents into IA-32 assembly; perturbation applied by removing all words in one category (verbs, structure-related nouns, or name-related tokens) per intent.",
            "nl_description_type": "developer-written NL code intents with omitted tokens",
            "code_implementation_type": "IA-32 assembly code snippets (shellcode)",
            "gap_type": "missing preprocessing/incomplete specification (omitted information)",
            "gap_description": "When intents omit crucial tokens (e.g., register names or action verbs), models cannot reliably derive the missing semantics from prior context and often generate incorrect or placeholder tokens (e.g., not var), especially for multi-line code where pronouns or implicit references occur.",
            "gap_location": "model input (NL intent) and contextual understanding module (lack thereof)",
            "detection_method": "compare model outputs and metrics between original and omission-perturbed test sets; manual inspection of failure cases showing missing operands or wrong registers",
            "measurement_method": "SYN/SEM/ROB metrics; semantic similarity checks of perturbed NL (sentence-transformer cosine) to filter only semantically-preserving omissions for evaluation; qualitative failure examples",
            "impact_on_results": "Severe: omission produced larger drops than substitution; Table 5 reports syntactic correctness falls of 14% (Seq2Seq), 26% (CodeBERT), 15% (CodeT5+); semantic correctness drops up to 32% for CodeT5+ and roughly halved for other models. Only ≤51% of predictions remained correct before and after omission perturbations.",
            "frequency_or_prevalence": "Among omission types, action-related omissions preserved semantics in 94.98% of perturbations, structure-related in 85.97%, name-related in 74.63% (measured by cosine similarity &gt;0.80). Omission perturbations were applied across the dataset (100% perturbed test cases in RQ1).",
            "root_cause": "developers omit information thinking it's implicit or redundant; models lack mechanisms to resolve cross-sentence or program-context references; training corpora often literal and explicit, so models don't learn inference of omitted details",
            "mitigation_approach": "Data augmentation with omission perturbations in training (25/50/100%); keep omission categories constrained (one category omitted per perturbed intent) and use standardized tokenization to preserve some structure",
            "mitigation_effectiveness": "Improves robustness (syntactic and semantic gains for many settings). Best syntactic results for omission obtained with 100% omission-perturbed training (e.g., +11% Seq2Seq, +27% CodeBERT, +15% CodeT5+). Semantic gains are moderate (e.g., up to +16% for CodeBERT). Nonetheless, even with augmentation semantic performance on perturbed tests often remains below the non-perturbed baseline (not fully closed).",
            "domain_or_field": "automated code generation for offensive security (machine translation / ML for code)",
            "reproducibility_impact": true,
            "uuid": "e497.1",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Word substitution gap",
            "name_full": "Semantic drift from unconstrained word substitution in NL intents",
            "brief_description": "Replacing words in NL intents without appropriate semantic/POS constraints can produce substitutions that change meaning or create incoherent intents, leading to incorrect code generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NMT-based AI offensive code generation pipeline",
            "system_description": "Word-level substitutions performed using counter-fitted embeddings and POS constraints (or without constraints for comparison) to generate perturbed NL intents for testing and augmentation.",
            "nl_description_type": "developer-written NL code intents with substituted words",
            "code_implementation_type": "IA-32 assembly code snippets (shellcode)",
            "gap_type": "incomplete/unsafe substitution leading to semantic change",
            "gap_description": "Blind synonym replacement can substitute across POS or choose context-irrelevant synonyms (e.g., 'store'→'stock' if POS not constrained), producing NL descriptions that do not preserve the original intent and thus mislead the generator.",
            "gap_location": "data augmentation / perturbation generation stage (perturbed inputs)",
            "detection_method": "semantic equivalence check using sentence-transformer cosine similarity and manual human survey; also comparing model performance with constrained vs unconstrained substitution",
            "measurement_method": "cosine similarity of sentence embeddings (threshold 0.80); percentage of perturbed descriptions above threshold; human confidence scores; model SYN/SEM/ROB differences between constrained/unconstrained perturbations",
            "impact_on_results": "Unconstrained substitution preserves semantics less often (median cosine 0.93 vs 0.97 with constraints); constrained substitutions preserved semantics for 96.20% of perturbations vs 92.90% without constraints. Model performance degraded more when substitutions altered meaning.",
            "frequency_or_prevalence": "In the experiments random substitution of 10% of eligible words was applied across dataset; constrained approach preserved semantics in ~96% of cases.",
            "root_cause": "lack of context-aware constraints in naive augmentation; programming-specific vocabulary requires careful handling (register names, labels, function names)",
            "mitigation_approach": "Use counter-fitted embeddings to propose candidates, enforce POS matching, cosine-similarity threshold (0.8), and a curated programming-vocabulary whitelist to prevent substitutions of programming-specific tokens.",
            "mitigation_effectiveness": "High: constraints increased semantic-preservation rates (cosine similarity median +~0.03) and improved downstream robustness gains when constrained substitutions were used for augmentation; augmentation with constrained substitution improved syntactic and semantic accuracies (see reported +% improvements).",
            "domain_or_field": "data augmentation for code generation / NLP",
            "reproducibility_impact": true,
            "uuid": "e497.2",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Token-overlap reliance",
            "name_full": "Model reliance on literal token overlap between NL and code outputs",
            "brief_description": "Models often copy or strongly rely on tokens present in the NL intents (register names, numeric constants), and when such tokens are omitted or altered they fail because they expect direct mapping rather than reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NMT-based AI offensive code generation pipeline",
            "system_description": "Models fine-tuned on a dataset where NL descriptions and output assembly share many tokens (register names, values), exposing models to direct token mapping behavior.",
            "nl_description_type": "developer-written NL intents with explicit tokens (register names, labels, constants)",
            "code_implementation_type": "IA-32 assembly code snippets (shellcode)",
            "gap_type": "implementation dependence on explicit tokens / inability to infer implicit references",
            "gap_description": "Because NL intents often contain tokens that appear verbatim in the target code, models learn to rely on copying these tokens; when missing (omitted) or paraphrased, the models may produce placeholders or incorrect operands instead of inferring appropriate values from context.",
            "gap_location": "model translation (generation) stage; training data characteristic",
            "detection_method": "analysis of failed outputs (e.g., generation of generic placeholders 'not var'), manual inspection of cases where prior NL context implies a register but the model outputs a placeholder",
            "measurement_method": "qualitative examples (Table 7/Table 9), counts of syntactic/semantic failures correlated with omitted name tokens, percentage of name-omission perturbations that preserve semantics (cosine similarity) vs model failure rates",
            "impact_on_results": "Direct cause of many failure cases under omission perturbations; major contributor to semantic accuracy collapse when name-related words are omitted (name-omission semantic-preservation only ~74.63% by cosine but models still failed often).",
            "frequency_or_prevalence": "Token overlap is common in shellcode NL/coding pairs—dataset design includes many shared tokens (shellcode registers, labels).",
            "root_cause": "training data contains many direct overlaps; models optimize to map tokens rather than to reason or use program context to resolve implicit references",
            "mitigation_approach": "Include perturbed training examples that remove or swap name tokens (data augmentation), standardize tokens during preprocessing (replace names by var#), and train models to handle implicit references; propose future work to leverage broader program context.",
            "mitigation_effectiveness": "Partially effective: augmentation that includes name-omission reduces failures but does not fully restore baseline semantic performance; standardization reduces variability but models still struggle with inferring missing information.",
            "domain_or_field": "code generation / ML for code",
            "reproducibility_impact": true,
            "uuid": "e497.3",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Semantic-equivalence detection mismatch",
            "name_full": "Mismatch between automatic semantic-equivalence checks and human judgement",
            "brief_description": "Automatic sentence-embedding-based cosine similarity (threshold 0.80) is used to filter perturbed NL descriptions that preserve semantics, but there is non-zero disagreement between the automatic threshold and human evaluators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Perturbation validation pipeline (sentence-embedding filter + human survey)",
            "system_description": "Pipeline computes sentence-transformer embeddings for original and perturbed NL descriptions and applies a cosine-similarity threshold (0.80) to select only semantically-preserving perturbations for model robustness experiments; validated via a human survey.",
            "nl_description_type": "original and perturbed developer-written NL intents",
            "code_implementation_type": "N/A (this entity concerns validation of NL perturbations)",
            "gap_type": "automated semantic-check vs human judgement mismatch / threshold sensitivity",
            "gap_description": "Using a fixed embedding-similarity threshold can both exclude valid perturbed examples and include some invalid ones; the authors observed that perturbations scoring &gt;0.8 aligned with human confidence (≈85% had human score ≥3), while those ≤0.8 mostly had low human confidence (≈71% scored 1-2).",
            "gap_location": "perturbation validation / dataset curation stage",
            "detection_method": "human survey of 1,104 perturbed descriptions rated by 31 domain experts comparing model-score-based inclusion to human judgement",
            "measurement_method": "cosine similarity distributions (violin plots), percent above threshold, human Likert scores distribution, sensitivity analysis using thresholds 0.70, 0.80, 0.90 (reported coverage changes)",
            "impact_on_results": "Filtering via threshold allowed large-scale automated experiments while retaining high semantic fidelity; but remaining small fraction of non-semantic-preserving perturbations in training/testing can affect model learning and evaluation.",
            "frequency_or_prevalence": "With threshold 0.80: substitution w/constraints preserved semantics for 96.20% of perturbations; substitution w/o constraints 92.90%; omission action 94.98%; omission structure 85.97%; omission name 74.63%. Human validation aligned with thresholded selection in majority of cases.",
            "root_cause": "imperfect correlation between embedding-space similarity and human concept of semantic equivalence, plus ambiguity and domain-specific vocabulary making embedding estimates noisy",
            "mitigation_approach": "Use embedding threshold (0.80) chosen via prior work and sensitivity analysis; perform human survey validation on sampled perturbations; exclude perturbed examples with cosine ≤0.80 from robustness experiments.",
            "mitigation_effectiveness": "High practical effectiveness: automatic filter aligned with human judgement in a majority of cases and enabled scalable experiments; sensitivity analysis shows tradeoffs at other thresholds (0.70 nearly all preserved, 0.90 stricter but excludes more).",
            "domain_or_field": "NLP validation / dataset curation for ML experiments",
            "reproducibility_impact": true,
            "uuid": "e497.4",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Distribution shift (JSD)",
            "name_full": "Training/test distribution divergence measured by Jensen-Shannon Divergence",
            "brief_description": "Perturbations change the token distribution of the NL inputs; Jensen-Shannon divergence (JSD) quantifies how perturbed test sets diverge from the original training distribution, which correlates with decreased generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Dataset distribution analysis module",
            "system_description": "Comparative analysis of token distributions across original/perturbed training and test splits using Jensen-Shannon divergence to quantify distributional shifts introduced by perturbations.",
            "nl_description_type": "developer-written NL intents (original and perturbed)",
            "code_implementation_type": "N/A (analysis of NL distributions)",
            "gap_type": "distribution shift between training and test data",
            "gap_description": "Perturbed test sets have higher JSD relative to original training distribution (baseline JSD 0.29); perturbed test JSDs range ~0.36–0.40, indicating that perturbations materially change data distributions and hinder model generalization when training lacks similar perturbations.",
            "gap_location": "data distribution / training vs. evaluation datasets",
            "detection_method": "compute Jensen-Shannon divergence between unigram/token distributions of training and test sets under various perturbations",
            "measurement_method": "JSD numeric values: baseline OriginalTraining vs OriginalTest = 0.29; OriginalTraining vs PerturbedTest: Omitted Action 0.38, Omitted Name 0.36, Omitted Structure 0.36, Word Substitution 0.40; PerturbedTraining(50%) vs PerturbedTest JSDs ≈0.32–0.34",
            "impact_on_results": "Higher JSD correlates with reduced performance when training lacks similar perturbations; training on perturbed data reduces JSD and improves robustness, demonstrating distributional mismatch as a major factor in failures.",
            "frequency_or_prevalence": "Quantified per experimental perturbation; all perturbation types increased JSD relative to baseline in the authors' dataset.",
            "root_cause": "perturbation-induced vocabulary and token-frequency changes that were not represented in original training data",
            "mitigation_approach": "Include perturbed examples in the training set (data augmentation at 25/50/100%) to reduce distributional gap; keep dataset size constant while increasing variety to isolate effect.",
            "mitigation_effectiveness": "Effective: JSD values between perturbed-training(50%) and perturbed-test decreased compared to OriginalTraining vs PerturbedTest, and models trained with perturbed data showed improved SYN/SEM/ROB on perturbed tests; however, some residual divergence and performance gap remain.",
            "domain_or_field": "ML robustness / dataset shift analysis",
            "reproducibility_impact": true,
            "uuid": "e497.5",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Pretraining sensitivity",
            "name_full": "Sensitivity of pre-trained models to unseen NL perturbations",
            "brief_description": "Pre-trained models (CodeBERT, CodeT5+) outperform non-pretrained Seq2Seq in baseline settings but can be more sensitive to unseen perturbations; pretraining confers advantages when fine-tuned on augmented data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Fine-tuned pre-trained transformer models for code generation",
            "system_description": "Pre-trained encoder/encoder-decoder (CodeBERT + decoder, CodeT5+) fine-tuned on the shellcode dataset and evaluated under perturbed NL inputs, compared to a Seq2Seq LSTM baseline.",
            "nl_description_type": "developer-written NL code intents",
            "code_implementation_type": "IA-32 assembly code snippets (shellcode)",
            "gap_type": "model-class sensitivity / mismatch between pretraining distribution and perturbed fine-tuning/test inputs",
            "gap_description": "Pre-trained models may be more sensitive to lexical/structural perturbations not seen during pretraining; without augmentation they show larger semantic drops in some perturbation settings than the non-pretrained Seq2Seq baseline. However, when trained with appropriate augmented data, pre-trained models gain larger robustness improvements.",
            "gap_location": "model architecture and pretraining mismatch with task perturbations",
            "detection_method": "comparative evaluation of SYN/SEM/ROB across Seq2Seq, CodeBERT, and CodeT5+ under baseline and perturbed test conditions, before and after data augmentation",
            "measurement_method": "metric differentials (e.g., CodeBERT/CodeT5+ semantic drops vs Seq2Seq), and gains after augmentation: pre-trained models regained or improved advantage when fine-tuned on perturbed data (e.g., performance boost up to 10–11% vs non-pretrained after augmentation).",
            "impact_on_results": "Pre-trained models had mixed sensitivity: they outperformed Seq2Seq in baseline but were sometimes more fragile to unseen perturbations; with augmentation they showed larger absolute gains (e.g., CodeT5+ semantic accuracy improvement up to ~25% under substitution augmentation).",
            "frequency_or_prevalence": "Observed consistently across experiments comparing the three model classes in this study.",
            "root_cause": "pretraining exposure mismatches—pretrained models encode priors that can be brittle when fine-tuned on narrow datasets without exposure to certain variations",
            "mitigation_approach": "Fine-tune pre-trained models on augmented datasets that include the targeted perturbations (≥50% perturbed training shown effective); leverage perturbation-aware augmentation to align fine-tuning distribution with evaluation scenarios.",
            "mitigation_effectiveness": "Effective: pre-trained models benefited most from augmentation; when trained with perturbed data they regained and exceeded performance of non-pretrained baselines on perturbed inputs, but semantic parity with original non-perturbed baseline was not always achieved.",
            "domain_or_field": "pretrained transformers for code generation / transfer learning",
            "reproducibility_impact": true,
            "uuid": "e497.6",
            "source_info": {
                "paper_title": "Enhancing Robustness of AI Offensive Code Generators via Data Augmentation",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the robustness of code generation techniques: An empirical study on github copilot",
            "rating": 2,
            "sanitized_title": "on_the_robustness_of_code_generation_techniques_an_empirical_study_on_github_copilot"
        },
        {
            "paper_title": "ReCode: Robustness evaluation of code generation models",
            "rating": 2,
            "sanitized_title": "recode_robustness_evaluation_of_code_generation_models"
        },
        {
            "paper_title": "Synthetic and natural noise both break neural machine translation",
            "rating": 1,
            "sanitized_title": "synthetic_and_natural_noise_both_break_neural_machine_translation"
        },
        {
            "paper_title": "TEAPOT: On evaluation of adversarial perturbations for sequence-to-sequence models",
            "rating": 1,
            "sanitized_title": "teapot_on_evaluation_of_adversarial_perturbations_for_sequencetosequence_models"
        },
        {
            "paper_title": "TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP",
            "rating": 1,
            "sanitized_title": "textattack_a_framework_for_adversarial_attacks_data_augmentation_and_adversarial_training_in_nlp"
        }
    ],
    "cost": 0.02000175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Robustness of AI Offensive Code Generators via Data Augmentation
19 Oct 2024</p>
<p>Cristina Improta cristina.improta@unina.it 
Pietro Liguori pietro.liguori@unina.it 
Roberto Natella roberto.natella@unina.it 
Bojan Cukic bcukic@charlotte.edu 
Domenico Cotroneo cotroneo@unina.it </p>
<p>University of Naples Federico II
Italy</p>
<p>University of Naples Federico II
Italy</p>
<p>University of Naples Federico II
Italy</p>
<p>University of North Carolina at Charlotte
USA</p>
<p>University of Naples Federico II
Italy</p>
<p>Enhancing Robustness of AI Offensive Code Generators via Data Augmentation
19 Oct 2024DEE4BD05071BFA17E99F1B59C62D8E63arXiv:2306.05079v3[cs.LG]Received: date / Accepted: dateML RobustnessData AugmentationAI Offensive Code GeneratorsNatural Language Perturbations
Since manually writing software exploits for offensive security is time-consuming and requires expert knowledge, AI-base code generators are an attractive solution to enhance security analysts' productivity by automatically crafting exploits for security testing.However, the variability in the natural language and technical skills used to describe offensive code poses unique challenges to their robustness and applicability.In this work, we present a method to add perturbations to the code descriptions to create new inputs in natural language (NL) from well-intentioned developers that diverge from the original ones due to the use of new words or because they miss part of them.The goal is to analyze how and to what extent perturbations affect the performance of AI code generators in the context of offensive code.First, we show that perturbed descriptions preserve the semantics of the original, nonperturbed ones.Then, we use the method to assess the robustness of three</p>
<p>Introduction</p>
<p>With the advent of modern AI technologies, we are witnessing a tremendous increase in using AI code generators, i.e., tools (or part of them) that employ powerful Neural Machine Translation (NMT) models, which promise deep changes in software development processes.These solutions automatically translate natural language (NL) descriptions (intents) into programming code (code snippets) [63,42].Popular examples include CodeBERT [15] and CodeT5+ [68], which are representative of the state-of-the-art in numerous software engineering tasks, including code search and program repair [80,18].</p>
<p>The inherent variability of NL often results in code generators producing inaccurate or inefficient code for semantically equivalent descriptions, posing a significant limit to their practical application in real-world development processes.To be used in real-world contexts, thus bridging the gap between NL descriptions and precise code snippets, these tools must be robust to this variability and generate the correct code snippet even when they meet different yet semantically equivalent NL code descriptions [41].</p>
<p>Improving the robustness of AI code generators is crucial for making software development better and more efficient [23,82].This requirement becomes even more relevant when it comes to writing code for offensive security activities (e.g., code for performing effective penetration tests).Offensive security code is particularly challenging because it requires developers to be very accurate and deep understanding of complex technical issues and low-level language aspects, such as the intricacies of memory layout and CPU registers [34,74,75].If something goes wrong in the generation of offensive code, it could lead to serious problems, such as leaving important software procedures and/or information vulnerable to attacks.This underscores the importance of Automatic Exploit Generation (AEG) as an attractive solution to enhance security analysts' productivity by automatically creating working exploits for security testing [8,59,73].</p>
<p>The problem of model robustness becomes even more critical due to the complexity of offensive code descriptions, as they require a strict vocabulary to detail the low-level operations needed to gain access to the system by exploiting memory layout and CPU registers.Indeed, slight variations in technical knowledge, terminology, and specificity among security experts can lead to discrepancies in the NL, altering the whole semantics of the description [69].</p>
<p>However, the evaluation of the robustness of AI-based solutions in the context of offensive code generation still poses unique challenges.The main issue concerns the choice of perturbations, i.e., intentional modifications made to the NL descriptions to test the robustness of AI code generators.Indeed, perturbations commonly used in human languages are often not suitable for code descriptions.It suffices to think that human languages are usually made up of assertions, which are, instead, almost absent from the descriptions of programming languages [64].Moreover, NL descriptions of offensive code can use ad hoc terms that are barely used in human languages (e.g., the names of assembly registers or logical operators) or are used with different meanings (e.g., the word label ).Therefore, they should be treated differently to preserve the meaning of the descriptions.</p>
<p>This paper proposes a novel method to assess and enhance the robustness of AI offensive code generators.Our approach reproduces the natural variability of how different developers describe code by introducing perturbations into the descriptions of offensive code.More precisely, we modify the original code descriptions by introducing new perturbed NL descriptions that diverge from the original ones due to the use of new terms (word substitution) or because they miss part of them (word omission).These perturbations are designed to maintain the original meaning while testing the AI's ability to handle variations in input.Our findings demonstrate that the generated perturbed descriptions successfully preserve the intent of the original code descriptions.We then assess the impact of these variations on the code generators' performance, focusing on the syntactic and semantic correctness of the code generated by the models.</p>
<p>Additionally, we analyze the use of these perturbed descriptions for data augmentation, aiming to diversify training data and thereby enhance the models' robustness.Data augmentation is an effective solution to increase the volume but especially the diversity of the data used to train AI models, in order to improve their performance on unseen data.In our work, we employ the data augmentation strategy with the primary objective of increasing the diversity of training examples without collecting new data.To the best of our knowledge, this is the first study to apply word-level perturbations to NL descriptions of offensive code as a means to test and improve the resilience of AI code generators.We validate our approach through experiments with three stateof-the-art models 1 , one non-pre-trained Seq2Seq model and two pre-trained models, i.e., CodeBERT and CodeT5+.To fine-tune them for the generation of offensive code, we extended the publicly available Shellcode IA32 dataset for automatically generating shellcodes from NL descriptions [33].The results of our analysis provide the following key findings:</p>
<ol>
<li>Vulnerability to NL Perturbations: The three models used in our experiments are vulnerable to new, perturbed code descriptions.Indeed, the performance of the models decreases in terms of syntactic correctness (∼ 10% and ∼ 18% for word substitution and word omission on average, respectively) and semantic correctness (∼ 20% and ∼ 34% on average).2. Robustness Enhancement through Data Augmentation: Augmenting training data with perturbed descriptions significantly boosts model performance, improving both syntax correctness (up to ∼ 17% for word substitution, and ∼ 27% for word omission) and semantic correctness (up to ∼ 25% and ∼ 16%).3. General Performance Improvement: Augmentation not only increases robustness to perturbations but also enhances overall performance on nonperturbed (original) code descriptions, with improvements in semantic correctness of up to ∼ 5% and up to ∼ 4% when we use word substitution and word omission in the training data, respectively.</li>
</ol>
<p>In the following, Section 2 discusses related work; Section 3 presents the proposed method; Section 4 describes our case study; Section 5 analyses the semantics of the perturbed descriptions; Section 6 shows the robustness assessment of the models; Section 7 discusses threats to validity; Section 8 discusses the limitations and future work; Section 9 concludes the paper.</p>
<p>2 Related Work AI Offensive Code Generators.Automatic exploit generation (AEG) research challenge consists of automatically generating working exploits [4].This task requires technical skills and expertise in low-level languages to gain full control of the memory layout and CPU registers and attack low-level mechanisms (e.g., heap metadata and stack return addresses) not otherwise accessible through high-level programming languages.Given their recent advances, AIcode generators have become a new and attractive solution to help developers and security testers in this challenging task [12].Although these solutions have shown high accuracy in the generation of software exploits, their robustness against new inputs has never been studied before.Liguori et al. [34] released a dataset containing NL descriptions and assembly code extracted from software exploits.They performed an empirical analysis showing that NMT models can correctly generate assembly code snippets from NL and that in many cases can generate entire exploits with no errors.The authors extended the analysis to the generation of Python offensive code used to obfuscate software exploits from systems' protection mechanisms [35].Yang et al. [74] proposed a datadriven approach to software exploit generation and summarization as a dual learning problem.The approach exploits the symmetric structure between the two tasks via dual learning and uses a shallow Transformer model to learn them simultaneously.Yang et al. [75] proposed a novel template-augmented exploit code generation approach.The approach uses a rule-based template parser to generate augmented NL descriptions and uses a semantic attention layer to extract and calculate each layer's representational information.Ruan et al. [59] proposed PT4Exploits, an approach for software exploit generation via prompt tuning.They designed a prompt template to build the contextual relationship between English comment and the corresponding code snippet, simulating the pre-training stage of the model to take advantage of the prior knowledge distribution.Xu et al. [73] introduced an artifact-assisted AEG solution that automatically summarizes the exploit patterns from artifacts of known exploits and uses them to guide the generation of new exploits.The authors implemented AutoPwn, an AEG system that automates the generation of heap exploits for Capture-The-Flag pwn competitions.Recent work also explored the role of GPT-based models, including ChatGPT and Auto-GPT, in the offensive security domain.Botacin [8] found that, by using these models, attackers can both create and deobfuscate malware by splitting the implementation of malicious behaviors into smaller building blocks.Pa et al. [52] and [19] proved the feasibility of generating malware and attack tools through the use of reverse psychology and jailbreak prompts, i.e., maliciously crafted prompts able to bypass the ethical and privacy safeguards for abuse prevention of AI code generators like ChatGPT.Gupta et al. [19] also examined the use of AI code generators to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, and malware detection.</p>
<p>Adversarial Inputs.Previous work investigated the use of perturbations at different linguistic levels (i.e., character, word, and sentence levels) to address the security concerns raised by the ML models, focusing on adversarial attacks, i.e., inputs that are specifically designed to mislead the models, rather than well-intentioned perturbations.Character-level perturbations include homograph attacks, where characters are replaced with homoglyphs, i.e., characters that render the same or a visually similar glyph (e.g., the lowercase Latin letter 'l' and the uppercase Latin letter 'I'), which are used to mislead models in question-answer problems [71] or to reproduce keyboard typos [6].Boucher et al. [9] proposed a class of encoding-based attacks using invisible characters, control characters, and homoglyphs, which are imperceptible to human inspection and can strongly harm machine learning systems.Heigold et al. [22] studied the effects of word scrambling and random noise insertion in machine translation, focusing on English and German languages.Their perturbation strategies include character flips and swaps of neighboring characters to imitate typos.At the word level, words in a sentence can be substituted with different random words, similar words in the word embedding space (i.e., real-valued vector representation of the words), or meaning-preserving words [30,45,24].Paraphrases, back translation, and reordering are some of the approaches to produce syntactically and semantically similar sentences to fool models [24].Models' Robustness.Recent work introduced several tools for the generation of adversarial inputs to assess how they impact the performance of the models in the generation of NL, but never targeted the code descriptions.As an example, Cheng et al. [10] proposed Seq2Sick, an optimization-based framework to generate adversarial examples for sequence-to-sequence neural network models.Michel et al. [45] presented TEAPOT, a toolkit to evaluate the effectiveness of adversarial perturbations on NMT models by taking into account the preservation of meaning in the source.TextBugger [30] is a framework to generate utility-preserving adversarial texts against real-world online text classification systems.Adversarial attacks have also been investigated against the code.These works focused on perturbing code snippets to mislead models on several code-code and code-NL tasks across different programming languages.ALERT [76] is a black-box attack that transforms code snippets to make pre-trained models fail predictions in three software engineering tasks.ACCENT [83] is an approach that crafts adversarial code snippets to mislead the models to produce completely irrelevant code comments.DAMP [77] is an approach to generate attacks on models of code by renaming variables and adding dead code.CodeAttack [25] is a black-box attack model that generates imperceptible adversarial code samples by leveraging the code structure.In the field of code generation from NL, Mastropaolo et al. [41] investigated the robustness of a public model against paraphrased descriptions, showing that semantically equivalent code descriptions can result in different outputs.Zhu and Zhang [84] proposed a white-box attack to measure the robustness of a pre-trained model in the generation of code solutions for programming problems, showing that the model is affected by character-level perturbations.Zhuo et al. [85] investigated the robustness of a prompt-based semantic parser in text-to-SQL tasks and proposed adversarial in-context learning, i.e., contextual guidance during a given task or interaction, as a means to improve robustness.Wang et al. [66] proposed ReCode, a robustness evaluation benchmark for code generation models, including CodeGen, InCoder, and GPT-J.They perform different perturbations on docstrings, function names, code syntax, and code format, and evaluate their benchmark using HumanEval and MBPP datasets.Data Augmentation.To enhance the robustness of the models, data augmentation has proven to be an effective solution in numerous NL processing tasks.However, it has never been applied in the specific task of code generation due to the difficulty of manipulating intents that use ad hoc terminology and precise structure to describe code snippets.Wang et al. [67] designed a data augmentation algorithm as an optimization problem, where they seek the policy that maximizes an objective that encourages both smoothness and diversity of the examples.Gao et al. [17] proposed to augment NMT training data by replacing a randomly chosen word in a sentence with asoft word, which is a probabilistic distribution over the vocabulary and is computed based on the contextual information of the sentence.Nguyen et al. [51] trained multiple models on forward translation, i.e., source-to-target, and on backward translation, i.e., target-to-source, and used them to generate a diverse set of synthetic training data from both lingual sides.Wei and Zou [70] presented an effective easy data augmentation strategy to reduce the problem of overfitting in text classification tasks.They improved the performance of models trained on smaller datasets by performing synonym replacement and random insertion, swap and deletion of words on different subsets of training examples.Yu et al. [79] proposed a solution to improve the models' generalization ability in the software engineering domain.They designed program transformation rules that can preserve both the semantics and the syntactic naturalness of code snippets to increase the accuracy of tasks such as method naming and code clone detection.In the context of code comment generation, Zhang et al. [81] employed the Metropolis-Hastings modifier (MHM) algorithm to augment the training data by perturbing source code and renaming identifiers, thereby enhancing model robustness.</p>
<p>Our study centers on the specific challenge of generating exploits from NL descriptions, a focus that distinguishes it from broader code generation research.Indeed, to the best of our knowledge, no prior work tackles the challenge of perturbing NL descriptions of exploit code.Existing work leverages synonym substitution and word deletion to augment textual data [70,24].We drew these ideas and tailored them to the task at hand by ensuring the substitution with context-appropriate synonyms and the omission of significant information (i.e., offensive code-related words).By focusing on the challenging domain of software exploit generation and introducing a data augmentation method, our study offers unique insights and contributions that set it apart from previous work.Differently from closely related work such as that of Mastropaolo et al. [41], our work focuses on the generation of software exploits from natural language descriptions, which is an even more challenging task than the broader scope of automatic code generation.The generation of offensive code, particularly for low-level programming languages like assembly, demands a high degree of technical expertise and the use of specialized vocabulary.This complexity makes evaluating model robustness more difficult, as it requires creating meaningful perturbations that maintain the original description's semantics.Unlike general-purpose code generation, which can rely on simple paraphrases, our task involves preserving the technical terminology used in shellcode generation, such as register names and low-level operations.Furthermore, our work extends beyond assessing the robustness of code-generation techniques.We have designed a novel and effective data augmentation strategy that improves the models' ability to handle varied inputs and enhances their robustness in generating offensive code.</p>
<p>Method</p>
<p>The challenge of evaluating the robustness of AI offensive code generators lies in selecting meaningful perturbations, issue that is further exacerbated by the purpose of the code to describe.Indeed, unlike regular high-level programming languages that focus on logically complex functional code fragments, offensive code often contains a large number of low-level arithmetic and logical operations.It leverages machine-level programming languages, such as assembly, to perform surgically crafted exploitation of the system's internals, including heap metadata and stack return addresses, that are not accessible through high-level programming languages.Fig. 1 Overview of the proposed method.</p>
<p>Word Substitution</p>
<p>For this reason, the NL used to describe the code snippets can not only be variable, depending on the writing style and technical expertise of the developers, but also contain ad hoc terminology that is rarely used when describing general-purpose code, such as register names and low-level operations.Moreover, a sentence can be expressed using different words, ordering words in different ways, lacking significant details, or being too specific.</p>
<p>Therefore, to assess the robustness of AI offensive code generators, we present a method that perturbs the original code descriptions to create new, equivalent NL descriptions, taking into account the offensive nature of the described code.Fig. 1 shows the overview of the proposed method.</p>
<p>To add perturbations to the NL intents of the corpora, we choose types of perturbation reflecting both the variability and ad hoc vocabulary of the NL.</p>
<p>A robust model should be resistant to this variability and be able to predict the same output when dealing with two different but equivalent code descriptions.Therefore, to measure the robustness of the code generators, we define a set of perturbations to the models' inputs.</p>
<p>Although character-level perturbations can be adopted to study the sensitivity of the models to human errors (e.g., typos), in this work we focus on perturbing words but still preserving the original meaning of the NL intents.To identify the perturbation types that best reproduce this variability, we look at the intents and analyze how each word contributes to the meaning of the whole sentence based on its part of speech.</p>
<p>Consider the example in Fig. 2, which shows a long and detailed NL intent used to describe assembly code extracted from a software exploit program for the IA-32 (the 32-bit version of the x86 Intel Architecture).The intent contains nine part-of-speech (POS), i.e., the category of words that have similar grammatical properties, in the English grammar 2 : noun (AX, register, contents, stack), verb (is, save, push), adjective (greater), adverb (then), pronoun NL intent with word omission if CX is greater than 100, save it into the AX register and then push the AX contents on the stack if CX is greater than 100, save it into the AX register and then push the AX contents on the stack  (it), preposition (if, than, into, on), conjunction (and), determiner (the) and number (100).The figure shows that we can express a semantically equivalent intent by using different NL descriptions.Indeed, POS such as verbs and nouns can be, in some cases, substituted with new words, while they can be omitted in others; adjectives can be replaced, but not omitted (i.e., the word greater is fundamental to this description); adverbs and determiners can be omitted without altering the sentence.</p>
<p>NL intent</p>
<p>Legend</p>
<p>Based on these assumptions, we focus on two types of perturbations: word substitution ( § 3.1), and the word omission ( § 3.2).The former can be used to evaluate the performance of code generation when descriptions diverge from the lexicon used in the training corpus due to variability in the cultural and technical background knowledge of developers [69].The latter, instead, allows us to assess the models' performance when developers omit information that would be redundant, such as information implicitly contained in the sentence or words already stated in previous intents (e.g., in a large program where the same operand or variable is accessed by multiple instructions in sequence, it would be redundant to refer to that operand/variable multiple times), or simply when the NL intents do not adequately describe the behavior of the code as they lack details [53].</p>
<p>The new set of perturbed NL intents is used as input to assess the robustness of the models in the generation of the code snippets ( § 3.3).Ideally, the performance of the models against the perturbed data should be comparable to the ones obtained against the original, non-perturbed data, otherwise, the code generator is not robust to the variability of the NL descriptions.</p>
<p>Word Substitution</p>
<p>Substituting words is not a trivial task, especially in a programming context.In fact, blindly replacing words may lead to the loss of the original meaning of the code description if we do not consider the context of code generation.For example, consider the NL intent "clear the contents of the EAX register ", which describes an assembly snippet that zeros out a register.A valid perturbation of the input can reasonably lead to the sentence "empty the contents of the register ", but not to "purify the contents of the register " since the verb purify is out of the programming context.This problem is further exacerbated by the use of words that belong only to the programming context (e.g., EAX) that cannot be substituted.</p>
<p>To address these issues, we impose a set of constraints on the word substitution process.First, we consider the set of the top-k nearest neighbors in the counter-fitted embedding space [49], i.e., a vector space constructed by injecting antonym and synonym constraints into real-valued vector representations to improve the vectors' ability to judge semantic similarity.Counter-fitted word vectors allow us to find better candidates for word substitution than regular word vectors as a result of nearest-neighbor searches [78].Then, to select the most suitable candidate, we compute the cosine similarity, which measures the similarity between two-word vectors by computing the cosine of the angle between them [32,21,43,78].The metric ranges between 0 (total dissimilarity) and 1 (total similarity).Additionally, we use the POS tag to further constrain the transformation, that is, we mark each word with its corresponding part of speech [26] to limit the substitution to words that share the same POS tag (e.g., a verb can be replaced with another verb, but not with a noun).Therefore, we replace the original word with a different one only if they have a cosine similarity value above a specific threshold and the same POS tag.We set the cosine similarity threshold to 0.8 to guarantee a good trade-off between the quality and strength of the generated perturbed example [30] and set to 20 the number of nearest-neighbors [78].By enforcing these constraints, we replace words with semantically and syntactically similar words, hence ensuring that the examples are perceptibly similar [3].</p>
<p>Table 1 shows how the use of constraints contributes to preserving the original meaning in the perturbed intent (the intent is chosen intentionally as simple for the example).In the original sentence "Store the shellcode pointer in the ESI register ", the word store clearly expresses the action of memorizing a pointer in a specific register.However, if we perform the substitution without using the constraints, the verb can be interpreted as a noun and be replaced with a similar noun, i.e., stock, producing an incoherent sentence.When we use constraints, instead, the word is replaced with the verb save, resulting in a semantically equivalent description.</p>
<p>A problem to deal with when we substitute words in the code descriptions is to replace words related to the programming language that cannot be expressed with different terms (e.g., the word class, the names of the variables, etc.).To overcome this issue, we use a vocabulary of programming-languagerelated words that cannot be substituted.We build the vocabulary by comparing the NL intents of the corpus used to train the models and a large comparison corpus not containing any text from the programming field (e.g., contemporary prose), with both corpora written in the same NL (e.g., English).The key idea is to add to the vocabulary all the words that occur more often in the corpus for code generation than in the comparison corpus or that only occur in the code generation corpus, regardless of their occurrence [11,37].</p>
<p>To this end, we first count the number of unique words in both the code generation and comparison corpora, excluding stopwords (i.e., articles, conjunctions, etc.) [61].Then, for both corpora, we compute the ratio of the occurrences of every word to the number of unique words in the corpus.If the ratio in the code generation corpus is at least 50 times higher than the ratio in the comparison corpus, or if the word never appears in the comparison corpus (i.e., ratio = 0), then we add the word to the vocabulary.Indeed, structurerelated words, i.e., words describing the structure of the code such as function, operand, variable, etc., are frequently mentioned in the code generation corpus, and rarely (or never) appear in the comparison corpus.Moreover, name-related words, i.e., words containing the names of labels, functions, variables, words between brackets or containing special characters, etc., even if used just in a few instructions of the code generation corpora, are unlikely to feature in the comparison corpus.</p>
<p>Once we build the vocabulary, we extract all the programming languagerelated words contained in the vocabulary from each NL intent, leaving only the subset of words eligible for substitution.Then, one or more words in this subset are swapped with a new one using counter-fitting vectors and applying constraints on the transformation.Finally, the words previously extracted are reinserted into the NL intents, resulting in the final perturbed intents.</p>
<p>Word Omission</p>
<p>Deleting information from NL intents is useful for analyzing how the behavior of the model and the comprehension of text vary when part of the information is omitted or can be derived from previous sentences.As shown in the example of Fig. 2, nouns, verbs, adverbs, and determiners can be removed under specific circumstances without losing the original meaning of the sentence.We focus on the omission of nouns and verbs since adverbs (e.g., then) and determiners (e.g., the) are noninformative and usually removed during data preprocessing in the code generation task [61].</p>
<p>To explain the process of word omission, consider the simple intent "copy 0x4 into the BL register ".The intent preserves its semantic meaning also if we remove the verb copy.This type of construct, often used to describe code, is known as a noun phrase [64].Other examples of noun phrases in programming are "2 &gt; 3" or "if result not negative", in which the verb is omitted.This NL intent can be further reduced by omitting the noun register, which is implied since BL is, in fact, a register.Furthermore, if the previous intent is preceded by the NL intent "zero out BL", we can again avoid specifying the name of the register, leading to the sentence "copy 0x4 into the register ".However, we cannot omit both nouns from this intent without resulting in a meaningless sentence (i.e."copy 0x4 into").For this reason, we need to constrain the perturbation process based on the role each noun has in the phrase.</p>
<p>Based on the considerations stated above, we identify three main categories of words that can be omitted without losing the original meaning: i) the action-related words, i.e., verbs, which contain information on the actions of the intents (e.g., is, define, push, etc.); ii) the structure-related words, i.e., nouns depending on the structure of the target programming language (e.g., the words function, variable, register, etc.); and iii) name-related words: i.e., words containing names of variables, functions, classes, labels, registers, etc. Table 2 shows the different types of word omission on the intent "Store the shellcode pointer in the ESI register ".In this example, the intent still preserves its meaning even without specifying the omitted words.The verb store and the word register are implicit (the pointer of the shellcode can only be moved to ESI, which is a register), while the name of the register can be derived from the context of the program (ESI is commonly used to store the shellcode).</p>
<p>To omit words from the NL intents, we use POS tagging to identify actionrelated words (i.e., verbs) and use the same vocabulary of programming languagerelated words used to avoid word substitution (described in § 3.1) to identify structure-related and name-related words.For each word omission category, we generate a different version of perturbed intent when possible.For example, if we identify action-related, structure-related and name-related words in the original NL intent, then we generate three perturbed NL intents, one for each category, but we never omit multiple word categories from the same NL intent to avoid altering the meaning of the original code description.</p>
<p>Code Generation Task</p>
<p>We use NMT models to generate code snippets starting from the NL intents perturbed with word substitution and word omission.To perform the genera-tion of offensive code, we follow the best practices in the field by supporting the models with data processing operations.The data processing steps are usually performed both before translation (pre-processing), to train the model and prepare the input data, and after translation (post-processing), to improve the quality and the readability of the code in output.</p>
<p>The preprocessing starts with the stopwords filtering, i.e., we remove a set of custom-compiled words (e.g., the, each, onto) from the intents to include only relevant data for machine translation.Next, we use a tokenizer to break the intents into fragments of text containing space-separated words (i.e., the tokens).To improve the performance of machine translation [31,46,35], we standardize the intents (i.e., we reduce the randomness of the NL descriptions) by using a named entity tagger, which returns a dictionary of standardizable tokens, such as specific values, names of labels, and parameters, extracted through regular expressions.We replace the selected tokens in every intent with "var #", where "#" denotes a number from 0 to |l|, and |l| is the number of tokens to standardize.Finally, the tokens are represented as real-valued vectors using word embedding.Pre-processed data is used to feed the model (model's training).Once the model is trained, we perform the code generation from the NL intents.Therefore, when the model takes as inputs new intents, it generates the related code snippets based on its knowledge (model's prediction).Finally, the code snippets predicted by the models are processed (post-processing) to improve the quality and readability of the code.The dictionary of standardizable tokens is used in the de-standardization process to replace all the "var #" with the corresponding values, names, and parameters.Moreover, code snippets are cleaned using regular expressions to remove any extra characters or spaces.</p>
<p>Case Study</p>
<p>AI Code Generators</p>
<p>To assess the robustness of the models, we consider three architectures, a standard Seq2Seq model with attention mechanisms, and two pre-trained models, CodeBERT and CodeT5+, which are representative of the state-of-the-art [18] and demonstrated efficacy in software engineering-related tasks.For instance, CodeT5+ 220M has been shown to outperform several larger decoder-only models in the text-to-code generation task [68].■ Seq2Seq is a model that maps an input of sequence to an output of sequence.Similar to the encoder-decoder architecture with attention mechanism [5], we use a bi-directional LSTM as the encoder to transform an embedded intent sequence into a vector of hidden states with equal length.We implement the Seq2Seq model using xnmt [50].We use an Adam optimizer [29] with β 1 = 0.9 and β 2 = 0.999, while the learning rate α is set to 0.001.We set all the remaining hyper-parameters in a basic configuration: layer dimension = 512, layers = 1, epochs = 200, beam size = 5.</p>
<p>■ CodeBERT [15] is a large multi-layer bidirectional Transformer architecture [65] pre-trained on millions of lines of code across six different programming languages.Our implementation uses an encoder-decoder framework where the encoder is initialized to the pre-trained CodeBERT weights.The encoder follows the RoBERTa architecture [38], with 12 attention heads, hidden layer dimension of 768, 12 encoder layers, and 514 for the size of position embeddings.We set the learning rate α = 0.00005, batch size = 32, and beam size = 10.We employ CodeBERT with a transformer decoder, composed of 6 stacked layers, to enable text-to-text translation tasks.Although CodeBERT alone does not fit the NMT category, its integration with a decoder during the fine-tuning phase effectively performs functions analogous to those of encoderdecoder models [1].■ CodeT5+ [68] is a new family of Transformer models pre-trained with a diverse set of pretraining tasks including causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data.We utilize the variant with model size 220M , which is trained from scratch following T5's architecture [55].It has an encoder-decoder architecture with 12 decoder layers, each with 12 attention heads and hidden layer dimension of 768, and 512 for the size of position embeddings.We set the learning rate α = 0.00005, batch size = 16, and beam size = 10.</p>
<p>During data pre-processing, we tokenize the NL intents using the nltk word tokenizer [7] and code snippets using the Python tokenize package [54].We use spaCy, an open-source, NL processing library written in Python and Cython [62], to implement the named entity tagger for the standardization of the NL intents.We trained the models for a total of 3 epochs.To mitigate the risk of overfitting, we utilized a validation set to evaluate the model's performance after each epoch.This approach allowed us to monitor the model's generalization ability and ensure it performed well on unseen data.Additionally, we utilized a learning rate scheduler that adjusted the learning rate during training.This adjustment contributed to the stabilization of the training process.The choice of training for 3 epochs was informed by preliminary experiments, which demonstrated that this configuration provided an optimal balance between training time and model performance.</p>
<p>Dataset</p>
<p>To fine-tune the models for the generation of offensive code, we extended the publicly available Shellcode IA32 dataset for automatically generating shellcodes from NL descriptions [33].A shellcode is a list of machine code instructions to be loaded in a vulnerable application at runtime.The traditional way to develop shellcodes is to write them using the assembly language, and by using an assembler to turn them into opcodes (operation codes, i.e., a machine language instruction in binary format, to be decoded and executed by the CPU) [16,44].Common objectives of shellcodes include spawning a system shell, killing or restarting other processes, causing a denial-of-service, etc.</p>
<p>The dataset consists of instructions in assembly language for IA-32 collected from publicly available security exploits [13,60], manually annotated with detailed English descriptions.In total, it contains 3, 200 unique pairs of assembly code snippets/English intents.We further enriched the dataset with additional up-to-date samples of shellcodes collected from publicly available security exploits, reaching 5, 900 unique pairs of assembly code snippets/English intents.To the best of our knowledge, the resulting dataset is the largest collection of offensive code available to date for code generation.Table 3 presents two examples of offensive code snippets drawn from our dataset to illustrate the nature of the software exploits and their NL descriptions.</p>
<p>To take into account the variability of descriptions in NL, multiple authors described independently different samples of the dataset in the English language.Where available, we used as NL descriptions the comments written by developers of the collected programs.The dataset's NL descriptions were independently crafted by three authors, all with a computer science background and expertise in assembly language and cybersecurity, including 2 Ph.D. students and a post-doc researcher.Each author initially worked independently on a subset of the dataset to create their respective descriptions.Then, to decide on the final version of the NL descriptions, all authors collaboratively reviewed all the samples included in the extended version of the dataset.The code descriptions were thoroughly cross-verified to ensure consistency, accuracy, and completeness.Any differences were resolved through consensus, ensuring that the final descriptions accurately represented the intent and functionality of the code.</p>
<p>The dataset also includes 1, 374 intents (∼23% of the dataset) that generate multiple lines of assembly code, separated by the newline character \n.These multi-line snippets contain many different assembly instructions (e.g., whole functions).Table 4 summarizes the statistics of the dataset used in this work, including the unique examples of NL intents and assembly code snippets, the unique number of tokens, and the average number of tokens per snippet and intent.</p>
<p>Implementation of Perturbations</p>
<p>■ Word Substitution.We used TextAttack [48], a Python framework for data augmentation, to substitute words and apply constraints, and Flair POStagging model [2] as POS tagger.The TextAttack framework implements the word swap by embedding transformation replacing words with their candidates in the counter-fitted vector space [49].To build the vocabulary of programming-language-related words, we compared our dataset with a book containing 832 pages describing the English language as it is spoken around the world [27].Unsurprisingly, structure-related words like stack, function, or pointer are among the most frequent words in our dataset but barely (or never) mentioned in the book, while the name-related words, such as the name of the registers (e.g., EAX) and labels (e.g., encoded shellcode) never appear in the book.We randomly substitute the 10% of words in every NL intent, ensuring that at least one word is swapped with a similar one [78].■ Word Omission.We removed action-related words from each intent (e.g., define, add, etc.) using Flair POS-tagging model [2] as POS tagger, while we identify structure-related words (e.g., register, label, etc.) and name-related words (e.g., EAX, start label, etc.) to omit from the NL intents by using the vocabulary of programming-language-related words.After identifying the words to omit, we removed from the intents all the words related to a word omission category (i.e., we removed all the verbs, all the structure-related words, or all the names), generating different perturbed intents, according to the word omission categories, but we did not generate a perturbed intent with multiple word omission categories.The disparity between the percentage of substituted words (10%) and the omission of entire word categories arises from the different nature and goals of these perturbations.Substitution is aimed at introducing variability without altering the overall semantic meaning too drastically, while omission is aimed at removing critical components to test the model's robustness to significant information loss (e.g. when developers omit information in the NL descriptions).Previous work showed that a value of 10% represents the best compromise between performance improvement and maintaining the integrity of the original sentences [70].As reported in their study, higher values tend to hurt performance, likely because replacing too many words in a sentence changes its original identity.Following best practices in the SOTA [70,78], we randomly substituted 10% of words in each NL intent, ensuring that at least one word is swapped with a similar one.Substituting a higher percentage of words risks significantly altering the original intent's meaning, potentially making the perturbed intent unrecognizable or too different from the original.Hence, by substituting 10% of the words, we ensure that the perturbations introduce sufficient variability to test robustness in the subsequent analyses while preserving the overall semantic equivalence of the intent.</p>
<p>In contrast, our approach to word omissions involved removing all words related to specific categories (verbs, structure-related words, or names), which are limited and specific.By removing 100% of the words in these categories (one category per perturbation), we obtain a targeted subset of omitted words that significantly impacts the intent's structure or content, yet this subset remains a limited portion of the total words.In fact, we computed, for each word-omission category, the average percentage of omission with respect to the total length of the NL intents in the dataset used for our experiments.We found that the average percentage of words that can be omitted is approximately 14% for action-related words, 13% for structure-related words, and 14% for name-related words.These values, although higher than 10%, are still in line with the percentage used for word substitutions.In summary, while the ratio of substituted words is slightly lower compared to the ratio of omitted words, our methodology ensures that we are testing different aspects of the model's robustness.</p>
<p>Metrics</p>
<p>To evaluate the performance of the models, we computed the Syntactic Accuracy (SYN) and the Semantic Accuracy (SEM).While the former gives insights into whether the code is correct according to the rules of the target language, the latter indicates whether the output is the exact translation of the NL intent into the target programming language (i.e., assembly).Syntactic/semantic accuracy is computed as the number of syntactically/semantically correct predictions over the total number of predictions (i.e., the total number of code snippets generated by the model).Furthermore, to better estimate the impact of the perturbations on the models, we computed the Robust Accuracy (ROB) [24].Similar to semantic accuracy, this metric evaluates the semantic correctness of the code predicted by the models.However, instead of considering all the examples in the test data, the robust accuracy limits the evaluation to the subset of examples correctly generated by the model without the use of perturbations (i.e., it discards from the evaluation the semantically incorrect predictions w/o perturbations in the NL intents).This means that, to compute ROB, we consider only the subset of predictions correctly generated in the baseline setting (i.e., no perturbations in the test set), and compute the percentage of predictions that are correct even after the perturbations in the test set.Hence, the formula for computation is: ROB = N SEM=1 before and after perturbation N SEM=1 before perturbation × 100</p>
<p>where:</p>
<p>-N SEM=1 before and after perturbation represents the number of samples for which the semantic accuracy (SEM) equals 1 both before and after the perturbation.</p>
<p>-N SEM=1 before perturbation represents the number of samples for which the semantic accuracy measure (SEM) equals 1 before the perturbation.</p>
<p>To evaluate the syntactic correctness of the predictions, we use the NASM assembler for the Intel x86 architecture in order to check whether the code generated by the models is compilable.For the semantic correctness of the predictions, instead, we manually analyze every code snippet generated by the models to inspect if it is the correct translation of the English intent.This analysis cannot be performed automatically (e.g., by comparing the predictions with ground-truth references) since an English intent can be translated into different but equivalent code snippets.Thereby, manual (human) evaluation is a common practice in machine translation [20,36].To reduce the possibility of errors in manual analysis, multiple authors performed this evaluation independently, obtaining a consensus for the semantic correctness of the predictions.</p>
<p>More precisely, the manual evaluation was conducted by a diverse group consisting of 3 human evaluators, all with a computer science background and expertise in assembly language and cybersecurity.The group included individuals with varying degrees of professional experience and educational qualifications.In particular, 2 Ph.D. students with a master's degree and a researcher with a Ph.D. in information technologies.The diversity and expertise of our evaluators ensured the reliability of our human evaluation process.Each model's prediction was independently evaluated by all 3 evaluators to ensure thoroughness, checking all predictions in each round of semantic correctness evaluation.Specifically, the manual assessment of the semantic correctness of a single code snippet took ∼10 seconds on average.We performed a total of 21 experiments for each of 3 models and evaluated them on a test set of 590 samples, resulting in a total of ∼ 100 of human hours.In the few cases of disagreement, a structured discussion was held to reach a consensus, and the group collectively decided on the semantic correctness of each prediction.This consensus-driven approach ensured that all differing viewpoints were considered and resolved.</p>
<p>Semantics Evaluation</p>
<p>A model is robust if the performance obtained on the original, non-perturbed data is similar to the one achieved against the perturbed data, i.e., the perturbations do not imply a significant drop in the model's performance.To properly assess the model's robustness, the key requirement is to guarantee that new, perturbed inputs, although syntactically different, preserve the semantics of the original ones.In fact, if the perturbations alter the semantics of the NL descriptions, then the model is tested with different inputs (e.g., inputs requiring other actions), hence invalidating the analysis.</p>
<p>Therefore, before analyzing the robustness of the models, we assessed whether the perturbations violate this requirement by comparing the semantics of inputs before and after the perturbations.To this aim, we perturbed all the 5, 900 examples in our dataset with word substitution and word omission.For the former, we considered both the substitution with and without constraints to verify if they helped to preserve the original semantics.For the latter, we performed an in-depth analysis that considered independently all the types of omission, i.e., action, structure, and name-related words.This helped us to understand what is the omission that impacted the most on the semantics.</p>
<p>Unfortunately, there is no automatic solution to check the semantic equivalence of the descriptions, i.e., solving the ambiguity in the NL.A common practice is represented by the inspection from different human annotators (e.g., through a survey) that manually analyze whether the perturbed descriptions are actually semantically equivalent to the original ones [41].However, a manual inspection becomes infeasible and too prone to errors due to the massive amount of NL descriptions to review, which, in our case study, depends on the number of examples in the dataset (i.e., 5, 900), and the number of different perturbations (i.e., word substitution with and without constraints, action, structure, and name-related omission).</p>
<p>Therefore, following the best practices of the field [30,24,47], we adopted multi-lingual models to compute sentence embeddings that are then compared with cosine similarity to find sentences with similar semantics.More precisely, these models, named sentence transformers, are fine-tuned to produce similar vector-space representations for semantically similar sentences within one language or across languages [57,58].We adopted the sentence transformers to create sentence embeddings of both the original, non-perturbed NL descriptions and the perturbed ones.Then, we compared the cosine similarity between the two embedded descriptions: only if the similarity is higher than a threshold, then we consider that the perturbation did not alter the semantics of the original description.We implemented these models by using SentenceTransformers, a Python framework for state-of-the-art sentence, text, and image embeddings [56].We set the threshold for similarity equal to 0.80 [30].</p>
<p>Fig. 3 shows, for each perturbation, the violin plots of the cosine similarity across all the examples in the dataset.First, the figure highlights that, regardless of the perturbations, the cosine similarity values are mostly distributed over the threshold of 0.80.For the word substitution, the median value is 0.97 and 0.93 with and without constraints, respectively (the average is 0.94 vs. 0.91).The percentage of perturbed descriptions having a cosine similarity higher than the threshold value is equal to 96.20% when we use constraints and 92.90% without constraints.For the word omission, the median values are 0.92, 0.90, and 0.88 for action-related, structure-related, and name-related words, respectively (the average values are 0.91, 0.90, and 0.86).In this case, the percentage of perturbations not violating the threshold value for the se- mantic similarity is 94.98% for action-related, 85.97% for structure-related and 74.63% for name-related words.</p>
<p>These results provide practical insights.Indeed, the word substitution perturbation impacts less than the word omission on the semantic similarity with the original descriptions.This happens because the removal of a word, different from its substitution, implies that the information contained in the omitted word is missing from the original sentence.A further takeaway of the analysis is that the constraints for word substitution help to preserve the original semantics of the descriptions.In fact, the cosine similarity is on average higher than 3% when compared with the substitution without constraints.Finally, among the word omission categories, the removal of action-related words is the most semantics-preserving, followed by structure-related and name-related words.The latter is the only perturbation that has a median (average) value below 0.90, although it is still higher than the threshold value.This category of words contains particular important information (e.g., the name of the variables, registers, etc.), but which are sometimes omitted either because they are redundant (e.g., repeated in the same NL description or in previous ones), or because programmers are not accurate when describing the code.</p>
<p>We also performed a sensitivity analysis of the threshold for cosine similarity, which revealed that if we use a more tolerant threshold of 0.70, the percentage of perturbed descriptions having a cosine similarity higher than the threshold value is close to 100% for word substitution, and 96% for the three word-omission types, on average.By using a very strict threshold of 0.90, instead, the percentage of descriptions meeting this requirement is 80% for word substitution and 55% for word omission types, on average.Following previous studies in the field [30], we set the threshold to 0.80, which allows us to test the models with perturbed descriptions that are not too similar to the original ones, yet preserve their semantics.</p>
<p>For the robustness analysis (see § 6), we train and test the models with perturbed intents that meet the similarity threshold, i.e., when the cosine similarity between the encoded code description before and after the perturbation is greater than 0.80.</p>
<p>Finally, we performed a manual verification to confirm the perturbed NL intents maintain their semantic equivalence to the original ones.To this aim, we conducted a comprehensive survey involving domain experts to evaluate the semantic equivalence of the perturbed descriptions.Indeed, given the large dataset (5, 900 samples) and the 5 perturbation categories applied, a full manual review was impractical.Therefore, we designed a survey to assess the semantic accuracy of the perturbations.We engaged 31 people with a strong background in software security and shellcode generation.The participants included 10 Ph.D. students, 6 postdoctoral researchers and 3 professors from our research group, 5 engineering MSc students from a software security course, and 7 students from a national Italian cybersecurity training course.Each participant was initially tasked with evaluating 50 perturbed descriptions, randomly extracted from our dataset (random sampling with replacement), including 10 random samples for each of the five categories: word substitution with constraints, word substitution without constraints, action-related word omission, structure-related word omission, and name-related word omission.Overall, we collected 1, 104 evaluated descriptions (not all participants completed the full evaluation of 50 descriptions, based on their availability and confidence).</p>
<p>In the survey, participants were provided with three columns of information: "Code" containing assembly code extracted from real shellcode programs, "Original Description" containing the original (i.e., the non-perturbed) NL description of the code, and "Perturbed Description" containing the original description perturbed with either word substitution or word omission.The participants were asked to rate their confidence that the perturbed description still accurately reflected the code.They used a scale of 1 to 5, where 5 indicated full confidence that the perturbed description was accurate and complete, 4 indicated high confidence with only minor discrepancies, 3 indicated some confidence, 2 indicated low confidence with evident inaccuracies, and 1 indicated no confidence, meaning the perturbed description poorly reflected the code and was notably different from the original description.Fig. 4 shows the results of the survey.The results demonstrated a clear pattern in the human evaluation scores based on the cosine similarity threshold of 0.80.For perturbations with a model score greater than the threshold (Fig. 4a), the majority of the human scores were 3, 4, or 5, indicating a high level of confidence in the semantic accuracy of these perturbations.Specifically, approximately 85% of these perturbations received a human score of 3 or higher.On the other hand, for perturbations with a model score less than or equal to the threshold (Fig. 4b), the human scores were predominantly 1 and 2 (around 71%), reflecting a low level of confidence in their semantic accuracy.These findings validate our approach, showing that human evaluators perceive the perturbations we use to test the models (those with a model score greater than the threshold) as semantically accurate.In contrast, those who do not meet the threshold generally receive low human scores, justifying their exclusion from the robustness analysis.Overall, the results show that the automatic method performs well even though a low percentage of non-semantic preserving perturbations are used for training and testing models.We achieved a very good trade-off, as it allows us to conduct large-scale automated evaluations, which would be impossible with human evaluation alone.This validates our methodology and the appropriateness of the constraints applied in the word substitution and omission approach.</p>
<p>Robustness Evaluation</p>
<p>To perform the experiments, we split the dataset into training (the set of examples used to fit the parameters), validation (the set used to tune the hyper-parameters of the models), and test (the set used for the evaluation of the models) sets using a common random 80%/10%/10% ratio [28,40].</p>
<p>We acknowledge that data splitting may influence the results.However, the huge amount of our experimentation and the subsequent manual analysis to assess the semantics correctness of the code for all perturbation types would make it infeasible to consider the influence of multiple data splits on the results.Therefore, we used random 80%-10%-10% training-dev-test sets splits, following the best practices in the SOTA.Moreover, we believe that the size of the dataset (5, 900 samples) allowed us both to have a great variability in the training set via data augmentation and perform a comprehensive robustness evaluation of the models on a wide set of different inputs, hence reducing the impact of the data splitting on our results.</p>
<p>To validate the proposed method, we conducted the experimental analysis by answering the following research questions (RQs): ▷ RQ1: Are models robust to perturbations in the code descriptions?We first assess the robustness of the models in the generation of offensive assembly code by using word substitution and word omission in the NL intents of the test set.In this RQ, the test set is 100% perturbed, i.e., we add a perturbation in every code description.This analysis aims to comprehensively estimate the model's robustness in a scenario of high linguistic variation (100% perturbed test set) with respect to its performance when tested on the original non-perturbed descriptions, which have a similar writing style to what the model has seen during training.</p>
<p>▷ RQ2: Can data augmentation enhance the robustness of the models against perturbed code descriptions?In this RQ, we assess if training the model with perturbed NL intents improves the performance of the models against perturbed code descriptions.We use a different percentage of perturbed intents in the training set (25%, 50%, and 100%) and compare the results with the case of no perturbed intents in the training set (0%).The test set is again perturbed 100%.▷ RQ3: Can data augmentation increase the performance of models against non-perturbed code descriptions?Finally, we assess whether data augmentation can improve the performance of the models against non-perturbed code descriptions, i.e., the original test set.The choice to evaluate the model's performance on a non-perturbed test set (0%) when trained on partially augmented data (50%) is meant to assess if the introduction of variability in the training data increases the performance of the model's performance on the original, non-perturbed test set.This split provides a good trade-off between robustness on varied data and performance on the original data.Consequently, we selected this perturbation percentage for further experimentation to ensure balanced results.</p>
<p>Robustness Evaluation On Perturbed Code Descriptions</p>
<p>To assess the robustness of the models in the generation of offensive code, we perturbed the test set by using word substitution or by removing words from the NL intents of the corpus.Then we compared the results with the performance of the models on the original, non-perturbed test set.Ideally, a model is robust if the results obtained with and without perturbations are similar.</p>
<p>As a primary goal, we separately evaluated the impact of each perturbation type to understand their individual effects on both model robustness and performance.Applying both perturbations at once may be too aggressive and compromise the semantic equivalence of the original code descriptions, potentially leading to unrealistic inputs that are not representative of real-world scenarios.By assessing each perturbation type separately, we aimed to determine how augmenting the original data with each perturbation (i.e., word substitution and word omission) independently affects the models' robustness.This approach allowed us to pinpoint specific strengths and limitations of the models and our perturbation-based methodology.</p>
<p>Table 5 shows the results of the models in terms of syntactic, semantic, and robust accuracy, with and without perturbations in the test set.Without any perturbation, the performance of all models is ≥ 90% for the syntactic accuracy, while the semantic one is equal to 65%, 69%, and 69% for Seq2Seq, CodeBERT and CodeT5+, respectively.Hence, the choice of models does not impact the ability to produce syntactically correct code.</p>
<p>When we evaluate the models using word substitution in the NL intents of the test set, the syntactic correctness of the models drops by 9%, 4%, and 17% for Seq2Seq, CodeBERT, and CodeT5+, respectively, while the semantic correctness of the predictions decreases by 14% for Seq2Seq, by 20% for Code-BERT, and by 27% for CodeT5+.According to the robust accuracy, only the 66%, 68%, and 58% of the code snippets are correctly predicted before and after perturbations by SeqSeq, CodeBERT, and CodeT5+, respectively.</p>
<p>The performance gets even worse when we omit words from the NL intents of the test set.Indeed, the syntactic correctness notably fell by 14%, 26%, and 15% for Seq2Seq, CodeBERT, and CodeT5+, respectively.Also, the table highlights that semantic correctness decreases by 32% for CodeT5+, and is approximately halved for the other two models when compared to the results obtained without perturbations.On top of that, only ≤ 51% of the models' predictions are correct both with and without perturbations.</p>
<p>This huge decrease in performance can be attributed to the fact that there are instances where input tokens appear directly in the output.This is particularly common in shellcode generation, where intents can be highly detailed since different registers have different, specific functionalities that the models are not able to comprehend if not correctly steered by the code descriptions (e.g., the ESI register is typically used to store the encoded shellcode, ECX is usually used to store the loop counter, etc.).When this crucial information is missing from the intent (as described in § 3.2, the model is generally not capable of deriving it from the surrounding context and fails to predict the correct translation.As an example, consider the sequence of NL intents "Subtract 8 from the current byte in ESI" and "Negate it", which translates to the instructions sub byte [ESI], 8 and not ESI.While a human developer would immediately understand that "it" refers to the previously mentioned ESI register, the models targeted in our experimental evaluation fail to make this connection, instead defaulting to a generic placeholder not var due to the lack of explicit mention of the register's name.This scenario underscores a critical limitation of current AI models in code generation and their struggle with contextual understanding.To address these challenges, future work includes leveraging contextual information to aid AI models in the translation process, even when faced with missing or implicit information.</p>
<p>To further appreciate the effects of both perturbation strategies on the models' robustness, we compared the performance of the models on singleline and multi-line code snippets.As anticipated in § 4.2, the dataset contains intents that generate multiple lines of assembly code, separated by the newline character \n.In particular, the test set is composed of 449 NL code descriptions that generate single-line snippets and 141 that generate multi-line snippets for a total of 590 samples.</p>
<p>Fig. 5 presents the three models' performance in the baseline setting, i.e., with no perturbation during training or testing, in terms of syntactic and semantic accuracy on single-line and multi-line snippets.Models perform better with shorter code, succeeding in generating up to 98% of syntactically correct single-line snippets against 90% of multi-line snippets.This gap in performance is further exacerbated for semantic accuracy, where models succeed on average on ∼71% of single-line samples and only ∼56% of multi-line samples, with CodeT5+ performing better than the other two models.</p>
<p>As depicted in Fig. 6, when trained on the original data and tested on fully perturbed data, models behave similarly against word substitutions (Fig. 6a, Fig. 6c, Fig. 6e on the left) and word omissions (Fig. 6b, Fig. 6d, Fig. 6f on the right), with the former having less negative impact on robustness.When dealing with word substitutions, models tend to generate more syntactically and semantically correct single-line snippets when compared to multi-line snippets, whereas robustness scores are approximately equal or even higher for multi-line snippets for newer pre-trained models like CodeT5+.This suggests that the word substitution perturbations do not affect the models' ability to generate shorter or longer code snippets.Instead, when dealing with word omissions, models' performance on more complex multi-line snippets tends to deteriorate.RQ1: Are models robust to perturbations in the code descriptions?</p>
<p>In our experiments, we found that models are not robust to intents that diverge from those of the corpus.Indeed, the dropping of the performance is too evident even when the intents slightly differ from the original ones (i.e., word substitution).The worst-case scenario occurs when the intent lacks information.In this case, the semantic correctness of the prediction is more than halved.Therefore, the model shows limited usability in practice since it is not able to deal with the variability of NL descriptions.We attribute this limitation to the corpora used to train the models since they are often oriented to specific writing styles, or because descriptions in the corpus are often too literal and cumbersome.</p>
<p>Overall, our results show that pre-trained models like CodeBERT and CodeT5+ consistently outperform non-pre-trained models like Seq2Seq (+4% performance boost).However, pre-trained models were shown to be more sensitive to input variations than non-pre-trained models when tested on unseen perturbed data, leading to a slight decrease in semantic accuracy for CodeBERT and a variable performance for CodeT5+ (i.e., a drop for word substitution and a boost for word omission).This slight performance drop can be attributed to the pre-trained models' sensitivity to input variations that they were not exposed to during their pre-training phase.</p>
<p>Data Augmentation Against Perturbed Code Descriptions</p>
<p>We investigated how the use of perturbed intents in the training phase3 improves the robustness of the models when dealing with perturbations in the test set.Therefore, we randomly perturbed different amounts of the original code descriptions in the training set (25%, 50%, and 100%), similarly to previous work [71,24].The models were then trained from scratch using the different perturbed training sets before comparing the results obtained by the models without the use of data augmentation (DA), i.e., 0% of perturbed intents in the training set (as shown in § 6.1).We remark that, in our experiments, we did not alter the size of the training data, i.e., the augmentation of the data refers to an increasing variability of the training examples obtained with the new, perturbed code descriptions, which replace the non-perturbed ones of the original dataset.</p>
<p>Data augmentation is an effective solution to increase the volume but especially the diversity of the data used to train AI models [14], in order to improve their generalization abilities and performance on unseen data.A twofold goal drove our experimental design choices: to ensure a fair comparison between the baseline and the results obtained after data augmentation and to assess the effectiveness of our data augmentation strategy thoroughly.To this aim, we kept the dataset size unaltered across the experiments to isolate the effect of the data augmentation strategy without the confounding variable of different dataset sizes.The data augmentation strategy was employed with the primary objective of increasing data diversity and enhancing the model's generalization capabilities.Then, by evaluating the model's performance on fully perturbed test sets, we were able to directly measure the impact of different amounts of data augmentation on the model's ability to handle diverse linguistic variations.Furthermore, when the fine-tuning dataset size is relatively large (i.e., ∼5,000 samples), the performance gain given by increasing the size is negligible (i.e., less than ∼ 0.5%) [70].Given the above considerations, we did not further increase the training data size.</p>
<p>It is also important to remark that, since the candidate word for replacement is chosen from a list of top candidates, the same word in different sentences might not always be replaced with the same synonym.The replacement can depend on several factors, including randomness, the list of candidate synonyms, and the selection criteria set in the augmentation process.Without specific configurations to enforce determinism (e.g., choosing the seed), the augmentation process is non-deterministic and involves some level of randomness, especially if multiple suitable synonyms are available.In this case, the same word could be replaced with different synonyms in different NL descriptions.This approach adds variability by introducing different perturbations for the same word.This is also proved by the size of the vocabulary (in terms of unique words) of the training data, which increases with the amount of perturbed data.In fact, we found that the original, non-perturbed training dataset contains 2536 words, while it increases to 2804, 2939 and 3094 with 25%, 50% and 100% of perturbations in the NL descriptions, respectively.</p>
<p>Table 6 shows the results of data augmentation.The table highlights that, for all the perturbations, the use of perturbed code descriptions increases the robustness of the model over all the metrics.For syntactic correctness, both models get a performance boost already with 25% of word substitution in the training set (+5% for Seq2Seq, +4% for CodeBERT, +17% for CodeT5+), while in the case of word omission, we obtain the best results with a training set 100% perturbed (+11% for Seq2Seq, +27% for CodeBERT, +15% for CodeT5+).</p>
<p>Semantic correctness analysis shows that all models gain greater benefits from the use of higher percentages of perturbed inputs (&gt;= 50%).In fact, in the case of the use of word substitution in the training set, Seq2Seq increases the number of semantically correct predictions up to 6%, and the robustness accuracy up to 21%, CodeBERT gets a boost in semantic correctness up to 17% and robust accuracy up to 22%, and CodeT5+ shows an evident improvement up to 25% and 34%.In the case of the omissions in the training set, Seq2Seq improves the semantic and robustness accuracy up to 7% and 16%, respectively, while CodeBERT gets an even more marked boost of the performance, increasing the semantic accuracy up to 16% and robustness accuracy up to 22%.Finally, for CodeT5+, the semantic and robustness accuracy increased by 10% and 13%.</p>
<p>To show the improvement in the code generation obtained with the data augmentation, Table 7 presents a qualitative analysis using cherry-picked examples from our test sets.In particular, the table shows examples of failed (i.e., semantically incorrect) predictions without using the data augmentation that resulted in successful (i.e., semantically correct) predictions after the augmentation of the code descriptions.In the case of word substitution, it is worth noticing that the model failed to predict the correct output even if the perturbed intent slightly differs from the original one.As concerns the omission of words, the table points out that the model's successful prediction is constrained to all the parts of the description, hence the removal of an unnecessary word from the intent (e.g., the word function) implied a failure prediction.The table also highlights that all these failure cases were fixed by the use of data augmentation since the predictions are all syntactically and semantically correct.</p>
<p>Table 7 Illustrative examples of predictions with and without data augmentation (DA) against the perturbed test set.The prediction errors are red.Underlined and @ @ @ @ slashed text refer to word substitution and to word omission in the test set, respectively.After showing that the use of perturbations in the training set is an effective solution to increase the robustness of the models, we compared the performance of the models with data augmentation on the perturbed test set with the performance of the models on the original training/test sets, i.e., without perturbations.We aimed at examining the best data augmentation setting against the baseline (i.e., non-perturbed training and test sets), to understand whether our data augmentation strategy could, at any fixed perturbation rate, achieve the baseline performance which is considered optimal.According to Table 6, perturbing 100% of the training data provides the best or closer to the best scores when models are tested on 100% perturbed data.Fig. 7 shows the results in terms of syntactic and semantic accuracy.We did not include robust accuracy since this metric cannot be evaluated without perturbations in the test set (i.e., for the None bar).Fig. 7a highlights that the syntactic correctness provided by the models with data augmentation on perturbed test sets is comparable to the one obtained on the original, nonperturbed training/test sets.Furthermore, the use of omissions in the training set helped CodeBERT to exceed the performances obtained without perturbations (93.84% vs 93.22%).However, the analysis of semantics (Fig. 7b) led to very different results.Indeed, all models provide performance lower than that obtained without perturbations.In the case of word substitution, Seq2Seq and CodeBERT showed a drop in the performance of 8% and 4%, while CodeT5+ is very close to the results of the original training/test set (2.2% decrease in semantic accuracy).The decrement of semantic accuracy is more pronounced in the case of omissions in the NL intents.Indeed, despite the use of data augmentation, Seq2Seq, CodeBERT, and CodeT5+ show a clear drop of 25%, 21%, and 23%, respectively.</p>
<p>Original intent</p>
<p>A key takeaway of our experimental evaluation is that AI code generation models are extremely brittle to unseen perturbations (i.e., non-perturbed training set, fully perturbed test set), but when trained on the augmented data (i.e., perturbed training set, fully perturbed test set) their performance greatly improves.However, it still does not achieve the same performance as the baseline (non-perturbed training set, non-perturbed test set).Future work includes providing the models with contextual information to help them gather the missing information from the surrounding context (i.e., previous sentences).</p>
<p>To verify if augmented data is sufficient to evaluate the robustness of models, we assessed if perturbed data results in different training/test sets in terms of their distributions.To this aim, we analyzed the data distribution differences between the various training and test sets using Jensen-Shannon divergence (JSD).The JSD ranges from 0 (identical distributions) to 1 (completely different distributions).</p>
<p>Table 8 shows JSD values indicate varying degrees of divergence between the training and test sets across different perturbations.The table shows that the JSD between the original training and the original test set: is 0.29.This value serves as a baseline, indicating moderate divergence between the original training and test sets.The JSD values between the original training and perturbed test sets, instead, are higher than the baseline (Omitted Action Words: 0.38, Omitted Name Words: 0.36, Omitted Structure Words: 0.36, Word Substitution: 0.40).These higher values demonstrate that the perturbations applied to the test set significantly altered the data distribution compared to the original training set.This also suggests that the model might struggle to generalize without being trained on similar data, indicating a robustness issue (as shown in § 6.1).Finally, the table shows that JSD values between perturbed training (50%) and perturbed test (Omitted Action Words: 0.34, Omitted Name Words: 0.32, Omitted Structure Words: 0.33, Word Substitution: 0.33) are still higher than the baseline, but lower than the previous case.These intermediate values indicate that while the perturbed training and test distributions are different, they are not as drastically different as the distributions between the original training and the perturbed test sets.This suggests that training on perturbed data (data augmentation) helps the model become more robust against similar perturbations.The JSD analysis supports our claim that these perturbations impact data distributions, and training on such data improves the model's ability to handle these variations, thus focusing on robustness rather than just generalization.Fig. 8 compares the models' performance on single-line and multi-line snippets after data augmentation.Indeed, when trained on partially augmented data (50%) and tested on fully perturbed data, syntactic accuracy remains relatively stable across different perturbations, whereas semantic accuracy is notably higher in single-line descriptions, particularly for Seq2Seq and Code-BERT, where multi-line descriptions show a significant drop.As for robust accuracy, both pre-trained models show high scores on multi-line snippets for both perturbation types, unlike Seq2Seq, whose ability to generate complex code is more negatively affected.Interestingly, data augmentation proves to be the most effective on largely pre-trained models like CodeT5+, for which the gap of performance on single-line and multi-line snippets is less evident than Seq2Seq and CodeBERT.RQ2: Can data augmentation enhance the robustness of the models against perturbed code descriptions?Data augmentation proved to be an effective solution to improve the robustness of models in the generation of offensive code.Indeed, our experiments showed that when the model is trained with a high percentage of perturbed intents, it can deal with the variability of the descriptions.For all the perturbations, the syntactic correctness of the predictions with the data augmentation is comparable to the one obtained by the model on the original, non-perturbed test set.However, the semantic correctness of the predictions on perturbed code descriptions is still lower than the one obtained on the original, non-perturbed data.This drop is more evident when the models deal with the omission of information.This suggests the need for more sophisticated solutions to derive the missing or implicit information from the context of the program [39,72].Moreover, our results provide valuable insights about the impact of pretraining on the robustness of AI-driven solutions for offensive code generation.Indeed, when pre-trained models are fine-tuned and then tested on perturbed data, they regain their advantage and outperform non-pretrained models, with a performance boost up to 10% and 11% for Code-BERT and CodeT5+, respectively.The exposure to diverse and varied inputs during the training phase helps pre-trained models leverage their extensive pre-training knowledge, allowing them to adapt to and handle the perturbations effectively.</p>
<p>Data Augmentation Against Non-perturbed Code Descriptions</p>
<p>In our last analysis, we assessed whether the use of perturbed code descriptions in the training phase improves the performance of the model against non-perturbed NL code descriptions.Therefore, we added perturbed inputs in the 50% of the training set, while the remaining 50% is not perturbed, and evaluated the performance of the models against non-perturbed data.The key idea is to understand whether the models gain benefits from the data augmentation when generating snippets starting from the original non-perturbed intents.</p>
<p>The rationale for perturbing 50% of the training set instead of alternative percentages was guided by our findings in RQ2, where different perturbation ratios were tested.The perturbing half of the training set showed a good tradeoff between maintaining performance on non-perturbed data and enhancing robustness against varied inputs.These findings motivated our decision to use the same percentage in RQ3 to validate the effectiveness of our data augmentation strategy on the original clean data.Indeed, in RQ3, our goal is to identify the optimal data augmentation setting that balances robustness and performance without being too aggressive or too mild.Perturbing 100% of the training data could overly distort the training data, whereas a 25% perturbation might not introduce enough diversity to significantly impact the model's robustness.Therefore, a 50% perturbation was chosen to compromise between the model being exposed to sufficient variation to improve generalization and maintain the integrity of the training data.Fig. 9 shows the results of the syntax and semantic evaluation for the three models.Again, we did not include robust accuracy in this analysis since the test set is not perturbed.Fig. 9a shows that the syntactic correctness of the models is pretty similar (≥ 90%), regardless of the perturbation added in the training phase and the model.Notably, CodeT5+ provides an increment of the syntax accuracy by 1% and 2% when trained with word substitution and word omission, respectively.For the semantic evaluation, Fig. 9b shows that the usage of word substitution in the training phase provides a performance that is superior to the one achieved without perturbations for the Seq2Seq and CodeT5+ (+4.6% and +3.1%, respectively).The usage of word omission in half of the training data boosts the performance of CodeT5+ (+4.0%), while the other models decrease their semantic accuracy up to ∼ 2%.</p>
<p>As for the comparison between the models' performance on single-line versus multi-line snippets, Fig. 10 shows that when trained on partially augmented data (50%) and tested on the original clean test set, for Seq2Seq and CodeBERT syntactic accuracy remains high across different perturbations, with single-line descriptions consistently performing slightly better than multi-line descriptions.Conversely, semantic accuracy on single-line snippets is significantly higher for both perturbation types.To summarize, the overall trend suggests that these models generate syntactically correct snippets regardless of their length, whereas are more capable of generating code semantically equivalent to the ground truth when dealing with shorter snippets, regardless of the perturbation type.On the other hand, a larger pre-trained model like CodeT5+ shows once again to benefit more from data augmentation than other models, becoming able to generate more complex multi-line snippets almost as correctly as single-line snippets.Table 9 shows two worth noticing examples in which, despite the nonperturbed test set, the models failed the prediction when trained with the original, non-perturbed training set, but returned syntactically and semantically correct predictions when trained with perturbed code descriptions.The first example of the table shows that the use of word substitution in 50% of the training set helped the model in predicting the right order of the instructions.In the second example, despite the model identifying the correct operands and their ordering, we found that the word move confused the model since this verb is mostly used to transfer contents (e.g., from a register into another one).The removal of words in the training set helped the model to emphasize also other parts of the intent, such as the word right.Therefore, after the data augmentation, the model correctly interpreted the move right by generating the shift right operation (shr instruction).RQ3: Can data augmentation increase the performance of the models against non-perturbed code descriptions?An important takeaway from our experiments is that the use of perturbations in the training data can increase the performance of the models in the code generation also against the original, non-perturbed code descriptions.Indeed, although the syntactic correctness is almost the same for all models (with an improvement up to 1.8%), we found that the semantic correctness increased up to 4.6% when using word substitution and up to 4.0% when using word omission.In particular, a newer, pretrained model such as CodeT5+, benefits more from descriptions that are shorter or contain more variability.Therefore, even if the test set is not perturbed, the models can improve the semantic correctness of the predictions by learning from the higher variability in the training set due to the use of perturbations.This is an effective solution to overcome a fixed structure of the NL intents derived from a single description style of the code used to fine-tune the models.</p>
<p>Overall, when fine-tuned on partially perturbed and then tested on the original data, Seq2Seq and CodeBERT show a behavior comparable to the baseline, proving a trade-off between robustness and performance, but their inability to fully leverage the benefits of data augmentation, due to the little (or zero) data seen during pre-training.On the other hand, models like CodeT5+, which are pre-trained on extensive datasets, can gain greater advantages from data augmentation.Their enhanced pre-training allows them to better generalize from the augmented data, improving their performance on both clean and perturbed data.In conclusion, synonym substitution proves to be an effective data augmentation strategy, being the preferred solution to improve the models' ability to generate code when tested both on perturbed and non-perturbed NL descriptions.</p>
<p>Threats to Validity</p>
<p>Construct Validity</p>
<p>Exclusion of Closed Models: Regarding the closed models, the design choice of not including these models at this stage of our research was driven by three primary motivations.First, current research on offensive code generation typically relies on fine-tuning open-source models to enhance their abilities to generate shellcodes [74,75,59].This approach allows for more granular control over the model's training process and the ability to tailor it specifically to the task at hand.Second, one of the primary goals of our work was to thoroughly assess the effectiveness of the proposed data augmentation strategy.This necessitated a fine-tuning approach, which is not feasible with closed models in a few-shot learning setting.Few-shot learning, while powerful, does not allow for a fair comparison with fine-tuning on an increasingly perturbed dataset, which was crucial for our study.Third, since both attackers and defenders need to avoid leaking their techniques and tactics to their counterparts (OPSEC), we consider the case of an attacker or defender who builds her own AI code generator by fine-tuning a model on a dataset of offensive code.Using few-shot learning on closed models is a promising avenue for future research.It would enable us to evaluate the robustness of public models in the challenging domain of exploit generation, offering a broader perspective on their capabilities and limitations.Use of Open-Source Models: Since the goal of our investigation is to not only assess the robustness of AI code generators but also to thoroughly evaluate the effectiveness of our data augmentation strategy, our analysis requires the fine-tuning of the targeted AI models on the new, perturbed data.Therefore, we fine-tuned three open-source models, representative of the stateof-the-art, on a corpus of offensive code.We plan to extend our assessment with other popular open-source LLMs, such as CodeLlama.Indeed, while finetuning open-source LLMs such as CodeLlama is extremely costly in terms of time and computing resources, it is an interesting path to explore for future work.However, we have also to consider that, although manipulable through prompt engineering, licenses for such models are designed to prevent misuse, including the generation of malicious code, and to promote the ethical use of AI technology.Choice of Perturbation Types: The choice of perturbation types is crucial for construct validity as it influences whether the perturbations effectively represent the variability in natural language descriptions.The exploration of perturbation types is an integral consideration in assessing the robustness of AI-based code generators.Incorporating a more extensive range of perturbations would undoubtedly offer a more comprehensive understanding of the model's responsiveness to the variability of the NL.This aspect forms a crucial direction for future research.In Section 3, we detailed the rationale behind the perturbation types chosen, emphasizing their prevalence in practice.The use of word substitutions and omissions stems from their representative nature in capturing the challenges posed by the variability in NL descriptions.Finally, we remark that the usage of word-based perturbations in NL for code generation models has never been addressed before.Therefore, we consider the implementation of five different perturbations across two categories a valid means to establish a fundamental understanding before exploring a broader spectrum of perturbations in future work.</p>
<p>Internal Validity</p>
<p>Dataset Representativeness: The dataset used in the study is vital for internal validity as it represents the basis for evaluating the robustness of AI-based solutions in generating software exploits.However, if the dataset is not comprehensive or representative of offensive code generation tasks, it may introduce bias, compromising the internal validity of the study's findings.The dataset used for our experiments fits perfectly with the scope of this work since it is the largest collection of offensive code available to date for code generation.This manually curated dataset contains high-quality and detailed descriptions of code, that are often not available in larger corpora for code generation.Indeed, the dataset provides NL descriptions both at the block and statement levels that are closer to the descriptions needed by the models for complex programming tasks.This feature makes the dataset suitable for the injection of the perturbations in the code descriptions.Realism of Shellcode Examples: Regarding the simplicity and size of the examples in our dataset, it is important to note that shellcodes are typically concise sequences of assembly instructions designed to access low-level system components such as memory, stack, and registers.For instance, the lines of codes of shellcode programs used in previous work range between 16 and 46 (average is 26) [35].Consequently, the code descriptions are typically short as they provide detailed, expertly crafted explanations.Nevertheless, the dataset also includes 1,374 intents (approximately 23% of the dataset) that generate multiple lines of assembly code, incorporating various assembly instructions, including complete functions.We acknowledge that there is often an overlap between tokens in the natural language intent and the output code.This overlap is inherent to the nature of shellcode generation, where specific terminology and technical jargon are essential for accurately describing and implementing these low-level operations.This makes the introduction of perturbations such as word omission a valuable solution to make AI code generators less dependent on detailed NL descriptions.The concern about the dataset's realism regarding how code exploits are found is valid.In practice, generating actual exploits often requires information from the targeted program, as not all potential exploits apply universally.However, creating a benchmark that includes such detailed contextual information is complex and resource-intensive.Although we acknowledge that the inclusion of this information can potentially lead to an improvement in performance, this investigation is not a specific goal of our work.Our current dataset is composed of real-world shellcodes collected from reputable online exploit databases [60,13].It represents the largest collection of assembly exploits annotated with natural language descriptions available to date.While compact, it offers a rich and authentic source of data for addressing the broader challenge of offensive code generation.To the best of our knowledge, there are no other comparable corpora in the context of offensive code generation from NL descriptions.As a matter of fact, our dataset is a widely used benchmark in the field of offensive code generation and is considered a high-quality corpora [74,75].We also emphasize the critical importance of assessing and improving model robustness, especially given the lack of high-quality data for training AI models in this domain.Our proposed data augmentation strategy is tailored to enhance the model's ability to generalize from the available data, ensuring that it can maintain performance even when important information is missing or altered.In summary, while our dataset has limitations, it is a significant and realistic resource for the task at hand.Future work includes refining and expanding our approaches to further enhance the robustness and applicability of AI models in this challenging and crucial area.</p>
<p>External Validity</p>
<p>Generalization to Other Code Generation Scenarios: Focusing solely on offensive code generation may limit the external validity of the study's findings, as they may not generalize to other code generation scenarios.However, although offensive code is different from general-purpose ones in terms of programming languages and characteristics, the proposed method can be applied to different code generation scenarios.The decision to focus on offensive code generation is driven by the critical importance of robustness in this specific field.The use of models to generate offensive code, a research topic that is gaining increasing interest in software security, poses unique challenges, requiring models to be robust against NL variations while maintaining syntactic and semantic correctness.The few benchmark datasets for security tasks are limited in size and have detailed NL descriptions [34,35], hence constraining the generalization of results and making offensive code generation an ideal domain for our study.By focusing on exploits, the study addresses a specific and highimpact application of code generation, providing insights into the challenges for assessing and improving the robustness of models via data augmentation in scenarios with stringent requirements.Generalization with Respect to Tokenization Process: In our study, we primarily focused on models using word-level tokenization to establish a consistent baseline and assess the effectiveness of our data augmentation strategy.Our perturbation process modifies the natural language description before tokenization, making it agnostic to the specific tokenizer used.This allows us to introduce variations in the input text, which are then captured by the model during training, hence, potentially generalizing to other tokenization methods, including subword-level tokenizers such as Byte Pair Encoding (BPE).The reason for focusing on word-level tokenization in our experiments was primarily based on the availability and widespread use of pre-trained models that utilize this tokenization scheme.However, we acknowledge that exploring the applicability of our approach with different tokenizers, including BPE, could be an interesting avenue for future research.</p>
<p>Limitations and Future Work</p>
<p>Despite the valuable insights gained from our study on the effects of linguistic perturbations on AI code generators, several limitations warrant discussion and pave the way for future research.Controlled Data Augmentation Strategy: Our methodology involved a controlled data augmentation strategy where we kept the dataset size constant.This approach allowed us to isolate the effects of different perturbation types on model performance without introducing confounding variables associated with dataset size changes.However, this also limits the scope of our findings concerning methodologies that involve increasing the training dataset size through data augmentation.Moreover, implementing this would require a significant reconfiguration of our experimental setup, including redoing all experiments under new conditions and substantially increasing the number of configurations and manual evaluations needed for assessing semantic correctness.Nevertheless, exploring the impact of expanding the training dataset with perturbed data remains an important direction for future work.Fixed Ratio of Word Substitutions: We set the word substitution ratio at 10% of the words identified as suitable for substitution.This decision was based on balancing variability and sentence integrity, aligning with best practices and findings from prior work [70].Nonetheless, we acknowledge that experimenting with varying substitution ratios could offer deeper insights into the model's robustness to different levels of perturbation.Conducting such experiments would significantly expand the number of experimental configurations and further strain the manual evaluation process, which already involves a substantial workload due to the need for assessing semantic correctness.Future research could explore the effects of different substitution ratios, potentially utilizing automated evaluation methods to manage the increased complexity.</p>
<p>Combination of Perturbation Strategies:</p>
<p>In our study, we applied word substitution and word omission perturbations separately to emulate distinct real-world scenarios.Word substitution mimics using different but equivalent ways to describe a code snippet, while word omission represents missing or incomplete information in a code description.Both strategies were effective in increasing model robustness against perturbed test sets, with word substitution also improving performance on the original, non-perturbed test set.However, we did not investigate the combined effect of applying multiple perturbation types simultaneously.Assessing the impact of combined perturbations could provide a more comprehensive understanding of how models cope with complex linguistic variations.This presents a promising avenue for future research.</p>
<p>Conclusion</p>
<p>In this paper, we proposed a method to assess the robustness of AI offensive code generators through the injection of perturbations in the NL code descriptions.We first showed that the perturbed descriptions preserve the semantics of the original ones.Then, we applied the method to assess three state-of-theart models in the automatic generation of offensive assembly code for software security exploits starting from the English language.Our experiments pointed out that the models are not robust when the intents deviate from the ones used in the corpora.In particular, the performance of the models drastically drops when developers do not explicitly specify all the information in the NL intents (word omission).To enhance the robustness of the code generators, we applied the method to perform data augmentation, i.e., to increase the variability of the code descriptions, showing that models increase the performance when at least 50% of the training data is perturbed.Finally, we found that the performance of the code generation task improves when the model is trained with a perturbed version of the code descriptions containing new words (word substitution) also against the original, non-perturbed test-set.</p>
<p>For practitioners looking to enhance model robustness, we recommend employing both word substitution and word omission perturbations in multiple rounds of data augmentation.If resources are constrained, prioritizing word substitution is advisable, as it has demonstrated benefits for both robustness and overall performance.</p>
<p>word substitution if CX is higher than 100, move it into the AX register and then put the AX value on the stack</p>
<p>Fig. 2
2
Fig.2Example of NL description for assembly code.Equivalent intents can be expressed with different words (word substitution) or by omitting some words (word omission).</p>
<p>Fig. 3
3
Fig.3Violin plots showing the cosine similarity between the original and the perturbed NL descriptions in the whole dataset.</p>
<p>n o c o n f i d e n c e l o w c o n f i d e n c e s o m e c o n f i d e n c e h i g h c o n f i d e n c e f u l l c o n f i d e n cn o c o n f i d e n c e l o w c o n f i d e n c e s o m e c o n f i d e n c e h i g h c o n f i d e n c e f u l l c o n f i d e n c</p>
<p>Cosine similarity higher than the threshold.Cosine similarity lower than the threshold.</p>
<p>Fig.</p>
<p>Fig. Human scores of perturbations with cosine similarity higher or lower than the threshold.</p>
<p>Fig. 5 Fig. 6
56
Fig. 5 Comparison between the models' performance on single-line code snippets vs. multiline code snippets in terms of syntactic (SYN) and semantic (SEM) accuracy.</p>
<p>Fig. 7
7
Fig. 7 Comparison between the results obtained by models with data augmentation on the perturbed test set against the results obtained on the original, non-perturbed training/test set (None).The None bar represents the baseline model performance, i.e., 0% perturbed training and test sets, whereas the Word Substitution and Word Omission bars represent the best model performance when trained on data 100% augmented with the word substitution and word omission perturbations and tested on a fully (100%) perturbed test set, respectively.</p>
<p>Fig. 8
8
Fig.8Comparison between the models' performance on single-line code snippets vs. multiline code snippets when trained on partially augmented data (50%) and tested on fully perturbed data in terms of syntactic (SYN), semantic (SEM), and robust accuracy (ROB).Figures on the left show the performance when data is perturbed with word substitution, while figures on the right show the performance when data is perturbed with word omission.</p>
<p>Fig. 9
9
Fig. 9 Comparison between results obtained by models with data augmentation on the original, non-perturbed test set against the results obtained on the original, non-perturbed training/test set (None).</p>
<p>Fig. 10
10
Fig.10Comparison between the models' performance on single-line code snippets vs. multi-line code snippets when trained on partially augmented data (50%) and tested on the original, clean data in terms of syntactic (SYN) and semantic (SEM) accuracy.Figures on the left show the performance when data is perturbed with word substitution, while figures on the right show the performance when data is perturbed with word omission.</p>
<p>Word Omission Code Generation Task
"Sum the contents ofVocabularyExtract vocabulary words from intent (i.e., register/label/ function names etc.)Replace one or more words with new wordsReinsert previously extracted words into final intentthe ebx register into the eax register""Add the contents of the"Add the contents of into""Sum the contents of into"ebx registerLegend intents Natural languageinto the eax register"omitted that may be Identify wordsOmit identified and regex POS-tagging words through"Add the contents of the ebx register into the eax register"Omission: "register" (2)∮ 4.1∮ 4.2Intents with wordIntents with∮ 4.3"add var1, var0""Add contents of var0 into var1"omissionword substitution"add eax, ebx"Post-processingNMT ModelPre-processingCode snippets"add var1, var0""Sum contents of var0 register into var1 register"</p>
<p>Table 1
1
Examples of the word substitution on the intents.Underlined text refers to the candidate word replacing the original one.
Word SubstitutionNL IntentNone (Original Intent)Store the shellcode pointer in the ESI register.w/o constraintsStock the shellcode pointer in the ESI register.with constraintsSave the shellcode pointer in the ESI register.</p>
<p>Table 2
2
Examples of word omission on the same intent.$$ $Slashed text refers to the omitted words.
Word OmissionIntentNone (Original Intent)Store the shellcode pointer in the ESI registerAction-related Words$ $ $ Store the shellcode pointer in the ESI registerStructure-related WordsStore the shellcode pointer in the ESI$ $ $
registerName-related WordsStore the shellcode pointer in the ËSI register</p>
<p>Table 3
3
Examples of assembly code with NL descriptions from our dataset.
Code SnippetEnglish IntentPerform the xor between BL register andxor bl, 0xBB \n jz formatting \n mov0xBB and jump to the label formatting ifcl, byte [esi]the result is zero else move the currentbyte of the shellcode in the CL register.xor ecx, ecx \n mul ecxZero out the EAX and ECX registers.</p>
<p>Table 4
4
Dataset statistics
MetricNL IntentsAssembly SnippetsUnique lines5, 7403, 316Unique tokens2, 8551, 770Avg. tokens per line9.185.83</p>
<p>Table 5
5
Evaluation of the models with and w/o perturbations in the test set.None indicates the original, non-perturbed test set.
Seq2SeqCodeBERTCodeT5+Perturb.SYNSEMROBSYNSEMROBSYNSEMROBNone0.950.65-0.930.69-0.900.69-Word Substit.0.860.510.660.890.490.680.730.420.58Word Omiss.0.810.330.450.670.320.440.750.370.51</p>
<p>Table 6
6
Evaluation of the models trained with different percentages of perturbed code descriptions.The test set is 100% perturbed.The worst performance per perturbation is red, while the best is blue.
Seq2SeqCodeBERTCodeT5+Perturb.Advers. InputsSYNSEMROBSYNSEMROBSYNSEMROB0%0.860.510.660.890.490.680.730.420.58Word Sub-25%0.910.520.800.930.620.860.900.630.88stitution50%0.910.570.850.920.660.900.880.620.87100%0.910.570.870.920.640.880.900.670.920%0.810.330.450.670.320.440.750.370.51Word25%0.890.380.570.890.450.610.890.460.62Omission50%0.900.390.610.910.460.630.890.470.64100%0.920.400.600.940.480.660.900.470.63</p>
<p>Table 8
8
Jensen-Shannon divergence (JSD) values indicating the divergence between various training and test set distributions.
ComparisonPerturbation TypeJSD ValueJSD between Original Training and Original Test SetNone0.29Omitted Action Words0.38JSD between Original Training andOmitted Name Words0.36Perturbed Test SetsOmitted Structure Words0.36Word Substitution0.40Omitted Action Words0.34JSD between Perturbed TrainingOmitted Name Words0.32(50%) and Perturbed Test SetsOmitted Structure Words0.33Word Substitution0.33</p>
<p>Table 9
9
Illustrative examples of predictions with and without data augmentation (DA) on the original, non-perturbed test set.The prediction errors are red.
Original IntentPrediction w/o DAPrediction with DApush eax and edx on the stackpush edx \n push eaxpush eax \n push edxmove the bits of ax to theright by a number equal to themov ax, clshr ax, clcontent of cl
The dataset, experimental results, and code to replicate the experiments are available at the following URL: https://github.com/dessertlab/Robustness-of-AI-Offensive-Code-Generators/
For this example, we used Parts-of-speech.Info, a POS tagger based on the Stanford University Part-Of-Speech Tagger.
We injected perturbations in both the training and validation sets.
Acknowledgements This work has been partially supported by the MUR PRIN 2022 program, project FLEGREA, CUP E53D23007950001, the IDA-Information Disorder Awareness Project funded by the European Union-Next Generation EU within the SERICS Program through the MUR National Recovery and Resilience Plan under Grant PE00000014, the SERENA-IIoT project funded by MUR (Ministero dell'Università e della Ricerca) and European Union (Next Generation EU) under the PRIN 2022 program (project code 2022CN4EBH) and the GENIO project (CUP B69J23005770005) funded by MIMIT.We are grateful to the DESSERT Research Group at the University of Naples Federico II, Italy, the students of the Italian training program CyberChallenge.IT 2024, team Naples, and anyone who participated in our survey.FundingThe authors have no relevant financial or non-financial interests to disclose.DeclarationsData and/or Code availabilityThe dataset, experimental results, and code to replicate the experiments are available at the following URL: https://github.com/dessertlab/Robustnessof-AI-Offensive-Code-Generators/.Conflicts of interestsThe authors have no conflicts of interest to declare that are relevant to the content of this article.All authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest or non-financial interest in the subject matter or materials discussed in this manuscript.The authors have no financial or proprietary interests in any material discussed in this article.
Automatic semantic augmentation of language model prompts (for code summarization). T Ahmed, K S Pai, P Devanbu, E Barr, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Contextual string embeddings for sequence labeling. A Akbik, D Blythe, R Vollgraf, Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018. the 27th International Conference on Computational Linguistics, COLING 2018Santa Fe, New Mexico, USAAugust 20-26, 2018. 2018</p>
<p>Generating natural language adversarial examples. M Alzantot, Y Sharma, A Elgohary, B Ho, M B Srivastava, K Chang, 10.18653/v1/d18-1316Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober 31 -November 4, 2018. 2018</p>
<p>Automatic exploit generation. T Avgerinos, S K Cha, A Rebert, E J Schwartz, M Woo, D Brumley, Communications of the ACM. 5722014</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, 3rd International Conference on Learning Representations, ICLR 2015. San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>Synthetic and natural noise both break neural machine translation. Y Belinkov, Y Bisk, 6th International Conference on Learning Representations, ICLR 2018, Vancouver. BC, CanadaApril 30 -May 3, 2018. 2018Conference Track Proceedings. OpenReview.net</p>
<p>Nltk: the natural language toolkit. S Bird, Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions. the COLING/ACL 2006 Interactive Presentation Sessions2006</p>
<p>Gpthreats-3: Is automatic malware generation a threat?. M Botacin, 2023 IEEE Security and Privacy Workshops (SPW). IEEE2023</p>
<p>Bad characters: Imperceptible nlp attacks. N Boucher, I Shumailov, R Anderson, N Papernot, 10.1109/SP46214.2022.98336412022 IEEE Symposium on Security and Privacy (SP). 2022</p>
<p>Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. M Cheng, J Yi, P Y Chen, H Zhang, C J Hsieh, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Identifying technical vocabulary. T M Chung, P Nation, System. 3222004</p>
<p>Automating the correctness assessment of ai-generated code for security contexts. D Cotroneo, A Foggia, C Improta, P Liguori, R Natella, 10.1016/j.jss.2024.112113.URLhttps://www.sciencedirect.com/science/article/pii/S0164121224001584Journal of Systems and Software. 2161121132024</p>
<p>Exploit-Db, Exploit Database Shellcodes. 2023</p>
<p>A survey of data augmentation approaches for NLP. S Y Feng, V Gangal, J Wei, S Chandar, S Vosoughi, T Mitamura, E Hovy, 10.18653/v1/2021.findings-acl.84Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, Association for Computational Linguistics2021</p>
<p>Codebert: A pre-trained model for programming and natural languages. Z Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, M Zhou, 10.18653/v1/2020.findings-emnlp.139Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics16-20 November 2020. 20202020Findings of ACL</p>
<p>J Foster, Sockets, Shellcode, Porting, and Coding: Reverse Engineering Exploits and Tool Coding for Security Professionals. Elsevier Science2005</p>
<p>Soft contextual data augmentation for neural machine translation. F Gao, J Zhu, L Wu, Y Xia, T Qin, X Cheng, W Zhou, T Y Liu, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Keeping pace with ever-increasing data: Towards continual learning of code intelligence models. S Gao, H Zhang, C Gao, C Wang, 10.1109/ICSE48619.2023.0001545th IEEE/ACM International Conference on Software Engineering, ICSE 2023. Melbourne, AustraliaIEEEMay 14-20, 2023. 2023</p>
<p>From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy. M Gupta, C Akiri, K Aryal, E Parker, L Praharaj, IEEE Access. 2023</p>
<p>Translation quality assessment: A brief survey on manual and automatic methods. L Han, A Smeaton, G Jones, Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age. for the First Workshop on Modelling Translation: Translatology in the Digital AgeAssociation for Computational Linguistics2021</p>
<p>BERT is robust! A case against synonym-based adversarial examples in text classification. J Hauser, Z Meng, D Pascual, R Wattenhofer, CoRR abs/2109.074032021</p>
<p>How robust are characterbased word embeddings in tagging and MT against wrod scramlbing or randdm nouse?. G Heigold, S Varanasi, G Neumann, J Van Genabith, Proceedings of the 13th Conference of the Association for Machine Translation in the Americas. the 13th Conference of the Association for Machine Translation in the AmericasBoston, MA20181Association for Machine Translation in the Americas</p>
<p>Semantic robustness of models of source code. J Henkel, G Ramakrishnan, Z Wang, A Albarghouthi, S Jha, T Reps, 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE2022</p>
<p>On robustness of neural semantic parsers. S Huang, Z Li, L Qu, L Pan, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021OnlineAssociation for Computational LinguisticsApril 19 -23, 2021. 2021</p>
<p>Codeattack: Code-based adversarial attacks for pre-trained programming language models. A Jha, C K Reddy, arXiv:2206.000522022arXiv preprint</p>
<p>Is bert really robust? a strong baseline for natural language attack on text classification and entailment. D Jin, Z Jin, J T Zhou, P Szolovits, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>B B Kachru, Y Kachru, C L Nelson, The handbook of world Englishes. John Wiley &amp; Sons200948</p>
<p>Artificial intelligence in fracture detection: transfer learning from deep convolutional neural networks. D Kim, T Mackinnon, Clinical radiology. 7352018</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, ICLR 20153rd International Conference on Learning Representations. San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>Textbugger: Generating adversarial text against real-world applications. J Li, S Ji, T Du, B Li, T Wang, 26th Annual Network and Distributed System Security Symposium, NDSS 2019. San Diego, California, USAThe Internet SocietyFebruary 24-27, 2019. 2019</p>
<p>Named-entity tagging and domain adaptation for better customized translation. Z Li, X Wang, A Aw, E S Chng, H Li, Proceedings of the seventh named entities workshop. the seventh named entities workshop2018</p>
<p>Searching for an effective defender: Benchmarking defense against adversarial word substitution. Z Li, J Xu, J Zeng, L Li, X Zheng, Q Zhang, K W Chang, C J Hsieh, 10.18653/v1/2021.emnlp-main.251Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Shellcode IA32: A dataset for automatic shellcode generation. P Liguori, E Al-Hossami, D Cotroneo, R Natella, B Cukic, S Shaikh, 10.18653/v1/2021.nlp4prog-1.7Proceedings of the 1st Workshop on Natural Language Processing for Programming. the 1st Workshop on Natural Language Processing for ProgrammingAssociation for Computational LinguisticsNLP4Prog 2021. 2021</p>
<p>Can we generate shellcodes via natural language? an empirical study. P Liguori, E Al-Hossami, D Cotroneo, R Natella, B Cukic, S Shaikh, 10.1007/s10515-022-00331-3Automated Software Engineering. 291302022</p>
<p>Evil: Exploiting software via natural language. P Liguori, E Al-Hossami, V Orbinato, R Natella, S Shaikh, D Cotroneo, B Cukic, 10.1109/ISSRE52982.2021.000422021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE). 2021</p>
<p>Who evaluates the evaluators? on automatic metrics for assessing ai-based offensive code generators. P Liguori, C Improta, R Natella, B Cukic, D Cotroneo, Expert Systems with Applications. 2251200732023</p>
<p>Technical vocabulary. D Liu, L Lei, The Routledge handbook of vocabulary studies. Routledge2019</p>
<p>Roberta: A robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, CoRR abs/1907.116922019</p>
<p>Coconut: combining context-aware neural translation models using ensemble for program repair. T Lutellier, H V Pham, L Pang, Y Li, M Wei, L Tan, Proceedings of the 29th ACM SIGSOFT international symposium on software testing and analysis. the 29th ACM SIGSOFT international symposium on software testing and analysis2020</p>
<p>Applying codebert for automated program repair of java simple bugs. E Mashhadi, H Hemmati, 10.1109/MSR52588.2021.0006318th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021. Madrid, SpainIEEEMay 17-19, 2021. 2021</p>
<p>On the robustness of code generation techniques: An empirical study on github copilot. A Mastropaolo, L Pascarella, E Guglielmi, M Ciniselli, S Scalabrino, R Oliveto, G Bavota, 10.1109/ICSE48619.2023.0018145th IEEE/ACM International Conference on Software Engineering, ICSE 2023. Melbourne, AustraliaIEEEMay 14-20, 2023. 2023</p>
<p>Studying the usage of text-to-text transfer transformer to support code-related tasks. A Mastropaolo, S Scalabrino, N Cooper, D N Palacio, D Poshyvanyk, R Oliveto, G Bavota, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Adversarial black-box attacks on text classifiers using multi-objective genetic optimization guided by deep networks. A Mathai, S Khare, S Tamilselvam, S Mani, CoRR abs/2011.039012020</p>
<p>Penetration Testing with Shellcode: Detect, exploit, and secure networklevel and operating system vulnerabilities. H Megahed, 2018Packt Publishing</p>
<p>On evaluation of adversarial perturbations for sequence-to-sequence models. P Michel, X Li, G Neubig, J M Pino, 10.18653/v1/n19-1314Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>Incorporating external annotation to improve named entity translation in nmt. M Modrzejewski, M Exel, B Buschbeck, T L Ha, A Waibel, Proceedings of the 22nd Annual Conference of the European Association for Machine Translation. the 22nd Annual Conference of the European Association for Machine Translation2020</p>
<p>J X Morris, E Lifland, J Lanchantin, Y Ji, Y Qi, arXiv:2004.14174Reevaluating adversarial examples in natural language. 2020arXiv preprint</p>
<p>Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. J X Morris, E Lifland, J Y Yoo, J Grigsby, D Jin, Y Qi, 10.18653/v1/2020.emnlp-demos.16Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -DemosAssociation for Computational LinguisticsNovember 16-20, 2020. 2020</p>
<p>Counter-fitting word vectors to linguistic constraints. N Mrkšić, D Séaghdha, B Thomson, M Gašić, L Rojas-Barahona, P H Su, D Vandyke, T H Wen, S Young, Proceedings of HLT-NAACL. HLT-NAACL2016</p>
<p>XNMT: The eXtensible Neural Machine Translation Toolkit. G Neubig, M Sperber, X Wang, M Felix, A Matthews, S Padmanabhan, Y Qi, D S Sachan, P Arthur, P Godard, J Hewitt, R Riad, L Wang, Conference of the Association for Machine Translation in the Americas (AMTA) Open Source Software Showcase. Boston, USA2018</p>
<p>Data diversification: A simple strategy for neural machine translation. X P Nguyen, S Joty, K Wu, A T Aw, Advances in Neural Information Processing Systems. 332020</p>
<p>An attacker's dream? exploring the capabilities of chatgpt for developing malware. Pa Pa, Y M Tanizaki, S Kou, T Van Eeten, M Yoshioka, K Matsumoto, T , Proceedings of the 16th Cyber Security Experimentation and Test Workshop. the 16th Cyber Security Experimentation and Test Workshop2023</p>
<p>Mining source code descriptions from developer communications. S Panichella, J Aponte, M Di Penta, A Marcus, G Canfora, 20th IEEE International Conference on Program Comprehension (ICPC). IEEE2012. 2012</p>
<p>Python: tokenize. 2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 21672020</p>
<p>N Reimers, Sentencetransformers documentation. 2022</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. N Reimers, I Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational LinguisticsNovember 3-7, 2019. 2019</p>
<p>Making monolingual sentence embeddings multilingual using knowledge distillation. N Reimers, I Gurevych, 10.18653/v1/2020.emnlp-main.365Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. November 16-20, 2020. 2020</p>
<p>Prompt learning for developing software exploits. X Ruan, Y Yu, W Ma, B Cai, Proceedings of the 14th Asia-Pacific Symposium on Internetware. the 14th Asia-Pacific Symposium on Internetware2023</p>
<p>Shell-Storm, Shellcodes database for study cases. 2022</p>
<p>The importance of stop word removal on recall values in text categorization. C Silva, B Ribeiro, 10.1109/IJCNN.2003.1223656Proceedings of the International Joint Conference on Neural Networks. the International Joint Conference on Neural Networks2003. 20033</p>
<p>Industrial-Strength Natural Language Processing. 2023</p>
<p>On learning meaningful code changes via neural machine translation. M Tufano, J Pantiuchina, C Watson, G Bavota, D Poshyvanyk, IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE2019. 2019</p>
<p>L Tychonievich, Parts of Programming Speech. 2014</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Advances in neural information processing systems. 2017</p>
<p>ReCode: Robustness evaluation of code generation models. S Wang, Z Li, H Qian, C Yang, Z Wang, M Shang, V Kumar, S Tan, B Ray, P Bhatia, R Nallapati, M K Ramanathan, D Roth, B Xiang, 10.18653/v1/2023.acl-long.773Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Switchout: an efficient data augmentation algorithm for neural machine translation. X Wang, H Pham, Z Dai, G Neubig, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Y Wang, H Le, A D Gotmare, N D Bui, J Li, S C Hoi, arXiv:2305.07922Codet5+: Open code large language models for code understanding and generation. 2023arXiv preprint</p>
<p>Study on the importance of cultural context analysis in machine translation. Z Wang, Proceedings of The Eighth International Conference on Bio-Inspired Computing: Theories and Applications. The Eighth International Conference on Bio-Inspired Computing: Theories and ApplicationsSpringer2013. 2013</p>
<p>Eda: Easy data augmentation techniques for boosting performance on text classification tasks. J Wei, K Zou, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Evaluating neural machine comprehension model robustness to noisy inputs and adversarial attacks. W Wu, D Arendt, S Volkova, CoRR abs/2005.001902020</p>
<p>A study of bert for context-aware neural machine translation. X Wu, Y Xia, J Zhu, L Wu, S Xie, T Qin, Machine Learning. 11132022</p>
<p>Autopwn: Artifact-assisted heap exploit generation for ctf pwn competitions. D Xu, K Chen, M Lin, C Lin, X Wang, IEEE Transactions on Information Forensics and Security. 2023</p>
<p>Dualsc: Automatic generation and summarization of shellcode via transformer and dual learning. G Yang, X Chen, Y Zhou, C Yu, 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE2022</p>
<p>Exploitgen: Templateaugmented exploit code generation based on codebert. G Yang, Y Zhou, X Chen, X Zhang, T Han, T Chen, Journal of Systems and Software. 1971115772023</p>
<p>Natural attack for pre-trained models of code. Z Yang, J Shi, J He, D Lo, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Adversarial examples for models of code. N Yefet, U Alon, E Yahav, 10.1145/3428230Proc. ACM Program. Lang. 4(OOPSLA). ACM Program. Lang. 4(OOPSLA)202016230</p>
<p>Towards improving adversarial training of NLP models. J Y Yoo, Y Qi, 10.18653/v1/2021.findings-emnlp.81Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Data augmentation by program transformation. S Yu, T Wang, J Wang, Journal of Systems and Software. 1901113042022</p>
<p>An extensive study on pretrained models for program understanding and generation. Z Zeng, H Tan, H Zhang, J Li, Y Zhang, L Zhang, Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis2022</p>
<p>Training deep code comment generation models via data augmentation. X Zhang, Y Zhou, T Han, T Chen, Proceedings of the 12th Asia-Pacific Symposium on Internetware. the 12th Asia-Pacific Symposium on Internetware2020</p>
<p>Y Zhang, X Wang, Z Xi, H Xia, T Gui, Q Zhang, X Huang, arXiv:2402.16431Rocoins: Enhancing robustness of large language models through code-style instructions. 2024arXiv preprint</p>
<p>Adversarial robustness of deep code comment generation. Y Zhou, X Zhang, J Shen, T Han, T Chen, H C Gall, 10.1145/3501256ACM Trans. Softw. Eng. Methodol. 314302022</p>
<p>How robust is a large pre-trained language model for code generationƒ a case on attacking gpt2. R Zhu, C Zhang, 10.1109/SANER56733.2023.000762023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). Los Alamitos, CA, USAIEEE Computer Society2023</p>
<p>On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. T Y Zhuo, Z Li, Y Huang, F Shiri, W Wang, G Haffari, Y F Li, 10.18653/v1/2023.eacl-main.77Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>            </div>
        </div>

    </div>
</body>
</html>
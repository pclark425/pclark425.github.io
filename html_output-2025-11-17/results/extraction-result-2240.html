<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2240 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2240</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2240</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-281889022</p>
                <p><strong>Paper Title:</strong> CEAF: Capsule network enhanced feature fusion architecture for Chinese Named Entity Recognition</p>
                <p><strong>Paper Abstract:</strong> Chinese Named Entity Recognition (NER) is a fundamental task in the field of natural language processing, where achieving deep semantic mining of nested entities and accurate disambiguation of character-level boundary ambiguities stands as its core challenge. Existing methods, mostly based on the BiLSTM-CRF sequence labeling framework or Transformer attention mechanisms, have inherent limitations in modeling the hierarchical structural dependencies of nested entities and resolving semantic conflicts in overlapping character spans. To address challenges such as the lack of morphological markers, propagation of boundary ambiguities, and insufficient geometric modeling in the feature space, we propose a novel multi-stage neural architecture—the CEAF model, a specialized neural framework tailored for Chinese NER tasks. The architecture leverages BERT-derived subword embeddings to capture character-level contextual representation and incorporates BiLSTM to model position-sensitive sequential patterns. Meanwhile, to effectively tackle the complex challenges of boundary uncertainty and nested entity composition, the CEAF model innovatively introduces the Deep Context Feature Attention Module (DCAM). This module pioneeringly integrates capsule routing protocols with position-aware attention mechanisms, processing information through dual parallel paths: on one hand, it leverages the powerful spatial relationship modeling capability of capsule networks to clearly parse the hierarchical structure and part-whole relationships between entities; on the other hand, it utilizes position-aware attention to focus on key positional information, dynamically adjust the attention to different positional information, accurately locate entity boundaries, effectively resolve boundary ambiguity, and achieve efficient and accurate modeling of nested entity structures. In addition, the Adaptive Feature Fusion Network (AFFN) effectively bridges the semantic gap between global contextual coherence and local boundary precision by selecting more discriminative fusion features. Generalization experiments on three Chinese benchmark datasets and one English dataset demonstrate that the CEAF model outperforms baseline models. Visualization analysis further verifies the modeling capability of the CEAF model, providing new insights into geometric deep learning approaches for Chinese NER.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2240.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2240.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CEAF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Capsule network Enhanced feature trAtion Fusion architecture (CEAF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage Chinese NER architecture that adaptively fuses BERT embeddings, Bi-LSTM sequential features, a capsule network with dynamic routing, and a Deep Context Attention Module (DCAM) via an Adaptive Feature Fusion Network (AFFN) to model nested entities and boundary ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CEAF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-level NER system combining: (1) BERT-derived contextual subword embeddings, (2) Bi-LSTM sequence modeling, (3) a capsule network with dynamic routing to form hierarchical vectorized entity representations, (4) DCAM — adaptive pooling + conv + position-aware attention — for boundary-aware context, and (5) AFFN — a gating-based adaptive fusion network — to dynamically weight and merge features before CRF decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>hierarchical vectorized capsules with dynamic routing (task-specific routing), position-aware adaptive attention (DCAM), and gating-based adaptive fusion (AFFN) to allocate representation capacity to salient local/global features</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chinese Named Entity Recognition (also evaluated in English NER benchmarks CoNLL-2003, OntoNotes 5.0, GENIA)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Chinese: Toutiao F1=90.4%, Weibo F1=89.1%, MSRA F1=91.8%, Chinese OntoNotes (Table 6) F1=92.2%. English cross-language: CoNLL-2003 F1=94.7%, OntoNotes (English eval, Table 9) F1=87.2%, GENIA F1=82.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>BERT-BiLSTM-CRF baseline (as reported in paper): Toutiao F1=85.2%, Weibo F1=84.4%, MSRA F1=87.8%, OntoNotes (Chinese) F1=88.3%. (CEAF shows clear F1 gains versus this baseline.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Qualitative: Requires substantial compute due to multi-stage architecture and dynamic routing; training reported to require ~24 GB memory; inference latency higher than lightweight models (ALBERT), and dynamic routing / high-dimensional capsule matrix ops increase inference time and memory usage. No FLOPs or exact latency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Not quantified numerically in paper for BERT-BiLSTM-CRF, but described qualitatively as lower inference cost than CEAF; baseline lacks the capsule dynamic routing overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Paper claims robustness to limited labeled data and better generalization from pretrained embeddings; no numeric sample-efficiency curves reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Cross-language/cross-domain results: CEAF improves CoNLL-2003 F1 by +1.2 points over SpanBERT; Monte Carlo cross-validation reports CEAF performance decay in cross-domain migration of 5.3% (lower than BERT-MRC), indicating stronger transfer/generalization. On GENIA CEAF improves recall on long-entity recognition by ~12% (reported qualitative/relative stat).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Visualization analyses reported (figures) that demonstrate CEAF's geometric/hierarchical modeling capability (capsule vectors & attention maps) and give qualitative interpretability of part–whole entity structure; no formal user study or attribution metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Single primary task (NER) with evaluation across multiple datasets/domains (social media, news, biomedical). Adaptive modules consistently improve F1 across datasets; no full multi-task benchmark reported.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Paper explicitly notes deployment constraints: inference slower and more memory-hungry than lightweight alternatives; recommends pruning/quantization/model parallelism for high-concurrency or edge deployment. Training memory footprint ~24GB.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adaptive, dynamically routed hierarchical representations (capsules + DCAM + AFFN) produce consistent F1 improvements across noisy/nested Chinese NER datasets and English benchmarks, at the cost of greater memory and inference overhead from dynamic routing.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>CEAF's adaptive allocation of representation capacity to local (capsule) vs global (BERT/DCAM) information yields empirical improvements vs uniform baselines, supporting the view that task-aligned, dynamic abstractions help performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2240.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CapsNet (dynamic routing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Capsule Network with Dynamic Routing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vector-valued capsules and an iterative dynamic routing algorithm that route low-level capsule outputs to higher-level capsules based on agreement, enabling hierarchical part-whole representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic Routing Between Capsules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Capsule Network (with dynamic routing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Capsules produce vector outputs whose length encodes existence probability and orientation encodes attributes; dynamic routing iteratively updates coupling coefficients between capsule layers to adaptively allocate information to higher-level capsules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>hierarchical vector representations with iterative dynamic routing (adaptive coupling coefficients); Leaky-SoftMax used to suppress noise.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used inside CEAF for Chinese NER (nested entity modeling); evaluated in ablation on Toutiao dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Ablation (Toutiao): F1 by number of high-level capsules — 8 capsules F1=86.2%, 16→88.1%, 32→89.8% (peak), 64→88.1%, 128→86.9% (shows a non-monotonic/inverted-U relationship with capacity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to BERT-BiLSTM-CRF baseline (Toutiao F1=85.2%), adding capsules raises F1 (e.g., CEAF-C variants show gains).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Dynamic routing increases compute: high-dimensional matrix transforms and iterative routing slow inference and increase memory; paper reports marked slowdown and higher memory usage without precise timings; optimal tradeoff observed at 32 capsules.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Baseline (no capsule) is computationally cheaper; precise comparative metrics not given.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Paper and related discussion claim CapsNets are effective in low-resource/noisy scenarios; no quantitative sample-efficiency numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Ablation suggests optimal capsule count improves generalization (32 capsules best). No cross-dataset capsule-only transfer metrics given.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Capsules provide part–whole vectorized geometry; visualization in paper supports interpretability of hierarchical relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Increasing capsule count beyond optimal degrades both generalization and computational efficiency (overfitting and routing chaos); large capsule banks unsuitable for resource-limited deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamic routing in capsule networks offers task-aligned hierarchical representations that increase NER F1 (peak at 32 capsules) but raises computation and memory costs, with too many capsules harming generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical ablation shows adaptive routing (task-aligned allocation) improves performance vs no-capsule/uniform schemes, confirming benefits of dynamic, task-aligned abstraction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2240.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Context Feature Attention Module (DCAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A position-aware attention module combining adaptive 1D average/max pooling, MLP gating, and 1×3 convolutions to reweight contextual features and improve boundary localization for entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DCAM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Processes Bi-LSTM outputs via parallel adaptive AvgPool/MaxPool, MLP gating to produce attention weights, multiplies weights into features, applies 1D conv (kernel size tuned to 3) to extract keyword semantic features and position-sensitive attention.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>adaptive pooling + learned MLP attention weights + position-aware 1D convolutional attention to allocate representational emphasis to boundary-relevant positions</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Feature enhancement for Chinese NER within CEAF; evaluated as CEAF-D ablation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CEAF-D variant (adds DCAM to baseline) F1s: Toutiao F1=86.6%, Weibo F1=85.0%, MSRA F1=89.4%, OntoNotes (Chinese) F1=89.5% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>BERT-BiLSTM-CRF baseline F1s: Toutiao 85.2%, Weibo 84.4%, MSRA 87.8%, OntoNotes 88.3%; DCAM variant shows improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Adds 1D pooling/MLP and small 1D convolution (kernel size 3 optimal). Paper reports kernel-size ablation (3 best) but no numerical cost figures; moderate extra compute vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Baseline lacks DCAM overhead; exact comparative costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Inclusion of DCAM improves performance on noisy/social datasets (Weibo/Toutiao) indicating better robustness/generalization to informal contexts; no explicit cross-domain numeric breakdown beyond dataset scores.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Position-aware attention maps and visualization indicate DCAM highlights boundary-relevant tokens; qualitative interpretability claimed.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Lightweight relative to capsule layer but still adds inference cost; no explicit constrained-device evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>DCAM adaptively reallocates representational emphasis to position-relevant context, improving F1 over the uniform baseline, especially on noisy social media datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>DCAM's input-dependent attention reallocates capacity to task-relevant positions and yields measurable gains vs uniform representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2240.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AFFN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Feature Fusion Network (AFFN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gating-based adaptive fusion module that dynamically weights and fuses local spatial features (from capsule routing) and global dependency features (from DCAM) to produce discriminative joint representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AFFN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies nonlinear transforms (tanh + sigmoid) to two feature streams, computes a gating vector z via sigmoid over combined signals and produces fused vector f = z * h_c + (1 - z) * h_d, enabling input-dependent, multi-granularity fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>gated adaptive fusion (input-dependent weighting) that dynamically assigns representational weight to local vs global features</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Feature fusion for Chinese NER within CEAF; evaluated as CEAF-A ablation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>CEAF-A variant (adds AFFN to baseline) F1s: Toutiao F1=86.2%, Weibo F1=85.4%, MSRA F1=89.0%, OntoNotes F1=89.4% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>BERT-BiLSTM-CRF baseline F1s: Toutiao 85.2%, Weibo 84.4%, MSRA 87.8%, OntoNotes 88.3%. AFFN variant shows gains.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Adds lightweight gating computations (elementwise ops and small MLPs); modest extra compute relative to capsule/DCAM components; no exact numeric costs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Baseline lacks dynamic gating, hence slightly cheaper; numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>AFFN improves aggregated multi-component CEAF performance and helps robustness on nested/blurred-boundary datasets; quantitative cross-domain transfer not isolated to AFFN alone.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Gating weights can be examined to understand which modality (local/global) the model emphasizes per instance; qualitative interpretability discussed but not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Lightweight relative to other modules; no explicit constrained-device evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Dynamic gating fusion of heterogeneous feature streams yields better NER performance than fixed fusion, demonstrating benefits of input-conditioned allocation of representational weight.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>AFFN's adaptive combination of features outperforms fixed-weight or simple concatenation strategies, aligning with task-aligned abstraction principles.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2240.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-BiLSTM-CRF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT + Bi-LSTM + Conditional Random Field</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common strong baseline in NER that stacks pre-trained BERT contextual embeddings, Bi-LSTM sequence modeling, and CRF decoding to perform sequence labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-BiLSTM-CRF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generates contextual token vectors via BERT, refines sequential patterns with Bi-LSTM, and enforces label consistency with a CRF output layer; represents a relatively uniform pipeline without explicit adaptive feature allocation across modules.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>uniform stacked representation pipeline (contextual embeddings + sequential encoder + uniform fusion into CRF)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chinese Named Entity Recognition (baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Reported baseline F1s (Table 6): Toutiao F1=85.2%, Weibo F1=84.4%, MSRA F1=87.8%, OntoNotes (Chinese) F1=88.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Not reported numerically in paper; described as lower inference/memory cost than CEAF qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Used as baseline; CEAF reports lower cross-domain degradation than some BERT-based baselines (e.g., BERT-MRC) but exact numbers per baseline vary.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Less resource intensive than CEAF; exact metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Serves as a strong uniform-representation baseline; CEAF's task-aligned, adaptive modules outperform this pipeline consistently on noisy/nested datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>BERT-BiLSTM-CRF is a strong baseline but lacks explicit task-aligned adaptive allocation; CEAF's improvements indicate such allocation helps, but BERT-based baselines remain competitive in cleaner domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2240.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpanKL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpanKL (span representation model for continual NER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A span-based model that builds span representations and treats spans as independent units for multi-label classification, intended to handle nested entities and continual learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpanKL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses BERT-large for context encoding and a span representation layer with entity-type-specific feedforward networks to represent and classify all candidate spans; this is a task-aligned span-centric representation instead of token-uniform labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-large (implicit in description)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>span-based representations + type-specific feedforward heads (allocates capacity to spans as task units)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chinese NER (span-based nested entity recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported in Table 7: Toutiao F1=85.3%, Weibo F1=83.8%, MSRA F1=87.4% (as compared in paper's comparative table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Span enumeration can be computationally costly (all spans treated as units), but exact costs not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Span enumeration has higher complexity and can be slower; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Span-centric, task-aligned representations are effective for nested entities but can be computationally expensive; in this paper CEAF outperforms SpanKL on the social/nested datasets reported.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>SpanKL exemplifies task-aligned representations (span units) that improve nested-entity modeling, consistent with the benefit of non-uniform, task-specific representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2240.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiNER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HiNER (hierarchical feature fusion for Chinese NER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical feature-fusion model designed to handle nested and discontinuous entities via local-global attention dual-path and hierarchical fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HiNER</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implements hierarchical feature fusion combining local relationship classification and multi-source semantic fusion with local-global dual-path attention to parse nested/discontinuous entities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>hierarchical representations with local-global dual-path attention (allocates different processing to local vs global features)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chinese NER (benchmark comparisons in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Table 7 comparisons: Toutiao F1=86.6%, Weibo F1=85.1%, MSRA F1=88.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Hierarchical fusion implies added compute vs simple baselines; no explicit metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Hierarchical, task-aligned fusion improves nested-entity recognition relative to simpler baselines; CEAF further improves on these datasets by combining capsules, DCAM, and AFFN.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>HiNER's hierarchical allocation of representational processing aligns with the benefits of task-specific abstractions described in CEAF.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2240.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoftLexicon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexicon-aware approach that dynamically matches dictionary lexical information to character representations via a soft selection mechanism, improving boundary ambiguity handling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SoftLexicon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dynamically fuses lexicon (word-level) information with character-level representations using a soft selection mechanism instead of hard dictionary matching, thereby adaptively incorporating lexical signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>lexicon-aware dynamic fusion (soft selection) that adaptively integrates dictionary features with token representations</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chinese NER (referenced as related work and comparative baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>SoftLexicon exemplifies adaptive lexical fusion (task-aligned signal incorporation) as an alternative to static dictionary injection; paper cites it as a strong lexicon-aware approach but does not re-run its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>SoftLexicon's dynamic lexicon fusion aligns with the claim that adaptive, task-aware incorporation of features improves NER performance over purely uniform schemes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2240.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baichuan2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baichuan2-13B-Chat (Baichuan2 series large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter open large language model trained on a large corpus, reported to have strong Chinese/English comprehension and competitive NER performance when used for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Baichuan2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based generative model pretrained on trillions of tokens, used as a baseline/competitor for NER tasks (in paper described qualitatively as strong in Chinese).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>large uniform transformer stacks with contextual self-attention (no explicit task-specific adaptive routing reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General language understanding; used as comparative baseline for Chinese NER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Paper comments that large LLMs show some strengths on normalized datasets (MSRA) but are less effective on nested/unnormalized social media texts due to domain adaptation issues.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Large model size implies high resource usage; paper notes data-missing issues and domain adaptation difficulty but gives no exact costs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Large LLMs provide strong general knowledge but may not adapt as effectively to nested / noisy NER without domain adaptation; CEAF outperforms them on several noisy Chinese datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Large uniform LMs are powerful but not specifically designed to allocate representational capacity per NER task demands; CEAF's task-aligned modules outperform them in targeted NER scenarios.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2240.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-14B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-14B-Chat (Qwen series large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 14B-parameter conversational transformer model supporting long contexts and strong Chinese/English capabilities; referenced as a comparative baseline for NER.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-14B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer model pretrained on large corpora with extended context length; used as a general-purpose baseline in paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>uniform transformer self-attention stacks</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General language understanding; referenced in Chinese NER comparison table</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Large uniform transformer LMs are reported as competitive generalist baselines but do not systematically outperform CEAF on noisy/nested Chinese datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Paper's comparisons indicate task-specific adaptive architectures (CEAF) can beat very large uniform models in specialized noisy NER settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2240.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2240.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LEBERT (Lexicon-Enhanced BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT variant that deeply injects vocabulary/lexicon information into mid-layers via adapters to improve boundary recognition by augmenting BERT with lexicon signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LEBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dynamically integrates dictionary/word-level information into BERT via vocabulary adapters and character-word pairs, enabling more task-aligned lexical representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>vocabulary adapters and character-word pair sequences injected into BERT mid-layers (adaptive lexicon enrichment)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chinese NER (vocabulary enhancement scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported in Table 7: Toutiao F1=87.9%, Weibo F1=86.5%, MSRA F1=89.6% (used as comparative method).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Adapter layers add modest parameters and compute compared to vanilla BERT; no exact numbers reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Mid-layer lexicon adapters provide task-aligned lexical augmentation that improves NER boundary detection over purely uniform BERT encodings in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>LEBERT demonstrates that injecting targeted lexical adapters (task-aligned augmentations) helps NER, consistent with the benefits of adaptive representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dynamic Routing Between Capsules <em>(Rating: 2)</em></li>
                <li>SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection <em>(Rating: 2)</em></li>
                <li>SpanBERT: Improving pre-training by representing and predicting spans <em>(Rating: 2)</em></li>
                <li>LUKE: Deep Contextualized Entity Representations with Entity-Aware Self-Attention <em>(Rating: 2)</em></li>
                <li>HiNER: hierarchical feature fusion for Chinese Named Entity Recognition <em>(Rating: 2)</em></li>
                <li>A Unified MRC Framework for Named Entity Recognition <em>(Rating: 1)</em></li>
                <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2240",
    "paper_id": "paper-281889022",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "CEAF",
            "name_full": "Capsule network Enhanced feature trAtion Fusion architecture (CEAF)",
            "brief_description": "A multi-stage Chinese NER architecture that adaptively fuses BERT embeddings, Bi-LSTM sequential features, a capsule network with dynamic routing, and a Deep Context Attention Module (DCAM) via an Adaptive Feature Fusion Network (AFFN) to model nested entities and boundary ambiguity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CEAF",
            "model_description": "Multi-level NER system combining: (1) BERT-derived contextual subword embeddings, (2) Bi-LSTM sequence modeling, (3) a capsule network with dynamic routing to form hierarchical vectorized entity representations, (4) DCAM — adaptive pooling + conv + position-aware attention — for boundary-aware context, and (5) AFFN — a gating-based adaptive fusion network — to dynamically weight and merge features before CRF decoding.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "hierarchical vectorized capsules with dynamic routing (task-specific routing), position-aware adaptive attention (DCAM), and gating-based adaptive fusion (AFFN) to allocate representation capacity to salient local/global features",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Chinese Named Entity Recognition (also evaluated in English NER benchmarks CoNLL-2003, OntoNotes 5.0, GENIA)",
            "performance_task_aligned": "Chinese: Toutiao F1=90.4%, Weibo F1=89.1%, MSRA F1=91.8%, Chinese OntoNotes (Table 6) F1=92.2%. English cross-language: CoNLL-2003 F1=94.7%, OntoNotes (English eval, Table 9) F1=87.2%, GENIA F1=82.1%.",
            "performance_uniform_baseline": "BERT-BiLSTM-CRF baseline (as reported in paper): Toutiao F1=85.2%, Weibo F1=84.4%, MSRA F1=87.8%, OntoNotes (Chinese) F1=88.3%. (CEAF shows clear F1 gains versus this baseline.)",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Qualitative: Requires substantial compute due to multi-stage architecture and dynamic routing; training reported to require ~24 GB memory; inference latency higher than lightweight models (ALBERT), and dynamic routing / high-dimensional capsule matrix ops increase inference time and memory usage. No FLOPs or exact latency numbers reported.",
            "computational_efficiency_baseline": "Not quantified numerically in paper for BERT-BiLSTM-CRF, but described qualitatively as lower inference cost than CEAF; baseline lacks the capsule dynamic routing overhead.",
            "sample_efficiency_results": "Paper claims robustness to limited labeled data and better generalization from pretrained embeddings; no numeric sample-efficiency curves reported.",
            "transfer_generalization_results": "Cross-language/cross-domain results: CEAF improves CoNLL-2003 F1 by +1.2 points over SpanBERT; Monte Carlo cross-validation reports CEAF performance decay in cross-domain migration of 5.3% (lower than BERT-MRC), indicating stronger transfer/generalization. On GENIA CEAF improves recall on long-entity recognition by ~12% (reported qualitative/relative stat).",
            "interpretability_results": "Visualization analyses reported (figures) that demonstrate CEAF's geometric/hierarchical modeling capability (capsule vectors & attention maps) and give qualitative interpretability of part–whole entity structure; no formal user study or attribution metrics.",
            "multi_task_performance": "Single primary task (NER) with evaluation across multiple datasets/domains (social media, news, biomedical). Adaptive modules consistently improve F1 across datasets; no full multi-task benchmark reported.",
            "resource_constrained_results": "Paper explicitly notes deployment constraints: inference slower and more memory-hungry than lightweight alternatives; recommends pruning/quantization/model parallelism for high-concurrency or edge deployment. Training memory footprint ~24GB.",
            "key_finding_summary": "Adaptive, dynamically routed hierarchical representations (capsules + DCAM + AFFN) produce consistent F1 improvements across noisy/nested Chinese NER datasets and English benchmarks, at the cost of greater memory and inference overhead from dynamic routing.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "CEAF's adaptive allocation of representation capacity to local (capsule) vs global (BERT/DCAM) information yields empirical improvements vs uniform baselines, supporting the view that task-aligned, dynamic abstractions help performance.",
            "uuid": "e2240.0"
        },
        {
            "name_short": "CapsNet (dynamic routing)",
            "name_full": "Capsule Network with Dynamic Routing",
            "brief_description": "Vector-valued capsules and an iterative dynamic routing algorithm that route low-level capsule outputs to higher-level capsules based on agreement, enabling hierarchical part-whole representations.",
            "citation_title": "Dynamic Routing Between Capsules",
            "mention_or_use": "use",
            "model_name": "Capsule Network (with dynamic routing)",
            "model_description": "Capsules produce vector outputs whose length encodes existence probability and orientation encodes attributes; dynamic routing iteratively updates coupling coefficients between capsule layers to adaptively allocate information to higher-level capsules.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "hierarchical vector representations with iterative dynamic routing (adaptive coupling coefficients); Leaky-SoftMax used to suppress noise.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Used inside CEAF for Chinese NER (nested entity modeling); evaluated in ablation on Toutiao dataset",
            "performance_task_aligned": "Ablation (Toutiao): F1 by number of high-level capsules — 8 capsules F1=86.2%, 16→88.1%, 32→89.8% (peak), 64→88.1%, 128→86.9% (shows a non-monotonic/inverted-U relationship with capacity).",
            "performance_uniform_baseline": "Compared to BERT-BiLSTM-CRF baseline (Toutiao F1=85.2%), adding capsules raises F1 (e.g., CEAF-C variants show gains).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Dynamic routing increases compute: high-dimensional matrix transforms and iterative routing slow inference and increase memory; paper reports marked slowdown and higher memory usage without precise timings; optimal tradeoff observed at 32 capsules.",
            "computational_efficiency_baseline": "Baseline (no capsule) is computationally cheaper; precise comparative metrics not given.",
            "sample_efficiency_results": "Paper and related discussion claim CapsNets are effective in low-resource/noisy scenarios; no quantitative sample-efficiency numbers provided in this paper.",
            "transfer_generalization_results": "Ablation suggests optimal capsule count improves generalization (32 capsules best). No cross-dataset capsule-only transfer metrics given.",
            "interpretability_results": "Capsules provide part–whole vectorized geometry; visualization in paper supports interpretability of hierarchical relationships.",
            "multi_task_performance": null,
            "resource_constrained_results": "Increasing capsule count beyond optimal degrades both generalization and computational efficiency (overfitting and routing chaos); large capsule banks unsuitable for resource-limited deployment.",
            "key_finding_summary": "Dynamic routing in capsule networks offers task-aligned hierarchical representations that increase NER F1 (peak at 32 capsules) but raises computation and memory costs, with too many capsules harming generalization.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical ablation shows adaptive routing (task-aligned allocation) improves performance vs no-capsule/uniform schemes, confirming benefits of dynamic, task-aligned abstraction.",
            "uuid": "e2240.1"
        },
        {
            "name_short": "DCAM",
            "name_full": "Deep Context Feature Attention Module (DCAM)",
            "brief_description": "A position-aware attention module combining adaptive 1D average/max pooling, MLP gating, and 1×3 convolutions to reweight contextual features and improve boundary localization for entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DCAM",
            "model_description": "Processes Bi-LSTM outputs via parallel adaptive AvgPool/MaxPool, MLP gating to produce attention weights, multiplies weights into features, applies 1D conv (kernel size tuned to 3) to extract keyword semantic features and position-sensitive attention.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "adaptive pooling + learned MLP attention weights + position-aware 1D convolutional attention to allocate representational emphasis to boundary-relevant positions",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Feature enhancement for Chinese NER within CEAF; evaluated as CEAF-D ablation",
            "performance_task_aligned": "CEAF-D variant (adds DCAM to baseline) F1s: Toutiao F1=86.6%, Weibo F1=85.0%, MSRA F1=89.4%, OntoNotes (Chinese) F1=89.5% (Table 6).",
            "performance_uniform_baseline": "BERT-BiLSTM-CRF baseline F1s: Toutiao 85.2%, Weibo 84.4%, MSRA 87.8%, OntoNotes 88.3%; DCAM variant shows improvements.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Adds 1D pooling/MLP and small 1D convolution (kernel size 3 optimal). Paper reports kernel-size ablation (3 best) but no numerical cost figures; moderate extra compute vs baseline.",
            "computational_efficiency_baseline": "Baseline lacks DCAM overhead; exact comparative costs not reported.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Inclusion of DCAM improves performance on noisy/social datasets (Weibo/Toutiao) indicating better robustness/generalization to informal contexts; no explicit cross-domain numeric breakdown beyond dataset scores.",
            "interpretability_results": "Position-aware attention maps and visualization indicate DCAM highlights boundary-relevant tokens; qualitative interpretability claimed.",
            "multi_task_performance": null,
            "resource_constrained_results": "Lightweight relative to capsule layer but still adds inference cost; no explicit constrained-device evaluation.",
            "key_finding_summary": "DCAM adaptively reallocates representational emphasis to position-relevant context, improving F1 over the uniform baseline, especially on noisy social media datasets.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "DCAM's input-dependent attention reallocates capacity to task-relevant positions and yields measurable gains vs uniform representations.",
            "uuid": "e2240.2"
        },
        {
            "name_short": "AFFN",
            "name_full": "Adaptive Feature Fusion Network (AFFN)",
            "brief_description": "A gating-based adaptive fusion module that dynamically weights and fuses local spatial features (from capsule routing) and global dependency features (from DCAM) to produce discriminative joint representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AFFN",
            "model_description": "Applies nonlinear transforms (tanh + sigmoid) to two feature streams, computes a gating vector z via sigmoid over combined signals and produces fused vector f = z * h_c + (1 - z) * h_d, enabling input-dependent, multi-granularity fusion.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "gated adaptive fusion (input-dependent weighting) that dynamically assigns representational weight to local vs global features",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Feature fusion for Chinese NER within CEAF; evaluated as CEAF-A ablation",
            "performance_task_aligned": "CEAF-A variant (adds AFFN to baseline) F1s: Toutiao F1=86.2%, Weibo F1=85.4%, MSRA F1=89.0%, OntoNotes F1=89.4% (Table 6).",
            "performance_uniform_baseline": "BERT-BiLSTM-CRF baseline F1s: Toutiao 85.2%, Weibo 84.4%, MSRA 87.8%, OntoNotes 88.3%. AFFN variant shows gains.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Adds lightweight gating computations (elementwise ops and small MLPs); modest extra compute relative to capsule/DCAM components; no exact numeric costs reported.",
            "computational_efficiency_baseline": "Baseline lacks dynamic gating, hence slightly cheaper; numbers not provided.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "AFFN improves aggregated multi-component CEAF performance and helps robustness on nested/blurred-boundary datasets; quantitative cross-domain transfer not isolated to AFFN alone.",
            "interpretability_results": "Gating weights can be examined to understand which modality (local/global) the model emphasizes per instance; qualitative interpretability discussed but not quantified.",
            "multi_task_performance": null,
            "resource_constrained_results": "Lightweight relative to other modules; no explicit constrained-device evaluation.",
            "key_finding_summary": "Dynamic gating fusion of heterogeneous feature streams yields better NER performance than fixed fusion, demonstrating benefits of input-conditioned allocation of representational weight.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "AFFN's adaptive combination of features outperforms fixed-weight or simple concatenation strategies, aligning with task-aligned abstraction principles.",
            "uuid": "e2240.3"
        },
        {
            "name_short": "BERT-BiLSTM-CRF",
            "name_full": "BERT + Bi-LSTM + Conditional Random Field",
            "brief_description": "A common strong baseline in NER that stacks pre-trained BERT contextual embeddings, Bi-LSTM sequence modeling, and CRF decoding to perform sequence labeling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-BiLSTM-CRF",
            "model_description": "Generates contextual token vectors via BERT, refines sequential patterns with Bi-LSTM, and enforces label consistency with a CRF output layer; represents a relatively uniform pipeline without explicit adaptive feature allocation across modules.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "uniform stacked representation pipeline (contextual embeddings + sequential encoder + uniform fusion into CRF)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Chinese Named Entity Recognition (baseline in this paper)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Reported baseline F1s (Table 6): Toutiao F1=85.2%, Weibo F1=84.4%, MSRA F1=87.8%, OntoNotes (Chinese) F1=88.3%.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": "Not reported numerically in paper; described as lower inference/memory cost than CEAF qualitatively.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Used as baseline; CEAF reports lower cross-domain degradation than some BERT-based baselines (e.g., BERT-MRC) but exact numbers per baseline vary.",
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": "Less resource intensive than CEAF; exact metrics not provided.",
            "key_finding_summary": "Serves as a strong uniform-representation baseline; CEAF's task-aligned, adaptive modules outperform this pipeline consistently on noisy/nested datasets.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "BERT-BiLSTM-CRF is a strong baseline but lacks explicit task-aligned adaptive allocation; CEAF's improvements indicate such allocation helps, but BERT-based baselines remain competitive in cleaner domains.",
            "uuid": "e2240.4"
        },
        {
            "name_short": "SpanKL",
            "name_full": "SpanKL (span representation model for continual NER)",
            "brief_description": "A span-based model that builds span representations and treats spans as independent units for multi-label classification, intended to handle nested entities and continual learning.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "SpanKL",
            "model_description": "Uses BERT-large for context encoding and a span representation layer with entity-type-specific feedforward networks to represent and classify all candidate spans; this is a task-aligned span-centric representation instead of token-uniform labeling.",
            "model_size": "BERT-large (implicit in description)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "span-based representations + type-specific feedforward heads (allocates capacity to spans as task units)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Chinese NER (span-based nested entity recognition)",
            "performance_task_aligned": "Reported in Table 7: Toutiao F1=85.3%, Weibo F1=83.8%, MSRA F1=87.4% (as compared in paper's comparative table).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Span enumeration can be computationally costly (all spans treated as units), but exact costs not provided in this paper.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": "Span enumeration has higher complexity and can be slower; not quantified here.",
            "key_finding_summary": "Span-centric, task-aligned representations are effective for nested entities but can be computationally expensive; in this paper CEAF outperforms SpanKL on the social/nested datasets reported.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "SpanKL exemplifies task-aligned representations (span units) that improve nested-entity modeling, consistent with the benefit of non-uniform, task-specific representations.",
            "uuid": "e2240.5"
        },
        {
            "name_short": "HiNER",
            "name_full": "HiNER (hierarchical feature fusion for Chinese NER)",
            "brief_description": "A hierarchical feature-fusion model designed to handle nested and discontinuous entities via local-global attention dual-path and hierarchical fusion.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "HiNER",
            "model_description": "Implements hierarchical feature fusion combining local relationship classification and multi-source semantic fusion with local-global dual-path attention to parse nested/discontinuous entities.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "hierarchical representations with local-global dual-path attention (allocates different processing to local vs global features)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Chinese NER (benchmark comparisons in paper)",
            "performance_task_aligned": "Table 7 comparisons: Toutiao F1=86.6%, Weibo F1=85.1%, MSRA F1=88.1%.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Hierarchical fusion implies added compute vs simple baselines; no explicit metrics reported in this paper.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Hierarchical, task-aligned fusion improves nested-entity recognition relative to simpler baselines; CEAF further improves on these datasets by combining capsules, DCAM, and AFFN.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "HiNER's hierarchical allocation of representational processing aligns with the benefits of task-specific abstractions described in CEAF.",
            "uuid": "e2240.6"
        },
        {
            "name_short": "SoftLexicon",
            "name_full": "SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection",
            "brief_description": "A lexicon-aware approach that dynamically matches dictionary lexical information to character representations via a soft selection mechanism, improving boundary ambiguity handling.",
            "citation_title": "SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection",
            "mention_or_use": "mention",
            "model_name": "SoftLexicon",
            "model_description": "Dynamically fuses lexicon (word-level) information with character-level representations using a soft selection mechanism instead of hard dictionary matching, thereby adaptively incorporating lexical signals.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "lexicon-aware dynamic fusion (soft selection) that adaptively integrates dictionary features with token representations",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Chinese NER (referenced as related work and comparative baseline)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "SoftLexicon exemplifies adaptive lexical fusion (task-aligned signal incorporation) as an alternative to static dictionary injection; paper cites it as a strong lexicon-aware approach but does not re-run its experiments.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "SoftLexicon's dynamic lexicon fusion aligns with the claim that adaptive, task-aware incorporation of features improves NER performance over purely uniform schemes.",
            "uuid": "e2240.7"
        },
        {
            "name_short": "Baichuan2-13B-Chat",
            "name_full": "Baichuan2-13B-Chat (Baichuan2 series large language model)",
            "brief_description": "A 13B-parameter open large language model trained on a large corpus, reported to have strong Chinese/English comprehension and competitive NER performance when used for downstream tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Baichuan2-13B-Chat",
            "model_description": "Large transformer-based generative model pretrained on trillions of tokens, used as a baseline/competitor for NER tasks (in paper described qualitatively as strong in Chinese).",
            "model_size": "13B",
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "large uniform transformer stacks with contextual self-attention (no explicit task-specific adaptive routing reported in paper)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "General language understanding; used as comparative baseline for Chinese NER",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Paper comments that large LLMs show some strengths on normalized datasets (MSRA) but are less effective on nested/unnormalized social media texts due to domain adaptation issues.",
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": "Large model size implies high resource usage; paper notes data-missing issues and domain adaptation difficulty but gives no exact costs.",
            "key_finding_summary": "Large LLMs provide strong general knowledge but may not adapt as effectively to nested / noisy NER without domain adaptation; CEAF outperforms them on several noisy Chinese datasets.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "Large uniform LMs are powerful but not specifically designed to allocate representational capacity per NER task demands; CEAF's task-aligned modules outperform them in targeted NER scenarios.",
            "uuid": "e2240.8"
        },
        {
            "name_short": "Qwen-14B-Chat",
            "name_full": "Qwen-14B-Chat (Qwen series large language model)",
            "brief_description": "A 14B-parameter conversational transformer model supporting long contexts and strong Chinese/English capabilities; referenced as a comparative baseline for NER.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Qwen-14B-Chat",
            "model_description": "Large transformer model pretrained on large corpora with extended context length; used as a general-purpose baseline in paper comparisons.",
            "model_size": "14B",
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "uniform transformer self-attention stacks",
            "is_dynamic_or_adaptive": false,
            "task_domain": "General language understanding; referenced in Chinese NER comparison table",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Large uniform transformer LMs are reported as competitive generalist baselines but do not systematically outperform CEAF on noisy/nested Chinese datasets.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "Paper's comparisons indicate task-specific adaptive architectures (CEAF) can beat very large uniform models in specialized noisy NER settings.",
            "uuid": "e2240.9"
        },
        {
            "name_short": "LEBERT",
            "name_full": "LEBERT (Lexicon-Enhanced BERT)",
            "brief_description": "A BERT variant that deeply injects vocabulary/lexicon information into mid-layers via adapters to improve boundary recognition by augmenting BERT with lexicon signals.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LEBERT",
            "model_description": "Dynamically integrates dictionary/word-level information into BERT via vocabulary adapters and character-word pairs, enabling more task-aligned lexical representations.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "vocabulary adapters and character-word pair sequences injected into BERT mid-layers (adaptive lexicon enrichment)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Chinese NER (vocabulary enhancement scenarios)",
            "performance_task_aligned": "Reported in Table 7: Toutiao F1=87.9%, Weibo F1=86.5%, MSRA F1=89.6% (used as comparative method).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Adapter layers add modest parameters and compute compared to vanilla BERT; no exact numbers reported in this paper.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Mid-layer lexicon adapters provide task-aligned lexical augmentation that improves NER boundary detection over purely uniform BERT encodings in many cases.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "LEBERT demonstrates that injecting targeted lexical adapters (task-aligned augmentations) helps NER, consistent with the benefits of adaptive representations.",
            "uuid": "e2240.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dynamic Routing Between Capsules",
            "rating": 2
        },
        {
            "paper_title": "SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection",
            "rating": 2
        },
        {
            "paper_title": "SpanBERT: Improving pre-training by representing and predicting spans",
            "rating": 2
        },
        {
            "paper_title": "LUKE: Deep Contextualized Entity Representations with Entity-Aware Self-Attention",
            "rating": 2
        },
        {
            "paper_title": "HiNER: hierarchical feature fusion for Chinese Named Entity Recognition",
            "rating": 2
        },
        {
            "paper_title": "A Unified MRC Framework for Named Entity Recognition",
            "rating": 1
        },
        {
            "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "rating": 1
        }
    ],
    "cost": 0.02474425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CEAF: Capsule network enhanced feature fusion architecture for Chinese Named Entity Recognition
October 7, 2025</p>
<p>Siyu Ma 
Guangzhong Liu gzhliu@shmtu.edu.cn 0000-0002-0325-6760
Yangshuyi Xu 0000-0001-7714-0798</p>
<p>NGP Institute of Technology
INDIA</p>
<p>of Information Engineering</p>
<p>CEAF: Capsule network enhanced feature fusion architecture for Chinese Named Entity Recognition
October 7, 2025B791FD0DDB597F9E56D853CC77F4AEB510.1371/journal.pone.0332622Received: April 22, 2025 Accepted: September 2, 2025
Chinese Named Entity Recognition (NER) is a fundamental task in the field of natural language processing, where achieving deep semantic mining of nested entities and accurate disambiguation of character-level boundary ambiguities stands as its core challenge.Existing methods, mostly based on the BiLSTM-CRF sequence labeling framework or Transformer attention mechanisms, have inherent limitations in modeling the hierarchical structural dependencies of nested entities and resolving semantic conflicts in overlapping character spans.To address challenges such as the lack of morphological markers, propagation of boundary ambiguities, and insufficient geometric modeling in the feature space, we propose a novel multi-stage neural architecture-the CEAF model, a specialized neural framework tailored for Chinese NER tasks.The architecture leverages BERT-derived subword embeddings to capture character-level contextual representation and incorporates BiLSTM to model position-sensitive sequential patterns.Meanwhile, to effectively tackle the complex challenges of boundary uncertainty and nested entity composition, the CEAF model innovatively introduces the Deep Context Feature Attention Module (DCAM).This module pioneeringly integrates capsule routing protocols with position-aware attention mechanisms, processing information through dual parallel paths: on one hand, it leverages the powerful spatial relationship modeling capability of capsule networks to clearly parse the hierarchical structure and part-whole relationships between entities; on the other hand, it utilizes position-aware attention to focus on key positional information, dynamically adjust the attention to different positional information, accurately locate entity boundaries, effectively resolve boundary ambiguity, and achieve efficient and accurate modeling of nested entity structures.In addition, the Adaptive Feature Fusion Network (AFFN) effectively bridges the semantic gap between global contextual coherence and local boundary precision by selecting more discriminative fusion features.Generalization experiments on three Chinese benchmark datasets and one English dataset demonstrate that the CEAF model outperforms baseline models.Visualization analysis further verifies the modeling capability of the CEAF model, providing new insights into geometric deep learning approaches for Chinese NER.</p>
<p>Introduction</p>
<p>In the era of social media, Chinese online texts are experiencing exponential growth, with embedded entity names and event feedback providing crucial evidence for effectiveness evaluation [38].However, the hierarchical nested entity structures requiring dynamic routing protocols, non-canonical linguistic phenomena with adversarial noise patterns in user-generated content, and heterogeneous feature fusion challenges in multimodal data streams pose significant challenges to entity recognition [12,[16][17][18].As a basic task in natural language processing, NER can identify key entities such as names of people, organization names, and places from unstructured text.It has important application value in public opinion monitoring, knowledge graph construction, intelligent customer service and other fields, and provides important support for effect analysis and monitoring.</p>
<p>In the named entity recognition task, Chinese and English show significant differences due to differences in language characteristics [3,[6][7][8].From the perspective of boundary determination, Chinese NER relies on attention-based contextual semantic modeling and sliding window enumeration with dynamic programming, while English NER leverages explicit tokenization rules and case-sensitive orthographic features to achieve deterministic segmentation, resulting in lower computational complexity for boundary recognition [9,10].The presence of nested entities further exacerbates this complexity.Nested entities refer to phenomena where one entity contains another entity of the same or different types within text boundaries.For nested entity processing, Chinese requires span-based graph neural networks or hierarchical boundary-aware modules to decode multi-layer nested structures, whereas English entities typically follow flat annotation schemas, with mainstream methods assuming entity independence by default [21,27].This hierarchical structure violates the traditional sequence labeling assumption of "non-overlapping independent entities, " demanding models to not only recognize entity boundaries but also parse inclusion relationships.In domain adaptation, Chinese NER necessitates domain-specific dictionary injection to mitigate pre-trained model knowledge gaps, while English models benefit from parameter-efficient fine-tuning enabled by large-scale corpus priors during cross-domain transfer.The strong reliance of Chinese NER on domain-specific dictionaries results in notable drawbacks compared to English NER.Firstly, its generalization capability is weak.Chinese terms lack consistent standardization and vary significantly across domains; when switching to a new domain without a corresponding dictionary, recognition accuracy drops drastically.In contrast, English NER relies on standardized orthography and semantic reasoning, enabling more flexible cross-domain transfer.Secondly, dictionary maintenance incurs high costs, as new Chinese terms require continuous updates by domain experts, whereas English neologisms can be inferred through word-formation rules, reducing manual dependency.Additionally, it is prone to data sparsity issues, with low-frequency terms struggling to acquire semantic features.Over-reliance on dictionaries also leads to mechanical matching that ignores contextual semantics, making it more challenging for Chinese NER to achieve recognition accuracy than English NER.</p>
<p>Take the sentence in Fig 1 above as an example.These two sentences are very similar in meaning, but the difficulty of recognition differs significantly due to inherent linguistic divergences in entity composition rules.When recognizing Chinese named entities, the system must simultaneously resolve span overlaps in nested entity structures-for instance, identifying "Peking University" as an independent institution while recognizing "Peking University School of Life Sciences" as a nested institution entity containing compositional semantics.This structural dependency relies on hierarchical semantic parsing of lexical collocations between "university" and "school." Additionally, the absence of word boundary delimiters in Chinese necessitates context-aware tokenization for character sequences like "Beijing, " "university, " "biology, " and "school, " requiring boundary disambiguation through bidirectional semantic constraints.In contrast, the English example "Harvard Medical School holds a conference" benefits from explicit morphological markers: Spaces naturally segment the institution name "Harvard Medical School" as a flat-structured entity without requiring nested resolution.Orthographic features like capitalization and whitespace substantially reduce boundary detection complexity.</p>
<p>In the field of Chinese named entity recognition, researchers have proposed various solutions addressing distinct technical challenges. 1) For handling blurred entity boundaries due to Chinese's separator-free nature, a radical-enhanced BiLSTM-CRF architecture leverages morphological decomposition of Han characters through bidirectional LSTM networks, while CRF-based label transition constraints optimize boundary detection in structured texts [9,10].</p>
<p>2) To address nested entity recognition, the SAT-CNER model implements a dynamic span enumeration strategy with sliding window operators, enhanced by adversarial training with gradient perturbation to improve structural robustness [11].3) To address the challenge of domain-specific dictionary injection, the dynamic dictionary fusion framework converts domain terms into learnable vector representations.It dynamically adjusts the weight allocation between dictionary features and contextual semantics through an attention mechanism, effectively avoiding context misjudgment caused by mechanical matching.4) The BERT-BiLSTM-CRF framework combines contextualized embeddings from transformer layers with sequential pattern learning via gated recurrent units, forming a hierarchical representation learning framework constrained by CRF-based state transitions [13,14].5) For vertical domain adaptation, a pyramidal CNN architecture hierarchically fuses multi-scale BERT representations through shallow convolutional filters for local n-gram patterns and deep dilated convolutions for global context integration, complemented by structured pruning for parameter sparsification and 8-bit quantization to reduce deployment overhead [15].However, existing methods still have multi-dimensional limitations.1) Although the radical-based model performs robustly in regular texts, it lacks the ability to parse the semantics of phonetic abbreviations and new words on social media, and it is difficult to capture the language characteristics of non-standard expressions [19,20].2) Although the dynamic sliding window mechanism improves the recognition rate of nested entities, it is inefficient in capturing entities that jump across long distances across sentences, and the window enumeration causes the computational complexity to grow exponentially [22,23].3) Although the deep integration of pre-trained models and CRF strengthens semantic modeling, the huge number of parameters of BERT causes a surge in training energy consumption [26].4) Domain-specific dictionary injection still has notable limitations in Chinese Named Entity Recognition (NER).Dictionaries struggle to fully cover domain-specific terms, often failing to include emerging vocabulary, dialects, and other special expressions.Additionally, delayed updates mean new terms cannot be promptly incorporated.When processing cross-domain texts, integrating different dictionaries easily leads to term conflicts and misjudgments.Furthermore, over-reliance on dictionary matching ignores contextual semantics, resulting in inaccurate entity judgments in complex contexts and undermining recognition performance.5)Verticaldomain models are limited by the scarcity of professional annotated data, and the entity recall rate is generally less than 40%, with significant performance degradation during domain migration due to the semantic gap [24,25].6) Although lightweight technology improves deployment efficiency, its dependence on large-scale annotated data is fundamentally inconsistent with the needs of low-resource scenarios, making it difficult to support application expansion in subdivided fields such as dialects and small languages [28].The deeper problem is that Chinese NER has not yet established a unified annotation standard, with differences in entity type systems across domains limiting the model's generalization ability, while cutting-edge directions such as multimodal knowledge fusion and unsupervised pre-training still need breakthrough progress.</p>
<p>In social media, user-generated content is massive and unstructured, containing critical information that urgently requires extraction-this underscores the importance of Chinese Named Entity Recognition (NER).It supports public opinion monitoring to capture public attitudes and enables the construction of social network graphs for analyzing social relationships.However, significant challenges persist: non-standard user expressions with numerous abbreviations and errors, the lack of distinct word boundaries in Chinese, and the continuous emergence of new vocabulary and buzzwords, all posing immense difficulties for recognition tasks.</p>
<p>To address these issues and limitations, particularly in processing nested entities and entities within informal texts, we innovatively propose the CEAF model.Its core innovation lies in constructing a dynamic collaborative semantic parsing framework for Chinese NER, breaking free from the constraints of traditional models that rely on simple module stacking or single-dimensional feature extraction.The model's key breakthrough lies in resolving the critical contradictions in Chinese NER: through the design of a "structure-semantics" dual-dimensional dynamic collaboration mechanism, it achieves deep coupling between finegrained internal entity features and global contextual semantics.Instead of treating entity structure modeling and contextual semantic capture as isolated processes, it establishes a mutual feedback relationship-enabling the accurate extraction of entity morphological features to dynamically respond to changes in contextual semantics, while contextual semantic capture reciprocally enhances the refined parsing of internal entity structures.More importantly, the model innovatively incorporates an adaptive semantic-aware fusion mechanism, eliminating the rigidity of traditional fixed-weight fusion.It dynamically adjusts feature aggregation strategies based on text semantic complexity, allowing precise alignment between local entity features and global contextual information in complex scenarios.This collaborative framework fundamentally enhances the model's ability to tackle core challenges such as boundary ambiguity caused by Chinese's lack of explicit separators, complex hierarchical relationships in nested entities, and variable semantic ambiguity scenarios, achieving an essential leap from "module concatenation" to "mechanism collaboration".The main contributions of this paper are summarized as follows:</p>
<p>1) This paper innovatively proposes the CEAF model architecture, a dynamically collaborative feature parsing system designed to address nested and irregular entity recognition in unstructured social media texts.Breaking free from traditional module stacking limitations, it organically decouples semantics of BERT, the sequence pattern of Bi-LSTM, and the geometric representation of the capsule network.Via the AFFN module, it enables dynamic multi-dimensional feature aggregation, adapting to non-standard expressions and emerging vocabulary to significantly boost entity recognition accuracy.</p>
<p>2) We propose the DCAM contextual feature attention module, specifically designed to resolve semantic ambiguity in Chinese NER within informal texts.The module uses adaptive pooling layers in parallel to capture global statistical features, combines 1D convolution to extract local context associations, and implements feature adaptive calibration through gated units.The multi-granularity fusion architecture dynamically aggregates global and local features to generate position-sensitive context representations.</p>
<p>3) We propose a feature enhancement network (AFFN) based on an adaptive feature fusion mechanism.This module breaks through the limitations of traditional fixed-pattern fusion.By constructing a dynamic feature interaction mechanism, it deeply explores the potential semantic correlations among multi-source heterogeneous features and can adaptively adjust fusion strategies based on text semantic complexity.This design enables the intelligent fusion of cross-level features, effectively enhancing the model's ability to recognize nested entities and entities with blurred boundaries, and providing key support for solving the core challenges in Chinese named entity recognition.</p>
<p>The structure of this paper is as follows: Sect 2 is Related work, which mainly reviews the research progress in the field of named entity recognition.Sect 3 introduces the model structure proposed in this paper in detail.Sect 4 is the experiment and result analysis to prove the effectiveness and interpretability of the model.Sect 5 is the conclusion part, which comprehensively reviews the research work of this paper.</p>
<p>Related works</p>
<p>Chinese Named Entity Recognition</p>
<p>The development of Chinese named entity recognition methods has gone through a process from traditional rule-driven to deep learning-led.Early named entity recognition mainly relied on rule templates and statistical models, such as rule-based methods based on dictionary matching and statistical methods like hidden Markov models (HMM) [29,31].These methods perform well in structured texts, but they have poor generalization ability when facing informal texts on social networks and are sensitive to data noise.With technological breakthroughs, in 2014, Tianchuan Du et al. proposed a Chinese NER model based on deep learning, using convolutional neural networks (CNNs) to extract features, which marked the beginning of deep learning being applied to Chinese NER tasks.Ashish Vaswani et al. (2017) proposed the Transformer model [41], a model based on the self-attention mechanism, which was subsequently widely used in NER tasks.Chinese NER methods based on deep learning have become the mainstream.This method mainly uses neural network models to extract text features and predict entity boundaries and types [32,33].Convolutional neural networks extract local features through convolutional layers and pooling layers, and are suitable for capturing local patterns in text, while recurrent neural networks, especially long short-term memory networks and gated recurrent units, can process sequence data and pass previous text information through hidden states, making them suitable for processing variable-length text.Despite significant advancements in Chinese NER driven by deep learning technologies, this task still faces pressing challenges today, and continuously improving its accuracy holds crucial practical significance-many key application scenarios heavily rely on the precision of entity recognition.For instance, in public opinion monitoring, failure to accurately identify core entities such as people, organizations, and events in texts will lead to deviations in public attitude analysis, affecting the timeliness and relevance of decision-making responses.In intelligent customer service and dialogue systems, omissions in entity recognition may cause misinterpretation of user intentions, directly reducing service quality and user experience.In the field of financial risk control, misjudgment of entities such as enterprise names and transaction subjects could even pose potential risk hazards.The inherent lack of explicit word boundary markers in Chinese makes accurate entity boundary segmentation inherently challenging.Particularly in informal scenarios like social media, users often employ mixed expressions involving homophones, abbreviations, English phrases, or emojis, further exacerbating text irregularity.Meanwhile, the hierarchical relationships of nested entities are complex and variable; a single text segment may contain multiple nested or overlapping entity types, making it difficult for traditional models to effectively capture such multi-dimensional semantic associations.Additionally, the continuous emergence of new domain terminology and internet buzzwords outpaces the iteration speed of models, resulting in widespread inadequacy in recognizing out-of-vocabulary words.Moreover, the scarcity of annotated data and domain differences make models prone to performance degradation during cross-scenario migration.The combination of these issues means that Chinese NER still struggles to meet the demands for high precision and strong robustness in practical applications, remaining a critical subject to be addressed in the field of natural language processing.Improvements in accuracy directly impact the reliability of downstream tasks and the realization of application value.</p>
<p>BERT module</p>
<p>The BERT (Bidirectional Encoder Representations from Transformers) model [40], proposed by Google in 2018, is a pre-trained language model that significantly improves the accuracy of Named Entity Recognition (NER) through pre-training and fine-tuning.It employs a bidirectional Transformer encoder to achieve deep semantic understanding of text.The core innovation of BERT lies in its large-scale unsupervised text learning of bidirectional contextual semantic representations.During pre-training, BERT utilizes two key tasks: 1. Masked Language Model (MLM): Randomly masking portions of input tokens and predicting them based on contextual clues, forcing the model to learn bidirectional contextual dependencies; 2. Next Sentence Prediction (NSP): Determining whether two sentences are consecutive text segments to enhance the model's comprehension of inter-sentence logical relationships.In NER tasks, BERT's application addresses traditional methods' heavy reliance on annotated data while demonstrating remarkable improvements in recognizing complex contextual patterns and novel entities.The model's architecture -featuring multi-layer Transformer encoders with token/segment/position embeddings-enables it to capture nuanced semantic relationships through self-attention mechanisms and positional encoding.</p>
<p>Capsule Network</p>
<p>Capsule Networks (CapsNets) [42], proposed by Geoffrey Hinton's team in 2017, address the limitations of conventional convolutional neural networks (CNNs) in image processing through vectorized "capsules" that replace scalar-valued neurons.Each capsule's output vector encodes both the existence probability of an entity and its spatial attributes, enabling hierarchical modeling of complex object structures.The dynamic routing mechanism, as the core algorithm, coordinates information flow between lower-level and higher-level capsules by iteratively optimizing coupling coefficients through semantic compatibility evaluation [36].In entity recognition tasks, lower-level capsules capture local lexical features whose vector outputs indicate potential entity membership while encoding semantic roles through directional components.Higher-level capsules integrate these features via dynamic routing, analyzing inter-word logical consistency to determine precise entity boundaries and types.This mechanism calculates semantic matching degrees between capsule layers through iterative weight adjustments, prioritizing coherent feature combinations while suppressing noise and ambiguous patterns.The squash function nonlinearly compresses vector magnitudes while preserving directional semantics, effectively modeling complex linguistic phenomena.Cap-sNets demonstrate particular efficacy in low-resource/noisy-data scenarios, such as identifying informally expressed entities in social media texts or adapting to new domains with minimal annotated samples.Despite computational complexity from high-dimensional transformation matrices, they offer an innovative NLP paradigm combining hierarchical semantic modeling with dynamic pathway optimization, balancing local feature extraction and global structural reasoning [37].</p>
<p>Attention mechanism</p>
<p>The attention mechanism is a computational paradigm inspired by biological selective attention processes [43], designed to enhance neural networks' focus on task-critical features through dynamic weight allocation.Originating from bionics research on human visual perception systems, this mechanism was first formalized for neural machine translation (NMT) by Bahdanau et al., effectively addressing the long-range dependency limitations and encoder capacity constraints inherent in conventional recurrent neural networks (RNNs).Its operational principle involves computing inter-feature relevance scores to guide computational resources toward discriminative regions within input data [34,35].</p>
<p>The mechanism's technical innovation manifests in two key aspects: dynamic adaptability and architectural transparency.Unlike static parameter matrices in traditional convolutional/fully-connected layers, attention weights are dynamically adapted based on input context, enabling flexible modeling of intricate semantic relationships.In Transformer architectures, sequence-level attention captures extended contextual dependencies through all-pair positional relevance computations, while multi-head attention leverages parallel projection subspaces to extract complementary feature representations.This dual mechanism achieves simultaneous preservation of sequential relationships and parallel computation efficiency, fundamentally overcoming RNNs' sequential processing constraints.</p>
<p>Feature fusion</p>
<p>As a pivotal technique for enhancing model representational capacity, feature fusion aims to construct more discriminative joint feature representations by integrating multi-source features across hierarchical levels, modalities, or scales.Its core objective addresses the informational constraints inherent in single-feature-space representations.Taking cross-lingual entity linking as a case study, this approach combines word-level cross-lingual embedding vectors with syntactic dependency tree structural features, while employing attention mechanisms to dynamically adjust interlingual alignment feature weights.Such implementation demonstrates a 19% improvement in entity disambiguation accuracy under low-resource language scenarios compared to baseline methods.Traditional methodologies primarily focus on early fusion and late fusion.Feature fusion enhancement, as a core deep learning optimization technology, fundamentally overcomes single-feature-space representation bottlenecks through multi-dimensional, multi-level feature interactions.It constructs joint feature spaces with enhanced semantic expressiveness via geometric constraints, dynamic weight adaptation, and cross-modal association strategies.This technical paradigm achieves progressive enhancement from low-level local features to high-level semantic abstractions, thereby improving model robustness and generalization in complex scenarios.A notable implementation is the dynamic Feature Pyramid Network (FPN), which optimizes scale adaptability in object detection through top-down fusion of high-level semantic features with low-level spatial details.</p>
<p>Model</p>
<p>Overall model architecture</p>
<p>The CEAF (Capsule Network Enhanced Feature Fusion Architecture) model we proposed is shown in Fig 2.</p>
<p>The CEAF model designs a multi-level collaborative architecture to address the core difficulties of Chinese NER, and improves entity boundary judgment and semantic parsing capabilities through hierarchical feature extraction and dynamic fusion mechanisms.The model first uses the BERT pre-trained language model to generate character-level dynamic semantic representations, and uses the Transformer's bidirectional attention mechanism to capture long-distance contextual dependencies, effectively solving the ambiguity problem of Chinese polysemy.Subsequently, the sequence is modeled using the Bi-LSTM network, and its gating mechanism can accurately capture boundary features such as entity prefixes and suffixes.The feature enhancement layer innovatively constructs a parallel path between the capsule network and the dynamic contextual attention DCAM module: the capsule network establishes a hierarchical association between character components and the overall entity through an iterative routing algorithm to enhance the recognition ability of nested entities; the DCAM recalibrates the contextual features based on position-sensitive attention weights to highlight the semantic contribution of the core components of the entity.The final adaptive feature fusion network (AFFN) dynamically integrates the global semantics of BERT, the sequence regularity of BiLSTM, and the structural features of the enhancement layer through a gating mechanism to achieve optimal weighting of multi-granular information.In the decoding stage, the CRF layer is used to constrain the label transfer path, and the Viterbi algorithm is used to decode the entity sequence that conforms to the grammatical rules, which significantly improves the rationality of the transfer probability between labels.This section will explain our model from three aspects: Sect 3.2 Feature extraction, Sect 3.3 Feature enhancement and feature fusion, and Sect 3.4 Entity recognition.</p>
<p>Feature extraction and representation learning</p>
<p>BERT layer.</p>
<p>In this paper, the BERT layer, as the underlying feature encoder, undertakes the core task of extracting deep semantic representations from the original text.Its workflow can be summarized as three stages: embedding mapping, context encoding, and feature transfer.Through the multi-level abstraction capabilities of the pre-trained language model, it provides dynamic feature expressions rich in grammatical structure and semantic relations for the upper-layer Bi-LSTM, feature enhancement module, and CRF.The core innovation of BERT lies in its bidirectional encoding mechanism.This bidirectional training method enables BERT to capture the complete semantic information of words in a sentence, thereby significantly improving the model's ability to understand complex contexts.By stacking multiple layers of Transformer encoders, BERT extracts deep semantic features of the text layer by layer, where the self-attention mechanism allows the model to dynamically capture the strength of association between words in different positions, as shown in Eq (1).
Attention = softmax ( QK T √ d h ) V (1)
The  features through non-linear transformations.With residual connections alleviating the gradient vanishing problem and layer normalization stabilizing the input distribution, features continuously deepen semantic abstraction during layer-by-layer transmission-evolving from basic morphological features to high-level semantic features.Finally, they are output as context vectors of each subword and passed to the Bi-LSTM layer, laying a solid foundation for sequence modeling.</p>
<p>At the base layer, the pre-trained BERT embeddings dynamically encode character-level semantics and global contextual dependencies.This layer solves the polysemy problem by generating context-sensitive representations for each Chinese character, using a bidirectional Transformer mechanism to capture long-range dependencies.The output is a sequence of context vectors H BERT = {h 1 , h 2 , ⋯, h n }, where h i ∈ ℝ d BERT corresponds to the i-th character in the input text.Convert tokens to input_ids through the vocabulary, and map input_ids to a vector  token ∈ ℝ n×d , where d=768 is the hidden layer dimension. pos ∈ ℝ n×d represents the position information of the encoded token. seg ∈ ℝ n×d is used to distinguish sentences A and B. Add the three together to get E ∈ ℝ n×d as the final input embedding matrix, as shown in Eq (2).
E = E token + E pos + E seg (2)
3.2.2Bi-LSTM layer.Bi-LSTM(Bidirectional Long Short-Term Memory) is a deep learning model widely used in sequence labeling tasks.Its core idea is to combine two independent LSTM layers to process the input sequence from the forward and reverse directions.The forward LSTM layer processes the sequence in chronological order to capture the dependency between the current moment and the previous moment; while the reverse LSTM layer processes the sequence in reverse chronological order to capture the dependency between the current moment and the next moment.At each time step, the outputs of the forward and reverse LSTM layers are fused by vector concatenation to generate a feature representation containing complete context information.This bidirectional structure is shown in Fig 4 below.</p>
<p>In this paper, Bi-LSTM is used as the middle layer of the model, receiving the contextrelated feature representation output by the BERT layer, and is responsible for further modeling the sequential dependencies of the text and enhancing the model's ability to understand long-distance contexts.Leveraging its unique bidirectional sequential modeling capability, Bi-LSTM not only can more precisely capture and model the hidden sequential dependencies in the text but also effectively enhances the model's ability to understand semantic associations across long-distance contexts.This provides the subsequent feature enhancement layer and decoding layer with feature support that is more logically sequential and contextually complete, thereby further improving the overall model's accuracy in recognizing entity boundaries and semantic relationships.</p>
<p>The internal structure of LSTM is shown in Fig 5 .It includes four gating mechanisms: forget gate, input gate, update gate, and output gate.The forget gate controls the information to be forgotten in the cell state, the input gate determines the storage of new information, the update gate updates the long-term memory unit, and the output gate controls the output of the current hidden state.As shown in formulas Eqs (3)-( 8):
f t = 𝛾(W f ⋅ [h t-1 , x t ] + b f )(3)i t = 𝛾(W i ⋅ [h t-1 , x t ] + b i ) (4) C t = tanh(W C ⋅ [h t-1 , x t ] + b C )(5)C t = f t ⊙ C t-1 + i t ⊙ C t (6)o t = 𝛾(W 0 ⋅ [h t-1 , x t ] + b o )(7)h t = o t ⊙ tanh(C t ) (8)
Where r is the sigmoid function, W f ∈ ℝ h×(h+d) , b f ∈ ℝ h , ⊙ represents element-by-element multiplication, and W * , b * represent the weight matrix and bias term.</p>
<p>In the feature extraction task, Bi-LSTM can use context information to identify nested entities and new entities.The combination of Bi-LSTM and BERT can effectively extract features and pass them to the next layer of structure.</p>
<p>Feature enhancement and feature fusion</p>
<p>In Chinese NER, the recognition of nested entities requires the model to capture multiple levels of semantic information at the same time.The noise and informality in Chinese text also increase the difficulty of entity recognition.Traditional models often have difficulty in effectively processing such complex structures.In order to address this limitation, we introduced two parallel subcomponents in the feature enhancement module: capsule network and DCAM module, and used AFFN module to adaptively fuse the enhanced features.When the Bi-LSTM layer transfers the extracted context features to this layer, the two modules use different technologies to enhance the features of effective information.Finally, the features extracted by the two modules are passed to the AFFN module for adaptive fusion to enhance the recognition of nested entities and irregular entities.We will explain it in detail below.</p>
<p>3.3.1 Capsule network.The core idea of capsule network is to represent different attributes of objects through capsules and model the relationship between features through dynamic routing mechanism.The basic unit of capsule network is capsule.The output of each capsule is a vector, whose direction represents the feature attribute and length represents the probability of the feature existence.Different from the traditional scalar output, the vectorized representation of capsule can describe the features more richly and enhance the expressive power of the model.</p>
<p>In the CEAF model, the capsule network receives the context-related feature representation output by the Bi-LSTM layer and captures the spatial relationship between features through a dynamic routing mechanism.The dynamic routing mechanism determines the connection weights between low-level capsules and high-level capsules through iterative calculations, thereby modeling the spatial relationship of features.</p>
<p>The dynamic routing mechanism is the core innovation that distinguishes the capsule network from the traditional neural network.Its existence or not directly determines the model's spatial relationship modeling capability, feature transfer efficiency, and task generalization performance.In a capsule network with dynamic routing, the low-level capsule iteratively adjusts the coupling coefficient to transfer the input vector to the high-level capsule that is "consistent" with its spatial attributes.This routing process simulates the bottom-up and topdown information integration mechanism in the biological neural system.If the dynamic routing is removed, the capsule network will degenerate into a static connection structure.At this time, the output of the low-level capsule is directly transferred to the high-level capsule with preset weights, similar to the fully connected layer of the traditional neural network.This static routing cannot dynamically adjust the information flow according to the input features, resulting in the model losing its equivariance to spatial transformations.The dynamic routing mechanism is shown in Fig 6 below.</p>
<p>Dynamic routing maps low-level capsules to high-level capsules by iteratively updating the coupling coefficient c ij .1) Initialize the coupling coefficient b i = 0, c ij = softmax(b ij ).2) Generate the prediction vector as shown in Eq (9). 3) The high-level capsule input s j obtained by weighted summation of the prediction vector, and the modulus length is limited by the compression function.As shown in Eqs (10) and (11).4) Update the similarity b ij between the high-level capsule output and the prediction vector, as shown in Eq (12).Where W i,j ∈ ℝ n×n encodes the spatial relationship, the modulus of V j represents the probability of entity existence, and the direction encodes the attribute.
1 û j|i = W ij p i (9)s j = ∑ i c ij û j|i (10)v j = squash(s j ) = ∥ s j ∥ 2 1+ ∥ s j ∥ 2 ⋅ s j ∥ s j ∥ (11) b ij ← b ij + v j ⋅ ûj|i(12)
The capsule network achieves feature enhancement by implementing multi-level semantic fusion through a dynamic routing mechanism.Character capsules encode basic semantics through bidirectional LSTM, vocabulary capsules integrate external dictionary information, and syntactic capsules capture long-distance constraints in dependencies.Dynamic routing suppresses the contribution of noise features through Leaky-SoftMax, and prioritizes local features with consistent spatial relationships to aggregate into high-level entity capsules.In the DCAM module, the key features T in the contextual features captured by the Bi -LSTM network is first learned and screened through one-dimensional adaptive maximum pooling and one-dimensional adaptive average pooling to obtain adaptive pooling attention features T p .The process is shown in Eq (13): Where (⋅) is the sigmoid nonlinear activation function; MLP represents a multi-layer perceptron; W 0 and W 1 are weight matrices used to perform linear transformation on the input in MLP.
T p = 𝛾(MLP(AvgPool 1 D(T)) + MLP(MaxPool 1 D(T))) = 𝛾 (W 1 (W 0 (T p a𝜈g )) + W 1 (W 0 (T p max )))(13)
Multiplying the text input feature T and the adaptive pooling attention feature T p gives the weighted pooling attention feature T m , as shown in Eq (14).
T m = T * T p (14)
By performing a one-dimensional convolution operation on the weighted pooled attention feature T m , the keyword semantic features in the context key features can be fully captured to obtain the convolutional attention feature T c , as shown in Eq (15).
T c = 𝛾 (f 1×3 ([AvgPool 1 D(T m ); MaxPool 1 D(T m )])) = 𝛾 (f 1×3 ([T c a𝜈g ; T c max ]))(15)
Among them, (⋅) represents the sigmoid nonlinear activation function; f 1×3 represents the one-dimensional convolution operation using a convolution kernel of size 1×3.</p>
<p>After obtaining the convolutional attention feature T c , it is multiplied by the weighted pooling attention feature T m , and the output feature representation T o of the DCAM module is obtained, as shown in Eq (16).
T o = T m * T c (16)
Compared with domain-specific dictionary injection methods, the DCAM module demonstrates significant advantages in adaptability and flexibility.Traditional domainspecific dictionary injection often relies on manually pre-constructed domain vocabulary lists, strengthening known domain entity features through static matching or weight assignment.However, this approach is limited by the coverage and update speed of the dictionary.When facing newly emerging words, variant expressions in the domain, or cross-domain mixed texts, it is highly prone to missing feature capture due to unrecorded vocabulary.In contrast, through adaptive pooling attention mechanisms and dynamic convolutional feature enhancement, the DCAM module can automatically learn the weight distribution of key features entirely based on the contextual semantics of the input text, without relying on predefined domain vocabulary sets.It can not only accurately focus on core semantic components in the text that are highly relevant to the entity recognition task but also flexibly adapt to the language styles and expression habits of texts in different domains.This effectively avoids the problem of insufficient generalization caused by the static nature of vocabulary in dictionary injection methods, ultimately achieving dynamic and fine-grained capture of key features and contextual association information in complex Chinese texts, and providing more targeted feature support for subsequent entity recognition tasks.By using the DCAM module, the Chinese text context features extracted by the Bi -LSTM network layer can be effectively extracted at a deeper and finer level, thereby effectively capturing the key features and key context feature information.DCAM module, there may still be some redundant noise and cross-modal semantic biases in this feature information.We need to perform multi-granularity gating screening on it through the AFFN module to obtain more discriminative cross-level fused features for the model to make the final prediction.Inspired by the research of Y Xu et al. [1], we propose an innovative adaptive multi-modal feature fusion module, AFFN.This module can significantly enhance the semantic expression ability of the feature subspace and achieve in-depth adaptive fusion of features from different modalities.</p>
<p>The AFFN module conducts adaptive feature fusion calculations on two types of modal features: the text local spatial feature representation C and the global dependency feature representation D extracted by DCAM with feature attention.As a result, the fused feature representation f is obtained.The calculation process is as follows:
h c = tanh(C) (17)h d = sigmoid(𝔻) (18) h = (h c * h c ) + (h d * h d )(19)z = sigmoid(h) (20)z c = z * h c (21)z d = (1 -z) * h d (22) f = z c + z d(23)
In the above calculation process, first, according to Eqs ( 17), (18), and ( 20), in -depth and detailed information filtering is performed on the text feature C, the global dependency feature D extracted by DCAM, and their weighted -sum feature h respectively.Subsequently, by using Formulas Eqs ( 21)-( 23), the input volume of feature information can be dynamically adjusted, thereby achieving the efficient integration of text features and self-attention features.As an innovative adaptive multi-modal feature fusion network, the AFFN module plays a crucial role in integrating and optimizing key features within the model architecture.It targets the local spatial feature representations encoded by the dynamic routing of the capsule network and the global dependency feature representations extracted by the DCAM module.Through a multi-granularity gating screening mechanism, it first performs in-depth and detailed information filtering on these two types of features as well as their weighted-sum features, effectively eliminating redundant noise and cross-modal semantic biases.Subsequently, by dynamically adjusting the input volume of feature information, it achieves efficient fusion of text features and self-attention features.For the highly challenging tasks of recognizing nested entities and informal entities in Chinese named entity recognition, the deep fusion capability of the AFFN module is particularly critical: it can accurately capture the entity boundary details in local spatial features and the contextual semantic associations in global dependency features, thereby effectively distinguishing the hierarchical relationships between nested entities and avoiding boundary misjudgments caused by feature confusion.Meanwhile, for issues such as semantic ambiguity and irregular formatting in informal expressions, its dynamic screening and fusion mechanism can strengthen the weights of core semantic features, weaken the interference of irrelevant noise, and enhance the model's stability in semantic understanding and recognition of such non-standard entities, providing more precise cross-level feature support for the final entity recognition.This design innovatively breaks through the limitations of simple concatenation or weighting in traditional feature fusion.Relying on a deep adaptive fusion mechanism, it significantly enhances the semantic expression ability of the feature subspace, making the fused feature representations more discriminative, as they not only retain the detailed information of local spatial features but also integrate the contextual associations of global dependency features.</p>
<p>Entity recognition</p>
<p>CRF (Conditional Random Field), a probabilistic graphical model widely applied in sequence labeling tasks, can effectively model the global dependency relationships of label sequences.The core idea of CRF is to model the conditional probability distribution between the input sequence and the label sequence by defining feature functions and a transition matrix.In this experiment, CRF models the global dependencies of label sequences and handles nested entities and long entities, ensuring the consistency of the label sequence through global optimization.The conditional probability of the CRF model is calculated via the weighted sum of all feature functions.Given an input sequence x and a label sequence y of length n, the CRF model defines a conditional probability P (Y | X), which is obtained by normalizing the sum of exponential functions over all possible label sequences, as shown in Eq (24):
P(Y|X) = 1 Z(X) exp ⎛ ⎝ T ∑ t=1 ∑ k 𝜆 k f k (y t-1 , y t , X, t)) (24)
where Z (X) serves as the partition function (normalization factor), ensuring the probability distribution sums to unity, as specified in Eq (25).
Z(X) = ∑ Y ′ exp ⎛ ⎝ T ∑ t=1 ∑ k 𝜆 k f k (y ′ t-1 , y ′ t , X, t) ⎞ ⎠(25)
Feature function f k (y t-1 , y t , X, t) characterizes the relationship between input sequence X, timestep t, and label transitions between y t-1 and y t .k is the weight of the feature function f k , which is learned from the training data.T is the length of the sequence.</p>
<p>In the model we proposed, the CRF is the last layer responsible for global optimization of the label sequence to ensure the rationality and consistency of entity recognition.The CRF receives the multi-level feature representation output by the capsule network and solves the optimal label sequence through the Viterbi algorithm.The Viterbi algorithm uses dynamic programming to find the label sequence that maximizes the global probability, thereby ensuring the rationality of entity recognition.</p>
<p>Experiments and analysis</p>
<p>To demonstrate the effectiveness of the CEAF model, this section is divided into four sections.In Sect 4.1, we introduce the evaluation metrics and hyperparameter settings used in this experiment; in Sect 4.2, we introduce the dataset used in this experiment; in Sect 4.</p>
<p>Evaluation metrics and hyperparameter settings</p>
<p>In this experiment, we adopt accuracy, precision, recall, and F1 score as evaluation metrics to comprehensively assess the model's performance in the Chinese NER task.Precision represents the proportion of true positive samples among all samples predicted as positive by the model; recall represents the proportion of true positive samples correctly predicted by the model among all actual positive samples; the F1 score is the harmonic mean of precision and recall, which can comprehensively reflect the model's performance; accuracy evaluates the model's overall performance across all samples.These metrics comprehensively reflect the model's ability to identify entity boundaries and types, as well as handle non-standard texts.The formulas are shown in Eqs ( 26)- (29).
Accuracy = L_correct L_true × 100% (26)Precision = E -correct E -all × 100% (27) Recall = E_correct E_true × 100% (28)F1 = 2 × Precision × Recall Precision + Recall × 100% (29)
Where E_correct is the number of correctly predicted entities, E_true is the total number of predicted entities, E_all is the total number of entities, L_correct is the number of correctly predicted labels, and L_true is the total number of predicted labels.</p>
<p>Following the recommendations of Reference [1,38,39], we set the hyperparameters as shown in Table 1.The maximum sequence length is set to 128 to adapt to the characteristics of social media texts, the learning rate is set to 2e-5 and a linear decay strategy is adopted, and the dropout rate is kept at 0.1 to prevent overfitting.The training process uses mixed precision acceleration technology, the batch size is set to 16, the optimizer is Adam.All experiments were completed on a workstation equipped with an NVIDIA 4060Ti graphics card and implemented based on the PyTorch framework.To ensure the reliability of the results, each experimental configuration was repeated 3 times to take the average indicator.</p>
<p>Datasets</p>
<p>To comprehensively evaluate the performance of the CEAF model, this study selected three representative Chinese named entity recognition datasets and one typical English dataset: Toutiao, Weibo, MSRA, and Onto Notes 5.0.The results of entity recognition tasks were compared on the three Chinese NER datasets to verify the performance advantage of the CEAF model in the Chinese NER task.At the same time, a generalization experiment of the DCBA model was conducted on the English dataset to further prove the effectiveness and interpretability of the model.1) Toutiao dataset: one of the widely used benchmark datasets in the field of natural language processing.Its core value lies in providing large-scale, multi-category Chinese short text samples, which are suitable for tasks such as text classification, entity recognition, and keyword extraction.This dataset is collected from news information on the Toutiao client and stored in a structured format.Each piece of data contains news ID, classification code, classification name, news title, and keywords.</p>
<p>2) Weibo dataset: This dataset contains user-generated content from the Sina Weibo platform, annotated with entity types such as names of people, places, and institutions.Its text has typical social media characteristics, including Internet slang, colloquial expressions, and non-standard grammatical structures, which can effectively test the model's ability to process non-standard text.</p>
<p>3) MSRA dataset: as a benchmark dataset in the field of Chinese named entity recognition, it contains standard text in the news field.The introduction of this dataset helps to evaluate the generalization performance of the model on standard text; 4) OntoNotes 5.0 dataset: a multi-domain and multi-task corpus jointly constructed by the University of Pennsylvania, the Information Science Institute of the University of Southern California, and other institutions.Its core value lies in providing a unified semantic annotation framework across text types.The dataset defines 18 categories of fine-grained entities, including time, cardinality, events, legal terms, etc.The diversity of the four datasets covers texts from different fields and styles, which can fully verify the robustness and adaptability of CEAF under Chinese texts and its transferability under English texts.The datasets are shown in Table 2:</p>
<p>Ablation experiment</p>
<p>Impact of the number of capsules in capsule networks on model performance.</p>
<p>In this experiment, we use the Toutiao dataset as the research object, and observe the model's ability to express Chinese nested entities and multi-type entities by dynamically adjusting the number of high-level capsules.If the number of capsules is insufficient, the model may be forced to compress different semantic entities into the same capsule, while too many capsules may cause routing dispersion and reduce the confidence in the judgment of entity boundaries.Therefore, Therefore, it is crucial to choose the right number of capsules.</p>
<p>In order to explore this effect, we set different numbers of capsules in the experiment and conducted comparative experiments under the same data set and training conditions.When the number of capsules is 8, the F1 score of the model is 86.2%.As the number of capsules increases, the model performance gradually improves.When the number of capsules is 32, the model achieves the best performance with an F1 score of 89.8%.When the number of capsules continues to increase to 64 and 128, the model performance decreases.This shows that the change in the number of capsules directly affects the recognition accuracy and generalization ability of the model.In summary, when the number of capsules is 32, the model performs best in the Chinese NER task, and can avoid overfitting while ensuring high recognition accuracy.The specific data are shown in Table 3.</p>
<p>The experimental results show that there is a non-monotonic relationship between the number of capsules and model performance across all evaluation metrics.When the number of capsules increases from 8 to 32, the accuracy, precision, recall and F1 score all show a gradual improvement, indicating that moderate parameter expansion enhances the feature representation ability.However, the performance begins to decline when the number of capsules exceeds 32: the accuracy drops to 88.1% at 64 capsules and further drops to 86.9% at 128 capsules.This inverted U-shaped curve indicates that overfitting occurs when the optimal complexity threshold is exceeded.Instead of focusing on the core semantic patterns required for entity recognition, the model overfits to noisy details in the training data, leading to a sharp decline in its generalization ability on unseen test data.Meanwhile, an excessive number of capsules also triggers feature redundancy: the calculation of coupling coefficients between low-level and high-level capsules becomes chaotic due to parameter scale expansion, making the dynamic routing mechanism unable to effectively focus on key features.Instead, it incorporates irrelevant noise into feature modeling, interfering with the judgment of entity boundaries and semantic categories.Furthermore, computational efficiency is significantly reduced.The high-dimensional vector operations and iterative routing calculations caused by a large number of capsules result in a marked slowdown in model inference speed, increasing memory usage and computational costs.The peak performance when the number of capsules reaches 32 indicates that the model capacity and computational efficiency have reached the best balance.The trend of change is shown in Fig 9.  4.</p>
<p>In the Chinese NER task, the 3×3 convolution kernel exhibits the best feature extraction capability, and its performance on the Toutiao, Weibo, and MSRA datasets significantly outperforms other sizes.The perception mechanism of this size has a dual advantage: it ensures that the key local context of the entity boundary is captured, and effectively avoids the semantic interference caused by an overly large receptive field.Experimental data show that when the convolution kernel size exceeds 3×3, the recall rate of the model decays significantly more than the precision rate.This feature is particularly evident in long text scenarios with blurred entity boundaries.Large-size convolution kernels will weaken the ability to capture finegrained semantic clues.For example, when a 1×3 convolution kernel is used in the Weibo   dataset, the F1 value drops by 4.8% due to the inability to effectively associate discrete semantic features.Research has confirmed that the optimization of convolution kernel size is essentially to establish a dynamic balance between feature perception granularity and noise suppression capability.The 3×3 configuration achieves the best engineering compromise under current experimental conditions by taking into account both local feature focus and global noise filtering.This finding effectively confirms the design concept of stacking small convolution kernels in deep networks.</p>
<p>From the visualization results in Fig 10, we can see that when the convolution kernel size in the DCAM module is set to 3, this model can achieve the best task effect on three datasets while fixing the remaining modules and related parameters.Therefore, the convolution kernel size parameter of the DCAM module is set to 3.</p>
<p>Impact of epoch on model performance.</p>
<p>In the Chinese NER task, the impact of epoch on model performance is important.In order to explore this impact, we set different epoch values and conducted comparative experiments on the three selected datasets while fixing other hyperparameters.The experimental results show that with the increase of epochs, the model performance shows a trend of first rising and then falling.In the early stage of training, the model quickly learns the basic entity pattern.As the training deepens, the performance continues to improve.At 30 epochs, the three datasets reach the best state synchronously.When the training cycle exceeds 30 epochs, the three datasets all show slight performance degradation.In summary, when the epoch=30, the CEAF model performs best, which can avoid overfitting problems while ensuring high performance.The specific data are shown in Table 5.</p>
<p>The experimental results reveal the relationship between training rounds and F1-score performance on three Chinese natural language processing datasets.The performance continued to improve from round 10 to round 30, with peak F1 scores reaching 87.5% for Toutiao, 88.5% for Weibo, and 87.9% for MSRA, indicating the gradual optimization of feature learning in  The experimental results show that modular integration can achieve progressive performance improvements, and synergistic effects are observed in multi-component configurations.The basic BERT-BiLSTM model achieves baseline F1 scores on Toutiao, Weibo, MSRA, and OntoNotes 5.0 datasets, establishing performance benchmarks for each dataset, but there is still room for improvement in complex entity recognition tasks.Single-module variants show differentiated gains: CEAF-D shows better improvements on MSRA and OntoNotes datasets compared to CEAF-C and CEAF-A, indicating its unique effectiveness in processing entity-rich corpora.The dual-module combination reveals nonlinear interactions between components.CEAF-DA surpasses other combinations with a higher F1 value, indicating that there is a complementary feature extraction mechanism between the attention-based DCAM module and the feed-forward AFFN module.CEAF, as a complete model, has the best performance, verifying the effectiveness of our proposed model.The specific data are shown in Table 6.</p>
<p>The experimental results confirm that the final complete model CEAF has the best performance.The line chart of F1 score comparison is shown in Fig 12.</p>
<p>Comparative experiments between CEAF Model and SOTA model</p>
<p>The comparison models used in the comparative experiments in this section are as follows:</p>
<p>(1) BERT [1] has achieved remarkable success in the application of Chinese Named Entity Recognition (NER).As a pre-trained language model based on the Transformer architecture, BERT leverages bidirectional context modeling to more accurately comprehend semantic relationships within text, thereby significantly enhancing the performance of NER.In Chinese NER tasks, BERT is typically employed as a feature extractor, converting input text into high-quality word vector representations that capture the meaning of words in varying contexts.These vectors are then fed into downstream models to perform entity boundary detection and type classification.Through this approach, BERT not only effectively addresses challenges such as Chinese word segmentation and ambiguity but also strengthens the model's (2) Baichuan2-13B-Chat [52] is a large language model with 13 billion parameters, developed by Baichuan Intelligence as part of the Baichuan2 series.It is an open-source and commercially available model trained on a high-quality corpus of 2.6 trillion tokens, demonstrating robust capabilities in both Chinese and English processing.The model excels in various general-purpose and domain-specific benchmark tests and supports both interactive dialogue and fine-tuning for downstream tasks, particularly showcasing exceptional comprehension and generation in Chinese contexts.In the field of Chinese NER, Baichuan2-13B-Chat leverages its strong contextual modeling and deep understanding of Chinese linguistic phenomena to efficiently identify entity types such as person names, location names, and organization names.It achieves competitive F1 scores on mainstream Chinese NER datasets including MSRA, Weibo, and Toutiao, demonstrating notable robustness especially when processing informal texts and complex syntactic structures.Baichuan Intelligence officially released the model on September 6, 2023, and announced its compatibility with the Ascend AI hardware and software platform as well as the MindSpore open-source community, further enhancing its ecosystem integration and practical deployment potential.</p>
<p>(3) Qwen-14B-Chat [53] is a 14-billion-parameter conversational large language model from the Qwen series, developed by Alibaba Cloud and officially open-sourced on September 25, 2023.Trained on large-scale, high-quality corpora, the model supports an extended context length of up to 8K tokens and demonstrates exceptional proficiency in both Chinese and English, with particularly strong performance in Chinese language comprehension and generation.In the context of Chinese NER, Qwen-14B-Chat exhibits outstanding performance.Its model architecture and training methodology enable it to effectively identify entity types such as person names, locations, and organizations, especially in complex sentence structures and contexts with strong dependencies.The model achieves competitive F1 scores across major Chinese NER datasets, including MSRA, Weibo, and Toutiao, demonstrating robust generalization and reliability.</p>
<p>(4) SoftLexicon [54], an innovative Chinese NER model introduced at ACL 2020, enhances character sequence representation by integrating lexical information to address the key challenge of Chinese NER: the absence of explicit word boundaries.Its innovation lies in a flexible lexical fusion mechanism-dynamically matching dictionary-based lexical information with character representations-thereby improving performance without substantial computational overhead, making it well-suited for boundary-ambiguous Chinese.SoftLexicon has shown strong performance across diverse Chinese NER tasks, including medical entity recognition and news text processing.In healthcare, it accurately identifies professional terms like disease and drug names, supporting medical knowledge base construction.</p>
<p>(5) BERT-BiLSTM-CRF [44]: BERT-BiLSTM-CRF is the mainstream deep learning model in the field of Chinese NER.Its core architecture combines the advantages of pre-trained language models, bidirectional sequence modeling and global sequence optimization.The model first generates dynamic contextualized word vectors through the BERT layer and captures deep language features at the phrase level, syntactic level and semantic level.After receiving the context vector output by BERT, the BiLSTM layer captures long-distance sequence dependencies through the forward and backward dual-path LSTM network, and is particularly good at dealing with the problem of blurred Chinese entity boundaries and nested entities.The CRF layer acts as a decoder and achieves global optimal labeling by modeling the label transfer probability matrix, effectively eliminating illegal label combinations generated by independent predictions.( 6) SpanKL [45]: SpanKL is an innovative span representation model designed for continuous learning scenarios in the field of Chinese NER.Its core solves the two major challenges of nested entity recognition and catastrophic forgetting through a dual-path architecture.The model uses BERT-large as a context encoder to generate dynamic word vectors, builds a span representation layer through an entity type-specific feedforward network, and treats all possible text spans in a sentence as independent units for multi-label classification, thereby effectively handling the problems of nested entities and overlapping entities.(7) HiNER [46]: The core breakthrough of the HiNER model is to solve the problem of nested entity and discontinuous entity recognition through a hierarchical feature fusion mechanism, and achieve the current optimal performance on 7 benchmark datasets.This model innovatively combines character-level relationship classification with multi-source semantic information fusion.At the feature representation level, HiNER adopts a local-global attention dual-path architecture.This hierarchical mechanism significantly improves the ability to parse complex Chinese language structures.( 8) RBAC [47]: RBAC is an innovative model proposed in the field of Chinese medical NER for data-scarce scenarios.Its core is to achieve dual optimization of deep semantic feature mining and data enhancement through a multi-task architecture that combines word segmentation and entity recognition.The model uses the RoBERTa pre-trained model as the base encoder, extracts the deep bidirectional semantic representation of the text through a bidirectional gated recurrent unit, and designs a two-way decoding structure: a word segmentation module and an entity recognition module, and finally outputs the global optimal label sequence through the CRF layer.RBAC innovatively introduces a semantic search-based data augmentation method, expanding training samples via similar entity replacement and contextual semantic matching.</p>
<p>(9) LEBERT [48]: LEBERT is an innovative model designed for vocabulary enhancement scenarios in the field of Chinese NER.Its core breakthrough lies in deeply injecting vocabulary information into the underlying encoding process of BERT, which overcomes the limitation of traditional methods that only integrate vocabulary features at the end of the model.The model dynamically integrates dictionary information by constructing character -word pair sequences and designs vocabulary adapters to achieve semantic fusion in the middle layer of BERT, significantly improving the accuracy of entity boundary recognition and type discrimination.</p>
<p>The CEAF model is compared with the other five SOTA models.Experimental evaluation shows that the geometric representation reconstruction achieved by the CEAF model through the capsule network has established a new paradigm for Chinese NER, fundamentally transforming the traditional symbol processing method into vector space modeling.In the Chinese NER task, this paper systematically evaluates the performance of various models on three datasets, namely Weibo, MSRA, and Toutiao.The experimental results use accuracy, precision, recall, and F1-score as the main evaluation metrics, comparing multiple methods including large language models such as BERT, Baichuan2-13B-Chat, Qwen-14B-Chat, and dictionary-injection-based models such as SoftLexicon and LEBERT.Although our proposed model is slightly inferior to the optimal model in the MSRA dataset with normalized texts, it achieves the best F1-score in the Weibo dataset and the Toutiao dataset, which contain a large number of nested entities and unnormalized texts, demonstrating excellent recognition performance.</p>
<p>Although large language models, relying on massive general knowledge pre-training, show certain basic performance in the MSRA dataset with normalized texts, they suffer from prominent data missing issues.Moreover, when facing scenarios with nested entities and unnormalized texts, due to the difficulty in accurately adapting to domain characteristics, their performance advantages are not obvious, and it is hard for them to effectively capture the boundaries and semantics of complex entities.Dictionary-injection-based models supplement knowledge using domain dictionaries and have adaptability in some scenarios.However, the coverage of dictionaries for unknown and emerging entities is limited.When dealing with the complex texts in the Weibo and Toutiao datasets, the flexibility and comprehensiveness of entity mining are restricted.</p>
<p>Our proposed CEAF model achieves the best F1-score in the Weibo and Toutiao datasets, which have numerous nested entities and a high proportion of unnormalized texts.It can accurately capture entity features and adapt to complex scenarios.In the MSRA dataset with normalized texts, the CEAF model also maintains a high recognition level, with a negligible gap from the optimal model.Relying on architectural advantages, the CEAF model efficiently mines entity features in complex texts.It demonstrates excellent performance in multiple types of datasets, especially in complex scenarios, deeply adapts to the diversity of the Chinese named entity recognition task, and highlights the superiority of the model design.The data are shown in Table 7.These data indicate that for a morphologically complex language like Chinese, vector space geometric modeling provides a viable alternative to traditional neural architectures.The visualization results in Figs 13,14,and 15 show that CEAF demonstrates superior capabilities in Chinese text through multi-scale feature fusion.</p>
<p>Cross-language generalization experiments</p>
<p>In order to test whether the CEAF model we proposed has the same superior performance in English NER, we will compare the current SOTA model on English datasets.In addition to the OntoNotes 5.0 dataset, we also selected CoNLL-2003 and GENIA datasets for verification, covering general fields, multi-domain complex entities, and professional fields to verify the generalization of the model.The dataset division is shown in Table 8 below:  The CEAF model was tested three times on these three datasets and the average value was taken to reduce the error.The experimental data are shown in Table 9 below.</p>
<p>Based on the cross-domain validation experimental data of the CEAF module, we found that the model also demonstrated significant technological breakthroughs and domain adaptability in English NER tasks.In the evaluation of the general domain dataset CoNLL-2003, the F1 value of the CEAF model reached 94.7%, an increase of 1.2 percentage points over the traditional SpanBERT [49].In the multi-domain complex entity scene dataset OntoNotes 5.0, the F1 value of the CEAF model we proposed surpasses the LUKE model [51].The core reason is that we introduced the boundary diffusion mechanism into the multi-head attention layer to enhance the semantic perception ability of entity boundaries.In the GENIA dataset, although the F1 value of CEAF is slightly lower than that of the LUKE model, its recall rate in long entity recognition is improved by 12%.Through Monte Carlo cross-validation, the model is subjected to multiple random partitioning tests and it is found that the performance decay rate of CEAF in cross-domain migration is only 5.3%, which is significantly lower than that of the BERT-MRC model [50], proving that its parameter sharing mechanism can effectively capture the cross-language entity feature rules.The data visualization is shown in Fig 16 below.</p>
<p>The CEAF model we proposed successfully migrated the vocabulary adapter module that has been proven effective in Chinese NER to the English scenario through retrieval-enhanced template generation technology.In the future, it can be further integrated to verify its universality in a larger cross-language corpus.</p>
<p>Discussion</p>
<p>The CEAF model proposed in this study demonstrates significant progress in addressing the complexities of Chinese NER through its integration of hierarchical feature extraction and adaptive fusion mechanisms.The model introduces the geometric feature routing mechanism of the capsule network to realize the hierarchical parsing of nested entities.At the same time, it uses the dynamic boundary attention module DCAM to strengthen the probabilistic modeling of entity boundaries, effectively dealing with the entity span ambiguity problem caused by the lack of explicit word segmentation boundaries in Chinese.Experimental verification on the selected dataset shows that the CEAF model can improve the F1 score of ambiguous entity recognition, and the recognition of nested entities and non-standard entities becomes more accurate.However, emphasis must be placed on the computational overhead and potential limitations of this multi-stage architecture.While the dynamic routing mechanism of capsule networks enhances the capability to parse nested entities, the matrix operations involved increase inference time, restricting its deployment in resource-constrained environments such as edge devices.Due to its reliance on BERT pre-trained embeddings, the model requires 24GB of memory for training, which constrains application flexibility in low-resource scenarios.Additionally, the fusion of multiple components prolongs the training cycle.Although the inference latency for single sentences meets basic real-time requirements, it remains significantly higher compared to lightweight models like ALBERT.Therefore, optimization through model parallelism or quantization acceleration is necessary for high-concurrency scenarios.Future research will explore lightweight alternatives, dynamic module pruning, and cross-hardware testing to further balance performance and resource requirements.</p>
<p>A key advantage of the CEAF model lies in its robustness to annotation noise and its ability to generalize from limited labeled data using pretrained embeddings, aligning with the emerging trend of integrating transfer learning and feature fusion in named entity recognition research.Nevertheless, the model's reliance on high-quality BERT embeddings raises concerns about domain-specific transfer issues.Incorporating adversarial domain adaptation modules into the training pipeline could further enhance cross-domain robustness, a direction currently being explored in boundary-aware model research.</p>
<p>Conclusion</p>
<p>In this paper, we proposed the CEAF model, which achieves state-of-the-art performance on multiple benchmark datasets by hierarchically integrating three core components: pretrained language modeling, sequential pattern learning, and adaptive feature enhancementfusion.The CEAF model uses BERT layer embeddings to capture character-level semantics and global contextual dependencies, while Bi-LSTM further refines local sequential pattern learning by modeling temporal dependencies between adjacent characters.It integrates feature enhancement modules-including the DCAM and capsule network-to strengthen boundary awareness and explicitly model hierarchical part-whole relationships.The AFFN module further ensures cohesive integration of heterogeneous feature representations, effectively mitigating information fragmentation and addressing limitations of existing methods in handling boundary ambiguity and nested entities.</p>
<p>Experimental verification in different fields shows that compared with classical models,the CEAF model delivers consistent quantitative gains while highlighting two critical design principles: preserving feature hierarchical structures and enhancing dynamic contextual adaptability.Additionally, it exhibits robustness to China's inherent annotation inconsistencies and morphological irregularities in text,while achieving notable improvements in recognizing structurally complex nested entities and semantically ambiguous terms.</p>
<p>The CEAF model realizes geometric feature routing via capsule networks and eliminates feature fragmentation through an adaptive fusion network.In future work, our research will focus on optimizing the computational efficiency of dynamic routing algorithms for longdistance semantic modeling, constructing cross-task transfer frameworks, and developing multimodal enhancement architectures to tackle entity disambiguation in complex social media scenarios with mixed text, images, and informal expressions.</p>
<p>Fig 1 .
1
Fig 1.Comparison of NER Cases between Chinese and English.https://doi.org/10.1371/journal.pone.0332622.g001</p>
<p>Fig 2 .
2
Fig 2. The overall framework of the CEAF model proposed in this paper.https://doi.org/10.1371/journal.pone.0332622.g002</p>
<p>Transformer model can achieve deep and fine-grained extraction of feature representations.Its model structure is shown in Fig 3.Each layer contains two core modules: selfattention and feed-forward neural network (FFN).The self-attention mechanism can dynamically capture the association strength of tokens at different positions in the text, enabling the modeling of long-distance semantic dependencies; the FFN deepens the expression of local</p>
<p>Fig 3 .
3
Fig 3. Transformer model architecture.https://doi.org/10.1371/journal.pone.0332622.g003</p>
<p>Fig 4 .
4
Fig 4. Bi-LSTM architecture.https://doi.org/10.1371/journal.pone.0332622.g004</p>
<p>Fig 5 .
5
Fig 5.The network structure of the LSTM.https://doi.org/10.1371/journal.pone.0332622.g005</p>
<p>Fig 6 .
6
Fig 6.Dynamic routing mechanism.https://doi.org/10.1371/journal.pone.0332622.g006</p>
<p>3 . 3 . 2
332
DCAM module.The module structure of the DCAM (Deep context feature attention module) is shown in Fig 7 below.</p>
<p>Fig 7 .
7
Fig 7. The structure of DCAM module.https://doi.org/10.1371/journal.pone.0332622.g007</p>
<p>3 . 3 . 3 Fig 8 .
3338
Fig 8.The structure of AFFN module.https://doi.org/10.1371/journal.pone.0332622.g008</p>
<p>4 . 3 . 2
432
Convolution kernel size ablation experiment in DCAM module.With the remaining modules and related parameters fixed, the selection of convolution kernel size in the DCAM module is experimentally explored on the Toutiao, Weibo and MSRA datasets.The experimental results are shown in Table</p>
<p>Fig 9 .
9
Fig 9.The effect of the number of capsules on the performance of the model.https://doi.org/10.1371/journal.pone.0332622.g009</p>
<p>Fig 10 .
10
Fig 10.F1 values of different convolutional kernel sizes.https://doi.org/10.1371/journal.pone.0332622.g010</p>
<p>4 . 3 . 4
434
the early training stage.As the number of epochs continues to increase, the model performance begins to decline.This phenomenon may be due to the over-learning of the model on the training set, resulting in a decrease in the generalization ability on the validation set.The trend of change is shown in Fig 11.Ablation experiment of CEAF model variants.This experiment compares seven models, the basic model BERT-BiLSTM and variant models CEAF-C, CEAF-D, CEAF-A, CEAF-CD, CEAF-CA, CEAF-DA, and CEAF.Among them, CEAF-C means adding a capsule network module to the basic model, CEAF-D means adding a DCAM module to the basic model, CEAF-A means adding an AFFN module to the basic model, CEAF-CD means adding a capsule network and DCAM modules to the basic model, CEAF-CA means adding a capsule network and AFFN modules to the basic model, CEAF-DA means adding a DCAM module and AFFN module to the basic model, and CEAF means the complete model.To ensure the reliability of the experimental results, we repeated the experiment three times on each model on different data sets on the same training and test data sets, which can effectively eliminate the influence of accidental factors.</p>
<p>Fig 11 .
11
Fig 11.The effect of epochs on experimental performance.https://doi.org/10.1371/journal.pone.0332622.g011</p>
<p>Fig 13 .Fig 14 .Fig 15 .
131415
Fig 13.Comparison with SOTA model s under the Toutiao dataset.https://doi.org/10.1371/journal.pone.0332622.g013</p>
<p>3, a series of ablation studies are conducted on the CEAF model: in Sect 4.3.1, the number of capsules in the capsule network is ablated; in Sect 4.3.2, the convolution kernel size in the DCAM module is ablated; in Sect 4.3.3, the epoch is ablated; in Sect 4.3.4,the model variants of CEAF are ablated; in Sect 4.4, we conduct cross-language generalization experiments; in Sect 4.5, the CEAF model is compared with the SOTA model; All experiments in this paper are completed using RTX 4060 Ti graphics card and deep learning framework Pytorch.</p>
<p>Table 1 .
1
Hyperparameters.
Parameter nameParameter valueOptimizerAdamBatch Size16Max_seq_length128Learning_rate2e-5Drop_out0.1
https://doi.org/10.1371/journal.pone.0332622.t001</p>
<p>Table 2 .
2
Dataset.
DatasetTrainingValidationTestToutiao537528702733Weibo5200510480MSRA675013501193OntoNotes 5.0998714211377</p>
<p>Table 3 . The results of the ablation experiment with the number of capsules.
3Capsule quantityAccuracy(%)Precision(%)Recall(%)F1 score(%)885.986.585.786.21687.988.487.888.13289.690.189.589.86488.188.688.088.312886.987.486.887.1
https://doi.org/10.1371/journal.pone.0332622.t003</p>
<p>Table 4 . Experimental results of convolution kernel size parameters of DCAM module.
4DatasetConvolutionAccuracy (%)Precision (%)Recall (%)F1 (%)Kernel SizeToutiao185.184.382.583.4286.786.084.885.4388.989.287.688.4487.387.585.986.7585.584.883.183.9Weibo183.281.579.880.6284.883.781.982.8386.586.184.785.4485.084.382.483.3583.682.180.281.1MSRA189.388.787.287.9290.189.588.488.9391.891.690.591.0490.590.088.789.3589.788.987.588.2https://doi.org/10.1371/journal.pone.0332622.t004</p>
<p>Table 5 . F1 values for different epochs.
5EpochToutiao-F1(%)Weibo-F1(%)MSRA-F1(%)1085.286.085.42086.887.687.03087.588.587.94087.087.887.45086.286.986.3
https://doi.org/10.1371/journal.pone.0332622.t005</p>
<p>Table 6 . Results of ablation experiments.
6
to recognize complex named entities, including locations, person names, and organization names.Furthermore, BERT's pre-training on large-scale Chinese corpora endows it with robust generalization capabilities, enabling it to perform exceptionally well across Chinese datasets such as MSRA, Weibo, and Toutiao.In practical applications, BERT is often combined with domain adaptation techniques to further improve recognition performance in specialized domains or low-resource environments.
Variant ModelToutiao-F1 (%)Weibo-F1 (%)MSRA-F1 (%)OntoNotes 5.0(%)BERT -BiLSTM85.284.487.888.3CEAF -C85.884.588.788.1CEAF -D86.685.089.489.5CEAF -A86.285.489.089.4CEAF -CD87.586.090.190.7CEAF -CA87.785.889.890.3CEAF -DA87.986.190.691.7CEAF88.686.591.292.2
https://doi.org/10.1371/journal.pone.0332622.t006Fig 12. F1 Scores of variant ablation.https://doi.org/10.1371/journal.pone.0332622.g012ability</p>
<p>Table 7 . Metrics compared to the SOTA model.
7DatabaseExisting modelAccuracy (%) Precision (%) Recall (%)F1 (%)ToutiaoBERT---82.9Baichuan2-13B-Chat----Qwen-14B-Chat----SoftLexicon----BERT -BiLSTM -CRF83.184.082.583.2SpanKL85.385.884.585.2HiNER86.787.385.986.6RBAC87.287.886.587.1LEBERT87.988.587.287.9CEAF (Ours)90.591.289.790.4WeiboBERT---80.4Baichuan2-13B-Chat----Qwen-14B-Chat----SoftLexicon---74.0BERT -BiLSTM -CRF81.482.080.681.3SpanKL83.884.583.083.7HiNER85.285.984.485.1RBAC85.986.685.185.8LEBERT86.587.285.786.4CEAF (Ours)89.189.888.489.1MSRABERT---92.8Baichuan2-13B-Chat---91.8Qwen-14B-Chat---92.5SoftLexicon---93.8BERT -BiLSTM -CRF86.286.785.386.0SpanKL87.588.086.887.4HiNER88.288.887.588.1RBAC88.989.588.288.8LEBERT89.690.288.989.5CEAF (Ours)91.892.491.291.8
https://doi.org/10.1371/journal.pone.0332622.t007</p>
<p>Table 8 . English dataset information.
8DatabaseFieldTrainValidationTestCoNLL-2003News1498734663684OntoNotes 5.0Multi-domain5992485288262GENIABiomedicine1440018001800
https://doi.org/10.1371/journal.pone.0332622.t008</p>
<p>Table 9 . F1 score comparison with SOTA models.
9SOTA ModelCoNLL -2003 (F1)OntoNotes5.0 (F1)GENIA (F1)(%)(%)(%)BERT -MRC93.381.276.8SpanBERT93.583.978.3LUKE94.184.586.5CEAF (ours)94.787.282.1
https://doi.org/10.1371/journal.pone.0332622.t009Fig 16.F1 scores compared with SOTA model.https://doi.org/10.1371/journal.pone.0332622.g016</p>
<p>PLOS One https://doi.org/10.1371/journal.pone.0332622October 7, 2025<br />
All relevant data are within the manuscript.Toutiao dataset: https://github.com/aceimnorstuvwxz/toutiao-text-classfication-dataset.Weibo dataset: https://github.com/GYXie/weibo-dataset.MSRA dataset: https://www.modelscope.cn/datasets/iic/msraner/files.OntoNotes5.0 dataset: https://huggingface.co/ datasets/ontonotes/conll2012-ontonotesv5.The funders (Shanghai Maritime University's Top Innovative Talents Training Program for Graduate Students in 2023 under Grant 2023YBR017) had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.The roles related to funding acquisition were undertaken by Yangshuyi Xu as stated in the contribution details.Author contributionsConceptualization: Guangzhong Liu.Funding acquisition: Guangzhong Liu.Methodology: Siyu Ma.Resources: Yangshuyi Xu.Validation: Yangshuyi Xu.Visualization: Siyu Ma.PLOS OneCapsule network enhanced feature fusion architecture for Chinese Named Entity Recognition Writing -original draft: Siyu Ma.Writing -review &amp; editing: Siyu Ma, Guangzhong Liu, Yangshuyi Xu.
An effective multi-modal adaptive contextual feature information fusion method for Chinese long text classification. Y Xu, G Liu, L Zhang, X Shen, S Luo, Artificial Intelligence Review. 5792332024</p>
<p>Aishell-ner: Named entity recognition from chinese speech. B Chen, G Xu, X Wang, P Xie, M Zhang, F Huang, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2022</p>
<p>MCL: Multi-Granularity Contrastive Learning Framework for Chinese NER. S Zhao, C Wang, M Hu, T Yan, M Wang, 10.1609/aaai.v37i11.26640AAAI. 37112023</p>
<p>MFE-NER: multi-feature fusion embedding for Chinese named entity recognition. J Li, K Meng, China National Conference on Chinese Computational Linguistics. 2024</p>
<p>Improving Chinese named entity recognition by large-scale syntactic dependency graph. P Zhu, D Cheng, F Yang, Y Luo, D Huang, W Qian, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 302022</p>
<p>Adaptive threshold selective self-attention for Chinese NER. B Hu, Z Huang, M Hu, Z Zhang, Y Dou, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>Named entity recognition for Chinese based on global pointer and adversarial training. H Li, M Cheng, Z Yang, L Yang, Y Chua, 10.1038/s41598-023-30355-y36828907Sci Rep. 13132422023</p>
<p>Chinese toponym recognition with variant neural structures from social media messages based on BERT methods. K Ma, Y Tan, Z Xie, Q Qiu, S Chen, Journal of Geographical Systems. 2422022</p>
<p>Chinese Named Entity Recognition of Epidemiological Investigation of Information on COVID-19 Based on BERT. C Yang, L Sheng, Z Wei, Wang W , 10.1109/access.2022.3210119IEEE Access. 102022</p>
<p>TOE: A grid-tagging discontinuous NER model enhanced by embedding tag/word relations and more fine-grained tags. J Liu, Ji D Li, J Xie, D Teng, C Zhao, L , IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312022</p>
<p>Delving deep into regularity: a simple but effective method for Chinese named entity recognition. Y Gu, X Qu, Z Wang, Y Zheng, B Huai, N J Yuan, arXiv:2204.055442022arXiv preprint</p>
<p>Nflat: Non-flat-lattice transformer for Chinese named entity recognition. S Wu, X Song, Z Feng, X J Wu, 10.48550/arXiv.2205.058322022arXiv preprint</p>
<p>Adversarial Multi-task Learning for Efficient Chinese Named Entity Recognition. Y Yan, P Zhu, D Cheng, F Yang, Y Luo, 10.1145/3603626ACM Trans Asian Low-Resour Lang Inf Process. 2272023</p>
<p>Nested named entity recognition: a survey. Y Wang, H Tong, Z Zhu, Y Li, ACM Transactions on Knowledge Discovery from Data (TKDD). 1662022</p>
<p>Chinese NER Using Multi-View Transformer. Y Xiao, Ji Z Li, J Han, M , Speech, and Language Processing. 2024</p>
<p>A Local Information Perception Enhancement-Based Method for Chinese NER. M Zhang, L Lu, 10.3390/app13179948Applied Sciences. 131799482023</p>
<p>DAE-NER: Dual-channel attention enhancement for Chinese named entity recognition. J Liu, M Sun, W Zhang, G Xie, Y Jing, X Li, Comput Speech Lang. 851015812024</p>
<p>MFE-NER: multi-feature fusion embedding for Chinese named entity recognition. J Li, K Meng, China National Conference on Chinese Computational Linguistics. 2024</p>
<p>LADA-trans-NER: Adaptive efficient transformer for Chinese named entity recognition using lexicon-attention and data-augmentation. J Liu, C Liu, N Li, S Gao, M Liu, D Zhu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>LLet: Lightweight lexicon-enhanced transformer for Chinese NER. Z Ji, Y Xiao, ICASSP). 2024. 12677-81Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. the IEEE International Conference on Acoustics, Speech and Signal Processing</p>
<p>MCL-NER: Cross-lingual named entity recognition via multi-view contrastive learning. Y Mo, J Yang, J Liu, Q Wang, R Chen, J Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>A multi-task BERT-BiLSTM-AM-CRF strategy for Chinese named entity recognition. X Tang, Y Huang, M Xia, C Long, Neural Processing Letters. 5522023</p>
<p>Attribute Feature Perturbation-Based Augmentation of SAR Target Data. R Jin, J Cheng, W Wang, H Zhang, J Zhang, 10.3390/s2415500639124052Sensors (Basel). 241550062024</p>
<p>Chinese named entity recognition based on BERT and lightweight feature extraction model. R Yang, Y Gan, C Zhang, Information. 13115152022</p>
<p>A robust Chinese named entity recognition method based on integrating dual-layer features and CSBERT. Y Xu, X Tan, X Tong, W Zhang, Applied Sciences. 14310602024</p>
<p>Marking word boundaries improves Chinese BERT. L Li, CCF Int. Conf. Nat. Lang. Process. Chin. ComputY Dai, CCF Int. Conf. Nat. Lang. Process. Chin. ComputD Tang, CCF Int. Conf. Nat. Lang. Process. Chin. ComputX Qiu, CCF Int. Conf. Nat. Lang. Process. Chin. ComputZ Xu, CCF Int. Conf. Nat. Lang. Process. Chin. ComputS Shi, CCF Int. Conf. Nat. Lang. Process. Chin. ComputMarkbert, CCF Int. Conf. Nat. Lang. Process. Chin. ComputProc. null2023</p>
<p>Enhanced Chinese domain named entity recognition: An approach with lexicon boundary and frequency weight features. Y Guo, S Feng, F Liu, Applied Sciences. 1413542023</p>
<p>Chinese Nested Named Entity Recognition Based on Boundary Prompt. Z Li, M Song, Y Zhu, L Zhang, Proceedings of the International Conference on Web Information Systems and Applications. the International Conference on Web Information Systems and Applications2023</p>
<p>Medical entity recognition and knowledge map relationship analysis of Chinese EMRs based on improved BiLSTM-CRF. J Ke, Wang W Chen, X Gou, J Gao, Y , Jin S , Computers and Electrical Engineering. 1081087092023</p>
<p>Research on Chinese Named Entity Recognition Based on RoBERTa and Word Fusion. W Wang, B Zhang, X Zhu, H Deng, Proc. IEEE Int. Conf. Inf. Technol. Netw. Electron. Autom. Control (ITNEC). 2023</p>
<p>Exploring Interactive and Contrastive Relations for Nested Named Entity Recognition. Q Zheng, Y Wu, G Wang, Y Chen, W Wu, Z Zhang, 10.1109/taslp.2023.3293047IEEE/ACM Trans Audio Speech Lang Process. 312023</p>
<p>Named Entity Recognition in Aerospace Based on Multi-Feature Fusion Transformer. J Chu, Y Liu, Q Yue, Scientific Reports. 1418272024</p>
<p>Named entity recognition method based on BERT-whitening and dynamic fusion model. M Liang, Y Shi, Proc. Int. Conf. Nat. Lang. Process. (ICNLP). Int. Conf. Nat. Lang. ess. (ICNLP)2023</p>
<p>Chinese named entity recognition method based on multi-feature fusion and biaffine. X Ke, X Wu, Z Ou, Complex &amp; Intelligent Systems. 1052024</p>
<p>Named Entity Recognition Model Based on Feature Fusion. Information. Z Sun, X Li, 202314133</p>
<p>A multi-granularity word fusion method for Chinese NER. T Liu, J Gao, W Ni, Applied Sciences. 13527892023</p>
<p>Improving Chinese Named Entity Recognition by Interactive Fusion of Contextual Representation and Glyph Representation. R Gu, T Wang, J Deng, Applied Sciences. 13742992023</p>
<p>Named Entity Recognition and Classification in Historical Documents: A Survey. M Ehrmann, A Hamdi, E L Pontes, ACM Comput Surv. 5622023</p>
<p>A survey on recent advances in named entity recognition. I Keraghel, S Morbieu, M Nadif, 10.48550/arXiv.2401.108252024arXiv preprint</p>
<p>Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M W Chang, K Lee, Proceedings of the NAACL-HLT. the NAACL-HLT2019</p>
<p>Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, Advances in Neural Information Processing Systems. 2017</p>
<p>Dynamic Routing Between Capsules. S Sabour, N Frosst, G E Hinton, Adv Neural Inf Process Syst. 302017</p>
<p>A review on the attention mechanism of deep learning. Z Niu, G Zhong, H Yu, Neurocomputing. 4522021</p>
<p>BERT-BiLSTM-CRF Chinese resume named entity recognition combining attention mechanisms. W He, Y Xu, Q Yu, Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering. the 4th International Conference on Artificial Intelligence and Computer Engineering2023</p>
<p>A neural span-based continual named entity recognition model. Y Zhang, Q Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023</p>
<p>HiNER: hierarchical feature fusion for Chinese Named Entity Recognition. S Hou, Y Qian, J Chen, Neurocomputing. 6111286672025</p>
<p>Chinese Named Entity Recognition Model based on multi-task learning. Q Fang, Y Li, H Feng, Y Ruan, Applied Sciences. 13847702023</p>
<p>Lexicon enhanced chinese sequence labeling using BERT adapter. W Liu, X Fu, Y Zhang, 10.48550/arXiv.2105.071482021arXiv preprint</p>
<p>Improving pre-training by representing and predicting spans. M Joshi, D Chen, Y Liu, Spanbert, Trans Assoc Comput Linguist. 82020</p>
<p>X Li, J Feng, Y Meng, arXiv:1910.11476.2019A Unified MRC Framework for Named Entity Recognition. arXiv preprint</p>
<p>I Yamada, A Asai, H Shindo, Luke, 10.48550/arXiv.2010.01057Deep Contextualized Entity Representations with Entity-Aware Self-Attention. 2020arXiv preprint</p>
<p>Baichuan 2: Open large-scale language models. Y Bai, 10.48550/arXiv.2309.10305v22023arXiv preprint</p>
<p>Large-scale language models with improved alignment and instruction following. Alibaba Cloud, T Qwen, 10.48550/arXiv.2309.16609v22023arXiv preprint</p>
<p>SoftLexicon: Incorporating Lexicon Knowledge into Chinese NER via Soft Selection. X Liu, P Qiu, X Huang, X Hu, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>            </div>
        </div>

    </div>
</body>
</html>